// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/ai/generativelanguage/v1/safety.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";

export const protobufPackage = "google.ai.generativelanguage.v1";

/**
 * The category of a rating.
 *
 * These categories cover various kinds of harms that developers
 * may wish to adjust.
 */
export enum HarmCategory {
  /** HARM_CATEGORY_UNSPECIFIED - Category is unspecified. */
  HARM_CATEGORY_UNSPECIFIED = 0,
  /**
   * HARM_CATEGORY_DEROGATORY - *PaLM** - Negative or harmful comments targeting identity and/or protected
   * attribute.
   */
  HARM_CATEGORY_DEROGATORY = 1,
  /** HARM_CATEGORY_TOXICITY - *PaLM** - Content that is rude, disrespectful, or profane. */
  HARM_CATEGORY_TOXICITY = 2,
  /**
   * HARM_CATEGORY_VIOLENCE - *PaLM** - Describes scenarios depicting violence against an individual or
   * group, or general descriptions of gore.
   */
  HARM_CATEGORY_VIOLENCE = 3,
  /** HARM_CATEGORY_SEXUAL - *PaLM** - Contains references to sexual acts or other lewd content. */
  HARM_CATEGORY_SEXUAL = 4,
  /** HARM_CATEGORY_MEDICAL - *PaLM** - Promotes unchecked medical advice. */
  HARM_CATEGORY_MEDICAL = 5,
  /**
   * HARM_CATEGORY_DANGEROUS - *PaLM** - Dangerous content that promotes, facilitates, or encourages
   * harmful acts.
   */
  HARM_CATEGORY_DANGEROUS = 6,
  /** HARM_CATEGORY_HARASSMENT - *Gemini** - Harassment content. */
  HARM_CATEGORY_HARASSMENT = 7,
  /** HARM_CATEGORY_HATE_SPEECH - *Gemini** - Hate speech and content. */
  HARM_CATEGORY_HATE_SPEECH = 8,
  /** HARM_CATEGORY_SEXUALLY_EXPLICIT - *Gemini** - Sexually explicit content. */
  HARM_CATEGORY_SEXUALLY_EXPLICIT = 9,
  /** HARM_CATEGORY_DANGEROUS_CONTENT - *Gemini** - Dangerous content. */
  HARM_CATEGORY_DANGEROUS_CONTENT = 10,
  /** HARM_CATEGORY_CIVIC_INTEGRITY - *Gemini** - Content that may be used to harm civic integrity. */
  HARM_CATEGORY_CIVIC_INTEGRITY = 11,
  UNRECOGNIZED = -1,
}

export function harmCategoryFromJSON(object: any): HarmCategory {
  switch (object) {
    case 0:
    case "HARM_CATEGORY_UNSPECIFIED":
      return HarmCategory.HARM_CATEGORY_UNSPECIFIED;
    case 1:
    case "HARM_CATEGORY_DEROGATORY":
      return HarmCategory.HARM_CATEGORY_DEROGATORY;
    case 2:
    case "HARM_CATEGORY_TOXICITY":
      return HarmCategory.HARM_CATEGORY_TOXICITY;
    case 3:
    case "HARM_CATEGORY_VIOLENCE":
      return HarmCategory.HARM_CATEGORY_VIOLENCE;
    case 4:
    case "HARM_CATEGORY_SEXUAL":
      return HarmCategory.HARM_CATEGORY_SEXUAL;
    case 5:
    case "HARM_CATEGORY_MEDICAL":
      return HarmCategory.HARM_CATEGORY_MEDICAL;
    case 6:
    case "HARM_CATEGORY_DANGEROUS":
      return HarmCategory.HARM_CATEGORY_DANGEROUS;
    case 7:
    case "HARM_CATEGORY_HARASSMENT":
      return HarmCategory.HARM_CATEGORY_HARASSMENT;
    case 8:
    case "HARM_CATEGORY_HATE_SPEECH":
      return HarmCategory.HARM_CATEGORY_HATE_SPEECH;
    case 9:
    case "HARM_CATEGORY_SEXUALLY_EXPLICIT":
      return HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT;
    case 10:
    case "HARM_CATEGORY_DANGEROUS_CONTENT":
      return HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT;
    case 11:
    case "HARM_CATEGORY_CIVIC_INTEGRITY":
      return HarmCategory.HARM_CATEGORY_CIVIC_INTEGRITY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return HarmCategory.UNRECOGNIZED;
  }
}

export function harmCategoryToJSON(object: HarmCategory): string {
  switch (object) {
    case HarmCategory.HARM_CATEGORY_UNSPECIFIED:
      return "HARM_CATEGORY_UNSPECIFIED";
    case HarmCategory.HARM_CATEGORY_DEROGATORY:
      return "HARM_CATEGORY_DEROGATORY";
    case HarmCategory.HARM_CATEGORY_TOXICITY:
      return "HARM_CATEGORY_TOXICITY";
    case HarmCategory.HARM_CATEGORY_VIOLENCE:
      return "HARM_CATEGORY_VIOLENCE";
    case HarmCategory.HARM_CATEGORY_SEXUAL:
      return "HARM_CATEGORY_SEXUAL";
    case HarmCategory.HARM_CATEGORY_MEDICAL:
      return "HARM_CATEGORY_MEDICAL";
    case HarmCategory.HARM_CATEGORY_DANGEROUS:
      return "HARM_CATEGORY_DANGEROUS";
    case HarmCategory.HARM_CATEGORY_HARASSMENT:
      return "HARM_CATEGORY_HARASSMENT";
    case HarmCategory.HARM_CATEGORY_HATE_SPEECH:
      return "HARM_CATEGORY_HATE_SPEECH";
    case HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT:
      return "HARM_CATEGORY_SEXUALLY_EXPLICIT";
    case HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT:
      return "HARM_CATEGORY_DANGEROUS_CONTENT";
    case HarmCategory.HARM_CATEGORY_CIVIC_INTEGRITY:
      return "HARM_CATEGORY_CIVIC_INTEGRITY";
    case HarmCategory.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Safety rating for a piece of content.
 *
 * The safety rating contains the category of harm and the
 * harm probability level in that category for a piece of content.
 * Content is classified for safety across a number of
 * harm categories and the probability of the harm classification is included
 * here.
 */
export interface SafetyRating {
  /** Required. The category for this rating. */
  category: HarmCategory;
  /** Required. The probability of harm for this content. */
  probability: SafetyRating_HarmProbability;
  /** Was this content blocked because of this rating? */
  blocked: boolean;
}

/**
 * The probability that a piece of content is harmful.
 *
 * The classification system gives the probability of the content being
 * unsafe. This does not indicate the severity of harm for a piece of content.
 */
export enum SafetyRating_HarmProbability {
  /** HARM_PROBABILITY_UNSPECIFIED - Probability is unspecified. */
  HARM_PROBABILITY_UNSPECIFIED = 0,
  /** NEGLIGIBLE - Content has a negligible chance of being unsafe. */
  NEGLIGIBLE = 1,
  /** LOW - Content has a low chance of being unsafe. */
  LOW = 2,
  /** MEDIUM - Content has a medium chance of being unsafe. */
  MEDIUM = 3,
  /** HIGH - Content has a high chance of being unsafe. */
  HIGH = 4,
  UNRECOGNIZED = -1,
}

export function safetyRating_HarmProbabilityFromJSON(object: any): SafetyRating_HarmProbability {
  switch (object) {
    case 0:
    case "HARM_PROBABILITY_UNSPECIFIED":
      return SafetyRating_HarmProbability.HARM_PROBABILITY_UNSPECIFIED;
    case 1:
    case "NEGLIGIBLE":
      return SafetyRating_HarmProbability.NEGLIGIBLE;
    case 2:
    case "LOW":
      return SafetyRating_HarmProbability.LOW;
    case 3:
    case "MEDIUM":
      return SafetyRating_HarmProbability.MEDIUM;
    case 4:
    case "HIGH":
      return SafetyRating_HarmProbability.HIGH;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SafetyRating_HarmProbability.UNRECOGNIZED;
  }
}

export function safetyRating_HarmProbabilityToJSON(object: SafetyRating_HarmProbability): string {
  switch (object) {
    case SafetyRating_HarmProbability.HARM_PROBABILITY_UNSPECIFIED:
      return "HARM_PROBABILITY_UNSPECIFIED";
    case SafetyRating_HarmProbability.NEGLIGIBLE:
      return "NEGLIGIBLE";
    case SafetyRating_HarmProbability.LOW:
      return "LOW";
    case SafetyRating_HarmProbability.MEDIUM:
      return "MEDIUM";
    case SafetyRating_HarmProbability.HIGH:
      return "HIGH";
    case SafetyRating_HarmProbability.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Safety setting, affecting the safety-blocking behavior.
 *
 * Passing a safety setting for a category changes the allowed probability that
 * content is blocked.
 */
export interface SafetySetting {
  /** Required. The category for this setting. */
  category: HarmCategory;
  /** Required. Controls the probability threshold at which harm is blocked. */
  threshold: SafetySetting_HarmBlockThreshold;
}

/** Block at and beyond a specified harm probability. */
export enum SafetySetting_HarmBlockThreshold {
  /** HARM_BLOCK_THRESHOLD_UNSPECIFIED - Threshold is unspecified. */
  HARM_BLOCK_THRESHOLD_UNSPECIFIED = 0,
  /** BLOCK_LOW_AND_ABOVE - Content with NEGLIGIBLE will be allowed. */
  BLOCK_LOW_AND_ABOVE = 1,
  /** BLOCK_MEDIUM_AND_ABOVE - Content with NEGLIGIBLE and LOW will be allowed. */
  BLOCK_MEDIUM_AND_ABOVE = 2,
  /** BLOCK_ONLY_HIGH - Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed. */
  BLOCK_ONLY_HIGH = 3,
  /** BLOCK_NONE - All content will be allowed. */
  BLOCK_NONE = 4,
  /** OFF - Turn off the safety filter. */
  OFF = 5,
  UNRECOGNIZED = -1,
}

export function safetySetting_HarmBlockThresholdFromJSON(object: any): SafetySetting_HarmBlockThreshold {
  switch (object) {
    case 0:
    case "HARM_BLOCK_THRESHOLD_UNSPECIFIED":
      return SafetySetting_HarmBlockThreshold.HARM_BLOCK_THRESHOLD_UNSPECIFIED;
    case 1:
    case "BLOCK_LOW_AND_ABOVE":
      return SafetySetting_HarmBlockThreshold.BLOCK_LOW_AND_ABOVE;
    case 2:
    case "BLOCK_MEDIUM_AND_ABOVE":
      return SafetySetting_HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE;
    case 3:
    case "BLOCK_ONLY_HIGH":
      return SafetySetting_HarmBlockThreshold.BLOCK_ONLY_HIGH;
    case 4:
    case "BLOCK_NONE":
      return SafetySetting_HarmBlockThreshold.BLOCK_NONE;
    case 5:
    case "OFF":
      return SafetySetting_HarmBlockThreshold.OFF;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SafetySetting_HarmBlockThreshold.UNRECOGNIZED;
  }
}

export function safetySetting_HarmBlockThresholdToJSON(object: SafetySetting_HarmBlockThreshold): string {
  switch (object) {
    case SafetySetting_HarmBlockThreshold.HARM_BLOCK_THRESHOLD_UNSPECIFIED:
      return "HARM_BLOCK_THRESHOLD_UNSPECIFIED";
    case SafetySetting_HarmBlockThreshold.BLOCK_LOW_AND_ABOVE:
      return "BLOCK_LOW_AND_ABOVE";
    case SafetySetting_HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE:
      return "BLOCK_MEDIUM_AND_ABOVE";
    case SafetySetting_HarmBlockThreshold.BLOCK_ONLY_HIGH:
      return "BLOCK_ONLY_HIGH";
    case SafetySetting_HarmBlockThreshold.BLOCK_NONE:
      return "BLOCK_NONE";
    case SafetySetting_HarmBlockThreshold.OFF:
      return "OFF";
    case SafetySetting_HarmBlockThreshold.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseSafetyRating(): SafetyRating {
  return { category: 0, probability: 0, blocked: false };
}

export const SafetyRating: MessageFns<SafetyRating> = {
  encode(message: SafetyRating, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.category !== 0) {
      writer.uint32(24).int32(message.category);
    }
    if (message.probability !== 0) {
      writer.uint32(32).int32(message.probability);
    }
    if (message.blocked !== false) {
      writer.uint32(40).bool(message.blocked);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SafetyRating {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSafetyRating();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 24) {
            break;
          }

          message.category = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.probability = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.blocked = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SafetyRating {
    return {
      category: isSet(object.category) ? harmCategoryFromJSON(object.category) : 0,
      probability: isSet(object.probability) ? safetyRating_HarmProbabilityFromJSON(object.probability) : 0,
      blocked: isSet(object.blocked) ? globalThis.Boolean(object.blocked) : false,
    };
  },

  toJSON(message: SafetyRating): unknown {
    const obj: any = {};
    if (message.category !== 0) {
      obj.category = harmCategoryToJSON(message.category);
    }
    if (message.probability !== 0) {
      obj.probability = safetyRating_HarmProbabilityToJSON(message.probability);
    }
    if (message.blocked !== false) {
      obj.blocked = message.blocked;
    }
    return obj;
  },

  create(base?: DeepPartial<SafetyRating>): SafetyRating {
    return SafetyRating.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SafetyRating>): SafetyRating {
    const message = createBaseSafetyRating();
    message.category = object.category ?? 0;
    message.probability = object.probability ?? 0;
    message.blocked = object.blocked ?? false;
    return message;
  },
};

function createBaseSafetySetting(): SafetySetting {
  return { category: 0, threshold: 0 };
}

export const SafetySetting: MessageFns<SafetySetting> = {
  encode(message: SafetySetting, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.category !== 0) {
      writer.uint32(24).int32(message.category);
    }
    if (message.threshold !== 0) {
      writer.uint32(32).int32(message.threshold);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SafetySetting {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSafetySetting();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 24) {
            break;
          }

          message.category = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.threshold = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SafetySetting {
    return {
      category: isSet(object.category) ? harmCategoryFromJSON(object.category) : 0,
      threshold: isSet(object.threshold) ? safetySetting_HarmBlockThresholdFromJSON(object.threshold) : 0,
    };
  },

  toJSON(message: SafetySetting): unknown {
    const obj: any = {};
    if (message.category !== 0) {
      obj.category = harmCategoryToJSON(message.category);
    }
    if (message.threshold !== 0) {
      obj.threshold = safetySetting_HarmBlockThresholdToJSON(message.threshold);
    }
    return obj;
  },

  create(base?: DeepPartial<SafetySetting>): SafetySetting {
    return SafetySetting.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SafetySetting>): SafetySetting {
    const message = createBaseSafetySetting();
    message.category = object.category ?? 0;
    message.threshold = object.threshold ?? 0;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
