// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/bigquery/migration/v2alpha/translation_task.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";

export const protobufPackage = "google.cloud.bigquery.migration.v2alpha";

/** Mapping between an input and output file to be translated in a subtask. */
export interface TranslationFileMapping {
  /** The Cloud Storage path for a file to translation in a subtask. */
  inputPath: string;
  /** The Cloud Storage path to write back the corresponding input file to. */
  outputPath: string;
}

/**
 * The translation task config to capture necessary settings for a translation
 * task and subtask.
 */
export interface TranslationTaskDetails {
  /** The Teradata SQL specific settings for the translation task. */
  teradataOptions?:
    | TeradataOptions
    | undefined;
  /** The BTEQ specific settings for the translation task. */
  bteqOptions?:
    | BteqOptions
    | undefined;
  /** The Cloud Storage path for translation input files. */
  inputPath: string;
  /** The Cloud Storage path for translation output files. */
  outputPath: string;
  /** Cloud Storage files to be processed for translation. */
  filePaths: TranslationFileMapping[];
  /**
   * The Cloud Storage path to DDL files as table schema to assist semantic
   * translation.
   */
  schemaPath: string;
  /** The file encoding type. */
  fileEncoding: TranslationTaskDetails_FileEncoding;
  /** The settings for SQL identifiers. */
  identifierSettings:
    | IdentifierSettings
    | undefined;
  /**
   * The map capturing special tokens to be replaced during translation. The key
   * is special token in string. The value is the token data type. This is used
   * to translate SQL query template which contains special token as place
   * holder. The special token makes a query invalid to parse. This map will be
   * applied to annotate those special token with types to let parser understand
   * how to parse them into proper structure with type information.
   */
  specialTokenMap: { [key: string]: TranslationTaskDetails_TokenType };
  /** The filter applied to translation details. */
  filter:
    | Filter
    | undefined;
  /**
   * Specifies the exact name of the bigquery table ("dataset.table") to be used
   * for surfacing raw translation errors. If the table does not exist, we will
   * create it. If it already exists and the schema is the same, we will re-use.
   * If the table exists and the schema is different, we will throw an error.
   */
  translationExceptionTable: string;
}

/** The file encoding types. */
export enum TranslationTaskDetails_FileEncoding {
  /** FILE_ENCODING_UNSPECIFIED - File encoding setting is not specified. */
  FILE_ENCODING_UNSPECIFIED = 0,
  /** UTF_8 - File encoding is UTF_8. */
  UTF_8 = 1,
  /** ISO_8859_1 - File encoding is ISO_8859_1. */
  ISO_8859_1 = 2,
  /** US_ASCII - File encoding is US_ASCII. */
  US_ASCII = 3,
  /** UTF_16 - File encoding is UTF_16. */
  UTF_16 = 4,
  /** UTF_16LE - File encoding is UTF_16LE. */
  UTF_16LE = 5,
  /** UTF_16BE - File encoding is UTF_16BE. */
  UTF_16BE = 6,
  UNRECOGNIZED = -1,
}

export function translationTaskDetails_FileEncodingFromJSON(object: any): TranslationTaskDetails_FileEncoding {
  switch (object) {
    case 0:
    case "FILE_ENCODING_UNSPECIFIED":
      return TranslationTaskDetails_FileEncoding.FILE_ENCODING_UNSPECIFIED;
    case 1:
    case "UTF_8":
      return TranslationTaskDetails_FileEncoding.UTF_8;
    case 2:
    case "ISO_8859_1":
      return TranslationTaskDetails_FileEncoding.ISO_8859_1;
    case 3:
    case "US_ASCII":
      return TranslationTaskDetails_FileEncoding.US_ASCII;
    case 4:
    case "UTF_16":
      return TranslationTaskDetails_FileEncoding.UTF_16;
    case 5:
    case "UTF_16LE":
      return TranslationTaskDetails_FileEncoding.UTF_16LE;
    case 6:
    case "UTF_16BE":
      return TranslationTaskDetails_FileEncoding.UTF_16BE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return TranslationTaskDetails_FileEncoding.UNRECOGNIZED;
  }
}

export function translationTaskDetails_FileEncodingToJSON(object: TranslationTaskDetails_FileEncoding): string {
  switch (object) {
    case TranslationTaskDetails_FileEncoding.FILE_ENCODING_UNSPECIFIED:
      return "FILE_ENCODING_UNSPECIFIED";
    case TranslationTaskDetails_FileEncoding.UTF_8:
      return "UTF_8";
    case TranslationTaskDetails_FileEncoding.ISO_8859_1:
      return "ISO_8859_1";
    case TranslationTaskDetails_FileEncoding.US_ASCII:
      return "US_ASCII";
    case TranslationTaskDetails_FileEncoding.UTF_16:
      return "UTF_16";
    case TranslationTaskDetails_FileEncoding.UTF_16LE:
      return "UTF_16LE";
    case TranslationTaskDetails_FileEncoding.UTF_16BE:
      return "UTF_16BE";
    case TranslationTaskDetails_FileEncoding.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The special token data type. */
export enum TranslationTaskDetails_TokenType {
  /** TOKEN_TYPE_UNSPECIFIED - Token type is not specified. */
  TOKEN_TYPE_UNSPECIFIED = 0,
  /** STRING - Token type as string. */
  STRING = 1,
  /** INT64 - Token type as integer. */
  INT64 = 2,
  /** NUMERIC - Token type as numeric. */
  NUMERIC = 3,
  /** BOOL - Token type as boolean. */
  BOOL = 4,
  /** FLOAT64 - Token type as float. */
  FLOAT64 = 5,
  /** DATE - Token type as date. */
  DATE = 6,
  /** TIMESTAMP - Token type as timestamp. */
  TIMESTAMP = 7,
  UNRECOGNIZED = -1,
}

export function translationTaskDetails_TokenTypeFromJSON(object: any): TranslationTaskDetails_TokenType {
  switch (object) {
    case 0:
    case "TOKEN_TYPE_UNSPECIFIED":
      return TranslationTaskDetails_TokenType.TOKEN_TYPE_UNSPECIFIED;
    case 1:
    case "STRING":
      return TranslationTaskDetails_TokenType.STRING;
    case 2:
    case "INT64":
      return TranslationTaskDetails_TokenType.INT64;
    case 3:
    case "NUMERIC":
      return TranslationTaskDetails_TokenType.NUMERIC;
    case 4:
    case "BOOL":
      return TranslationTaskDetails_TokenType.BOOL;
    case 5:
    case "FLOAT64":
      return TranslationTaskDetails_TokenType.FLOAT64;
    case 6:
    case "DATE":
      return TranslationTaskDetails_TokenType.DATE;
    case 7:
    case "TIMESTAMP":
      return TranslationTaskDetails_TokenType.TIMESTAMP;
    case -1:
    case "UNRECOGNIZED":
    default:
      return TranslationTaskDetails_TokenType.UNRECOGNIZED;
  }
}

export function translationTaskDetails_TokenTypeToJSON(object: TranslationTaskDetails_TokenType): string {
  switch (object) {
    case TranslationTaskDetails_TokenType.TOKEN_TYPE_UNSPECIFIED:
      return "TOKEN_TYPE_UNSPECIFIED";
    case TranslationTaskDetails_TokenType.STRING:
      return "STRING";
    case TranslationTaskDetails_TokenType.INT64:
      return "INT64";
    case TranslationTaskDetails_TokenType.NUMERIC:
      return "NUMERIC";
    case TranslationTaskDetails_TokenType.BOOL:
      return "BOOL";
    case TranslationTaskDetails_TokenType.FLOAT64:
      return "FLOAT64";
    case TranslationTaskDetails_TokenType.DATE:
      return "DATE";
    case TranslationTaskDetails_TokenType.TIMESTAMP:
      return "TIMESTAMP";
    case TranslationTaskDetails_TokenType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export interface TranslationTaskDetails_SpecialTokenMapEntry {
  key: string;
  value: TranslationTaskDetails_TokenType;
}

/** The filter applied to fields of translation details. */
export interface Filter {
  /** The list of prefixes used to exclude processing for input files. */
  inputFileExclusionPrefixes: string[];
}

/** Settings related to SQL identifiers. */
export interface IdentifierSettings {
  /** The setting to control output queries' identifier case. */
  outputIdentifierCase: IdentifierSettings_IdentifierCase;
  /** Specifies the rewrite mode for SQL identifiers. */
  identifierRewriteMode: IdentifierSettings_IdentifierRewriteMode;
}

/** The identifier case type. */
export enum IdentifierSettings_IdentifierCase {
  /** IDENTIFIER_CASE_UNSPECIFIED - The identifier case is not specified. */
  IDENTIFIER_CASE_UNSPECIFIED = 0,
  /** ORIGINAL - Identifiers' cases will be kept as the original cases. */
  ORIGINAL = 1,
  /** UPPER - Identifiers will be in upper cases. */
  UPPER = 2,
  /** LOWER - Identifiers will be in lower cases. */
  LOWER = 3,
  UNRECOGNIZED = -1,
}

export function identifierSettings_IdentifierCaseFromJSON(object: any): IdentifierSettings_IdentifierCase {
  switch (object) {
    case 0:
    case "IDENTIFIER_CASE_UNSPECIFIED":
      return IdentifierSettings_IdentifierCase.IDENTIFIER_CASE_UNSPECIFIED;
    case 1:
    case "ORIGINAL":
      return IdentifierSettings_IdentifierCase.ORIGINAL;
    case 2:
    case "UPPER":
      return IdentifierSettings_IdentifierCase.UPPER;
    case 3:
    case "LOWER":
      return IdentifierSettings_IdentifierCase.LOWER;
    case -1:
    case "UNRECOGNIZED":
    default:
      return IdentifierSettings_IdentifierCase.UNRECOGNIZED;
  }
}

export function identifierSettings_IdentifierCaseToJSON(object: IdentifierSettings_IdentifierCase): string {
  switch (object) {
    case IdentifierSettings_IdentifierCase.IDENTIFIER_CASE_UNSPECIFIED:
      return "IDENTIFIER_CASE_UNSPECIFIED";
    case IdentifierSettings_IdentifierCase.ORIGINAL:
      return "ORIGINAL";
    case IdentifierSettings_IdentifierCase.UPPER:
      return "UPPER";
    case IdentifierSettings_IdentifierCase.LOWER:
      return "LOWER";
    case IdentifierSettings_IdentifierCase.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The SQL identifier rewrite mode. */
export enum IdentifierSettings_IdentifierRewriteMode {
  /** IDENTIFIER_REWRITE_MODE_UNSPECIFIED - SQL Identifier rewrite mode is unspecified. */
  IDENTIFIER_REWRITE_MODE_UNSPECIFIED = 0,
  /** NONE - SQL identifiers won't be rewrite. */
  NONE = 1,
  /** REWRITE_ALL - All SQL identifiers will be rewrite. */
  REWRITE_ALL = 2,
  UNRECOGNIZED = -1,
}

export function identifierSettings_IdentifierRewriteModeFromJSON(
  object: any,
): IdentifierSettings_IdentifierRewriteMode {
  switch (object) {
    case 0:
    case "IDENTIFIER_REWRITE_MODE_UNSPECIFIED":
      return IdentifierSettings_IdentifierRewriteMode.IDENTIFIER_REWRITE_MODE_UNSPECIFIED;
    case 1:
    case "NONE":
      return IdentifierSettings_IdentifierRewriteMode.NONE;
    case 2:
    case "REWRITE_ALL":
      return IdentifierSettings_IdentifierRewriteMode.REWRITE_ALL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return IdentifierSettings_IdentifierRewriteMode.UNRECOGNIZED;
  }
}

export function identifierSettings_IdentifierRewriteModeToJSON(
  object: IdentifierSettings_IdentifierRewriteMode,
): string {
  switch (object) {
    case IdentifierSettings_IdentifierRewriteMode.IDENTIFIER_REWRITE_MODE_UNSPECIFIED:
      return "IDENTIFIER_REWRITE_MODE_UNSPECIFIED";
    case IdentifierSettings_IdentifierRewriteMode.NONE:
      return "NONE";
    case IdentifierSettings_IdentifierRewriteMode.REWRITE_ALL:
      return "REWRITE_ALL";
    case IdentifierSettings_IdentifierRewriteMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Teradata SQL specific translation task related settings. */
export interface TeradataOptions {
}

/** BTEQ translation task related settings. */
export interface BteqOptions {
  /**
   * Specifies the project and dataset in BigQuery that will be used for
   * external table creation during the translation.
   */
  projectDataset:
    | DatasetReference
    | undefined;
  /**
   * The Cloud Storage location to be used as the default path for files that
   * are not otherwise specified in the file replacement map.
   */
  defaultPathUri: string;
  /**
   * Maps the local paths that are used in BTEQ scripts (the keys) to the paths
   * in Cloud Storage that should be used in their stead in the translation (the
   * value).
   */
  fileReplacementMap: { [key: string]: string };
}

export interface BteqOptions_FileReplacementMapEntry {
  key: string;
  value: string;
}

/** Reference to a BigQuery dataset. */
export interface DatasetReference {
  /**
   * A unique ID for this dataset, without the project name. The ID
   * must contain only letters (a-z, A-Z), numbers (0-9), or underscores (_).
   * The maximum length is 1,024 characters.
   */
  datasetId: string;
  /** The ID of the project containing this dataset. */
  projectId: string;
}

function createBaseTranslationFileMapping(): TranslationFileMapping {
  return { inputPath: "", outputPath: "" };
}

export const TranslationFileMapping: MessageFns<TranslationFileMapping> = {
  encode(message: TranslationFileMapping, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputPath !== "") {
      writer.uint32(10).string(message.inputPath);
    }
    if (message.outputPath !== "") {
      writer.uint32(18).string(message.outputPath);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TranslationFileMapping {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTranslationFileMapping();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputPath = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.outputPath = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TranslationFileMapping {
    return {
      inputPath: isSet(object.inputPath) ? globalThis.String(object.inputPath) : "",
      outputPath: isSet(object.outputPath) ? globalThis.String(object.outputPath) : "",
    };
  },

  toJSON(message: TranslationFileMapping): unknown {
    const obj: any = {};
    if (message.inputPath !== "") {
      obj.inputPath = message.inputPath;
    }
    if (message.outputPath !== "") {
      obj.outputPath = message.outputPath;
    }
    return obj;
  },

  create(base?: DeepPartial<TranslationFileMapping>): TranslationFileMapping {
    return TranslationFileMapping.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TranslationFileMapping>): TranslationFileMapping {
    const message = createBaseTranslationFileMapping();
    message.inputPath = object.inputPath ?? "";
    message.outputPath = object.outputPath ?? "";
    return message;
  },
};

function createBaseTranslationTaskDetails(): TranslationTaskDetails {
  return {
    teradataOptions: undefined,
    bteqOptions: undefined,
    inputPath: "",
    outputPath: "",
    filePaths: [],
    schemaPath: "",
    fileEncoding: 0,
    identifierSettings: undefined,
    specialTokenMap: {},
    filter: undefined,
    translationExceptionTable: "",
  };
}

export const TranslationTaskDetails: MessageFns<TranslationTaskDetails> = {
  encode(message: TranslationTaskDetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.teradataOptions !== undefined) {
      TeradataOptions.encode(message.teradataOptions, writer.uint32(82).fork()).join();
    }
    if (message.bteqOptions !== undefined) {
      BteqOptions.encode(message.bteqOptions, writer.uint32(90).fork()).join();
    }
    if (message.inputPath !== "") {
      writer.uint32(10).string(message.inputPath);
    }
    if (message.outputPath !== "") {
      writer.uint32(18).string(message.outputPath);
    }
    for (const v of message.filePaths) {
      TranslationFileMapping.encode(v!, writer.uint32(98).fork()).join();
    }
    if (message.schemaPath !== "") {
      writer.uint32(26).string(message.schemaPath);
    }
    if (message.fileEncoding !== 0) {
      writer.uint32(32).int32(message.fileEncoding);
    }
    if (message.identifierSettings !== undefined) {
      IdentifierSettings.encode(message.identifierSettings, writer.uint32(42).fork()).join();
    }
    Object.entries(message.specialTokenMap).forEach(([key, value]) => {
      TranslationTaskDetails_SpecialTokenMapEntry.encode({ key: key as any, value }, writer.uint32(50).fork()).join();
    });
    if (message.filter !== undefined) {
      Filter.encode(message.filter, writer.uint32(58).fork()).join();
    }
    if (message.translationExceptionTable !== "") {
      writer.uint32(106).string(message.translationExceptionTable);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TranslationTaskDetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTranslationTaskDetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 10:
          if (tag !== 82) {
            break;
          }

          message.teradataOptions = TeradataOptions.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.bteqOptions = BteqOptions.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputPath = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.outputPath = reader.string();
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.filePaths.push(TranslationFileMapping.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.schemaPath = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.fileEncoding = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.identifierSettings = IdentifierSettings.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          const entry6 = TranslationTaskDetails_SpecialTokenMapEntry.decode(reader, reader.uint32());
          if (entry6.value !== undefined) {
            message.specialTokenMap[entry6.key] = entry6.value;
          }
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.filter = Filter.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.translationExceptionTable = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TranslationTaskDetails {
    return {
      teradataOptions: isSet(object.teradataOptions) ? TeradataOptions.fromJSON(object.teradataOptions) : undefined,
      bteqOptions: isSet(object.bteqOptions) ? BteqOptions.fromJSON(object.bteqOptions) : undefined,
      inputPath: isSet(object.inputPath) ? globalThis.String(object.inputPath) : "",
      outputPath: isSet(object.outputPath) ? globalThis.String(object.outputPath) : "",
      filePaths: globalThis.Array.isArray(object?.filePaths)
        ? object.filePaths.map((e: any) => TranslationFileMapping.fromJSON(e))
        : [],
      schemaPath: isSet(object.schemaPath) ? globalThis.String(object.schemaPath) : "",
      fileEncoding: isSet(object.fileEncoding) ? translationTaskDetails_FileEncodingFromJSON(object.fileEncoding) : 0,
      identifierSettings: isSet(object.identifierSettings)
        ? IdentifierSettings.fromJSON(object.identifierSettings)
        : undefined,
      specialTokenMap: isObject(object.specialTokenMap)
        ? Object.entries(object.specialTokenMap).reduce<{ [key: string]: TranslationTaskDetails_TokenType }>(
          (acc, [key, value]) => {
            acc[key] = translationTaskDetails_TokenTypeFromJSON(value);
            return acc;
          },
          {},
        )
        : {},
      filter: isSet(object.filter) ? Filter.fromJSON(object.filter) : undefined,
      translationExceptionTable: isSet(object.translationExceptionTable)
        ? globalThis.String(object.translationExceptionTable)
        : "",
    };
  },

  toJSON(message: TranslationTaskDetails): unknown {
    const obj: any = {};
    if (message.teradataOptions !== undefined) {
      obj.teradataOptions = TeradataOptions.toJSON(message.teradataOptions);
    }
    if (message.bteqOptions !== undefined) {
      obj.bteqOptions = BteqOptions.toJSON(message.bteqOptions);
    }
    if (message.inputPath !== "") {
      obj.inputPath = message.inputPath;
    }
    if (message.outputPath !== "") {
      obj.outputPath = message.outputPath;
    }
    if (message.filePaths?.length) {
      obj.filePaths = message.filePaths.map((e) => TranslationFileMapping.toJSON(e));
    }
    if (message.schemaPath !== "") {
      obj.schemaPath = message.schemaPath;
    }
    if (message.fileEncoding !== 0) {
      obj.fileEncoding = translationTaskDetails_FileEncodingToJSON(message.fileEncoding);
    }
    if (message.identifierSettings !== undefined) {
      obj.identifierSettings = IdentifierSettings.toJSON(message.identifierSettings);
    }
    if (message.specialTokenMap) {
      const entries = Object.entries(message.specialTokenMap);
      if (entries.length > 0) {
        obj.specialTokenMap = {};
        entries.forEach(([k, v]) => {
          obj.specialTokenMap[k] = translationTaskDetails_TokenTypeToJSON(v);
        });
      }
    }
    if (message.filter !== undefined) {
      obj.filter = Filter.toJSON(message.filter);
    }
    if (message.translationExceptionTable !== "") {
      obj.translationExceptionTable = message.translationExceptionTable;
    }
    return obj;
  },

  create(base?: DeepPartial<TranslationTaskDetails>): TranslationTaskDetails {
    return TranslationTaskDetails.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TranslationTaskDetails>): TranslationTaskDetails {
    const message = createBaseTranslationTaskDetails();
    message.teradataOptions = (object.teradataOptions !== undefined && object.teradataOptions !== null)
      ? TeradataOptions.fromPartial(object.teradataOptions)
      : undefined;
    message.bteqOptions = (object.bteqOptions !== undefined && object.bteqOptions !== null)
      ? BteqOptions.fromPartial(object.bteqOptions)
      : undefined;
    message.inputPath = object.inputPath ?? "";
    message.outputPath = object.outputPath ?? "";
    message.filePaths = object.filePaths?.map((e) => TranslationFileMapping.fromPartial(e)) || [];
    message.schemaPath = object.schemaPath ?? "";
    message.fileEncoding = object.fileEncoding ?? 0;
    message.identifierSettings = (object.identifierSettings !== undefined && object.identifierSettings !== null)
      ? IdentifierSettings.fromPartial(object.identifierSettings)
      : undefined;
    message.specialTokenMap = Object.entries(object.specialTokenMap ?? {}).reduce<
      { [key: string]: TranslationTaskDetails_TokenType }
    >((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = value as TranslationTaskDetails_TokenType;
      }
      return acc;
    }, {});
    message.filter = (object.filter !== undefined && object.filter !== null)
      ? Filter.fromPartial(object.filter)
      : undefined;
    message.translationExceptionTable = object.translationExceptionTable ?? "";
    return message;
  },
};

function createBaseTranslationTaskDetails_SpecialTokenMapEntry(): TranslationTaskDetails_SpecialTokenMapEntry {
  return { key: "", value: 0 };
}

export const TranslationTaskDetails_SpecialTokenMapEntry: MessageFns<TranslationTaskDetails_SpecialTokenMapEntry> = {
  encode(
    message: TranslationTaskDetails_SpecialTokenMapEntry,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== 0) {
      writer.uint32(16).int32(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TranslationTaskDetails_SpecialTokenMapEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTranslationTaskDetails_SpecialTokenMapEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.value = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TranslationTaskDetails_SpecialTokenMapEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? translationTaskDetails_TokenTypeFromJSON(object.value) : 0,
    };
  },

  toJSON(message: TranslationTaskDetails_SpecialTokenMapEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== 0) {
      obj.value = translationTaskDetails_TokenTypeToJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<TranslationTaskDetails_SpecialTokenMapEntry>): TranslationTaskDetails_SpecialTokenMapEntry {
    return TranslationTaskDetails_SpecialTokenMapEntry.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<TranslationTaskDetails_SpecialTokenMapEntry>,
  ): TranslationTaskDetails_SpecialTokenMapEntry {
    const message = createBaseTranslationTaskDetails_SpecialTokenMapEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? 0;
    return message;
  },
};

function createBaseFilter(): Filter {
  return { inputFileExclusionPrefixes: [] };
}

export const Filter: MessageFns<Filter> = {
  encode(message: Filter, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.inputFileExclusionPrefixes) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Filter {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFilter();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputFileExclusionPrefixes.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Filter {
    return {
      inputFileExclusionPrefixes: globalThis.Array.isArray(object?.inputFileExclusionPrefixes)
        ? object.inputFileExclusionPrefixes.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: Filter): unknown {
    const obj: any = {};
    if (message.inputFileExclusionPrefixes?.length) {
      obj.inputFileExclusionPrefixes = message.inputFileExclusionPrefixes;
    }
    return obj;
  },

  create(base?: DeepPartial<Filter>): Filter {
    return Filter.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Filter>): Filter {
    const message = createBaseFilter();
    message.inputFileExclusionPrefixes = object.inputFileExclusionPrefixes?.map((e) => e) || [];
    return message;
  },
};

function createBaseIdentifierSettings(): IdentifierSettings {
  return { outputIdentifierCase: 0, identifierRewriteMode: 0 };
}

export const IdentifierSettings: MessageFns<IdentifierSettings> = {
  encode(message: IdentifierSettings, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.outputIdentifierCase !== 0) {
      writer.uint32(8).int32(message.outputIdentifierCase);
    }
    if (message.identifierRewriteMode !== 0) {
      writer.uint32(16).int32(message.identifierRewriteMode);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): IdentifierSettings {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseIdentifierSettings();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.outputIdentifierCase = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.identifierRewriteMode = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): IdentifierSettings {
    return {
      outputIdentifierCase: isSet(object.outputIdentifierCase)
        ? identifierSettings_IdentifierCaseFromJSON(object.outputIdentifierCase)
        : 0,
      identifierRewriteMode: isSet(object.identifierRewriteMode)
        ? identifierSettings_IdentifierRewriteModeFromJSON(object.identifierRewriteMode)
        : 0,
    };
  },

  toJSON(message: IdentifierSettings): unknown {
    const obj: any = {};
    if (message.outputIdentifierCase !== 0) {
      obj.outputIdentifierCase = identifierSettings_IdentifierCaseToJSON(message.outputIdentifierCase);
    }
    if (message.identifierRewriteMode !== 0) {
      obj.identifierRewriteMode = identifierSettings_IdentifierRewriteModeToJSON(message.identifierRewriteMode);
    }
    return obj;
  },

  create(base?: DeepPartial<IdentifierSettings>): IdentifierSettings {
    return IdentifierSettings.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<IdentifierSettings>): IdentifierSettings {
    const message = createBaseIdentifierSettings();
    message.outputIdentifierCase = object.outputIdentifierCase ?? 0;
    message.identifierRewriteMode = object.identifierRewriteMode ?? 0;
    return message;
  },
};

function createBaseTeradataOptions(): TeradataOptions {
  return {};
}

export const TeradataOptions: MessageFns<TeradataOptions> = {
  encode(_: TeradataOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TeradataOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTeradataOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): TeradataOptions {
    return {};
  },

  toJSON(_: TeradataOptions): unknown {
    const obj: any = {};
    return obj;
  },

  create(base?: DeepPartial<TeradataOptions>): TeradataOptions {
    return TeradataOptions.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<TeradataOptions>): TeradataOptions {
    const message = createBaseTeradataOptions();
    return message;
  },
};

function createBaseBteqOptions(): BteqOptions {
  return { projectDataset: undefined, defaultPathUri: "", fileReplacementMap: {} };
}

export const BteqOptions: MessageFns<BteqOptions> = {
  encode(message: BteqOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectDataset !== undefined) {
      DatasetReference.encode(message.projectDataset, writer.uint32(10).fork()).join();
    }
    if (message.defaultPathUri !== "") {
      writer.uint32(18).string(message.defaultPathUri);
    }
    Object.entries(message.fileReplacementMap).forEach(([key, value]) => {
      BteqOptions_FileReplacementMapEntry.encode({ key: key as any, value }, writer.uint32(26).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BteqOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBteqOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectDataset = DatasetReference.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.defaultPathUri = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          const entry3 = BteqOptions_FileReplacementMapEntry.decode(reader, reader.uint32());
          if (entry3.value !== undefined) {
            message.fileReplacementMap[entry3.key] = entry3.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BteqOptions {
    return {
      projectDataset: isSet(object.projectDataset) ? DatasetReference.fromJSON(object.projectDataset) : undefined,
      defaultPathUri: isSet(object.defaultPathUri) ? globalThis.String(object.defaultPathUri) : "",
      fileReplacementMap: isObject(object.fileReplacementMap)
        ? Object.entries(object.fileReplacementMap).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: BteqOptions): unknown {
    const obj: any = {};
    if (message.projectDataset !== undefined) {
      obj.projectDataset = DatasetReference.toJSON(message.projectDataset);
    }
    if (message.defaultPathUri !== "") {
      obj.defaultPathUri = message.defaultPathUri;
    }
    if (message.fileReplacementMap) {
      const entries = Object.entries(message.fileReplacementMap);
      if (entries.length > 0) {
        obj.fileReplacementMap = {};
        entries.forEach(([k, v]) => {
          obj.fileReplacementMap[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<BteqOptions>): BteqOptions {
    return BteqOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BteqOptions>): BteqOptions {
    const message = createBaseBteqOptions();
    message.projectDataset = (object.projectDataset !== undefined && object.projectDataset !== null)
      ? DatasetReference.fromPartial(object.projectDataset)
      : undefined;
    message.defaultPathUri = object.defaultPathUri ?? "";
    message.fileReplacementMap = Object.entries(object.fileReplacementMap ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseBteqOptions_FileReplacementMapEntry(): BteqOptions_FileReplacementMapEntry {
  return { key: "", value: "" };
}

export const BteqOptions_FileReplacementMapEntry: MessageFns<BteqOptions_FileReplacementMapEntry> = {
  encode(message: BteqOptions_FileReplacementMapEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BteqOptions_FileReplacementMapEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBteqOptions_FileReplacementMapEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BteqOptions_FileReplacementMapEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: BteqOptions_FileReplacementMapEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<BteqOptions_FileReplacementMapEntry>): BteqOptions_FileReplacementMapEntry {
    return BteqOptions_FileReplacementMapEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BteqOptions_FileReplacementMapEntry>): BteqOptions_FileReplacementMapEntry {
    const message = createBaseBteqOptions_FileReplacementMapEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseDatasetReference(): DatasetReference {
  return { datasetId: "", projectId: "" };
}

export const DatasetReference: MessageFns<DatasetReference> = {
  encode(message: DatasetReference, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.datasetId !== "") {
      writer.uint32(10).string(message.datasetId);
    }
    if (message.projectId !== "") {
      writer.uint32(18).string(message.projectId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatasetReference {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatasetReference();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.projectId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatasetReference {
    return {
      datasetId: isSet(object.datasetId) ? globalThis.String(object.datasetId) : "",
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
    };
  },

  toJSON(message: DatasetReference): unknown {
    const obj: any = {};
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    return obj;
  },

  create(base?: DeepPartial<DatasetReference>): DatasetReference {
    return DatasetReference.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatasetReference>): DatasetReference {
    const message = createBaseDatasetReference();
    message.datasetId = object.datasetId ?? "";
    message.projectId = object.projectId ?? "";
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
