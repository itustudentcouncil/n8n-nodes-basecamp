// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/bigquery/storage/v1/storage.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Timestamp } from "../../../../protobuf/timestamp.js";
import { Int64Value } from "../../../../protobuf/wrappers.js";
import { Status } from "../../../../rpc/status.js";
import { ArrowRecordBatch, ArrowSchema } from "./arrow.js";
import { AvroRows, AvroSchema } from "./avro.js";
import { ProtoRows, ProtoSchema } from "./protobuf.js";
import {
  ReadSession,
  ReadStream,
  WriteStream,
  WriteStreamView,
  writeStreamViewFromJSON,
  writeStreamViewToJSON,
} from "./stream.js";
import { TableSchema } from "./table.js";

export const protobufPackage = "google.cloud.bigquery.storage.v1";

/** Request message for `CreateReadSession`. */
export interface CreateReadSessionRequest {
  /**
   * Required. The request project that owns the session, in the form of
   * `projects/{project_id}`.
   */
  parent: string;
  /** Required. Session to be created. */
  readSession:
    | ReadSession
    | undefined;
  /**
   * Max initial number of streams. If unset or zero, the server will
   * provide a value of streams so as to produce reasonable throughput. Must be
   * non-negative. The number of streams may be lower than the requested number,
   * depending on the amount parallelism that is reasonable for the table.
   * There is a default system max limit of 1,000.
   *
   * This must be greater than or equal to preferred_min_stream_count.
   * Typically, clients should either leave this unset to let the system to
   * determine an upper bound OR set this a size for the maximum "units of work"
   * it can gracefully handle.
   */
  maxStreamCount: number;
  /**
   * The minimum preferred stream count. This parameter can be used to inform
   * the service that there is a desired lower bound on the number of streams.
   * This is typically a target parallelism of the client (e.g. a Spark
   * cluster with N-workers would set this to a low multiple of N to ensure
   * good cluster utilization).
   *
   * The system will make a best effort to provide at least this number of
   * streams, but in some cases might provide less.
   */
  preferredMinStreamCount: number;
}

/** Request message for `ReadRows`. */
export interface ReadRowsRequest {
  /** Required. Stream to read rows from. */
  readStream: string;
  /**
   * The offset requested must be less than the last row read from Read.
   * Requesting a larger offset is undefined. If not specified, start reading
   * from offset zero.
   */
  offset: Long;
}

/** Information on if the current connection is being throttled. */
export interface ThrottleState {
  /**
   * How much this connection is being throttled. Zero means no throttling,
   * 100 means fully throttled.
   */
  throttlePercent: number;
}

/** Estimated stream statistics for a given read Stream. */
export interface StreamStats {
  /** Represents the progress of the current stream. */
  progress: StreamStats_Progress | undefined;
}

export interface StreamStats_Progress {
  /**
   * The fraction of rows assigned to the stream that have been processed by
   * the server so far, not including the rows in the current response
   * message.
   *
   * This value, along with `at_response_end`, can be used to interpolate
   * the progress made as the rows in the message are being processed using
   * the following formula: `at_response_start + (at_response_end -
   * at_response_start) * rows_processed_from_response / rows_in_response`.
   *
   * Note that if a filter is provided, the `at_response_end` value of the
   * previous response may not necessarily be equal to the
   * `at_response_start` value of the current response.
   */
  atResponseStart: number;
  /**
   * Similar to `at_response_start`, except that this value includes the
   * rows in the current response.
   */
  atResponseEnd: number;
}

/**
 * Response from calling `ReadRows` may include row data, progress and
 * throttling information.
 */
export interface ReadRowsResponse {
  /** Serialized row data in AVRO format. */
  avroRows?:
    | AvroRows
    | undefined;
  /** Serialized row data in Arrow RecordBatch format. */
  arrowRecordBatch?:
    | ArrowRecordBatch
    | undefined;
  /** Number of serialized rows in the rows block. */
  rowCount: Long;
  /** Statistics for the stream. */
  stats:
    | StreamStats
    | undefined;
  /**
   * Throttling state. If unset, the latest response still describes
   * the current throttling status.
   */
  throttleState:
    | ThrottleState
    | undefined;
  /** Output only. Avro schema. */
  avroSchema?:
    | AvroSchema
    | undefined;
  /** Output only. Arrow schema. */
  arrowSchema?:
    | ArrowSchema
    | undefined;
  /**
   * Optional. If the row data in this ReadRowsResponse is compressed, then
   * uncompressed byte size is the original size of the uncompressed row data.
   * If it is set to a value greater than 0, then decompress into a buffer of
   * size uncompressed_byte_size using the compression codec that was requested
   * during session creation time and which is specified in
   * TableReadOptions.response_compression_codec in ReadSession.
   * This value is not set if no response_compression_codec was not requested
   * and it is -1 if the requested compression would not have reduced the size
   * of this ReadRowsResponse's row data. This attempts to match Apache Arrow's
   * behavior described here https://github.com/apache/arrow/issues/15102 where
   * the uncompressed length may be set to -1 to indicate that the data that
   * follows is not compressed, which can be useful for cases where compression
   * does not yield appreciable savings. When uncompressed_byte_size is not
   * greater than 0, the client should skip decompression.
   */
  uncompressedByteSize?: Long | undefined;
}

/** Request message for `SplitReadStream`. */
export interface SplitReadStreamRequest {
  /** Required. Name of the stream to split. */
  name: string;
  /**
   * A value in the range (0.0, 1.0) that specifies the fractional point at
   * which the original stream should be split. The actual split point is
   * evaluated on pre-filtered rows, so if a filter is provided, then there is
   * no guarantee that the division of the rows between the new child streams
   * will be proportional to this fractional value. Additionally, because the
   * server-side unit for assigning data is collections of rows, this fraction
   * will always map to a data storage boundary on the server side.
   */
  fraction: number;
}

/** Response message for `SplitReadStream`. */
export interface SplitReadStreamResponse {
  /**
   * Primary stream, which contains the beginning portion of
   * |original_stream|. An empty value indicates that the original stream can no
   * longer be split.
   */
  primaryStream:
    | ReadStream
    | undefined;
  /**
   * Remainder stream, which contains the tail of |original_stream|. An empty
   * value indicates that the original stream can no longer be split.
   */
  remainderStream: ReadStream | undefined;
}

/** Request message for `CreateWriteStream`. */
export interface CreateWriteStreamRequest {
  /**
   * Required. Reference to the table to which the stream belongs, in the format
   * of `projects/{project}/datasets/{dataset}/tables/{table}`.
   */
  parent: string;
  /** Required. Stream to be created. */
  writeStream: WriteStream | undefined;
}

/**
 * Request message for `AppendRows`.
 *
 * Because AppendRows is a bidirectional streaming RPC, certain parts of the
 * AppendRowsRequest need only be specified for the first request before
 * switching table destinations. You can also switch table destinations within
 * the same connection for the default stream.
 *
 * The size of a single AppendRowsRequest must be less than 10 MB in size.
 * Requests larger than this return an error, typically `INVALID_ARGUMENT`.
 */
export interface AppendRowsRequest {
  /**
   * Required. The write_stream identifies the append operation. It must be
   * provided in the following scenarios:
   *
   * * In the first request to an AppendRows connection.
   *
   * * In all subsequent requests to an AppendRows connection, if you use the
   * same connection to write to multiple tables or change the input schema for
   * default streams.
   *
   * For explicitly created write streams, the format is:
   *
   * * `projects/{project}/datasets/{dataset}/tables/{table}/streams/{id}`
   *
   * For the special default stream, the format is:
   *
   * * `projects/{project}/datasets/{dataset}/tables/{table}/streams/_default`.
   *
   * An example of a possible sequence of requests with write_stream fields
   * within a single connection:
   *
   * * r1: {write_stream: stream_name_1}
   *
   * * r2: {write_stream: /*omit* /}
   *
   * * r3: {write_stream: /*omit* /}
   *
   * * r4: {write_stream: stream_name_2}
   *
   * * r5: {write_stream: stream_name_2}
   *
   * The destination changed in request_4, so the write_stream field must be
   * populated in all subsequent requests in this stream.
   */
  writeStream: string;
  /**
   * If present, the write is only performed if the next append offset is same
   * as the provided value. If not present, the write is performed at the
   * current end of stream. Specifying a value for this field is not allowed
   * when calling AppendRows for the '_default' stream.
   */
  offset:
    | Long
    | undefined;
  /** Rows in proto format. */
  protoRows?:
    | AppendRowsRequest_ProtoData
    | undefined;
  /**
   * Id set by client to annotate its identity. Only initial request setting is
   * respected.
   */
  traceId: string;
  /**
   * A map to indicate how to interpret missing value for some fields. Missing
   * values are fields present in user schema but missing in rows. The key is
   * the field name. The value is the interpretation of missing values for the
   * field.
   *
   * For example, a map {'foo': NULL_VALUE, 'bar': DEFAULT_VALUE} means all
   * missing values in field foo are interpreted as NULL, all missing values in
   * field bar are interpreted as the default value of field bar in table
   * schema.
   *
   * If a field is not in this map and has missing values, the missing values
   * in this field are interpreted as NULL.
   *
   * This field only applies to the current request, it won't affect other
   * requests on the connection.
   *
   * Currently, field name can only be top-level column name, can't be a struct
   * field path like 'foo.bar'.
   */
  missingValueInterpretations: { [key: string]: AppendRowsRequest_MissingValueInterpretation };
  /**
   * Optional. Default missing value interpretation for all columns in the
   * table. When a value is specified on an `AppendRowsRequest`, it is applied
   * to all requests on the connection from that point forward, until a
   * subsequent `AppendRowsRequest` sets it to a different value.
   * `missing_value_interpretation` can override
   * `default_missing_value_interpretation`. For example, if you want to write
   * `NULL` instead of using default values for some columns, you can set
   * `default_missing_value_interpretation` to `DEFAULT_VALUE` and at the same
   * time, set `missing_value_interpretations` to `NULL_VALUE` on those columns.
   */
  defaultMissingValueInterpretation: AppendRowsRequest_MissingValueInterpretation;
}

/**
 * An enum to indicate how to interpret missing values of fields that are
 * present in user schema but missing in rows. A missing value can represent a
 * NULL or a column default value defined in BigQuery table schema.
 */
export enum AppendRowsRequest_MissingValueInterpretation {
  /**
   * MISSING_VALUE_INTERPRETATION_UNSPECIFIED - Invalid missing value interpretation. Requests with this value will be
   * rejected.
   */
  MISSING_VALUE_INTERPRETATION_UNSPECIFIED = 0,
  /** NULL_VALUE - Missing value is interpreted as NULL. */
  NULL_VALUE = 1,
  /**
   * DEFAULT_VALUE - Missing value is interpreted as column default value if declared in the
   * table schema, NULL otherwise.
   */
  DEFAULT_VALUE = 2,
  UNRECOGNIZED = -1,
}

export function appendRowsRequest_MissingValueInterpretationFromJSON(
  object: any,
): AppendRowsRequest_MissingValueInterpretation {
  switch (object) {
    case 0:
    case "MISSING_VALUE_INTERPRETATION_UNSPECIFIED":
      return AppendRowsRequest_MissingValueInterpretation.MISSING_VALUE_INTERPRETATION_UNSPECIFIED;
    case 1:
    case "NULL_VALUE":
      return AppendRowsRequest_MissingValueInterpretation.NULL_VALUE;
    case 2:
    case "DEFAULT_VALUE":
      return AppendRowsRequest_MissingValueInterpretation.DEFAULT_VALUE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AppendRowsRequest_MissingValueInterpretation.UNRECOGNIZED;
  }
}

export function appendRowsRequest_MissingValueInterpretationToJSON(
  object: AppendRowsRequest_MissingValueInterpretation,
): string {
  switch (object) {
    case AppendRowsRequest_MissingValueInterpretation.MISSING_VALUE_INTERPRETATION_UNSPECIFIED:
      return "MISSING_VALUE_INTERPRETATION_UNSPECIFIED";
    case AppendRowsRequest_MissingValueInterpretation.NULL_VALUE:
      return "NULL_VALUE";
    case AppendRowsRequest_MissingValueInterpretation.DEFAULT_VALUE:
      return "DEFAULT_VALUE";
    case AppendRowsRequest_MissingValueInterpretation.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * ProtoData contains the data rows and schema when constructing append
 * requests.
 */
export interface AppendRowsRequest_ProtoData {
  /**
   * The protocol buffer schema used to serialize the data. Provide this value
   * whenever:
   *
   * * You send the first request of an RPC connection.
   *
   * * You change the input schema.
   *
   * * You specify a new destination table.
   */
  writerSchema:
    | ProtoSchema
    | undefined;
  /**
   * Serialized row data in protobuf message format.
   * Currently, the backend expects the serialized rows to adhere to
   * proto2 semantics when appending rows, particularly with respect to
   * how default values are encoded.
   */
  rows: ProtoRows | undefined;
}

export interface AppendRowsRequest_MissingValueInterpretationsEntry {
  key: string;
  value: AppendRowsRequest_MissingValueInterpretation;
}

/** Response message for `AppendRows`. */
export interface AppendRowsResponse {
  /** Result if the append is successful. */
  appendResult?:
    | AppendRowsResponse_AppendResult
    | undefined;
  /**
   * Error returned when problems were encountered.  If present,
   * it indicates rows were not accepted into the system.
   * Users can retry or continue with other append requests within the
   * same connection.
   *
   * Additional information about error signalling:
   *
   * ALREADY_EXISTS: Happens when an append specified an offset, and the
   * backend already has received data at this offset.  Typically encountered
   * in retry scenarios, and can be ignored.
   *
   * OUT_OF_RANGE: Returned when the specified offset in the stream is beyond
   * the current end of the stream.
   *
   * INVALID_ARGUMENT: Indicates a malformed request or data.
   *
   * ABORTED: Request processing is aborted because of prior failures.  The
   * request can be retried if previous failure is addressed.
   *
   * INTERNAL: Indicates server side error(s) that can be retried.
   */
  error?:
    | Status
    | undefined;
  /**
   * If backend detects a schema update, pass it to user so that user can
   * use it to input new type of message. It will be empty when no schema
   * updates have occurred.
   */
  updatedSchema:
    | TableSchema
    | undefined;
  /**
   * If a request failed due to corrupted rows, no rows in the batch will be
   * appended. The API will return row level error info, so that the caller can
   * remove the bad rows and retry the request.
   */
  rowErrors: RowError[];
  /**
   * The target of the append operation. Matches the write_stream in the
   * corresponding request.
   */
  writeStream: string;
}

/** AppendResult is returned for successful append requests. */
export interface AppendRowsResponse_AppendResult {
  /**
   * The row offset at which the last append occurred. The offset will not be
   * set if appending using default streams.
   */
  offset: Long | undefined;
}

/** Request message for `GetWriteStreamRequest`. */
export interface GetWriteStreamRequest {
  /**
   * Required. Name of the stream to get, in the form of
   * `projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}`.
   */
  name: string;
  /**
   * Indicates whether to get full or partial view of the WriteStream. If
   * not set, view returned will be basic.
   */
  view: WriteStreamView;
}

/** Request message for `BatchCommitWriteStreams`. */
export interface BatchCommitWriteStreamsRequest {
  /**
   * Required. Parent table that all the streams should belong to, in the form
   * of `projects/{project}/datasets/{dataset}/tables/{table}`.
   */
  parent: string;
  /** Required. The group of streams that will be committed atomically. */
  writeStreams: string[];
}

/** Response message for `BatchCommitWriteStreams`. */
export interface BatchCommitWriteStreamsResponse {
  /**
   * The time at which streams were committed in microseconds granularity.
   * This field will only exist when there are no stream errors.
   * **Note** if this field is not set, it means the commit was not successful.
   */
  commitTime:
    | Date
    | undefined;
  /**
   * Stream level error if commit failed. Only streams with error will be in
   * the list.
   * If empty, there is no error and all streams are committed successfully.
   * If non empty, certain streams have errors and ZERO stream is committed due
   * to atomicity guarantee.
   */
  streamErrors: StorageError[];
}

/** Request message for invoking `FinalizeWriteStream`. */
export interface FinalizeWriteStreamRequest {
  /**
   * Required. Name of the stream to finalize, in the form of
   * `projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}`.
   */
  name: string;
}

/** Response message for `FinalizeWriteStream`. */
export interface FinalizeWriteStreamResponse {
  /** Number of rows in the finalized stream. */
  rowCount: Long;
}

/** Request message for `FlushRows`. */
export interface FlushRowsRequest {
  /** Required. The stream that is the target of the flush operation. */
  writeStream: string;
  /**
   * Ending offset of the flush operation. Rows before this offset(including
   * this offset) will be flushed.
   */
  offset: Long | undefined;
}

/** Respond message for `FlushRows`. */
export interface FlushRowsResponse {
  /** The rows before this offset (including this offset) are flushed. */
  offset: Long;
}

/**
 * Structured custom BigQuery Storage error message. The error can be attached
 * as error details in the returned rpc Status. In particular, the use of error
 * codes allows more structured error handling, and reduces the need to evaluate
 * unstructured error text strings.
 */
export interface StorageError {
  /** BigQuery Storage specific error code. */
  code: StorageError_StorageErrorCode;
  /** Name of the failed entity. */
  entity: string;
  /** Message that describes the error. */
  errorMessage: string;
}

/** Error code for `StorageError`. */
export enum StorageError_StorageErrorCode {
  /** STORAGE_ERROR_CODE_UNSPECIFIED - Default error. */
  STORAGE_ERROR_CODE_UNSPECIFIED = 0,
  /** TABLE_NOT_FOUND - Table is not found in the system. */
  TABLE_NOT_FOUND = 1,
  /** STREAM_ALREADY_COMMITTED - Stream is already committed. */
  STREAM_ALREADY_COMMITTED = 2,
  /** STREAM_NOT_FOUND - Stream is not found. */
  STREAM_NOT_FOUND = 3,
  /**
   * INVALID_STREAM_TYPE - Invalid Stream type.
   * For example, you try to commit a stream that is not pending.
   */
  INVALID_STREAM_TYPE = 4,
  /**
   * INVALID_STREAM_STATE - Invalid Stream state.
   * For example, you try to commit a stream that is not finalized or is
   * garbaged.
   */
  INVALID_STREAM_STATE = 5,
  /** STREAM_FINALIZED - Stream is finalized. */
  STREAM_FINALIZED = 6,
  /**
   * SCHEMA_MISMATCH_EXTRA_FIELDS - There is a schema mismatch and it is caused by user schema has extra
   * field than bigquery schema.
   */
  SCHEMA_MISMATCH_EXTRA_FIELDS = 7,
  /** OFFSET_ALREADY_EXISTS - Offset already exists. */
  OFFSET_ALREADY_EXISTS = 8,
  /** OFFSET_OUT_OF_RANGE - Offset out of range. */
  OFFSET_OUT_OF_RANGE = 9,
  /**
   * CMEK_NOT_PROVIDED - Customer-managed encryption key (CMEK) not provided for CMEK-enabled
   * data.
   */
  CMEK_NOT_PROVIDED = 10,
  /** INVALID_CMEK_PROVIDED - Customer-managed encryption key (CMEK) was incorrectly provided. */
  INVALID_CMEK_PROVIDED = 11,
  /** CMEK_ENCRYPTION_ERROR - There is an encryption error while using customer-managed encryption key. */
  CMEK_ENCRYPTION_ERROR = 12,
  /**
   * KMS_SERVICE_ERROR - Key Management Service (KMS) service returned an error, which can be
   * retried.
   */
  KMS_SERVICE_ERROR = 13,
  /** KMS_PERMISSION_DENIED - Permission denied while using customer-managed encryption key. */
  KMS_PERMISSION_DENIED = 14,
  UNRECOGNIZED = -1,
}

export function storageError_StorageErrorCodeFromJSON(object: any): StorageError_StorageErrorCode {
  switch (object) {
    case 0:
    case "STORAGE_ERROR_CODE_UNSPECIFIED":
      return StorageError_StorageErrorCode.STORAGE_ERROR_CODE_UNSPECIFIED;
    case 1:
    case "TABLE_NOT_FOUND":
      return StorageError_StorageErrorCode.TABLE_NOT_FOUND;
    case 2:
    case "STREAM_ALREADY_COMMITTED":
      return StorageError_StorageErrorCode.STREAM_ALREADY_COMMITTED;
    case 3:
    case "STREAM_NOT_FOUND":
      return StorageError_StorageErrorCode.STREAM_NOT_FOUND;
    case 4:
    case "INVALID_STREAM_TYPE":
      return StorageError_StorageErrorCode.INVALID_STREAM_TYPE;
    case 5:
    case "INVALID_STREAM_STATE":
      return StorageError_StorageErrorCode.INVALID_STREAM_STATE;
    case 6:
    case "STREAM_FINALIZED":
      return StorageError_StorageErrorCode.STREAM_FINALIZED;
    case 7:
    case "SCHEMA_MISMATCH_EXTRA_FIELDS":
      return StorageError_StorageErrorCode.SCHEMA_MISMATCH_EXTRA_FIELDS;
    case 8:
    case "OFFSET_ALREADY_EXISTS":
      return StorageError_StorageErrorCode.OFFSET_ALREADY_EXISTS;
    case 9:
    case "OFFSET_OUT_OF_RANGE":
      return StorageError_StorageErrorCode.OFFSET_OUT_OF_RANGE;
    case 10:
    case "CMEK_NOT_PROVIDED":
      return StorageError_StorageErrorCode.CMEK_NOT_PROVIDED;
    case 11:
    case "INVALID_CMEK_PROVIDED":
      return StorageError_StorageErrorCode.INVALID_CMEK_PROVIDED;
    case 12:
    case "CMEK_ENCRYPTION_ERROR":
      return StorageError_StorageErrorCode.CMEK_ENCRYPTION_ERROR;
    case 13:
    case "KMS_SERVICE_ERROR":
      return StorageError_StorageErrorCode.KMS_SERVICE_ERROR;
    case 14:
    case "KMS_PERMISSION_DENIED":
      return StorageError_StorageErrorCode.KMS_PERMISSION_DENIED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return StorageError_StorageErrorCode.UNRECOGNIZED;
  }
}

export function storageError_StorageErrorCodeToJSON(object: StorageError_StorageErrorCode): string {
  switch (object) {
    case StorageError_StorageErrorCode.STORAGE_ERROR_CODE_UNSPECIFIED:
      return "STORAGE_ERROR_CODE_UNSPECIFIED";
    case StorageError_StorageErrorCode.TABLE_NOT_FOUND:
      return "TABLE_NOT_FOUND";
    case StorageError_StorageErrorCode.STREAM_ALREADY_COMMITTED:
      return "STREAM_ALREADY_COMMITTED";
    case StorageError_StorageErrorCode.STREAM_NOT_FOUND:
      return "STREAM_NOT_FOUND";
    case StorageError_StorageErrorCode.INVALID_STREAM_TYPE:
      return "INVALID_STREAM_TYPE";
    case StorageError_StorageErrorCode.INVALID_STREAM_STATE:
      return "INVALID_STREAM_STATE";
    case StorageError_StorageErrorCode.STREAM_FINALIZED:
      return "STREAM_FINALIZED";
    case StorageError_StorageErrorCode.SCHEMA_MISMATCH_EXTRA_FIELDS:
      return "SCHEMA_MISMATCH_EXTRA_FIELDS";
    case StorageError_StorageErrorCode.OFFSET_ALREADY_EXISTS:
      return "OFFSET_ALREADY_EXISTS";
    case StorageError_StorageErrorCode.OFFSET_OUT_OF_RANGE:
      return "OFFSET_OUT_OF_RANGE";
    case StorageError_StorageErrorCode.CMEK_NOT_PROVIDED:
      return "CMEK_NOT_PROVIDED";
    case StorageError_StorageErrorCode.INVALID_CMEK_PROVIDED:
      return "INVALID_CMEK_PROVIDED";
    case StorageError_StorageErrorCode.CMEK_ENCRYPTION_ERROR:
      return "CMEK_ENCRYPTION_ERROR";
    case StorageError_StorageErrorCode.KMS_SERVICE_ERROR:
      return "KMS_SERVICE_ERROR";
    case StorageError_StorageErrorCode.KMS_PERMISSION_DENIED:
      return "KMS_PERMISSION_DENIED";
    case StorageError_StorageErrorCode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The message that presents row level error info in a request. */
export interface RowError {
  /** Index of the malformed row in the request. */
  index: Long;
  /** Structured error reason for a row error. */
  code: RowError_RowErrorCode;
  /** Description of the issue encountered when processing the row. */
  message: string;
}

/** Error code for `RowError`. */
export enum RowError_RowErrorCode {
  /** ROW_ERROR_CODE_UNSPECIFIED - Default error. */
  ROW_ERROR_CODE_UNSPECIFIED = 0,
  /** FIELDS_ERROR - One or more fields in the row has errors. */
  FIELDS_ERROR = 1,
  UNRECOGNIZED = -1,
}

export function rowError_RowErrorCodeFromJSON(object: any): RowError_RowErrorCode {
  switch (object) {
    case 0:
    case "ROW_ERROR_CODE_UNSPECIFIED":
      return RowError_RowErrorCode.ROW_ERROR_CODE_UNSPECIFIED;
    case 1:
    case "FIELDS_ERROR":
      return RowError_RowErrorCode.FIELDS_ERROR;
    case -1:
    case "UNRECOGNIZED":
    default:
      return RowError_RowErrorCode.UNRECOGNIZED;
  }
}

export function rowError_RowErrorCodeToJSON(object: RowError_RowErrorCode): string {
  switch (object) {
    case RowError_RowErrorCode.ROW_ERROR_CODE_UNSPECIFIED:
      return "ROW_ERROR_CODE_UNSPECIFIED";
    case RowError_RowErrorCode.FIELDS_ERROR:
      return "FIELDS_ERROR";
    case RowError_RowErrorCode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseCreateReadSessionRequest(): CreateReadSessionRequest {
  return { parent: "", readSession: undefined, maxStreamCount: 0, preferredMinStreamCount: 0 };
}

export const CreateReadSessionRequest: MessageFns<CreateReadSessionRequest> = {
  encode(message: CreateReadSessionRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.readSession !== undefined) {
      ReadSession.encode(message.readSession, writer.uint32(18).fork()).join();
    }
    if (message.maxStreamCount !== 0) {
      writer.uint32(24).int32(message.maxStreamCount);
    }
    if (message.preferredMinStreamCount !== 0) {
      writer.uint32(32).int32(message.preferredMinStreamCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateReadSessionRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateReadSessionRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.readSession = ReadSession.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.maxStreamCount = reader.int32();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.preferredMinStreamCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateReadSessionRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      readSession: isSet(object.readSession) ? ReadSession.fromJSON(object.readSession) : undefined,
      maxStreamCount: isSet(object.maxStreamCount) ? globalThis.Number(object.maxStreamCount) : 0,
      preferredMinStreamCount: isSet(object.preferredMinStreamCount)
        ? globalThis.Number(object.preferredMinStreamCount)
        : 0,
    };
  },

  toJSON(message: CreateReadSessionRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.readSession !== undefined) {
      obj.readSession = ReadSession.toJSON(message.readSession);
    }
    if (message.maxStreamCount !== 0) {
      obj.maxStreamCount = Math.round(message.maxStreamCount);
    }
    if (message.preferredMinStreamCount !== 0) {
      obj.preferredMinStreamCount = Math.round(message.preferredMinStreamCount);
    }
    return obj;
  },

  create(base?: DeepPartial<CreateReadSessionRequest>): CreateReadSessionRequest {
    return CreateReadSessionRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateReadSessionRequest>): CreateReadSessionRequest {
    const message = createBaseCreateReadSessionRequest();
    message.parent = object.parent ?? "";
    message.readSession = (object.readSession !== undefined && object.readSession !== null)
      ? ReadSession.fromPartial(object.readSession)
      : undefined;
    message.maxStreamCount = object.maxStreamCount ?? 0;
    message.preferredMinStreamCount = object.preferredMinStreamCount ?? 0;
    return message;
  },
};

function createBaseReadRowsRequest(): ReadRowsRequest {
  return { readStream: "", offset: Long.ZERO };
}

export const ReadRowsRequest: MessageFns<ReadRowsRequest> = {
  encode(message: ReadRowsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.readStream !== "") {
      writer.uint32(10).string(message.readStream);
    }
    if (!message.offset.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.offset.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReadRowsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReadRowsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.readStream = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.offset = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReadRowsRequest {
    return {
      readStream: isSet(object.readStream) ? globalThis.String(object.readStream) : "",
      offset: isSet(object.offset) ? Long.fromValue(object.offset) : Long.ZERO,
    };
  },

  toJSON(message: ReadRowsRequest): unknown {
    const obj: any = {};
    if (message.readStream !== "") {
      obj.readStream = message.readStream;
    }
    if (!message.offset.equals(Long.ZERO)) {
      obj.offset = (message.offset || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ReadRowsRequest>): ReadRowsRequest {
    return ReadRowsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReadRowsRequest>): ReadRowsRequest {
    const message = createBaseReadRowsRequest();
    message.readStream = object.readStream ?? "";
    message.offset = (object.offset !== undefined && object.offset !== null)
      ? Long.fromValue(object.offset)
      : Long.ZERO;
    return message;
  },
};

function createBaseThrottleState(): ThrottleState {
  return { throttlePercent: 0 };
}

export const ThrottleState: MessageFns<ThrottleState> = {
  encode(message: ThrottleState, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.throttlePercent !== 0) {
      writer.uint32(8).int32(message.throttlePercent);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ThrottleState {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseThrottleState();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.throttlePercent = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ThrottleState {
    return { throttlePercent: isSet(object.throttlePercent) ? globalThis.Number(object.throttlePercent) : 0 };
  },

  toJSON(message: ThrottleState): unknown {
    const obj: any = {};
    if (message.throttlePercent !== 0) {
      obj.throttlePercent = Math.round(message.throttlePercent);
    }
    return obj;
  },

  create(base?: DeepPartial<ThrottleState>): ThrottleState {
    return ThrottleState.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ThrottleState>): ThrottleState {
    const message = createBaseThrottleState();
    message.throttlePercent = object.throttlePercent ?? 0;
    return message;
  },
};

function createBaseStreamStats(): StreamStats {
  return { progress: undefined };
}

export const StreamStats: MessageFns<StreamStats> = {
  encode(message: StreamStats, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.progress !== undefined) {
      StreamStats_Progress.encode(message.progress, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamStats {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamStats();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.progress = StreamStats_Progress.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamStats {
    return { progress: isSet(object.progress) ? StreamStats_Progress.fromJSON(object.progress) : undefined };
  },

  toJSON(message: StreamStats): unknown {
    const obj: any = {};
    if (message.progress !== undefined) {
      obj.progress = StreamStats_Progress.toJSON(message.progress);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamStats>): StreamStats {
    return StreamStats.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamStats>): StreamStats {
    const message = createBaseStreamStats();
    message.progress = (object.progress !== undefined && object.progress !== null)
      ? StreamStats_Progress.fromPartial(object.progress)
      : undefined;
    return message;
  },
};

function createBaseStreamStats_Progress(): StreamStats_Progress {
  return { atResponseStart: 0, atResponseEnd: 0 };
}

export const StreamStats_Progress: MessageFns<StreamStats_Progress> = {
  encode(message: StreamStats_Progress, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.atResponseStart !== 0) {
      writer.uint32(9).double(message.atResponseStart);
    }
    if (message.atResponseEnd !== 0) {
      writer.uint32(17).double(message.atResponseEnd);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamStats_Progress {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamStats_Progress();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 9) {
            break;
          }

          message.atResponseStart = reader.double();
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.atResponseEnd = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamStats_Progress {
    return {
      atResponseStart: isSet(object.atResponseStart) ? globalThis.Number(object.atResponseStart) : 0,
      atResponseEnd: isSet(object.atResponseEnd) ? globalThis.Number(object.atResponseEnd) : 0,
    };
  },

  toJSON(message: StreamStats_Progress): unknown {
    const obj: any = {};
    if (message.atResponseStart !== 0) {
      obj.atResponseStart = message.atResponseStart;
    }
    if (message.atResponseEnd !== 0) {
      obj.atResponseEnd = message.atResponseEnd;
    }
    return obj;
  },

  create(base?: DeepPartial<StreamStats_Progress>): StreamStats_Progress {
    return StreamStats_Progress.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamStats_Progress>): StreamStats_Progress {
    const message = createBaseStreamStats_Progress();
    message.atResponseStart = object.atResponseStart ?? 0;
    message.atResponseEnd = object.atResponseEnd ?? 0;
    return message;
  },
};

function createBaseReadRowsResponse(): ReadRowsResponse {
  return {
    avroRows: undefined,
    arrowRecordBatch: undefined,
    rowCount: Long.ZERO,
    stats: undefined,
    throttleState: undefined,
    avroSchema: undefined,
    arrowSchema: undefined,
    uncompressedByteSize: undefined,
  };
}

export const ReadRowsResponse: MessageFns<ReadRowsResponse> = {
  encode(message: ReadRowsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.avroRows !== undefined) {
      AvroRows.encode(message.avroRows, writer.uint32(26).fork()).join();
    }
    if (message.arrowRecordBatch !== undefined) {
      ArrowRecordBatch.encode(message.arrowRecordBatch, writer.uint32(34).fork()).join();
    }
    if (!message.rowCount.equals(Long.ZERO)) {
      writer.uint32(48).int64(message.rowCount.toString());
    }
    if (message.stats !== undefined) {
      StreamStats.encode(message.stats, writer.uint32(18).fork()).join();
    }
    if (message.throttleState !== undefined) {
      ThrottleState.encode(message.throttleState, writer.uint32(42).fork()).join();
    }
    if (message.avroSchema !== undefined) {
      AvroSchema.encode(message.avroSchema, writer.uint32(58).fork()).join();
    }
    if (message.arrowSchema !== undefined) {
      ArrowSchema.encode(message.arrowSchema, writer.uint32(66).fork()).join();
    }
    if (message.uncompressedByteSize !== undefined) {
      writer.uint32(72).int64(message.uncompressedByteSize.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReadRowsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReadRowsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 26) {
            break;
          }

          message.avroRows = AvroRows.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.arrowRecordBatch = ArrowRecordBatch.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.rowCount = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.stats = StreamStats.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.throttleState = ThrottleState.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.avroSchema = AvroSchema.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.arrowSchema = ArrowSchema.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.uncompressedByteSize = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReadRowsResponse {
    return {
      avroRows: isSet(object.avroRows) ? AvroRows.fromJSON(object.avroRows) : undefined,
      arrowRecordBatch: isSet(object.arrowRecordBatch) ? ArrowRecordBatch.fromJSON(object.arrowRecordBatch) : undefined,
      rowCount: isSet(object.rowCount) ? Long.fromValue(object.rowCount) : Long.ZERO,
      stats: isSet(object.stats) ? StreamStats.fromJSON(object.stats) : undefined,
      throttleState: isSet(object.throttleState) ? ThrottleState.fromJSON(object.throttleState) : undefined,
      avroSchema: isSet(object.avroSchema) ? AvroSchema.fromJSON(object.avroSchema) : undefined,
      arrowSchema: isSet(object.arrowSchema) ? ArrowSchema.fromJSON(object.arrowSchema) : undefined,
      uncompressedByteSize: isSet(object.uncompressedByteSize)
        ? Long.fromValue(object.uncompressedByteSize)
        : undefined,
    };
  },

  toJSON(message: ReadRowsResponse): unknown {
    const obj: any = {};
    if (message.avroRows !== undefined) {
      obj.avroRows = AvroRows.toJSON(message.avroRows);
    }
    if (message.arrowRecordBatch !== undefined) {
      obj.arrowRecordBatch = ArrowRecordBatch.toJSON(message.arrowRecordBatch);
    }
    if (!message.rowCount.equals(Long.ZERO)) {
      obj.rowCount = (message.rowCount || Long.ZERO).toString();
    }
    if (message.stats !== undefined) {
      obj.stats = StreamStats.toJSON(message.stats);
    }
    if (message.throttleState !== undefined) {
      obj.throttleState = ThrottleState.toJSON(message.throttleState);
    }
    if (message.avroSchema !== undefined) {
      obj.avroSchema = AvroSchema.toJSON(message.avroSchema);
    }
    if (message.arrowSchema !== undefined) {
      obj.arrowSchema = ArrowSchema.toJSON(message.arrowSchema);
    }
    if (message.uncompressedByteSize !== undefined) {
      obj.uncompressedByteSize = (message.uncompressedByteSize || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ReadRowsResponse>): ReadRowsResponse {
    return ReadRowsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReadRowsResponse>): ReadRowsResponse {
    const message = createBaseReadRowsResponse();
    message.avroRows = (object.avroRows !== undefined && object.avroRows !== null)
      ? AvroRows.fromPartial(object.avroRows)
      : undefined;
    message.arrowRecordBatch = (object.arrowRecordBatch !== undefined && object.arrowRecordBatch !== null)
      ? ArrowRecordBatch.fromPartial(object.arrowRecordBatch)
      : undefined;
    message.rowCount = (object.rowCount !== undefined && object.rowCount !== null)
      ? Long.fromValue(object.rowCount)
      : Long.ZERO;
    message.stats = (object.stats !== undefined && object.stats !== null)
      ? StreamStats.fromPartial(object.stats)
      : undefined;
    message.throttleState = (object.throttleState !== undefined && object.throttleState !== null)
      ? ThrottleState.fromPartial(object.throttleState)
      : undefined;
    message.avroSchema = (object.avroSchema !== undefined && object.avroSchema !== null)
      ? AvroSchema.fromPartial(object.avroSchema)
      : undefined;
    message.arrowSchema = (object.arrowSchema !== undefined && object.arrowSchema !== null)
      ? ArrowSchema.fromPartial(object.arrowSchema)
      : undefined;
    message.uncompressedByteSize = (object.uncompressedByteSize !== undefined && object.uncompressedByteSize !== null)
      ? Long.fromValue(object.uncompressedByteSize)
      : undefined;
    return message;
  },
};

function createBaseSplitReadStreamRequest(): SplitReadStreamRequest {
  return { name: "", fraction: 0 };
}

export const SplitReadStreamRequest: MessageFns<SplitReadStreamRequest> = {
  encode(message: SplitReadStreamRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.fraction !== 0) {
      writer.uint32(17).double(message.fraction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SplitReadStreamRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSplitReadStreamRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.fraction = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SplitReadStreamRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      fraction: isSet(object.fraction) ? globalThis.Number(object.fraction) : 0,
    };
  },

  toJSON(message: SplitReadStreamRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.fraction !== 0) {
      obj.fraction = message.fraction;
    }
    return obj;
  },

  create(base?: DeepPartial<SplitReadStreamRequest>): SplitReadStreamRequest {
    return SplitReadStreamRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SplitReadStreamRequest>): SplitReadStreamRequest {
    const message = createBaseSplitReadStreamRequest();
    message.name = object.name ?? "";
    message.fraction = object.fraction ?? 0;
    return message;
  },
};

function createBaseSplitReadStreamResponse(): SplitReadStreamResponse {
  return { primaryStream: undefined, remainderStream: undefined };
}

export const SplitReadStreamResponse: MessageFns<SplitReadStreamResponse> = {
  encode(message: SplitReadStreamResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.primaryStream !== undefined) {
      ReadStream.encode(message.primaryStream, writer.uint32(10).fork()).join();
    }
    if (message.remainderStream !== undefined) {
      ReadStream.encode(message.remainderStream, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SplitReadStreamResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSplitReadStreamResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.primaryStream = ReadStream.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.remainderStream = ReadStream.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SplitReadStreamResponse {
    return {
      primaryStream: isSet(object.primaryStream) ? ReadStream.fromJSON(object.primaryStream) : undefined,
      remainderStream: isSet(object.remainderStream) ? ReadStream.fromJSON(object.remainderStream) : undefined,
    };
  },

  toJSON(message: SplitReadStreamResponse): unknown {
    const obj: any = {};
    if (message.primaryStream !== undefined) {
      obj.primaryStream = ReadStream.toJSON(message.primaryStream);
    }
    if (message.remainderStream !== undefined) {
      obj.remainderStream = ReadStream.toJSON(message.remainderStream);
    }
    return obj;
  },

  create(base?: DeepPartial<SplitReadStreamResponse>): SplitReadStreamResponse {
    return SplitReadStreamResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SplitReadStreamResponse>): SplitReadStreamResponse {
    const message = createBaseSplitReadStreamResponse();
    message.primaryStream = (object.primaryStream !== undefined && object.primaryStream !== null)
      ? ReadStream.fromPartial(object.primaryStream)
      : undefined;
    message.remainderStream = (object.remainderStream !== undefined && object.remainderStream !== null)
      ? ReadStream.fromPartial(object.remainderStream)
      : undefined;
    return message;
  },
};

function createBaseCreateWriteStreamRequest(): CreateWriteStreamRequest {
  return { parent: "", writeStream: undefined };
}

export const CreateWriteStreamRequest: MessageFns<CreateWriteStreamRequest> = {
  encode(message: CreateWriteStreamRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.writeStream !== undefined) {
      WriteStream.encode(message.writeStream, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateWriteStreamRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateWriteStreamRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.writeStream = WriteStream.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateWriteStreamRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      writeStream: isSet(object.writeStream) ? WriteStream.fromJSON(object.writeStream) : undefined,
    };
  },

  toJSON(message: CreateWriteStreamRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.writeStream !== undefined) {
      obj.writeStream = WriteStream.toJSON(message.writeStream);
    }
    return obj;
  },

  create(base?: DeepPartial<CreateWriteStreamRequest>): CreateWriteStreamRequest {
    return CreateWriteStreamRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateWriteStreamRequest>): CreateWriteStreamRequest {
    const message = createBaseCreateWriteStreamRequest();
    message.parent = object.parent ?? "";
    message.writeStream = (object.writeStream !== undefined && object.writeStream !== null)
      ? WriteStream.fromPartial(object.writeStream)
      : undefined;
    return message;
  },
};

function createBaseAppendRowsRequest(): AppendRowsRequest {
  return {
    writeStream: "",
    offset: undefined,
    protoRows: undefined,
    traceId: "",
    missingValueInterpretations: {},
    defaultMissingValueInterpretation: 0,
  };
}

export const AppendRowsRequest: MessageFns<AppendRowsRequest> = {
  encode(message: AppendRowsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.writeStream !== "") {
      writer.uint32(10).string(message.writeStream);
    }
    if (message.offset !== undefined) {
      Int64Value.encode({ value: message.offset! }, writer.uint32(18).fork()).join();
    }
    if (message.protoRows !== undefined) {
      AppendRowsRequest_ProtoData.encode(message.protoRows, writer.uint32(34).fork()).join();
    }
    if (message.traceId !== "") {
      writer.uint32(50).string(message.traceId);
    }
    Object.entries(message.missingValueInterpretations).forEach(([key, value]) => {
      AppendRowsRequest_MissingValueInterpretationsEntry.encode({ key: key as any, value }, writer.uint32(58).fork())
        .join();
    });
    if (message.defaultMissingValueInterpretation !== 0) {
      writer.uint32(64).int32(message.defaultMissingValueInterpretation);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppendRowsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppendRowsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.writeStream = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.offset = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.protoRows = AppendRowsRequest_ProtoData.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.traceId = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          const entry7 = AppendRowsRequest_MissingValueInterpretationsEntry.decode(reader, reader.uint32());
          if (entry7.value !== undefined) {
            message.missingValueInterpretations[entry7.key] = entry7.value;
          }
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.defaultMissingValueInterpretation = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AppendRowsRequest {
    return {
      writeStream: isSet(object.writeStream) ? globalThis.String(object.writeStream) : "",
      offset: isSet(object.offset) ? Long.fromValue(object.offset) : undefined,
      protoRows: isSet(object.protoRows) ? AppendRowsRequest_ProtoData.fromJSON(object.protoRows) : undefined,
      traceId: isSet(object.traceId) ? globalThis.String(object.traceId) : "",
      missingValueInterpretations: isObject(object.missingValueInterpretations)
        ? Object.entries(object.missingValueInterpretations).reduce<
          { [key: string]: AppendRowsRequest_MissingValueInterpretation }
        >((acc, [key, value]) => {
          acc[key] = appendRowsRequest_MissingValueInterpretationFromJSON(value);
          return acc;
        }, {})
        : {},
      defaultMissingValueInterpretation: isSet(object.defaultMissingValueInterpretation)
        ? appendRowsRequest_MissingValueInterpretationFromJSON(object.defaultMissingValueInterpretation)
        : 0,
    };
  },

  toJSON(message: AppendRowsRequest): unknown {
    const obj: any = {};
    if (message.writeStream !== "") {
      obj.writeStream = message.writeStream;
    }
    if (message.offset !== undefined) {
      obj.offset = message.offset;
    }
    if (message.protoRows !== undefined) {
      obj.protoRows = AppendRowsRequest_ProtoData.toJSON(message.protoRows);
    }
    if (message.traceId !== "") {
      obj.traceId = message.traceId;
    }
    if (message.missingValueInterpretations) {
      const entries = Object.entries(message.missingValueInterpretations);
      if (entries.length > 0) {
        obj.missingValueInterpretations = {};
        entries.forEach(([k, v]) => {
          obj.missingValueInterpretations[k] = appendRowsRequest_MissingValueInterpretationToJSON(v);
        });
      }
    }
    if (message.defaultMissingValueInterpretation !== 0) {
      obj.defaultMissingValueInterpretation = appendRowsRequest_MissingValueInterpretationToJSON(
        message.defaultMissingValueInterpretation,
      );
    }
    return obj;
  },

  create(base?: DeepPartial<AppendRowsRequest>): AppendRowsRequest {
    return AppendRowsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AppendRowsRequest>): AppendRowsRequest {
    const message = createBaseAppendRowsRequest();
    message.writeStream = object.writeStream ?? "";
    message.offset = (object.offset !== undefined && object.offset !== null)
      ? Long.fromValue(object.offset)
      : undefined;
    message.protoRows = (object.protoRows !== undefined && object.protoRows !== null)
      ? AppendRowsRequest_ProtoData.fromPartial(object.protoRows)
      : undefined;
    message.traceId = object.traceId ?? "";
    message.missingValueInterpretations = Object.entries(object.missingValueInterpretations ?? {}).reduce<
      { [key: string]: AppendRowsRequest_MissingValueInterpretation }
    >((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = value as AppendRowsRequest_MissingValueInterpretation;
      }
      return acc;
    }, {});
    message.defaultMissingValueInterpretation = object.defaultMissingValueInterpretation ?? 0;
    return message;
  },
};

function createBaseAppendRowsRequest_ProtoData(): AppendRowsRequest_ProtoData {
  return { writerSchema: undefined, rows: undefined };
}

export const AppendRowsRequest_ProtoData: MessageFns<AppendRowsRequest_ProtoData> = {
  encode(message: AppendRowsRequest_ProtoData, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.writerSchema !== undefined) {
      ProtoSchema.encode(message.writerSchema, writer.uint32(10).fork()).join();
    }
    if (message.rows !== undefined) {
      ProtoRows.encode(message.rows, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppendRowsRequest_ProtoData {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppendRowsRequest_ProtoData();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.writerSchema = ProtoSchema.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.rows = ProtoRows.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AppendRowsRequest_ProtoData {
    return {
      writerSchema: isSet(object.writerSchema) ? ProtoSchema.fromJSON(object.writerSchema) : undefined,
      rows: isSet(object.rows) ? ProtoRows.fromJSON(object.rows) : undefined,
    };
  },

  toJSON(message: AppendRowsRequest_ProtoData): unknown {
    const obj: any = {};
    if (message.writerSchema !== undefined) {
      obj.writerSchema = ProtoSchema.toJSON(message.writerSchema);
    }
    if (message.rows !== undefined) {
      obj.rows = ProtoRows.toJSON(message.rows);
    }
    return obj;
  },

  create(base?: DeepPartial<AppendRowsRequest_ProtoData>): AppendRowsRequest_ProtoData {
    return AppendRowsRequest_ProtoData.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AppendRowsRequest_ProtoData>): AppendRowsRequest_ProtoData {
    const message = createBaseAppendRowsRequest_ProtoData();
    message.writerSchema = (object.writerSchema !== undefined && object.writerSchema !== null)
      ? ProtoSchema.fromPartial(object.writerSchema)
      : undefined;
    message.rows = (object.rows !== undefined && object.rows !== null) ? ProtoRows.fromPartial(object.rows) : undefined;
    return message;
  },
};

function createBaseAppendRowsRequest_MissingValueInterpretationsEntry(): AppendRowsRequest_MissingValueInterpretationsEntry {
  return { key: "", value: 0 };
}

export const AppendRowsRequest_MissingValueInterpretationsEntry: MessageFns<
  AppendRowsRequest_MissingValueInterpretationsEntry
> = {
  encode(
    message: AppendRowsRequest_MissingValueInterpretationsEntry,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== 0) {
      writer.uint32(16).int32(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppendRowsRequest_MissingValueInterpretationsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppendRowsRequest_MissingValueInterpretationsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.value = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AppendRowsRequest_MissingValueInterpretationsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? appendRowsRequest_MissingValueInterpretationFromJSON(object.value) : 0,
    };
  },

  toJSON(message: AppendRowsRequest_MissingValueInterpretationsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== 0) {
      obj.value = appendRowsRequest_MissingValueInterpretationToJSON(message.value);
    }
    return obj;
  },

  create(
    base?: DeepPartial<AppendRowsRequest_MissingValueInterpretationsEntry>,
  ): AppendRowsRequest_MissingValueInterpretationsEntry {
    return AppendRowsRequest_MissingValueInterpretationsEntry.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<AppendRowsRequest_MissingValueInterpretationsEntry>,
  ): AppendRowsRequest_MissingValueInterpretationsEntry {
    const message = createBaseAppendRowsRequest_MissingValueInterpretationsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? 0;
    return message;
  },
};

function createBaseAppendRowsResponse(): AppendRowsResponse {
  return { appendResult: undefined, error: undefined, updatedSchema: undefined, rowErrors: [], writeStream: "" };
}

export const AppendRowsResponse: MessageFns<AppendRowsResponse> = {
  encode(message: AppendRowsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.appendResult !== undefined) {
      AppendRowsResponse_AppendResult.encode(message.appendResult, writer.uint32(10).fork()).join();
    }
    if (message.error !== undefined) {
      Status.encode(message.error, writer.uint32(18).fork()).join();
    }
    if (message.updatedSchema !== undefined) {
      TableSchema.encode(message.updatedSchema, writer.uint32(26).fork()).join();
    }
    for (const v of message.rowErrors) {
      RowError.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.writeStream !== "") {
      writer.uint32(42).string(message.writeStream);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppendRowsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppendRowsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.appendResult = AppendRowsResponse_AppendResult.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.error = Status.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.updatedSchema = TableSchema.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.rowErrors.push(RowError.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.writeStream = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AppendRowsResponse {
    return {
      appendResult: isSet(object.appendResult)
        ? AppendRowsResponse_AppendResult.fromJSON(object.appendResult)
        : undefined,
      error: isSet(object.error) ? Status.fromJSON(object.error) : undefined,
      updatedSchema: isSet(object.updatedSchema) ? TableSchema.fromJSON(object.updatedSchema) : undefined,
      rowErrors: globalThis.Array.isArray(object?.rowErrors)
        ? object.rowErrors.map((e: any) => RowError.fromJSON(e))
        : [],
      writeStream: isSet(object.writeStream) ? globalThis.String(object.writeStream) : "",
    };
  },

  toJSON(message: AppendRowsResponse): unknown {
    const obj: any = {};
    if (message.appendResult !== undefined) {
      obj.appendResult = AppendRowsResponse_AppendResult.toJSON(message.appendResult);
    }
    if (message.error !== undefined) {
      obj.error = Status.toJSON(message.error);
    }
    if (message.updatedSchema !== undefined) {
      obj.updatedSchema = TableSchema.toJSON(message.updatedSchema);
    }
    if (message.rowErrors?.length) {
      obj.rowErrors = message.rowErrors.map((e) => RowError.toJSON(e));
    }
    if (message.writeStream !== "") {
      obj.writeStream = message.writeStream;
    }
    return obj;
  },

  create(base?: DeepPartial<AppendRowsResponse>): AppendRowsResponse {
    return AppendRowsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AppendRowsResponse>): AppendRowsResponse {
    const message = createBaseAppendRowsResponse();
    message.appendResult = (object.appendResult !== undefined && object.appendResult !== null)
      ? AppendRowsResponse_AppendResult.fromPartial(object.appendResult)
      : undefined;
    message.error = (object.error !== undefined && object.error !== null)
      ? Status.fromPartial(object.error)
      : undefined;
    message.updatedSchema = (object.updatedSchema !== undefined && object.updatedSchema !== null)
      ? TableSchema.fromPartial(object.updatedSchema)
      : undefined;
    message.rowErrors = object.rowErrors?.map((e) => RowError.fromPartial(e)) || [];
    message.writeStream = object.writeStream ?? "";
    return message;
  },
};

function createBaseAppendRowsResponse_AppendResult(): AppendRowsResponse_AppendResult {
  return { offset: undefined };
}

export const AppendRowsResponse_AppendResult: MessageFns<AppendRowsResponse_AppendResult> = {
  encode(message: AppendRowsResponse_AppendResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.offset !== undefined) {
      Int64Value.encode({ value: message.offset! }, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppendRowsResponse_AppendResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppendRowsResponse_AppendResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.offset = Int64Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AppendRowsResponse_AppendResult {
    return { offset: isSet(object.offset) ? Long.fromValue(object.offset) : undefined };
  },

  toJSON(message: AppendRowsResponse_AppendResult): unknown {
    const obj: any = {};
    if (message.offset !== undefined) {
      obj.offset = message.offset;
    }
    return obj;
  },

  create(base?: DeepPartial<AppendRowsResponse_AppendResult>): AppendRowsResponse_AppendResult {
    return AppendRowsResponse_AppendResult.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AppendRowsResponse_AppendResult>): AppendRowsResponse_AppendResult {
    const message = createBaseAppendRowsResponse_AppendResult();
    message.offset = (object.offset !== undefined && object.offset !== null)
      ? Long.fromValue(object.offset)
      : undefined;
    return message;
  },
};

function createBaseGetWriteStreamRequest(): GetWriteStreamRequest {
  return { name: "", view: 0 };
}

export const GetWriteStreamRequest: MessageFns<GetWriteStreamRequest> = {
  encode(message: GetWriteStreamRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.view !== 0) {
      writer.uint32(24).int32(message.view);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetWriteStreamRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetWriteStreamRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.view = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetWriteStreamRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      view: isSet(object.view) ? writeStreamViewFromJSON(object.view) : 0,
    };
  },

  toJSON(message: GetWriteStreamRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.view !== 0) {
      obj.view = writeStreamViewToJSON(message.view);
    }
    return obj;
  },

  create(base?: DeepPartial<GetWriteStreamRequest>): GetWriteStreamRequest {
    return GetWriteStreamRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetWriteStreamRequest>): GetWriteStreamRequest {
    const message = createBaseGetWriteStreamRequest();
    message.name = object.name ?? "";
    message.view = object.view ?? 0;
    return message;
  },
};

function createBaseBatchCommitWriteStreamsRequest(): BatchCommitWriteStreamsRequest {
  return { parent: "", writeStreams: [] };
}

export const BatchCommitWriteStreamsRequest: MessageFns<BatchCommitWriteStreamsRequest> = {
  encode(message: BatchCommitWriteStreamsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    for (const v of message.writeStreams) {
      writer.uint32(18).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchCommitWriteStreamsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchCommitWriteStreamsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.writeStreams.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchCommitWriteStreamsRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      writeStreams: globalThis.Array.isArray(object?.writeStreams)
        ? object.writeStreams.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: BatchCommitWriteStreamsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.writeStreams?.length) {
      obj.writeStreams = message.writeStreams;
    }
    return obj;
  },

  create(base?: DeepPartial<BatchCommitWriteStreamsRequest>): BatchCommitWriteStreamsRequest {
    return BatchCommitWriteStreamsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchCommitWriteStreamsRequest>): BatchCommitWriteStreamsRequest {
    const message = createBaseBatchCommitWriteStreamsRequest();
    message.parent = object.parent ?? "";
    message.writeStreams = object.writeStreams?.map((e) => e) || [];
    return message;
  },
};

function createBaseBatchCommitWriteStreamsResponse(): BatchCommitWriteStreamsResponse {
  return { commitTime: undefined, streamErrors: [] };
}

export const BatchCommitWriteStreamsResponse: MessageFns<BatchCommitWriteStreamsResponse> = {
  encode(message: BatchCommitWriteStreamsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.commitTime !== undefined) {
      Timestamp.encode(toTimestamp(message.commitTime), writer.uint32(10).fork()).join();
    }
    for (const v of message.streamErrors) {
      StorageError.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchCommitWriteStreamsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchCommitWriteStreamsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.commitTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.streamErrors.push(StorageError.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchCommitWriteStreamsResponse {
    return {
      commitTime: isSet(object.commitTime) ? fromJsonTimestamp(object.commitTime) : undefined,
      streamErrors: globalThis.Array.isArray(object?.streamErrors)
        ? object.streamErrors.map((e: any) => StorageError.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchCommitWriteStreamsResponse): unknown {
    const obj: any = {};
    if (message.commitTime !== undefined) {
      obj.commitTime = message.commitTime.toISOString();
    }
    if (message.streamErrors?.length) {
      obj.streamErrors = message.streamErrors.map((e) => StorageError.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchCommitWriteStreamsResponse>): BatchCommitWriteStreamsResponse {
    return BatchCommitWriteStreamsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchCommitWriteStreamsResponse>): BatchCommitWriteStreamsResponse {
    const message = createBaseBatchCommitWriteStreamsResponse();
    message.commitTime = object.commitTime ?? undefined;
    message.streamErrors = object.streamErrors?.map((e) => StorageError.fromPartial(e)) || [];
    return message;
  },
};

function createBaseFinalizeWriteStreamRequest(): FinalizeWriteStreamRequest {
  return { name: "" };
}

export const FinalizeWriteStreamRequest: MessageFns<FinalizeWriteStreamRequest> = {
  encode(message: FinalizeWriteStreamRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FinalizeWriteStreamRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFinalizeWriteStreamRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FinalizeWriteStreamRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: FinalizeWriteStreamRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<FinalizeWriteStreamRequest>): FinalizeWriteStreamRequest {
    return FinalizeWriteStreamRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FinalizeWriteStreamRequest>): FinalizeWriteStreamRequest {
    const message = createBaseFinalizeWriteStreamRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseFinalizeWriteStreamResponse(): FinalizeWriteStreamResponse {
  return { rowCount: Long.ZERO };
}

export const FinalizeWriteStreamResponse: MessageFns<FinalizeWriteStreamResponse> = {
  encode(message: FinalizeWriteStreamResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.rowCount.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.rowCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FinalizeWriteStreamResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFinalizeWriteStreamResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.rowCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FinalizeWriteStreamResponse {
    return { rowCount: isSet(object.rowCount) ? Long.fromValue(object.rowCount) : Long.ZERO };
  },

  toJSON(message: FinalizeWriteStreamResponse): unknown {
    const obj: any = {};
    if (!message.rowCount.equals(Long.ZERO)) {
      obj.rowCount = (message.rowCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<FinalizeWriteStreamResponse>): FinalizeWriteStreamResponse {
    return FinalizeWriteStreamResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FinalizeWriteStreamResponse>): FinalizeWriteStreamResponse {
    const message = createBaseFinalizeWriteStreamResponse();
    message.rowCount = (object.rowCount !== undefined && object.rowCount !== null)
      ? Long.fromValue(object.rowCount)
      : Long.ZERO;
    return message;
  },
};

function createBaseFlushRowsRequest(): FlushRowsRequest {
  return { writeStream: "", offset: undefined };
}

export const FlushRowsRequest: MessageFns<FlushRowsRequest> = {
  encode(message: FlushRowsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.writeStream !== "") {
      writer.uint32(10).string(message.writeStream);
    }
    if (message.offset !== undefined) {
      Int64Value.encode({ value: message.offset! }, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FlushRowsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFlushRowsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.writeStream = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.offset = Int64Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FlushRowsRequest {
    return {
      writeStream: isSet(object.writeStream) ? globalThis.String(object.writeStream) : "",
      offset: isSet(object.offset) ? Long.fromValue(object.offset) : undefined,
    };
  },

  toJSON(message: FlushRowsRequest): unknown {
    const obj: any = {};
    if (message.writeStream !== "") {
      obj.writeStream = message.writeStream;
    }
    if (message.offset !== undefined) {
      obj.offset = message.offset;
    }
    return obj;
  },

  create(base?: DeepPartial<FlushRowsRequest>): FlushRowsRequest {
    return FlushRowsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FlushRowsRequest>): FlushRowsRequest {
    const message = createBaseFlushRowsRequest();
    message.writeStream = object.writeStream ?? "";
    message.offset = (object.offset !== undefined && object.offset !== null)
      ? Long.fromValue(object.offset)
      : undefined;
    return message;
  },
};

function createBaseFlushRowsResponse(): FlushRowsResponse {
  return { offset: Long.ZERO };
}

export const FlushRowsResponse: MessageFns<FlushRowsResponse> = {
  encode(message: FlushRowsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.offset.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.offset.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FlushRowsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFlushRowsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.offset = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FlushRowsResponse {
    return { offset: isSet(object.offset) ? Long.fromValue(object.offset) : Long.ZERO };
  },

  toJSON(message: FlushRowsResponse): unknown {
    const obj: any = {};
    if (!message.offset.equals(Long.ZERO)) {
      obj.offset = (message.offset || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<FlushRowsResponse>): FlushRowsResponse {
    return FlushRowsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FlushRowsResponse>): FlushRowsResponse {
    const message = createBaseFlushRowsResponse();
    message.offset = (object.offset !== undefined && object.offset !== null)
      ? Long.fromValue(object.offset)
      : Long.ZERO;
    return message;
  },
};

function createBaseStorageError(): StorageError {
  return { code: 0, entity: "", errorMessage: "" };
}

export const StorageError: MessageFns<StorageError> = {
  encode(message: StorageError, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.code !== 0) {
      writer.uint32(8).int32(message.code);
    }
    if (message.entity !== "") {
      writer.uint32(18).string(message.entity);
    }
    if (message.errorMessage !== "") {
      writer.uint32(26).string(message.errorMessage);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StorageError {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStorageError();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.code = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.entity = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.errorMessage = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StorageError {
    return {
      code: isSet(object.code) ? storageError_StorageErrorCodeFromJSON(object.code) : 0,
      entity: isSet(object.entity) ? globalThis.String(object.entity) : "",
      errorMessage: isSet(object.errorMessage) ? globalThis.String(object.errorMessage) : "",
    };
  },

  toJSON(message: StorageError): unknown {
    const obj: any = {};
    if (message.code !== 0) {
      obj.code = storageError_StorageErrorCodeToJSON(message.code);
    }
    if (message.entity !== "") {
      obj.entity = message.entity;
    }
    if (message.errorMessage !== "") {
      obj.errorMessage = message.errorMessage;
    }
    return obj;
  },

  create(base?: DeepPartial<StorageError>): StorageError {
    return StorageError.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StorageError>): StorageError {
    const message = createBaseStorageError();
    message.code = object.code ?? 0;
    message.entity = object.entity ?? "";
    message.errorMessage = object.errorMessage ?? "";
    return message;
  },
};

function createBaseRowError(): RowError {
  return { index: Long.ZERO, code: 0, message: "" };
}

export const RowError: MessageFns<RowError> = {
  encode(message: RowError, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.index.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.index.toString());
    }
    if (message.code !== 0) {
      writer.uint32(16).int32(message.code);
    }
    if (message.message !== "") {
      writer.uint32(26).string(message.message);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RowError {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRowError();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.index = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.code = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.message = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RowError {
    return {
      index: isSet(object.index) ? Long.fromValue(object.index) : Long.ZERO,
      code: isSet(object.code) ? rowError_RowErrorCodeFromJSON(object.code) : 0,
      message: isSet(object.message) ? globalThis.String(object.message) : "",
    };
  },

  toJSON(message: RowError): unknown {
    const obj: any = {};
    if (!message.index.equals(Long.ZERO)) {
      obj.index = (message.index || Long.ZERO).toString();
    }
    if (message.code !== 0) {
      obj.code = rowError_RowErrorCodeToJSON(message.code);
    }
    if (message.message !== "") {
      obj.message = message.message;
    }
    return obj;
  },

  create(base?: DeepPartial<RowError>): RowError {
    return RowError.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RowError>): RowError {
    const message = createBaseRowError();
    message.index = (object.index !== undefined && object.index !== null) ? Long.fromValue(object.index) : Long.ZERO;
    message.code = object.code ?? 0;
    message.message = object.message ?? "";
    return message;
  },
};

/**
 * BigQuery Read API.
 *
 * The Read API can be used to read data from BigQuery.
 */
export type BigQueryReadDefinition = typeof BigQueryReadDefinition;
export const BigQueryReadDefinition = {
  name: "BigQueryRead",
  fullName: "google.cloud.bigquery.storage.v1.BigQueryRead",
  methods: {
    /**
     * Creates a new read session. A read session divides the contents of a
     * BigQuery table into one or more streams, which can then be used to read
     * data from the table. The read session also specifies properties of the
     * data to be read, such as a list of columns or a push-down filter describing
     * the rows to be returned.
     *
     * A particular row can be read by at most one stream. When the caller has
     * reached the end of each stream in the session, then all the data in the
     * table has been read.
     *
     * Data is assigned to each stream such that roughly the same number of
     * rows can be read from each stream. Because the server-side unit for
     * assigning data is collections of rows, the API does not guarantee that
     * each stream will return the same number or rows. Additionally, the
     * limits are enforced based on the number of pre-filtered rows, so some
     * filters can lead to lopsided assignments.
     *
     * Read sessions automatically expire 6 hours after they are created and do
     * not require manual clean-up by the caller.
     */
    createReadSession: {
      name: "CreateReadSession",
      requestType: CreateReadSessionRequest,
      requestStream: false,
      responseType: ReadSession,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              36,
              112,
              97,
              114,
              101,
              110,
              116,
              44,
              114,
              101,
              97,
              100,
              95,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              44,
              109,
              97,
              120,
              95,
              115,
              116,
              114,
              101,
              97,
              109,
              95,
              99,
              111,
              117,
              110,
              116,
            ]),
          ],
          578365826: [
            Buffer.from([
              60,
              58,
              1,
              42,
              34,
              55,
              47,
              118,
              49,
              47,
              123,
              114,
              101,
              97,
              100,
              95,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              46,
              116,
              97,
              98,
              108,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Reads rows from the stream in the format prescribed by the ReadSession.
     * Each response contains one or more table rows, up to a maximum of 100 MiB
     * per response; read requests which attempt to read individual rows larger
     * than 100 MiB will fail.
     *
     * Each request also returns a set of stream statistics reflecting the current
     * state of the stream.
     */
    readRows: {
      name: "ReadRows",
      requestType: ReadRowsRequest,
      requestStream: false,
      responseType: ReadRowsResponse,
      responseStream: true,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([18, 114, 101, 97, 100, 95, 115, 116, 114, 101, 97, 109, 44, 111, 102, 102, 115, 101, 116]),
          ],
          578365826: [
            Buffer.from([
              63,
              18,
              61,
              47,
              118,
              49,
              47,
              123,
              114,
              101,
              97,
              100,
              95,
              115,
              116,
              114,
              101,
              97,
              109,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Splits a given `ReadStream` into two `ReadStream` objects. These
     * `ReadStream` objects are referred to as the primary and the residual
     * streams of the split. The original `ReadStream` can still be read from in
     * the same manner as before. Both of the returned `ReadStream` objects can
     * also be read from, and the rows returned by both child streams will be
     * the same as the rows read from the original stream.
     *
     * Moreover, the two child streams will be allocated back-to-back in the
     * original `ReadStream`. Concretely, it is guaranteed that for streams
     * original, primary, and residual, that original[0-j] = primary[0-j] and
     * original[j-n] = residual[0-m] once the streams have been read to
     * completion.
     */
    splitReadStream: {
      name: "SplitReadStream",
      requestType: SplitReadStreamRequest,
      requestStream: false,
      responseType: SplitReadStreamResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              56,
              18,
              54,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface BigQueryReadServiceImplementation<CallContextExt = {}> {
  /**
   * Creates a new read session. A read session divides the contents of a
   * BigQuery table into one or more streams, which can then be used to read
   * data from the table. The read session also specifies properties of the
   * data to be read, such as a list of columns or a push-down filter describing
   * the rows to be returned.
   *
   * A particular row can be read by at most one stream. When the caller has
   * reached the end of each stream in the session, then all the data in the
   * table has been read.
   *
   * Data is assigned to each stream such that roughly the same number of
   * rows can be read from each stream. Because the server-side unit for
   * assigning data is collections of rows, the API does not guarantee that
   * each stream will return the same number or rows. Additionally, the
   * limits are enforced based on the number of pre-filtered rows, so some
   * filters can lead to lopsided assignments.
   *
   * Read sessions automatically expire 6 hours after they are created and do
   * not require manual clean-up by the caller.
   */
  createReadSession(
    request: CreateReadSessionRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ReadSession>>;
  /**
   * Reads rows from the stream in the format prescribed by the ReadSession.
   * Each response contains one or more table rows, up to a maximum of 100 MiB
   * per response; read requests which attempt to read individual rows larger
   * than 100 MiB will fail.
   *
   * Each request also returns a set of stream statistics reflecting the current
   * state of the stream.
   */
  readRows(
    request: ReadRowsRequest,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<ReadRowsResponse>>;
  /**
   * Splits a given `ReadStream` into two `ReadStream` objects. These
   * `ReadStream` objects are referred to as the primary and the residual
   * streams of the split. The original `ReadStream` can still be read from in
   * the same manner as before. Both of the returned `ReadStream` objects can
   * also be read from, and the rows returned by both child streams will be
   * the same as the rows read from the original stream.
   *
   * Moreover, the two child streams will be allocated back-to-back in the
   * original `ReadStream`. Concretely, it is guaranteed that for streams
   * original, primary, and residual, that original[0-j] = primary[0-j] and
   * original[j-n] = residual[0-m] once the streams have been read to
   * completion.
   */
  splitReadStream(
    request: SplitReadStreamRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<SplitReadStreamResponse>>;
}

export interface BigQueryReadClient<CallOptionsExt = {}> {
  /**
   * Creates a new read session. A read session divides the contents of a
   * BigQuery table into one or more streams, which can then be used to read
   * data from the table. The read session also specifies properties of the
   * data to be read, such as a list of columns or a push-down filter describing
   * the rows to be returned.
   *
   * A particular row can be read by at most one stream. When the caller has
   * reached the end of each stream in the session, then all the data in the
   * table has been read.
   *
   * Data is assigned to each stream such that roughly the same number of
   * rows can be read from each stream. Because the server-side unit for
   * assigning data is collections of rows, the API does not guarantee that
   * each stream will return the same number or rows. Additionally, the
   * limits are enforced based on the number of pre-filtered rows, so some
   * filters can lead to lopsided assignments.
   *
   * Read sessions automatically expire 6 hours after they are created and do
   * not require manual clean-up by the caller.
   */
  createReadSession(
    request: DeepPartial<CreateReadSessionRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ReadSession>;
  /**
   * Reads rows from the stream in the format prescribed by the ReadSession.
   * Each response contains one or more table rows, up to a maximum of 100 MiB
   * per response; read requests which attempt to read individual rows larger
   * than 100 MiB will fail.
   *
   * Each request also returns a set of stream statistics reflecting the current
   * state of the stream.
   */
  readRows(
    request: DeepPartial<ReadRowsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<ReadRowsResponse>;
  /**
   * Splits a given `ReadStream` into two `ReadStream` objects. These
   * `ReadStream` objects are referred to as the primary and the residual
   * streams of the split. The original `ReadStream` can still be read from in
   * the same manner as before. Both of the returned `ReadStream` objects can
   * also be read from, and the rows returned by both child streams will be
   * the same as the rows read from the original stream.
   *
   * Moreover, the two child streams will be allocated back-to-back in the
   * original `ReadStream`. Concretely, it is guaranteed that for streams
   * original, primary, and residual, that original[0-j] = primary[0-j] and
   * original[j-n] = residual[0-m] once the streams have been read to
   * completion.
   */
  splitReadStream(
    request: DeepPartial<SplitReadStreamRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<SplitReadStreamResponse>;
}

/**
 * BigQuery Write API.
 *
 * The Write API can be used to write data to BigQuery.
 *
 * For supplementary information about the Write API, see:
 * https://cloud.google.com/bigquery/docs/write-api
 */
export type BigQueryWriteDefinition = typeof BigQueryWriteDefinition;
export const BigQueryWriteDefinition = {
  name: "BigQueryWrite",
  fullName: "google.cloud.bigquery.storage.v1.BigQueryWrite",
  methods: {
    /**
     * Creates a write stream to the given table.
     * Additionally, every table has a special stream named '_default'
     * to which data can be written. This stream doesn't need to be created using
     * CreateWriteStream. It is a stream that can be used simultaneously by any
     * number of clients. Data written to this stream is considered committed as
     * soon as an acknowledgement is received.
     */
    createWriteStream: {
      name: "CreateWriteStream",
      requestType: CreateWriteStreamRequest,
      requestStream: false,
      responseType: WriteStream,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              19,
              112,
              97,
              114,
              101,
              110,
              116,
              44,
              119,
              114,
              105,
              116,
              101,
              95,
              115,
              116,
              114,
              101,
              97,
              109,
            ]),
          ],
          578365826: [
            Buffer.from([
              59,
              58,
              12,
              119,
              114,
              105,
              116,
              101,
              95,
              115,
              116,
              114,
              101,
              97,
              109,
              34,
              43,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Appends data to the given stream.
     *
     * If `offset` is specified, the `offset` is checked against the end of
     * stream. The server returns `OUT_OF_RANGE` in `AppendRowsResponse` if an
     * attempt is made to append to an offset beyond the current end of the stream
     * or `ALREADY_EXISTS` if user provides an `offset` that has already been
     * written to. User can retry with adjusted offset within the same RPC
     * connection. If `offset` is not specified, append happens at the end of the
     * stream.
     *
     * The response contains an optional offset at which the append
     * happened.  No offset information will be returned for appends to a
     * default stream.
     *
     * Responses are received in the same order in which requests are sent.
     * There will be one response for each successful inserted request.  Responses
     * may optionally embed error information if the originating AppendRequest was
     * not successfully processed.
     *
     * The specifics of when successfully appended data is made visible to the
     * table are governed by the type of stream:
     *
     * * For COMMITTED streams (which includes the default stream), data is
     * visible immediately upon successful append.
     *
     * * For BUFFERED streams, data is made visible via a subsequent `FlushRows`
     * rpc which advances a cursor to a newer offset in the stream.
     *
     * * For PENDING streams, data is not made visible until the stream itself is
     * finalized (via the `FinalizeWriteStream` rpc), and the stream is explicitly
     * committed via the `BatchCommitWriteStreams` rpc.
     */
    appendRows: {
      name: "AppendRows",
      requestType: AppendRowsRequest,
      requestStream: true,
      responseType: AppendRowsResponse,
      responseStream: true,
      options: {
        _unknownFields: {
          8410: [Buffer.from([12, 119, 114, 105, 116, 101, 95, 115, 116, 114, 101, 97, 109])],
          578365826: [
            Buffer.from([
              64,
              58,
              1,
              42,
              34,
              59,
              47,
              118,
              49,
              47,
              123,
              119,
              114,
              105,
              116,
              101,
              95,
              115,
              116,
              114,
              101,
              97,
              109,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              47,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /** Gets information about a write stream. */
    getWriteStream: {
      name: "GetWriteStream",
      requestType: GetWriteStreamRequest,
      requestStream: false,
      responseType: WriteStream,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              56,
              58,
              1,
              42,
              34,
              51,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              47,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Finalize a write stream so that no new data can be appended to the
     * stream. Finalize is not supported on the '_default' stream.
     */
    finalizeWriteStream: {
      name: "FinalizeWriteStream",
      requestType: FinalizeWriteStreamRequest,
      requestStream: false,
      responseType: FinalizeWriteStreamResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              56,
              58,
              1,
              42,
              34,
              51,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              47,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Atomically commits a group of `PENDING` streams that belong to the same
     * `parent` table.
     *
     * Streams must be finalized before commit and cannot be committed multiple
     * times. Once a stream is committed, data in the stream becomes available
     * for read operations.
     */
    batchCommitWriteStreams: {
      name: "BatchCommitWriteStreams",
      requestType: BatchCommitWriteStreamsRequest,
      requestStream: false,
      responseType: BatchCommitWriteStreamsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([6, 112, 97, 114, 101, 110, 116])],
          578365826: [
            Buffer.from([
              45,
              18,
              43,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Flushes rows to a BUFFERED stream.
     *
     * If users are appending rows to BUFFERED stream, flush operation is
     * required in order for the rows to become available for reading. A
     * Flush operation flushes up to any previously flushed offset in a BUFFERED
     * stream, to the offset specified in the request.
     *
     * Flush is not supported on the _default stream, since it is not BUFFERED.
     */
    flushRows: {
      name: "FlushRows",
      requestType: FlushRowsRequest,
      requestStream: false,
      responseType: FlushRowsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([12, 119, 114, 105, 116, 101, 95, 115, 116, 114, 101, 97, 109])],
          578365826: [
            Buffer.from([
              64,
              58,
              1,
              42,
              34,
              59,
              47,
              118,
              49,
              47,
              123,
              119,
              114,
              105,
              116,
              101,
              95,
              115,
              116,
              114,
              101,
              97,
              109,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              47,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface BigQueryWriteServiceImplementation<CallContextExt = {}> {
  /**
   * Creates a write stream to the given table.
   * Additionally, every table has a special stream named '_default'
   * to which data can be written. This stream doesn't need to be created using
   * CreateWriteStream. It is a stream that can be used simultaneously by any
   * number of clients. Data written to this stream is considered committed as
   * soon as an acknowledgement is received.
   */
  createWriteStream(
    request: CreateWriteStreamRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<WriteStream>>;
  /**
   * Appends data to the given stream.
   *
   * If `offset` is specified, the `offset` is checked against the end of
   * stream. The server returns `OUT_OF_RANGE` in `AppendRowsResponse` if an
   * attempt is made to append to an offset beyond the current end of the stream
   * or `ALREADY_EXISTS` if user provides an `offset` that has already been
   * written to. User can retry with adjusted offset within the same RPC
   * connection. If `offset` is not specified, append happens at the end of the
   * stream.
   *
   * The response contains an optional offset at which the append
   * happened.  No offset information will be returned for appends to a
   * default stream.
   *
   * Responses are received in the same order in which requests are sent.
   * There will be one response for each successful inserted request.  Responses
   * may optionally embed error information if the originating AppendRequest was
   * not successfully processed.
   *
   * The specifics of when successfully appended data is made visible to the
   * table are governed by the type of stream:
   *
   * * For COMMITTED streams (which includes the default stream), data is
   * visible immediately upon successful append.
   *
   * * For BUFFERED streams, data is made visible via a subsequent `FlushRows`
   * rpc which advances a cursor to a newer offset in the stream.
   *
   * * For PENDING streams, data is not made visible until the stream itself is
   * finalized (via the `FinalizeWriteStream` rpc), and the stream is explicitly
   * committed via the `BatchCommitWriteStreams` rpc.
   */
  appendRows(
    request: AsyncIterable<AppendRowsRequest>,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<AppendRowsResponse>>;
  /** Gets information about a write stream. */
  getWriteStream(
    request: GetWriteStreamRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<WriteStream>>;
  /**
   * Finalize a write stream so that no new data can be appended to the
   * stream. Finalize is not supported on the '_default' stream.
   */
  finalizeWriteStream(
    request: FinalizeWriteStreamRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<FinalizeWriteStreamResponse>>;
  /**
   * Atomically commits a group of `PENDING` streams that belong to the same
   * `parent` table.
   *
   * Streams must be finalized before commit and cannot be committed multiple
   * times. Once a stream is committed, data in the stream becomes available
   * for read operations.
   */
  batchCommitWriteStreams(
    request: BatchCommitWriteStreamsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<BatchCommitWriteStreamsResponse>>;
  /**
   * Flushes rows to a BUFFERED stream.
   *
   * If users are appending rows to BUFFERED stream, flush operation is
   * required in order for the rows to become available for reading. A
   * Flush operation flushes up to any previously flushed offset in a BUFFERED
   * stream, to the offset specified in the request.
   *
   * Flush is not supported on the _default stream, since it is not BUFFERED.
   */
  flushRows(request: FlushRowsRequest, context: CallContext & CallContextExt): Promise<DeepPartial<FlushRowsResponse>>;
}

export interface BigQueryWriteClient<CallOptionsExt = {}> {
  /**
   * Creates a write stream to the given table.
   * Additionally, every table has a special stream named '_default'
   * to which data can be written. This stream doesn't need to be created using
   * CreateWriteStream. It is a stream that can be used simultaneously by any
   * number of clients. Data written to this stream is considered committed as
   * soon as an acknowledgement is received.
   */
  createWriteStream(
    request: DeepPartial<CreateWriteStreamRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<WriteStream>;
  /**
   * Appends data to the given stream.
   *
   * If `offset` is specified, the `offset` is checked against the end of
   * stream. The server returns `OUT_OF_RANGE` in `AppendRowsResponse` if an
   * attempt is made to append to an offset beyond the current end of the stream
   * or `ALREADY_EXISTS` if user provides an `offset` that has already been
   * written to. User can retry with adjusted offset within the same RPC
   * connection. If `offset` is not specified, append happens at the end of the
   * stream.
   *
   * The response contains an optional offset at which the append
   * happened.  No offset information will be returned for appends to a
   * default stream.
   *
   * Responses are received in the same order in which requests are sent.
   * There will be one response for each successful inserted request.  Responses
   * may optionally embed error information if the originating AppendRequest was
   * not successfully processed.
   *
   * The specifics of when successfully appended data is made visible to the
   * table are governed by the type of stream:
   *
   * * For COMMITTED streams (which includes the default stream), data is
   * visible immediately upon successful append.
   *
   * * For BUFFERED streams, data is made visible via a subsequent `FlushRows`
   * rpc which advances a cursor to a newer offset in the stream.
   *
   * * For PENDING streams, data is not made visible until the stream itself is
   * finalized (via the `FinalizeWriteStream` rpc), and the stream is explicitly
   * committed via the `BatchCommitWriteStreams` rpc.
   */
  appendRows(
    request: AsyncIterable<DeepPartial<AppendRowsRequest>>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<AppendRowsResponse>;
  /** Gets information about a write stream. */
  getWriteStream(
    request: DeepPartial<GetWriteStreamRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<WriteStream>;
  /**
   * Finalize a write stream so that no new data can be appended to the
   * stream. Finalize is not supported on the '_default' stream.
   */
  finalizeWriteStream(
    request: DeepPartial<FinalizeWriteStreamRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<FinalizeWriteStreamResponse>;
  /**
   * Atomically commits a group of `PENDING` streams that belong to the same
   * `parent` table.
   *
   * Streams must be finalized before commit and cannot be committed multiple
   * times. Once a stream is committed, data in the stream becomes available
   * for read operations.
   */
  batchCommitWriteStreams(
    request: DeepPartial<BatchCommitWriteStreamsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<BatchCommitWriteStreamsResponse>;
  /**
   * Flushes rows to a BUFFERED stream.
   *
   * If users are appending rows to BUFFERED stream, flush operation is
   * required in order for the rows to become available for reading. A
   * Flush operation flushes up to any previously flushed offset in a BUFFERED
   * stream, to the offset specified in the request.
   *
   * Flush is not supported on the _default stream, since it is not BUFFERED.
   */
  flushRows(request: DeepPartial<FlushRowsRequest>, options?: CallOptions & CallOptionsExt): Promise<FlushRowsResponse>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export type ServerStreamingMethodResult<Response> = { [Symbol.asyncIterator](): AsyncIterator<Response, void> };

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
