// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/bigquery/storage/v1/stream.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Timestamp } from "../../../../protobuf/timestamp.js";
import { ArrowSchema, ArrowSerializationOptions } from "./arrow.js";
import { AvroSchema, AvroSerializationOptions } from "./avro.js";
import { TableSchema } from "./table.js";

export const protobufPackage = "google.cloud.bigquery.storage.v1";

/** Data format for input or output data. */
export enum DataFormat {
  /** DATA_FORMAT_UNSPECIFIED - Data format is unspecified. */
  DATA_FORMAT_UNSPECIFIED = 0,
  /**
   * AVRO - Avro is a standard open source row based file format.
   * See https://avro.apache.org/ for more details.
   */
  AVRO = 1,
  /**
   * ARROW - Arrow is a standard open source column-based message format.
   * See https://arrow.apache.org/ for more details.
   */
  ARROW = 2,
  UNRECOGNIZED = -1,
}

export function dataFormatFromJSON(object: any): DataFormat {
  switch (object) {
    case 0:
    case "DATA_FORMAT_UNSPECIFIED":
      return DataFormat.DATA_FORMAT_UNSPECIFIED;
    case 1:
    case "AVRO":
      return DataFormat.AVRO;
    case 2:
    case "ARROW":
      return DataFormat.ARROW;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DataFormat.UNRECOGNIZED;
  }
}

export function dataFormatToJSON(object: DataFormat): string {
  switch (object) {
    case DataFormat.DATA_FORMAT_UNSPECIFIED:
      return "DATA_FORMAT_UNSPECIFIED";
    case DataFormat.AVRO:
      return "AVRO";
    case DataFormat.ARROW:
      return "ARROW";
    case DataFormat.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * WriteStreamView is a view enum that controls what details about a write
 * stream should be returned.
 */
export enum WriteStreamView {
  /** WRITE_STREAM_VIEW_UNSPECIFIED - The default / unset value. */
  WRITE_STREAM_VIEW_UNSPECIFIED = 0,
  /**
   * BASIC - The BASIC projection returns basic metadata about a write stream.  The
   * basic view does not include schema information.  This is the default view
   * returned by GetWriteStream.
   */
  BASIC = 1,
  /**
   * FULL - The FULL projection returns all available write stream metadata, including
   * the schema.  CreateWriteStream returns the full projection of write stream
   * metadata.
   */
  FULL = 2,
  UNRECOGNIZED = -1,
}

export function writeStreamViewFromJSON(object: any): WriteStreamView {
  switch (object) {
    case 0:
    case "WRITE_STREAM_VIEW_UNSPECIFIED":
      return WriteStreamView.WRITE_STREAM_VIEW_UNSPECIFIED;
    case 1:
    case "BASIC":
      return WriteStreamView.BASIC;
    case 2:
    case "FULL":
      return WriteStreamView.FULL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return WriteStreamView.UNRECOGNIZED;
  }
}

export function writeStreamViewToJSON(object: WriteStreamView): string {
  switch (object) {
    case WriteStreamView.WRITE_STREAM_VIEW_UNSPECIFIED:
      return "WRITE_STREAM_VIEW_UNSPECIFIED";
    case WriteStreamView.BASIC:
      return "BASIC";
    case WriteStreamView.FULL:
      return "FULL";
    case WriteStreamView.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Information about the ReadSession. */
export interface ReadSession {
  /**
   * Output only. Unique identifier for the session, in the form
   * `projects/{project_id}/locations/{location}/sessions/{session_id}`.
   */
  name: string;
  /**
   * Output only. Time at which the session becomes invalid. After this time,
   * subsequent requests to read this Session will return errors. The
   * expire_time is automatically assigned and currently cannot be specified or
   * updated.
   */
  expireTime:
    | Date
    | undefined;
  /**
   * Immutable. Data format of the output data. DATA_FORMAT_UNSPECIFIED not
   * supported.
   */
  dataFormat: DataFormat;
  /** Output only. Avro schema. */
  avroSchema?:
    | AvroSchema
    | undefined;
  /** Output only. Arrow schema. */
  arrowSchema?:
    | ArrowSchema
    | undefined;
  /**
   * Immutable. Table that this ReadSession is reading from, in the form
   * `projects/{project_id}/datasets/{dataset_id}/tables/{table_id}`
   */
  table: string;
  /**
   * Optional. Any modifiers which are applied when reading from the specified
   * table.
   */
  tableModifiers:
    | ReadSession_TableModifiers
    | undefined;
  /** Optional. Read options for this session (e.g. column selection, filters). */
  readOptions:
    | ReadSession_TableReadOptions
    | undefined;
  /**
   * Output only. A list of streams created with the session.
   *
   * At least one stream is created with the session. In the future, larger
   * request_stream_count values *may* result in this list being unpopulated,
   * in that case, the user will need to use a List method to get the streams
   * instead, which is not yet available.
   */
  streams: ReadStream[];
  /**
   * Output only. An estimate on the number of bytes this session will scan when
   * all streams are completely consumed. This estimate is based on
   * metadata from the table which might be incomplete or stale.
   */
  estimatedTotalBytesScanned: Long;
  /**
   * Output only. A pre-projected estimate of the total physical size of files
   * (in bytes) that this session will scan when all streams are consumed. This
   * estimate is independent of the selected columns and can be based on
   * incomplete or stale metadata from the table.  This field is only set for
   * BigLake tables.
   */
  estimatedTotalPhysicalFileSize: Long;
  /**
   * Output only. An estimate on the number of rows present in this session's
   * streams. This estimate is based on metadata from the table which might be
   * incomplete or stale.
   */
  estimatedRowCount: Long;
  /**
   * Optional. ID set by client to annotate a session identity.  This does not
   * need to be strictly unique, but instead the same ID should be used to group
   * logically connected sessions (e.g. All using the same ID for all sessions
   * needed to complete a Spark SQL query is reasonable).
   *
   * Maximum length is 256 bytes.
   */
  traceId: string;
}

/** Additional attributes when reading a table. */
export interface ReadSession_TableModifiers {
  /** The snapshot time of the table. If not set, interpreted as now. */
  snapshotTime: Date | undefined;
}

/** Options dictating how we read a table. */
export interface ReadSession_TableReadOptions {
  /**
   * Optional. The names of the fields in the table to be returned. If no
   * field names are specified, then all fields in the table are returned.
   *
   * Nested fields -- the child elements of a STRUCT field -- can be selected
   * individually using their fully-qualified names, and will be returned as
   * record fields containing only the selected nested fields. If a STRUCT
   * field is specified in the selected fields list, all of the child elements
   * will be returned.
   *
   * As an example, consider a table with the following schema:
   *
   *   {
   *       "name": "struct_field",
   *       "type": "RECORD",
   *       "mode": "NULLABLE",
   *       "fields": [
   *           {
   *               "name": "string_field1",
   *               "type": "STRING",
   * .              "mode": "NULLABLE"
   *           },
   *           {
   *               "name": "string_field2",
   *               "type": "STRING",
   *               "mode": "NULLABLE"
   *           }
   *       ]
   *   }
   *
   * Specifying "struct_field" in the selected fields list will result in a
   * read session schema with the following logical structure:
   *
   *   struct_field {
   *       string_field1
   *       string_field2
   *   }
   *
   * Specifying "struct_field.string_field1" in the selected fields list will
   * result in a read session schema with the following logical structure:
   *
   *   struct_field {
   *       string_field1
   *   }
   *
   * The order of the fields in the read session schema is derived from the
   * table schema and does not correspond to the order in which the fields are
   * specified in this list.
   */
  selectedFields: string[];
  /**
   * SQL text filtering statement, similar to a WHERE clause in a query.
   * Aggregates are not supported.
   *
   * Examples: "int_field > 5"
   *           "date_field = CAST('2014-9-27' as DATE)"
   *           "nullable_field is not NULL"
   *           "st_equals(geo_field, st_geofromtext("POINT(2, 2)"))"
   *           "numeric_field BETWEEN 1.0 AND 5.0"
   *
   * Restricted to a maximum length for 1 MB.
   */
  rowRestriction: string;
  /** Optional. Options specific to the Apache Arrow output format. */
  arrowSerializationOptions?:
    | ArrowSerializationOptions
    | undefined;
  /** Optional. Options specific to the Apache Avro output format */
  avroSerializationOptions?:
    | AvroSerializationOptions
    | undefined;
  /**
   * Optional. Specifies a table sampling percentage. Specifically, the query
   * planner will use TABLESAMPLE SYSTEM (sample_percentage PERCENT). The
   * sampling percentage is applied at the data block granularity. It will
   * randomly choose for each data block whether to read the rows in that data
   * block. For more details, see
   * https://cloud.google.com/bigquery/docs/table-sampling)
   */
  samplePercentage?:
    | number
    | undefined;
  /**
   * Optional. Set response_compression_codec when creating a read session to
   * enable application-level compression of ReadRows responses.
   */
  responseCompressionCodec?: ReadSession_TableReadOptions_ResponseCompressionCodec | undefined;
}

/**
 * Specifies which compression codec to attempt on the entire serialized
 * response payload (either Arrow record batch or Avro rows). This is
 * not to be confused with the Apache Arrow native compression codecs
 * specified in ArrowSerializationOptions. For performance reasons, when
 * creating a read session requesting Arrow responses, setting both native
 * Arrow compression and application-level response compression will not be
 * allowed - choose, at most, one kind of compression.
 */
export enum ReadSession_TableReadOptions_ResponseCompressionCodec {
  /** RESPONSE_COMPRESSION_CODEC_UNSPECIFIED - Default is no compression. */
  RESPONSE_COMPRESSION_CODEC_UNSPECIFIED = 0,
  /** RESPONSE_COMPRESSION_CODEC_LZ4 - Use raw LZ4 compression. */
  RESPONSE_COMPRESSION_CODEC_LZ4 = 2,
  UNRECOGNIZED = -1,
}

export function readSession_TableReadOptions_ResponseCompressionCodecFromJSON(
  object: any,
): ReadSession_TableReadOptions_ResponseCompressionCodec {
  switch (object) {
    case 0:
    case "RESPONSE_COMPRESSION_CODEC_UNSPECIFIED":
      return ReadSession_TableReadOptions_ResponseCompressionCodec.RESPONSE_COMPRESSION_CODEC_UNSPECIFIED;
    case 2:
    case "RESPONSE_COMPRESSION_CODEC_LZ4":
      return ReadSession_TableReadOptions_ResponseCompressionCodec.RESPONSE_COMPRESSION_CODEC_LZ4;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ReadSession_TableReadOptions_ResponseCompressionCodec.UNRECOGNIZED;
  }
}

export function readSession_TableReadOptions_ResponseCompressionCodecToJSON(
  object: ReadSession_TableReadOptions_ResponseCompressionCodec,
): string {
  switch (object) {
    case ReadSession_TableReadOptions_ResponseCompressionCodec.RESPONSE_COMPRESSION_CODEC_UNSPECIFIED:
      return "RESPONSE_COMPRESSION_CODEC_UNSPECIFIED";
    case ReadSession_TableReadOptions_ResponseCompressionCodec.RESPONSE_COMPRESSION_CODEC_LZ4:
      return "RESPONSE_COMPRESSION_CODEC_LZ4";
    case ReadSession_TableReadOptions_ResponseCompressionCodec.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Information about a single stream that gets data out of the storage system.
 * Most of the information about `ReadStream` instances is aggregated, making
 * `ReadStream` lightweight.
 */
export interface ReadStream {
  /**
   * Output only. Name of the stream, in the form
   * `projects/{project_id}/locations/{location}/sessions/{session_id}/streams/{stream_id}`.
   */
  name: string;
}

/** Information about a single stream that gets data inside the storage system. */
export interface WriteStream {
  /**
   * Output only. Name of the stream, in the form
   * `projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}`.
   */
  name: string;
  /** Immutable. Type of the stream. */
  type: WriteStream_Type;
  /**
   * Output only. Create time of the stream. For the _default stream, this is
   * the creation_time of the table.
   */
  createTime:
    | Date
    | undefined;
  /**
   * Output only. Commit time of the stream.
   * If a stream is of `COMMITTED` type, then it will have a commit_time same as
   * `create_time`. If the stream is of `PENDING` type, empty commit_time
   * means it is not committed.
   */
  commitTime:
    | Date
    | undefined;
  /**
   * Output only. The schema of the destination table. It is only returned in
   * `CreateWriteStream` response. Caller should generate data that's
   * compatible with this schema to send in initial `AppendRowsRequest`.
   * The table schema could go out of date during the life time of the stream.
   */
  tableSchema:
    | TableSchema
    | undefined;
  /** Immutable. Mode of the stream. */
  writeMode: WriteStream_WriteMode;
  /**
   * Immutable. The geographic location where the stream's dataset resides. See
   * https://cloud.google.com/bigquery/docs/locations for supported
   * locations.
   */
  location: string;
}

/** Type enum of the stream. */
export enum WriteStream_Type {
  /** TYPE_UNSPECIFIED - Unknown type. */
  TYPE_UNSPECIFIED = 0,
  /**
   * COMMITTED - Data will commit automatically and appear as soon as the write is
   * acknowledged.
   */
  COMMITTED = 1,
  /** PENDING - Data is invisible until the stream is committed. */
  PENDING = 2,
  /** BUFFERED - Data is only visible up to the offset to which it was flushed. */
  BUFFERED = 3,
  UNRECOGNIZED = -1,
}

export function writeStream_TypeFromJSON(object: any): WriteStream_Type {
  switch (object) {
    case 0:
    case "TYPE_UNSPECIFIED":
      return WriteStream_Type.TYPE_UNSPECIFIED;
    case 1:
    case "COMMITTED":
      return WriteStream_Type.COMMITTED;
    case 2:
    case "PENDING":
      return WriteStream_Type.PENDING;
    case 3:
    case "BUFFERED":
      return WriteStream_Type.BUFFERED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return WriteStream_Type.UNRECOGNIZED;
  }
}

export function writeStream_TypeToJSON(object: WriteStream_Type): string {
  switch (object) {
    case WriteStream_Type.TYPE_UNSPECIFIED:
      return "TYPE_UNSPECIFIED";
    case WriteStream_Type.COMMITTED:
      return "COMMITTED";
    case WriteStream_Type.PENDING:
      return "PENDING";
    case WriteStream_Type.BUFFERED:
      return "BUFFERED";
    case WriteStream_Type.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Mode enum of the stream. */
export enum WriteStream_WriteMode {
  /** WRITE_MODE_UNSPECIFIED - Unknown type. */
  WRITE_MODE_UNSPECIFIED = 0,
  /**
   * INSERT - Insert new records into the table.
   * It is the default value if customers do not specify it.
   */
  INSERT = 1,
  UNRECOGNIZED = -1,
}

export function writeStream_WriteModeFromJSON(object: any): WriteStream_WriteMode {
  switch (object) {
    case 0:
    case "WRITE_MODE_UNSPECIFIED":
      return WriteStream_WriteMode.WRITE_MODE_UNSPECIFIED;
    case 1:
    case "INSERT":
      return WriteStream_WriteMode.INSERT;
    case -1:
    case "UNRECOGNIZED":
    default:
      return WriteStream_WriteMode.UNRECOGNIZED;
  }
}

export function writeStream_WriteModeToJSON(object: WriteStream_WriteMode): string {
  switch (object) {
    case WriteStream_WriteMode.WRITE_MODE_UNSPECIFIED:
      return "WRITE_MODE_UNSPECIFIED";
    case WriteStream_WriteMode.INSERT:
      return "INSERT";
    case WriteStream_WriteMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseReadSession(): ReadSession {
  return {
    name: "",
    expireTime: undefined,
    dataFormat: 0,
    avroSchema: undefined,
    arrowSchema: undefined,
    table: "",
    tableModifiers: undefined,
    readOptions: undefined,
    streams: [],
    estimatedTotalBytesScanned: Long.ZERO,
    estimatedTotalPhysicalFileSize: Long.ZERO,
    estimatedRowCount: Long.ZERO,
    traceId: "",
  };
}

export const ReadSession: MessageFns<ReadSession> = {
  encode(message: ReadSession, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.expireTime !== undefined) {
      Timestamp.encode(toTimestamp(message.expireTime), writer.uint32(18).fork()).join();
    }
    if (message.dataFormat !== 0) {
      writer.uint32(24).int32(message.dataFormat);
    }
    if (message.avroSchema !== undefined) {
      AvroSchema.encode(message.avroSchema, writer.uint32(34).fork()).join();
    }
    if (message.arrowSchema !== undefined) {
      ArrowSchema.encode(message.arrowSchema, writer.uint32(42).fork()).join();
    }
    if (message.table !== "") {
      writer.uint32(50).string(message.table);
    }
    if (message.tableModifiers !== undefined) {
      ReadSession_TableModifiers.encode(message.tableModifiers, writer.uint32(58).fork()).join();
    }
    if (message.readOptions !== undefined) {
      ReadSession_TableReadOptions.encode(message.readOptions, writer.uint32(66).fork()).join();
    }
    for (const v of message.streams) {
      ReadStream.encode(v!, writer.uint32(82).fork()).join();
    }
    if (!message.estimatedTotalBytesScanned.equals(Long.ZERO)) {
      writer.uint32(96).int64(message.estimatedTotalBytesScanned.toString());
    }
    if (!message.estimatedTotalPhysicalFileSize.equals(Long.ZERO)) {
      writer.uint32(120).int64(message.estimatedTotalPhysicalFileSize.toString());
    }
    if (!message.estimatedRowCount.equals(Long.ZERO)) {
      writer.uint32(112).int64(message.estimatedRowCount.toString());
    }
    if (message.traceId !== "") {
      writer.uint32(106).string(message.traceId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReadSession {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReadSession();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.expireTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.dataFormat = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.avroSchema = AvroSchema.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.arrowSchema = ArrowSchema.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.table = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.tableModifiers = ReadSession_TableModifiers.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.readOptions = ReadSession_TableReadOptions.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.streams.push(ReadStream.decode(reader, reader.uint32()));
          continue;
        case 12:
          if (tag !== 96) {
            break;
          }

          message.estimatedTotalBytesScanned = Long.fromString(reader.int64().toString());
          continue;
        case 15:
          if (tag !== 120) {
            break;
          }

          message.estimatedTotalPhysicalFileSize = Long.fromString(reader.int64().toString());
          continue;
        case 14:
          if (tag !== 112) {
            break;
          }

          message.estimatedRowCount = Long.fromString(reader.int64().toString());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.traceId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReadSession {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      expireTime: isSet(object.expireTime) ? fromJsonTimestamp(object.expireTime) : undefined,
      dataFormat: isSet(object.dataFormat) ? dataFormatFromJSON(object.dataFormat) : 0,
      avroSchema: isSet(object.avroSchema) ? AvroSchema.fromJSON(object.avroSchema) : undefined,
      arrowSchema: isSet(object.arrowSchema) ? ArrowSchema.fromJSON(object.arrowSchema) : undefined,
      table: isSet(object.table) ? globalThis.String(object.table) : "",
      tableModifiers: isSet(object.tableModifiers)
        ? ReadSession_TableModifiers.fromJSON(object.tableModifiers)
        : undefined,
      readOptions: isSet(object.readOptions) ? ReadSession_TableReadOptions.fromJSON(object.readOptions) : undefined,
      streams: globalThis.Array.isArray(object?.streams) ? object.streams.map((e: any) => ReadStream.fromJSON(e)) : [],
      estimatedTotalBytesScanned: isSet(object.estimatedTotalBytesScanned)
        ? Long.fromValue(object.estimatedTotalBytesScanned)
        : Long.ZERO,
      estimatedTotalPhysicalFileSize: isSet(object.estimatedTotalPhysicalFileSize)
        ? Long.fromValue(object.estimatedTotalPhysicalFileSize)
        : Long.ZERO,
      estimatedRowCount: isSet(object.estimatedRowCount) ? Long.fromValue(object.estimatedRowCount) : Long.ZERO,
      traceId: isSet(object.traceId) ? globalThis.String(object.traceId) : "",
    };
  },

  toJSON(message: ReadSession): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.expireTime !== undefined) {
      obj.expireTime = message.expireTime.toISOString();
    }
    if (message.dataFormat !== 0) {
      obj.dataFormat = dataFormatToJSON(message.dataFormat);
    }
    if (message.avroSchema !== undefined) {
      obj.avroSchema = AvroSchema.toJSON(message.avroSchema);
    }
    if (message.arrowSchema !== undefined) {
      obj.arrowSchema = ArrowSchema.toJSON(message.arrowSchema);
    }
    if (message.table !== "") {
      obj.table = message.table;
    }
    if (message.tableModifiers !== undefined) {
      obj.tableModifiers = ReadSession_TableModifiers.toJSON(message.tableModifiers);
    }
    if (message.readOptions !== undefined) {
      obj.readOptions = ReadSession_TableReadOptions.toJSON(message.readOptions);
    }
    if (message.streams?.length) {
      obj.streams = message.streams.map((e) => ReadStream.toJSON(e));
    }
    if (!message.estimatedTotalBytesScanned.equals(Long.ZERO)) {
      obj.estimatedTotalBytesScanned = (message.estimatedTotalBytesScanned || Long.ZERO).toString();
    }
    if (!message.estimatedTotalPhysicalFileSize.equals(Long.ZERO)) {
      obj.estimatedTotalPhysicalFileSize = (message.estimatedTotalPhysicalFileSize || Long.ZERO).toString();
    }
    if (!message.estimatedRowCount.equals(Long.ZERO)) {
      obj.estimatedRowCount = (message.estimatedRowCount || Long.ZERO).toString();
    }
    if (message.traceId !== "") {
      obj.traceId = message.traceId;
    }
    return obj;
  },

  create(base?: DeepPartial<ReadSession>): ReadSession {
    return ReadSession.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReadSession>): ReadSession {
    const message = createBaseReadSession();
    message.name = object.name ?? "";
    message.expireTime = object.expireTime ?? undefined;
    message.dataFormat = object.dataFormat ?? 0;
    message.avroSchema = (object.avroSchema !== undefined && object.avroSchema !== null)
      ? AvroSchema.fromPartial(object.avroSchema)
      : undefined;
    message.arrowSchema = (object.arrowSchema !== undefined && object.arrowSchema !== null)
      ? ArrowSchema.fromPartial(object.arrowSchema)
      : undefined;
    message.table = object.table ?? "";
    message.tableModifiers = (object.tableModifiers !== undefined && object.tableModifiers !== null)
      ? ReadSession_TableModifiers.fromPartial(object.tableModifiers)
      : undefined;
    message.readOptions = (object.readOptions !== undefined && object.readOptions !== null)
      ? ReadSession_TableReadOptions.fromPartial(object.readOptions)
      : undefined;
    message.streams = object.streams?.map((e) => ReadStream.fromPartial(e)) || [];
    message.estimatedTotalBytesScanned =
      (object.estimatedTotalBytesScanned !== undefined && object.estimatedTotalBytesScanned !== null)
        ? Long.fromValue(object.estimatedTotalBytesScanned)
        : Long.ZERO;
    message.estimatedTotalPhysicalFileSize =
      (object.estimatedTotalPhysicalFileSize !== undefined && object.estimatedTotalPhysicalFileSize !== null)
        ? Long.fromValue(object.estimatedTotalPhysicalFileSize)
        : Long.ZERO;
    message.estimatedRowCount = (object.estimatedRowCount !== undefined && object.estimatedRowCount !== null)
      ? Long.fromValue(object.estimatedRowCount)
      : Long.ZERO;
    message.traceId = object.traceId ?? "";
    return message;
  },
};

function createBaseReadSession_TableModifiers(): ReadSession_TableModifiers {
  return { snapshotTime: undefined };
}

export const ReadSession_TableModifiers: MessageFns<ReadSession_TableModifiers> = {
  encode(message: ReadSession_TableModifiers, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.snapshotTime !== undefined) {
      Timestamp.encode(toTimestamp(message.snapshotTime), writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReadSession_TableModifiers {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReadSession_TableModifiers();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.snapshotTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReadSession_TableModifiers {
    return { snapshotTime: isSet(object.snapshotTime) ? fromJsonTimestamp(object.snapshotTime) : undefined };
  },

  toJSON(message: ReadSession_TableModifiers): unknown {
    const obj: any = {};
    if (message.snapshotTime !== undefined) {
      obj.snapshotTime = message.snapshotTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<ReadSession_TableModifiers>): ReadSession_TableModifiers {
    return ReadSession_TableModifiers.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReadSession_TableModifiers>): ReadSession_TableModifiers {
    const message = createBaseReadSession_TableModifiers();
    message.snapshotTime = object.snapshotTime ?? undefined;
    return message;
  },
};

function createBaseReadSession_TableReadOptions(): ReadSession_TableReadOptions {
  return {
    selectedFields: [],
    rowRestriction: "",
    arrowSerializationOptions: undefined,
    avroSerializationOptions: undefined,
    samplePercentage: undefined,
    responseCompressionCodec: undefined,
  };
}

export const ReadSession_TableReadOptions: MessageFns<ReadSession_TableReadOptions> = {
  encode(message: ReadSession_TableReadOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.selectedFields) {
      writer.uint32(10).string(v!);
    }
    if (message.rowRestriction !== "") {
      writer.uint32(18).string(message.rowRestriction);
    }
    if (message.arrowSerializationOptions !== undefined) {
      ArrowSerializationOptions.encode(message.arrowSerializationOptions, writer.uint32(26).fork()).join();
    }
    if (message.avroSerializationOptions !== undefined) {
      AvroSerializationOptions.encode(message.avroSerializationOptions, writer.uint32(34).fork()).join();
    }
    if (message.samplePercentage !== undefined) {
      writer.uint32(41).double(message.samplePercentage);
    }
    if (message.responseCompressionCodec !== undefined) {
      writer.uint32(48).int32(message.responseCompressionCodec);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReadSession_TableReadOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReadSession_TableReadOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.selectedFields.push(reader.string());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.rowRestriction = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.arrowSerializationOptions = ArrowSerializationOptions.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.avroSerializationOptions = AvroSerializationOptions.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 41) {
            break;
          }

          message.samplePercentage = reader.double();
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.responseCompressionCodec = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReadSession_TableReadOptions {
    return {
      selectedFields: globalThis.Array.isArray(object?.selectedFields)
        ? object.selectedFields.map((e: any) => globalThis.String(e))
        : [],
      rowRestriction: isSet(object.rowRestriction) ? globalThis.String(object.rowRestriction) : "",
      arrowSerializationOptions: isSet(object.arrowSerializationOptions)
        ? ArrowSerializationOptions.fromJSON(object.arrowSerializationOptions)
        : undefined,
      avroSerializationOptions: isSet(object.avroSerializationOptions)
        ? AvroSerializationOptions.fromJSON(object.avroSerializationOptions)
        : undefined,
      samplePercentage: isSet(object.samplePercentage) ? globalThis.Number(object.samplePercentage) : undefined,
      responseCompressionCodec: isSet(object.responseCompressionCodec)
        ? readSession_TableReadOptions_ResponseCompressionCodecFromJSON(object.responseCompressionCodec)
        : undefined,
    };
  },

  toJSON(message: ReadSession_TableReadOptions): unknown {
    const obj: any = {};
    if (message.selectedFields?.length) {
      obj.selectedFields = message.selectedFields;
    }
    if (message.rowRestriction !== "") {
      obj.rowRestriction = message.rowRestriction;
    }
    if (message.arrowSerializationOptions !== undefined) {
      obj.arrowSerializationOptions = ArrowSerializationOptions.toJSON(message.arrowSerializationOptions);
    }
    if (message.avroSerializationOptions !== undefined) {
      obj.avroSerializationOptions = AvroSerializationOptions.toJSON(message.avroSerializationOptions);
    }
    if (message.samplePercentage !== undefined) {
      obj.samplePercentage = message.samplePercentage;
    }
    if (message.responseCompressionCodec !== undefined) {
      obj.responseCompressionCodec = readSession_TableReadOptions_ResponseCompressionCodecToJSON(
        message.responseCompressionCodec,
      );
    }
    return obj;
  },

  create(base?: DeepPartial<ReadSession_TableReadOptions>): ReadSession_TableReadOptions {
    return ReadSession_TableReadOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReadSession_TableReadOptions>): ReadSession_TableReadOptions {
    const message = createBaseReadSession_TableReadOptions();
    message.selectedFields = object.selectedFields?.map((e) => e) || [];
    message.rowRestriction = object.rowRestriction ?? "";
    message.arrowSerializationOptions =
      (object.arrowSerializationOptions !== undefined && object.arrowSerializationOptions !== null)
        ? ArrowSerializationOptions.fromPartial(object.arrowSerializationOptions)
        : undefined;
    message.avroSerializationOptions =
      (object.avroSerializationOptions !== undefined && object.avroSerializationOptions !== null)
        ? AvroSerializationOptions.fromPartial(object.avroSerializationOptions)
        : undefined;
    message.samplePercentage = object.samplePercentage ?? undefined;
    message.responseCompressionCodec = object.responseCompressionCodec ?? undefined;
    return message;
  },
};

function createBaseReadStream(): ReadStream {
  return { name: "" };
}

export const ReadStream: MessageFns<ReadStream> = {
  encode(message: ReadStream, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReadStream {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReadStream();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReadStream {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: ReadStream): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<ReadStream>): ReadStream {
    return ReadStream.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReadStream>): ReadStream {
    const message = createBaseReadStream();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseWriteStream(): WriteStream {
  return {
    name: "",
    type: 0,
    createTime: undefined,
    commitTime: undefined,
    tableSchema: undefined,
    writeMode: 0,
    location: "",
  };
}

export const WriteStream: MessageFns<WriteStream> = {
  encode(message: WriteStream, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.type !== 0) {
      writer.uint32(16).int32(message.type);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(26).fork()).join();
    }
    if (message.commitTime !== undefined) {
      Timestamp.encode(toTimestamp(message.commitTime), writer.uint32(34).fork()).join();
    }
    if (message.tableSchema !== undefined) {
      TableSchema.encode(message.tableSchema, writer.uint32(42).fork()).join();
    }
    if (message.writeMode !== 0) {
      writer.uint32(56).int32(message.writeMode);
    }
    if (message.location !== "") {
      writer.uint32(66).string(message.location);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): WriteStream {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseWriteStream();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.commitTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.tableSchema = TableSchema.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.writeMode = reader.int32() as any;
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.location = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): WriteStream {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      type: isSet(object.type) ? writeStream_TypeFromJSON(object.type) : 0,
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      commitTime: isSet(object.commitTime) ? fromJsonTimestamp(object.commitTime) : undefined,
      tableSchema: isSet(object.tableSchema) ? TableSchema.fromJSON(object.tableSchema) : undefined,
      writeMode: isSet(object.writeMode) ? writeStream_WriteModeFromJSON(object.writeMode) : 0,
      location: isSet(object.location) ? globalThis.String(object.location) : "",
    };
  },

  toJSON(message: WriteStream): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.type !== 0) {
      obj.type = writeStream_TypeToJSON(message.type);
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.commitTime !== undefined) {
      obj.commitTime = message.commitTime.toISOString();
    }
    if (message.tableSchema !== undefined) {
      obj.tableSchema = TableSchema.toJSON(message.tableSchema);
    }
    if (message.writeMode !== 0) {
      obj.writeMode = writeStream_WriteModeToJSON(message.writeMode);
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    return obj;
  },

  create(base?: DeepPartial<WriteStream>): WriteStream {
    return WriteStream.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<WriteStream>): WriteStream {
    const message = createBaseWriteStream();
    message.name = object.name ?? "";
    message.type = object.type ?? 0;
    message.createTime = object.createTime ?? undefined;
    message.commitTime = object.commitTime ?? undefined;
    message.tableSchema = (object.tableSchema !== undefined && object.tableSchema !== null)
      ? TableSchema.fromPartial(object.tableSchema)
      : undefined;
    message.writeMode = object.writeMode ?? 0;
    message.location = object.location ?? "";
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
