// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/bigquery/storage/v1alpha/metastore_partition.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Empty } from "../../../../protobuf/empty.js";
import { FieldMask } from "../../../../protobuf/field_mask.js";
import { MetastorePartition, MetastorePartitionList, MetastorePartitionValues, StreamList } from "./partition.js";

export const protobufPackage = "google.cloud.bigquery.storage.v1alpha";

/**
 * Request message for CreateMetastorePartition. The MetastorePartition is
 * uniquely identified by values, which is an ordered list. Hence, there is no
 * separate name or partition id field.
 */
export interface CreateMetastorePartitionRequest {
  /**
   * Required. Reference to the table to where the metastore partition to be
   * added, in the format of
   * projects/{project}/databases/{databases}/tables/{table}.
   */
  parent: string;
  /** Required. The metastore partition to be added. */
  metastorePartition: MetastorePartition | undefined;
}

/** Request message for BatchCreateMetastorePartitions. */
export interface BatchCreateMetastorePartitionsRequest {
  /**
   * Required. Reference to the table to where the metastore partitions to be
   * added, in the format of
   * projects/{project}/locations/{location}/datasets/{dataset}/tables/{table}.
   */
  parent: string;
  /** Required. Requests to add metastore partitions to the table. */
  requests: CreateMetastorePartitionRequest[];
  /**
   * Optional. Mimics the ifNotExists flag in IMetaStoreClient
   * add_partitions(..). If the flag is set to false, the server will return
   * ALREADY_EXISTS if any partition already exists. If the flag is set to true,
   * the server will skip existing partitions and insert only the non-existing
   * partitions.
   */
  skipExistingPartitions: boolean;
}

/** Response message for BatchCreateMetastorePartitions. */
export interface BatchCreateMetastorePartitionsResponse {
  /** The list of metastore partitions that have been created. */
  partitions: MetastorePartition[];
}

/**
 * Request message for BatchDeleteMetastorePartitions. The MetastorePartition is
 * uniquely identified by values, which is an ordered list. Hence, there is no
 * separate name or partition id field.
 */
export interface BatchDeleteMetastorePartitionsRequest {
  /**
   * Required. Reference to the table to which these metastore partitions
   * belong, in the format of
   * projects/{project}/locations/{location}/datasets/{dataset}/tables/{table}.
   */
  parent: string;
  /**
   * Required. The list of metastore partitions (identified by its values) to be
   * deleted. A maximum of 100 partitions can be deleted in a batch.
   */
  partitionValues: MetastorePartitionValues[];
}

/** Request message for UpdateMetastorePartition. */
export interface UpdateMetastorePartitionRequest {
  /** Required. The metastore partition to be updated. */
  metastorePartition:
    | MetastorePartition
    | undefined;
  /** Optional. The list of fields to update. */
  updateMask: string[] | undefined;
}

/** Request message for BatchUpdateMetastorePartitions. */
export interface BatchUpdateMetastorePartitionsRequest {
  /**
   * Required. Reference to the table to which these metastore partitions
   * belong, in the format of
   * projects/{project}/locations/{location}/datasets/{dataset}/tables/{table}.
   */
  parent: string;
  /** Required. Requests to update metastore partitions in the table. */
  requests: UpdateMetastorePartitionRequest[];
}

/** Response message for BatchUpdateMetastorePartitions. */
export interface BatchUpdateMetastorePartitionsResponse {
  /** The list of metastore partitions that have been updated. */
  partitions: MetastorePartition[];
}

/** Request message for ListMetastorePartitions. */
export interface ListMetastorePartitionsRequest {
  /**
   * Required. Reference to the table to which these metastore partitions
   * belong, in the format of
   * projects/{project}/locations/{location}/datasets/{dataset}/tables/{table}.
   */
  parent: string;
  /**
   * Optional. SQL text filtering statement, similar to a WHERE clause in a
   * query. Only supports single-row expressions.  Aggregate functions are not
   * supported.
   *
   * Examples: "int_field > 5"
   *           "date_field = CAST('2014-9-27' as DATE)"
   *           "nullable_field is not NULL"
   *           "st_equals(geo_field, st_geofromtext("POINT(2, 2)"))"
   *           "numeric_field BETWEEN 1.0 AND 5.0"
   * Restricted to a maximum length for 1 MB.
   */
  filter: string;
}

/** Response message for ListMetastorePartitions. */
export interface ListMetastorePartitionsResponse {
  /** The list of partitions. */
  partitions?:
    | MetastorePartitionList
    | undefined;
  /** The list of streams. */
  streams?: StreamList | undefined;
}

/**
 * The top-level message sent by the client to the
 * [Partitions.StreamMetastorePartitions][] method.
 * Follows the default gRPC streaming maximum size of 4 MB.
 */
export interface StreamMetastorePartitionsRequest {
  /**
   * Required. Reference to the table to where the partition to be added, in the
   * format of
   * projects/{project}/locations/{location}/datasets/{dataset}/tables/{table}.
   */
  parent: string;
  /** Optional. A list of metastore partitions to be added to the table. */
  metastorePartitions: MetastorePartition[];
  /**
   * Optional. Mimics the ifNotExists flag in IMetaStoreClient
   * add_partitions(..). If the flag is set to false, the server will return
   * ALREADY_EXISTS on commit if any partition already exists. If the flag is
   * set to true:
   *  1) the server will skip existing partitions
   *  insert only the non-existing partitions as part of the commit.
   *  2) The client must set the `skip_existing_partitions` field to true for
   *  all requests in the stream.
   */
  skipExistingPartitions: boolean;
}

/**
 * This is the response message sent by the server
 * to the client for the [Partitions.StreamMetastorePartitions][] method when
 * the commit is successful. Server will close the stream after sending this
 * message.
 */
export interface StreamMetastorePartitionsResponse {
  /**
   * Total count of partitions streamed by the client during the lifetime of the
   * stream. This is only set in the final response message before closing the
   * stream.
   */
  totalPartitionsStreamedCount: Long;
  /**
   * Total count of partitions inserted by the server during the lifetime of the
   * stream. This is only set in the final response message before closing the
   * stream.
   */
  totalPartitionsInsertedCount: Long;
}

/**
 * Structured custom error message for batch size too large error.
 * The error can be attached as error details in the returned rpc Status for
 * more structured error handling in the client.
 */
export interface BatchSizeTooLargeError {
  /**
   * The maximum number of items that are supported in a single batch. This is
   * returned as a hint to the client to adjust the batch size.
   */
  maxBatchSize: Long;
  /** Optional. The error message that is returned to the client. */
  errorMessage: string;
}

function createBaseCreateMetastorePartitionRequest(): CreateMetastorePartitionRequest {
  return { parent: "", metastorePartition: undefined };
}

export const CreateMetastorePartitionRequest: MessageFns<CreateMetastorePartitionRequest> = {
  encode(message: CreateMetastorePartitionRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.metastorePartition !== undefined) {
      MetastorePartition.encode(message.metastorePartition, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateMetastorePartitionRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateMetastorePartitionRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.metastorePartition = MetastorePartition.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateMetastorePartitionRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      metastorePartition: isSet(object.metastorePartition)
        ? MetastorePartition.fromJSON(object.metastorePartition)
        : undefined,
    };
  },

  toJSON(message: CreateMetastorePartitionRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.metastorePartition !== undefined) {
      obj.metastorePartition = MetastorePartition.toJSON(message.metastorePartition);
    }
    return obj;
  },

  create(base?: DeepPartial<CreateMetastorePartitionRequest>): CreateMetastorePartitionRequest {
    return CreateMetastorePartitionRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateMetastorePartitionRequest>): CreateMetastorePartitionRequest {
    const message = createBaseCreateMetastorePartitionRequest();
    message.parent = object.parent ?? "";
    message.metastorePartition = (object.metastorePartition !== undefined && object.metastorePartition !== null)
      ? MetastorePartition.fromPartial(object.metastorePartition)
      : undefined;
    return message;
  },
};

function createBaseBatchCreateMetastorePartitionsRequest(): BatchCreateMetastorePartitionsRequest {
  return { parent: "", requests: [], skipExistingPartitions: false };
}

export const BatchCreateMetastorePartitionsRequest: MessageFns<BatchCreateMetastorePartitionsRequest> = {
  encode(message: BatchCreateMetastorePartitionsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    for (const v of message.requests) {
      CreateMetastorePartitionRequest.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.skipExistingPartitions !== false) {
      writer.uint32(24).bool(message.skipExistingPartitions);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchCreateMetastorePartitionsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchCreateMetastorePartitionsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.requests.push(CreateMetastorePartitionRequest.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.skipExistingPartitions = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchCreateMetastorePartitionsRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      requests: globalThis.Array.isArray(object?.requests)
        ? object.requests.map((e: any) => CreateMetastorePartitionRequest.fromJSON(e))
        : [],
      skipExistingPartitions: isSet(object.skipExistingPartitions)
        ? globalThis.Boolean(object.skipExistingPartitions)
        : false,
    };
  },

  toJSON(message: BatchCreateMetastorePartitionsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.requests?.length) {
      obj.requests = message.requests.map((e) => CreateMetastorePartitionRequest.toJSON(e));
    }
    if (message.skipExistingPartitions !== false) {
      obj.skipExistingPartitions = message.skipExistingPartitions;
    }
    return obj;
  },

  create(base?: DeepPartial<BatchCreateMetastorePartitionsRequest>): BatchCreateMetastorePartitionsRequest {
    return BatchCreateMetastorePartitionsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchCreateMetastorePartitionsRequest>): BatchCreateMetastorePartitionsRequest {
    const message = createBaseBatchCreateMetastorePartitionsRequest();
    message.parent = object.parent ?? "";
    message.requests = object.requests?.map((e) => CreateMetastorePartitionRequest.fromPartial(e)) || [];
    message.skipExistingPartitions = object.skipExistingPartitions ?? false;
    return message;
  },
};

function createBaseBatchCreateMetastorePartitionsResponse(): BatchCreateMetastorePartitionsResponse {
  return { partitions: [] };
}

export const BatchCreateMetastorePartitionsResponse: MessageFns<BatchCreateMetastorePartitionsResponse> = {
  encode(message: BatchCreateMetastorePartitionsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.partitions) {
      MetastorePartition.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchCreateMetastorePartitionsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchCreateMetastorePartitionsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.partitions.push(MetastorePartition.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchCreateMetastorePartitionsResponse {
    return {
      partitions: globalThis.Array.isArray(object?.partitions)
        ? object.partitions.map((e: any) => MetastorePartition.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchCreateMetastorePartitionsResponse): unknown {
    const obj: any = {};
    if (message.partitions?.length) {
      obj.partitions = message.partitions.map((e) => MetastorePartition.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchCreateMetastorePartitionsResponse>): BatchCreateMetastorePartitionsResponse {
    return BatchCreateMetastorePartitionsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchCreateMetastorePartitionsResponse>): BatchCreateMetastorePartitionsResponse {
    const message = createBaseBatchCreateMetastorePartitionsResponse();
    message.partitions = object.partitions?.map((e) => MetastorePartition.fromPartial(e)) || [];
    return message;
  },
};

function createBaseBatchDeleteMetastorePartitionsRequest(): BatchDeleteMetastorePartitionsRequest {
  return { parent: "", partitionValues: [] };
}

export const BatchDeleteMetastorePartitionsRequest: MessageFns<BatchDeleteMetastorePartitionsRequest> = {
  encode(message: BatchDeleteMetastorePartitionsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    for (const v of message.partitionValues) {
      MetastorePartitionValues.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchDeleteMetastorePartitionsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchDeleteMetastorePartitionsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.partitionValues.push(MetastorePartitionValues.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchDeleteMetastorePartitionsRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      partitionValues: globalThis.Array.isArray(object?.partitionValues)
        ? object.partitionValues.map((e: any) => MetastorePartitionValues.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchDeleteMetastorePartitionsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.partitionValues?.length) {
      obj.partitionValues = message.partitionValues.map((e) => MetastorePartitionValues.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchDeleteMetastorePartitionsRequest>): BatchDeleteMetastorePartitionsRequest {
    return BatchDeleteMetastorePartitionsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchDeleteMetastorePartitionsRequest>): BatchDeleteMetastorePartitionsRequest {
    const message = createBaseBatchDeleteMetastorePartitionsRequest();
    message.parent = object.parent ?? "";
    message.partitionValues = object.partitionValues?.map((e) => MetastorePartitionValues.fromPartial(e)) || [];
    return message;
  },
};

function createBaseUpdateMetastorePartitionRequest(): UpdateMetastorePartitionRequest {
  return { metastorePartition: undefined, updateMask: undefined };
}

export const UpdateMetastorePartitionRequest: MessageFns<UpdateMetastorePartitionRequest> = {
  encode(message: UpdateMetastorePartitionRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.metastorePartition !== undefined) {
      MetastorePartition.encode(message.metastorePartition, writer.uint32(10).fork()).join();
    }
    if (message.updateMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.updateMask), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateMetastorePartitionRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateMetastorePartitionRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.metastorePartition = MetastorePartition.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.updateMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateMetastorePartitionRequest {
    return {
      metastorePartition: isSet(object.metastorePartition)
        ? MetastorePartition.fromJSON(object.metastorePartition)
        : undefined,
      updateMask: isSet(object.updateMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.updateMask)) : undefined,
    };
  },

  toJSON(message: UpdateMetastorePartitionRequest): unknown {
    const obj: any = {};
    if (message.metastorePartition !== undefined) {
      obj.metastorePartition = MetastorePartition.toJSON(message.metastorePartition);
    }
    if (message.updateMask !== undefined) {
      obj.updateMask = FieldMask.toJSON(FieldMask.wrap(message.updateMask));
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateMetastorePartitionRequest>): UpdateMetastorePartitionRequest {
    return UpdateMetastorePartitionRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateMetastorePartitionRequest>): UpdateMetastorePartitionRequest {
    const message = createBaseUpdateMetastorePartitionRequest();
    message.metastorePartition = (object.metastorePartition !== undefined && object.metastorePartition !== null)
      ? MetastorePartition.fromPartial(object.metastorePartition)
      : undefined;
    message.updateMask = object.updateMask ?? undefined;
    return message;
  },
};

function createBaseBatchUpdateMetastorePartitionsRequest(): BatchUpdateMetastorePartitionsRequest {
  return { parent: "", requests: [] };
}

export const BatchUpdateMetastorePartitionsRequest: MessageFns<BatchUpdateMetastorePartitionsRequest> = {
  encode(message: BatchUpdateMetastorePartitionsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    for (const v of message.requests) {
      UpdateMetastorePartitionRequest.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchUpdateMetastorePartitionsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchUpdateMetastorePartitionsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.requests.push(UpdateMetastorePartitionRequest.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchUpdateMetastorePartitionsRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      requests: globalThis.Array.isArray(object?.requests)
        ? object.requests.map((e: any) => UpdateMetastorePartitionRequest.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchUpdateMetastorePartitionsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.requests?.length) {
      obj.requests = message.requests.map((e) => UpdateMetastorePartitionRequest.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchUpdateMetastorePartitionsRequest>): BatchUpdateMetastorePartitionsRequest {
    return BatchUpdateMetastorePartitionsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchUpdateMetastorePartitionsRequest>): BatchUpdateMetastorePartitionsRequest {
    const message = createBaseBatchUpdateMetastorePartitionsRequest();
    message.parent = object.parent ?? "";
    message.requests = object.requests?.map((e) => UpdateMetastorePartitionRequest.fromPartial(e)) || [];
    return message;
  },
};

function createBaseBatchUpdateMetastorePartitionsResponse(): BatchUpdateMetastorePartitionsResponse {
  return { partitions: [] };
}

export const BatchUpdateMetastorePartitionsResponse: MessageFns<BatchUpdateMetastorePartitionsResponse> = {
  encode(message: BatchUpdateMetastorePartitionsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.partitions) {
      MetastorePartition.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchUpdateMetastorePartitionsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchUpdateMetastorePartitionsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.partitions.push(MetastorePartition.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchUpdateMetastorePartitionsResponse {
    return {
      partitions: globalThis.Array.isArray(object?.partitions)
        ? object.partitions.map((e: any) => MetastorePartition.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchUpdateMetastorePartitionsResponse): unknown {
    const obj: any = {};
    if (message.partitions?.length) {
      obj.partitions = message.partitions.map((e) => MetastorePartition.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchUpdateMetastorePartitionsResponse>): BatchUpdateMetastorePartitionsResponse {
    return BatchUpdateMetastorePartitionsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchUpdateMetastorePartitionsResponse>): BatchUpdateMetastorePartitionsResponse {
    const message = createBaseBatchUpdateMetastorePartitionsResponse();
    message.partitions = object.partitions?.map((e) => MetastorePartition.fromPartial(e)) || [];
    return message;
  },
};

function createBaseListMetastorePartitionsRequest(): ListMetastorePartitionsRequest {
  return { parent: "", filter: "" };
}

export const ListMetastorePartitionsRequest: MessageFns<ListMetastorePartitionsRequest> = {
  encode(message: ListMetastorePartitionsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.filter !== "") {
      writer.uint32(18).string(message.filter);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListMetastorePartitionsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListMetastorePartitionsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.filter = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListMetastorePartitionsRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      filter: isSet(object.filter) ? globalThis.String(object.filter) : "",
    };
  },

  toJSON(message: ListMetastorePartitionsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.filter !== "") {
      obj.filter = message.filter;
    }
    return obj;
  },

  create(base?: DeepPartial<ListMetastorePartitionsRequest>): ListMetastorePartitionsRequest {
    return ListMetastorePartitionsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListMetastorePartitionsRequest>): ListMetastorePartitionsRequest {
    const message = createBaseListMetastorePartitionsRequest();
    message.parent = object.parent ?? "";
    message.filter = object.filter ?? "";
    return message;
  },
};

function createBaseListMetastorePartitionsResponse(): ListMetastorePartitionsResponse {
  return { partitions: undefined, streams: undefined };
}

export const ListMetastorePartitionsResponse: MessageFns<ListMetastorePartitionsResponse> = {
  encode(message: ListMetastorePartitionsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.partitions !== undefined) {
      MetastorePartitionList.encode(message.partitions, writer.uint32(10).fork()).join();
    }
    if (message.streams !== undefined) {
      StreamList.encode(message.streams, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListMetastorePartitionsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListMetastorePartitionsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.partitions = MetastorePartitionList.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.streams = StreamList.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListMetastorePartitionsResponse {
    return {
      partitions: isSet(object.partitions) ? MetastorePartitionList.fromJSON(object.partitions) : undefined,
      streams: isSet(object.streams) ? StreamList.fromJSON(object.streams) : undefined,
    };
  },

  toJSON(message: ListMetastorePartitionsResponse): unknown {
    const obj: any = {};
    if (message.partitions !== undefined) {
      obj.partitions = MetastorePartitionList.toJSON(message.partitions);
    }
    if (message.streams !== undefined) {
      obj.streams = StreamList.toJSON(message.streams);
    }
    return obj;
  },

  create(base?: DeepPartial<ListMetastorePartitionsResponse>): ListMetastorePartitionsResponse {
    return ListMetastorePartitionsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListMetastorePartitionsResponse>): ListMetastorePartitionsResponse {
    const message = createBaseListMetastorePartitionsResponse();
    message.partitions = (object.partitions !== undefined && object.partitions !== null)
      ? MetastorePartitionList.fromPartial(object.partitions)
      : undefined;
    message.streams = (object.streams !== undefined && object.streams !== null)
      ? StreamList.fromPartial(object.streams)
      : undefined;
    return message;
  },
};

function createBaseStreamMetastorePartitionsRequest(): StreamMetastorePartitionsRequest {
  return { parent: "", metastorePartitions: [], skipExistingPartitions: false };
}

export const StreamMetastorePartitionsRequest: MessageFns<StreamMetastorePartitionsRequest> = {
  encode(message: StreamMetastorePartitionsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    for (const v of message.metastorePartitions) {
      MetastorePartition.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.skipExistingPartitions !== false) {
      writer.uint32(24).bool(message.skipExistingPartitions);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamMetastorePartitionsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamMetastorePartitionsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.metastorePartitions.push(MetastorePartition.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.skipExistingPartitions = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamMetastorePartitionsRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      metastorePartitions: globalThis.Array.isArray(object?.metastorePartitions)
        ? object.metastorePartitions.map((e: any) => MetastorePartition.fromJSON(e))
        : [],
      skipExistingPartitions: isSet(object.skipExistingPartitions)
        ? globalThis.Boolean(object.skipExistingPartitions)
        : false,
    };
  },

  toJSON(message: StreamMetastorePartitionsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.metastorePartitions?.length) {
      obj.metastorePartitions = message.metastorePartitions.map((e) => MetastorePartition.toJSON(e));
    }
    if (message.skipExistingPartitions !== false) {
      obj.skipExistingPartitions = message.skipExistingPartitions;
    }
    return obj;
  },

  create(base?: DeepPartial<StreamMetastorePartitionsRequest>): StreamMetastorePartitionsRequest {
    return StreamMetastorePartitionsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamMetastorePartitionsRequest>): StreamMetastorePartitionsRequest {
    const message = createBaseStreamMetastorePartitionsRequest();
    message.parent = object.parent ?? "";
    message.metastorePartitions = object.metastorePartitions?.map((e) => MetastorePartition.fromPartial(e)) || [];
    message.skipExistingPartitions = object.skipExistingPartitions ?? false;
    return message;
  },
};

function createBaseStreamMetastorePartitionsResponse(): StreamMetastorePartitionsResponse {
  return { totalPartitionsStreamedCount: Long.ZERO, totalPartitionsInsertedCount: Long.ZERO };
}

export const StreamMetastorePartitionsResponse: MessageFns<StreamMetastorePartitionsResponse> = {
  encode(message: StreamMetastorePartitionsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.totalPartitionsStreamedCount.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.totalPartitionsStreamedCount.toString());
    }
    if (!message.totalPartitionsInsertedCount.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.totalPartitionsInsertedCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamMetastorePartitionsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamMetastorePartitionsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 16) {
            break;
          }

          message.totalPartitionsStreamedCount = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.totalPartitionsInsertedCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamMetastorePartitionsResponse {
    return {
      totalPartitionsStreamedCount: isSet(object.totalPartitionsStreamedCount)
        ? Long.fromValue(object.totalPartitionsStreamedCount)
        : Long.ZERO,
      totalPartitionsInsertedCount: isSet(object.totalPartitionsInsertedCount)
        ? Long.fromValue(object.totalPartitionsInsertedCount)
        : Long.ZERO,
    };
  },

  toJSON(message: StreamMetastorePartitionsResponse): unknown {
    const obj: any = {};
    if (!message.totalPartitionsStreamedCount.equals(Long.ZERO)) {
      obj.totalPartitionsStreamedCount = (message.totalPartitionsStreamedCount || Long.ZERO).toString();
    }
    if (!message.totalPartitionsInsertedCount.equals(Long.ZERO)) {
      obj.totalPartitionsInsertedCount = (message.totalPartitionsInsertedCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<StreamMetastorePartitionsResponse>): StreamMetastorePartitionsResponse {
    return StreamMetastorePartitionsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamMetastorePartitionsResponse>): StreamMetastorePartitionsResponse {
    const message = createBaseStreamMetastorePartitionsResponse();
    message.totalPartitionsStreamedCount =
      (object.totalPartitionsStreamedCount !== undefined && object.totalPartitionsStreamedCount !== null)
        ? Long.fromValue(object.totalPartitionsStreamedCount)
        : Long.ZERO;
    message.totalPartitionsInsertedCount =
      (object.totalPartitionsInsertedCount !== undefined && object.totalPartitionsInsertedCount !== null)
        ? Long.fromValue(object.totalPartitionsInsertedCount)
        : Long.ZERO;
    return message;
  },
};

function createBaseBatchSizeTooLargeError(): BatchSizeTooLargeError {
  return { maxBatchSize: Long.ZERO, errorMessage: "" };
}

export const BatchSizeTooLargeError: MessageFns<BatchSizeTooLargeError> = {
  encode(message: BatchSizeTooLargeError, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.maxBatchSize.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.maxBatchSize.toString());
    }
    if (message.errorMessage !== "") {
      writer.uint32(18).string(message.errorMessage);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchSizeTooLargeError {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchSizeTooLargeError();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.maxBatchSize = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.errorMessage = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchSizeTooLargeError {
    return {
      maxBatchSize: isSet(object.maxBatchSize) ? Long.fromValue(object.maxBatchSize) : Long.ZERO,
      errorMessage: isSet(object.errorMessage) ? globalThis.String(object.errorMessage) : "",
    };
  },

  toJSON(message: BatchSizeTooLargeError): unknown {
    const obj: any = {};
    if (!message.maxBatchSize.equals(Long.ZERO)) {
      obj.maxBatchSize = (message.maxBatchSize || Long.ZERO).toString();
    }
    if (message.errorMessage !== "") {
      obj.errorMessage = message.errorMessage;
    }
    return obj;
  },

  create(base?: DeepPartial<BatchSizeTooLargeError>): BatchSizeTooLargeError {
    return BatchSizeTooLargeError.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchSizeTooLargeError>): BatchSizeTooLargeError {
    const message = createBaseBatchSizeTooLargeError();
    message.maxBatchSize = (object.maxBatchSize !== undefined && object.maxBatchSize !== null)
      ? Long.fromValue(object.maxBatchSize)
      : Long.ZERO;
    message.errorMessage = object.errorMessage ?? "";
    return message;
  },
};

/**
 * BigQuery Metastore Partition Service API.
 * This service is used for managing metastore partitions in BigQuery metastore.
 * The service supports only batch operations for write.
 */
export type MetastorePartitionServiceDefinition = typeof MetastorePartitionServiceDefinition;
export const MetastorePartitionServiceDefinition = {
  name: "MetastorePartitionService",
  fullName: "google.cloud.bigquery.storage.v1alpha.MetastorePartitionService",
  methods: {
    /** Adds metastore partitions to a table. */
    batchCreateMetastorePartitions: {
      name: "BatchCreateMetastorePartitions",
      requestType: BatchCreateMetastorePartitionsRequest,
      requestStream: false,
      responseType: BatchCreateMetastorePartitionsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              88,
              58,
              1,
              42,
              34,
              83,
              47,
              118,
              49,
              97,
              108,
              112,
              104,
              97,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              125,
              47,
              112,
              97,
              114,
              116,
              105,
              116,
              105,
              111,
              110,
              115,
              58,
              98,
              97,
              116,
              99,
              104,
              67,
              114,
              101,
              97,
              116,
              101,
            ]),
          ],
        },
      },
    },
    /** Deletes metastore partitions from a table. */
    batchDeleteMetastorePartitions: {
      name: "BatchDeleteMetastorePartitions",
      requestType: BatchDeleteMetastorePartitionsRequest,
      requestStream: false,
      responseType: Empty,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              88,
              58,
              1,
              42,
              34,
              83,
              47,
              118,
              49,
              97,
              108,
              112,
              104,
              97,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              125,
              47,
              112,
              97,
              114,
              116,
              105,
              116,
              105,
              111,
              110,
              115,
              58,
              98,
              97,
              116,
              99,
              104,
              68,
              101,
              108,
              101,
              116,
              101,
            ]),
          ],
        },
      },
    },
    /** Updates metastore partitions in a table. */
    batchUpdateMetastorePartitions: {
      name: "BatchUpdateMetastorePartitions",
      requestType: BatchUpdateMetastorePartitionsRequest,
      requestStream: false,
      responseType: BatchUpdateMetastorePartitionsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              88,
              58,
              1,
              42,
              34,
              83,
              47,
              118,
              49,
              97,
              108,
              112,
              104,
              97,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              125,
              47,
              112,
              97,
              114,
              116,
              105,
              116,
              105,
              111,
              110,
              115,
              58,
              98,
              97,
              116,
              99,
              104,
              85,
              112,
              100,
              97,
              116,
              101,
            ]),
          ],
        },
      },
    },
    /** Gets metastore partitions from a table. */
    listMetastorePartitions: {
      name: "ListMetastorePartitions",
      requestType: ListMetastorePartitionsRequest,
      requestStream: false,
      responseType: ListMetastorePartitionsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([6, 112, 97, 114, 101, 110, 116])],
          578365826: [
            Buffer.from([
              78,
              18,
              76,
              47,
              118,
              49,
              97,
              108,
              112,
              104,
              97,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              125,
              47,
              112,
              97,
              114,
              116,
              105,
              116,
              105,
              111,
              110,
              115,
              58,
              108,
              105,
              115,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * This is a bi-di streaming rpc method that allows the client to send
     * a stream of partitions and commit all of them atomically at the end.
     * If the commit is successful, the server will return a
     * response and close the stream. If the commit fails (due to duplicate
     * partitions or other reason), the server will close the stream with an
     * error. This method is only available via the gRPC API (not REST).
     */
    streamMetastorePartitions: {
      name: "StreamMetastorePartitions",
      requestType: StreamMetastorePartitionsRequest,
      requestStream: true,
      responseType: StreamMetastorePartitionsResponse,
      responseStream: true,
      options: {},
    },
  },
} as const;

export interface MetastorePartitionServiceImplementation<CallContextExt = {}> {
  /** Adds metastore partitions to a table. */
  batchCreateMetastorePartitions(
    request: BatchCreateMetastorePartitionsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<BatchCreateMetastorePartitionsResponse>>;
  /** Deletes metastore partitions from a table. */
  batchDeleteMetastorePartitions(
    request: BatchDeleteMetastorePartitionsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Empty>>;
  /** Updates metastore partitions in a table. */
  batchUpdateMetastorePartitions(
    request: BatchUpdateMetastorePartitionsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<BatchUpdateMetastorePartitionsResponse>>;
  /** Gets metastore partitions from a table. */
  listMetastorePartitions(
    request: ListMetastorePartitionsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ListMetastorePartitionsResponse>>;
  /**
   * This is a bi-di streaming rpc method that allows the client to send
   * a stream of partitions and commit all of them atomically at the end.
   * If the commit is successful, the server will return a
   * response and close the stream. If the commit fails (due to duplicate
   * partitions or other reason), the server will close the stream with an
   * error. This method is only available via the gRPC API (not REST).
   */
  streamMetastorePartitions(
    request: AsyncIterable<StreamMetastorePartitionsRequest>,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<StreamMetastorePartitionsResponse>>;
}

export interface MetastorePartitionServiceClient<CallOptionsExt = {}> {
  /** Adds metastore partitions to a table. */
  batchCreateMetastorePartitions(
    request: DeepPartial<BatchCreateMetastorePartitionsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<BatchCreateMetastorePartitionsResponse>;
  /** Deletes metastore partitions from a table. */
  batchDeleteMetastorePartitions(
    request: DeepPartial<BatchDeleteMetastorePartitionsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Empty>;
  /** Updates metastore partitions in a table. */
  batchUpdateMetastorePartitions(
    request: DeepPartial<BatchUpdateMetastorePartitionsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<BatchUpdateMetastorePartitionsResponse>;
  /** Gets metastore partitions from a table. */
  listMetastorePartitions(
    request: DeepPartial<ListMetastorePartitionsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ListMetastorePartitionsResponse>;
  /**
   * This is a bi-di streaming rpc method that allows the client to send
   * a stream of partitions and commit all of them atomically at the end.
   * If the commit is successful, the server will return a
   * response and close the stream. If the commit fails (due to duplicate
   * partitions or other reason), the server will close the stream with an
   * error. This method is only available via the gRPC API (not REST).
   */
  streamMetastorePartitions(
    request: AsyncIterable<DeepPartial<StreamMetastorePartitionsRequest>>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<StreamMetastorePartitionsResponse>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export type ServerStreamingMethodResult<Response> = { [Symbol.asyncIterator](): AsyncIterator<Response, void> };

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
