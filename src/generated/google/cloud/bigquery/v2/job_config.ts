// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/bigquery/v2/job_config.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { BoolValue, Int32Value, Int64Value, StringValue } from "../../../protobuf/wrappers.js";
import { Clustering } from "./clustering.js";
import { DatasetReference } from "./dataset_reference.js";
import { DecimalTargetType, decimalTargetTypeFromJSON, decimalTargetTypeToJSON } from "./decimal_target_types.js";
import { EncryptionConfiguration } from "./encryption_config.js";
import { ExternalDataConfiguration, ParquetOptions } from "./external_data_config.js";
import { FileSetSpecType, fileSetSpecTypeFromJSON, fileSetSpecTypeToJSON } from "./file_set_specification_type.js";
import { HivePartitioningOptions } from "./hive_partitioning.js";
import { JsonExtension, jsonExtensionFromJSON, jsonExtensionToJSON } from "./json_extension.js";
import { ModelReference } from "./model_reference.js";
import { QueryParameter } from "./query_parameter.js";
import { RangePartitioning } from "./range_partitioning.js";
import { SystemVariables } from "./system_variable.js";
import { TableReference } from "./table_reference.js";
import { TableSchema } from "./table_schema.js";
import { TimePartitioning } from "./time_partitioning.js";
import { UserDefinedFunctionResource } from "./udf_resource.js";

export const protobufPackage = "google.cloud.bigquery.v2";

/** Properties for the destination table. */
export interface DestinationTableProperties {
  /**
   * Optional. Friendly name for the destination table. If the table already
   * exists, it should be same as the existing friendly name.
   */
  friendlyName:
    | string
    | undefined;
  /**
   * Optional. The description for the destination table.
   * This will only be used if the destination table is newly created.
   * If the table already exists and a value different than the current
   * description is provided, the job will fail.
   */
  description:
    | string
    | undefined;
  /**
   * Optional. The labels associated with this table. You can use these to
   * organize and group your tables. This will only be used if the destination
   * table is newly created. If the table already exists and labels are
   * different than the current labels are provided, the job will fail.
   */
  labels: { [key: string]: string };
}

export interface DestinationTableProperties_LabelsEntry {
  key: string;
  value: string;
}

/**
 * A connection-level property to customize query behavior. Under JDBC, these
 * correspond directly to connection properties passed to the DriverManager.
 * Under ODBC, these correspond to properties in the connection string.
 *
 * Currently supported connection properties:
 *
 * * **dataset_project_id**: represents the default project for datasets that
 * are used in the query. Setting the
 * system variable `@@dataset_project_id` achieves the same behavior.  For
 * more information about system variables, see:
 * https://cloud.google.com/bigquery/docs/reference/system-variables
 *
 * * **time_zone**: represents the default timezone used to run the query.
 *
 * * **session_id**: associates the query with a given session.
 *
 * * **query_label**: associates the query with a given job label. If set,
 * all subsequent queries in a script or session will have this label. For the
 * format in which a you can specify a query label, see labels
 * in the JobConfiguration resource type:
 * https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfiguration
 *
 * * **service_account**: indicates the service account to use to run a
 * continuous query. If set, the query job uses the service account to access
 * Google Cloud resources. Service account access is bounded by the IAM
 * permissions that you have granted to the service account.
 *
 * Additional properties are allowed, but ignored. Specifying multiple
 * connection properties with the same key returns an error.
 */
export interface ConnectionProperty {
  /** The key of the property to set. */
  key: string;
  /** The value of the property to set. */
  value: string;
}

/** JobConfigurationQuery configures a BigQuery query job. */
export interface JobConfigurationQuery {
  /**
   * [Required] SQL query text to execute. The useLegacySql field can be used
   * to indicate whether the query uses legacy SQL or GoogleSQL.
   */
  query: string;
  /**
   * Optional. Describes the table where the query results should be stored.
   * This property must be set for large results that exceed the maximum
   * response size.  For queries that produce anonymous (cached) results, this
   * field will be populated by BigQuery.
   */
  destinationTable:
    | TableReference
    | undefined;
  /**
   * Optional. You can specify external table definitions, which operate as
   * ephemeral tables that can be queried.  These definitions are configured
   * using a JSON map, where the string key represents the table identifier, and
   * the value is the corresponding external data configuration object.
   */
  externalTableDefinitions: { [key: string]: ExternalDataConfiguration };
  /** Describes user-defined function resources used in the query. */
  userDefinedFunctionResources: UserDefinedFunctionResource[];
  /**
   * Optional. Specifies whether the job is allowed to create new tables.
   * The following values are supported:
   *
   * * CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the
   * table.
   * * CREATE_NEVER: The table must already exist. If it does not,
   * a 'notFound' error is returned in the job result.
   *
   * The default value is CREATE_IF_NEEDED.
   * Creation, truncation and append actions occur as one atomic update
   * upon job completion.
   */
  createDisposition: string;
  /**
   * Optional. Specifies the action that occurs if the destination table
   * already exists. The following values are supported:
   *
   * * WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the
   * data, removes the constraints, and uses the schema from the query result.
   * * WRITE_APPEND: If the table already exists, BigQuery appends the data to
   * the table.
   * * WRITE_EMPTY: If the table already exists and contains data, a 'duplicate'
   * error is returned in the job result.
   *
   * The default value is WRITE_EMPTY. Each action is atomic and only occurs if
   * BigQuery is able to complete the job successfully. Creation, truncation and
   * append actions occur as one atomic update upon job completion.
   */
  writeDisposition: string;
  /**
   * Optional. Specifies the default dataset to use for unqualified
   * table names in the query. This setting does not alter behavior of
   * unqualified dataset names. Setting the system variable
   * `@@dataset_id` achieves the same behavior.  See
   * https://cloud.google.com/bigquery/docs/reference/system-variables for more
   * information on system variables.
   */
  defaultDataset:
    | DatasetReference
    | undefined;
  /**
   * Optional. Specifies a priority for the query. Possible values include
   * INTERACTIVE and BATCH. The default value is INTERACTIVE.
   */
  priority: string;
  /**
   * Optional. If true and query uses legacy SQL dialect, allows the query
   * to produce arbitrarily large result tables at a slight cost in performance.
   * Requires destinationTable to be set.
   * For GoogleSQL queries, this flag is ignored and large results are
   * always allowed.  However, you must still set destinationTable when result
   * size exceeds the allowed maximum response size.
   */
  allowLargeResults:
    | boolean
    | undefined;
  /**
   * Optional. Whether to look for the result in the query cache. The query
   * cache is a best-effort cache that will be flushed whenever tables in the
   * query are modified. Moreover, the query cache is only available when a
   * query does not have a destination table specified. The default value is
   * true.
   */
  useQueryCache:
    | boolean
    | undefined;
  /**
   * Optional. If true and query uses legacy SQL dialect, flattens all nested
   * and repeated fields in the query results.
   * allowLargeResults must be true if this is set to false.
   * For GoogleSQL queries, this flag is ignored and results are never
   * flattened.
   */
  flattenResults:
    | boolean
    | undefined;
  /**
   * Limits the bytes billed for this job. Queries that will have
   * bytes billed beyond this limit will fail (without incurring a charge).
   * If unspecified, this will be set to your project default.
   */
  maximumBytesBilled:
    | Long
    | undefined;
  /**
   * Optional. Specifies whether to use BigQuery's legacy SQL dialect for this
   * query. The default value is true. If set to false, the query will use
   * BigQuery's GoogleSQL:
   * https://cloud.google.com/bigquery/sql-reference/
   *
   * When useLegacySql is set to false, the value of flattenResults is ignored;
   * query will be run as if flattenResults is false.
   */
  useLegacySql:
    | boolean
    | undefined;
  /**
   * GoogleSQL only. Set to POSITIONAL to use positional (?) query parameters
   * or to NAMED to use named (@myparam) query parameters in this query.
   */
  parameterMode: string;
  /** Query parameters for GoogleSQL queries. */
  queryParameters: QueryParameter[];
  /**
   * Output only. System variables for GoogleSQL queries. A system variable is
   * output if the variable is settable and its value differs from the system
   * default.
   * "@@" prefix is not included in the name of the System variables.
   */
  systemVariables?:
    | SystemVariables
    | undefined;
  /**
   * Allows the schema of the destination table to be updated as a side effect
   * of the query job. Schema update options are supported in two cases:
   * when writeDisposition is WRITE_APPEND;
   * when writeDisposition is WRITE_TRUNCATE and the destination table is a
   * partition of a table, specified by partition decorators. For normal tables,
   * WRITE_TRUNCATE will always overwrite the schema.
   * One or more of the following values are specified:
   *
   * * ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
   * * ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original
   * schema to nullable.
   */
  schemaUpdateOptions: string[];
  /**
   * Time-based partitioning specification for the destination table. Only one
   * of timePartitioning and rangePartitioning should be specified.
   */
  timePartitioning:
    | TimePartitioning
    | undefined;
  /**
   * Range partitioning specification for the destination table.
   * Only one of timePartitioning and rangePartitioning should be specified.
   */
  rangePartitioning:
    | RangePartitioning
    | undefined;
  /** Clustering specification for the destination table. */
  clustering:
    | Clustering
    | undefined;
  /** Custom encryption configuration (e.g., Cloud KMS keys) */
  destinationEncryptionConfiguration:
    | EncryptionConfiguration
    | undefined;
  /** Options controlling the execution of scripts. */
  scriptOptions:
    | ScriptOptions
    | undefined;
  /** Connection properties which can modify the query behavior. */
  connectionProperties: ConnectionProperty[];
  /**
   * If this property is true, the job creates a new session using a randomly
   * generated session_id.  To continue using a created session with
   * subsequent queries, pass the existing session identifier as a
   * `ConnectionProperty` value.  The session identifier is returned as part of
   * the `SessionInfo` message within the query statistics.
   *
   * The new session's location will be set to `Job.JobReference.location` if it
   * is present, otherwise it's set to the default location based on existing
   * routing logic.
   */
  createSession:
    | boolean
    | undefined;
  /**
   * Optional. Whether to run the query as continuous or a regular query.
   * Continuous query is currently in experimental stage and not ready for
   * general usage.
   */
  continuous: boolean | undefined;
}

export interface JobConfigurationQuery_ExternalTableDefinitionsEntry {
  key: string;
  value: ExternalDataConfiguration | undefined;
}

/** Options related to script execution. */
export interface ScriptOptions {
  /** Timeout period for each statement in a script. */
  statementTimeoutMs:
    | Long
    | undefined;
  /**
   * Limit on the number of bytes billed per statement. Exceeding this budget
   * results in an error.
   */
  statementByteBudget:
    | Long
    | undefined;
  /**
   * Determines which statement in the script represents the "key result",
   * used to populate the schema and query results of the script job.
   * Default is LAST.
   */
  keyResultStatement: ScriptOptions_KeyResultStatementKind;
}

/** KeyResultStatementKind controls how the key result is determined. */
export enum ScriptOptions_KeyResultStatementKind {
  /** KEY_RESULT_STATEMENT_KIND_UNSPECIFIED - Default value. */
  KEY_RESULT_STATEMENT_KIND_UNSPECIFIED = 0,
  /** LAST - The last result determines the key result. */
  LAST = 1,
  /** FIRST_SELECT - The first SELECT statement determines the key result. */
  FIRST_SELECT = 2,
  UNRECOGNIZED = -1,
}

export function scriptOptions_KeyResultStatementKindFromJSON(object: any): ScriptOptions_KeyResultStatementKind {
  switch (object) {
    case 0:
    case "KEY_RESULT_STATEMENT_KIND_UNSPECIFIED":
      return ScriptOptions_KeyResultStatementKind.KEY_RESULT_STATEMENT_KIND_UNSPECIFIED;
    case 1:
    case "LAST":
      return ScriptOptions_KeyResultStatementKind.LAST;
    case 2:
    case "FIRST_SELECT":
      return ScriptOptions_KeyResultStatementKind.FIRST_SELECT;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ScriptOptions_KeyResultStatementKind.UNRECOGNIZED;
  }
}

export function scriptOptions_KeyResultStatementKindToJSON(object: ScriptOptions_KeyResultStatementKind): string {
  switch (object) {
    case ScriptOptions_KeyResultStatementKind.KEY_RESULT_STATEMENT_KIND_UNSPECIFIED:
      return "KEY_RESULT_STATEMENT_KIND_UNSPECIFIED";
    case ScriptOptions_KeyResultStatementKind.LAST:
      return "LAST";
    case ScriptOptions_KeyResultStatementKind.FIRST_SELECT:
      return "FIRST_SELECT";
    case ScriptOptions_KeyResultStatementKind.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * JobConfigurationLoad contains the configuration properties for loading data
 * into a destination table.
 */
export interface JobConfigurationLoad {
  /**
   * [Required] The fully-qualified URIs that point to your data in Google
   * Cloud.
   * For Google Cloud Storage URIs:
   *   Each URI can contain one '*' wildcard character and it must come after
   *   the 'bucket' name. Size limits related to load jobs apply to external
   *   data sources.
   * For Google Cloud Bigtable URIs:
   *   Exactly one URI can be specified and it has be a fully specified and
   *   valid HTTPS URL for a Google Cloud Bigtable table.
   * For Google Cloud Datastore backups:
   *  Exactly one URI can be specified. Also, the '*' wildcard character is not
   *  allowed.
   */
  sourceUris: string[];
  /**
   * Optional. Specifies how source URIs are interpreted for constructing the
   * file set to load. By default, source URIs are expanded against the
   * underlying storage. You can also specify manifest files to control how the
   * file set is constructed. This option is only applicable to object storage
   * systems.
   */
  fileSetSpecType: FileSetSpecType;
  /**
   * Optional. The schema for the destination table. The schema can be
   * omitted if the destination table already exists, or if you're loading data
   * from Google Cloud Datastore.
   */
  schema:
    | TableSchema
    | undefined;
  /** [Required] The destination table to load the data into. */
  destinationTable:
    | TableReference
    | undefined;
  /**
   * Optional. [Experimental] Properties with which to create the destination
   * table if it is new.
   */
  destinationTableProperties:
    | DestinationTableProperties
    | undefined;
  /**
   * Optional. Specifies whether the job is allowed to create new tables.
   * The following values are supported:
   *
   * * CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the
   * table.
   * * CREATE_NEVER: The table must already exist. If it does not,
   * a 'notFound' error is returned in the job result.
   * The default value is CREATE_IF_NEEDED.
   * Creation, truncation and append actions occur as one atomic update
   * upon job completion.
   */
  createDisposition: string;
  /**
   * Optional. Specifies the action that occurs if the destination table
   * already exists. The following values are supported:
   *
   * * WRITE_TRUNCATE:  If the table already exists, BigQuery overwrites the
   * data, removes the constraints and uses the schema from the load job.
   * * WRITE_APPEND: If the table already exists, BigQuery appends the data to
   * the table.
   * * WRITE_EMPTY: If the table already exists and contains data, a 'duplicate'
   * error is returned in the job result.
   *
   * The default value is WRITE_APPEND.
   * Each action is atomic and only occurs if BigQuery is able to complete the
   * job successfully.
   * Creation, truncation and append actions occur as one atomic update
   * upon job completion.
   */
  writeDisposition: string;
  /**
   * Optional. Specifies a string that represents a null value in a CSV file.
   * For example, if you specify "\N", BigQuery interprets "\N" as a null value
   * when loading a CSV file.
   * The default value is the empty string. If you set this property to a custom
   * value, BigQuery throws an error if an empty string is present for all data
   * types except for STRING and BYTE. For STRING and BYTE columns, BigQuery
   * interprets the empty string as an empty value.
   */
  nullMarker:
    | string
    | undefined;
  /**
   * Optional. The separator character for fields in a CSV file. The separator
   * is interpreted as a single byte. For files encoded in ISO-8859-1, any
   * single character can be used as a separator. For files encoded in UTF-8,
   * characters represented in decimal range 1-127 (U+0001-U+007F) can be used
   * without any modification. UTF-8 characters encoded with multiple bytes
   * (i.e. U+0080 and above) will have only the first byte used for separating
   * fields. The remaining bytes will be treated as a part of the field.
   * BigQuery also supports the escape sequence "\t" (U+0009) to specify a tab
   * separator. The default value is comma (",", U+002C).
   */
  fieldDelimiter: string;
  /**
   * Optional. The number of rows at the top of a CSV file that BigQuery will
   * skip when loading the data. The default value is 0. This property is useful
   * if you have header rows in the file that should be skipped. When autodetect
   * is on, the behavior is the following:
   *
   * * skipLeadingRows unspecified - Autodetect tries to detect headers in the
   *   first row. If they are not detected, the row is read as data. Otherwise
   *   data is read starting from the second row.
   * * skipLeadingRows is 0 - Instructs autodetect that there are no headers and
   *   data should be read starting from the first row.
   * * skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect
   *   headers in row N. If headers are not detected, row N is just skipped.
   *   Otherwise row N is used to extract column names for the detected schema.
   */
  skipLeadingRows:
    | number
    | undefined;
  /**
   * Optional. The character encoding of the data.
   * The supported values are UTF-8, ISO-8859-1, UTF-16BE, UTF-16LE, UTF-32BE,
   * and UTF-32LE. The default value is UTF-8. BigQuery decodes the data after
   * the raw, binary data has been split using the values of the `quote` and
   * `fieldDelimiter` properties.
   *
   * If you don't specify an encoding, or if you specify a UTF-8 encoding when
   * the CSV file is not UTF-8 encoded, BigQuery attempts to convert the data to
   * UTF-8. Generally, your data loads successfully, but it may not match
   * byte-for-byte what you expect. To avoid this, specify the correct encoding
   * by using the `--encoding` flag.
   *
   * If BigQuery can't convert a character other than the ASCII `0` character,
   * BigQuery converts the character to the standard Unicode replacement
   * character: &#65533;.
   */
  encoding: string;
  /**
   * Optional. The value that is used to quote data sections in a CSV file.
   * BigQuery converts the string to ISO-8859-1 encoding, and then uses the
   * first byte of the encoded string to split the data in its raw, binary
   * state.
   * The default value is a double-quote ('"').
   * If your data does not contain quoted sections, set the property value to an
   * empty string.
   * If your data contains quoted newline characters, you must also set the
   * allowQuotedNewlines property to true.
   * To include the specific quote character within a quoted value, precede it
   * with an additional matching quote character. For example, if you want to
   * escape the default character  ' " ', use ' "" '.
   * @default "
   */
  quote:
    | string
    | undefined;
  /**
   * Optional. The maximum number of bad records that BigQuery can ignore when
   * running the job. If the number of bad records exceeds this value, an
   * invalid error is returned in the job result.
   * The default value is 0, which requires that all records are valid.
   * This is only supported for CSV and NEWLINE_DELIMITED_JSON file formats.
   */
  maxBadRecords:
    | number
    | undefined;
  /**
   * Indicates if BigQuery should allow quoted data sections that contain
   * newline characters in a CSV file. The default value is false.
   */
  allowQuotedNewlines:
    | boolean
    | undefined;
  /**
   * Optional. The format of the data files.
   * For CSV files, specify "CSV". For datastore backups,
   * specify "DATASTORE_BACKUP". For newline-delimited JSON,
   * specify "NEWLINE_DELIMITED_JSON". For Avro, specify "AVRO".
   * For parquet, specify "PARQUET". For orc, specify "ORC".
   * The default value is CSV.
   */
  sourceFormat: string;
  /**
   * Optional. Accept rows that are missing trailing optional columns.
   * The missing values are treated as nulls.
   * If false, records with missing trailing columns are treated as bad records,
   * and if there are too many bad records, an invalid error is returned in the
   * job result.
   * The default value is false.
   * Only applicable to CSV, ignored for other formats.
   */
  allowJaggedRows:
    | boolean
    | undefined;
  /**
   * Optional. Indicates if BigQuery should allow extra values that are not
   * represented in the table schema.
   * If true, the extra values are ignored.
   * If false, records with extra columns are treated as bad records, and if
   * there are too many bad records, an invalid error is returned in the job
   * result. The default value is false.
   * The sourceFormat property determines what BigQuery treats as an extra
   * value:
   *   CSV: Trailing columns
   *   JSON: Named values that don't match any column names in the table schema
   *   Avro, Parquet, ORC: Fields in the file schema that don't exist in the
   *   table schema.
   */
  ignoreUnknownValues:
    | boolean
    | undefined;
  /**
   * If sourceFormat is set to "DATASTORE_BACKUP", indicates which entity
   * properties to load into BigQuery from a Cloud Datastore backup. Property
   * names are case sensitive and must be top-level properties. If no properties
   * are specified, BigQuery loads all properties. If any named property isn't
   * found in the Cloud Datastore backup, an invalid error is returned in the
   * job result.
   */
  projectionFields: string[];
  /**
   * Optional. Indicates if we should automatically infer the options and
   * schema for CSV and JSON sources.
   */
  autodetect:
    | boolean
    | undefined;
  /**
   * Allows the schema of the destination table to be updated as a side effect
   * of the load job if a schema is autodetected or supplied in the job
   * configuration.
   * Schema update options are supported in two cases:
   * when writeDisposition is WRITE_APPEND;
   * when writeDisposition is WRITE_TRUNCATE and the destination table is a
   * partition of a table, specified by partition decorators. For normal tables,
   * WRITE_TRUNCATE will always overwrite the schema.
   * One or more of the following values are specified:
   *
   * * ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
   * * ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original
   * schema to nullable.
   */
  schemaUpdateOptions: string[];
  /**
   * Time-based partitioning specification for the destination table. Only one
   * of timePartitioning and rangePartitioning should be specified.
   */
  timePartitioning:
    | TimePartitioning
    | undefined;
  /**
   * Range partitioning specification for the destination table.
   * Only one of timePartitioning and rangePartitioning should be specified.
   */
  rangePartitioning:
    | RangePartitioning
    | undefined;
  /** Clustering specification for the destination table. */
  clustering:
    | Clustering
    | undefined;
  /** Custom encryption configuration (e.g., Cloud KMS keys) */
  destinationEncryptionConfiguration:
    | EncryptionConfiguration
    | undefined;
  /**
   * Optional. If sourceFormat is set to "AVRO", indicates whether to interpret
   * logical types as the corresponding BigQuery data type (for example,
   * TIMESTAMP), instead of using the raw type (for example, INTEGER).
   */
  useAvroLogicalTypes:
    | boolean
    | undefined;
  /**
   * Optional. The user can provide a reference file with the reader schema.
   * This file is only loaded if it is part of source URIs, but is not loaded
   * otherwise. It is enabled for the following formats: AVRO, PARQUET, ORC.
   */
  referenceFileSchemaUri:
    | string
    | undefined;
  /**
   * Optional. When set, configures hive partitioning support.
   * Not all storage formats support hive partitioning -- requesting hive
   * partitioning on an unsupported format will lead to an error, as will
   * providing an invalid specification.
   */
  hivePartitioningOptions:
    | HivePartitioningOptions
    | undefined;
  /**
   * Defines the list of possible SQL data types to which the source decimal
   * values are converted. This list and the precision and the scale parameters
   * of the decimal field determine the target type. In the order of NUMERIC,
   * BIGNUMERIC, and STRING, a
   * type is picked if it is in the specified list and if it supports the
   * precision and the scale. STRING supports all precision and scale values.
   * If none of the listed types supports the precision and the scale, the type
   * supporting the widest range in the specified list is picked, and if a value
   * exceeds the supported range when reading the data, an error will be thrown.
   *
   * Example: Suppose the value of this field is ["NUMERIC", "BIGNUMERIC"].
   * If (precision,scale) is:
   *
   * * (38,9) -> NUMERIC;
   * * (39,9) -> BIGNUMERIC (NUMERIC cannot hold 30 integer digits);
   * * (38,10) -> BIGNUMERIC (NUMERIC cannot hold 10 fractional digits);
   * * (76,38) -> BIGNUMERIC;
   * * (77,38) -> BIGNUMERIC (error if value exeeds supported range).
   *
   * This field cannot contain duplicate types. The order of the types in this
   * field is ignored. For example, ["BIGNUMERIC", "NUMERIC"] is the same as
   * ["NUMERIC", "BIGNUMERIC"] and NUMERIC always takes precedence over
   * BIGNUMERIC.
   *
   * Defaults to ["NUMERIC", "STRING"] for ORC and ["NUMERIC"] for the other
   * file formats.
   */
  decimalTargetTypes: DecimalTargetType[];
  /**
   * Optional. Load option to be used together with source_format
   * newline-delimited JSON to indicate that a variant of JSON is being loaded.
   * To load newline-delimited GeoJSON, specify GEOJSON (and source_format must
   * be set to NEWLINE_DELIMITED_JSON).
   */
  jsonExtension: JsonExtension;
  /** Optional. Additional properties to set if sourceFormat is set to PARQUET. */
  parquetOptions:
    | ParquetOptions
    | undefined;
  /**
   * Optional. When sourceFormat is set to "CSV", this indicates whether the
   * embedded ASCII control characters (the first 32 characters in the
   * ASCII-table, from
   * '\x00' to '\x1F') are preserved.
   */
  preserveAsciiControlCharacters:
    | boolean
    | undefined;
  /**
   * Optional. Connection properties which can modify the load job behavior.
   * Currently, only the 'session_id' connection property is supported, and is
   * used to resolve _SESSION appearing as the dataset id.
   */
  connectionProperties: ConnectionProperty[];
  /**
   * Optional. If this property is true, the job creates a new session using a
   * randomly generated session_id.  To continue using a created session with
   * subsequent queries, pass the existing session identifier as a
   * `ConnectionProperty` value.  The session identifier is returned as part of
   * the `SessionInfo` message within the query statistics.
   *
   * The new session's location will be set to `Job.JobReference.location` if it
   * is present, otherwise it's set to the default location based on existing
   * routing logic.
   */
  createSession:
    | boolean
    | undefined;
  /**
   * Optional. Character map supported for column names in CSV/Parquet loads.
   * Defaults to STRICT and can be overridden by Project Config Service. Using
   * this option with unsupporting load formats will result in an error.
   */
  columnNameCharacterMap: JobConfigurationLoad_ColumnNameCharacterMap;
  /**
   * Optional. [Experimental] Configures the load job to copy files directly to
   * the destination BigLake managed table, bypassing file content reading and
   * rewriting.
   *
   * Copying files only is supported when all the following are true:
   *
   * * `source_uris` are located in the same Cloud Storage location as the
   *   destination table's `storage_uri` location.
   * * `source_format` is `PARQUET`.
   * * `destination_table` is an existing BigLake managed table. The table's
   *   schema does not have flexible column names. The table's columns do not
   *   have type parameters other than precision and scale.
   * * No options other than the above are specified.
   */
  copyFilesOnly: boolean | undefined;
}

/** Indicates the character map used for column names. */
export enum JobConfigurationLoad_ColumnNameCharacterMap {
  /** COLUMN_NAME_CHARACTER_MAP_UNSPECIFIED - Unspecified column name character map. */
  COLUMN_NAME_CHARACTER_MAP_UNSPECIFIED = 0,
  /** STRICT - Support flexible column name and reject invalid column names. */
  STRICT = 1,
  /**
   * V1 - Support alphanumeric + underscore characters and names must start with a
   * letter or underscore. Invalid column names will be normalized.
   */
  V1 = 2,
  /** V2 - Support flexible column name. Invalid column names will be normalized. */
  V2 = 3,
  UNRECOGNIZED = -1,
}

export function jobConfigurationLoad_ColumnNameCharacterMapFromJSON(
  object: any,
): JobConfigurationLoad_ColumnNameCharacterMap {
  switch (object) {
    case 0:
    case "COLUMN_NAME_CHARACTER_MAP_UNSPECIFIED":
      return JobConfigurationLoad_ColumnNameCharacterMap.COLUMN_NAME_CHARACTER_MAP_UNSPECIFIED;
    case 1:
    case "STRICT":
      return JobConfigurationLoad_ColumnNameCharacterMap.STRICT;
    case 2:
    case "V1":
      return JobConfigurationLoad_ColumnNameCharacterMap.V1;
    case 3:
    case "V2":
      return JobConfigurationLoad_ColumnNameCharacterMap.V2;
    case -1:
    case "UNRECOGNIZED":
    default:
      return JobConfigurationLoad_ColumnNameCharacterMap.UNRECOGNIZED;
  }
}

export function jobConfigurationLoad_ColumnNameCharacterMapToJSON(
  object: JobConfigurationLoad_ColumnNameCharacterMap,
): string {
  switch (object) {
    case JobConfigurationLoad_ColumnNameCharacterMap.COLUMN_NAME_CHARACTER_MAP_UNSPECIFIED:
      return "COLUMN_NAME_CHARACTER_MAP_UNSPECIFIED";
    case JobConfigurationLoad_ColumnNameCharacterMap.STRICT:
      return "STRICT";
    case JobConfigurationLoad_ColumnNameCharacterMap.V1:
      return "V1";
    case JobConfigurationLoad_ColumnNameCharacterMap.V2:
      return "V2";
    case JobConfigurationLoad_ColumnNameCharacterMap.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * JobConfigurationTableCopy configures a job that copies data from one table
 * to another.
 * For more information on copying tables, see [Copy a
 *  table](https://cloud.google.com/bigquery/docs/managing-tables#copy-table).
 */
export interface JobConfigurationTableCopy {
  /** [Pick one] Source table to copy. */
  sourceTable:
    | TableReference
    | undefined;
  /** [Pick one] Source tables to copy. */
  sourceTables: TableReference[];
  /** [Required] The destination table. */
  destinationTable:
    | TableReference
    | undefined;
  /**
   * Optional. Specifies whether the job is allowed to create new tables.
   * The following values are supported:
   *
   * * CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the
   * table.
   * * CREATE_NEVER: The table must already exist. If it does not,
   * a 'notFound' error is returned in the job result.
   *
   * The default value is CREATE_IF_NEEDED.
   * Creation, truncation and append actions occur as one atomic update
   * upon job completion.
   */
  createDisposition: string;
  /**
   * Optional. Specifies the action that occurs if the destination table
   * already exists. The following values are supported:
   *
   * * WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the
   * table data and uses the schema and table constraints from the source table.
   * * WRITE_APPEND: If the table already exists, BigQuery appends the data to
   * the table.
   * * WRITE_EMPTY: If the table already exists and contains data, a 'duplicate'
   * error is returned in the job result.
   *
   * The default value is WRITE_EMPTY. Each action is atomic and only occurs if
   * BigQuery is able to complete the job successfully. Creation, truncation and
   * append actions occur as one atomic update upon job completion.
   */
  writeDisposition: string;
  /** Custom encryption configuration (e.g., Cloud KMS keys). */
  destinationEncryptionConfiguration:
    | EncryptionConfiguration
    | undefined;
  /** Optional. Supported operation types in table copy job. */
  operationType: JobConfigurationTableCopy_OperationType;
  /**
   * Optional. The time when the destination table expires. Expired tables will
   * be deleted and their storage reclaimed.
   */
  destinationExpirationTime: Date | undefined;
}

/** Indicates different operation types supported in table copy job. */
export enum JobConfigurationTableCopy_OperationType {
  /** OPERATION_TYPE_UNSPECIFIED - Unspecified operation type. */
  OPERATION_TYPE_UNSPECIFIED = 0,
  /** COPY - The source and destination table have the same table type. */
  COPY = 1,
  /**
   * SNAPSHOT - The source table type is TABLE and
   * the destination table type is SNAPSHOT.
   */
  SNAPSHOT = 2,
  /**
   * RESTORE - The source table type is SNAPSHOT and
   * the destination table type is TABLE.
   */
  RESTORE = 3,
  /**
   * CLONE - The source and destination table have the same table type,
   * but only bill for unique data.
   */
  CLONE = 4,
  UNRECOGNIZED = -1,
}

export function jobConfigurationTableCopy_OperationTypeFromJSON(object: any): JobConfigurationTableCopy_OperationType {
  switch (object) {
    case 0:
    case "OPERATION_TYPE_UNSPECIFIED":
      return JobConfigurationTableCopy_OperationType.OPERATION_TYPE_UNSPECIFIED;
    case 1:
    case "COPY":
      return JobConfigurationTableCopy_OperationType.COPY;
    case 2:
    case "SNAPSHOT":
      return JobConfigurationTableCopy_OperationType.SNAPSHOT;
    case 3:
    case "RESTORE":
      return JobConfigurationTableCopy_OperationType.RESTORE;
    case 4:
    case "CLONE":
      return JobConfigurationTableCopy_OperationType.CLONE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return JobConfigurationTableCopy_OperationType.UNRECOGNIZED;
  }
}

export function jobConfigurationTableCopy_OperationTypeToJSON(object: JobConfigurationTableCopy_OperationType): string {
  switch (object) {
    case JobConfigurationTableCopy_OperationType.OPERATION_TYPE_UNSPECIFIED:
      return "OPERATION_TYPE_UNSPECIFIED";
    case JobConfigurationTableCopy_OperationType.COPY:
      return "COPY";
    case JobConfigurationTableCopy_OperationType.SNAPSHOT:
      return "SNAPSHOT";
    case JobConfigurationTableCopy_OperationType.RESTORE:
      return "RESTORE";
    case JobConfigurationTableCopy_OperationType.CLONE:
      return "CLONE";
    case JobConfigurationTableCopy_OperationType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * JobConfigurationExtract configures a job that exports data from a BigQuery
 * table into Google Cloud Storage.
 */
export interface JobConfigurationExtract {
  /** A reference to the table being exported. */
  sourceTable?:
    | TableReference
    | undefined;
  /** A reference to the model being exported. */
  sourceModel?:
    | ModelReference
    | undefined;
  /**
   * [Pick one] A list of fully-qualified Google Cloud Storage URIs where the
   * extracted table should be written.
   */
  destinationUris: string[];
  /**
   * Optional. Whether to print out a header row in the results.
   * Default is true. Not applicable when extracting models.
   */
  printHeader:
    | boolean
    | undefined;
  /**
   * Optional. When extracting data in CSV format, this defines the
   * delimiter to use between fields in the exported data.
   * Default is ','. Not applicable when extracting models.
   */
  fieldDelimiter: string;
  /**
   * Optional. The exported file format. Possible values include CSV,
   * NEWLINE_DELIMITED_JSON, PARQUET, or AVRO for tables and ML_TF_SAVED_MODEL
   * or ML_XGBOOST_BOOSTER for models. The default value for tables is CSV.
   * Tables with nested or repeated fields cannot be exported as CSV. The
   * default value for models is ML_TF_SAVED_MODEL.
   */
  destinationFormat: string;
  /**
   * Optional. The compression type to use for exported files. Possible values
   * include DEFLATE, GZIP, NONE, SNAPPY, and ZSTD. The default value is NONE.
   * Not all compression formats are support for all file formats. DEFLATE is
   * only supported for Avro. ZSTD is only supported for Parquet. Not applicable
   * when extracting models.
   */
  compression: string;
  /**
   * Whether to use logical types when extracting to AVRO format. Not applicable
   * when extracting models.
   */
  useAvroLogicalTypes:
    | boolean
    | undefined;
  /** Optional. Model extract options only applicable when extracting models. */
  modelExtractOptions: JobConfigurationExtract_ModelExtractOptions | undefined;
}

/** Options related to model extraction. */
export interface JobConfigurationExtract_ModelExtractOptions {
  /**
   * The 1-based ID of the trial to be exported from a hyperparameter tuning
   * model. If not specified, the trial with id =
   * [Model](https://cloud.google.com/bigquery/docs/reference/rest/v2/models#resource:-model).defaultTrialId
   * is exported. This field is ignored for models not trained with
   * hyperparameter tuning.
   */
  trialId: Long | undefined;
}

export interface JobConfiguration {
  /**
   * Output only. The type of the job. Can be QUERY, LOAD, EXTRACT, COPY or
   * UNKNOWN.
   */
  jobType: string;
  /** [Pick one] Configures a query job. */
  query:
    | JobConfigurationQuery
    | undefined;
  /** [Pick one] Configures a load job. */
  load:
    | JobConfigurationLoad
    | undefined;
  /** [Pick one] Copies a table. */
  copy:
    | JobConfigurationTableCopy
    | undefined;
  /** [Pick one] Configures an extract job. */
  extract:
    | JobConfigurationExtract
    | undefined;
  /**
   * Optional. If set, don't actually run this job. A valid query will return
   * a mostly empty response with some processing statistics, while an invalid
   * query will return the same error it would if it wasn't a dry run. Behavior
   * of non-query jobs is undefined.
   */
  dryRun:
    | boolean
    | undefined;
  /**
   * Optional. Job timeout in milliseconds. If this time limit is exceeded,
   * BigQuery will attempt to stop a longer job, but may not always succeed in
   * canceling it before the job completes. For example, a job that takes more
   * than 60 seconds to complete has a better chance of being stopped than a job
   * that takes 10 seconds to complete.
   */
  jobTimeoutMs:
    | Long
    | undefined;
  /**
   * The labels associated with this job. You can use these to organize and
   * group your jobs.
   * Label keys and values can be no longer than 63 characters, can only contain
   * lowercase letters, numeric characters, underscores and dashes.
   * International characters are allowed. Label values are optional.  Label
   * keys must start with a letter and each label in the list must have a
   * different key.
   */
  labels: { [key: string]: string };
}

export interface JobConfiguration_LabelsEntry {
  key: string;
  value: string;
}

function createBaseDestinationTableProperties(): DestinationTableProperties {
  return { friendlyName: undefined, description: undefined, labels: {} };
}

export const DestinationTableProperties: MessageFns<DestinationTableProperties> = {
  encode(message: DestinationTableProperties, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.friendlyName !== undefined) {
      StringValue.encode({ value: message.friendlyName! }, writer.uint32(10).fork()).join();
    }
    if (message.description !== undefined) {
      StringValue.encode({ value: message.description! }, writer.uint32(18).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      DestinationTableProperties_LabelsEntry.encode({ key: key as any, value }, writer.uint32(26).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DestinationTableProperties {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDestinationTableProperties();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.friendlyName = StringValue.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.description = StringValue.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          const entry3 = DestinationTableProperties_LabelsEntry.decode(reader, reader.uint32());
          if (entry3.value !== undefined) {
            message.labels[entry3.key] = entry3.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DestinationTableProperties {
    return {
      friendlyName: isSet(object.friendlyName) ? String(object.friendlyName) : undefined,
      description: isSet(object.description) ? String(object.description) : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: DestinationTableProperties): unknown {
    const obj: any = {};
    if (message.friendlyName !== undefined) {
      obj.friendlyName = message.friendlyName;
    }
    if (message.description !== undefined) {
      obj.description = message.description;
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<DestinationTableProperties>): DestinationTableProperties {
    return DestinationTableProperties.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DestinationTableProperties>): DestinationTableProperties {
    const message = createBaseDestinationTableProperties();
    message.friendlyName = object.friendlyName ?? undefined;
    message.description = object.description ?? undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseDestinationTableProperties_LabelsEntry(): DestinationTableProperties_LabelsEntry {
  return { key: "", value: "" };
}

export const DestinationTableProperties_LabelsEntry: MessageFns<DestinationTableProperties_LabelsEntry> = {
  encode(message: DestinationTableProperties_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DestinationTableProperties_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDestinationTableProperties_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DestinationTableProperties_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: DestinationTableProperties_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<DestinationTableProperties_LabelsEntry>): DestinationTableProperties_LabelsEntry {
    return DestinationTableProperties_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DestinationTableProperties_LabelsEntry>): DestinationTableProperties_LabelsEntry {
    const message = createBaseDestinationTableProperties_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseConnectionProperty(): ConnectionProperty {
  return { key: "", value: "" };
}

export const ConnectionProperty: MessageFns<ConnectionProperty> = {
  encode(message: ConnectionProperty, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ConnectionProperty {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseConnectionProperty();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ConnectionProperty {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: ConnectionProperty): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<ConnectionProperty>): ConnectionProperty {
    return ConnectionProperty.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ConnectionProperty>): ConnectionProperty {
    const message = createBaseConnectionProperty();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseJobConfigurationQuery(): JobConfigurationQuery {
  return {
    query: "",
    destinationTable: undefined,
    externalTableDefinitions: {},
    userDefinedFunctionResources: [],
    createDisposition: "",
    writeDisposition: "",
    defaultDataset: undefined,
    priority: "",
    allowLargeResults: undefined,
    useQueryCache: undefined,
    flattenResults: undefined,
    maximumBytesBilled: undefined,
    useLegacySql: undefined,
    parameterMode: "",
    queryParameters: [],
    systemVariables: undefined,
    schemaUpdateOptions: [],
    timePartitioning: undefined,
    rangePartitioning: undefined,
    clustering: undefined,
    destinationEncryptionConfiguration: undefined,
    scriptOptions: undefined,
    connectionProperties: [],
    createSession: undefined,
    continuous: undefined,
  };
}

export const JobConfigurationQuery: MessageFns<JobConfigurationQuery> = {
  encode(message: JobConfigurationQuery, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.query !== "") {
      writer.uint32(10).string(message.query);
    }
    if (message.destinationTable !== undefined) {
      TableReference.encode(message.destinationTable, writer.uint32(18).fork()).join();
    }
    Object.entries(message.externalTableDefinitions).forEach(([key, value]) => {
      JobConfigurationQuery_ExternalTableDefinitionsEntry.encode({ key: key as any, value }, writer.uint32(186).fork())
        .join();
    });
    for (const v of message.userDefinedFunctionResources) {
      UserDefinedFunctionResource.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.createDisposition !== "") {
      writer.uint32(42).string(message.createDisposition);
    }
    if (message.writeDisposition !== "") {
      writer.uint32(50).string(message.writeDisposition);
    }
    if (message.defaultDataset !== undefined) {
      DatasetReference.encode(message.defaultDataset, writer.uint32(58).fork()).join();
    }
    if (message.priority !== "") {
      writer.uint32(66).string(message.priority);
    }
    if (message.allowLargeResults !== undefined) {
      BoolValue.encode({ value: message.allowLargeResults! }, writer.uint32(82).fork()).join();
    }
    if (message.useQueryCache !== undefined) {
      BoolValue.encode({ value: message.useQueryCache! }, writer.uint32(90).fork()).join();
    }
    if (message.flattenResults !== undefined) {
      BoolValue.encode({ value: message.flattenResults! }, writer.uint32(98).fork()).join();
    }
    if (message.maximumBytesBilled !== undefined) {
      Int64Value.encode({ value: message.maximumBytesBilled! }, writer.uint32(114).fork()).join();
    }
    if (message.useLegacySql !== undefined) {
      BoolValue.encode({ value: message.useLegacySql! }, writer.uint32(122).fork()).join();
    }
    if (message.parameterMode !== "") {
      writer.uint32(130).string(message.parameterMode);
    }
    for (const v of message.queryParameters) {
      QueryParameter.encode(v!, writer.uint32(138).fork()).join();
    }
    if (message.systemVariables !== undefined) {
      SystemVariables.encode(message.systemVariables, writer.uint32(282).fork()).join();
    }
    for (const v of message.schemaUpdateOptions) {
      writer.uint32(146).string(v!);
    }
    if (message.timePartitioning !== undefined) {
      TimePartitioning.encode(message.timePartitioning, writer.uint32(154).fork()).join();
    }
    if (message.rangePartitioning !== undefined) {
      RangePartitioning.encode(message.rangePartitioning, writer.uint32(178).fork()).join();
    }
    if (message.clustering !== undefined) {
      Clustering.encode(message.clustering, writer.uint32(162).fork()).join();
    }
    if (message.destinationEncryptionConfiguration !== undefined) {
      EncryptionConfiguration.encode(message.destinationEncryptionConfiguration, writer.uint32(170).fork()).join();
    }
    if (message.scriptOptions !== undefined) {
      ScriptOptions.encode(message.scriptOptions, writer.uint32(194).fork()).join();
    }
    for (const v of message.connectionProperties) {
      ConnectionProperty.encode(v!, writer.uint32(266).fork()).join();
    }
    if (message.createSession !== undefined) {
      BoolValue.encode({ value: message.createSession! }, writer.uint32(274).fork()).join();
    }
    if (message.continuous !== undefined) {
      BoolValue.encode({ value: message.continuous! }, writer.uint32(290).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobConfigurationQuery {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobConfigurationQuery();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.query = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.destinationTable = TableReference.decode(reader, reader.uint32());
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          const entry23 = JobConfigurationQuery_ExternalTableDefinitionsEntry.decode(reader, reader.uint32());
          if (entry23.value !== undefined) {
            message.externalTableDefinitions[entry23.key] = entry23.value;
          }
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.userDefinedFunctionResources.push(UserDefinedFunctionResource.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.createDisposition = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.writeDisposition = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.defaultDataset = DatasetReference.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.priority = reader.string();
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.allowLargeResults = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.useQueryCache = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.flattenResults = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.maximumBytesBilled = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.useLegacySql = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.parameterMode = reader.string();
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.queryParameters.push(QueryParameter.decode(reader, reader.uint32()));
          continue;
        case 35:
          if (tag !== 282) {
            break;
          }

          message.systemVariables = SystemVariables.decode(reader, reader.uint32());
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.schemaUpdateOptions.push(reader.string());
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.timePartitioning = TimePartitioning.decode(reader, reader.uint32());
          continue;
        case 22:
          if (tag !== 178) {
            break;
          }

          message.rangePartitioning = RangePartitioning.decode(reader, reader.uint32());
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.clustering = Clustering.decode(reader, reader.uint32());
          continue;
        case 21:
          if (tag !== 170) {
            break;
          }

          message.destinationEncryptionConfiguration = EncryptionConfiguration.decode(reader, reader.uint32());
          continue;
        case 24:
          if (tag !== 194) {
            break;
          }

          message.scriptOptions = ScriptOptions.decode(reader, reader.uint32());
          continue;
        case 33:
          if (tag !== 266) {
            break;
          }

          message.connectionProperties.push(ConnectionProperty.decode(reader, reader.uint32()));
          continue;
        case 34:
          if (tag !== 274) {
            break;
          }

          message.createSession = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 36:
          if (tag !== 290) {
            break;
          }

          message.continuous = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobConfigurationQuery {
    return {
      query: isSet(object.query) ? globalThis.String(object.query) : "",
      destinationTable: isSet(object.destinationTable) ? TableReference.fromJSON(object.destinationTable) : undefined,
      externalTableDefinitions: isObject(object.externalTableDefinitions)
        ? Object.entries(object.externalTableDefinitions).reduce<{ [key: string]: ExternalDataConfiguration }>(
          (acc, [key, value]) => {
            acc[key] = ExternalDataConfiguration.fromJSON(value);
            return acc;
          },
          {},
        )
        : {},
      userDefinedFunctionResources: globalThis.Array.isArray(object?.userDefinedFunctionResources)
        ? object.userDefinedFunctionResources.map((e: any) => UserDefinedFunctionResource.fromJSON(e))
        : [],
      createDisposition: isSet(object.createDisposition) ? globalThis.String(object.createDisposition) : "",
      writeDisposition: isSet(object.writeDisposition) ? globalThis.String(object.writeDisposition) : "",
      defaultDataset: isSet(object.defaultDataset) ? DatasetReference.fromJSON(object.defaultDataset) : undefined,
      priority: isSet(object.priority) ? globalThis.String(object.priority) : "",
      allowLargeResults: isSet(object.allowLargeResults) ? Boolean(object.allowLargeResults) : undefined,
      useQueryCache: isSet(object.useQueryCache) ? Boolean(object.useQueryCache) : undefined,
      flattenResults: isSet(object.flattenResults) ? Boolean(object.flattenResults) : undefined,
      maximumBytesBilled: isSet(object.maximumBytesBilled) ? Long.fromValue(object.maximumBytesBilled) : undefined,
      useLegacySql: isSet(object.useLegacySql) ? Boolean(object.useLegacySql) : undefined,
      parameterMode: isSet(object.parameterMode) ? globalThis.String(object.parameterMode) : "",
      queryParameters: globalThis.Array.isArray(object?.queryParameters)
        ? object.queryParameters.map((e: any) => QueryParameter.fromJSON(e))
        : [],
      systemVariables: isSet(object.systemVariables) ? SystemVariables.fromJSON(object.systemVariables) : undefined,
      schemaUpdateOptions: globalThis.Array.isArray(object?.schemaUpdateOptions)
        ? object.schemaUpdateOptions.map((e: any) => globalThis.String(e))
        : [],
      timePartitioning: isSet(object.timePartitioning) ? TimePartitioning.fromJSON(object.timePartitioning) : undefined,
      rangePartitioning: isSet(object.rangePartitioning)
        ? RangePartitioning.fromJSON(object.rangePartitioning)
        : undefined,
      clustering: isSet(object.clustering) ? Clustering.fromJSON(object.clustering) : undefined,
      destinationEncryptionConfiguration: isSet(object.destinationEncryptionConfiguration)
        ? EncryptionConfiguration.fromJSON(object.destinationEncryptionConfiguration)
        : undefined,
      scriptOptions: isSet(object.scriptOptions) ? ScriptOptions.fromJSON(object.scriptOptions) : undefined,
      connectionProperties: globalThis.Array.isArray(object?.connectionProperties)
        ? object.connectionProperties.map((e: any) => ConnectionProperty.fromJSON(e))
        : [],
      createSession: isSet(object.createSession) ? Boolean(object.createSession) : undefined,
      continuous: isSet(object.continuous) ? Boolean(object.continuous) : undefined,
    };
  },

  toJSON(message: JobConfigurationQuery): unknown {
    const obj: any = {};
    if (message.query !== "") {
      obj.query = message.query;
    }
    if (message.destinationTable !== undefined) {
      obj.destinationTable = TableReference.toJSON(message.destinationTable);
    }
    if (message.externalTableDefinitions) {
      const entries = Object.entries(message.externalTableDefinitions);
      if (entries.length > 0) {
        obj.externalTableDefinitions = {};
        entries.forEach(([k, v]) => {
          obj.externalTableDefinitions[k] = ExternalDataConfiguration.toJSON(v);
        });
      }
    }
    if (message.userDefinedFunctionResources?.length) {
      obj.userDefinedFunctionResources = message.userDefinedFunctionResources.map((e) =>
        UserDefinedFunctionResource.toJSON(e)
      );
    }
    if (message.createDisposition !== "") {
      obj.createDisposition = message.createDisposition;
    }
    if (message.writeDisposition !== "") {
      obj.writeDisposition = message.writeDisposition;
    }
    if (message.defaultDataset !== undefined) {
      obj.defaultDataset = DatasetReference.toJSON(message.defaultDataset);
    }
    if (message.priority !== "") {
      obj.priority = message.priority;
    }
    if (message.allowLargeResults !== undefined) {
      obj.allowLargeResults = message.allowLargeResults;
    }
    if (message.useQueryCache !== undefined) {
      obj.useQueryCache = message.useQueryCache;
    }
    if (message.flattenResults !== undefined) {
      obj.flattenResults = message.flattenResults;
    }
    if (message.maximumBytesBilled !== undefined) {
      obj.maximumBytesBilled = message.maximumBytesBilled;
    }
    if (message.useLegacySql !== undefined) {
      obj.useLegacySql = message.useLegacySql;
    }
    if (message.parameterMode !== "") {
      obj.parameterMode = message.parameterMode;
    }
    if (message.queryParameters?.length) {
      obj.queryParameters = message.queryParameters.map((e) => QueryParameter.toJSON(e));
    }
    if (message.systemVariables !== undefined) {
      obj.systemVariables = SystemVariables.toJSON(message.systemVariables);
    }
    if (message.schemaUpdateOptions?.length) {
      obj.schemaUpdateOptions = message.schemaUpdateOptions;
    }
    if (message.timePartitioning !== undefined) {
      obj.timePartitioning = TimePartitioning.toJSON(message.timePartitioning);
    }
    if (message.rangePartitioning !== undefined) {
      obj.rangePartitioning = RangePartitioning.toJSON(message.rangePartitioning);
    }
    if (message.clustering !== undefined) {
      obj.clustering = Clustering.toJSON(message.clustering);
    }
    if (message.destinationEncryptionConfiguration !== undefined) {
      obj.destinationEncryptionConfiguration = EncryptionConfiguration.toJSON(
        message.destinationEncryptionConfiguration,
      );
    }
    if (message.scriptOptions !== undefined) {
      obj.scriptOptions = ScriptOptions.toJSON(message.scriptOptions);
    }
    if (message.connectionProperties?.length) {
      obj.connectionProperties = message.connectionProperties.map((e) => ConnectionProperty.toJSON(e));
    }
    if (message.createSession !== undefined) {
      obj.createSession = message.createSession;
    }
    if (message.continuous !== undefined) {
      obj.continuous = message.continuous;
    }
    return obj;
  },

  create(base?: DeepPartial<JobConfigurationQuery>): JobConfigurationQuery {
    return JobConfigurationQuery.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobConfigurationQuery>): JobConfigurationQuery {
    const message = createBaseJobConfigurationQuery();
    message.query = object.query ?? "";
    message.destinationTable = (object.destinationTable !== undefined && object.destinationTable !== null)
      ? TableReference.fromPartial(object.destinationTable)
      : undefined;
    message.externalTableDefinitions = Object.entries(object.externalTableDefinitions ?? {}).reduce<
      { [key: string]: ExternalDataConfiguration }
    >((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = ExternalDataConfiguration.fromPartial(value);
      }
      return acc;
    }, {});
    message.userDefinedFunctionResources =
      object.userDefinedFunctionResources?.map((e) => UserDefinedFunctionResource.fromPartial(e)) || [];
    message.createDisposition = object.createDisposition ?? "";
    message.writeDisposition = object.writeDisposition ?? "";
    message.defaultDataset = (object.defaultDataset !== undefined && object.defaultDataset !== null)
      ? DatasetReference.fromPartial(object.defaultDataset)
      : undefined;
    message.priority = object.priority ?? "";
    message.allowLargeResults = object.allowLargeResults ?? undefined;
    message.useQueryCache = object.useQueryCache ?? undefined;
    message.flattenResults = object.flattenResults ?? undefined;
    message.maximumBytesBilled = (object.maximumBytesBilled !== undefined && object.maximumBytesBilled !== null)
      ? Long.fromValue(object.maximumBytesBilled)
      : undefined;
    message.useLegacySql = object.useLegacySql ?? undefined;
    message.parameterMode = object.parameterMode ?? "";
    message.queryParameters = object.queryParameters?.map((e) => QueryParameter.fromPartial(e)) || [];
    message.systemVariables = (object.systemVariables !== undefined && object.systemVariables !== null)
      ? SystemVariables.fromPartial(object.systemVariables)
      : undefined;
    message.schemaUpdateOptions = object.schemaUpdateOptions?.map((e) => e) || [];
    message.timePartitioning = (object.timePartitioning !== undefined && object.timePartitioning !== null)
      ? TimePartitioning.fromPartial(object.timePartitioning)
      : undefined;
    message.rangePartitioning = (object.rangePartitioning !== undefined && object.rangePartitioning !== null)
      ? RangePartitioning.fromPartial(object.rangePartitioning)
      : undefined;
    message.clustering = (object.clustering !== undefined && object.clustering !== null)
      ? Clustering.fromPartial(object.clustering)
      : undefined;
    message.destinationEncryptionConfiguration =
      (object.destinationEncryptionConfiguration !== undefined && object.destinationEncryptionConfiguration !== null)
        ? EncryptionConfiguration.fromPartial(object.destinationEncryptionConfiguration)
        : undefined;
    message.scriptOptions = (object.scriptOptions !== undefined && object.scriptOptions !== null)
      ? ScriptOptions.fromPartial(object.scriptOptions)
      : undefined;
    message.connectionProperties = object.connectionProperties?.map((e) => ConnectionProperty.fromPartial(e)) || [];
    message.createSession = object.createSession ?? undefined;
    message.continuous = object.continuous ?? undefined;
    return message;
  },
};

function createBaseJobConfigurationQuery_ExternalTableDefinitionsEntry(): JobConfigurationQuery_ExternalTableDefinitionsEntry {
  return { key: "", value: undefined };
}

export const JobConfigurationQuery_ExternalTableDefinitionsEntry: MessageFns<
  JobConfigurationQuery_ExternalTableDefinitionsEntry
> = {
  encode(
    message: JobConfigurationQuery_ExternalTableDefinitionsEntry,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      ExternalDataConfiguration.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobConfigurationQuery_ExternalTableDefinitionsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobConfigurationQuery_ExternalTableDefinitionsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = ExternalDataConfiguration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobConfigurationQuery_ExternalTableDefinitionsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? ExternalDataConfiguration.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: JobConfigurationQuery_ExternalTableDefinitionsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = ExternalDataConfiguration.toJSON(message.value);
    }
    return obj;
  },

  create(
    base?: DeepPartial<JobConfigurationQuery_ExternalTableDefinitionsEntry>,
  ): JobConfigurationQuery_ExternalTableDefinitionsEntry {
    return JobConfigurationQuery_ExternalTableDefinitionsEntry.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<JobConfigurationQuery_ExternalTableDefinitionsEntry>,
  ): JobConfigurationQuery_ExternalTableDefinitionsEntry {
    const message = createBaseJobConfigurationQuery_ExternalTableDefinitionsEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? ExternalDataConfiguration.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseScriptOptions(): ScriptOptions {
  return { statementTimeoutMs: undefined, statementByteBudget: undefined, keyResultStatement: 0 };
}

export const ScriptOptions: MessageFns<ScriptOptions> = {
  encode(message: ScriptOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.statementTimeoutMs !== undefined) {
      Int64Value.encode({ value: message.statementTimeoutMs! }, writer.uint32(10).fork()).join();
    }
    if (message.statementByteBudget !== undefined) {
      Int64Value.encode({ value: message.statementByteBudget! }, writer.uint32(18).fork()).join();
    }
    if (message.keyResultStatement !== 0) {
      writer.uint32(32).int32(message.keyResultStatement);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ScriptOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseScriptOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.statementTimeoutMs = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.statementByteBudget = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.keyResultStatement = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ScriptOptions {
    return {
      statementTimeoutMs: isSet(object.statementTimeoutMs) ? Long.fromValue(object.statementTimeoutMs) : undefined,
      statementByteBudget: isSet(object.statementByteBudget) ? Long.fromValue(object.statementByteBudget) : undefined,
      keyResultStatement: isSet(object.keyResultStatement)
        ? scriptOptions_KeyResultStatementKindFromJSON(object.keyResultStatement)
        : 0,
    };
  },

  toJSON(message: ScriptOptions): unknown {
    const obj: any = {};
    if (message.statementTimeoutMs !== undefined) {
      obj.statementTimeoutMs = message.statementTimeoutMs;
    }
    if (message.statementByteBudget !== undefined) {
      obj.statementByteBudget = message.statementByteBudget;
    }
    if (message.keyResultStatement !== 0) {
      obj.keyResultStatement = scriptOptions_KeyResultStatementKindToJSON(message.keyResultStatement);
    }
    return obj;
  },

  create(base?: DeepPartial<ScriptOptions>): ScriptOptions {
    return ScriptOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ScriptOptions>): ScriptOptions {
    const message = createBaseScriptOptions();
    message.statementTimeoutMs = (object.statementTimeoutMs !== undefined && object.statementTimeoutMs !== null)
      ? Long.fromValue(object.statementTimeoutMs)
      : undefined;
    message.statementByteBudget = (object.statementByteBudget !== undefined && object.statementByteBudget !== null)
      ? Long.fromValue(object.statementByteBudget)
      : undefined;
    message.keyResultStatement = object.keyResultStatement ?? 0;
    return message;
  },
};

function createBaseJobConfigurationLoad(): JobConfigurationLoad {
  return {
    sourceUris: [],
    fileSetSpecType: 0,
    schema: undefined,
    destinationTable: undefined,
    destinationTableProperties: undefined,
    createDisposition: "",
    writeDisposition: "",
    nullMarker: undefined,
    fieldDelimiter: "",
    skipLeadingRows: undefined,
    encoding: "",
    quote: undefined,
    maxBadRecords: undefined,
    allowQuotedNewlines: undefined,
    sourceFormat: "",
    allowJaggedRows: undefined,
    ignoreUnknownValues: undefined,
    projectionFields: [],
    autodetect: undefined,
    schemaUpdateOptions: [],
    timePartitioning: undefined,
    rangePartitioning: undefined,
    clustering: undefined,
    destinationEncryptionConfiguration: undefined,
    useAvroLogicalTypes: undefined,
    referenceFileSchemaUri: undefined,
    hivePartitioningOptions: undefined,
    decimalTargetTypes: [],
    jsonExtension: 0,
    parquetOptions: undefined,
    preserveAsciiControlCharacters: undefined,
    connectionProperties: [],
    createSession: undefined,
    columnNameCharacterMap: 0,
    copyFilesOnly: undefined,
  };
}

export const JobConfigurationLoad: MessageFns<JobConfigurationLoad> = {
  encode(message: JobConfigurationLoad, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.sourceUris) {
      writer.uint32(10).string(v!);
    }
    if (message.fileSetSpecType !== 0) {
      writer.uint32(392).int32(message.fileSetSpecType);
    }
    if (message.schema !== undefined) {
      TableSchema.encode(message.schema, writer.uint32(18).fork()).join();
    }
    if (message.destinationTable !== undefined) {
      TableReference.encode(message.destinationTable, writer.uint32(26).fork()).join();
    }
    if (message.destinationTableProperties !== undefined) {
      DestinationTableProperties.encode(message.destinationTableProperties, writer.uint32(34).fork()).join();
    }
    if (message.createDisposition !== "") {
      writer.uint32(42).string(message.createDisposition);
    }
    if (message.writeDisposition !== "") {
      writer.uint32(50).string(message.writeDisposition);
    }
    if (message.nullMarker !== undefined) {
      StringValue.encode({ value: message.nullMarker! }, writer.uint32(58).fork()).join();
    }
    if (message.fieldDelimiter !== "") {
      writer.uint32(66).string(message.fieldDelimiter);
    }
    if (message.skipLeadingRows !== undefined) {
      Int32Value.encode({ value: message.skipLeadingRows! }, writer.uint32(74).fork()).join();
    }
    if (message.encoding !== "") {
      writer.uint32(82).string(message.encoding);
    }
    if (message.quote !== undefined) {
      StringValue.encode({ value: message.quote! }, writer.uint32(90).fork()).join();
    }
    if (message.maxBadRecords !== undefined) {
      Int32Value.encode({ value: message.maxBadRecords! }, writer.uint32(98).fork()).join();
    }
    if (message.allowQuotedNewlines !== undefined) {
      BoolValue.encode({ value: message.allowQuotedNewlines! }, writer.uint32(122).fork()).join();
    }
    if (message.sourceFormat !== "") {
      writer.uint32(130).string(message.sourceFormat);
    }
    if (message.allowJaggedRows !== undefined) {
      BoolValue.encode({ value: message.allowJaggedRows! }, writer.uint32(138).fork()).join();
    }
    if (message.ignoreUnknownValues !== undefined) {
      BoolValue.encode({ value: message.ignoreUnknownValues! }, writer.uint32(146).fork()).join();
    }
    for (const v of message.projectionFields) {
      writer.uint32(154).string(v!);
    }
    if (message.autodetect !== undefined) {
      BoolValue.encode({ value: message.autodetect! }, writer.uint32(162).fork()).join();
    }
    for (const v of message.schemaUpdateOptions) {
      writer.uint32(170).string(v!);
    }
    if (message.timePartitioning !== undefined) {
      TimePartitioning.encode(message.timePartitioning, writer.uint32(178).fork()).join();
    }
    if (message.rangePartitioning !== undefined) {
      RangePartitioning.encode(message.rangePartitioning, writer.uint32(210).fork()).join();
    }
    if (message.clustering !== undefined) {
      Clustering.encode(message.clustering, writer.uint32(186).fork()).join();
    }
    if (message.destinationEncryptionConfiguration !== undefined) {
      EncryptionConfiguration.encode(message.destinationEncryptionConfiguration, writer.uint32(194).fork()).join();
    }
    if (message.useAvroLogicalTypes !== undefined) {
      BoolValue.encode({ value: message.useAvroLogicalTypes! }, writer.uint32(202).fork()).join();
    }
    if (message.referenceFileSchemaUri !== undefined) {
      StringValue.encode({ value: message.referenceFileSchemaUri! }, writer.uint32(362).fork()).join();
    }
    if (message.hivePartitioningOptions !== undefined) {
      HivePartitioningOptions.encode(message.hivePartitioningOptions, writer.uint32(298).fork()).join();
    }
    writer.uint32(314).fork();
    for (const v of message.decimalTargetTypes) {
      writer.int32(v);
    }
    writer.join();
    if (message.jsonExtension !== 0) {
      writer.uint32(328).int32(message.jsonExtension);
    }
    if (message.parquetOptions !== undefined) {
      ParquetOptions.encode(message.parquetOptions, writer.uint32(338).fork()).join();
    }
    if (message.preserveAsciiControlCharacters !== undefined) {
      BoolValue.encode({ value: message.preserveAsciiControlCharacters! }, writer.uint32(354).fork()).join();
    }
    for (const v of message.connectionProperties) {
      ConnectionProperty.encode(v!, writer.uint32(370).fork()).join();
    }
    if (message.createSession !== undefined) {
      BoolValue.encode({ value: message.createSession! }, writer.uint32(378).fork()).join();
    }
    if (message.columnNameCharacterMap !== 0) {
      writer.uint32(400).int32(message.columnNameCharacterMap);
    }
    if (message.copyFilesOnly !== undefined) {
      BoolValue.encode({ value: message.copyFilesOnly! }, writer.uint32(410).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobConfigurationLoad {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobConfigurationLoad();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.sourceUris.push(reader.string());
          continue;
        case 49:
          if (tag !== 392) {
            break;
          }

          message.fileSetSpecType = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.schema = TableSchema.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.destinationTable = TableReference.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.destinationTableProperties = DestinationTableProperties.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.createDisposition = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.writeDisposition = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.nullMarker = StringValue.decode(reader, reader.uint32()).value;
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.fieldDelimiter = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.skipLeadingRows = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.encoding = reader.string();
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.quote = StringValue.decode(reader, reader.uint32()).value;
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.maxBadRecords = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.allowQuotedNewlines = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.sourceFormat = reader.string();
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.allowJaggedRows = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.ignoreUnknownValues = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.projectionFields.push(reader.string());
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.autodetect = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 21:
          if (tag !== 170) {
            break;
          }

          message.schemaUpdateOptions.push(reader.string());
          continue;
        case 22:
          if (tag !== 178) {
            break;
          }

          message.timePartitioning = TimePartitioning.decode(reader, reader.uint32());
          continue;
        case 26:
          if (tag !== 210) {
            break;
          }

          message.rangePartitioning = RangePartitioning.decode(reader, reader.uint32());
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.clustering = Clustering.decode(reader, reader.uint32());
          continue;
        case 24:
          if (tag !== 194) {
            break;
          }

          message.destinationEncryptionConfiguration = EncryptionConfiguration.decode(reader, reader.uint32());
          continue;
        case 25:
          if (tag !== 202) {
            break;
          }

          message.useAvroLogicalTypes = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 45:
          if (tag !== 362) {
            break;
          }

          message.referenceFileSchemaUri = StringValue.decode(reader, reader.uint32()).value;
          continue;
        case 37:
          if (tag !== 298) {
            break;
          }

          message.hivePartitioningOptions = HivePartitioningOptions.decode(reader, reader.uint32());
          continue;
        case 39:
          if (tag === 312) {
            message.decimalTargetTypes.push(reader.int32() as any);

            continue;
          }

          if (tag === 314) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.decimalTargetTypes.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 41:
          if (tag !== 328) {
            break;
          }

          message.jsonExtension = reader.int32() as any;
          continue;
        case 42:
          if (tag !== 338) {
            break;
          }

          message.parquetOptions = ParquetOptions.decode(reader, reader.uint32());
          continue;
        case 44:
          if (tag !== 354) {
            break;
          }

          message.preserveAsciiControlCharacters = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 46:
          if (tag !== 370) {
            break;
          }

          message.connectionProperties.push(ConnectionProperty.decode(reader, reader.uint32()));
          continue;
        case 47:
          if (tag !== 378) {
            break;
          }

          message.createSession = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 50:
          if (tag !== 400) {
            break;
          }

          message.columnNameCharacterMap = reader.int32() as any;
          continue;
        case 51:
          if (tag !== 410) {
            break;
          }

          message.copyFilesOnly = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobConfigurationLoad {
    return {
      sourceUris: globalThis.Array.isArray(object?.sourceUris)
        ? object.sourceUris.map((e: any) => globalThis.String(e))
        : [],
      fileSetSpecType: isSet(object.fileSetSpecType) ? fileSetSpecTypeFromJSON(object.fileSetSpecType) : 0,
      schema: isSet(object.schema) ? TableSchema.fromJSON(object.schema) : undefined,
      destinationTable: isSet(object.destinationTable) ? TableReference.fromJSON(object.destinationTable) : undefined,
      destinationTableProperties: isSet(object.destinationTableProperties)
        ? DestinationTableProperties.fromJSON(object.destinationTableProperties)
        : undefined,
      createDisposition: isSet(object.createDisposition) ? globalThis.String(object.createDisposition) : "",
      writeDisposition: isSet(object.writeDisposition) ? globalThis.String(object.writeDisposition) : "",
      nullMarker: isSet(object.nullMarker) ? String(object.nullMarker) : undefined,
      fieldDelimiter: isSet(object.fieldDelimiter) ? globalThis.String(object.fieldDelimiter) : "",
      skipLeadingRows: isSet(object.skipLeadingRows) ? Number(object.skipLeadingRows) : undefined,
      encoding: isSet(object.encoding) ? globalThis.String(object.encoding) : "",
      quote: isSet(object.quote) ? String(object.quote) : undefined,
      maxBadRecords: isSet(object.maxBadRecords) ? Number(object.maxBadRecords) : undefined,
      allowQuotedNewlines: isSet(object.allowQuotedNewlines) ? Boolean(object.allowQuotedNewlines) : undefined,
      sourceFormat: isSet(object.sourceFormat) ? globalThis.String(object.sourceFormat) : "",
      allowJaggedRows: isSet(object.allowJaggedRows) ? Boolean(object.allowJaggedRows) : undefined,
      ignoreUnknownValues: isSet(object.ignoreUnknownValues) ? Boolean(object.ignoreUnknownValues) : undefined,
      projectionFields: globalThis.Array.isArray(object?.projectionFields)
        ? object.projectionFields.map((e: any) => globalThis.String(e))
        : [],
      autodetect: isSet(object.autodetect) ? Boolean(object.autodetect) : undefined,
      schemaUpdateOptions: globalThis.Array.isArray(object?.schemaUpdateOptions)
        ? object.schemaUpdateOptions.map((e: any) => globalThis.String(e))
        : [],
      timePartitioning: isSet(object.timePartitioning) ? TimePartitioning.fromJSON(object.timePartitioning) : undefined,
      rangePartitioning: isSet(object.rangePartitioning)
        ? RangePartitioning.fromJSON(object.rangePartitioning)
        : undefined,
      clustering: isSet(object.clustering) ? Clustering.fromJSON(object.clustering) : undefined,
      destinationEncryptionConfiguration: isSet(object.destinationEncryptionConfiguration)
        ? EncryptionConfiguration.fromJSON(object.destinationEncryptionConfiguration)
        : undefined,
      useAvroLogicalTypes: isSet(object.useAvroLogicalTypes) ? Boolean(object.useAvroLogicalTypes) : undefined,
      referenceFileSchemaUri: isSet(object.referenceFileSchemaUri) ? String(object.referenceFileSchemaUri) : undefined,
      hivePartitioningOptions: isSet(object.hivePartitioningOptions)
        ? HivePartitioningOptions.fromJSON(object.hivePartitioningOptions)
        : undefined,
      decimalTargetTypes: globalThis.Array.isArray(object?.decimalTargetTypes)
        ? object.decimalTargetTypes.map((e: any) => decimalTargetTypeFromJSON(e))
        : [],
      jsonExtension: isSet(object.jsonExtension) ? jsonExtensionFromJSON(object.jsonExtension) : 0,
      parquetOptions: isSet(object.parquetOptions) ? ParquetOptions.fromJSON(object.parquetOptions) : undefined,
      preserveAsciiControlCharacters: isSet(object.preserveAsciiControlCharacters)
        ? Boolean(object.preserveAsciiControlCharacters)
        : undefined,
      connectionProperties: globalThis.Array.isArray(object?.connectionProperties)
        ? object.connectionProperties.map((e: any) => ConnectionProperty.fromJSON(e))
        : [],
      createSession: isSet(object.createSession) ? Boolean(object.createSession) : undefined,
      columnNameCharacterMap: isSet(object.columnNameCharacterMap)
        ? jobConfigurationLoad_ColumnNameCharacterMapFromJSON(object.columnNameCharacterMap)
        : 0,
      copyFilesOnly: isSet(object.copyFilesOnly) ? Boolean(object.copyFilesOnly) : undefined,
    };
  },

  toJSON(message: JobConfigurationLoad): unknown {
    const obj: any = {};
    if (message.sourceUris?.length) {
      obj.sourceUris = message.sourceUris;
    }
    if (message.fileSetSpecType !== 0) {
      obj.fileSetSpecType = fileSetSpecTypeToJSON(message.fileSetSpecType);
    }
    if (message.schema !== undefined) {
      obj.schema = TableSchema.toJSON(message.schema);
    }
    if (message.destinationTable !== undefined) {
      obj.destinationTable = TableReference.toJSON(message.destinationTable);
    }
    if (message.destinationTableProperties !== undefined) {
      obj.destinationTableProperties = DestinationTableProperties.toJSON(message.destinationTableProperties);
    }
    if (message.createDisposition !== "") {
      obj.createDisposition = message.createDisposition;
    }
    if (message.writeDisposition !== "") {
      obj.writeDisposition = message.writeDisposition;
    }
    if (message.nullMarker !== undefined) {
      obj.nullMarker = message.nullMarker;
    }
    if (message.fieldDelimiter !== "") {
      obj.fieldDelimiter = message.fieldDelimiter;
    }
    if (message.skipLeadingRows !== undefined) {
      obj.skipLeadingRows = message.skipLeadingRows;
    }
    if (message.encoding !== "") {
      obj.encoding = message.encoding;
    }
    if (message.quote !== undefined) {
      obj.quote = message.quote;
    }
    if (message.maxBadRecords !== undefined) {
      obj.maxBadRecords = message.maxBadRecords;
    }
    if (message.allowQuotedNewlines !== undefined) {
      obj.allowQuotedNewlines = message.allowQuotedNewlines;
    }
    if (message.sourceFormat !== "") {
      obj.sourceFormat = message.sourceFormat;
    }
    if (message.allowJaggedRows !== undefined) {
      obj.allowJaggedRows = message.allowJaggedRows;
    }
    if (message.ignoreUnknownValues !== undefined) {
      obj.ignoreUnknownValues = message.ignoreUnknownValues;
    }
    if (message.projectionFields?.length) {
      obj.projectionFields = message.projectionFields;
    }
    if (message.autodetect !== undefined) {
      obj.autodetect = message.autodetect;
    }
    if (message.schemaUpdateOptions?.length) {
      obj.schemaUpdateOptions = message.schemaUpdateOptions;
    }
    if (message.timePartitioning !== undefined) {
      obj.timePartitioning = TimePartitioning.toJSON(message.timePartitioning);
    }
    if (message.rangePartitioning !== undefined) {
      obj.rangePartitioning = RangePartitioning.toJSON(message.rangePartitioning);
    }
    if (message.clustering !== undefined) {
      obj.clustering = Clustering.toJSON(message.clustering);
    }
    if (message.destinationEncryptionConfiguration !== undefined) {
      obj.destinationEncryptionConfiguration = EncryptionConfiguration.toJSON(
        message.destinationEncryptionConfiguration,
      );
    }
    if (message.useAvroLogicalTypes !== undefined) {
      obj.useAvroLogicalTypes = message.useAvroLogicalTypes;
    }
    if (message.referenceFileSchemaUri !== undefined) {
      obj.referenceFileSchemaUri = message.referenceFileSchemaUri;
    }
    if (message.hivePartitioningOptions !== undefined) {
      obj.hivePartitioningOptions = HivePartitioningOptions.toJSON(message.hivePartitioningOptions);
    }
    if (message.decimalTargetTypes?.length) {
      obj.decimalTargetTypes = message.decimalTargetTypes.map((e) => decimalTargetTypeToJSON(e));
    }
    if (message.jsonExtension !== 0) {
      obj.jsonExtension = jsonExtensionToJSON(message.jsonExtension);
    }
    if (message.parquetOptions !== undefined) {
      obj.parquetOptions = ParquetOptions.toJSON(message.parquetOptions);
    }
    if (message.preserveAsciiControlCharacters !== undefined) {
      obj.preserveAsciiControlCharacters = message.preserveAsciiControlCharacters;
    }
    if (message.connectionProperties?.length) {
      obj.connectionProperties = message.connectionProperties.map((e) => ConnectionProperty.toJSON(e));
    }
    if (message.createSession !== undefined) {
      obj.createSession = message.createSession;
    }
    if (message.columnNameCharacterMap !== 0) {
      obj.columnNameCharacterMap = jobConfigurationLoad_ColumnNameCharacterMapToJSON(message.columnNameCharacterMap);
    }
    if (message.copyFilesOnly !== undefined) {
      obj.copyFilesOnly = message.copyFilesOnly;
    }
    return obj;
  },

  create(base?: DeepPartial<JobConfigurationLoad>): JobConfigurationLoad {
    return JobConfigurationLoad.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobConfigurationLoad>): JobConfigurationLoad {
    const message = createBaseJobConfigurationLoad();
    message.sourceUris = object.sourceUris?.map((e) => e) || [];
    message.fileSetSpecType = object.fileSetSpecType ?? 0;
    message.schema = (object.schema !== undefined && object.schema !== null)
      ? TableSchema.fromPartial(object.schema)
      : undefined;
    message.destinationTable = (object.destinationTable !== undefined && object.destinationTable !== null)
      ? TableReference.fromPartial(object.destinationTable)
      : undefined;
    message.destinationTableProperties =
      (object.destinationTableProperties !== undefined && object.destinationTableProperties !== null)
        ? DestinationTableProperties.fromPartial(object.destinationTableProperties)
        : undefined;
    message.createDisposition = object.createDisposition ?? "";
    message.writeDisposition = object.writeDisposition ?? "";
    message.nullMarker = object.nullMarker ?? undefined;
    message.fieldDelimiter = object.fieldDelimiter ?? "";
    message.skipLeadingRows = object.skipLeadingRows ?? undefined;
    message.encoding = object.encoding ?? "";
    message.quote = object.quote ?? undefined;
    message.maxBadRecords = object.maxBadRecords ?? undefined;
    message.allowQuotedNewlines = object.allowQuotedNewlines ?? undefined;
    message.sourceFormat = object.sourceFormat ?? "";
    message.allowJaggedRows = object.allowJaggedRows ?? undefined;
    message.ignoreUnknownValues = object.ignoreUnknownValues ?? undefined;
    message.projectionFields = object.projectionFields?.map((e) => e) || [];
    message.autodetect = object.autodetect ?? undefined;
    message.schemaUpdateOptions = object.schemaUpdateOptions?.map((e) => e) || [];
    message.timePartitioning = (object.timePartitioning !== undefined && object.timePartitioning !== null)
      ? TimePartitioning.fromPartial(object.timePartitioning)
      : undefined;
    message.rangePartitioning = (object.rangePartitioning !== undefined && object.rangePartitioning !== null)
      ? RangePartitioning.fromPartial(object.rangePartitioning)
      : undefined;
    message.clustering = (object.clustering !== undefined && object.clustering !== null)
      ? Clustering.fromPartial(object.clustering)
      : undefined;
    message.destinationEncryptionConfiguration =
      (object.destinationEncryptionConfiguration !== undefined && object.destinationEncryptionConfiguration !== null)
        ? EncryptionConfiguration.fromPartial(object.destinationEncryptionConfiguration)
        : undefined;
    message.useAvroLogicalTypes = object.useAvroLogicalTypes ?? undefined;
    message.referenceFileSchemaUri = object.referenceFileSchemaUri ?? undefined;
    message.hivePartitioningOptions =
      (object.hivePartitioningOptions !== undefined && object.hivePartitioningOptions !== null)
        ? HivePartitioningOptions.fromPartial(object.hivePartitioningOptions)
        : undefined;
    message.decimalTargetTypes = object.decimalTargetTypes?.map((e) => e) || [];
    message.jsonExtension = object.jsonExtension ?? 0;
    message.parquetOptions = (object.parquetOptions !== undefined && object.parquetOptions !== null)
      ? ParquetOptions.fromPartial(object.parquetOptions)
      : undefined;
    message.preserveAsciiControlCharacters = object.preserveAsciiControlCharacters ?? undefined;
    message.connectionProperties = object.connectionProperties?.map((e) => ConnectionProperty.fromPartial(e)) || [];
    message.createSession = object.createSession ?? undefined;
    message.columnNameCharacterMap = object.columnNameCharacterMap ?? 0;
    message.copyFilesOnly = object.copyFilesOnly ?? undefined;
    return message;
  },
};

function createBaseJobConfigurationTableCopy(): JobConfigurationTableCopy {
  return {
    sourceTable: undefined,
    sourceTables: [],
    destinationTable: undefined,
    createDisposition: "",
    writeDisposition: "",
    destinationEncryptionConfiguration: undefined,
    operationType: 0,
    destinationExpirationTime: undefined,
  };
}

export const JobConfigurationTableCopy: MessageFns<JobConfigurationTableCopy> = {
  encode(message: JobConfigurationTableCopy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.sourceTable !== undefined) {
      TableReference.encode(message.sourceTable, writer.uint32(10).fork()).join();
    }
    for (const v of message.sourceTables) {
      TableReference.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.destinationTable !== undefined) {
      TableReference.encode(message.destinationTable, writer.uint32(26).fork()).join();
    }
    if (message.createDisposition !== "") {
      writer.uint32(34).string(message.createDisposition);
    }
    if (message.writeDisposition !== "") {
      writer.uint32(42).string(message.writeDisposition);
    }
    if (message.destinationEncryptionConfiguration !== undefined) {
      EncryptionConfiguration.encode(message.destinationEncryptionConfiguration, writer.uint32(50).fork()).join();
    }
    if (message.operationType !== 0) {
      writer.uint32(64).int32(message.operationType);
    }
    if (message.destinationExpirationTime !== undefined) {
      Timestamp.encode(toTimestamp(message.destinationExpirationTime), writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobConfigurationTableCopy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobConfigurationTableCopy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.sourceTable = TableReference.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.sourceTables.push(TableReference.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.destinationTable = TableReference.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.createDisposition = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.writeDisposition = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.destinationEncryptionConfiguration = EncryptionConfiguration.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.operationType = reader.int32() as any;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.destinationExpirationTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobConfigurationTableCopy {
    return {
      sourceTable: isSet(object.sourceTable) ? TableReference.fromJSON(object.sourceTable) : undefined,
      sourceTables: globalThis.Array.isArray(object?.sourceTables)
        ? object.sourceTables.map((e: any) => TableReference.fromJSON(e))
        : [],
      destinationTable: isSet(object.destinationTable) ? TableReference.fromJSON(object.destinationTable) : undefined,
      createDisposition: isSet(object.createDisposition) ? globalThis.String(object.createDisposition) : "",
      writeDisposition: isSet(object.writeDisposition) ? globalThis.String(object.writeDisposition) : "",
      destinationEncryptionConfiguration: isSet(object.destinationEncryptionConfiguration)
        ? EncryptionConfiguration.fromJSON(object.destinationEncryptionConfiguration)
        : undefined,
      operationType: isSet(object.operationType)
        ? jobConfigurationTableCopy_OperationTypeFromJSON(object.operationType)
        : 0,
      destinationExpirationTime: isSet(object.destinationExpirationTime)
        ? fromJsonTimestamp(object.destinationExpirationTime)
        : undefined,
    };
  },

  toJSON(message: JobConfigurationTableCopy): unknown {
    const obj: any = {};
    if (message.sourceTable !== undefined) {
      obj.sourceTable = TableReference.toJSON(message.sourceTable);
    }
    if (message.sourceTables?.length) {
      obj.sourceTables = message.sourceTables.map((e) => TableReference.toJSON(e));
    }
    if (message.destinationTable !== undefined) {
      obj.destinationTable = TableReference.toJSON(message.destinationTable);
    }
    if (message.createDisposition !== "") {
      obj.createDisposition = message.createDisposition;
    }
    if (message.writeDisposition !== "") {
      obj.writeDisposition = message.writeDisposition;
    }
    if (message.destinationEncryptionConfiguration !== undefined) {
      obj.destinationEncryptionConfiguration = EncryptionConfiguration.toJSON(
        message.destinationEncryptionConfiguration,
      );
    }
    if (message.operationType !== 0) {
      obj.operationType = jobConfigurationTableCopy_OperationTypeToJSON(message.operationType);
    }
    if (message.destinationExpirationTime !== undefined) {
      obj.destinationExpirationTime = message.destinationExpirationTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<JobConfigurationTableCopy>): JobConfigurationTableCopy {
    return JobConfigurationTableCopy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobConfigurationTableCopy>): JobConfigurationTableCopy {
    const message = createBaseJobConfigurationTableCopy();
    message.sourceTable = (object.sourceTable !== undefined && object.sourceTable !== null)
      ? TableReference.fromPartial(object.sourceTable)
      : undefined;
    message.sourceTables = object.sourceTables?.map((e) => TableReference.fromPartial(e)) || [];
    message.destinationTable = (object.destinationTable !== undefined && object.destinationTable !== null)
      ? TableReference.fromPartial(object.destinationTable)
      : undefined;
    message.createDisposition = object.createDisposition ?? "";
    message.writeDisposition = object.writeDisposition ?? "";
    message.destinationEncryptionConfiguration =
      (object.destinationEncryptionConfiguration !== undefined && object.destinationEncryptionConfiguration !== null)
        ? EncryptionConfiguration.fromPartial(object.destinationEncryptionConfiguration)
        : undefined;
    message.operationType = object.operationType ?? 0;
    message.destinationExpirationTime = object.destinationExpirationTime ?? undefined;
    return message;
  },
};

function createBaseJobConfigurationExtract(): JobConfigurationExtract {
  return {
    sourceTable: undefined,
    sourceModel: undefined,
    destinationUris: [],
    printHeader: undefined,
    fieldDelimiter: "",
    destinationFormat: "",
    compression: "",
    useAvroLogicalTypes: undefined,
    modelExtractOptions: undefined,
  };
}

export const JobConfigurationExtract: MessageFns<JobConfigurationExtract> = {
  encode(message: JobConfigurationExtract, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.sourceTable !== undefined) {
      TableReference.encode(message.sourceTable, writer.uint32(10).fork()).join();
    }
    if (message.sourceModel !== undefined) {
      ModelReference.encode(message.sourceModel, writer.uint32(74).fork()).join();
    }
    for (const v of message.destinationUris) {
      writer.uint32(26).string(v!);
    }
    if (message.printHeader !== undefined) {
      BoolValue.encode({ value: message.printHeader! }, writer.uint32(34).fork()).join();
    }
    if (message.fieldDelimiter !== "") {
      writer.uint32(42).string(message.fieldDelimiter);
    }
    if (message.destinationFormat !== "") {
      writer.uint32(50).string(message.destinationFormat);
    }
    if (message.compression !== "") {
      writer.uint32(58).string(message.compression);
    }
    if (message.useAvroLogicalTypes !== undefined) {
      BoolValue.encode({ value: message.useAvroLogicalTypes! }, writer.uint32(106).fork()).join();
    }
    if (message.modelExtractOptions !== undefined) {
      JobConfigurationExtract_ModelExtractOptions.encode(message.modelExtractOptions, writer.uint32(114).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobConfigurationExtract {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobConfigurationExtract();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.sourceTable = TableReference.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.sourceModel = ModelReference.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.destinationUris.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.printHeader = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.fieldDelimiter = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.destinationFormat = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.compression = reader.string();
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.useAvroLogicalTypes = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.modelExtractOptions = JobConfigurationExtract_ModelExtractOptions.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobConfigurationExtract {
    return {
      sourceTable: isSet(object.sourceTable) ? TableReference.fromJSON(object.sourceTable) : undefined,
      sourceModel: isSet(object.sourceModel) ? ModelReference.fromJSON(object.sourceModel) : undefined,
      destinationUris: globalThis.Array.isArray(object?.destinationUris)
        ? object.destinationUris.map((e: any) => globalThis.String(e))
        : [],
      printHeader: isSet(object.printHeader) ? Boolean(object.printHeader) : undefined,
      fieldDelimiter: isSet(object.fieldDelimiter) ? globalThis.String(object.fieldDelimiter) : "",
      destinationFormat: isSet(object.destinationFormat) ? globalThis.String(object.destinationFormat) : "",
      compression: isSet(object.compression) ? globalThis.String(object.compression) : "",
      useAvroLogicalTypes: isSet(object.useAvroLogicalTypes) ? Boolean(object.useAvroLogicalTypes) : undefined,
      modelExtractOptions: isSet(object.modelExtractOptions)
        ? JobConfigurationExtract_ModelExtractOptions.fromJSON(object.modelExtractOptions)
        : undefined,
    };
  },

  toJSON(message: JobConfigurationExtract): unknown {
    const obj: any = {};
    if (message.sourceTable !== undefined) {
      obj.sourceTable = TableReference.toJSON(message.sourceTable);
    }
    if (message.sourceModel !== undefined) {
      obj.sourceModel = ModelReference.toJSON(message.sourceModel);
    }
    if (message.destinationUris?.length) {
      obj.destinationUris = message.destinationUris;
    }
    if (message.printHeader !== undefined) {
      obj.printHeader = message.printHeader;
    }
    if (message.fieldDelimiter !== "") {
      obj.fieldDelimiter = message.fieldDelimiter;
    }
    if (message.destinationFormat !== "") {
      obj.destinationFormat = message.destinationFormat;
    }
    if (message.compression !== "") {
      obj.compression = message.compression;
    }
    if (message.useAvroLogicalTypes !== undefined) {
      obj.useAvroLogicalTypes = message.useAvroLogicalTypes;
    }
    if (message.modelExtractOptions !== undefined) {
      obj.modelExtractOptions = JobConfigurationExtract_ModelExtractOptions.toJSON(message.modelExtractOptions);
    }
    return obj;
  },

  create(base?: DeepPartial<JobConfigurationExtract>): JobConfigurationExtract {
    return JobConfigurationExtract.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobConfigurationExtract>): JobConfigurationExtract {
    const message = createBaseJobConfigurationExtract();
    message.sourceTable = (object.sourceTable !== undefined && object.sourceTable !== null)
      ? TableReference.fromPartial(object.sourceTable)
      : undefined;
    message.sourceModel = (object.sourceModel !== undefined && object.sourceModel !== null)
      ? ModelReference.fromPartial(object.sourceModel)
      : undefined;
    message.destinationUris = object.destinationUris?.map((e) => e) || [];
    message.printHeader = object.printHeader ?? undefined;
    message.fieldDelimiter = object.fieldDelimiter ?? "";
    message.destinationFormat = object.destinationFormat ?? "";
    message.compression = object.compression ?? "";
    message.useAvroLogicalTypes = object.useAvroLogicalTypes ?? undefined;
    message.modelExtractOptions = (object.modelExtractOptions !== undefined && object.modelExtractOptions !== null)
      ? JobConfigurationExtract_ModelExtractOptions.fromPartial(object.modelExtractOptions)
      : undefined;
    return message;
  },
};

function createBaseJobConfigurationExtract_ModelExtractOptions(): JobConfigurationExtract_ModelExtractOptions {
  return { trialId: undefined };
}

export const JobConfigurationExtract_ModelExtractOptions: MessageFns<JobConfigurationExtract_ModelExtractOptions> = {
  encode(
    message: JobConfigurationExtract_ModelExtractOptions,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.trialId !== undefined) {
      Int64Value.encode({ value: message.trialId! }, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobConfigurationExtract_ModelExtractOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobConfigurationExtract_ModelExtractOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.trialId = Int64Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobConfigurationExtract_ModelExtractOptions {
    return { trialId: isSet(object.trialId) ? Long.fromValue(object.trialId) : undefined };
  },

  toJSON(message: JobConfigurationExtract_ModelExtractOptions): unknown {
    const obj: any = {};
    if (message.trialId !== undefined) {
      obj.trialId = message.trialId;
    }
    return obj;
  },

  create(base?: DeepPartial<JobConfigurationExtract_ModelExtractOptions>): JobConfigurationExtract_ModelExtractOptions {
    return JobConfigurationExtract_ModelExtractOptions.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<JobConfigurationExtract_ModelExtractOptions>,
  ): JobConfigurationExtract_ModelExtractOptions {
    const message = createBaseJobConfigurationExtract_ModelExtractOptions();
    message.trialId = (object.trialId !== undefined && object.trialId !== null)
      ? Long.fromValue(object.trialId)
      : undefined;
    return message;
  },
};

function createBaseJobConfiguration(): JobConfiguration {
  return {
    jobType: "",
    query: undefined,
    load: undefined,
    copy: undefined,
    extract: undefined,
    dryRun: undefined,
    jobTimeoutMs: undefined,
    labels: {},
  };
}

export const JobConfiguration: MessageFns<JobConfiguration> = {
  encode(message: JobConfiguration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.jobType !== "") {
      writer.uint32(66).string(message.jobType);
    }
    if (message.query !== undefined) {
      JobConfigurationQuery.encode(message.query, writer.uint32(10).fork()).join();
    }
    if (message.load !== undefined) {
      JobConfigurationLoad.encode(message.load, writer.uint32(18).fork()).join();
    }
    if (message.copy !== undefined) {
      JobConfigurationTableCopy.encode(message.copy, writer.uint32(26).fork()).join();
    }
    if (message.extract !== undefined) {
      JobConfigurationExtract.encode(message.extract, writer.uint32(34).fork()).join();
    }
    if (message.dryRun !== undefined) {
      BoolValue.encode({ value: message.dryRun! }, writer.uint32(42).fork()).join();
    }
    if (message.jobTimeoutMs !== undefined) {
      Int64Value.encode({ value: message.jobTimeoutMs! }, writer.uint32(50).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      JobConfiguration_LabelsEntry.encode({ key: key as any, value }, writer.uint32(58).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobConfiguration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobConfiguration();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 8:
          if (tag !== 66) {
            break;
          }

          message.jobType = reader.string();
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.query = JobConfigurationQuery.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.load = JobConfigurationLoad.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.copy = JobConfigurationTableCopy.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.extract = JobConfigurationExtract.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.dryRun = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.jobTimeoutMs = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          const entry7 = JobConfiguration_LabelsEntry.decode(reader, reader.uint32());
          if (entry7.value !== undefined) {
            message.labels[entry7.key] = entry7.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobConfiguration {
    return {
      jobType: isSet(object.jobType) ? globalThis.String(object.jobType) : "",
      query: isSet(object.query) ? JobConfigurationQuery.fromJSON(object.query) : undefined,
      load: isSet(object.load) ? JobConfigurationLoad.fromJSON(object.load) : undefined,
      copy: isSet(object.copy) ? JobConfigurationTableCopy.fromJSON(object.copy) : undefined,
      extract: isSet(object.extract) ? JobConfigurationExtract.fromJSON(object.extract) : undefined,
      dryRun: isSet(object.dryRun) ? Boolean(object.dryRun) : undefined,
      jobTimeoutMs: isSet(object.jobTimeoutMs) ? Long.fromValue(object.jobTimeoutMs) : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: JobConfiguration): unknown {
    const obj: any = {};
    if (message.jobType !== "") {
      obj.jobType = message.jobType;
    }
    if (message.query !== undefined) {
      obj.query = JobConfigurationQuery.toJSON(message.query);
    }
    if (message.load !== undefined) {
      obj.load = JobConfigurationLoad.toJSON(message.load);
    }
    if (message.copy !== undefined) {
      obj.copy = JobConfigurationTableCopy.toJSON(message.copy);
    }
    if (message.extract !== undefined) {
      obj.extract = JobConfigurationExtract.toJSON(message.extract);
    }
    if (message.dryRun !== undefined) {
      obj.dryRun = message.dryRun;
    }
    if (message.jobTimeoutMs !== undefined) {
      obj.jobTimeoutMs = message.jobTimeoutMs;
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<JobConfiguration>): JobConfiguration {
    return JobConfiguration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobConfiguration>): JobConfiguration {
    const message = createBaseJobConfiguration();
    message.jobType = object.jobType ?? "";
    message.query = (object.query !== undefined && object.query !== null)
      ? JobConfigurationQuery.fromPartial(object.query)
      : undefined;
    message.load = (object.load !== undefined && object.load !== null)
      ? JobConfigurationLoad.fromPartial(object.load)
      : undefined;
    message.copy = (object.copy !== undefined && object.copy !== null)
      ? JobConfigurationTableCopy.fromPartial(object.copy)
      : undefined;
    message.extract = (object.extract !== undefined && object.extract !== null)
      ? JobConfigurationExtract.fromPartial(object.extract)
      : undefined;
    message.dryRun = object.dryRun ?? undefined;
    message.jobTimeoutMs = (object.jobTimeoutMs !== undefined && object.jobTimeoutMs !== null)
      ? Long.fromValue(object.jobTimeoutMs)
      : undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseJobConfiguration_LabelsEntry(): JobConfiguration_LabelsEntry {
  return { key: "", value: "" };
}

export const JobConfiguration_LabelsEntry: MessageFns<JobConfiguration_LabelsEntry> = {
  encode(message: JobConfiguration_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobConfiguration_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobConfiguration_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobConfiguration_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: JobConfiguration_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<JobConfiguration_LabelsEntry>): JobConfiguration_LabelsEntry {
    return JobConfiguration_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobConfiguration_LabelsEntry>): JobConfiguration_LabelsEntry {
    const message = createBaseJobConfiguration_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
