// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/bigquery/v2/dataset.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Empty } from "../../../protobuf/empty.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { BoolValue, Int64Value, StringValue, UInt32Value } from "../../../protobuf/wrappers.js";
import { DatasetReference } from "./dataset_reference.js";
import { EncryptionConfiguration } from "./encryption_config.js";
import { ExternalCatalogDatasetOptions } from "./external_catalog_dataset_options.js";
import { ExternalDatasetReference } from "./external_dataset_reference.js";
import { RestrictionConfig } from "./restriction_config.js";
import { RoutineReference } from "./routine_reference.js";
import { TableReference } from "./table_reference.js";
import {
  TableFieldSchema_RoundingMode,
  tableFieldSchema_RoundingModeFromJSON,
  tableFieldSchema_RoundingModeToJSON,
} from "./table_schema.js";

export const protobufPackage = "google.cloud.bigquery.v2";

/**
 * Grants all resources of particular types in a particular dataset read access
 * to the current dataset.
 *
 * Similar to how individually authorized views work, updates to any resource
 * granted through its dataset (including creation of new resources) requires
 * read permission to referenced resources, plus write permission to the
 * authorizing dataset.
 */
export interface DatasetAccessEntry {
  /** The dataset this entry applies to */
  dataset:
    | DatasetReference
    | undefined;
  /**
   * Which resources in the dataset this entry applies to. Currently, only
   * views are supported, but additional target types may be added in the
   * future.
   */
  targetTypes: DatasetAccessEntry_TargetType[];
}

/** Indicates the type of resources in a dataset that the entry applies to. */
export enum DatasetAccessEntry_TargetType {
  /** TARGET_TYPE_UNSPECIFIED - Do not use. You must set a target type explicitly. */
  TARGET_TYPE_UNSPECIFIED = 0,
  /** VIEWS - This entry applies to views in the dataset. */
  VIEWS = 1,
  /** ROUTINES - This entry applies to routines in the dataset. */
  ROUTINES = 2,
  UNRECOGNIZED = -1,
}

export function datasetAccessEntry_TargetTypeFromJSON(object: any): DatasetAccessEntry_TargetType {
  switch (object) {
    case 0:
    case "TARGET_TYPE_UNSPECIFIED":
      return DatasetAccessEntry_TargetType.TARGET_TYPE_UNSPECIFIED;
    case 1:
    case "VIEWS":
      return DatasetAccessEntry_TargetType.VIEWS;
    case 2:
    case "ROUTINES":
      return DatasetAccessEntry_TargetType.ROUTINES;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DatasetAccessEntry_TargetType.UNRECOGNIZED;
  }
}

export function datasetAccessEntry_TargetTypeToJSON(object: DatasetAccessEntry_TargetType): string {
  switch (object) {
    case DatasetAccessEntry_TargetType.TARGET_TYPE_UNSPECIFIED:
      return "TARGET_TYPE_UNSPECIFIED";
    case DatasetAccessEntry_TargetType.VIEWS:
      return "VIEWS";
    case DatasetAccessEntry_TargetType.ROUTINES:
      return "ROUTINES";
    case DatasetAccessEntry_TargetType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** An object that defines dataset access for an entity. */
export interface Access {
  /**
   * An IAM role ID that should be granted to the user, group,
   * or domain specified in this access entry.
   * The following legacy mappings will be applied:
   *
   * * `OWNER`: `roles/bigquery.dataOwner`
   * * `WRITER`: `roles/bigquery.dataEditor`
   * * `READER`: `roles/bigquery.dataViewer`
   *
   * This field will accept any of the above formats, but will return only
   * the legacy format. For example, if you set this field to
   * "roles/bigquery.dataOwner", it will be returned back as "OWNER".
   */
  role: string;
  /**
   * [Pick one] An email address of a user to grant access to. For example:
   * fred@example.com. Maps to IAM policy member "user:EMAIL" or
   * "serviceAccount:EMAIL".
   */
  userByEmail: string;
  /**
   * [Pick one] An email address of a Google Group to grant access to.
   * Maps to IAM policy member "group:GROUP".
   */
  groupByEmail: string;
  /**
   * [Pick one] A domain to grant access to. Any users signed in with the domain
   * specified will be granted the specified access. Example: "example.com".
   * Maps to IAM policy member "domain:DOMAIN".
   */
  domain: string;
  /**
   * [Pick one] A special group to grant access to. Possible values include:
   *
   *   * projectOwners: Owners of the enclosing project.
   *   * projectReaders: Readers of the enclosing project.
   *   * projectWriters: Writers of the enclosing project.
   *   * allAuthenticatedUsers: All authenticated BigQuery users.
   *
   * Maps to similarly-named IAM members.
   */
  specialGroup: string;
  /**
   * [Pick one] Some other type of member that appears in the IAM Policy but
   * isn't a user, group, domain, or special group.
   */
  iamMember: string;
  /**
   * [Pick one] A view from a different dataset to grant access to. Queries
   * executed against that view will have read access to views/tables/routines
   * in this dataset.
   * The role field is not required when this field is set. If that view is
   * updated by any user, access to the view needs to be granted again via an
   * update operation.
   */
  view:
    | TableReference
    | undefined;
  /**
   * [Pick one] A routine from a different dataset to grant access to. Queries
   * executed against that routine will have read access to
   * views/tables/routines in this dataset. Only UDF is supported for now.
   * The role field is not required when this field is set. If that routine is
   * updated by any user, access to the routine needs to be granted again via
   * an update operation.
   */
  routine:
    | RoutineReference
    | undefined;
  /**
   * [Pick one] A grant authorizing all resources of a particular type in a
   * particular dataset access to this dataset. Only views are supported for
   * now. The role field is not required when this field is set. If that dataset
   * is deleted and re-created, its access needs to be granted again via an
   * update operation.
   */
  dataset: DatasetAccessEntry | undefined;
}

/** Represents a BigQuery dataset. */
export interface Dataset {
  /** Output only. The resource type. */
  kind: string;
  /** Output only. A hash of the resource. */
  etag: string;
  /**
   * Output only. The fully-qualified unique name of the dataset in the format
   * projectId:datasetId. The dataset name without the project name is given in
   * the datasetId field. When creating a new dataset, leave this field blank,
   * and instead specify the datasetId field.
   */
  id: string;
  /**
   * Output only. A URL that can be used to access the resource again. You can
   * use this URL in Get or Update requests to the resource.
   */
  selfLink: string;
  /** Required. A reference that identifies the dataset. */
  datasetReference:
    | DatasetReference
    | undefined;
  /** Optional. A descriptive name for the dataset. */
  friendlyName:
    | string
    | undefined;
  /** Optional. A user-friendly description of the dataset. */
  description:
    | string
    | undefined;
  /**
   * Optional. The default lifetime of all tables in the dataset, in
   * milliseconds. The minimum lifetime value is 3600000 milliseconds (one
   * hour). To clear an existing default expiration with a PATCH request, set to
   * 0. Once this property is set, all newly-created tables in the dataset will
   * have an expirationTime property set to the creation time plus the value in
   * this property, and changing the value will only affect new tables, not
   * existing ones. When the expirationTime for a given table is reached, that
   * table will be deleted automatically.
   * If a table's expirationTime is modified or removed before the table
   * expires, or if you provide an explicit expirationTime when creating a
   * table, that value takes precedence over the default expiration time
   * indicated by this property.
   */
  defaultTableExpirationMs:
    | Long
    | undefined;
  /**
   * This default partition expiration, expressed in milliseconds.
   *
   * When new time-partitioned tables are created in a dataset where this
   * property is set, the table will inherit this value, propagated as the
   * `TimePartitioning.expirationMs` property on the new table.  If you set
   * `TimePartitioning.expirationMs` explicitly when creating a table,
   * the `defaultPartitionExpirationMs` of the containing dataset is ignored.
   *
   * When creating a partitioned table, if `defaultPartitionExpirationMs`
   * is set, the `defaultTableExpirationMs` value is ignored and the table
   * will not be inherit a table expiration deadline.
   */
  defaultPartitionExpirationMs:
    | Long
    | undefined;
  /**
   * The labels associated with this dataset. You can use these
   * to organize and group your datasets.
   * You can set this property when inserting or updating a dataset.
   * See [Creating and Updating Dataset
   * Labels](https://cloud.google.com/bigquery/docs/creating-managing-labels#creating_and_updating_dataset_labels)
   * for more information.
   */
  labels: { [key: string]: string };
  /**
   * Optional. An array of objects that define dataset access for one or more
   * entities. You can set this property when inserting or updating a dataset in
   * order to control who is allowed to access the data. If unspecified at
   * dataset creation time, BigQuery adds default dataset access for the
   * following entities: access.specialGroup: projectReaders; access.role:
   * READER; access.specialGroup: projectWriters; access.role: WRITER;
   * access.specialGroup: projectOwners; access.role: OWNER;
   * access.userByEmail: [dataset creator email]; access.role: OWNER;
   * If you patch a dataset, then this field is overwritten by the patched
   * dataset's access field. To add entities, you must supply the entire
   * existing access array in addition to any new entities that you want to add.
   */
  access: Access[];
  /**
   * Output only. The time when this dataset was created, in milliseconds since
   * the epoch.
   */
  creationTime: Long;
  /**
   * Output only. The date when this dataset was last modified, in milliseconds
   * since the epoch.
   */
  lastModifiedTime: Long;
  /**
   * The geographic location where the dataset should reside. See
   * https://cloud.google.com/bigquery/docs/locations for supported
   * locations.
   */
  location: string;
  /**
   * The default encryption key for all tables in the dataset.
   * After this property is set, the encryption key of all newly-created tables
   * in the dataset is set to this value unless the table creation request or
   * query explicitly overrides the key.
   */
  defaultEncryptionConfiguration:
    | EncryptionConfiguration
    | undefined;
  /** Output only. Reserved for future use. */
  satisfiesPzs:
    | boolean
    | undefined;
  /** Output only. Reserved for future use. */
  satisfiesPzi:
    | boolean
    | undefined;
  /**
   * Output only. Same as `type` in `ListFormatDataset`.
   * The type of the dataset, one of:
   *
   * * DEFAULT - only accessible by owner and authorized accounts,
   * * PUBLIC - accessible by everyone,
   * * LINKED - linked dataset,
   * * EXTERNAL - dataset with definition in external metadata catalog.
   */
  type: string;
  /**
   * Optional. The source dataset reference when the dataset is of type LINKED.
   * For all other dataset types it is not set. This field cannot be updated
   * once it is set. Any attempt to update this field using Update and Patch API
   * Operations will be ignored.
   */
  linkedDatasetSource:
    | LinkedDatasetSource
    | undefined;
  /**
   * Output only. Metadata about the LinkedDataset. Filled out when the dataset
   * type is LINKED.
   */
  linkedDatasetMetadata:
    | LinkedDatasetMetadata
    | undefined;
  /**
   * Optional. Reference to a read-only external dataset defined in data
   * catalogs outside of BigQuery. Filled out when the dataset type is EXTERNAL.
   */
  externalDatasetReference:
    | ExternalDatasetReference
    | undefined;
  /**
   * Optional. Options defining open source compatible datasets living in the
   * BigQuery catalog. Contains metadata of open source database, schema or
   * namespace represented by the current dataset.
   */
  externalCatalogDatasetOptions:
    | ExternalCatalogDatasetOptions
    | undefined;
  /**
   * Optional. TRUE if the dataset and its table names are case-insensitive,
   * otherwise FALSE. By default, this is FALSE, which means the dataset and its
   * table names are case-sensitive. This field does not affect routine
   * references.
   */
  isCaseInsensitive:
    | boolean
    | undefined;
  /**
   * Optional. Defines the default collation specification of future tables
   * created in the dataset. If a table is created in this dataset without
   * table-level default collation, then the table inherits the dataset default
   * collation, which is applied to the string fields that do not have explicit
   * collation specified. A change to this field affects only tables created
   * afterwards, and does not alter the existing tables.
   * The following values are supported:
   *
   * * 'und:ci': undetermined locale, case insensitive.
   * * '': empty string. Default to case-sensitive behavior.
   */
  defaultCollation:
    | string
    | undefined;
  /**
   * Optional. Defines the default rounding mode specification of new tables
   * created within this dataset. During table creation, if this field is
   * specified, the table within this dataset will inherit the default rounding
   * mode of the dataset. Setting the default rounding mode on a table overrides
   * this option. Existing tables in the dataset are unaffected.
   * If columns are defined during that table creation,
   * they will immediately inherit the table's default rounding mode,
   * unless otherwise specified.
   */
  defaultRoundingMode: TableFieldSchema_RoundingMode;
  /**
   * Optional. Defines the time travel window in hours. The value can be from 48
   * to 168 hours (2 to 7 days). The default value is 168 hours if this is not
   * set.
   */
  maxTimeTravelHours:
    | Long
    | undefined;
  /**
   * Output only. Tags for the dataset. To provide tags as inputs, use the
   * `resourceTags` field.
   *
   * @deprecated
   */
  tags: GcpTag[];
  /** Optional. Updates storage_billing_model for the dataset. */
  storageBillingModel: Dataset_StorageBillingModel;
  /**
   * Optional. Output only. Restriction config for all tables and dataset. If
   * set, restrict certain accesses on the dataset and all its tables based on
   * the config. See [Data
   * egress](https://cloud.google.com/bigquery/docs/analytics-hub-introduction#data_egress)
   * for more details.
   */
  restrictions:
    | RestrictionConfig
    | undefined;
  /**
   * Optional. The [tags](https://cloud.google.com/bigquery/docs/tags) attached
   * to this dataset. Tag keys are globally unique. Tag key is expected to be in
   * the namespaced format, for example "123456789012/environment" where
   * 123456789012 is the ID of the parent organization or project resource for
   * this tag key. Tag value is expected to be the short name, for example
   * "Production". See [Tag
   * definitions](https://cloud.google.com/iam/docs/tags-access-control#definitions)
   * for more details.
   */
  resourceTags: { [key: string]: string };
}

/** Indicates the billing model that will be applied to the dataset. */
export enum Dataset_StorageBillingModel {
  /** STORAGE_BILLING_MODEL_UNSPECIFIED - Value not set. */
  STORAGE_BILLING_MODEL_UNSPECIFIED = 0,
  /** LOGICAL - Billing for logical bytes. */
  LOGICAL = 1,
  /** PHYSICAL - Billing for physical bytes. */
  PHYSICAL = 2,
  UNRECOGNIZED = -1,
}

export function dataset_StorageBillingModelFromJSON(object: any): Dataset_StorageBillingModel {
  switch (object) {
    case 0:
    case "STORAGE_BILLING_MODEL_UNSPECIFIED":
      return Dataset_StorageBillingModel.STORAGE_BILLING_MODEL_UNSPECIFIED;
    case 1:
    case "LOGICAL":
      return Dataset_StorageBillingModel.LOGICAL;
    case 2:
    case "PHYSICAL":
      return Dataset_StorageBillingModel.PHYSICAL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Dataset_StorageBillingModel.UNRECOGNIZED;
  }
}

export function dataset_StorageBillingModelToJSON(object: Dataset_StorageBillingModel): string {
  switch (object) {
    case Dataset_StorageBillingModel.STORAGE_BILLING_MODEL_UNSPECIFIED:
      return "STORAGE_BILLING_MODEL_UNSPECIFIED";
    case Dataset_StorageBillingModel.LOGICAL:
      return "LOGICAL";
    case Dataset_StorageBillingModel.PHYSICAL:
      return "PHYSICAL";
    case Dataset_StorageBillingModel.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export interface Dataset_LabelsEntry {
  key: string;
  value: string;
}

export interface Dataset_ResourceTagsEntry {
  key: string;
  value: string;
}

/**
 * A global tag managed by Resource Manager.
 * https://cloud.google.com/iam/docs/tags-access-control#definitions
 */
export interface GcpTag {
  /**
   * Required. The namespaced friendly name of the tag key, e.g.
   * "12345/environment" where 12345 is org id.
   */
  tagKey: string;
  /** Required. The friendly short name of the tag value, e.g. "production". */
  tagValue: string;
}

/** A dataset source type which refers to another BigQuery dataset. */
export interface LinkedDatasetSource {
  /** The source dataset reference contains project numbers and not project ids. */
  sourceDataset: DatasetReference | undefined;
}

/** Metadata about the Linked Dataset. */
export interface LinkedDatasetMetadata {
  /**
   * Output only. Specifies whether Linked Dataset is currently in a linked
   * state or not.
   */
  linkState: LinkedDatasetMetadata_LinkState;
}

/** Specifies whether Linked Dataset is currently in a linked state or not. */
export enum LinkedDatasetMetadata_LinkState {
  /**
   * LINK_STATE_UNSPECIFIED - The default value.
   * Default to the LINKED state.
   */
  LINK_STATE_UNSPECIFIED = 0,
  /** LINKED - Normal Linked Dataset state. Data is queryable via the Linked Dataset. */
  LINKED = 1,
  /**
   * UNLINKED - Data publisher or owner has unlinked this Linked Dataset. It means you
   * can no longer query or see the data in the Linked Dataset.
   */
  UNLINKED = 2,
  UNRECOGNIZED = -1,
}

export function linkedDatasetMetadata_LinkStateFromJSON(object: any): LinkedDatasetMetadata_LinkState {
  switch (object) {
    case 0:
    case "LINK_STATE_UNSPECIFIED":
      return LinkedDatasetMetadata_LinkState.LINK_STATE_UNSPECIFIED;
    case 1:
    case "LINKED":
      return LinkedDatasetMetadata_LinkState.LINKED;
    case 2:
    case "UNLINKED":
      return LinkedDatasetMetadata_LinkState.UNLINKED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return LinkedDatasetMetadata_LinkState.UNRECOGNIZED;
  }
}

export function linkedDatasetMetadata_LinkStateToJSON(object: LinkedDatasetMetadata_LinkState): string {
  switch (object) {
    case LinkedDatasetMetadata_LinkState.LINK_STATE_UNSPECIFIED:
      return "LINK_STATE_UNSPECIFIED";
    case LinkedDatasetMetadata_LinkState.LINKED:
      return "LINKED";
    case LinkedDatasetMetadata_LinkState.UNLINKED:
      return "UNLINKED";
    case LinkedDatasetMetadata_LinkState.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Request format for getting information about a dataset. */
export interface GetDatasetRequest {
  /** Required. Project ID of the requested dataset */
  projectId: string;
  /** Required. Dataset ID of the requested dataset */
  datasetId: string;
  /**
   * Optional. Specifies the view that determines which dataset information is
   * returned. By default, metadata and ACL information are returned.
   */
  datasetView: GetDatasetRequest_DatasetView;
}

/** DatasetView specifies which dataset information is returned. */
export enum GetDatasetRequest_DatasetView {
  /**
   * DATASET_VIEW_UNSPECIFIED - The default value.
   * Default to the FULL view.
   */
  DATASET_VIEW_UNSPECIFIED = 0,
  /**
   * METADATA - Includes metadata information for the dataset, such as location,
   * etag, lastModifiedTime, etc.
   */
  METADATA = 1,
  /**
   * ACL - Includes ACL information for the dataset, which defines dataset access
   * for one or more entities.
   */
  ACL = 2,
  /** FULL - Includes both dataset metadata and ACL information. */
  FULL = 3,
  UNRECOGNIZED = -1,
}

export function getDatasetRequest_DatasetViewFromJSON(object: any): GetDatasetRequest_DatasetView {
  switch (object) {
    case 0:
    case "DATASET_VIEW_UNSPECIFIED":
      return GetDatasetRequest_DatasetView.DATASET_VIEW_UNSPECIFIED;
    case 1:
    case "METADATA":
      return GetDatasetRequest_DatasetView.METADATA;
    case 2:
    case "ACL":
      return GetDatasetRequest_DatasetView.ACL;
    case 3:
    case "FULL":
      return GetDatasetRequest_DatasetView.FULL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return GetDatasetRequest_DatasetView.UNRECOGNIZED;
  }
}

export function getDatasetRequest_DatasetViewToJSON(object: GetDatasetRequest_DatasetView): string {
  switch (object) {
    case GetDatasetRequest_DatasetView.DATASET_VIEW_UNSPECIFIED:
      return "DATASET_VIEW_UNSPECIFIED";
    case GetDatasetRequest_DatasetView.METADATA:
      return "METADATA";
    case GetDatasetRequest_DatasetView.ACL:
      return "ACL";
    case GetDatasetRequest_DatasetView.FULL:
      return "FULL";
    case GetDatasetRequest_DatasetView.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Request format for inserting a dataset. */
export interface InsertDatasetRequest {
  /** Required. Project ID of the new dataset */
  projectId: string;
  /** Required. Datasets resource to use for the new dataset */
  dataset: Dataset | undefined;
}

/** Message for updating or patching a dataset. */
export interface UpdateOrPatchDatasetRequest {
  /** Required. Project ID of the dataset being updated */
  projectId: string;
  /** Required. Dataset ID of the dataset being updated */
  datasetId: string;
  /**
   * Required. Datasets resource which will replace or patch the specified
   * dataset.
   */
  dataset: Dataset | undefined;
}

/** Request format for deleting a dataset. */
export interface DeleteDatasetRequest {
  /** Required. Project ID of the dataset being deleted */
  projectId: string;
  /** Required. Dataset ID of dataset being deleted */
  datasetId: string;
  /**
   * If True, delete all the tables in the dataset.
   * If False and the dataset contains tables, the request will fail.
   * Default is False
   */
  deleteContents: boolean;
}

export interface ListDatasetsRequest {
  /** Required. Project ID of the datasets to be listed */
  projectId: string;
  /**
   * The maximum number of results to return in a single response page.
   * Leverage the page tokens to iterate through the entire collection.
   */
  maxResults:
    | number
    | undefined;
  /**
   * Page token, returned by a previous call, to request the next page of
   * results
   */
  pageToken: string;
  /** Whether to list all datasets, including hidden ones */
  all: boolean;
  /**
   * An expression for filtering the results of the request by label.
   * The syntax is `labels.<name>[:<value>]`.
   * Multiple filters can be ANDed together by connecting with a space.
   * Example: `labels.department:receiving labels.active`.
   * See [Filtering datasets using
   * labels](https://cloud.google.com/bigquery/docs/filtering-labels#filtering_datasets_using_labels)
   * for details.
   */
  filter: string;
}

/**
 * A dataset resource with only a subset of fields, to be returned in a list of
 * datasets.
 */
export interface ListFormatDataset {
  /**
   * The resource type.
   * This property always returns the value "bigquery#dataset"
   */
  kind: string;
  /** The fully-qualified, unique, opaque ID of the dataset. */
  id: string;
  /**
   * The dataset reference.
   * Use this property to access specific parts of the dataset's ID, such as
   * project ID or dataset ID.
   */
  datasetReference:
    | DatasetReference
    | undefined;
  /**
   * The labels associated with this dataset.
   * You can use these to organize and group your datasets.
   */
  labels: { [key: string]: string };
  /**
   * An alternate name for the dataset.  The friendly name is purely
   * decorative in nature.
   */
  friendlyName:
    | string
    | undefined;
  /** The geographic location where the dataset resides. */
  location: string;
}

export interface ListFormatDataset_LabelsEntry {
  key: string;
  value: string;
}

/** Response format for a page of results when listing datasets. */
export interface DatasetList {
  /**
   * Output only. The resource type.
   * This property always returns the value "bigquery#datasetList"
   */
  kind: string;
  /**
   * Output only. A hash value of the results page. You can use this property to
   * determine if the page has changed since the last request.
   */
  etag: string;
  /**
   * A token that can be used to request the next results page. This property is
   * omitted on the final results page.
   */
  nextPageToken: string;
  /**
   * An array of the dataset resources in the project.
   * Each resource contains basic information.
   * For full information about a particular dataset resource, use the Datasets:
   * get method. This property is omitted when there are no datasets in the
   * project.
   */
  datasets: ListFormatDataset[];
  /**
   * A list of skipped locations that were unreachable. For more information
   * about BigQuery locations, see:
   * https://cloud.google.com/bigquery/docs/locations. Example: "europe-west5"
   */
  unreachable: string[];
}

/** Request format for undeleting a dataset. */
export interface UndeleteDatasetRequest {
  /** Required. Project ID of the dataset to be undeleted */
  projectId: string;
  /** Required. Dataset ID of dataset being deleted */
  datasetId: string;
  /**
   * Optional. The exact time when the dataset was deleted. If not specified,
   * the most recently deleted version is undeleted. Undeleting a dataset
   * using deletion time is not supported.
   */
  deletionTime: Date | undefined;
}

function createBaseDatasetAccessEntry(): DatasetAccessEntry {
  return { dataset: undefined, targetTypes: [] };
}

export const DatasetAccessEntry: MessageFns<DatasetAccessEntry> = {
  encode(message: DatasetAccessEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dataset !== undefined) {
      DatasetReference.encode(message.dataset, writer.uint32(10).fork()).join();
    }
    writer.uint32(18).fork();
    for (const v of message.targetTypes) {
      writer.int32(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatasetAccessEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatasetAccessEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dataset = DatasetReference.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag === 16) {
            message.targetTypes.push(reader.int32() as any);

            continue;
          }

          if (tag === 18) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.targetTypes.push(reader.int32() as any);
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatasetAccessEntry {
    return {
      dataset: isSet(object.dataset) ? DatasetReference.fromJSON(object.dataset) : undefined,
      targetTypes: globalThis.Array.isArray(object?.targetTypes)
        ? object.targetTypes.map((e: any) => datasetAccessEntry_TargetTypeFromJSON(e))
        : [],
    };
  },

  toJSON(message: DatasetAccessEntry): unknown {
    const obj: any = {};
    if (message.dataset !== undefined) {
      obj.dataset = DatasetReference.toJSON(message.dataset);
    }
    if (message.targetTypes?.length) {
      obj.targetTypes = message.targetTypes.map((e) => datasetAccessEntry_TargetTypeToJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<DatasetAccessEntry>): DatasetAccessEntry {
    return DatasetAccessEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatasetAccessEntry>): DatasetAccessEntry {
    const message = createBaseDatasetAccessEntry();
    message.dataset = (object.dataset !== undefined && object.dataset !== null)
      ? DatasetReference.fromPartial(object.dataset)
      : undefined;
    message.targetTypes = object.targetTypes?.map((e) => e) || [];
    return message;
  },
};

function createBaseAccess(): Access {
  return {
    role: "",
    userByEmail: "",
    groupByEmail: "",
    domain: "",
    specialGroup: "",
    iamMember: "",
    view: undefined,
    routine: undefined,
    dataset: undefined,
  };
}

export const Access: MessageFns<Access> = {
  encode(message: Access, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.role !== "") {
      writer.uint32(10).string(message.role);
    }
    if (message.userByEmail !== "") {
      writer.uint32(18).string(message.userByEmail);
    }
    if (message.groupByEmail !== "") {
      writer.uint32(26).string(message.groupByEmail);
    }
    if (message.domain !== "") {
      writer.uint32(34).string(message.domain);
    }
    if (message.specialGroup !== "") {
      writer.uint32(42).string(message.specialGroup);
    }
    if (message.iamMember !== "") {
      writer.uint32(58).string(message.iamMember);
    }
    if (message.view !== undefined) {
      TableReference.encode(message.view, writer.uint32(50).fork()).join();
    }
    if (message.routine !== undefined) {
      RoutineReference.encode(message.routine, writer.uint32(66).fork()).join();
    }
    if (message.dataset !== undefined) {
      DatasetAccessEntry.encode(message.dataset, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Access {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAccess();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.role = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.userByEmail = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.groupByEmail = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.domain = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.specialGroup = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.iamMember = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.view = TableReference.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.routine = RoutineReference.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.dataset = DatasetAccessEntry.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Access {
    return {
      role: isSet(object.role) ? globalThis.String(object.role) : "",
      userByEmail: isSet(object.userByEmail) ? globalThis.String(object.userByEmail) : "",
      groupByEmail: isSet(object.groupByEmail) ? globalThis.String(object.groupByEmail) : "",
      domain: isSet(object.domain) ? globalThis.String(object.domain) : "",
      specialGroup: isSet(object.specialGroup) ? globalThis.String(object.specialGroup) : "",
      iamMember: isSet(object.iamMember) ? globalThis.String(object.iamMember) : "",
      view: isSet(object.view) ? TableReference.fromJSON(object.view) : undefined,
      routine: isSet(object.routine) ? RoutineReference.fromJSON(object.routine) : undefined,
      dataset: isSet(object.dataset) ? DatasetAccessEntry.fromJSON(object.dataset) : undefined,
    };
  },

  toJSON(message: Access): unknown {
    const obj: any = {};
    if (message.role !== "") {
      obj.role = message.role;
    }
    if (message.userByEmail !== "") {
      obj.userByEmail = message.userByEmail;
    }
    if (message.groupByEmail !== "") {
      obj.groupByEmail = message.groupByEmail;
    }
    if (message.domain !== "") {
      obj.domain = message.domain;
    }
    if (message.specialGroup !== "") {
      obj.specialGroup = message.specialGroup;
    }
    if (message.iamMember !== "") {
      obj.iamMember = message.iamMember;
    }
    if (message.view !== undefined) {
      obj.view = TableReference.toJSON(message.view);
    }
    if (message.routine !== undefined) {
      obj.routine = RoutineReference.toJSON(message.routine);
    }
    if (message.dataset !== undefined) {
      obj.dataset = DatasetAccessEntry.toJSON(message.dataset);
    }
    return obj;
  },

  create(base?: DeepPartial<Access>): Access {
    return Access.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Access>): Access {
    const message = createBaseAccess();
    message.role = object.role ?? "";
    message.userByEmail = object.userByEmail ?? "";
    message.groupByEmail = object.groupByEmail ?? "";
    message.domain = object.domain ?? "";
    message.specialGroup = object.specialGroup ?? "";
    message.iamMember = object.iamMember ?? "";
    message.view = (object.view !== undefined && object.view !== null)
      ? TableReference.fromPartial(object.view)
      : undefined;
    message.routine = (object.routine !== undefined && object.routine !== null)
      ? RoutineReference.fromPartial(object.routine)
      : undefined;
    message.dataset = (object.dataset !== undefined && object.dataset !== null)
      ? DatasetAccessEntry.fromPartial(object.dataset)
      : undefined;
    return message;
  },
};

function createBaseDataset(): Dataset {
  return {
    kind: "",
    etag: "",
    id: "",
    selfLink: "",
    datasetReference: undefined,
    friendlyName: undefined,
    description: undefined,
    defaultTableExpirationMs: undefined,
    defaultPartitionExpirationMs: undefined,
    labels: {},
    access: [],
    creationTime: Long.ZERO,
    lastModifiedTime: Long.ZERO,
    location: "",
    defaultEncryptionConfiguration: undefined,
    satisfiesPzs: undefined,
    satisfiesPzi: undefined,
    type: "",
    linkedDatasetSource: undefined,
    linkedDatasetMetadata: undefined,
    externalDatasetReference: undefined,
    externalCatalogDatasetOptions: undefined,
    isCaseInsensitive: undefined,
    defaultCollation: undefined,
    defaultRoundingMode: 0,
    maxTimeTravelHours: undefined,
    tags: [],
    storageBillingModel: 0,
    restrictions: undefined,
    resourceTags: {},
  };
}

export const Dataset: MessageFns<Dataset> = {
  encode(message: Dataset, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.etag !== "") {
      writer.uint32(18).string(message.etag);
    }
    if (message.id !== "") {
      writer.uint32(26).string(message.id);
    }
    if (message.selfLink !== "") {
      writer.uint32(34).string(message.selfLink);
    }
    if (message.datasetReference !== undefined) {
      DatasetReference.encode(message.datasetReference, writer.uint32(42).fork()).join();
    }
    if (message.friendlyName !== undefined) {
      StringValue.encode({ value: message.friendlyName! }, writer.uint32(50).fork()).join();
    }
    if (message.description !== undefined) {
      StringValue.encode({ value: message.description! }, writer.uint32(58).fork()).join();
    }
    if (message.defaultTableExpirationMs !== undefined) {
      Int64Value.encode({ value: message.defaultTableExpirationMs! }, writer.uint32(66).fork()).join();
    }
    if (message.defaultPartitionExpirationMs !== undefined) {
      Int64Value.encode({ value: message.defaultPartitionExpirationMs! }, writer.uint32(114).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      Dataset_LabelsEntry.encode({ key: key as any, value }, writer.uint32(74).fork()).join();
    });
    for (const v of message.access) {
      Access.encode(v!, writer.uint32(82).fork()).join();
    }
    if (!message.creationTime.equals(Long.ZERO)) {
      writer.uint32(88).int64(message.creationTime.toString());
    }
    if (!message.lastModifiedTime.equals(Long.ZERO)) {
      writer.uint32(96).int64(message.lastModifiedTime.toString());
    }
    if (message.location !== "") {
      writer.uint32(106).string(message.location);
    }
    if (message.defaultEncryptionConfiguration !== undefined) {
      EncryptionConfiguration.encode(message.defaultEncryptionConfiguration, writer.uint32(130).fork()).join();
    }
    if (message.satisfiesPzs !== undefined) {
      BoolValue.encode({ value: message.satisfiesPzs! }, writer.uint32(138).fork()).join();
    }
    if (message.satisfiesPzi !== undefined) {
      BoolValue.encode({ value: message.satisfiesPzi! }, writer.uint32(250).fork()).join();
    }
    if (message.type !== "") {
      writer.uint32(146).string(message.type);
    }
    if (message.linkedDatasetSource !== undefined) {
      LinkedDatasetSource.encode(message.linkedDatasetSource, writer.uint32(154).fork()).join();
    }
    if (message.linkedDatasetMetadata !== undefined) {
      LinkedDatasetMetadata.encode(message.linkedDatasetMetadata, writer.uint32(234).fork()).join();
    }
    if (message.externalDatasetReference !== undefined) {
      ExternalDatasetReference.encode(message.externalDatasetReference, writer.uint32(162).fork()).join();
    }
    if (message.externalCatalogDatasetOptions !== undefined) {
      ExternalCatalogDatasetOptions.encode(message.externalCatalogDatasetOptions, writer.uint32(258).fork()).join();
    }
    if (message.isCaseInsensitive !== undefined) {
      BoolValue.encode({ value: message.isCaseInsensitive! }, writer.uint32(170).fork()).join();
    }
    if (message.defaultCollation !== undefined) {
      StringValue.encode({ value: message.defaultCollation! }, writer.uint32(178).fork()).join();
    }
    if (message.defaultRoundingMode !== 0) {
      writer.uint32(208).int32(message.defaultRoundingMode);
    }
    if (message.maxTimeTravelHours !== undefined) {
      Int64Value.encode({ value: message.maxTimeTravelHours! }, writer.uint32(186).fork()).join();
    }
    for (const v of message.tags) {
      GcpTag.encode(v!, writer.uint32(194).fork()).join();
    }
    if (message.storageBillingModel !== 0) {
      writer.uint32(200).int32(message.storageBillingModel);
    }
    if (message.restrictions !== undefined) {
      RestrictionConfig.encode(message.restrictions, writer.uint32(218).fork()).join();
    }
    Object.entries(message.resourceTags).forEach(([key, value]) => {
      Dataset_ResourceTagsEntry.encode({ key: key as any, value }, writer.uint32(242).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Dataset {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDataset();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.etag = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.id = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.selfLink = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.datasetReference = DatasetReference.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.friendlyName = StringValue.decode(reader, reader.uint32()).value;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.description = StringValue.decode(reader, reader.uint32()).value;
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.defaultTableExpirationMs = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.defaultPartitionExpirationMs = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          const entry9 = Dataset_LabelsEntry.decode(reader, reader.uint32());
          if (entry9.value !== undefined) {
            message.labels[entry9.key] = entry9.value;
          }
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.access.push(Access.decode(reader, reader.uint32()));
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.creationTime = Long.fromString(reader.int64().toString());
          continue;
        case 12:
          if (tag !== 96) {
            break;
          }

          message.lastModifiedTime = Long.fromString(reader.int64().toString());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.location = reader.string();
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.defaultEncryptionConfiguration = EncryptionConfiguration.decode(reader, reader.uint32());
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.satisfiesPzs = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 31:
          if (tag !== 250) {
            break;
          }

          message.satisfiesPzi = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.type = reader.string();
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.linkedDatasetSource = LinkedDatasetSource.decode(reader, reader.uint32());
          continue;
        case 29:
          if (tag !== 234) {
            break;
          }

          message.linkedDatasetMetadata = LinkedDatasetMetadata.decode(reader, reader.uint32());
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.externalDatasetReference = ExternalDatasetReference.decode(reader, reader.uint32());
          continue;
        case 32:
          if (tag !== 258) {
            break;
          }

          message.externalCatalogDatasetOptions = ExternalCatalogDatasetOptions.decode(reader, reader.uint32());
          continue;
        case 21:
          if (tag !== 170) {
            break;
          }

          message.isCaseInsensitive = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 22:
          if (tag !== 178) {
            break;
          }

          message.defaultCollation = StringValue.decode(reader, reader.uint32()).value;
          continue;
        case 26:
          if (tag !== 208) {
            break;
          }

          message.defaultRoundingMode = reader.int32() as any;
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.maxTimeTravelHours = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 24:
          if (tag !== 194) {
            break;
          }

          message.tags.push(GcpTag.decode(reader, reader.uint32()));
          continue;
        case 25:
          if (tag !== 200) {
            break;
          }

          message.storageBillingModel = reader.int32() as any;
          continue;
        case 27:
          if (tag !== 218) {
            break;
          }

          message.restrictions = RestrictionConfig.decode(reader, reader.uint32());
          continue;
        case 30:
          if (tag !== 242) {
            break;
          }

          const entry30 = Dataset_ResourceTagsEntry.decode(reader, reader.uint32());
          if (entry30.value !== undefined) {
            message.resourceTags[entry30.key] = entry30.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Dataset {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      etag: isSet(object.etag) ? globalThis.String(object.etag) : "",
      id: isSet(object.id) ? globalThis.String(object.id) : "",
      selfLink: isSet(object.selfLink) ? globalThis.String(object.selfLink) : "",
      datasetReference: isSet(object.datasetReference) ? DatasetReference.fromJSON(object.datasetReference) : undefined,
      friendlyName: isSet(object.friendlyName) ? String(object.friendlyName) : undefined,
      description: isSet(object.description) ? String(object.description) : undefined,
      defaultTableExpirationMs: isSet(object.defaultTableExpirationMs)
        ? Long.fromValue(object.defaultTableExpirationMs)
        : undefined,
      defaultPartitionExpirationMs: isSet(object.defaultPartitionExpirationMs)
        ? Long.fromValue(object.defaultPartitionExpirationMs)
        : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      access: globalThis.Array.isArray(object?.access) ? object.access.map((e: any) => Access.fromJSON(e)) : [],
      creationTime: isSet(object.creationTime) ? Long.fromValue(object.creationTime) : Long.ZERO,
      lastModifiedTime: isSet(object.lastModifiedTime) ? Long.fromValue(object.lastModifiedTime) : Long.ZERO,
      location: isSet(object.location) ? globalThis.String(object.location) : "",
      defaultEncryptionConfiguration: isSet(object.defaultEncryptionConfiguration)
        ? EncryptionConfiguration.fromJSON(object.defaultEncryptionConfiguration)
        : undefined,
      satisfiesPzs: isSet(object.satisfiesPzs) ? Boolean(object.satisfiesPzs) : undefined,
      satisfiesPzi: isSet(object.satisfiesPzi) ? Boolean(object.satisfiesPzi) : undefined,
      type: isSet(object.type) ? globalThis.String(object.type) : "",
      linkedDatasetSource: isSet(object.linkedDatasetSource)
        ? LinkedDatasetSource.fromJSON(object.linkedDatasetSource)
        : undefined,
      linkedDatasetMetadata: isSet(object.linkedDatasetMetadata)
        ? LinkedDatasetMetadata.fromJSON(object.linkedDatasetMetadata)
        : undefined,
      externalDatasetReference: isSet(object.externalDatasetReference)
        ? ExternalDatasetReference.fromJSON(object.externalDatasetReference)
        : undefined,
      externalCatalogDatasetOptions: isSet(object.externalCatalogDatasetOptions)
        ? ExternalCatalogDatasetOptions.fromJSON(object.externalCatalogDatasetOptions)
        : undefined,
      isCaseInsensitive: isSet(object.isCaseInsensitive) ? Boolean(object.isCaseInsensitive) : undefined,
      defaultCollation: isSet(object.defaultCollation) ? String(object.defaultCollation) : undefined,
      defaultRoundingMode: isSet(object.defaultRoundingMode)
        ? tableFieldSchema_RoundingModeFromJSON(object.defaultRoundingMode)
        : 0,
      maxTimeTravelHours: isSet(object.maxTimeTravelHours) ? Long.fromValue(object.maxTimeTravelHours) : undefined,
      tags: globalThis.Array.isArray(object?.tags)
        ? object.tags.map((e: any) => GcpTag.fromJSON(e))
        : [],
      storageBillingModel: isSet(object.storageBillingModel)
        ? dataset_StorageBillingModelFromJSON(object.storageBillingModel)
        : 0,
      restrictions: isSet(object.restrictions) ? RestrictionConfig.fromJSON(object.restrictions) : undefined,
      resourceTags: isObject(object.resourceTags)
        ? Object.entries(object.resourceTags).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: Dataset): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.etag !== "") {
      obj.etag = message.etag;
    }
    if (message.id !== "") {
      obj.id = message.id;
    }
    if (message.selfLink !== "") {
      obj.selfLink = message.selfLink;
    }
    if (message.datasetReference !== undefined) {
      obj.datasetReference = DatasetReference.toJSON(message.datasetReference);
    }
    if (message.friendlyName !== undefined) {
      obj.friendlyName = message.friendlyName;
    }
    if (message.description !== undefined) {
      obj.description = message.description;
    }
    if (message.defaultTableExpirationMs !== undefined) {
      obj.defaultTableExpirationMs = message.defaultTableExpirationMs;
    }
    if (message.defaultPartitionExpirationMs !== undefined) {
      obj.defaultPartitionExpirationMs = message.defaultPartitionExpirationMs;
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.access?.length) {
      obj.access = message.access.map((e) => Access.toJSON(e));
    }
    if (!message.creationTime.equals(Long.ZERO)) {
      obj.creationTime = (message.creationTime || Long.ZERO).toString();
    }
    if (!message.lastModifiedTime.equals(Long.ZERO)) {
      obj.lastModifiedTime = (message.lastModifiedTime || Long.ZERO).toString();
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    if (message.defaultEncryptionConfiguration !== undefined) {
      obj.defaultEncryptionConfiguration = EncryptionConfiguration.toJSON(message.defaultEncryptionConfiguration);
    }
    if (message.satisfiesPzs !== undefined) {
      obj.satisfiesPzs = message.satisfiesPzs;
    }
    if (message.satisfiesPzi !== undefined) {
      obj.satisfiesPzi = message.satisfiesPzi;
    }
    if (message.type !== "") {
      obj.type = message.type;
    }
    if (message.linkedDatasetSource !== undefined) {
      obj.linkedDatasetSource = LinkedDatasetSource.toJSON(message.linkedDatasetSource);
    }
    if (message.linkedDatasetMetadata !== undefined) {
      obj.linkedDatasetMetadata = LinkedDatasetMetadata.toJSON(message.linkedDatasetMetadata);
    }
    if (message.externalDatasetReference !== undefined) {
      obj.externalDatasetReference = ExternalDatasetReference.toJSON(message.externalDatasetReference);
    }
    if (message.externalCatalogDatasetOptions !== undefined) {
      obj.externalCatalogDatasetOptions = ExternalCatalogDatasetOptions.toJSON(message.externalCatalogDatasetOptions);
    }
    if (message.isCaseInsensitive !== undefined) {
      obj.isCaseInsensitive = message.isCaseInsensitive;
    }
    if (message.defaultCollation !== undefined) {
      obj.defaultCollation = message.defaultCollation;
    }
    if (message.defaultRoundingMode !== 0) {
      obj.defaultRoundingMode = tableFieldSchema_RoundingModeToJSON(message.defaultRoundingMode);
    }
    if (message.maxTimeTravelHours !== undefined) {
      obj.maxTimeTravelHours = message.maxTimeTravelHours;
    }
    if (message.tags?.length) {
      obj.tags = message.tags.map((e) => GcpTag.toJSON(e));
    }
    if (message.storageBillingModel !== 0) {
      obj.storageBillingModel = dataset_StorageBillingModelToJSON(message.storageBillingModel);
    }
    if (message.restrictions !== undefined) {
      obj.restrictions = RestrictionConfig.toJSON(message.restrictions);
    }
    if (message.resourceTags) {
      const entries = Object.entries(message.resourceTags);
      if (entries.length > 0) {
        obj.resourceTags = {};
        entries.forEach(([k, v]) => {
          obj.resourceTags[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<Dataset>): Dataset {
    return Dataset.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Dataset>): Dataset {
    const message = createBaseDataset();
    message.kind = object.kind ?? "";
    message.etag = object.etag ?? "";
    message.id = object.id ?? "";
    message.selfLink = object.selfLink ?? "";
    message.datasetReference = (object.datasetReference !== undefined && object.datasetReference !== null)
      ? DatasetReference.fromPartial(object.datasetReference)
      : undefined;
    message.friendlyName = object.friendlyName ?? undefined;
    message.description = object.description ?? undefined;
    message.defaultTableExpirationMs =
      (object.defaultTableExpirationMs !== undefined && object.defaultTableExpirationMs !== null)
        ? Long.fromValue(object.defaultTableExpirationMs)
        : undefined;
    message.defaultPartitionExpirationMs =
      (object.defaultPartitionExpirationMs !== undefined && object.defaultPartitionExpirationMs !== null)
        ? Long.fromValue(object.defaultPartitionExpirationMs)
        : undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.access = object.access?.map((e) => Access.fromPartial(e)) || [];
    message.creationTime = (object.creationTime !== undefined && object.creationTime !== null)
      ? Long.fromValue(object.creationTime)
      : Long.ZERO;
    message.lastModifiedTime = (object.lastModifiedTime !== undefined && object.lastModifiedTime !== null)
      ? Long.fromValue(object.lastModifiedTime)
      : Long.ZERO;
    message.location = object.location ?? "";
    message.defaultEncryptionConfiguration =
      (object.defaultEncryptionConfiguration !== undefined && object.defaultEncryptionConfiguration !== null)
        ? EncryptionConfiguration.fromPartial(object.defaultEncryptionConfiguration)
        : undefined;
    message.satisfiesPzs = object.satisfiesPzs ?? undefined;
    message.satisfiesPzi = object.satisfiesPzi ?? undefined;
    message.type = object.type ?? "";
    message.linkedDatasetSource = (object.linkedDatasetSource !== undefined && object.linkedDatasetSource !== null)
      ? LinkedDatasetSource.fromPartial(object.linkedDatasetSource)
      : undefined;
    message.linkedDatasetMetadata =
      (object.linkedDatasetMetadata !== undefined && object.linkedDatasetMetadata !== null)
        ? LinkedDatasetMetadata.fromPartial(object.linkedDatasetMetadata)
        : undefined;
    message.externalDatasetReference =
      (object.externalDatasetReference !== undefined && object.externalDatasetReference !== null)
        ? ExternalDatasetReference.fromPartial(object.externalDatasetReference)
        : undefined;
    message.externalCatalogDatasetOptions =
      (object.externalCatalogDatasetOptions !== undefined && object.externalCatalogDatasetOptions !== null)
        ? ExternalCatalogDatasetOptions.fromPartial(object.externalCatalogDatasetOptions)
        : undefined;
    message.isCaseInsensitive = object.isCaseInsensitive ?? undefined;
    message.defaultCollation = object.defaultCollation ?? undefined;
    message.defaultRoundingMode = object.defaultRoundingMode ?? 0;
    message.maxTimeTravelHours = (object.maxTimeTravelHours !== undefined && object.maxTimeTravelHours !== null)
      ? Long.fromValue(object.maxTimeTravelHours)
      : undefined;
    message.tags = object.tags?.map((e) => GcpTag.fromPartial(e)) || [];
    message.storageBillingModel = object.storageBillingModel ?? 0;
    message.restrictions = (object.restrictions !== undefined && object.restrictions !== null)
      ? RestrictionConfig.fromPartial(object.restrictions)
      : undefined;
    message.resourceTags = Object.entries(object.resourceTags ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseDataset_LabelsEntry(): Dataset_LabelsEntry {
  return { key: "", value: "" };
}

export const Dataset_LabelsEntry: MessageFns<Dataset_LabelsEntry> = {
  encode(message: Dataset_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Dataset_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDataset_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Dataset_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Dataset_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Dataset_LabelsEntry>): Dataset_LabelsEntry {
    return Dataset_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Dataset_LabelsEntry>): Dataset_LabelsEntry {
    const message = createBaseDataset_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseDataset_ResourceTagsEntry(): Dataset_ResourceTagsEntry {
  return { key: "", value: "" };
}

export const Dataset_ResourceTagsEntry: MessageFns<Dataset_ResourceTagsEntry> = {
  encode(message: Dataset_ResourceTagsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Dataset_ResourceTagsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDataset_ResourceTagsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Dataset_ResourceTagsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Dataset_ResourceTagsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Dataset_ResourceTagsEntry>): Dataset_ResourceTagsEntry {
    return Dataset_ResourceTagsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Dataset_ResourceTagsEntry>): Dataset_ResourceTagsEntry {
    const message = createBaseDataset_ResourceTagsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseGcpTag(): GcpTag {
  return { tagKey: "", tagValue: "" };
}

export const GcpTag: MessageFns<GcpTag> = {
  encode(message: GcpTag, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.tagKey !== "") {
      writer.uint32(10).string(message.tagKey);
    }
    if (message.tagValue !== "") {
      writer.uint32(18).string(message.tagValue);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GcpTag {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGcpTag();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.tagKey = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.tagValue = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GcpTag {
    return {
      tagKey: isSet(object.tagKey) ? globalThis.String(object.tagKey) : "",
      tagValue: isSet(object.tagValue) ? globalThis.String(object.tagValue) : "",
    };
  },

  toJSON(message: GcpTag): unknown {
    const obj: any = {};
    if (message.tagKey !== "") {
      obj.tagKey = message.tagKey;
    }
    if (message.tagValue !== "") {
      obj.tagValue = message.tagValue;
    }
    return obj;
  },

  create(base?: DeepPartial<GcpTag>): GcpTag {
    return GcpTag.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GcpTag>): GcpTag {
    const message = createBaseGcpTag();
    message.tagKey = object.tagKey ?? "";
    message.tagValue = object.tagValue ?? "";
    return message;
  },
};

function createBaseLinkedDatasetSource(): LinkedDatasetSource {
  return { sourceDataset: undefined };
}

export const LinkedDatasetSource: MessageFns<LinkedDatasetSource> = {
  encode(message: LinkedDatasetSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.sourceDataset !== undefined) {
      DatasetReference.encode(message.sourceDataset, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LinkedDatasetSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLinkedDatasetSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.sourceDataset = DatasetReference.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LinkedDatasetSource {
    return { sourceDataset: isSet(object.sourceDataset) ? DatasetReference.fromJSON(object.sourceDataset) : undefined };
  },

  toJSON(message: LinkedDatasetSource): unknown {
    const obj: any = {};
    if (message.sourceDataset !== undefined) {
      obj.sourceDataset = DatasetReference.toJSON(message.sourceDataset);
    }
    return obj;
  },

  create(base?: DeepPartial<LinkedDatasetSource>): LinkedDatasetSource {
    return LinkedDatasetSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LinkedDatasetSource>): LinkedDatasetSource {
    const message = createBaseLinkedDatasetSource();
    message.sourceDataset = (object.sourceDataset !== undefined && object.sourceDataset !== null)
      ? DatasetReference.fromPartial(object.sourceDataset)
      : undefined;
    return message;
  },
};

function createBaseLinkedDatasetMetadata(): LinkedDatasetMetadata {
  return { linkState: 0 };
}

export const LinkedDatasetMetadata: MessageFns<LinkedDatasetMetadata> = {
  encode(message: LinkedDatasetMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.linkState !== 0) {
      writer.uint32(8).int32(message.linkState);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LinkedDatasetMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLinkedDatasetMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.linkState = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LinkedDatasetMetadata {
    return { linkState: isSet(object.linkState) ? linkedDatasetMetadata_LinkStateFromJSON(object.linkState) : 0 };
  },

  toJSON(message: LinkedDatasetMetadata): unknown {
    const obj: any = {};
    if (message.linkState !== 0) {
      obj.linkState = linkedDatasetMetadata_LinkStateToJSON(message.linkState);
    }
    return obj;
  },

  create(base?: DeepPartial<LinkedDatasetMetadata>): LinkedDatasetMetadata {
    return LinkedDatasetMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LinkedDatasetMetadata>): LinkedDatasetMetadata {
    const message = createBaseLinkedDatasetMetadata();
    message.linkState = object.linkState ?? 0;
    return message;
  },
};

function createBaseGetDatasetRequest(): GetDatasetRequest {
  return { projectId: "", datasetId: "", datasetView: 0 };
}

export const GetDatasetRequest: MessageFns<GetDatasetRequest> = {
  encode(message: GetDatasetRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.datasetId !== "") {
      writer.uint32(18).string(message.datasetId);
    }
    if (message.datasetView !== 0) {
      writer.uint32(24).int32(message.datasetView);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetDatasetRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetDatasetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.datasetView = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetDatasetRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      datasetId: isSet(object.datasetId) ? globalThis.String(object.datasetId) : "",
      datasetView: isSet(object.datasetView) ? getDatasetRequest_DatasetViewFromJSON(object.datasetView) : 0,
    };
  },

  toJSON(message: GetDatasetRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    if (message.datasetView !== 0) {
      obj.datasetView = getDatasetRequest_DatasetViewToJSON(message.datasetView);
    }
    return obj;
  },

  create(base?: DeepPartial<GetDatasetRequest>): GetDatasetRequest {
    return GetDatasetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetDatasetRequest>): GetDatasetRequest {
    const message = createBaseGetDatasetRequest();
    message.projectId = object.projectId ?? "";
    message.datasetId = object.datasetId ?? "";
    message.datasetView = object.datasetView ?? 0;
    return message;
  },
};

function createBaseInsertDatasetRequest(): InsertDatasetRequest {
  return { projectId: "", dataset: undefined };
}

export const InsertDatasetRequest: MessageFns<InsertDatasetRequest> = {
  encode(message: InsertDatasetRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.dataset !== undefined) {
      Dataset.encode(message.dataset, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InsertDatasetRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInsertDatasetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.dataset = Dataset.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InsertDatasetRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      dataset: isSet(object.dataset) ? Dataset.fromJSON(object.dataset) : undefined,
    };
  },

  toJSON(message: InsertDatasetRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.dataset !== undefined) {
      obj.dataset = Dataset.toJSON(message.dataset);
    }
    return obj;
  },

  create(base?: DeepPartial<InsertDatasetRequest>): InsertDatasetRequest {
    return InsertDatasetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InsertDatasetRequest>): InsertDatasetRequest {
    const message = createBaseInsertDatasetRequest();
    message.projectId = object.projectId ?? "";
    message.dataset = (object.dataset !== undefined && object.dataset !== null)
      ? Dataset.fromPartial(object.dataset)
      : undefined;
    return message;
  },
};

function createBaseUpdateOrPatchDatasetRequest(): UpdateOrPatchDatasetRequest {
  return { projectId: "", datasetId: "", dataset: undefined };
}

export const UpdateOrPatchDatasetRequest: MessageFns<UpdateOrPatchDatasetRequest> = {
  encode(message: UpdateOrPatchDatasetRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.datasetId !== "") {
      writer.uint32(18).string(message.datasetId);
    }
    if (message.dataset !== undefined) {
      Dataset.encode(message.dataset, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateOrPatchDatasetRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateOrPatchDatasetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.dataset = Dataset.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateOrPatchDatasetRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      datasetId: isSet(object.datasetId) ? globalThis.String(object.datasetId) : "",
      dataset: isSet(object.dataset) ? Dataset.fromJSON(object.dataset) : undefined,
    };
  },

  toJSON(message: UpdateOrPatchDatasetRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    if (message.dataset !== undefined) {
      obj.dataset = Dataset.toJSON(message.dataset);
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateOrPatchDatasetRequest>): UpdateOrPatchDatasetRequest {
    return UpdateOrPatchDatasetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateOrPatchDatasetRequest>): UpdateOrPatchDatasetRequest {
    const message = createBaseUpdateOrPatchDatasetRequest();
    message.projectId = object.projectId ?? "";
    message.datasetId = object.datasetId ?? "";
    message.dataset = (object.dataset !== undefined && object.dataset !== null)
      ? Dataset.fromPartial(object.dataset)
      : undefined;
    return message;
  },
};

function createBaseDeleteDatasetRequest(): DeleteDatasetRequest {
  return { projectId: "", datasetId: "", deleteContents: false };
}

export const DeleteDatasetRequest: MessageFns<DeleteDatasetRequest> = {
  encode(message: DeleteDatasetRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.datasetId !== "") {
      writer.uint32(18).string(message.datasetId);
    }
    if (message.deleteContents !== false) {
      writer.uint32(24).bool(message.deleteContents);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeleteDatasetRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeleteDatasetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.deleteContents = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeleteDatasetRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      datasetId: isSet(object.datasetId) ? globalThis.String(object.datasetId) : "",
      deleteContents: isSet(object.deleteContents) ? globalThis.Boolean(object.deleteContents) : false,
    };
  },

  toJSON(message: DeleteDatasetRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    if (message.deleteContents !== false) {
      obj.deleteContents = message.deleteContents;
    }
    return obj;
  },

  create(base?: DeepPartial<DeleteDatasetRequest>): DeleteDatasetRequest {
    return DeleteDatasetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeleteDatasetRequest>): DeleteDatasetRequest {
    const message = createBaseDeleteDatasetRequest();
    message.projectId = object.projectId ?? "";
    message.datasetId = object.datasetId ?? "";
    message.deleteContents = object.deleteContents ?? false;
    return message;
  },
};

function createBaseListDatasetsRequest(): ListDatasetsRequest {
  return { projectId: "", maxResults: undefined, pageToken: "", all: false, filter: "" };
}

export const ListDatasetsRequest: MessageFns<ListDatasetsRequest> = {
  encode(message: ListDatasetsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.maxResults !== undefined) {
      UInt32Value.encode({ value: message.maxResults! }, writer.uint32(18).fork()).join();
    }
    if (message.pageToken !== "") {
      writer.uint32(26).string(message.pageToken);
    }
    if (message.all !== false) {
      writer.uint32(32).bool(message.all);
    }
    if (message.filter !== "") {
      writer.uint32(42).string(message.filter);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListDatasetsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListDatasetsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.maxResults = UInt32Value.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pageToken = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.all = reader.bool();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.filter = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListDatasetsRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      maxResults: isSet(object.maxResults) ? Number(object.maxResults) : undefined,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
      all: isSet(object.all) ? globalThis.Boolean(object.all) : false,
      filter: isSet(object.filter) ? globalThis.String(object.filter) : "",
    };
  },

  toJSON(message: ListDatasetsRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.maxResults !== undefined) {
      obj.maxResults = message.maxResults;
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    if (message.all !== false) {
      obj.all = message.all;
    }
    if (message.filter !== "") {
      obj.filter = message.filter;
    }
    return obj;
  },

  create(base?: DeepPartial<ListDatasetsRequest>): ListDatasetsRequest {
    return ListDatasetsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListDatasetsRequest>): ListDatasetsRequest {
    const message = createBaseListDatasetsRequest();
    message.projectId = object.projectId ?? "";
    message.maxResults = object.maxResults ?? undefined;
    message.pageToken = object.pageToken ?? "";
    message.all = object.all ?? false;
    message.filter = object.filter ?? "";
    return message;
  },
};

function createBaseListFormatDataset(): ListFormatDataset {
  return { kind: "", id: "", datasetReference: undefined, labels: {}, friendlyName: undefined, location: "" };
}

export const ListFormatDataset: MessageFns<ListFormatDataset> = {
  encode(message: ListFormatDataset, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.id !== "") {
      writer.uint32(18).string(message.id);
    }
    if (message.datasetReference !== undefined) {
      DatasetReference.encode(message.datasetReference, writer.uint32(26).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      ListFormatDataset_LabelsEntry.encode({ key: key as any, value }, writer.uint32(34).fork()).join();
    });
    if (message.friendlyName !== undefined) {
      StringValue.encode({ value: message.friendlyName! }, writer.uint32(42).fork()).join();
    }
    if (message.location !== "") {
      writer.uint32(50).string(message.location);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListFormatDataset {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListFormatDataset();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.id = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.datasetReference = DatasetReference.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          const entry4 = ListFormatDataset_LabelsEntry.decode(reader, reader.uint32());
          if (entry4.value !== undefined) {
            message.labels[entry4.key] = entry4.value;
          }
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.friendlyName = StringValue.decode(reader, reader.uint32()).value;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.location = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListFormatDataset {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      id: isSet(object.id) ? globalThis.String(object.id) : "",
      datasetReference: isSet(object.datasetReference) ? DatasetReference.fromJSON(object.datasetReference) : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      friendlyName: isSet(object.friendlyName) ? String(object.friendlyName) : undefined,
      location: isSet(object.location) ? globalThis.String(object.location) : "",
    };
  },

  toJSON(message: ListFormatDataset): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.id !== "") {
      obj.id = message.id;
    }
    if (message.datasetReference !== undefined) {
      obj.datasetReference = DatasetReference.toJSON(message.datasetReference);
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.friendlyName !== undefined) {
      obj.friendlyName = message.friendlyName;
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    return obj;
  },

  create(base?: DeepPartial<ListFormatDataset>): ListFormatDataset {
    return ListFormatDataset.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListFormatDataset>): ListFormatDataset {
    const message = createBaseListFormatDataset();
    message.kind = object.kind ?? "";
    message.id = object.id ?? "";
    message.datasetReference = (object.datasetReference !== undefined && object.datasetReference !== null)
      ? DatasetReference.fromPartial(object.datasetReference)
      : undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.friendlyName = object.friendlyName ?? undefined;
    message.location = object.location ?? "";
    return message;
  },
};

function createBaseListFormatDataset_LabelsEntry(): ListFormatDataset_LabelsEntry {
  return { key: "", value: "" };
}

export const ListFormatDataset_LabelsEntry: MessageFns<ListFormatDataset_LabelsEntry> = {
  encode(message: ListFormatDataset_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListFormatDataset_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListFormatDataset_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListFormatDataset_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: ListFormatDataset_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<ListFormatDataset_LabelsEntry>): ListFormatDataset_LabelsEntry {
    return ListFormatDataset_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListFormatDataset_LabelsEntry>): ListFormatDataset_LabelsEntry {
    const message = createBaseListFormatDataset_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseDatasetList(): DatasetList {
  return { kind: "", etag: "", nextPageToken: "", datasets: [], unreachable: [] };
}

export const DatasetList: MessageFns<DatasetList> = {
  encode(message: DatasetList, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.etag !== "") {
      writer.uint32(18).string(message.etag);
    }
    if (message.nextPageToken !== "") {
      writer.uint32(26).string(message.nextPageToken);
    }
    for (const v of message.datasets) {
      ListFormatDataset.encode(v!, writer.uint32(34).fork()).join();
    }
    for (const v of message.unreachable) {
      writer.uint32(42).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatasetList {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatasetList();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.etag = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.datasets.push(ListFormatDataset.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.unreachable.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatasetList {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      etag: isSet(object.etag) ? globalThis.String(object.etag) : "",
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
      datasets: globalThis.Array.isArray(object?.datasets)
        ? object.datasets.map((e: any) => ListFormatDataset.fromJSON(e))
        : [],
      unreachable: globalThis.Array.isArray(object?.unreachable)
        ? object.unreachable.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: DatasetList): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.etag !== "") {
      obj.etag = message.etag;
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    if (message.datasets?.length) {
      obj.datasets = message.datasets.map((e) => ListFormatDataset.toJSON(e));
    }
    if (message.unreachable?.length) {
      obj.unreachable = message.unreachable;
    }
    return obj;
  },

  create(base?: DeepPartial<DatasetList>): DatasetList {
    return DatasetList.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatasetList>): DatasetList {
    const message = createBaseDatasetList();
    message.kind = object.kind ?? "";
    message.etag = object.etag ?? "";
    message.nextPageToken = object.nextPageToken ?? "";
    message.datasets = object.datasets?.map((e) => ListFormatDataset.fromPartial(e)) || [];
    message.unreachable = object.unreachable?.map((e) => e) || [];
    return message;
  },
};

function createBaseUndeleteDatasetRequest(): UndeleteDatasetRequest {
  return { projectId: "", datasetId: "", deletionTime: undefined };
}

export const UndeleteDatasetRequest: MessageFns<UndeleteDatasetRequest> = {
  encode(message: UndeleteDatasetRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.datasetId !== "") {
      writer.uint32(18).string(message.datasetId);
    }
    if (message.deletionTime !== undefined) {
      Timestamp.encode(toTimestamp(message.deletionTime), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UndeleteDatasetRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUndeleteDatasetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.deletionTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UndeleteDatasetRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      datasetId: isSet(object.datasetId) ? globalThis.String(object.datasetId) : "",
      deletionTime: isSet(object.deletionTime) ? fromJsonTimestamp(object.deletionTime) : undefined,
    };
  },

  toJSON(message: UndeleteDatasetRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    if (message.deletionTime !== undefined) {
      obj.deletionTime = message.deletionTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<UndeleteDatasetRequest>): UndeleteDatasetRequest {
    return UndeleteDatasetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UndeleteDatasetRequest>): UndeleteDatasetRequest {
    const message = createBaseUndeleteDatasetRequest();
    message.projectId = object.projectId ?? "";
    message.datasetId = object.datasetId ?? "";
    message.deletionTime = object.deletionTime ?? undefined;
    return message;
  },
};

/**
 * This is an experimental RPC service definition for the BigQuery
 * Dataset Service.
 *
 * It should not be relied on for production use cases at this time.
 */
export type DatasetServiceDefinition = typeof DatasetServiceDefinition;
export const DatasetServiceDefinition = {
  name: "DatasetService",
  fullName: "google.cloud.bigquery.v2.DatasetService",
  methods: {
    /** Returns the dataset specified by datasetID. */
    getDataset: {
      name: "GetDataset",
      requestType: GetDatasetRequest,
      requestStream: false,
      responseType: Dataset,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              62,
              18,
              60,
              47,
              98,
              105,
              103,
              113,
              117,
              101,
              114,
              121,
              47,
              118,
              50,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              61,
              42,
              125,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              123,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              95,
              105,
              100,
              61,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /** Creates a new empty dataset. */
    insertDataset: {
      name: "InsertDataset",
      requestType: InsertDatasetRequest,
      requestStream: false,
      responseType: Dataset,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              56,
              58,
              7,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              34,
              45,
              47,
              98,
              105,
              103,
              113,
              117,
              101,
              114,
              121,
              47,
              118,
              50,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              61,
              42,
              125,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Updates information in an existing dataset. The update method replaces the
     * entire dataset resource, whereas the patch method only replaces fields that
     * are provided in the submitted dataset resource.
     * This method supports RFC5789 patch semantics.
     */
    patchDataset: {
      name: "PatchDataset",
      requestType: UpdateOrPatchDatasetRequest,
      requestStream: false,
      responseType: Dataset,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              71,
              58,
              7,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              50,
              60,
              47,
              98,
              105,
              103,
              113,
              117,
              101,
              114,
              121,
              47,
              118,
              50,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              61,
              42,
              125,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              123,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              95,
              105,
              100,
              61,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Updates information in an existing dataset. The update method replaces the
     * entire dataset resource, whereas the patch method only replaces fields that
     * are provided in the submitted dataset resource.
     */
    updateDataset: {
      name: "UpdateDataset",
      requestType: UpdateOrPatchDatasetRequest,
      requestStream: false,
      responseType: Dataset,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              71,
              58,
              7,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              26,
              60,
              47,
              98,
              105,
              103,
              113,
              117,
              101,
              114,
              121,
              47,
              118,
              50,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              61,
              42,
              125,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              123,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              95,
              105,
              100,
              61,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Deletes the dataset specified by the datasetId value. Before you can delete
     * a dataset, you must delete all its tables, either manually or by specifying
     * deleteContents. Immediately after deletion, you can create another dataset
     * with the same name.
     */
    deleteDataset: {
      name: "DeleteDataset",
      requestType: DeleteDatasetRequest,
      requestStream: false,
      responseType: Empty,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              62,
              42,
              60,
              47,
              98,
              105,
              103,
              113,
              117,
              101,
              114,
              121,
              47,
              118,
              50,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              61,
              42,
              125,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              123,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              95,
              105,
              100,
              61,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Lists all datasets in the specified project to which the user has been
     * granted the READER dataset role.
     */
    listDatasets: {
      name: "ListDatasets",
      requestType: ListDatasetsRequest,
      requestStream: false,
      responseType: DatasetList,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              47,
              18,
              45,
              47,
              98,
              105,
              103,
              113,
              117,
              101,
              114,
              121,
              47,
              118,
              50,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              61,
              42,
              125,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Undeletes a dataset which is within time travel window based on datasetId.
     * If a time is specified, the dataset version deleted at that time is
     * undeleted, else the last live version is undeleted.
     */
    undeleteDataset: {
      name: "UndeleteDataset",
      requestType: UndeleteDatasetRequest,
      requestStream: false,
      responseType: Dataset,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              74,
              58,
              1,
              42,
              34,
              69,
              47,
              98,
              105,
              103,
              113,
              117,
              101,
              114,
              121,
              47,
              118,
              50,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              61,
              42,
              125,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              123,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              95,
              105,
              100,
              61,
              42,
              125,
              58,
              117,
              110,
              100,
              101,
              108,
              101,
              116,
              101,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface DatasetServiceImplementation<CallContextExt = {}> {
  /** Returns the dataset specified by datasetID. */
  getDataset(request: GetDatasetRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Dataset>>;
  /** Creates a new empty dataset. */
  insertDataset(request: InsertDatasetRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Dataset>>;
  /**
   * Updates information in an existing dataset. The update method replaces the
   * entire dataset resource, whereas the patch method only replaces fields that
   * are provided in the submitted dataset resource.
   * This method supports RFC5789 patch semantics.
   */
  patchDataset(
    request: UpdateOrPatchDatasetRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Dataset>>;
  /**
   * Updates information in an existing dataset. The update method replaces the
   * entire dataset resource, whereas the patch method only replaces fields that
   * are provided in the submitted dataset resource.
   */
  updateDataset(
    request: UpdateOrPatchDatasetRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Dataset>>;
  /**
   * Deletes the dataset specified by the datasetId value. Before you can delete
   * a dataset, you must delete all its tables, either manually or by specifying
   * deleteContents. Immediately after deletion, you can create another dataset
   * with the same name.
   */
  deleteDataset(request: DeleteDatasetRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Empty>>;
  /**
   * Lists all datasets in the specified project to which the user has been
   * granted the READER dataset role.
   */
  listDatasets(request: ListDatasetsRequest, context: CallContext & CallContextExt): Promise<DeepPartial<DatasetList>>;
  /**
   * Undeletes a dataset which is within time travel window based on datasetId.
   * If a time is specified, the dataset version deleted at that time is
   * undeleted, else the last live version is undeleted.
   */
  undeleteDataset(
    request: UndeleteDatasetRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Dataset>>;
}

export interface DatasetServiceClient<CallOptionsExt = {}> {
  /** Returns the dataset specified by datasetID. */
  getDataset(request: DeepPartial<GetDatasetRequest>, options?: CallOptions & CallOptionsExt): Promise<Dataset>;
  /** Creates a new empty dataset. */
  insertDataset(request: DeepPartial<InsertDatasetRequest>, options?: CallOptions & CallOptionsExt): Promise<Dataset>;
  /**
   * Updates information in an existing dataset. The update method replaces the
   * entire dataset resource, whereas the patch method only replaces fields that
   * are provided in the submitted dataset resource.
   * This method supports RFC5789 patch semantics.
   */
  patchDataset(
    request: DeepPartial<UpdateOrPatchDatasetRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Dataset>;
  /**
   * Updates information in an existing dataset. The update method replaces the
   * entire dataset resource, whereas the patch method only replaces fields that
   * are provided in the submitted dataset resource.
   */
  updateDataset(
    request: DeepPartial<UpdateOrPatchDatasetRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Dataset>;
  /**
   * Deletes the dataset specified by the datasetId value. Before you can delete
   * a dataset, you must delete all its tables, either manually or by specifying
   * deleteContents. Immediately after deletion, you can create another dataset
   * with the same name.
   */
  deleteDataset(request: DeepPartial<DeleteDatasetRequest>, options?: CallOptions & CallOptionsExt): Promise<Empty>;
  /**
   * Lists all datasets in the specified project to which the user has been
   * granted the READER dataset role.
   */
  listDatasets(request: DeepPartial<ListDatasetsRequest>, options?: CallOptions & CallOptionsExt): Promise<DatasetList>;
  /**
   * Undeletes a dataset which is within time travel window based on datasetId.
   * If a time is specified, the dataset version deleted at that time is
   * undeleted, else the last live version is undeleted.
   */
  undeleteDataset(
    request: DeepPartial<UndeleteDatasetRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Dataset>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
