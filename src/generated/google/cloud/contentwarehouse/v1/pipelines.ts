// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/contentwarehouse/v1/pipelines.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Policy } from "../../../iam/v1/policy.js";
import { Status } from "../../../rpc/status.js";
import { UserInfo } from "./common.js";

export const protobufPackage = "google.cloud.contentwarehouse.v1";

/** Response message of RunPipeline method. */
export interface RunPipelineResponse {
}

/** Metadata message of RunPipeline method. */
export interface RunPipelineMetadata {
  /** Number of files that were processed by the pipeline. */
  totalFileCount: number;
  /** Number of files that have failed at some point in the pipeline. */
  failedFileCount: number;
  /** User unique identification and groups information. */
  userInfo:
    | UserInfo
    | undefined;
  /** The pipeline metadata for GcsIngest pipeline. */
  gcsIngestPipelineMetadata?:
    | RunPipelineMetadata_GcsIngestPipelineMetadata
    | undefined;
  /** The pipeline metadata for Export-to-CDW pipeline. */
  exportToCdwPipelineMetadata?:
    | RunPipelineMetadata_ExportToCdwPipelineMetadata
    | undefined;
  /** The pipeline metadata for Process-with-DocAi pipeline. */
  processWithDocAiPipelineMetadata?:
    | RunPipelineMetadata_ProcessWithDocAiPipelineMetadata
    | undefined;
  /** The list of response details of each document. */
  individualDocumentStatuses: RunPipelineMetadata_IndividualDocumentStatus[];
}

/** The metadata message for GcsIngest pipeline. */
export interface RunPipelineMetadata_GcsIngestPipelineMetadata {
  /**
   * The input Cloud Storage folder in this pipeline.
   * Format: `gs://<bucket-name>/<folder-name>`.
   */
  inputPath: string;
}

/** The metadata message for Export-to-CDW pipeline. */
export interface RunPipelineMetadata_ExportToCdwPipelineMetadata {
  /** The input list of all the resource names of the documents to be exported. */
  documents: string[];
  /** The output CDW dataset resource name. */
  docAiDataset: string;
  /** The output Cloud Storage folder in this pipeline. */
  outputPath: string;
}

/** The metadata message for Process-with-DocAi pipeline. */
export interface RunPipelineMetadata_ProcessWithDocAiPipelineMetadata {
  /**
   * The input list of all the resource names of the documents to be
   * processed.
   */
  documents: string[];
  /** The DocAI processor to process the documents with. */
  processorInfo: ProcessorInfo | undefined;
}

/** The status of processing a document. */
export interface RunPipelineMetadata_IndividualDocumentStatus {
  /** Document identifier of an existing document. */
  documentId: string;
  /** The status processing the document. */
  status: Status | undefined;
}

/** The DocAI processor information. */
export interface ProcessorInfo {
  /**
   * The processor resource name.
   * Format is `projects/{project}/locations/{location}/processors/{processor}`,
   * or
   * `projects/{project}/locations/{location}/processors/{processor}/processorVersions/{processorVersion}`
   */
  processorName: string;
  /** The processor will process the documents with this document type. */
  documentType: string;
  /**
   * The Document schema resource name. All documents processed by this
   * processor will use this schema.
   * Format:
   * projects/{project_number}/locations/{location}/documentSchemas/{document_schema_id}.
   */
  schemaName: string;
}

/** The ingestion pipeline config. */
export interface IngestPipelineConfig {
  /**
   * The document level acl policy config.
   * This refers to an Identity and Access (IAM) policy, which specifies access
   * controls for all documents ingested by the pipeline. The
   * [role][google.iam.v1.Binding.role] and
   * [members][google.iam.v1.Binding.role] under the policy needs to be
   * specified.
   *
   * The following roles are supported for document level acl control:
   * * roles/contentwarehouse.documentAdmin
   * * roles/contentwarehouse.documentEditor
   * * roles/contentwarehouse.documentViewer
   *
   * The following members are supported for document level acl control:
   * * user:user-email@example.com
   * * group:group-email@example.com
   * Note that for documents searched with LLM, only single level user or group
   * acl check is supported.
   */
  documentAclPolicy:
    | Policy
    | undefined;
  /**
   * The document text extraction enabled flag.
   * If the flag is set to true, DWH will perform text extraction on the raw
   * document.
   */
  enableDocumentTextExtraction: boolean;
  /**
   * Optional. The name of the folder to which all ingested documents will be
   * linked during ingestion process. Format is
   * `projects/{project}/locations/{location}/documents/{folder_id}`
   */
  folder: string;
  /**
   * The Cloud Function resource name. The Cloud Function needs to live inside
   * consumer project and is accessible to Document AI Warehouse P4SA.
   * Only Cloud Functions V2 is supported. Cloud function execution should
   * complete within 5 minutes or this file ingestion may fail due to timeout.
   * Format: `https://{region}-{project_id}.cloudfunctions.net/{cloud_function}`
   * The following keys are available the request json payload.
   * * display_name
   * * properties
   * * plain_text
   * * reference_id
   * * document_schema_name
   * * raw_document_path
   * * raw_document_file_type
   *
   * The following keys from the cloud function json response payload will be
   * ingested to the Document AI Warehouse as part of Document proto content
   * and/or related information. The original values will be overridden if any
   * key is present in the response.
   * * display_name
   * * properties
   * * plain_text
   * * document_acl_policy
   * * folder
   */
  cloudFunction: string;
}

/** The configuration of the Cloud Storage Ingestion pipeline. */
export interface GcsIngestPipeline {
  /**
   * The input Cloud Storage folder. All files under this folder will be
   * imported to Document Warehouse.
   * Format: `gs://<bucket-name>/<folder-name>`.
   */
  inputPath: string;
  /**
   * The Document Warehouse schema resource name. All documents processed by
   * this pipeline will use this schema.
   * Format:
   * projects/{project_number}/locations/{location}/documentSchemas/{document_schema_id}.
   */
  schemaName: string;
  /**
   * The Doc AI processor type name. Only used when the format of ingested
   * files is Doc AI Document proto format.
   */
  processorType: string;
  /**
   * The flag whether to skip ingested documents.
   * If it is set to true, documents in Cloud Storage contains key "status" with
   * value "status=ingested" in custom metadata will be skipped to ingest.
   */
  skipIngestedDocuments: boolean;
  /**
   * Optional. The config for the Cloud Storage Ingestion pipeline.
   * It provides additional customization options to run the pipeline and can be
   * skipped if it is not applicable.
   */
  pipelineConfig: IngestPipelineConfig | undefined;
}

/**
 * The configuration of the Cloud Storage Ingestion with DocAI Processors
 * pipeline.
 */
export interface GcsIngestWithDocAiProcessorsPipeline {
  /**
   * The input Cloud Storage folder. All files under this folder will be
   * imported to Document Warehouse.
   * Format: `gs://<bucket-name>/<folder-name>`.
   */
  inputPath: string;
  /**
   * The split and classify processor information.
   * The split and classify result will be used to find a matched extract
   * processor.
   */
  splitClassifyProcessorInfo:
    | ProcessorInfo
    | undefined;
  /**
   * The extract processors information.
   * One matched extract processor will be used to process documents based on
   * the classify processor result. If no classify processor is specified, the
   * first extract processor will be used.
   */
  extractProcessorInfos: ProcessorInfo[];
  /**
   * The Cloud Storage folder path used to store the raw results from
   * processors.
   * Format: `gs://<bucket-name>/<folder-name>`.
   */
  processorResultsFolderPath: string;
  /**
   * The flag whether to skip ingested documents.
   * If it is set to true, documents in Cloud Storage contains key "status" with
   * value "status=ingested" in custom metadata will be skipped to ingest.
   */
  skipIngestedDocuments: boolean;
  /**
   * Optional. The config for the Cloud Storage Ingestion with DocAI Processors
   * pipeline. It provides additional customization options to run the pipeline
   * and can be skipped if it is not applicable.
   */
  pipelineConfig: IngestPipelineConfig | undefined;
}

/**
 * The configuration of exporting documents from the Document Warehouse to CDW
 * pipeline.
 */
export interface ExportToCdwPipeline {
  /**
   * The list of all the resource names of the documents to be processed.
   * Format:
   * projects/{project_number}/locations/{location}/documents/{document_id}.
   */
  documents: string[];
  /**
   * The Cloud Storage folder path used to store the exported documents before
   * being sent to CDW.
   * Format: `gs://<bucket-name>/<folder-name>`.
   */
  exportFolderPath: string;
  /**
   * Optional. The CDW dataset resource name. This field is optional. If not
   * set, the documents will be exported to Cloud Storage only. Format:
   * projects/{project}/locations/{location}/processors/{processor}/dataset
   */
  docAiDataset: string;
  /**
   * Ratio of training dataset split. When importing into Document AI Workbench,
   * documents will be automatically split into training and test split category
   * with the specified ratio. This field is required if doc_ai_dataset is set.
   */
  trainingSplitRatio: number;
}

/**
 * The configuration of processing documents in Document Warehouse with DocAi
 * processors pipeline.
 */
export interface ProcessWithDocAiPipeline {
  /**
   * The list of all the resource names of the documents to be processed.
   * Format:
   * projects/{project_number}/locations/{location}/documents/{document_id}.
   */
  documents: string[];
  /**
   * The Cloud Storage folder path used to store the exported documents before
   * being sent to CDW.
   * Format: `gs://<bucket-name>/<folder-name>`.
   */
  exportFolderPath: string;
  /** The CDW processor information. */
  processorInfo:
    | ProcessorInfo
    | undefined;
  /**
   * The Cloud Storage folder path used to store the raw results from
   * processors.
   * Format: `gs://<bucket-name>/<folder-name>`.
   */
  processorResultsFolderPath: string;
}

function createBaseRunPipelineResponse(): RunPipelineResponse {
  return {};
}

export const RunPipelineResponse: MessageFns<RunPipelineResponse> = {
  encode(_: RunPipelineResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RunPipelineResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRunPipelineResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): RunPipelineResponse {
    return {};
  },

  toJSON(_: RunPipelineResponse): unknown {
    const obj: any = {};
    return obj;
  },

  create(base?: DeepPartial<RunPipelineResponse>): RunPipelineResponse {
    return RunPipelineResponse.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<RunPipelineResponse>): RunPipelineResponse {
    const message = createBaseRunPipelineResponse();
    return message;
  },
};

function createBaseRunPipelineMetadata(): RunPipelineMetadata {
  return {
    totalFileCount: 0,
    failedFileCount: 0,
    userInfo: undefined,
    gcsIngestPipelineMetadata: undefined,
    exportToCdwPipelineMetadata: undefined,
    processWithDocAiPipelineMetadata: undefined,
    individualDocumentStatuses: [],
  };
}

export const RunPipelineMetadata: MessageFns<RunPipelineMetadata> = {
  encode(message: RunPipelineMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.totalFileCount !== 0) {
      writer.uint32(8).int32(message.totalFileCount);
    }
    if (message.failedFileCount !== 0) {
      writer.uint32(16).int32(message.failedFileCount);
    }
    if (message.userInfo !== undefined) {
      UserInfo.encode(message.userInfo, writer.uint32(26).fork()).join();
    }
    if (message.gcsIngestPipelineMetadata !== undefined) {
      RunPipelineMetadata_GcsIngestPipelineMetadata.encode(message.gcsIngestPipelineMetadata, writer.uint32(34).fork())
        .join();
    }
    if (message.exportToCdwPipelineMetadata !== undefined) {
      RunPipelineMetadata_ExportToCdwPipelineMetadata.encode(
        message.exportToCdwPipelineMetadata,
        writer.uint32(50).fork(),
      ).join();
    }
    if (message.processWithDocAiPipelineMetadata !== undefined) {
      RunPipelineMetadata_ProcessWithDocAiPipelineMetadata.encode(
        message.processWithDocAiPipelineMetadata,
        writer.uint32(58).fork(),
      ).join();
    }
    for (const v of message.individualDocumentStatuses) {
      RunPipelineMetadata_IndividualDocumentStatus.encode(v!, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RunPipelineMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRunPipelineMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.totalFileCount = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.failedFileCount = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.userInfo = UserInfo.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.gcsIngestPipelineMetadata = RunPipelineMetadata_GcsIngestPipelineMetadata.decode(
            reader,
            reader.uint32(),
          );
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.exportToCdwPipelineMetadata = RunPipelineMetadata_ExportToCdwPipelineMetadata.decode(
            reader,
            reader.uint32(),
          );
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.processWithDocAiPipelineMetadata = RunPipelineMetadata_ProcessWithDocAiPipelineMetadata.decode(
            reader,
            reader.uint32(),
          );
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.individualDocumentStatuses.push(
            RunPipelineMetadata_IndividualDocumentStatus.decode(reader, reader.uint32()),
          );
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RunPipelineMetadata {
    return {
      totalFileCount: isSet(object.totalFileCount) ? globalThis.Number(object.totalFileCount) : 0,
      failedFileCount: isSet(object.failedFileCount) ? globalThis.Number(object.failedFileCount) : 0,
      userInfo: isSet(object.userInfo) ? UserInfo.fromJSON(object.userInfo) : undefined,
      gcsIngestPipelineMetadata: isSet(object.gcsIngestPipelineMetadata)
        ? RunPipelineMetadata_GcsIngestPipelineMetadata.fromJSON(object.gcsIngestPipelineMetadata)
        : undefined,
      exportToCdwPipelineMetadata: isSet(object.exportToCdwPipelineMetadata)
        ? RunPipelineMetadata_ExportToCdwPipelineMetadata.fromJSON(object.exportToCdwPipelineMetadata)
        : undefined,
      processWithDocAiPipelineMetadata: isSet(object.processWithDocAiPipelineMetadata)
        ? RunPipelineMetadata_ProcessWithDocAiPipelineMetadata.fromJSON(object.processWithDocAiPipelineMetadata)
        : undefined,
      individualDocumentStatuses: globalThis.Array.isArray(object?.individualDocumentStatuses)
        ? object.individualDocumentStatuses.map((e: any) => RunPipelineMetadata_IndividualDocumentStatus.fromJSON(e))
        : [],
    };
  },

  toJSON(message: RunPipelineMetadata): unknown {
    const obj: any = {};
    if (message.totalFileCount !== 0) {
      obj.totalFileCount = Math.round(message.totalFileCount);
    }
    if (message.failedFileCount !== 0) {
      obj.failedFileCount = Math.round(message.failedFileCount);
    }
    if (message.userInfo !== undefined) {
      obj.userInfo = UserInfo.toJSON(message.userInfo);
    }
    if (message.gcsIngestPipelineMetadata !== undefined) {
      obj.gcsIngestPipelineMetadata = RunPipelineMetadata_GcsIngestPipelineMetadata.toJSON(
        message.gcsIngestPipelineMetadata,
      );
    }
    if (message.exportToCdwPipelineMetadata !== undefined) {
      obj.exportToCdwPipelineMetadata = RunPipelineMetadata_ExportToCdwPipelineMetadata.toJSON(
        message.exportToCdwPipelineMetadata,
      );
    }
    if (message.processWithDocAiPipelineMetadata !== undefined) {
      obj.processWithDocAiPipelineMetadata = RunPipelineMetadata_ProcessWithDocAiPipelineMetadata.toJSON(
        message.processWithDocAiPipelineMetadata,
      );
    }
    if (message.individualDocumentStatuses?.length) {
      obj.individualDocumentStatuses = message.individualDocumentStatuses.map((e) =>
        RunPipelineMetadata_IndividualDocumentStatus.toJSON(e)
      );
    }
    return obj;
  },

  create(base?: DeepPartial<RunPipelineMetadata>): RunPipelineMetadata {
    return RunPipelineMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RunPipelineMetadata>): RunPipelineMetadata {
    const message = createBaseRunPipelineMetadata();
    message.totalFileCount = object.totalFileCount ?? 0;
    message.failedFileCount = object.failedFileCount ?? 0;
    message.userInfo = (object.userInfo !== undefined && object.userInfo !== null)
      ? UserInfo.fromPartial(object.userInfo)
      : undefined;
    message.gcsIngestPipelineMetadata =
      (object.gcsIngestPipelineMetadata !== undefined && object.gcsIngestPipelineMetadata !== null)
        ? RunPipelineMetadata_GcsIngestPipelineMetadata.fromPartial(object.gcsIngestPipelineMetadata)
        : undefined;
    message.exportToCdwPipelineMetadata =
      (object.exportToCdwPipelineMetadata !== undefined && object.exportToCdwPipelineMetadata !== null)
        ? RunPipelineMetadata_ExportToCdwPipelineMetadata.fromPartial(object.exportToCdwPipelineMetadata)
        : undefined;
    message.processWithDocAiPipelineMetadata =
      (object.processWithDocAiPipelineMetadata !== undefined && object.processWithDocAiPipelineMetadata !== null)
        ? RunPipelineMetadata_ProcessWithDocAiPipelineMetadata.fromPartial(object.processWithDocAiPipelineMetadata)
        : undefined;
    message.individualDocumentStatuses =
      object.individualDocumentStatuses?.map((e) => RunPipelineMetadata_IndividualDocumentStatus.fromPartial(e)) || [];
    return message;
  },
};

function createBaseRunPipelineMetadata_GcsIngestPipelineMetadata(): RunPipelineMetadata_GcsIngestPipelineMetadata {
  return { inputPath: "" };
}

export const RunPipelineMetadata_GcsIngestPipelineMetadata: MessageFns<RunPipelineMetadata_GcsIngestPipelineMetadata> =
  {
    encode(
      message: RunPipelineMetadata_GcsIngestPipelineMetadata,
      writer: BinaryWriter = new BinaryWriter(),
    ): BinaryWriter {
      if (message.inputPath !== "") {
        writer.uint32(10).string(message.inputPath);
      }
      return writer;
    },

    decode(input: BinaryReader | Uint8Array, length?: number): RunPipelineMetadata_GcsIngestPipelineMetadata {
      const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
      let end = length === undefined ? reader.len : reader.pos + length;
      const message = createBaseRunPipelineMetadata_GcsIngestPipelineMetadata();
      while (reader.pos < end) {
        const tag = reader.uint32();
        switch (tag >>> 3) {
          case 1:
            if (tag !== 10) {
              break;
            }

            message.inputPath = reader.string();
            continue;
        }
        if ((tag & 7) === 4 || tag === 0) {
          break;
        }
        reader.skip(tag & 7);
      }
      return message;
    },

    fromJSON(object: any): RunPipelineMetadata_GcsIngestPipelineMetadata {
      return { inputPath: isSet(object.inputPath) ? globalThis.String(object.inputPath) : "" };
    },

    toJSON(message: RunPipelineMetadata_GcsIngestPipelineMetadata): unknown {
      const obj: any = {};
      if (message.inputPath !== "") {
        obj.inputPath = message.inputPath;
      }
      return obj;
    },

    create(
      base?: DeepPartial<RunPipelineMetadata_GcsIngestPipelineMetadata>,
    ): RunPipelineMetadata_GcsIngestPipelineMetadata {
      return RunPipelineMetadata_GcsIngestPipelineMetadata.fromPartial(base ?? {});
    },
    fromPartial(
      object: DeepPartial<RunPipelineMetadata_GcsIngestPipelineMetadata>,
    ): RunPipelineMetadata_GcsIngestPipelineMetadata {
      const message = createBaseRunPipelineMetadata_GcsIngestPipelineMetadata();
      message.inputPath = object.inputPath ?? "";
      return message;
    },
  };

function createBaseRunPipelineMetadata_ExportToCdwPipelineMetadata(): RunPipelineMetadata_ExportToCdwPipelineMetadata {
  return { documents: [], docAiDataset: "", outputPath: "" };
}

export const RunPipelineMetadata_ExportToCdwPipelineMetadata: MessageFns<
  RunPipelineMetadata_ExportToCdwPipelineMetadata
> = {
  encode(
    message: RunPipelineMetadata_ExportToCdwPipelineMetadata,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    for (const v of message.documents) {
      writer.uint32(10).string(v!);
    }
    if (message.docAiDataset !== "") {
      writer.uint32(18).string(message.docAiDataset);
    }
    if (message.outputPath !== "") {
      writer.uint32(26).string(message.outputPath);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RunPipelineMetadata_ExportToCdwPipelineMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRunPipelineMetadata_ExportToCdwPipelineMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.documents.push(reader.string());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.docAiDataset = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.outputPath = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RunPipelineMetadata_ExportToCdwPipelineMetadata {
    return {
      documents: globalThis.Array.isArray(object?.documents)
        ? object.documents.map((e: any) => globalThis.String(e))
        : [],
      docAiDataset: isSet(object.docAiDataset) ? globalThis.String(object.docAiDataset) : "",
      outputPath: isSet(object.outputPath) ? globalThis.String(object.outputPath) : "",
    };
  },

  toJSON(message: RunPipelineMetadata_ExportToCdwPipelineMetadata): unknown {
    const obj: any = {};
    if (message.documents?.length) {
      obj.documents = message.documents;
    }
    if (message.docAiDataset !== "") {
      obj.docAiDataset = message.docAiDataset;
    }
    if (message.outputPath !== "") {
      obj.outputPath = message.outputPath;
    }
    return obj;
  },

  create(
    base?: DeepPartial<RunPipelineMetadata_ExportToCdwPipelineMetadata>,
  ): RunPipelineMetadata_ExportToCdwPipelineMetadata {
    return RunPipelineMetadata_ExportToCdwPipelineMetadata.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<RunPipelineMetadata_ExportToCdwPipelineMetadata>,
  ): RunPipelineMetadata_ExportToCdwPipelineMetadata {
    const message = createBaseRunPipelineMetadata_ExportToCdwPipelineMetadata();
    message.documents = object.documents?.map((e) => e) || [];
    message.docAiDataset = object.docAiDataset ?? "";
    message.outputPath = object.outputPath ?? "";
    return message;
  },
};

function createBaseRunPipelineMetadata_ProcessWithDocAiPipelineMetadata(): RunPipelineMetadata_ProcessWithDocAiPipelineMetadata {
  return { documents: [], processorInfo: undefined };
}

export const RunPipelineMetadata_ProcessWithDocAiPipelineMetadata: MessageFns<
  RunPipelineMetadata_ProcessWithDocAiPipelineMetadata
> = {
  encode(
    message: RunPipelineMetadata_ProcessWithDocAiPipelineMetadata,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    for (const v of message.documents) {
      writer.uint32(10).string(v!);
    }
    if (message.processorInfo !== undefined) {
      ProcessorInfo.encode(message.processorInfo, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RunPipelineMetadata_ProcessWithDocAiPipelineMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRunPipelineMetadata_ProcessWithDocAiPipelineMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.documents.push(reader.string());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.processorInfo = ProcessorInfo.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RunPipelineMetadata_ProcessWithDocAiPipelineMetadata {
    return {
      documents: globalThis.Array.isArray(object?.documents)
        ? object.documents.map((e: any) => globalThis.String(e))
        : [],
      processorInfo: isSet(object.processorInfo) ? ProcessorInfo.fromJSON(object.processorInfo) : undefined,
    };
  },

  toJSON(message: RunPipelineMetadata_ProcessWithDocAiPipelineMetadata): unknown {
    const obj: any = {};
    if (message.documents?.length) {
      obj.documents = message.documents;
    }
    if (message.processorInfo !== undefined) {
      obj.processorInfo = ProcessorInfo.toJSON(message.processorInfo);
    }
    return obj;
  },

  create(
    base?: DeepPartial<RunPipelineMetadata_ProcessWithDocAiPipelineMetadata>,
  ): RunPipelineMetadata_ProcessWithDocAiPipelineMetadata {
    return RunPipelineMetadata_ProcessWithDocAiPipelineMetadata.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<RunPipelineMetadata_ProcessWithDocAiPipelineMetadata>,
  ): RunPipelineMetadata_ProcessWithDocAiPipelineMetadata {
    const message = createBaseRunPipelineMetadata_ProcessWithDocAiPipelineMetadata();
    message.documents = object.documents?.map((e) => e) || [];
    message.processorInfo = (object.processorInfo !== undefined && object.processorInfo !== null)
      ? ProcessorInfo.fromPartial(object.processorInfo)
      : undefined;
    return message;
  },
};

function createBaseRunPipelineMetadata_IndividualDocumentStatus(): RunPipelineMetadata_IndividualDocumentStatus {
  return { documentId: "", status: undefined };
}

export const RunPipelineMetadata_IndividualDocumentStatus: MessageFns<RunPipelineMetadata_IndividualDocumentStatus> = {
  encode(
    message: RunPipelineMetadata_IndividualDocumentStatus,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.documentId !== "") {
      writer.uint32(10).string(message.documentId);
    }
    if (message.status !== undefined) {
      Status.encode(message.status, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RunPipelineMetadata_IndividualDocumentStatus {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRunPipelineMetadata_IndividualDocumentStatus();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.documentId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.status = Status.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RunPipelineMetadata_IndividualDocumentStatus {
    return {
      documentId: isSet(object.documentId) ? globalThis.String(object.documentId) : "",
      status: isSet(object.status) ? Status.fromJSON(object.status) : undefined,
    };
  },

  toJSON(message: RunPipelineMetadata_IndividualDocumentStatus): unknown {
    const obj: any = {};
    if (message.documentId !== "") {
      obj.documentId = message.documentId;
    }
    if (message.status !== undefined) {
      obj.status = Status.toJSON(message.status);
    }
    return obj;
  },

  create(
    base?: DeepPartial<RunPipelineMetadata_IndividualDocumentStatus>,
  ): RunPipelineMetadata_IndividualDocumentStatus {
    return RunPipelineMetadata_IndividualDocumentStatus.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<RunPipelineMetadata_IndividualDocumentStatus>,
  ): RunPipelineMetadata_IndividualDocumentStatus {
    const message = createBaseRunPipelineMetadata_IndividualDocumentStatus();
    message.documentId = object.documentId ?? "";
    message.status = (object.status !== undefined && object.status !== null)
      ? Status.fromPartial(object.status)
      : undefined;
    return message;
  },
};

function createBaseProcessorInfo(): ProcessorInfo {
  return { processorName: "", documentType: "", schemaName: "" };
}

export const ProcessorInfo: MessageFns<ProcessorInfo> = {
  encode(message: ProcessorInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.processorName !== "") {
      writer.uint32(10).string(message.processorName);
    }
    if (message.documentType !== "") {
      writer.uint32(18).string(message.documentType);
    }
    if (message.schemaName !== "") {
      writer.uint32(26).string(message.schemaName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ProcessorInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProcessorInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.processorName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.documentType = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.schemaName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ProcessorInfo {
    return {
      processorName: isSet(object.processorName) ? globalThis.String(object.processorName) : "",
      documentType: isSet(object.documentType) ? globalThis.String(object.documentType) : "",
      schemaName: isSet(object.schemaName) ? globalThis.String(object.schemaName) : "",
    };
  },

  toJSON(message: ProcessorInfo): unknown {
    const obj: any = {};
    if (message.processorName !== "") {
      obj.processorName = message.processorName;
    }
    if (message.documentType !== "") {
      obj.documentType = message.documentType;
    }
    if (message.schemaName !== "") {
      obj.schemaName = message.schemaName;
    }
    return obj;
  },

  create(base?: DeepPartial<ProcessorInfo>): ProcessorInfo {
    return ProcessorInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ProcessorInfo>): ProcessorInfo {
    const message = createBaseProcessorInfo();
    message.processorName = object.processorName ?? "";
    message.documentType = object.documentType ?? "";
    message.schemaName = object.schemaName ?? "";
    return message;
  },
};

function createBaseIngestPipelineConfig(): IngestPipelineConfig {
  return { documentAclPolicy: undefined, enableDocumentTextExtraction: false, folder: "", cloudFunction: "" };
}

export const IngestPipelineConfig: MessageFns<IngestPipelineConfig> = {
  encode(message: IngestPipelineConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.documentAclPolicy !== undefined) {
      Policy.encode(message.documentAclPolicy, writer.uint32(10).fork()).join();
    }
    if (message.enableDocumentTextExtraction !== false) {
      writer.uint32(16).bool(message.enableDocumentTextExtraction);
    }
    if (message.folder !== "") {
      writer.uint32(26).string(message.folder);
    }
    if (message.cloudFunction !== "") {
      writer.uint32(34).string(message.cloudFunction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): IngestPipelineConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseIngestPipelineConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.documentAclPolicy = Policy.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.enableDocumentTextExtraction = reader.bool();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.folder = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.cloudFunction = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): IngestPipelineConfig {
    return {
      documentAclPolicy: isSet(object.documentAclPolicy) ? Policy.fromJSON(object.documentAclPolicy) : undefined,
      enableDocumentTextExtraction: isSet(object.enableDocumentTextExtraction)
        ? globalThis.Boolean(object.enableDocumentTextExtraction)
        : false,
      folder: isSet(object.folder) ? globalThis.String(object.folder) : "",
      cloudFunction: isSet(object.cloudFunction) ? globalThis.String(object.cloudFunction) : "",
    };
  },

  toJSON(message: IngestPipelineConfig): unknown {
    const obj: any = {};
    if (message.documentAclPolicy !== undefined) {
      obj.documentAclPolicy = Policy.toJSON(message.documentAclPolicy);
    }
    if (message.enableDocumentTextExtraction !== false) {
      obj.enableDocumentTextExtraction = message.enableDocumentTextExtraction;
    }
    if (message.folder !== "") {
      obj.folder = message.folder;
    }
    if (message.cloudFunction !== "") {
      obj.cloudFunction = message.cloudFunction;
    }
    return obj;
  },

  create(base?: DeepPartial<IngestPipelineConfig>): IngestPipelineConfig {
    return IngestPipelineConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<IngestPipelineConfig>): IngestPipelineConfig {
    const message = createBaseIngestPipelineConfig();
    message.documentAclPolicy = (object.documentAclPolicy !== undefined && object.documentAclPolicy !== null)
      ? Policy.fromPartial(object.documentAclPolicy)
      : undefined;
    message.enableDocumentTextExtraction = object.enableDocumentTextExtraction ?? false;
    message.folder = object.folder ?? "";
    message.cloudFunction = object.cloudFunction ?? "";
    return message;
  },
};

function createBaseGcsIngestPipeline(): GcsIngestPipeline {
  return { inputPath: "", schemaName: "", processorType: "", skipIngestedDocuments: false, pipelineConfig: undefined };
}

export const GcsIngestPipeline: MessageFns<GcsIngestPipeline> = {
  encode(message: GcsIngestPipeline, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputPath !== "") {
      writer.uint32(10).string(message.inputPath);
    }
    if (message.schemaName !== "") {
      writer.uint32(18).string(message.schemaName);
    }
    if (message.processorType !== "") {
      writer.uint32(26).string(message.processorType);
    }
    if (message.skipIngestedDocuments !== false) {
      writer.uint32(32).bool(message.skipIngestedDocuments);
    }
    if (message.pipelineConfig !== undefined) {
      IngestPipelineConfig.encode(message.pipelineConfig, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GcsIngestPipeline {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGcsIngestPipeline();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputPath = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.schemaName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.processorType = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.skipIngestedDocuments = reader.bool();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.pipelineConfig = IngestPipelineConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GcsIngestPipeline {
    return {
      inputPath: isSet(object.inputPath) ? globalThis.String(object.inputPath) : "",
      schemaName: isSet(object.schemaName) ? globalThis.String(object.schemaName) : "",
      processorType: isSet(object.processorType) ? globalThis.String(object.processorType) : "",
      skipIngestedDocuments: isSet(object.skipIngestedDocuments)
        ? globalThis.Boolean(object.skipIngestedDocuments)
        : false,
      pipelineConfig: isSet(object.pipelineConfig) ? IngestPipelineConfig.fromJSON(object.pipelineConfig) : undefined,
    };
  },

  toJSON(message: GcsIngestPipeline): unknown {
    const obj: any = {};
    if (message.inputPath !== "") {
      obj.inputPath = message.inputPath;
    }
    if (message.schemaName !== "") {
      obj.schemaName = message.schemaName;
    }
    if (message.processorType !== "") {
      obj.processorType = message.processorType;
    }
    if (message.skipIngestedDocuments !== false) {
      obj.skipIngestedDocuments = message.skipIngestedDocuments;
    }
    if (message.pipelineConfig !== undefined) {
      obj.pipelineConfig = IngestPipelineConfig.toJSON(message.pipelineConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<GcsIngestPipeline>): GcsIngestPipeline {
    return GcsIngestPipeline.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GcsIngestPipeline>): GcsIngestPipeline {
    const message = createBaseGcsIngestPipeline();
    message.inputPath = object.inputPath ?? "";
    message.schemaName = object.schemaName ?? "";
    message.processorType = object.processorType ?? "";
    message.skipIngestedDocuments = object.skipIngestedDocuments ?? false;
    message.pipelineConfig = (object.pipelineConfig !== undefined && object.pipelineConfig !== null)
      ? IngestPipelineConfig.fromPartial(object.pipelineConfig)
      : undefined;
    return message;
  },
};

function createBaseGcsIngestWithDocAiProcessorsPipeline(): GcsIngestWithDocAiProcessorsPipeline {
  return {
    inputPath: "",
    splitClassifyProcessorInfo: undefined,
    extractProcessorInfos: [],
    processorResultsFolderPath: "",
    skipIngestedDocuments: false,
    pipelineConfig: undefined,
  };
}

export const GcsIngestWithDocAiProcessorsPipeline: MessageFns<GcsIngestWithDocAiProcessorsPipeline> = {
  encode(message: GcsIngestWithDocAiProcessorsPipeline, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputPath !== "") {
      writer.uint32(10).string(message.inputPath);
    }
    if (message.splitClassifyProcessorInfo !== undefined) {
      ProcessorInfo.encode(message.splitClassifyProcessorInfo, writer.uint32(18).fork()).join();
    }
    for (const v of message.extractProcessorInfos) {
      ProcessorInfo.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.processorResultsFolderPath !== "") {
      writer.uint32(34).string(message.processorResultsFolderPath);
    }
    if (message.skipIngestedDocuments !== false) {
      writer.uint32(40).bool(message.skipIngestedDocuments);
    }
    if (message.pipelineConfig !== undefined) {
      IngestPipelineConfig.encode(message.pipelineConfig, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GcsIngestWithDocAiProcessorsPipeline {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGcsIngestWithDocAiProcessorsPipeline();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputPath = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.splitClassifyProcessorInfo = ProcessorInfo.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.extractProcessorInfos.push(ProcessorInfo.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.processorResultsFolderPath = reader.string();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.skipIngestedDocuments = reader.bool();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.pipelineConfig = IngestPipelineConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GcsIngestWithDocAiProcessorsPipeline {
    return {
      inputPath: isSet(object.inputPath) ? globalThis.String(object.inputPath) : "",
      splitClassifyProcessorInfo: isSet(object.splitClassifyProcessorInfo)
        ? ProcessorInfo.fromJSON(object.splitClassifyProcessorInfo)
        : undefined,
      extractProcessorInfos: globalThis.Array.isArray(object?.extractProcessorInfos)
        ? object.extractProcessorInfos.map((e: any) => ProcessorInfo.fromJSON(e))
        : [],
      processorResultsFolderPath: isSet(object.processorResultsFolderPath)
        ? globalThis.String(object.processorResultsFolderPath)
        : "",
      skipIngestedDocuments: isSet(object.skipIngestedDocuments)
        ? globalThis.Boolean(object.skipIngestedDocuments)
        : false,
      pipelineConfig: isSet(object.pipelineConfig) ? IngestPipelineConfig.fromJSON(object.pipelineConfig) : undefined,
    };
  },

  toJSON(message: GcsIngestWithDocAiProcessorsPipeline): unknown {
    const obj: any = {};
    if (message.inputPath !== "") {
      obj.inputPath = message.inputPath;
    }
    if (message.splitClassifyProcessorInfo !== undefined) {
      obj.splitClassifyProcessorInfo = ProcessorInfo.toJSON(message.splitClassifyProcessorInfo);
    }
    if (message.extractProcessorInfos?.length) {
      obj.extractProcessorInfos = message.extractProcessorInfos.map((e) => ProcessorInfo.toJSON(e));
    }
    if (message.processorResultsFolderPath !== "") {
      obj.processorResultsFolderPath = message.processorResultsFolderPath;
    }
    if (message.skipIngestedDocuments !== false) {
      obj.skipIngestedDocuments = message.skipIngestedDocuments;
    }
    if (message.pipelineConfig !== undefined) {
      obj.pipelineConfig = IngestPipelineConfig.toJSON(message.pipelineConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<GcsIngestWithDocAiProcessorsPipeline>): GcsIngestWithDocAiProcessorsPipeline {
    return GcsIngestWithDocAiProcessorsPipeline.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GcsIngestWithDocAiProcessorsPipeline>): GcsIngestWithDocAiProcessorsPipeline {
    const message = createBaseGcsIngestWithDocAiProcessorsPipeline();
    message.inputPath = object.inputPath ?? "";
    message.splitClassifyProcessorInfo =
      (object.splitClassifyProcessorInfo !== undefined && object.splitClassifyProcessorInfo !== null)
        ? ProcessorInfo.fromPartial(object.splitClassifyProcessorInfo)
        : undefined;
    message.extractProcessorInfos = object.extractProcessorInfos?.map((e) => ProcessorInfo.fromPartial(e)) || [];
    message.processorResultsFolderPath = object.processorResultsFolderPath ?? "";
    message.skipIngestedDocuments = object.skipIngestedDocuments ?? false;
    message.pipelineConfig = (object.pipelineConfig !== undefined && object.pipelineConfig !== null)
      ? IngestPipelineConfig.fromPartial(object.pipelineConfig)
      : undefined;
    return message;
  },
};

function createBaseExportToCdwPipeline(): ExportToCdwPipeline {
  return { documents: [], exportFolderPath: "", docAiDataset: "", trainingSplitRatio: 0 };
}

export const ExportToCdwPipeline: MessageFns<ExportToCdwPipeline> = {
  encode(message: ExportToCdwPipeline, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.documents) {
      writer.uint32(10).string(v!);
    }
    if (message.exportFolderPath !== "") {
      writer.uint32(18).string(message.exportFolderPath);
    }
    if (message.docAiDataset !== "") {
      writer.uint32(26).string(message.docAiDataset);
    }
    if (message.trainingSplitRatio !== 0) {
      writer.uint32(37).float(message.trainingSplitRatio);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportToCdwPipeline {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportToCdwPipeline();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.documents.push(reader.string());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.exportFolderPath = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.docAiDataset = reader.string();
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.trainingSplitRatio = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportToCdwPipeline {
    return {
      documents: globalThis.Array.isArray(object?.documents)
        ? object.documents.map((e: any) => globalThis.String(e))
        : [],
      exportFolderPath: isSet(object.exportFolderPath) ? globalThis.String(object.exportFolderPath) : "",
      docAiDataset: isSet(object.docAiDataset) ? globalThis.String(object.docAiDataset) : "",
      trainingSplitRatio: isSet(object.trainingSplitRatio) ? globalThis.Number(object.trainingSplitRatio) : 0,
    };
  },

  toJSON(message: ExportToCdwPipeline): unknown {
    const obj: any = {};
    if (message.documents?.length) {
      obj.documents = message.documents;
    }
    if (message.exportFolderPath !== "") {
      obj.exportFolderPath = message.exportFolderPath;
    }
    if (message.docAiDataset !== "") {
      obj.docAiDataset = message.docAiDataset;
    }
    if (message.trainingSplitRatio !== 0) {
      obj.trainingSplitRatio = message.trainingSplitRatio;
    }
    return obj;
  },

  create(base?: DeepPartial<ExportToCdwPipeline>): ExportToCdwPipeline {
    return ExportToCdwPipeline.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportToCdwPipeline>): ExportToCdwPipeline {
    const message = createBaseExportToCdwPipeline();
    message.documents = object.documents?.map((e) => e) || [];
    message.exportFolderPath = object.exportFolderPath ?? "";
    message.docAiDataset = object.docAiDataset ?? "";
    message.trainingSplitRatio = object.trainingSplitRatio ?? 0;
    return message;
  },
};

function createBaseProcessWithDocAiPipeline(): ProcessWithDocAiPipeline {
  return { documents: [], exportFolderPath: "", processorInfo: undefined, processorResultsFolderPath: "" };
}

export const ProcessWithDocAiPipeline: MessageFns<ProcessWithDocAiPipeline> = {
  encode(message: ProcessWithDocAiPipeline, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.documents) {
      writer.uint32(10).string(v!);
    }
    if (message.exportFolderPath !== "") {
      writer.uint32(18).string(message.exportFolderPath);
    }
    if (message.processorInfo !== undefined) {
      ProcessorInfo.encode(message.processorInfo, writer.uint32(26).fork()).join();
    }
    if (message.processorResultsFolderPath !== "") {
      writer.uint32(34).string(message.processorResultsFolderPath);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ProcessWithDocAiPipeline {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProcessWithDocAiPipeline();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.documents.push(reader.string());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.exportFolderPath = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.processorInfo = ProcessorInfo.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.processorResultsFolderPath = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ProcessWithDocAiPipeline {
    return {
      documents: globalThis.Array.isArray(object?.documents)
        ? object.documents.map((e: any) => globalThis.String(e))
        : [],
      exportFolderPath: isSet(object.exportFolderPath) ? globalThis.String(object.exportFolderPath) : "",
      processorInfo: isSet(object.processorInfo) ? ProcessorInfo.fromJSON(object.processorInfo) : undefined,
      processorResultsFolderPath: isSet(object.processorResultsFolderPath)
        ? globalThis.String(object.processorResultsFolderPath)
        : "",
    };
  },

  toJSON(message: ProcessWithDocAiPipeline): unknown {
    const obj: any = {};
    if (message.documents?.length) {
      obj.documents = message.documents;
    }
    if (message.exportFolderPath !== "") {
      obj.exportFolderPath = message.exportFolderPath;
    }
    if (message.processorInfo !== undefined) {
      obj.processorInfo = ProcessorInfo.toJSON(message.processorInfo);
    }
    if (message.processorResultsFolderPath !== "") {
      obj.processorResultsFolderPath = message.processorResultsFolderPath;
    }
    return obj;
  },

  create(base?: DeepPartial<ProcessWithDocAiPipeline>): ProcessWithDocAiPipeline {
    return ProcessWithDocAiPipeline.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ProcessWithDocAiPipeline>): ProcessWithDocAiPipeline {
    const message = createBaseProcessWithDocAiPipeline();
    message.documents = object.documents?.map((e) => e) || [];
    message.exportFolderPath = object.exportFolderPath ?? "";
    message.processorInfo = (object.processorInfo !== undefined && object.processorInfo !== null)
      ? ProcessorInfo.fromPartial(object.processorInfo)
      : undefined;
    message.processorResultsFolderPath = object.processorResultsFolderPath ?? "";
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
