// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/aiplatform/v1beta1/machine_resources.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { AcceleratorType, acceleratorTypeFromJSON, acceleratorTypeToJSON } from "./accelerator_type.js";
import { ReservationAffinity } from "./reservation_affinity.js";

export const protobufPackage = "google.cloud.aiplatform.v1beta1";

/** Specification of a single machine. */
export interface MachineSpec {
  /**
   * Immutable. The type of the machine.
   *
   * See the [list of machine types supported for
   * prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types)
   *
   * See the [list of machine types supported for custom
   * training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types).
   *
   * For [DeployedModel][google.cloud.aiplatform.v1beta1.DeployedModel] this
   * field is optional, and the default value is `n1-standard-2`. For
   * [BatchPredictionJob][google.cloud.aiplatform.v1beta1.BatchPredictionJob] or
   * as part of [WorkerPoolSpec][google.cloud.aiplatform.v1beta1.WorkerPoolSpec]
   * this field is required.
   */
  machineType: string;
  /**
   * Immutable. The type of accelerator(s) that may be attached to the machine
   * as per
   * [accelerator_count][google.cloud.aiplatform.v1beta1.MachineSpec.accelerator_count].
   */
  acceleratorType: AcceleratorType;
  /** The number of accelerators to attach to the machine. */
  acceleratorCount: number;
  /**
   * Immutable. The topology of the TPUs. Corresponds to the TPU topologies
   * available from GKE. (Example: tpu_topology: "2x2x1").
   */
  tpuTopology: string;
  /**
   * Optional. Immutable. Configuration controlling how this resource pool
   * consumes reservation.
   */
  reservationAffinity: ReservationAffinity | undefined;
}

/**
 * A description of resources that are dedicated to a DeployedModel, and
 * that need a higher degree of manual configuration.
 */
export interface DedicatedResources {
  /**
   * Required. Immutable. The specification of a single machine used by the
   * prediction.
   */
  machineSpec:
    | MachineSpec
    | undefined;
  /**
   * Required. Immutable. The minimum number of machine replicas this
   * DeployedModel will be always deployed on. This value must be greater than
   * or equal to 1.
   *
   * If traffic against the DeployedModel increases, it may dynamically be
   * deployed onto more replicas, and as traffic decreases, some of these extra
   * replicas may be freed.
   */
  minReplicaCount: number;
  /**
   * Immutable. The maximum number of replicas this DeployedModel may be
   * deployed on when the traffic against it increases. If the requested value
   * is too large, the deployment will error, but if deployment succeeds then
   * the ability to scale the model to that many replicas is guaranteed (barring
   * service outages). If traffic against the DeployedModel increases beyond
   * what its replicas at maximum may handle, a portion of the traffic will be
   * dropped. If this value is not provided, will use
   * [min_replica_count][google.cloud.aiplatform.v1beta1.DedicatedResources.min_replica_count]
   * as the default value.
   *
   * The value of this field impacts the charge against Vertex CPU and GPU
   * quotas. Specifically, you will be charged for (max_replica_count *
   * number of cores in the selected machine type) and (max_replica_count *
   * number of GPUs per replica in the selected machine type).
   */
  maxReplicaCount: number;
  /**
   * Immutable. The metric specifications that overrides a resource
   * utilization metric (CPU utilization, accelerator's duty cycle, and so on)
   * target value (default to 60 if not set). At most one entry is allowed per
   * metric.
   *
   * If
   * [machine_spec.accelerator_count][google.cloud.aiplatform.v1beta1.MachineSpec.accelerator_count]
   * is above 0, the autoscaling will be based on both CPU utilization and
   * accelerator's duty cycle metrics and scale up when either metrics exceeds
   * its target value while scale down if both metrics are under their target
   * value. The default target value is 60 for both metrics.
   *
   * If
   * [machine_spec.accelerator_count][google.cloud.aiplatform.v1beta1.MachineSpec.accelerator_count]
   * is 0, the autoscaling will be based on CPU utilization metric only with
   * default target value 60 if not explicitly set.
   *
   * For example, in the case of Online Prediction, if you want to override
   * target CPU utilization to 80, you should set
   * [autoscaling_metric_specs.metric_name][google.cloud.aiplatform.v1beta1.AutoscalingMetricSpec.metric_name]
   * to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and
   * [autoscaling_metric_specs.target][google.cloud.aiplatform.v1beta1.AutoscalingMetricSpec.target]
   * to `80`.
   */
  autoscalingMetricSpecs: AutoscalingMetricSpec[];
  /**
   * Optional. If true, schedule the deployment workload on [spot
   * VMs](https://cloud.google.com/kubernetes-engine/docs/concepts/spot-vms).
   */
  spot: boolean;
}

/**
 * A description of resources that to large degree are decided by Vertex AI,
 * and require only a modest additional configuration.
 * Each Model supporting these resources documents its specific guidelines.
 */
export interface AutomaticResources {
  /**
   * Immutable. The minimum number of replicas this DeployedModel will be always
   * deployed on. If traffic against it increases, it may dynamically be
   * deployed onto more replicas up to
   * [max_replica_count][google.cloud.aiplatform.v1beta1.AutomaticResources.max_replica_count],
   * and as traffic decreases, some of these extra replicas may be freed. If the
   * requested value is too large, the deployment will error.
   */
  minReplicaCount: number;
  /**
   * Immutable. The maximum number of replicas this DeployedModel may be
   * deployed on when the traffic against it increases. If the requested value
   * is too large, the deployment will error, but if deployment succeeds then
   * the ability to scale the model to that many replicas is guaranteed (barring
   * service outages). If traffic against the DeployedModel increases beyond
   * what its replicas at maximum may handle, a portion of the traffic will be
   * dropped. If this value is not provided, a no upper bound for scaling under
   * heavy traffic will be assume, though Vertex AI may be unable to scale
   * beyond certain replica number.
   */
  maxReplicaCount: number;
}

/**
 * A description of resources that are used for performing batch operations, are
 * dedicated to a Model, and need manual configuration.
 */
export interface BatchDedicatedResources {
  /** Required. Immutable. The specification of a single machine. */
  machineSpec:
    | MachineSpec
    | undefined;
  /**
   * Immutable. The number of machine replicas used at the start of the batch
   * operation. If not set, Vertex AI decides starting number, not greater than
   * [max_replica_count][google.cloud.aiplatform.v1beta1.BatchDedicatedResources.max_replica_count]
   */
  startingReplicaCount: number;
  /**
   * Immutable. The maximum number of machine replicas the batch operation may
   * be scaled to. The default value is 10.
   */
  maxReplicaCount: number;
}

/** Statistics information about resource consumption. */
export interface ResourcesConsumed {
  /**
   * Output only. The number of replica hours used. Note that many replicas may
   * run in parallel, and additionally any given work may be queued for some
   * time. Therefore this value is not strictly related to wall time.
   */
  replicaHours: number;
}

/** Represents the spec of disk options. */
export interface DiskSpec {
  /**
   * Type of the boot disk (default is "pd-ssd").
   * Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or
   * "pd-standard" (Persistent Disk Hard Disk Drive).
   */
  bootDiskType: string;
  /** Size in GB of the boot disk (default is 100GB). */
  bootDiskSizeGb: number;
}

/**
 * Represents the spec of [persistent
 * disk][https://cloud.google.com/compute/docs/disks/persistent-disks] options.
 */
export interface PersistentDiskSpec {
  /**
   * Type of the disk (default is "pd-standard").
   * Valid values: "pd-ssd" (Persistent Disk Solid State Drive)
   * "pd-standard" (Persistent Disk Hard Disk Drive)
   * "pd-balanced" (Balanced Persistent Disk)
   * "pd-extreme" (Extreme Persistent Disk)
   */
  diskType: string;
  /** Size in GB of the disk (default is 100GB). */
  diskSizeGb: Long;
}

/** Represents a mount configuration for Network File System (NFS) to mount. */
export interface NfsMount {
  /** Required. IP address of the NFS server. */
  server: string;
  /**
   * Required. Source path exported from NFS server.
   * Has to start with '/', and combined with the ip address, it indicates
   * the source mount path in the form of `server:path`
   */
  path: string;
  /**
   * Required. Destination mount path. The NFS will be mounted for the user
   * under /mnt/nfs/<mount_point>
   */
  mountPoint: string;
}

/**
 * The metric specification that defines the target resource utilization
 * (CPU utilization, accelerator's duty cycle, and so on) for calculating the
 * desired replica count.
 */
export interface AutoscalingMetricSpec {
  /**
   * Required. The resource metric name.
   * Supported metrics:
   *
   * * For Online Prediction:
   * * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle`
   * * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
   */
  metricName: string;
  /**
   * The target resource utilization in percentage (1% - 100%) for the given
   * metric; once the real usage deviates from the target by a certain
   * percentage, the machine replicas change. The default value is 60
   * (representing 60%) if not provided.
   */
  target: number;
}

/**
 * A set of Shielded Instance options.
 * See [Images using supported Shielded VM
 * features](https://cloud.google.com/compute/docs/instances/modifying-shielded-vm).
 */
export interface ShieldedVmConfig {
  /**
   * Defines whether the instance has [Secure
   * Boot](https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#secure-boot)
   * enabled.
   *
   * Secure Boot helps ensure that the system only runs authentic software by
   * verifying the digital signature of all boot components, and halting the
   * boot process if signature verification fails.
   */
  enableSecureBoot: boolean;
}

function createBaseMachineSpec(): MachineSpec {
  return { machineType: "", acceleratorType: 0, acceleratorCount: 0, tpuTopology: "", reservationAffinity: undefined };
}

export const MachineSpec: MessageFns<MachineSpec> = {
  encode(message: MachineSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.machineType !== "") {
      writer.uint32(10).string(message.machineType);
    }
    if (message.acceleratorType !== 0) {
      writer.uint32(16).int32(message.acceleratorType);
    }
    if (message.acceleratorCount !== 0) {
      writer.uint32(24).int32(message.acceleratorCount);
    }
    if (message.tpuTopology !== "") {
      writer.uint32(34).string(message.tpuTopology);
    }
    if (message.reservationAffinity !== undefined) {
      ReservationAffinity.encode(message.reservationAffinity, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MachineSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMachineSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.machineType = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.acceleratorType = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.acceleratorCount = reader.int32();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.tpuTopology = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.reservationAffinity = ReservationAffinity.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MachineSpec {
    return {
      machineType: isSet(object.machineType) ? globalThis.String(object.machineType) : "",
      acceleratorType: isSet(object.acceleratorType) ? acceleratorTypeFromJSON(object.acceleratorType) : 0,
      acceleratorCount: isSet(object.acceleratorCount) ? globalThis.Number(object.acceleratorCount) : 0,
      tpuTopology: isSet(object.tpuTopology) ? globalThis.String(object.tpuTopology) : "",
      reservationAffinity: isSet(object.reservationAffinity)
        ? ReservationAffinity.fromJSON(object.reservationAffinity)
        : undefined,
    };
  },

  toJSON(message: MachineSpec): unknown {
    const obj: any = {};
    if (message.machineType !== "") {
      obj.machineType = message.machineType;
    }
    if (message.acceleratorType !== 0) {
      obj.acceleratorType = acceleratorTypeToJSON(message.acceleratorType);
    }
    if (message.acceleratorCount !== 0) {
      obj.acceleratorCount = Math.round(message.acceleratorCount);
    }
    if (message.tpuTopology !== "") {
      obj.tpuTopology = message.tpuTopology;
    }
    if (message.reservationAffinity !== undefined) {
      obj.reservationAffinity = ReservationAffinity.toJSON(message.reservationAffinity);
    }
    return obj;
  },

  create(base?: DeepPartial<MachineSpec>): MachineSpec {
    return MachineSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MachineSpec>): MachineSpec {
    const message = createBaseMachineSpec();
    message.machineType = object.machineType ?? "";
    message.acceleratorType = object.acceleratorType ?? 0;
    message.acceleratorCount = object.acceleratorCount ?? 0;
    message.tpuTopology = object.tpuTopology ?? "";
    message.reservationAffinity = (object.reservationAffinity !== undefined && object.reservationAffinity !== null)
      ? ReservationAffinity.fromPartial(object.reservationAffinity)
      : undefined;
    return message;
  },
};

function createBaseDedicatedResources(): DedicatedResources {
  return { machineSpec: undefined, minReplicaCount: 0, maxReplicaCount: 0, autoscalingMetricSpecs: [], spot: false };
}

export const DedicatedResources: MessageFns<DedicatedResources> = {
  encode(message: DedicatedResources, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.machineSpec !== undefined) {
      MachineSpec.encode(message.machineSpec, writer.uint32(10).fork()).join();
    }
    if (message.minReplicaCount !== 0) {
      writer.uint32(16).int32(message.minReplicaCount);
    }
    if (message.maxReplicaCount !== 0) {
      writer.uint32(24).int32(message.maxReplicaCount);
    }
    for (const v of message.autoscalingMetricSpecs) {
      AutoscalingMetricSpec.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.spot !== false) {
      writer.uint32(40).bool(message.spot);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DedicatedResources {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDedicatedResources();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.machineSpec = MachineSpec.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.minReplicaCount = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.maxReplicaCount = reader.int32();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.autoscalingMetricSpecs.push(AutoscalingMetricSpec.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.spot = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DedicatedResources {
    return {
      machineSpec: isSet(object.machineSpec) ? MachineSpec.fromJSON(object.machineSpec) : undefined,
      minReplicaCount: isSet(object.minReplicaCount) ? globalThis.Number(object.minReplicaCount) : 0,
      maxReplicaCount: isSet(object.maxReplicaCount) ? globalThis.Number(object.maxReplicaCount) : 0,
      autoscalingMetricSpecs: globalThis.Array.isArray(object?.autoscalingMetricSpecs)
        ? object.autoscalingMetricSpecs.map((e: any) => AutoscalingMetricSpec.fromJSON(e))
        : [],
      spot: isSet(object.spot) ? globalThis.Boolean(object.spot) : false,
    };
  },

  toJSON(message: DedicatedResources): unknown {
    const obj: any = {};
    if (message.machineSpec !== undefined) {
      obj.machineSpec = MachineSpec.toJSON(message.machineSpec);
    }
    if (message.minReplicaCount !== 0) {
      obj.minReplicaCount = Math.round(message.minReplicaCount);
    }
    if (message.maxReplicaCount !== 0) {
      obj.maxReplicaCount = Math.round(message.maxReplicaCount);
    }
    if (message.autoscalingMetricSpecs?.length) {
      obj.autoscalingMetricSpecs = message.autoscalingMetricSpecs.map((e) => AutoscalingMetricSpec.toJSON(e));
    }
    if (message.spot !== false) {
      obj.spot = message.spot;
    }
    return obj;
  },

  create(base?: DeepPartial<DedicatedResources>): DedicatedResources {
    return DedicatedResources.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DedicatedResources>): DedicatedResources {
    const message = createBaseDedicatedResources();
    message.machineSpec = (object.machineSpec !== undefined && object.machineSpec !== null)
      ? MachineSpec.fromPartial(object.machineSpec)
      : undefined;
    message.minReplicaCount = object.minReplicaCount ?? 0;
    message.maxReplicaCount = object.maxReplicaCount ?? 0;
    message.autoscalingMetricSpecs = object.autoscalingMetricSpecs?.map((e) => AutoscalingMetricSpec.fromPartial(e)) ||
      [];
    message.spot = object.spot ?? false;
    return message;
  },
};

function createBaseAutomaticResources(): AutomaticResources {
  return { minReplicaCount: 0, maxReplicaCount: 0 };
}

export const AutomaticResources: MessageFns<AutomaticResources> = {
  encode(message: AutomaticResources, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.minReplicaCount !== 0) {
      writer.uint32(8).int32(message.minReplicaCount);
    }
    if (message.maxReplicaCount !== 0) {
      writer.uint32(16).int32(message.maxReplicaCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutomaticResources {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutomaticResources();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.minReplicaCount = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.maxReplicaCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutomaticResources {
    return {
      minReplicaCount: isSet(object.minReplicaCount) ? globalThis.Number(object.minReplicaCount) : 0,
      maxReplicaCount: isSet(object.maxReplicaCount) ? globalThis.Number(object.maxReplicaCount) : 0,
    };
  },

  toJSON(message: AutomaticResources): unknown {
    const obj: any = {};
    if (message.minReplicaCount !== 0) {
      obj.minReplicaCount = Math.round(message.minReplicaCount);
    }
    if (message.maxReplicaCount !== 0) {
      obj.maxReplicaCount = Math.round(message.maxReplicaCount);
    }
    return obj;
  },

  create(base?: DeepPartial<AutomaticResources>): AutomaticResources {
    return AutomaticResources.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutomaticResources>): AutomaticResources {
    const message = createBaseAutomaticResources();
    message.minReplicaCount = object.minReplicaCount ?? 0;
    message.maxReplicaCount = object.maxReplicaCount ?? 0;
    return message;
  },
};

function createBaseBatchDedicatedResources(): BatchDedicatedResources {
  return { machineSpec: undefined, startingReplicaCount: 0, maxReplicaCount: 0 };
}

export const BatchDedicatedResources: MessageFns<BatchDedicatedResources> = {
  encode(message: BatchDedicatedResources, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.machineSpec !== undefined) {
      MachineSpec.encode(message.machineSpec, writer.uint32(10).fork()).join();
    }
    if (message.startingReplicaCount !== 0) {
      writer.uint32(16).int32(message.startingReplicaCount);
    }
    if (message.maxReplicaCount !== 0) {
      writer.uint32(24).int32(message.maxReplicaCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchDedicatedResources {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchDedicatedResources();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.machineSpec = MachineSpec.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.startingReplicaCount = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.maxReplicaCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchDedicatedResources {
    return {
      machineSpec: isSet(object.machineSpec) ? MachineSpec.fromJSON(object.machineSpec) : undefined,
      startingReplicaCount: isSet(object.startingReplicaCount) ? globalThis.Number(object.startingReplicaCount) : 0,
      maxReplicaCount: isSet(object.maxReplicaCount) ? globalThis.Number(object.maxReplicaCount) : 0,
    };
  },

  toJSON(message: BatchDedicatedResources): unknown {
    const obj: any = {};
    if (message.machineSpec !== undefined) {
      obj.machineSpec = MachineSpec.toJSON(message.machineSpec);
    }
    if (message.startingReplicaCount !== 0) {
      obj.startingReplicaCount = Math.round(message.startingReplicaCount);
    }
    if (message.maxReplicaCount !== 0) {
      obj.maxReplicaCount = Math.round(message.maxReplicaCount);
    }
    return obj;
  },

  create(base?: DeepPartial<BatchDedicatedResources>): BatchDedicatedResources {
    return BatchDedicatedResources.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchDedicatedResources>): BatchDedicatedResources {
    const message = createBaseBatchDedicatedResources();
    message.machineSpec = (object.machineSpec !== undefined && object.machineSpec !== null)
      ? MachineSpec.fromPartial(object.machineSpec)
      : undefined;
    message.startingReplicaCount = object.startingReplicaCount ?? 0;
    message.maxReplicaCount = object.maxReplicaCount ?? 0;
    return message;
  },
};

function createBaseResourcesConsumed(): ResourcesConsumed {
  return { replicaHours: 0 };
}

export const ResourcesConsumed: MessageFns<ResourcesConsumed> = {
  encode(message: ResourcesConsumed, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.replicaHours !== 0) {
      writer.uint32(9).double(message.replicaHours);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ResourcesConsumed {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseResourcesConsumed();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 9) {
            break;
          }

          message.replicaHours = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ResourcesConsumed {
    return { replicaHours: isSet(object.replicaHours) ? globalThis.Number(object.replicaHours) : 0 };
  },

  toJSON(message: ResourcesConsumed): unknown {
    const obj: any = {};
    if (message.replicaHours !== 0) {
      obj.replicaHours = message.replicaHours;
    }
    return obj;
  },

  create(base?: DeepPartial<ResourcesConsumed>): ResourcesConsumed {
    return ResourcesConsumed.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ResourcesConsumed>): ResourcesConsumed {
    const message = createBaseResourcesConsumed();
    message.replicaHours = object.replicaHours ?? 0;
    return message;
  },
};

function createBaseDiskSpec(): DiskSpec {
  return { bootDiskType: "", bootDiskSizeGb: 0 };
}

export const DiskSpec: MessageFns<DiskSpec> = {
  encode(message: DiskSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.bootDiskType !== "") {
      writer.uint32(10).string(message.bootDiskType);
    }
    if (message.bootDiskSizeGb !== 0) {
      writer.uint32(16).int32(message.bootDiskSizeGb);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DiskSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDiskSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.bootDiskType = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.bootDiskSizeGb = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DiskSpec {
    return {
      bootDiskType: isSet(object.bootDiskType) ? globalThis.String(object.bootDiskType) : "",
      bootDiskSizeGb: isSet(object.bootDiskSizeGb) ? globalThis.Number(object.bootDiskSizeGb) : 0,
    };
  },

  toJSON(message: DiskSpec): unknown {
    const obj: any = {};
    if (message.bootDiskType !== "") {
      obj.bootDiskType = message.bootDiskType;
    }
    if (message.bootDiskSizeGb !== 0) {
      obj.bootDiskSizeGb = Math.round(message.bootDiskSizeGb);
    }
    return obj;
  },

  create(base?: DeepPartial<DiskSpec>): DiskSpec {
    return DiskSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DiskSpec>): DiskSpec {
    const message = createBaseDiskSpec();
    message.bootDiskType = object.bootDiskType ?? "";
    message.bootDiskSizeGb = object.bootDiskSizeGb ?? 0;
    return message;
  },
};

function createBasePersistentDiskSpec(): PersistentDiskSpec {
  return { diskType: "", diskSizeGb: Long.ZERO };
}

export const PersistentDiskSpec: MessageFns<PersistentDiskSpec> = {
  encode(message: PersistentDiskSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.diskType !== "") {
      writer.uint32(10).string(message.diskType);
    }
    if (!message.diskSizeGb.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.diskSizeGb.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PersistentDiskSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePersistentDiskSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.diskType = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.diskSizeGb = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PersistentDiskSpec {
    return {
      diskType: isSet(object.diskType) ? globalThis.String(object.diskType) : "",
      diskSizeGb: isSet(object.diskSizeGb) ? Long.fromValue(object.diskSizeGb) : Long.ZERO,
    };
  },

  toJSON(message: PersistentDiskSpec): unknown {
    const obj: any = {};
    if (message.diskType !== "") {
      obj.diskType = message.diskType;
    }
    if (!message.diskSizeGb.equals(Long.ZERO)) {
      obj.diskSizeGb = (message.diskSizeGb || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<PersistentDiskSpec>): PersistentDiskSpec {
    return PersistentDiskSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PersistentDiskSpec>): PersistentDiskSpec {
    const message = createBasePersistentDiskSpec();
    message.diskType = object.diskType ?? "";
    message.diskSizeGb = (object.diskSizeGb !== undefined && object.diskSizeGb !== null)
      ? Long.fromValue(object.diskSizeGb)
      : Long.ZERO;
    return message;
  },
};

function createBaseNfsMount(): NfsMount {
  return { server: "", path: "", mountPoint: "" };
}

export const NfsMount: MessageFns<NfsMount> = {
  encode(message: NfsMount, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.server !== "") {
      writer.uint32(10).string(message.server);
    }
    if (message.path !== "") {
      writer.uint32(18).string(message.path);
    }
    if (message.mountPoint !== "") {
      writer.uint32(26).string(message.mountPoint);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NfsMount {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNfsMount();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.server = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.path = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.mountPoint = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NfsMount {
    return {
      server: isSet(object.server) ? globalThis.String(object.server) : "",
      path: isSet(object.path) ? globalThis.String(object.path) : "",
      mountPoint: isSet(object.mountPoint) ? globalThis.String(object.mountPoint) : "",
    };
  },

  toJSON(message: NfsMount): unknown {
    const obj: any = {};
    if (message.server !== "") {
      obj.server = message.server;
    }
    if (message.path !== "") {
      obj.path = message.path;
    }
    if (message.mountPoint !== "") {
      obj.mountPoint = message.mountPoint;
    }
    return obj;
  },

  create(base?: DeepPartial<NfsMount>): NfsMount {
    return NfsMount.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<NfsMount>): NfsMount {
    const message = createBaseNfsMount();
    message.server = object.server ?? "";
    message.path = object.path ?? "";
    message.mountPoint = object.mountPoint ?? "";
    return message;
  },
};

function createBaseAutoscalingMetricSpec(): AutoscalingMetricSpec {
  return { metricName: "", target: 0 };
}

export const AutoscalingMetricSpec: MessageFns<AutoscalingMetricSpec> = {
  encode(message: AutoscalingMetricSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.metricName !== "") {
      writer.uint32(10).string(message.metricName);
    }
    if (message.target !== 0) {
      writer.uint32(16).int32(message.target);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutoscalingMetricSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutoscalingMetricSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.metricName = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.target = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutoscalingMetricSpec {
    return {
      metricName: isSet(object.metricName) ? globalThis.String(object.metricName) : "",
      target: isSet(object.target) ? globalThis.Number(object.target) : 0,
    };
  },

  toJSON(message: AutoscalingMetricSpec): unknown {
    const obj: any = {};
    if (message.metricName !== "") {
      obj.metricName = message.metricName;
    }
    if (message.target !== 0) {
      obj.target = Math.round(message.target);
    }
    return obj;
  },

  create(base?: DeepPartial<AutoscalingMetricSpec>): AutoscalingMetricSpec {
    return AutoscalingMetricSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutoscalingMetricSpec>): AutoscalingMetricSpec {
    const message = createBaseAutoscalingMetricSpec();
    message.metricName = object.metricName ?? "";
    message.target = object.target ?? 0;
    return message;
  },
};

function createBaseShieldedVmConfig(): ShieldedVmConfig {
  return { enableSecureBoot: false };
}

export const ShieldedVmConfig: MessageFns<ShieldedVmConfig> = {
  encode(message: ShieldedVmConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.enableSecureBoot !== false) {
      writer.uint32(8).bool(message.enableSecureBoot);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ShieldedVmConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseShieldedVmConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.enableSecureBoot = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ShieldedVmConfig {
    return { enableSecureBoot: isSet(object.enableSecureBoot) ? globalThis.Boolean(object.enableSecureBoot) : false };
  },

  toJSON(message: ShieldedVmConfig): unknown {
    const obj: any = {};
    if (message.enableSecureBoot !== false) {
      obj.enableSecureBoot = message.enableSecureBoot;
    }
    return obj;
  },

  create(base?: DeepPartial<ShieldedVmConfig>): ShieldedVmConfig {
    return ShieldedVmConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ShieldedVmConfig>): ShieldedVmConfig {
    const message = createBaseShieldedVmConfig();
    message.enableSecureBoot = object.enableSecureBoot ?? false;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
