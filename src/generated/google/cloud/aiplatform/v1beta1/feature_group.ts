// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/aiplatform/v1beta1/feature_group.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { BigQuerySource } from "./io.js";

export const protobufPackage = "google.cloud.aiplatform.v1beta1";

/** Vertex AI Feature Group. */
export interface FeatureGroup {
  /**
   * Indicates that features for this group come from BigQuery Table/View.
   * By default treats the source as a sparse time series source. The BigQuery
   * source table or view must have at least one entity ID column and a column
   * named `feature_timestamp`.
   */
  bigQuery?:
    | FeatureGroup_BigQuery
    | undefined;
  /**
   * Identifier. Name of the FeatureGroup. Format:
   * `projects/{project}/locations/{location}/featureGroups/{featureGroup}`
   */
  name: string;
  /** Output only. Timestamp when this FeatureGroup was created. */
  createTime:
    | Date
    | undefined;
  /** Output only. Timestamp when this FeatureGroup was last updated. */
  updateTime:
    | Date
    | undefined;
  /**
   * Optional. Used to perform consistent read-modify-write updates. If not set,
   * a blind "overwrite" update happens.
   */
  etag: string;
  /**
   * Optional. The labels with user-defined metadata to organize your
   * FeatureGroup.
   *
   * Label keys and values can be no longer than 64 characters
   * (Unicode codepoints), can only contain lowercase letters, numeric
   * characters, underscores and dashes. International characters are allowed.
   *
   * See https://goo.gl/xmQnxf for more information on and examples of labels.
   * No more than 64 user labels can be associated with one
   * FeatureGroup(System labels are excluded)." System reserved label keys
   * are prefixed with "aiplatform.googleapis.com/" and are immutable.
   */
  labels: { [key: string]: string };
  /** Optional. Description of the FeatureGroup. */
  description: string;
}

/** Input source type for BigQuery Tables and Views. */
export interface FeatureGroup_BigQuery {
  /**
   * Required. Immutable. The BigQuery source URI that points to either a
   * BigQuery Table or View.
   */
  bigQuerySource:
    | BigQuerySource
    | undefined;
  /**
   * Optional. Columns to construct entity_id / row keys.
   * If not provided defaults to `entity_id`.
   */
  entityIdColumns: string[];
  /** Optional. Set if the data source is not a time-series. */
  staticDataSource: boolean;
  /**
   * Optional. If the source is a time-series source, this can be set to
   * control how downstream sources (ex:
   * [FeatureView][google.cloud.aiplatform.v1beta1.FeatureView] ) will treat
   * time-series sources. If not set, will treat the source as a time-series
   * source with `feature_timestamp` as timestamp column and no scan boundary.
   */
  timeSeries:
    | FeatureGroup_BigQuery_TimeSeries
    | undefined;
  /**
   * Optional. If set, all feature values will be fetched
   * from a single row per unique entityId including nulls.
   * If not set, will collapse all rows for each unique entityId into a singe
   * row with any non-null values if present, if no non-null values are
   * present will sync null.
   * ex: If source has schema
   * `(entity_id, feature_timestamp, f0, f1)` and the following rows:
   * `(e1, 2020-01-01T10:00:00.123Z, 10, 15)`
   * `(e1, 2020-02-01T10:00:00.123Z, 20, null)`
   * If dense is set, `(e1, 20, null)` is synced to online stores. If dense is
   * not set, `(e1, 20, 15)` is synced to online stores.
   */
  dense: boolean;
}

export interface FeatureGroup_BigQuery_TimeSeries {
  /**
   * Optional. Column hosting timestamp values for a time-series source.
   * Will be used to determine the latest `feature_values` for each entity.
   * Optional. If not provided, column named `feature_timestamp` of
   * type `TIMESTAMP` will be used.
   */
  timestampColumn: string;
}

export interface FeatureGroup_LabelsEntry {
  key: string;
  value: string;
}

function createBaseFeatureGroup(): FeatureGroup {
  return {
    bigQuery: undefined,
    name: "",
    createTime: undefined,
    updateTime: undefined,
    etag: "",
    labels: {},
    description: "",
  };
}

export const FeatureGroup: MessageFns<FeatureGroup> = {
  encode(message: FeatureGroup, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.bigQuery !== undefined) {
      FeatureGroup_BigQuery.encode(message.bigQuery, writer.uint32(58).fork()).join();
    }
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(18).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(26).fork()).join();
    }
    if (message.etag !== "") {
      writer.uint32(34).string(message.etag);
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      FeatureGroup_LabelsEntry.encode({ key: key as any, value }, writer.uint32(42).fork()).join();
    });
    if (message.description !== "") {
      writer.uint32(50).string(message.description);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FeatureGroup {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFeatureGroup();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 7:
          if (tag !== 58) {
            break;
          }

          message.bigQuery = FeatureGroup_BigQuery.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.etag = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          const entry5 = FeatureGroup_LabelsEntry.decode(reader, reader.uint32());
          if (entry5.value !== undefined) {
            message.labels[entry5.key] = entry5.value;
          }
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.description = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FeatureGroup {
    return {
      bigQuery: isSet(object.bigQuery) ? FeatureGroup_BigQuery.fromJSON(object.bigQuery) : undefined,
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      etag: isSet(object.etag) ? globalThis.String(object.etag) : "",
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      description: isSet(object.description) ? globalThis.String(object.description) : "",
    };
  },

  toJSON(message: FeatureGroup): unknown {
    const obj: any = {};
    if (message.bigQuery !== undefined) {
      obj.bigQuery = FeatureGroup_BigQuery.toJSON(message.bigQuery);
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (message.etag !== "") {
      obj.etag = message.etag;
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    return obj;
  },

  create(base?: DeepPartial<FeatureGroup>): FeatureGroup {
    return FeatureGroup.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FeatureGroup>): FeatureGroup {
    const message = createBaseFeatureGroup();
    message.bigQuery = (object.bigQuery !== undefined && object.bigQuery !== null)
      ? FeatureGroup_BigQuery.fromPartial(object.bigQuery)
      : undefined;
    message.name = object.name ?? "";
    message.createTime = object.createTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.etag = object.etag ?? "";
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.description = object.description ?? "";
    return message;
  },
};

function createBaseFeatureGroup_BigQuery(): FeatureGroup_BigQuery {
  return {
    bigQuerySource: undefined,
    entityIdColumns: [],
    staticDataSource: false,
    timeSeries: undefined,
    dense: false,
  };
}

export const FeatureGroup_BigQuery: MessageFns<FeatureGroup_BigQuery> = {
  encode(message: FeatureGroup_BigQuery, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.bigQuerySource !== undefined) {
      BigQuerySource.encode(message.bigQuerySource, writer.uint32(10).fork()).join();
    }
    for (const v of message.entityIdColumns) {
      writer.uint32(18).string(v!);
    }
    if (message.staticDataSource !== false) {
      writer.uint32(24).bool(message.staticDataSource);
    }
    if (message.timeSeries !== undefined) {
      FeatureGroup_BigQuery_TimeSeries.encode(message.timeSeries, writer.uint32(34).fork()).join();
    }
    if (message.dense !== false) {
      writer.uint32(40).bool(message.dense);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FeatureGroup_BigQuery {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFeatureGroup_BigQuery();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.bigQuerySource = BigQuerySource.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.entityIdColumns.push(reader.string());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.staticDataSource = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.timeSeries = FeatureGroup_BigQuery_TimeSeries.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.dense = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FeatureGroup_BigQuery {
    return {
      bigQuerySource: isSet(object.bigQuerySource) ? BigQuerySource.fromJSON(object.bigQuerySource) : undefined,
      entityIdColumns: globalThis.Array.isArray(object?.entityIdColumns)
        ? object.entityIdColumns.map((e: any) => globalThis.String(e))
        : [],
      staticDataSource: isSet(object.staticDataSource) ? globalThis.Boolean(object.staticDataSource) : false,
      timeSeries: isSet(object.timeSeries) ? FeatureGroup_BigQuery_TimeSeries.fromJSON(object.timeSeries) : undefined,
      dense: isSet(object.dense) ? globalThis.Boolean(object.dense) : false,
    };
  },

  toJSON(message: FeatureGroup_BigQuery): unknown {
    const obj: any = {};
    if (message.bigQuerySource !== undefined) {
      obj.bigQuerySource = BigQuerySource.toJSON(message.bigQuerySource);
    }
    if (message.entityIdColumns?.length) {
      obj.entityIdColumns = message.entityIdColumns;
    }
    if (message.staticDataSource !== false) {
      obj.staticDataSource = message.staticDataSource;
    }
    if (message.timeSeries !== undefined) {
      obj.timeSeries = FeatureGroup_BigQuery_TimeSeries.toJSON(message.timeSeries);
    }
    if (message.dense !== false) {
      obj.dense = message.dense;
    }
    return obj;
  },

  create(base?: DeepPartial<FeatureGroup_BigQuery>): FeatureGroup_BigQuery {
    return FeatureGroup_BigQuery.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FeatureGroup_BigQuery>): FeatureGroup_BigQuery {
    const message = createBaseFeatureGroup_BigQuery();
    message.bigQuerySource = (object.bigQuerySource !== undefined && object.bigQuerySource !== null)
      ? BigQuerySource.fromPartial(object.bigQuerySource)
      : undefined;
    message.entityIdColumns = object.entityIdColumns?.map((e) => e) || [];
    message.staticDataSource = object.staticDataSource ?? false;
    message.timeSeries = (object.timeSeries !== undefined && object.timeSeries !== null)
      ? FeatureGroup_BigQuery_TimeSeries.fromPartial(object.timeSeries)
      : undefined;
    message.dense = object.dense ?? false;
    return message;
  },
};

function createBaseFeatureGroup_BigQuery_TimeSeries(): FeatureGroup_BigQuery_TimeSeries {
  return { timestampColumn: "" };
}

export const FeatureGroup_BigQuery_TimeSeries: MessageFns<FeatureGroup_BigQuery_TimeSeries> = {
  encode(message: FeatureGroup_BigQuery_TimeSeries, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.timestampColumn !== "") {
      writer.uint32(10).string(message.timestampColumn);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FeatureGroup_BigQuery_TimeSeries {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFeatureGroup_BigQuery_TimeSeries();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.timestampColumn = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FeatureGroup_BigQuery_TimeSeries {
    return { timestampColumn: isSet(object.timestampColumn) ? globalThis.String(object.timestampColumn) : "" };
  },

  toJSON(message: FeatureGroup_BigQuery_TimeSeries): unknown {
    const obj: any = {};
    if (message.timestampColumn !== "") {
      obj.timestampColumn = message.timestampColumn;
    }
    return obj;
  },

  create(base?: DeepPartial<FeatureGroup_BigQuery_TimeSeries>): FeatureGroup_BigQuery_TimeSeries {
    return FeatureGroup_BigQuery_TimeSeries.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FeatureGroup_BigQuery_TimeSeries>): FeatureGroup_BigQuery_TimeSeries {
    const message = createBaseFeatureGroup_BigQuery_TimeSeries();
    message.timestampColumn = object.timestampColumn ?? "";
    return message;
  },
};

function createBaseFeatureGroup_LabelsEntry(): FeatureGroup_LabelsEntry {
  return { key: "", value: "" };
}

export const FeatureGroup_LabelsEntry: MessageFns<FeatureGroup_LabelsEntry> = {
  encode(message: FeatureGroup_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FeatureGroup_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFeatureGroup_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FeatureGroup_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: FeatureGroup_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<FeatureGroup_LabelsEntry>): FeatureGroup_LabelsEntry {
    return FeatureGroup_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FeatureGroup_LabelsEntry>): FeatureGroup_LabelsEntry {
    const message = createBaseFeatureGroup_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
