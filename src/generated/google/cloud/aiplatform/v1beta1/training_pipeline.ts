// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/aiplatform/v1beta1/training_pipeline.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Value } from "../../../protobuf/struct.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { Status } from "../../../rpc/status.js";
import { EncryptionSpec } from "./encryption_spec.js";
import { BigQueryDestination, GcsDestination } from "./io.js";
import { Model } from "./model.js";
import { PipelineState, pipelineStateFromJSON, pipelineStateToJSON } from "./pipeline_state.js";

export const protobufPackage = "google.cloud.aiplatform.v1beta1";

/**
 * The TrainingPipeline orchestrates tasks associated with training a Model. It
 * always executes the training task, and optionally may also
 * export data from Vertex AI's Dataset which becomes the training input,
 * [upload][google.cloud.aiplatform.v1beta1.ModelService.UploadModel] the Model
 * to Vertex AI, and evaluate the Model.
 */
export interface TrainingPipeline {
  /** Output only. Resource name of the TrainingPipeline. */
  name: string;
  /** Required. The user-defined name of this TrainingPipeline. */
  displayName: string;
  /**
   * Specifies Vertex AI owned input data that may be used for training the
   * Model. The TrainingPipeline's
   * [training_task_definition][google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition]
   * should make clear whether this config is used and if there are any special
   * requirements on how it should be filled. If nothing about this config is
   * mentioned in the
   * [training_task_definition][google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition],
   * then it should be assumed that the TrainingPipeline does not depend on this
   * configuration.
   */
  inputDataConfig:
    | InputDataConfig
    | undefined;
  /**
   * Required. A Google Cloud Storage path to the YAML file that defines the
   * training task which is responsible for producing the model artifact, and
   * may also include additional auxiliary work. The definition files that can
   * be used here are found in
   * gs://google-cloud-aiplatform/schema/trainingjob/definition/.
   * Note: The URI given on output will be immutable and probably different,
   * including the URI scheme, than the one given on input. The output URI will
   * point to a location where the user only has a read access.
   */
  trainingTaskDefinition: string;
  /**
   * Required. The training task's parameter(s), as specified in the
   * [training_task_definition][google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition]'s
   * `inputs`.
   */
  trainingTaskInputs:
    | any
    | undefined;
  /**
   * Output only. The metadata information as specified in the
   * [training_task_definition][google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition]'s
   * `metadata`. This metadata is an auxiliary runtime and final information
   * about the training task. While the pipeline is running this information is
   * populated only at a best effort basis. Only present if the
   * pipeline's
   * [training_task_definition][google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition]
   * contains `metadata` object.
   */
  trainingTaskMetadata:
    | any
    | undefined;
  /**
   * Describes the Model that may be uploaded (via
   * [ModelService.UploadModel][google.cloud.aiplatform.v1beta1.ModelService.UploadModel])
   * by this TrainingPipeline. The TrainingPipeline's
   * [training_task_definition][google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition]
   * should make clear whether this Model description should be populated, and
   * if there are any special requirements regarding how it should be filled. If
   * nothing is mentioned in the
   * [training_task_definition][google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition],
   * then it should be assumed that this field should not be filled and the
   * training task either uploads the Model without a need of this information,
   * or that training task does not support uploading a Model as part of the
   * pipeline. When the Pipeline's state becomes `PIPELINE_STATE_SUCCEEDED` and
   * the trained Model had been uploaded into Vertex AI, then the
   * model_to_upload's resource
   * [name][google.cloud.aiplatform.v1beta1.Model.name] is populated. The Model
   * is always uploaded into the Project and Location in which this pipeline
   * is.
   */
  modelToUpload:
    | Model
    | undefined;
  /**
   * Optional. The ID to use for the uploaded Model, which will become the final
   * component of the model resource name.
   *
   * This value may be up to 63 characters, and valid characters are
   * `[a-z0-9_-]`. The first character cannot be a number or hyphen.
   */
  modelId: string;
  /**
   * Optional. When specify this field, the `model_to_upload` will not be
   * uploaded as a new model, instead, it will become a new version of this
   * `parent_model`.
   */
  parentModel: string;
  /** Output only. The detailed state of the pipeline. */
  state: PipelineState;
  /**
   * Output only. Only populated when the pipeline's state is
   * `PIPELINE_STATE_FAILED` or `PIPELINE_STATE_CANCELLED`.
   */
  error:
    | Status
    | undefined;
  /** Output only. Time when the TrainingPipeline was created. */
  createTime:
    | Date
    | undefined;
  /**
   * Output only. Time when the TrainingPipeline for the first time entered the
   * `PIPELINE_STATE_RUNNING` state.
   */
  startTime:
    | Date
    | undefined;
  /**
   * Output only. Time when the TrainingPipeline entered any of the following
   * states: `PIPELINE_STATE_SUCCEEDED`, `PIPELINE_STATE_FAILED`,
   * `PIPELINE_STATE_CANCELLED`.
   */
  endTime:
    | Date
    | undefined;
  /** Output only. Time when the TrainingPipeline was most recently updated. */
  updateTime:
    | Date
    | undefined;
  /**
   * The labels with user-defined metadata to organize TrainingPipelines.
   *
   * Label keys and values can be no longer than 64 characters
   * (Unicode codepoints), can only contain lowercase letters, numeric
   * characters, underscores and dashes. International characters are allowed.
   *
   * See https://goo.gl/xmQnxf for more information and examples of labels.
   */
  labels: { [key: string]: string };
  /**
   * Customer-managed encryption key spec for a TrainingPipeline. If set, this
   * TrainingPipeline will be secured by this key.
   *
   * Note: Model trained by this TrainingPipeline is also secured by this key if
   * [model_to_upload][google.cloud.aiplatform.v1beta1.TrainingPipeline.encryption_spec]
   * is not set separately.
   */
  encryptionSpec: EncryptionSpec | undefined;
}

export interface TrainingPipeline_LabelsEntry {
  key: string;
  value: string;
}

/**
 * Specifies Vertex AI owned input data to be used for training, and
 * possibly evaluating, the Model.
 */
export interface InputDataConfig {
  /** Split based on fractions defining the size of each set. */
  fractionSplit?:
    | FractionSplit
    | undefined;
  /** Split based on the provided filters for each set. */
  filterSplit?:
    | FilterSplit
    | undefined;
  /**
   * Supported only for tabular Datasets.
   *
   * Split based on a predefined key.
   */
  predefinedSplit?:
    | PredefinedSplit
    | undefined;
  /**
   * Supported only for tabular Datasets.
   *
   * Split based on the timestamp of the input data pieces.
   */
  timestampSplit?:
    | TimestampSplit
    | undefined;
  /**
   * Supported only for tabular Datasets.
   *
   * Split based on the distribution of the specified column.
   */
  stratifiedSplit?:
    | StratifiedSplit
    | undefined;
  /**
   * The Cloud Storage location where the training data is to be
   * written to. In the given directory a new directory is created with
   * name:
   * `dataset-<dataset-id>-<annotation-type>-<timestamp-of-training-call>`
   * where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format.
   * All training input data is written into that directory.
   *
   * The Vertex AI environment variables representing Cloud Storage
   * data URIs are represented in the Cloud Storage wildcard
   * format to support sharded data. e.g.: "gs://.../training-*.jsonl"
   *
   * * AIP_DATA_FORMAT = "jsonl" for non-tabular data, "csv" for tabular data
   * * AIP_TRAINING_DATA_URI =
   * "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/training-*.${AIP_DATA_FORMAT}"
   *
   * * AIP_VALIDATION_DATA_URI =
   * "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/validation-*.${AIP_DATA_FORMAT}"
   *
   * * AIP_TEST_DATA_URI =
   * "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/test-*.${AIP_DATA_FORMAT}"
   */
  gcsDestination?:
    | GcsDestination
    | undefined;
  /**
   * Only applicable to custom training with tabular Dataset with BigQuery
   * source.
   *
   * The BigQuery project location where the training data is to be written
   * to. In the given project a new dataset is created with name
   * `dataset_<dataset-id>_<annotation-type>_<timestamp-of-training-call>`
   * where timestamp is in YYYY_MM_DDThh_mm_ss_sssZ format. All training
   * input data is written into that dataset. In the dataset three
   * tables are created, `training`, `validation` and `test`.
   *
   * * AIP_DATA_FORMAT = "bigquery".
   * * AIP_TRAINING_DATA_URI  =
   * "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.training"
   *
   * * AIP_VALIDATION_DATA_URI =
   * "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.validation"
   *
   * * AIP_TEST_DATA_URI =
   * "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.test"
   */
  bigqueryDestination?:
    | BigQueryDestination
    | undefined;
  /**
   * Required. The ID of the Dataset in the same Project and Location which data
   * will be used to train the Model. The Dataset must use schema compatible
   * with Model being trained, and what is compatible should be described in the
   * used TrainingPipeline's [training_task_definition]
   * [google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition].
   * For tabular Datasets, all their data is exported to training, to pick
   * and choose from.
   */
  datasetId: string;
  /**
   * Applicable only to Datasets that have DataItems and Annotations.
   *
   * A filter on Annotations of the Dataset. Only Annotations that both
   * match this filter and belong to DataItems not ignored by the split method
   * are used in respectively training, validation or test role, depending on
   * the role of the DataItem they are on (for the auto-assigned that role is
   * decided by Vertex AI). A filter with same syntax as the one used in
   * [ListAnnotations][google.cloud.aiplatform.v1beta1.DatasetService.ListAnnotations]
   * may be used, but note here it filters across all Annotations of the
   * Dataset, and not just within a single DataItem.
   */
  annotationsFilter: string;
  /**
   * Applicable only to custom training with Datasets that have DataItems and
   * Annotations.
   *
   * Cloud Storage URI that points to a YAML file describing the annotation
   * schema. The schema is defined as an OpenAPI 3.0.2 [Schema
   * Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject).
   * The schema files that can be used here are found in
   * gs://google-cloud-aiplatform/schema/dataset/annotation/ , note that the
   * chosen schema must be consistent with
   * [metadata][google.cloud.aiplatform.v1beta1.Dataset.metadata_schema_uri] of
   * the Dataset specified by
   * [dataset_id][google.cloud.aiplatform.v1beta1.InputDataConfig.dataset_id].
   *
   * Only Annotations that both match this schema and belong to DataItems not
   * ignored by the split method are used in respectively training, validation
   * or test role, depending on the role of the DataItem they are on.
   *
   * When used in conjunction with
   * [annotations_filter][google.cloud.aiplatform.v1beta1.InputDataConfig.annotations_filter],
   * the Annotations used for training are filtered by both
   * [annotations_filter][google.cloud.aiplatform.v1beta1.InputDataConfig.annotations_filter]
   * and
   * [annotation_schema_uri][google.cloud.aiplatform.v1beta1.InputDataConfig.annotation_schema_uri].
   */
  annotationSchemaUri: string;
  /**
   * Only applicable to Datasets that have SavedQueries.
   *
   * The ID of a SavedQuery (annotation set) under the Dataset specified by
   * [dataset_id][google.cloud.aiplatform.v1beta1.InputDataConfig.dataset_id]
   * used for filtering Annotations for training.
   *
   * Only Annotations that are associated with this SavedQuery are used in
   * respectively training. When used in conjunction with
   * [annotations_filter][google.cloud.aiplatform.v1beta1.InputDataConfig.annotations_filter],
   * the Annotations used for training are filtered by both
   * [saved_query_id][google.cloud.aiplatform.v1beta1.InputDataConfig.saved_query_id]
   * and
   * [annotations_filter][google.cloud.aiplatform.v1beta1.InputDataConfig.annotations_filter].
   *
   * Only one of
   * [saved_query_id][google.cloud.aiplatform.v1beta1.InputDataConfig.saved_query_id]
   * and
   * [annotation_schema_uri][google.cloud.aiplatform.v1beta1.InputDataConfig.annotation_schema_uri]
   * should be specified as both of them represent the same thing: problem type.
   */
  savedQueryId: string;
  /** Whether to persist the ML use assignment to data item system labels. */
  persistMlUseAssignment: boolean;
}

/**
 * Assigns the input data to training, validation, and test sets as per the
 * given fractions. Any of `training_fraction`, `validation_fraction` and
 * `test_fraction` may optionally be provided, they must sum to up to 1. If the
 * provided ones sum to less than 1, the remainder is assigned to sets as
 * decided by Vertex AI. If none of the fractions are set, by default roughly
 * 80% of data is used for training, 10% for validation, and 10% for test.
 */
export interface FractionSplit {
  /** The fraction of the input data that is to be used to train the Model. */
  trainingFraction: number;
  /** The fraction of the input data that is to be used to validate the Model. */
  validationFraction: number;
  /** The fraction of the input data that is to be used to evaluate the Model. */
  testFraction: number;
}

/**
 * Assigns input data to training, validation, and test sets based on the given
 * filters, data pieces not matched by any filter are ignored. Currently only
 * supported for Datasets containing DataItems.
 * If any of the filters in this message are to match nothing, then they can be
 * set as '-' (the minus sign).
 *
 * Supported only for unstructured Datasets.
 */
export interface FilterSplit {
  /**
   * Required. A filter on DataItems of the Dataset. DataItems that match
   * this filter are used to train the Model. A filter with same syntax
   * as the one used in
   * [DatasetService.ListDataItems][google.cloud.aiplatform.v1beta1.DatasetService.ListDataItems]
   * may be used. If a single DataItem is matched by more than one of the
   * FilterSplit filters, then it is assigned to the first set that applies to
   * it in the training, validation, test order.
   */
  trainingFilter: string;
  /**
   * Required. A filter on DataItems of the Dataset. DataItems that match
   * this filter are used to validate the Model. A filter with same syntax
   * as the one used in
   * [DatasetService.ListDataItems][google.cloud.aiplatform.v1beta1.DatasetService.ListDataItems]
   * may be used. If a single DataItem is matched by more than one of the
   * FilterSplit filters, then it is assigned to the first set that applies to
   * it in the training, validation, test order.
   */
  validationFilter: string;
  /**
   * Required. A filter on DataItems of the Dataset. DataItems that match
   * this filter are used to test the Model. A filter with same syntax
   * as the one used in
   * [DatasetService.ListDataItems][google.cloud.aiplatform.v1beta1.DatasetService.ListDataItems]
   * may be used. If a single DataItem is matched by more than one of the
   * FilterSplit filters, then it is assigned to the first set that applies to
   * it in the training, validation, test order.
   */
  testFilter: string;
}

/**
 * Assigns input data to training, validation, and test sets based on the
 * value of a provided key.
 *
 * Supported only for tabular Datasets.
 */
export interface PredefinedSplit {
  /**
   * Required. The key is a name of one of the Dataset's data columns.
   * The value of the key (either the label's value or value in the column)
   * must be one of {`training`, `validation`, `test`}, and it defines to which
   * set the given piece of data is assigned. If for a piece of data the key
   * is not present or has an invalid value, that piece is ignored by the
   * pipeline.
   */
  key: string;
}

/**
 * Assigns input data to training, validation, and test sets based on a
 * provided timestamps. The youngest data pieces are assigned to training set,
 * next to validation set, and the oldest to the test set.
 *
 * Supported only for tabular Datasets.
 */
export interface TimestampSplit {
  /** The fraction of the input data that is to be used to train the Model. */
  trainingFraction: number;
  /** The fraction of the input data that is to be used to validate the Model. */
  validationFraction: number;
  /** The fraction of the input data that is to be used to evaluate the Model. */
  testFraction: number;
  /**
   * Required. The key is a name of one of the Dataset's data columns.
   * The values of the key (the values in the column) must be in RFC 3339
   * `date-time` format, where `time-offset` = `"Z"`
   * (e.g. 1985-04-12T23:20:50.52Z). If for a piece of data the key is not
   * present or has an invalid value, that piece is ignored by the pipeline.
   */
  key: string;
}

/**
 * Assigns input data to the training, validation, and test sets so that the
 * distribution of values found in the categorical column (as specified by the
 * `key` field) is mirrored within each split. The fraction values determine
 * the relative sizes of the splits.
 *
 * For example, if the specified column has three values, with 50% of the rows
 * having value "A", 25% value "B", and 25% value "C", and the split fractions
 * are specified as 80/10/10, then the training set will constitute 80% of the
 * training data, with about 50% of the training set rows having the value "A"
 * for the specified column, about 25% having the value "B", and about 25%
 * having the value "C".
 *
 * Only the top 500 occurring values are used; any values not in the top
 * 500 values are randomly assigned to a split. If less than three rows contain
 * a specific value, those rows are randomly assigned.
 *
 * Supported only for tabular Datasets.
 */
export interface StratifiedSplit {
  /** The fraction of the input data that is to be used to train the Model. */
  trainingFraction: number;
  /** The fraction of the input data that is to be used to validate the Model. */
  validationFraction: number;
  /** The fraction of the input data that is to be used to evaluate the Model. */
  testFraction: number;
  /**
   * Required. The key is a name of one of the Dataset's data columns.
   * The key provided must be for a categorical column.
   */
  key: string;
}

function createBaseTrainingPipeline(): TrainingPipeline {
  return {
    name: "",
    displayName: "",
    inputDataConfig: undefined,
    trainingTaskDefinition: "",
    trainingTaskInputs: undefined,
    trainingTaskMetadata: undefined,
    modelToUpload: undefined,
    modelId: "",
    parentModel: "",
    state: 0,
    error: undefined,
    createTime: undefined,
    startTime: undefined,
    endTime: undefined,
    updateTime: undefined,
    labels: {},
    encryptionSpec: undefined,
  };
}

export const TrainingPipeline: MessageFns<TrainingPipeline> = {
  encode(message: TrainingPipeline, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.displayName !== "") {
      writer.uint32(18).string(message.displayName);
    }
    if (message.inputDataConfig !== undefined) {
      InputDataConfig.encode(message.inputDataConfig, writer.uint32(26).fork()).join();
    }
    if (message.trainingTaskDefinition !== "") {
      writer.uint32(34).string(message.trainingTaskDefinition);
    }
    if (message.trainingTaskInputs !== undefined) {
      Value.encode(Value.wrap(message.trainingTaskInputs), writer.uint32(42).fork()).join();
    }
    if (message.trainingTaskMetadata !== undefined) {
      Value.encode(Value.wrap(message.trainingTaskMetadata), writer.uint32(50).fork()).join();
    }
    if (message.modelToUpload !== undefined) {
      Model.encode(message.modelToUpload, writer.uint32(58).fork()).join();
    }
    if (message.modelId !== "") {
      writer.uint32(178).string(message.modelId);
    }
    if (message.parentModel !== "") {
      writer.uint32(170).string(message.parentModel);
    }
    if (message.state !== 0) {
      writer.uint32(72).int32(message.state);
    }
    if (message.error !== undefined) {
      Status.encode(message.error, writer.uint32(82).fork()).join();
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(90).fork()).join();
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(98).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(106).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(114).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      TrainingPipeline_LabelsEntry.encode({ key: key as any, value }, writer.uint32(122).fork()).join();
    });
    if (message.encryptionSpec !== undefined) {
      EncryptionSpec.encode(message.encryptionSpec, writer.uint32(146).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TrainingPipeline {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTrainingPipeline();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.inputDataConfig = InputDataConfig.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.trainingTaskDefinition = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.trainingTaskInputs = Value.unwrap(Value.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.trainingTaskMetadata = Value.unwrap(Value.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.modelToUpload = Model.decode(reader, reader.uint32());
          continue;
        case 22:
          if (tag !== 178) {
            break;
          }

          message.modelId = reader.string();
          continue;
        case 21:
          if (tag !== 170) {
            break;
          }

          message.parentModel = reader.string();
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.error = Status.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          const entry15 = TrainingPipeline_LabelsEntry.decode(reader, reader.uint32());
          if (entry15.value !== undefined) {
            message.labels[entry15.key] = entry15.value;
          }
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.encryptionSpec = EncryptionSpec.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TrainingPipeline {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      inputDataConfig: isSet(object.inputDataConfig) ? InputDataConfig.fromJSON(object.inputDataConfig) : undefined,
      trainingTaskDefinition: isSet(object.trainingTaskDefinition)
        ? globalThis.String(object.trainingTaskDefinition)
        : "",
      trainingTaskInputs: isSet(object?.trainingTaskInputs) ? object.trainingTaskInputs : undefined,
      trainingTaskMetadata: isSet(object?.trainingTaskMetadata) ? object.trainingTaskMetadata : undefined,
      modelToUpload: isSet(object.modelToUpload) ? Model.fromJSON(object.modelToUpload) : undefined,
      modelId: isSet(object.modelId) ? globalThis.String(object.modelId) : "",
      parentModel: isSet(object.parentModel) ? globalThis.String(object.parentModel) : "",
      state: isSet(object.state) ? pipelineStateFromJSON(object.state) : 0,
      error: isSet(object.error) ? Status.fromJSON(object.error) : undefined,
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      encryptionSpec: isSet(object.encryptionSpec) ? EncryptionSpec.fromJSON(object.encryptionSpec) : undefined,
    };
  },

  toJSON(message: TrainingPipeline): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.inputDataConfig !== undefined) {
      obj.inputDataConfig = InputDataConfig.toJSON(message.inputDataConfig);
    }
    if (message.trainingTaskDefinition !== "") {
      obj.trainingTaskDefinition = message.trainingTaskDefinition;
    }
    if (message.trainingTaskInputs !== undefined) {
      obj.trainingTaskInputs = message.trainingTaskInputs;
    }
    if (message.trainingTaskMetadata !== undefined) {
      obj.trainingTaskMetadata = message.trainingTaskMetadata;
    }
    if (message.modelToUpload !== undefined) {
      obj.modelToUpload = Model.toJSON(message.modelToUpload);
    }
    if (message.modelId !== "") {
      obj.modelId = message.modelId;
    }
    if (message.parentModel !== "") {
      obj.parentModel = message.parentModel;
    }
    if (message.state !== 0) {
      obj.state = pipelineStateToJSON(message.state);
    }
    if (message.error !== undefined) {
      obj.error = Status.toJSON(message.error);
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.encryptionSpec !== undefined) {
      obj.encryptionSpec = EncryptionSpec.toJSON(message.encryptionSpec);
    }
    return obj;
  },

  create(base?: DeepPartial<TrainingPipeline>): TrainingPipeline {
    return TrainingPipeline.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TrainingPipeline>): TrainingPipeline {
    const message = createBaseTrainingPipeline();
    message.name = object.name ?? "";
    message.displayName = object.displayName ?? "";
    message.inputDataConfig = (object.inputDataConfig !== undefined && object.inputDataConfig !== null)
      ? InputDataConfig.fromPartial(object.inputDataConfig)
      : undefined;
    message.trainingTaskDefinition = object.trainingTaskDefinition ?? "";
    message.trainingTaskInputs = object.trainingTaskInputs ?? undefined;
    message.trainingTaskMetadata = object.trainingTaskMetadata ?? undefined;
    message.modelToUpload = (object.modelToUpload !== undefined && object.modelToUpload !== null)
      ? Model.fromPartial(object.modelToUpload)
      : undefined;
    message.modelId = object.modelId ?? "";
    message.parentModel = object.parentModel ?? "";
    message.state = object.state ?? 0;
    message.error = (object.error !== undefined && object.error !== null)
      ? Status.fromPartial(object.error)
      : undefined;
    message.createTime = object.createTime ?? undefined;
    message.startTime = object.startTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.encryptionSpec = (object.encryptionSpec !== undefined && object.encryptionSpec !== null)
      ? EncryptionSpec.fromPartial(object.encryptionSpec)
      : undefined;
    return message;
  },
};

function createBaseTrainingPipeline_LabelsEntry(): TrainingPipeline_LabelsEntry {
  return { key: "", value: "" };
}

export const TrainingPipeline_LabelsEntry: MessageFns<TrainingPipeline_LabelsEntry> = {
  encode(message: TrainingPipeline_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TrainingPipeline_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTrainingPipeline_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TrainingPipeline_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: TrainingPipeline_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<TrainingPipeline_LabelsEntry>): TrainingPipeline_LabelsEntry {
    return TrainingPipeline_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TrainingPipeline_LabelsEntry>): TrainingPipeline_LabelsEntry {
    const message = createBaseTrainingPipeline_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseInputDataConfig(): InputDataConfig {
  return {
    fractionSplit: undefined,
    filterSplit: undefined,
    predefinedSplit: undefined,
    timestampSplit: undefined,
    stratifiedSplit: undefined,
    gcsDestination: undefined,
    bigqueryDestination: undefined,
    datasetId: "",
    annotationsFilter: "",
    annotationSchemaUri: "",
    savedQueryId: "",
    persistMlUseAssignment: false,
  };
}

export const InputDataConfig: MessageFns<InputDataConfig> = {
  encode(message: InputDataConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.fractionSplit !== undefined) {
      FractionSplit.encode(message.fractionSplit, writer.uint32(18).fork()).join();
    }
    if (message.filterSplit !== undefined) {
      FilterSplit.encode(message.filterSplit, writer.uint32(26).fork()).join();
    }
    if (message.predefinedSplit !== undefined) {
      PredefinedSplit.encode(message.predefinedSplit, writer.uint32(34).fork()).join();
    }
    if (message.timestampSplit !== undefined) {
      TimestampSplit.encode(message.timestampSplit, writer.uint32(42).fork()).join();
    }
    if (message.stratifiedSplit !== undefined) {
      StratifiedSplit.encode(message.stratifiedSplit, writer.uint32(98).fork()).join();
    }
    if (message.gcsDestination !== undefined) {
      GcsDestination.encode(message.gcsDestination, writer.uint32(66).fork()).join();
    }
    if (message.bigqueryDestination !== undefined) {
      BigQueryDestination.encode(message.bigqueryDestination, writer.uint32(82).fork()).join();
    }
    if (message.datasetId !== "") {
      writer.uint32(10).string(message.datasetId);
    }
    if (message.annotationsFilter !== "") {
      writer.uint32(50).string(message.annotationsFilter);
    }
    if (message.annotationSchemaUri !== "") {
      writer.uint32(74).string(message.annotationSchemaUri);
    }
    if (message.savedQueryId !== "") {
      writer.uint32(58).string(message.savedQueryId);
    }
    if (message.persistMlUseAssignment !== false) {
      writer.uint32(88).bool(message.persistMlUseAssignment);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InputDataConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInputDataConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.fractionSplit = FractionSplit.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.filterSplit = FilterSplit.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.predefinedSplit = PredefinedSplit.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.timestampSplit = TimestampSplit.decode(reader, reader.uint32());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.stratifiedSplit = StratifiedSplit.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.gcsDestination = GcsDestination.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.bigqueryDestination = BigQueryDestination.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.annotationsFilter = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.annotationSchemaUri = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.savedQueryId = reader.string();
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.persistMlUseAssignment = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InputDataConfig {
    return {
      fractionSplit: isSet(object.fractionSplit) ? FractionSplit.fromJSON(object.fractionSplit) : undefined,
      filterSplit: isSet(object.filterSplit) ? FilterSplit.fromJSON(object.filterSplit) : undefined,
      predefinedSplit: isSet(object.predefinedSplit) ? PredefinedSplit.fromJSON(object.predefinedSplit) : undefined,
      timestampSplit: isSet(object.timestampSplit) ? TimestampSplit.fromJSON(object.timestampSplit) : undefined,
      stratifiedSplit: isSet(object.stratifiedSplit) ? StratifiedSplit.fromJSON(object.stratifiedSplit) : undefined,
      gcsDestination: isSet(object.gcsDestination) ? GcsDestination.fromJSON(object.gcsDestination) : undefined,
      bigqueryDestination: isSet(object.bigqueryDestination)
        ? BigQueryDestination.fromJSON(object.bigqueryDestination)
        : undefined,
      datasetId: isSet(object.datasetId) ? globalThis.String(object.datasetId) : "",
      annotationsFilter: isSet(object.annotationsFilter) ? globalThis.String(object.annotationsFilter) : "",
      annotationSchemaUri: isSet(object.annotationSchemaUri) ? globalThis.String(object.annotationSchemaUri) : "",
      savedQueryId: isSet(object.savedQueryId) ? globalThis.String(object.savedQueryId) : "",
      persistMlUseAssignment: isSet(object.persistMlUseAssignment)
        ? globalThis.Boolean(object.persistMlUseAssignment)
        : false,
    };
  },

  toJSON(message: InputDataConfig): unknown {
    const obj: any = {};
    if (message.fractionSplit !== undefined) {
      obj.fractionSplit = FractionSplit.toJSON(message.fractionSplit);
    }
    if (message.filterSplit !== undefined) {
      obj.filterSplit = FilterSplit.toJSON(message.filterSplit);
    }
    if (message.predefinedSplit !== undefined) {
      obj.predefinedSplit = PredefinedSplit.toJSON(message.predefinedSplit);
    }
    if (message.timestampSplit !== undefined) {
      obj.timestampSplit = TimestampSplit.toJSON(message.timestampSplit);
    }
    if (message.stratifiedSplit !== undefined) {
      obj.stratifiedSplit = StratifiedSplit.toJSON(message.stratifiedSplit);
    }
    if (message.gcsDestination !== undefined) {
      obj.gcsDestination = GcsDestination.toJSON(message.gcsDestination);
    }
    if (message.bigqueryDestination !== undefined) {
      obj.bigqueryDestination = BigQueryDestination.toJSON(message.bigqueryDestination);
    }
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    if (message.annotationsFilter !== "") {
      obj.annotationsFilter = message.annotationsFilter;
    }
    if (message.annotationSchemaUri !== "") {
      obj.annotationSchemaUri = message.annotationSchemaUri;
    }
    if (message.savedQueryId !== "") {
      obj.savedQueryId = message.savedQueryId;
    }
    if (message.persistMlUseAssignment !== false) {
      obj.persistMlUseAssignment = message.persistMlUseAssignment;
    }
    return obj;
  },

  create(base?: DeepPartial<InputDataConfig>): InputDataConfig {
    return InputDataConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InputDataConfig>): InputDataConfig {
    const message = createBaseInputDataConfig();
    message.fractionSplit = (object.fractionSplit !== undefined && object.fractionSplit !== null)
      ? FractionSplit.fromPartial(object.fractionSplit)
      : undefined;
    message.filterSplit = (object.filterSplit !== undefined && object.filterSplit !== null)
      ? FilterSplit.fromPartial(object.filterSplit)
      : undefined;
    message.predefinedSplit = (object.predefinedSplit !== undefined && object.predefinedSplit !== null)
      ? PredefinedSplit.fromPartial(object.predefinedSplit)
      : undefined;
    message.timestampSplit = (object.timestampSplit !== undefined && object.timestampSplit !== null)
      ? TimestampSplit.fromPartial(object.timestampSplit)
      : undefined;
    message.stratifiedSplit = (object.stratifiedSplit !== undefined && object.stratifiedSplit !== null)
      ? StratifiedSplit.fromPartial(object.stratifiedSplit)
      : undefined;
    message.gcsDestination = (object.gcsDestination !== undefined && object.gcsDestination !== null)
      ? GcsDestination.fromPartial(object.gcsDestination)
      : undefined;
    message.bigqueryDestination = (object.bigqueryDestination !== undefined && object.bigqueryDestination !== null)
      ? BigQueryDestination.fromPartial(object.bigqueryDestination)
      : undefined;
    message.datasetId = object.datasetId ?? "";
    message.annotationsFilter = object.annotationsFilter ?? "";
    message.annotationSchemaUri = object.annotationSchemaUri ?? "";
    message.savedQueryId = object.savedQueryId ?? "";
    message.persistMlUseAssignment = object.persistMlUseAssignment ?? false;
    return message;
  },
};

function createBaseFractionSplit(): FractionSplit {
  return { trainingFraction: 0, validationFraction: 0, testFraction: 0 };
}

export const FractionSplit: MessageFns<FractionSplit> = {
  encode(message: FractionSplit, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.trainingFraction !== 0) {
      writer.uint32(9).double(message.trainingFraction);
    }
    if (message.validationFraction !== 0) {
      writer.uint32(17).double(message.validationFraction);
    }
    if (message.testFraction !== 0) {
      writer.uint32(25).double(message.testFraction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FractionSplit {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFractionSplit();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 9) {
            break;
          }

          message.trainingFraction = reader.double();
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.validationFraction = reader.double();
          continue;
        case 3:
          if (tag !== 25) {
            break;
          }

          message.testFraction = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FractionSplit {
    return {
      trainingFraction: isSet(object.trainingFraction) ? globalThis.Number(object.trainingFraction) : 0,
      validationFraction: isSet(object.validationFraction) ? globalThis.Number(object.validationFraction) : 0,
      testFraction: isSet(object.testFraction) ? globalThis.Number(object.testFraction) : 0,
    };
  },

  toJSON(message: FractionSplit): unknown {
    const obj: any = {};
    if (message.trainingFraction !== 0) {
      obj.trainingFraction = message.trainingFraction;
    }
    if (message.validationFraction !== 0) {
      obj.validationFraction = message.validationFraction;
    }
    if (message.testFraction !== 0) {
      obj.testFraction = message.testFraction;
    }
    return obj;
  },

  create(base?: DeepPartial<FractionSplit>): FractionSplit {
    return FractionSplit.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FractionSplit>): FractionSplit {
    const message = createBaseFractionSplit();
    message.trainingFraction = object.trainingFraction ?? 0;
    message.validationFraction = object.validationFraction ?? 0;
    message.testFraction = object.testFraction ?? 0;
    return message;
  },
};

function createBaseFilterSplit(): FilterSplit {
  return { trainingFilter: "", validationFilter: "", testFilter: "" };
}

export const FilterSplit: MessageFns<FilterSplit> = {
  encode(message: FilterSplit, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.trainingFilter !== "") {
      writer.uint32(10).string(message.trainingFilter);
    }
    if (message.validationFilter !== "") {
      writer.uint32(18).string(message.validationFilter);
    }
    if (message.testFilter !== "") {
      writer.uint32(26).string(message.testFilter);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FilterSplit {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFilterSplit();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.trainingFilter = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.validationFilter = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.testFilter = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FilterSplit {
    return {
      trainingFilter: isSet(object.trainingFilter) ? globalThis.String(object.trainingFilter) : "",
      validationFilter: isSet(object.validationFilter) ? globalThis.String(object.validationFilter) : "",
      testFilter: isSet(object.testFilter) ? globalThis.String(object.testFilter) : "",
    };
  },

  toJSON(message: FilterSplit): unknown {
    const obj: any = {};
    if (message.trainingFilter !== "") {
      obj.trainingFilter = message.trainingFilter;
    }
    if (message.validationFilter !== "") {
      obj.validationFilter = message.validationFilter;
    }
    if (message.testFilter !== "") {
      obj.testFilter = message.testFilter;
    }
    return obj;
  },

  create(base?: DeepPartial<FilterSplit>): FilterSplit {
    return FilterSplit.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FilterSplit>): FilterSplit {
    const message = createBaseFilterSplit();
    message.trainingFilter = object.trainingFilter ?? "";
    message.validationFilter = object.validationFilter ?? "";
    message.testFilter = object.testFilter ?? "";
    return message;
  },
};

function createBasePredefinedSplit(): PredefinedSplit {
  return { key: "" };
}

export const PredefinedSplit: MessageFns<PredefinedSplit> = {
  encode(message: PredefinedSplit, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PredefinedSplit {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePredefinedSplit();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PredefinedSplit {
    return { key: isSet(object.key) ? globalThis.String(object.key) : "" };
  },

  toJSON(message: PredefinedSplit): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    return obj;
  },

  create(base?: DeepPartial<PredefinedSplit>): PredefinedSplit {
    return PredefinedSplit.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PredefinedSplit>): PredefinedSplit {
    const message = createBasePredefinedSplit();
    message.key = object.key ?? "";
    return message;
  },
};

function createBaseTimestampSplit(): TimestampSplit {
  return { trainingFraction: 0, validationFraction: 0, testFraction: 0, key: "" };
}

export const TimestampSplit: MessageFns<TimestampSplit> = {
  encode(message: TimestampSplit, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.trainingFraction !== 0) {
      writer.uint32(9).double(message.trainingFraction);
    }
    if (message.validationFraction !== 0) {
      writer.uint32(17).double(message.validationFraction);
    }
    if (message.testFraction !== 0) {
      writer.uint32(25).double(message.testFraction);
    }
    if (message.key !== "") {
      writer.uint32(34).string(message.key);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TimestampSplit {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTimestampSplit();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 9) {
            break;
          }

          message.trainingFraction = reader.double();
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.validationFraction = reader.double();
          continue;
        case 3:
          if (tag !== 25) {
            break;
          }

          message.testFraction = reader.double();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.key = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TimestampSplit {
    return {
      trainingFraction: isSet(object.trainingFraction) ? globalThis.Number(object.trainingFraction) : 0,
      validationFraction: isSet(object.validationFraction) ? globalThis.Number(object.validationFraction) : 0,
      testFraction: isSet(object.testFraction) ? globalThis.Number(object.testFraction) : 0,
      key: isSet(object.key) ? globalThis.String(object.key) : "",
    };
  },

  toJSON(message: TimestampSplit): unknown {
    const obj: any = {};
    if (message.trainingFraction !== 0) {
      obj.trainingFraction = message.trainingFraction;
    }
    if (message.validationFraction !== 0) {
      obj.validationFraction = message.validationFraction;
    }
    if (message.testFraction !== 0) {
      obj.testFraction = message.testFraction;
    }
    if (message.key !== "") {
      obj.key = message.key;
    }
    return obj;
  },

  create(base?: DeepPartial<TimestampSplit>): TimestampSplit {
    return TimestampSplit.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TimestampSplit>): TimestampSplit {
    const message = createBaseTimestampSplit();
    message.trainingFraction = object.trainingFraction ?? 0;
    message.validationFraction = object.validationFraction ?? 0;
    message.testFraction = object.testFraction ?? 0;
    message.key = object.key ?? "";
    return message;
  },
};

function createBaseStratifiedSplit(): StratifiedSplit {
  return { trainingFraction: 0, validationFraction: 0, testFraction: 0, key: "" };
}

export const StratifiedSplit: MessageFns<StratifiedSplit> = {
  encode(message: StratifiedSplit, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.trainingFraction !== 0) {
      writer.uint32(9).double(message.trainingFraction);
    }
    if (message.validationFraction !== 0) {
      writer.uint32(17).double(message.validationFraction);
    }
    if (message.testFraction !== 0) {
      writer.uint32(25).double(message.testFraction);
    }
    if (message.key !== "") {
      writer.uint32(34).string(message.key);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StratifiedSplit {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStratifiedSplit();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 9) {
            break;
          }

          message.trainingFraction = reader.double();
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.validationFraction = reader.double();
          continue;
        case 3:
          if (tag !== 25) {
            break;
          }

          message.testFraction = reader.double();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.key = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StratifiedSplit {
    return {
      trainingFraction: isSet(object.trainingFraction) ? globalThis.Number(object.trainingFraction) : 0,
      validationFraction: isSet(object.validationFraction) ? globalThis.Number(object.validationFraction) : 0,
      testFraction: isSet(object.testFraction) ? globalThis.Number(object.testFraction) : 0,
      key: isSet(object.key) ? globalThis.String(object.key) : "",
    };
  },

  toJSON(message: StratifiedSplit): unknown {
    const obj: any = {};
    if (message.trainingFraction !== 0) {
      obj.trainingFraction = message.trainingFraction;
    }
    if (message.validationFraction !== 0) {
      obj.validationFraction = message.validationFraction;
    }
    if (message.testFraction !== 0) {
      obj.testFraction = message.testFraction;
    }
    if (message.key !== "") {
      obj.key = message.key;
    }
    return obj;
  },

  create(base?: DeepPartial<StratifiedSplit>): StratifiedSplit {
    return StratifiedSplit.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StratifiedSplit>): StratifiedSplit {
    const message = createBaseStratifiedSplit();
    message.trainingFraction = object.trainingFraction ?? 0;
    message.validationFraction = object.validationFraction ?? 0;
    message.testFraction = object.testFraction ?? 0;
    message.key = object.key ?? "";
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
