// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/aiplatform/v1beta1/dataset.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Value } from "../../../protobuf/struct.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { EncryptionSpec } from "./encryption_spec.js";
import { GcsDestination, GcsSource } from "./io.js";
import { SavedQuery } from "./saved_query.js";

export const protobufPackage = "google.cloud.aiplatform.v1beta1";

/** A collection of DataItems and Annotations on them. */
export interface Dataset {
  /** Output only. Identifier. The resource name of the Dataset. */
  name: string;
  /**
   * Required. The user-defined name of the Dataset.
   * The name can be up to 128 characters long and can consist of any UTF-8
   * characters.
   */
  displayName: string;
  /** The description of the Dataset. */
  description: string;
  /**
   * Required. Points to a YAML file stored on Google Cloud Storage describing
   * additional information about the Dataset. The schema is defined as an
   * OpenAPI 3.0.2 Schema Object. The schema files that can be used here are
   * found in gs://google-cloud-aiplatform/schema/dataset/metadata/.
   */
  metadataSchemaUri: string;
  /** Required. Additional information about the Dataset. */
  metadata:
    | any
    | undefined;
  /**
   * Output only. The number of DataItems in this Dataset. Only apply for
   * non-structured Dataset.
   */
  dataItemCount: Long;
  /** Output only. Timestamp when this Dataset was created. */
  createTime:
    | Date
    | undefined;
  /** Output only. Timestamp when this Dataset was last updated. */
  updateTime:
    | Date
    | undefined;
  /**
   * Used to perform consistent read-modify-write updates. If not set, a blind
   * "overwrite" update happens.
   */
  etag: string;
  /**
   * The labels with user-defined metadata to organize your Datasets.
   *
   * Label keys and values can be no longer than 64 characters
   * (Unicode codepoints), can only contain lowercase letters, numeric
   * characters, underscores and dashes. International characters are allowed.
   * No more than 64 user labels can be associated with one Dataset (System
   * labels are excluded).
   *
   * See https://goo.gl/xmQnxf for more information and examples of labels.
   * System reserved label keys are prefixed with "aiplatform.googleapis.com/"
   * and are immutable. Following system labels exist for each Dataset:
   *
   * * "aiplatform.googleapis.com/dataset_metadata_schema": output only, its
   *   value is the
   *   [metadata_schema's][google.cloud.aiplatform.v1beta1.Dataset.metadata_schema_uri]
   *   title.
   */
  labels: { [key: string]: string };
  /**
   * All SavedQueries belong to the Dataset will be returned in List/Get
   * Dataset response. The annotation_specs field
   * will not be populated except for UI cases which will only use
   * [annotation_spec_count][google.cloud.aiplatform.v1beta1.SavedQuery.annotation_spec_count].
   * In CreateDataset request, a SavedQuery is created together if
   * this field is set, up to one SavedQuery can be set in CreateDatasetRequest.
   * The SavedQuery should not contain any AnnotationSpec.
   */
  savedQueries: SavedQuery[];
  /**
   * Customer-managed encryption key spec for a Dataset. If set, this Dataset
   * and all sub-resources of this Dataset will be secured by this key.
   */
  encryptionSpec:
    | EncryptionSpec
    | undefined;
  /**
   * Output only. The resource name of the Artifact that was created in
   * MetadataStore when creating the Dataset. The Artifact resource name pattern
   * is
   * `projects/{project}/locations/{location}/metadataStores/{metadata_store}/artifacts/{artifact}`.
   */
  metadataArtifact: string;
  /**
   * Optional. Reference to the public base model last used by the dataset. Only
   * set for prompt datasets.
   */
  modelReference: string;
  /** Output only. Reserved for future use. */
  satisfiesPzs: boolean;
  /** Output only. Reserved for future use. */
  satisfiesPzi: boolean;
}

export interface Dataset_LabelsEntry {
  key: string;
  value: string;
}

/**
 * Describes the location from where we import data into a Dataset, together
 * with the labels that will be applied to the DataItems and the Annotations.
 */
export interface ImportDataConfig {
  /** The Google Cloud Storage location for the input content. */
  gcsSource?:
    | GcsSource
    | undefined;
  /**
   * Labels that will be applied to newly imported DataItems. If an identical
   * DataItem as one being imported already exists in the Dataset, then these
   * labels will be appended to these of the already existing one, and if labels
   * with identical key is imported before, the old label value will be
   * overwritten. If two DataItems are identical in the same import data
   * operation, the labels will be combined and if key collision happens in this
   * case, one of the values will be picked randomly. Two DataItems are
   * considered identical if their content bytes are identical (e.g. image bytes
   * or pdf bytes).
   * These labels will be overridden by Annotation labels specified inside index
   * file referenced by
   * [import_schema_uri][google.cloud.aiplatform.v1beta1.ImportDataConfig.import_schema_uri],
   * e.g. jsonl file.
   */
  dataItemLabels: { [key: string]: string };
  /**
   * Labels that will be applied to newly imported Annotations. If two
   * Annotations are identical, one of them will be deduped. Two Annotations are
   * considered identical if their
   * [payload][google.cloud.aiplatform.v1beta1.Annotation.payload],
   * [payload_schema_uri][google.cloud.aiplatform.v1beta1.Annotation.payload_schema_uri]
   * and all of their
   * [labels][google.cloud.aiplatform.v1beta1.Annotation.labels] are the same.
   * These labels will be overridden by Annotation labels specified inside index
   * file referenced by
   * [import_schema_uri][google.cloud.aiplatform.v1beta1.ImportDataConfig.import_schema_uri],
   * e.g. jsonl file.
   */
  annotationLabels: { [key: string]: string };
  /**
   * Required. Points to a YAML file stored on Google Cloud Storage describing
   * the import format. Validation will be done against the schema. The schema
   * is defined as an [OpenAPI 3.0.2 Schema
   * Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject).
   */
  importSchemaUri: string;
}

export interface ImportDataConfig_DataItemLabelsEntry {
  key: string;
  value: string;
}

export interface ImportDataConfig_AnnotationLabelsEntry {
  key: string;
  value: string;
}

/**
 * Describes what part of the Dataset is to be exported, the destination of
 * the export and how to export.
 */
export interface ExportDataConfig {
  /**
   * The Google Cloud Storage location where the output is to be written to.
   * In the given directory a new directory will be created with name:
   * `export-data-<dataset-display-name>-<timestamp-of-export-call>` where
   * timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. All export
   * output will be written into that directory. Inside that directory,
   * annotations with the same schema will be grouped into sub directories
   * which are named with the corresponding annotations' schema title. Inside
   * these sub directories, a schema.yaml will be created to describe the
   * output format.
   */
  gcsDestination?:
    | GcsDestination
    | undefined;
  /** Split based on fractions defining the size of each set. */
  fractionSplit?:
    | ExportFractionSplit
    | undefined;
  /**
   * An expression for filtering what part of the Dataset is to be exported.
   * Only Annotations that match this filter will be exported. The filter syntax
   * is the same as in
   * [ListAnnotations][google.cloud.aiplatform.v1beta1.DatasetService.ListAnnotations].
   */
  annotationsFilter: string;
}

/**
 * Assigns the input data to training, validation, and test sets as per the
 * given fractions. Any of `training_fraction`, `validation_fraction` and
 * `test_fraction` may optionally be provided, they must sum to up to 1. If the
 * provided ones sum to less than 1, the remainder is assigned to sets as
 * decided by Vertex AI. If none of the fractions are set, by default roughly
 * 80% of data is used for training, 10% for validation, and 10% for test.
 */
export interface ExportFractionSplit {
  /** The fraction of the input data that is to be used to train the Model. */
  trainingFraction: number;
  /** The fraction of the input data that is to be used to validate the Model. */
  validationFraction: number;
  /** The fraction of the input data that is to be used to evaluate the Model. */
  testFraction: number;
}

function createBaseDataset(): Dataset {
  return {
    name: "",
    displayName: "",
    description: "",
    metadataSchemaUri: "",
    metadata: undefined,
    dataItemCount: Long.ZERO,
    createTime: undefined,
    updateTime: undefined,
    etag: "",
    labels: {},
    savedQueries: [],
    encryptionSpec: undefined,
    metadataArtifact: "",
    modelReference: "",
    satisfiesPzs: false,
    satisfiesPzi: false,
  };
}

export const Dataset: MessageFns<Dataset> = {
  encode(message: Dataset, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.displayName !== "") {
      writer.uint32(18).string(message.displayName);
    }
    if (message.description !== "") {
      writer.uint32(130).string(message.description);
    }
    if (message.metadataSchemaUri !== "") {
      writer.uint32(26).string(message.metadataSchemaUri);
    }
    if (message.metadata !== undefined) {
      Value.encode(Value.wrap(message.metadata), writer.uint32(66).fork()).join();
    }
    if (!message.dataItemCount.equals(Long.ZERO)) {
      writer.uint32(80).int64(message.dataItemCount.toString());
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(34).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(42).fork()).join();
    }
    if (message.etag !== "") {
      writer.uint32(50).string(message.etag);
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      Dataset_LabelsEntry.encode({ key: key as any, value }, writer.uint32(58).fork()).join();
    });
    for (const v of message.savedQueries) {
      SavedQuery.encode(v!, writer.uint32(74).fork()).join();
    }
    if (message.encryptionSpec !== undefined) {
      EncryptionSpec.encode(message.encryptionSpec, writer.uint32(90).fork()).join();
    }
    if (message.metadataArtifact !== "") {
      writer.uint32(138).string(message.metadataArtifact);
    }
    if (message.modelReference !== "") {
      writer.uint32(146).string(message.modelReference);
    }
    if (message.satisfiesPzs !== false) {
      writer.uint32(152).bool(message.satisfiesPzs);
    }
    if (message.satisfiesPzi !== false) {
      writer.uint32(160).bool(message.satisfiesPzi);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Dataset {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDataset();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.description = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.metadataSchemaUri = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.metadata = Value.unwrap(Value.decode(reader, reader.uint32()));
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.dataItemCount = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.etag = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          const entry7 = Dataset_LabelsEntry.decode(reader, reader.uint32());
          if (entry7.value !== undefined) {
            message.labels[entry7.key] = entry7.value;
          }
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.savedQueries.push(SavedQuery.decode(reader, reader.uint32()));
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.encryptionSpec = EncryptionSpec.decode(reader, reader.uint32());
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.metadataArtifact = reader.string();
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.modelReference = reader.string();
          continue;
        case 19:
          if (tag !== 152) {
            break;
          }

          message.satisfiesPzs = reader.bool();
          continue;
        case 20:
          if (tag !== 160) {
            break;
          }

          message.satisfiesPzi = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Dataset {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      metadataSchemaUri: isSet(object.metadataSchemaUri) ? globalThis.String(object.metadataSchemaUri) : "",
      metadata: isSet(object?.metadata) ? object.metadata : undefined,
      dataItemCount: isSet(object.dataItemCount) ? Long.fromValue(object.dataItemCount) : Long.ZERO,
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      etag: isSet(object.etag) ? globalThis.String(object.etag) : "",
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      savedQueries: globalThis.Array.isArray(object?.savedQueries)
        ? object.savedQueries.map((e: any) => SavedQuery.fromJSON(e))
        : [],
      encryptionSpec: isSet(object.encryptionSpec) ? EncryptionSpec.fromJSON(object.encryptionSpec) : undefined,
      metadataArtifact: isSet(object.metadataArtifact) ? globalThis.String(object.metadataArtifact) : "",
      modelReference: isSet(object.modelReference) ? globalThis.String(object.modelReference) : "",
      satisfiesPzs: isSet(object.satisfiesPzs) ? globalThis.Boolean(object.satisfiesPzs) : false,
      satisfiesPzi: isSet(object.satisfiesPzi) ? globalThis.Boolean(object.satisfiesPzi) : false,
    };
  },

  toJSON(message: Dataset): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.metadataSchemaUri !== "") {
      obj.metadataSchemaUri = message.metadataSchemaUri;
    }
    if (message.metadata !== undefined) {
      obj.metadata = message.metadata;
    }
    if (!message.dataItemCount.equals(Long.ZERO)) {
      obj.dataItemCount = (message.dataItemCount || Long.ZERO).toString();
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (message.etag !== "") {
      obj.etag = message.etag;
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.savedQueries?.length) {
      obj.savedQueries = message.savedQueries.map((e) => SavedQuery.toJSON(e));
    }
    if (message.encryptionSpec !== undefined) {
      obj.encryptionSpec = EncryptionSpec.toJSON(message.encryptionSpec);
    }
    if (message.metadataArtifact !== "") {
      obj.metadataArtifact = message.metadataArtifact;
    }
    if (message.modelReference !== "") {
      obj.modelReference = message.modelReference;
    }
    if (message.satisfiesPzs !== false) {
      obj.satisfiesPzs = message.satisfiesPzs;
    }
    if (message.satisfiesPzi !== false) {
      obj.satisfiesPzi = message.satisfiesPzi;
    }
    return obj;
  },

  create(base?: DeepPartial<Dataset>): Dataset {
    return Dataset.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Dataset>): Dataset {
    const message = createBaseDataset();
    message.name = object.name ?? "";
    message.displayName = object.displayName ?? "";
    message.description = object.description ?? "";
    message.metadataSchemaUri = object.metadataSchemaUri ?? "";
    message.metadata = object.metadata ?? undefined;
    message.dataItemCount = (object.dataItemCount !== undefined && object.dataItemCount !== null)
      ? Long.fromValue(object.dataItemCount)
      : Long.ZERO;
    message.createTime = object.createTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.etag = object.etag ?? "";
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.savedQueries = object.savedQueries?.map((e) => SavedQuery.fromPartial(e)) || [];
    message.encryptionSpec = (object.encryptionSpec !== undefined && object.encryptionSpec !== null)
      ? EncryptionSpec.fromPartial(object.encryptionSpec)
      : undefined;
    message.metadataArtifact = object.metadataArtifact ?? "";
    message.modelReference = object.modelReference ?? "";
    message.satisfiesPzs = object.satisfiesPzs ?? false;
    message.satisfiesPzi = object.satisfiesPzi ?? false;
    return message;
  },
};

function createBaseDataset_LabelsEntry(): Dataset_LabelsEntry {
  return { key: "", value: "" };
}

export const Dataset_LabelsEntry: MessageFns<Dataset_LabelsEntry> = {
  encode(message: Dataset_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Dataset_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDataset_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Dataset_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Dataset_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Dataset_LabelsEntry>): Dataset_LabelsEntry {
    return Dataset_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Dataset_LabelsEntry>): Dataset_LabelsEntry {
    const message = createBaseDataset_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseImportDataConfig(): ImportDataConfig {
  return { gcsSource: undefined, dataItemLabels: {}, annotationLabels: {}, importSchemaUri: "" };
}

export const ImportDataConfig: MessageFns<ImportDataConfig> = {
  encode(message: ImportDataConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcsSource !== undefined) {
      GcsSource.encode(message.gcsSource, writer.uint32(10).fork()).join();
    }
    Object.entries(message.dataItemLabels).forEach(([key, value]) => {
      ImportDataConfig_DataItemLabelsEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    Object.entries(message.annotationLabels).forEach(([key, value]) => {
      ImportDataConfig_AnnotationLabelsEntry.encode({ key: key as any, value }, writer.uint32(26).fork()).join();
    });
    if (message.importSchemaUri !== "") {
      writer.uint32(34).string(message.importSchemaUri);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportDataConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportDataConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.gcsSource = GcsSource.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = ImportDataConfig_DataItemLabelsEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.dataItemLabels[entry2.key] = entry2.value;
          }
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          const entry3 = ImportDataConfig_AnnotationLabelsEntry.decode(reader, reader.uint32());
          if (entry3.value !== undefined) {
            message.annotationLabels[entry3.key] = entry3.value;
          }
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.importSchemaUri = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportDataConfig {
    return {
      gcsSource: isSet(object.gcsSource) ? GcsSource.fromJSON(object.gcsSource) : undefined,
      dataItemLabels: isObject(object.dataItemLabels)
        ? Object.entries(object.dataItemLabels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      annotationLabels: isObject(object.annotationLabels)
        ? Object.entries(object.annotationLabels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      importSchemaUri: isSet(object.importSchemaUri) ? globalThis.String(object.importSchemaUri) : "",
    };
  },

  toJSON(message: ImportDataConfig): unknown {
    const obj: any = {};
    if (message.gcsSource !== undefined) {
      obj.gcsSource = GcsSource.toJSON(message.gcsSource);
    }
    if (message.dataItemLabels) {
      const entries = Object.entries(message.dataItemLabels);
      if (entries.length > 0) {
        obj.dataItemLabels = {};
        entries.forEach(([k, v]) => {
          obj.dataItemLabels[k] = v;
        });
      }
    }
    if (message.annotationLabels) {
      const entries = Object.entries(message.annotationLabels);
      if (entries.length > 0) {
        obj.annotationLabels = {};
        entries.forEach(([k, v]) => {
          obj.annotationLabels[k] = v;
        });
      }
    }
    if (message.importSchemaUri !== "") {
      obj.importSchemaUri = message.importSchemaUri;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportDataConfig>): ImportDataConfig {
    return ImportDataConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportDataConfig>): ImportDataConfig {
    const message = createBaseImportDataConfig();
    message.gcsSource = (object.gcsSource !== undefined && object.gcsSource !== null)
      ? GcsSource.fromPartial(object.gcsSource)
      : undefined;
    message.dataItemLabels = Object.entries(object.dataItemLabels ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.annotationLabels = Object.entries(object.annotationLabels ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.importSchemaUri = object.importSchemaUri ?? "";
    return message;
  },
};

function createBaseImportDataConfig_DataItemLabelsEntry(): ImportDataConfig_DataItemLabelsEntry {
  return { key: "", value: "" };
}

export const ImportDataConfig_DataItemLabelsEntry: MessageFns<ImportDataConfig_DataItemLabelsEntry> = {
  encode(message: ImportDataConfig_DataItemLabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportDataConfig_DataItemLabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportDataConfig_DataItemLabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportDataConfig_DataItemLabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: ImportDataConfig_DataItemLabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportDataConfig_DataItemLabelsEntry>): ImportDataConfig_DataItemLabelsEntry {
    return ImportDataConfig_DataItemLabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportDataConfig_DataItemLabelsEntry>): ImportDataConfig_DataItemLabelsEntry {
    const message = createBaseImportDataConfig_DataItemLabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseImportDataConfig_AnnotationLabelsEntry(): ImportDataConfig_AnnotationLabelsEntry {
  return { key: "", value: "" };
}

export const ImportDataConfig_AnnotationLabelsEntry: MessageFns<ImportDataConfig_AnnotationLabelsEntry> = {
  encode(message: ImportDataConfig_AnnotationLabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportDataConfig_AnnotationLabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportDataConfig_AnnotationLabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportDataConfig_AnnotationLabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: ImportDataConfig_AnnotationLabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportDataConfig_AnnotationLabelsEntry>): ImportDataConfig_AnnotationLabelsEntry {
    return ImportDataConfig_AnnotationLabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportDataConfig_AnnotationLabelsEntry>): ImportDataConfig_AnnotationLabelsEntry {
    const message = createBaseImportDataConfig_AnnotationLabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseExportDataConfig(): ExportDataConfig {
  return { gcsDestination: undefined, fractionSplit: undefined, annotationsFilter: "" };
}

export const ExportDataConfig: MessageFns<ExportDataConfig> = {
  encode(message: ExportDataConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcsDestination !== undefined) {
      GcsDestination.encode(message.gcsDestination, writer.uint32(10).fork()).join();
    }
    if (message.fractionSplit !== undefined) {
      ExportFractionSplit.encode(message.fractionSplit, writer.uint32(42).fork()).join();
    }
    if (message.annotationsFilter !== "") {
      writer.uint32(18).string(message.annotationsFilter);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportDataConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportDataConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.gcsDestination = GcsDestination.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.fractionSplit = ExportFractionSplit.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.annotationsFilter = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportDataConfig {
    return {
      gcsDestination: isSet(object.gcsDestination) ? GcsDestination.fromJSON(object.gcsDestination) : undefined,
      fractionSplit: isSet(object.fractionSplit) ? ExportFractionSplit.fromJSON(object.fractionSplit) : undefined,
      annotationsFilter: isSet(object.annotationsFilter) ? globalThis.String(object.annotationsFilter) : "",
    };
  },

  toJSON(message: ExportDataConfig): unknown {
    const obj: any = {};
    if (message.gcsDestination !== undefined) {
      obj.gcsDestination = GcsDestination.toJSON(message.gcsDestination);
    }
    if (message.fractionSplit !== undefined) {
      obj.fractionSplit = ExportFractionSplit.toJSON(message.fractionSplit);
    }
    if (message.annotationsFilter !== "") {
      obj.annotationsFilter = message.annotationsFilter;
    }
    return obj;
  },

  create(base?: DeepPartial<ExportDataConfig>): ExportDataConfig {
    return ExportDataConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportDataConfig>): ExportDataConfig {
    const message = createBaseExportDataConfig();
    message.gcsDestination = (object.gcsDestination !== undefined && object.gcsDestination !== null)
      ? GcsDestination.fromPartial(object.gcsDestination)
      : undefined;
    message.fractionSplit = (object.fractionSplit !== undefined && object.fractionSplit !== null)
      ? ExportFractionSplit.fromPartial(object.fractionSplit)
      : undefined;
    message.annotationsFilter = object.annotationsFilter ?? "";
    return message;
  },
};

function createBaseExportFractionSplit(): ExportFractionSplit {
  return { trainingFraction: 0, validationFraction: 0, testFraction: 0 };
}

export const ExportFractionSplit: MessageFns<ExportFractionSplit> = {
  encode(message: ExportFractionSplit, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.trainingFraction !== 0) {
      writer.uint32(9).double(message.trainingFraction);
    }
    if (message.validationFraction !== 0) {
      writer.uint32(17).double(message.validationFraction);
    }
    if (message.testFraction !== 0) {
      writer.uint32(25).double(message.testFraction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportFractionSplit {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportFractionSplit();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 9) {
            break;
          }

          message.trainingFraction = reader.double();
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.validationFraction = reader.double();
          continue;
        case 3:
          if (tag !== 25) {
            break;
          }

          message.testFraction = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportFractionSplit {
    return {
      trainingFraction: isSet(object.trainingFraction) ? globalThis.Number(object.trainingFraction) : 0,
      validationFraction: isSet(object.validationFraction) ? globalThis.Number(object.validationFraction) : 0,
      testFraction: isSet(object.testFraction) ? globalThis.Number(object.testFraction) : 0,
    };
  },

  toJSON(message: ExportFractionSplit): unknown {
    const obj: any = {};
    if (message.trainingFraction !== 0) {
      obj.trainingFraction = message.trainingFraction;
    }
    if (message.validationFraction !== 0) {
      obj.validationFraction = message.validationFraction;
    }
    if (message.testFraction !== 0) {
      obj.testFraction = message.testFraction;
    }
    return obj;
  },

  create(base?: DeepPartial<ExportFractionSplit>): ExportFractionSplit {
    return ExportFractionSplit.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportFractionSplit>): ExportFractionSplit {
    const message = createBaseExportFractionSplit();
    message.trainingFraction = object.trainingFraction ?? 0;
    message.validationFraction = object.validationFraction ?? 0;
    message.testFraction = object.testFraction ?? 0;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
