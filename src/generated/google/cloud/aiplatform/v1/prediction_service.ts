// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/aiplatform/v1/prediction_service.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { HttpBody } from "../../../api/httpbody.js";
import { Value } from "../../../protobuf/struct.js";
import { Candidate, Content, GenerationConfig, SafetyRating, SafetySetting } from "./content.js";
import { Explanation, ExplanationSpecOverride } from "./explanation.js";
import { Tool, ToolConfig } from "./tool.js";
import { Tensor } from "./types.js";

export const protobufPackage = "google.cloud.aiplatform.v1";

/**
 * Request message for
 * [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict].
 */
export interface PredictRequest {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   */
  endpoint: string;
  /**
   * Required. The instances that are the input to the prediction call.
   * A DeployedModel may have an upper limit on the number of instances it
   * supports per request, and when it is exceeded the prediction call errors
   * in case of AutoML Models, or, in case of customer created Models, the
   * behaviour is as documented by that Model.
   * The schema of any single instance may be specified via Endpoint's
   * DeployedModels' [Model's][google.cloud.aiplatform.v1.DeployedModel.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
   * [instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri].
   */
  instances: any[];
  /**
   * The parameters that govern the prediction. The schema of the parameters may
   * be specified via Endpoint's DeployedModels' [Model's
   * ][google.cloud.aiplatform.v1.DeployedModel.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
   * [parameters_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri].
   */
  parameters: any | undefined;
}

/**
 * Response message for
 * [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict].
 */
export interface PredictResponse {
  /**
   * The predictions that are the output of the predictions call.
   * The schema of any single prediction may be specified via Endpoint's
   * DeployedModels' [Model's ][google.cloud.aiplatform.v1.DeployedModel.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
   * [prediction_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.prediction_schema_uri].
   */
  predictions: any[];
  /** ID of the Endpoint's DeployedModel that served this prediction. */
  deployedModelId: string;
  /**
   * Output only. The resource name of the Model which is deployed as the
   * DeployedModel that this prediction hits.
   */
  model: string;
  /**
   * Output only. The version ID of the Model which is deployed as the
   * DeployedModel that this prediction hits.
   */
  modelVersionId: string;
  /**
   * Output only. The [display
   * name][google.cloud.aiplatform.v1.Model.display_name] of the Model which is
   * deployed as the DeployedModel that this prediction hits.
   */
  modelDisplayName: string;
  /**
   * Output only. Request-level metadata returned by the model. The metadata
   * type will be dependent upon the model implementation.
   */
  metadata: any | undefined;
}

/**
 * Request message for
 * [PredictionService.RawPredict][google.cloud.aiplatform.v1.PredictionService.RawPredict].
 */
export interface RawPredictRequest {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   */
  endpoint: string;
  /**
   * The prediction input. Supports HTTP headers and arbitrary data payload.
   *
   * A [DeployedModel][google.cloud.aiplatform.v1.DeployedModel] may have an
   * upper limit on the number of instances it supports per request. When this
   * limit it is exceeded for an AutoML model, the
   * [RawPredict][google.cloud.aiplatform.v1.PredictionService.RawPredict]
   * method returns an error. When this limit is exceeded for a custom-trained
   * model, the behavior varies depending on the model.
   *
   * You can specify the schema for each instance in the
   * [predict_schemata.instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
   * field when you create a [Model][google.cloud.aiplatform.v1.Model]. This
   * schema applies when you deploy the `Model` as a `DeployedModel` to an
   * [Endpoint][google.cloud.aiplatform.v1.Endpoint] and use the `RawPredict`
   * method.
   */
  httpBody: HttpBody | undefined;
}

/**
 * Request message for
 * [PredictionService.StreamRawPredict][google.cloud.aiplatform.v1.PredictionService.StreamRawPredict].
 */
export interface StreamRawPredictRequest {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   */
  endpoint: string;
  /** The prediction input. Supports HTTP headers and arbitrary data payload. */
  httpBody: HttpBody | undefined;
}

/**
 * Request message for
 * [PredictionService.DirectPredict][google.cloud.aiplatform.v1.PredictionService.DirectPredict].
 */
export interface DirectPredictRequest {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   */
  endpoint: string;
  /** The prediction input. */
  inputs: Tensor[];
  /** The parameters that govern the prediction. */
  parameters: Tensor | undefined;
}

/**
 * Response message for
 * [PredictionService.DirectPredict][google.cloud.aiplatform.v1.PredictionService.DirectPredict].
 */
export interface DirectPredictResponse {
  /** The prediction output. */
  outputs: Tensor[];
  /** The parameters that govern the prediction. */
  parameters: Tensor | undefined;
}

/**
 * Request message for
 * [PredictionService.DirectRawPredict][google.cloud.aiplatform.v1.PredictionService.DirectRawPredict].
 */
export interface DirectRawPredictRequest {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   */
  endpoint: string;
  /**
   * Fully qualified name of the API method being invoked to perform
   * predictions.
   *
   * Format:
   * `/namespace.Service/Method/`
   * Example:
   * `/tensorflow.serving.PredictionService/Predict`
   */
  methodName: string;
  /** The prediction input. */
  input: Buffer;
}

/**
 * Response message for
 * [PredictionService.DirectRawPredict][google.cloud.aiplatform.v1.PredictionService.DirectRawPredict].
 */
export interface DirectRawPredictResponse {
  /** The prediction output. */
  output: Buffer;
}

/**
 * Request message for
 * [PredictionService.StreamDirectPredict][google.cloud.aiplatform.v1.PredictionService.StreamDirectPredict].
 *
 * The first message must contain
 * [endpoint][google.cloud.aiplatform.v1.StreamDirectPredictRequest.endpoint]
 * field and optionally [input][]. The subsequent messages must contain
 * [input][].
 */
export interface StreamDirectPredictRequest {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   */
  endpoint: string;
  /** Optional. The prediction input. */
  inputs: Tensor[];
  /** Optional. The parameters that govern the prediction. */
  parameters: Tensor | undefined;
}

/**
 * Response message for
 * [PredictionService.StreamDirectPredict][google.cloud.aiplatform.v1.PredictionService.StreamDirectPredict].
 */
export interface StreamDirectPredictResponse {
  /** The prediction output. */
  outputs: Tensor[];
  /** The parameters that govern the prediction. */
  parameters: Tensor | undefined;
}

/**
 * Request message for
 * [PredictionService.StreamDirectRawPredict][google.cloud.aiplatform.v1.PredictionService.StreamDirectRawPredict].
 *
 * The first message must contain
 * [endpoint][google.cloud.aiplatform.v1.StreamDirectRawPredictRequest.endpoint]
 * and
 * [method_name][google.cloud.aiplatform.v1.StreamDirectRawPredictRequest.method_name]
 * fields and optionally
 * [input][google.cloud.aiplatform.v1.StreamDirectRawPredictRequest.input]. The
 * subsequent messages must contain
 * [input][google.cloud.aiplatform.v1.StreamDirectRawPredictRequest.input].
 * [method_name][google.cloud.aiplatform.v1.StreamDirectRawPredictRequest.method_name]
 * in the subsequent messages have no effect.
 */
export interface StreamDirectRawPredictRequest {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   */
  endpoint: string;
  /**
   * Optional. Fully qualified name of the API method being invoked to perform
   * predictions.
   *
   * Format:
   * `/namespace.Service/Method/`
   * Example:
   * `/tensorflow.serving.PredictionService/Predict`
   */
  methodName: string;
  /** Optional. The prediction input. */
  input: Buffer;
}

/**
 * Response message for
 * [PredictionService.StreamDirectRawPredict][google.cloud.aiplatform.v1.PredictionService.StreamDirectRawPredict].
 */
export interface StreamDirectRawPredictResponse {
  /** The prediction output. */
  output: Buffer;
}

/**
 * Request message for
 * [PredictionService.StreamingPredict][google.cloud.aiplatform.v1.PredictionService.StreamingPredict].
 *
 * The first message must contain
 * [endpoint][google.cloud.aiplatform.v1.StreamingPredictRequest.endpoint] field
 * and optionally [input][]. The subsequent messages must contain [input][].
 */
export interface StreamingPredictRequest {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   */
  endpoint: string;
  /** The prediction input. */
  inputs: Tensor[];
  /** The parameters that govern the prediction. */
  parameters: Tensor | undefined;
}

/**
 * Response message for
 * [PredictionService.StreamingPredict][google.cloud.aiplatform.v1.PredictionService.StreamingPredict].
 */
export interface StreamingPredictResponse {
  /** The prediction output. */
  outputs: Tensor[];
  /** The parameters that govern the prediction. */
  parameters: Tensor | undefined;
}

/**
 * Request message for
 * [PredictionService.StreamingRawPredict][google.cloud.aiplatform.v1.PredictionService.StreamingRawPredict].
 *
 * The first message must contain
 * [endpoint][google.cloud.aiplatform.v1.StreamingRawPredictRequest.endpoint]
 * and
 * [method_name][google.cloud.aiplatform.v1.StreamingRawPredictRequest.method_name]
 * fields and optionally
 * [input][google.cloud.aiplatform.v1.StreamingRawPredictRequest.input]. The
 * subsequent messages must contain
 * [input][google.cloud.aiplatform.v1.StreamingRawPredictRequest.input].
 * [method_name][google.cloud.aiplatform.v1.StreamingRawPredictRequest.method_name]
 * in the subsequent messages have no effect.
 */
export interface StreamingRawPredictRequest {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   */
  endpoint: string;
  /**
   * Fully qualified name of the API method being invoked to perform
   * predictions.
   *
   * Format:
   * `/namespace.Service/Method/`
   * Example:
   * `/tensorflow.serving.PredictionService/Predict`
   */
  methodName: string;
  /** The prediction input. */
  input: Buffer;
}

/**
 * Response message for
 * [PredictionService.StreamingRawPredict][google.cloud.aiplatform.v1.PredictionService.StreamingRawPredict].
 */
export interface StreamingRawPredictResponse {
  /** The prediction output. */
  output: Buffer;
}

/**
 * Request message for
 * [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain].
 */
export interface ExplainRequest {
  /**
   * Required. The name of the Endpoint requested to serve the explanation.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   */
  endpoint: string;
  /**
   * Required. The instances that are the input to the explanation call.
   * A DeployedModel may have an upper limit on the number of instances it
   * supports per request, and when it is exceeded the explanation call errors
   * in case of AutoML Models, or, in case of customer created Models, the
   * behaviour is as documented by that Model.
   * The schema of any single instance may be specified via Endpoint's
   * DeployedModels' [Model's][google.cloud.aiplatform.v1.DeployedModel.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
   * [instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri].
   */
  instances: any[];
  /**
   * The parameters that govern the prediction. The schema of the parameters may
   * be specified via Endpoint's DeployedModels' [Model's
   * ][google.cloud.aiplatform.v1.DeployedModel.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
   * [parameters_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri].
   */
  parameters:
    | any
    | undefined;
  /**
   * If specified, overrides the
   * [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
   * of the DeployedModel. Can be used for explaining prediction results with
   * different configurations, such as:
   *  - Explaining top-5 predictions results as opposed to top-1;
   *  - Increasing path count or step count of the attribution methods to reduce
   *    approximate errors;
   *  - Using different baselines for explaining the prediction results.
   */
  explanationSpecOverride:
    | ExplanationSpecOverride
    | undefined;
  /**
   * If specified, this ExplainRequest will be served by the chosen
   * DeployedModel, overriding
   * [Endpoint.traffic_split][google.cloud.aiplatform.v1.Endpoint.traffic_split].
   */
  deployedModelId: string;
}

/**
 * Response message for
 * [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain].
 */
export interface ExplainResponse {
  /**
   * The explanations of the Model's
   * [PredictResponse.predictions][google.cloud.aiplatform.v1.PredictResponse.predictions].
   *
   * It has the same number of elements as
   * [instances][google.cloud.aiplatform.v1.ExplainRequest.instances] to be
   * explained.
   */
  explanations: Explanation[];
  /** ID of the Endpoint's DeployedModel that served this explanation. */
  deployedModelId: string;
  /**
   * The predictions that are the output of the predictions call.
   * Same as
   * [PredictResponse.predictions][google.cloud.aiplatform.v1.PredictResponse.predictions].
   */
  predictions: any[];
}

/** Request message for [PredictionService.CountTokens][]. */
export interface CountTokensRequest {
  /**
   * Required. The name of the Endpoint requested to perform token counting.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   */
  endpoint: string;
  /**
   * Optional. The name of the publisher model requested to serve the
   * prediction. Format:
   * `projects/{project}/locations/{location}/publishers/* /models/*`
   */
  model: string;
  /**
   * Optional. The instances that are the input to token counting call.
   * Schema is identical to the prediction schema of the underlying model.
   */
  instances: any[];
  /** Optional. Input content. */
  contents: Content[];
  /**
   * Optional. The user provided system instructions for the model.
   * Note: only text should be used in parts and content in each part will be in
   * a separate paragraph.
   */
  systemInstruction?:
    | Content
    | undefined;
  /**
   * Optional. A list of `Tools` the model may use to generate the next
   * response.
   *
   * A `Tool` is a piece of code that enables the system to interact with
   * external systems to perform an action, or set of actions, outside of
   * knowledge and scope of the model.
   */
  tools: Tool[];
  /**
   * Optional. Generation config that the model will use to generate the
   * response.
   */
  generationConfig?: GenerationConfig | undefined;
}

/** Response message for [PredictionService.CountTokens][]. */
export interface CountTokensResponse {
  /** The total number of tokens counted across all instances from the request. */
  totalTokens: number;
  /**
   * The total number of billable characters counted across all instances from
   * the request.
   */
  totalBillableCharacters: number;
}

/** Request message for [PredictionService.GenerateContent]. */
export interface GenerateContentRequest {
  /**
   * Required. The fully qualified name of the publisher model or tuned model
   * endpoint to use.
   *
   * Publisher model format:
   * `projects/{project}/locations/{location}/publishers/* /models/*`
   *
   * Tuned model endpoint format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   */
  model: string;
  /**
   * Required. The content of the current conversation with the model.
   *
   * For single-turn queries, this is a single instance. For multi-turn queries,
   * this is a repeated field that contains conversation history + latest
   * request.
   */
  contents: Content[];
  /**
   * Optional. The user provided system instructions for the model.
   * Note: only text should be used in parts and content in each part will be in
   * a separate paragraph.
   */
  systemInstruction?:
    | Content
    | undefined;
  /**
   * Optional. A list of `Tools` the model may use to generate the next
   * response.
   *
   * A `Tool` is a piece of code that enables the system to interact with
   * external systems to perform an action, or set of actions, outside of
   * knowledge and scope of the model.
   */
  tools: Tool[];
  /**
   * Optional. Tool config. This config is shared for all tools provided in the
   * request.
   */
  toolConfig:
    | ToolConfig
    | undefined;
  /**
   * Optional. The labels with user-defined metadata for the request. It is used
   * for billing and reporting only.
   *
   * Label keys and values can be no longer than 63 characters
   * (Unicode codepoints) and can only contain lowercase letters, numeric
   * characters, underscores, and dashes. International characters are allowed.
   * Label values are optional. Label keys must start with a letter.
   */
  labels: { [key: string]: string };
  /**
   * Optional. Per request settings for blocking unsafe content.
   * Enforced on GenerateContentResponse.candidates.
   */
  safetySettings: SafetySetting[];
  /** Optional. Generation config. */
  generationConfig: GenerationConfig | undefined;
}

export interface GenerateContentRequest_LabelsEntry {
  key: string;
  value: string;
}

/** Response message for [PredictionService.GenerateContent]. */
export interface GenerateContentResponse {
  /** Output only. Generated candidates. */
  candidates: Candidate[];
  /** Output only. The model version used to generate the response. */
  modelVersion: string;
  /**
   * Output only. Content filter results for a prompt sent in the request.
   * Note: Sent only in the first stream chunk.
   * Only happens when no candidates were generated due to content violations.
   */
  promptFeedback:
    | GenerateContentResponse_PromptFeedback
    | undefined;
  /** Usage metadata about the response(s). */
  usageMetadata: GenerateContentResponse_UsageMetadata | undefined;
}

/** Content filter results for a prompt sent in the request. */
export interface GenerateContentResponse_PromptFeedback {
  /** Output only. Blocked reason. */
  blockReason: GenerateContentResponse_PromptFeedback_BlockedReason;
  /** Output only. Safety ratings. */
  safetyRatings: SafetyRating[];
  /** Output only. A readable block reason message. */
  blockReasonMessage: string;
}

/** Blocked reason enumeration. */
export enum GenerateContentResponse_PromptFeedback_BlockedReason {
  /** BLOCKED_REASON_UNSPECIFIED - Unspecified blocked reason. */
  BLOCKED_REASON_UNSPECIFIED = 0,
  /** SAFETY - Candidates blocked due to safety. */
  SAFETY = 1,
  /** OTHER - Candidates blocked due to other reason. */
  OTHER = 2,
  /**
   * BLOCKLIST - Candidates blocked due to the terms which are included from the
   * terminology blocklist.
   */
  BLOCKLIST = 3,
  /** PROHIBITED_CONTENT - Candidates blocked due to prohibited content. */
  PROHIBITED_CONTENT = 4,
  UNRECOGNIZED = -1,
}

export function generateContentResponse_PromptFeedback_BlockedReasonFromJSON(
  object: any,
): GenerateContentResponse_PromptFeedback_BlockedReason {
  switch (object) {
    case 0:
    case "BLOCKED_REASON_UNSPECIFIED":
      return GenerateContentResponse_PromptFeedback_BlockedReason.BLOCKED_REASON_UNSPECIFIED;
    case 1:
    case "SAFETY":
      return GenerateContentResponse_PromptFeedback_BlockedReason.SAFETY;
    case 2:
    case "OTHER":
      return GenerateContentResponse_PromptFeedback_BlockedReason.OTHER;
    case 3:
    case "BLOCKLIST":
      return GenerateContentResponse_PromptFeedback_BlockedReason.BLOCKLIST;
    case 4:
    case "PROHIBITED_CONTENT":
      return GenerateContentResponse_PromptFeedback_BlockedReason.PROHIBITED_CONTENT;
    case -1:
    case "UNRECOGNIZED":
    default:
      return GenerateContentResponse_PromptFeedback_BlockedReason.UNRECOGNIZED;
  }
}

export function generateContentResponse_PromptFeedback_BlockedReasonToJSON(
  object: GenerateContentResponse_PromptFeedback_BlockedReason,
): string {
  switch (object) {
    case GenerateContentResponse_PromptFeedback_BlockedReason.BLOCKED_REASON_UNSPECIFIED:
      return "BLOCKED_REASON_UNSPECIFIED";
    case GenerateContentResponse_PromptFeedback_BlockedReason.SAFETY:
      return "SAFETY";
    case GenerateContentResponse_PromptFeedback_BlockedReason.OTHER:
      return "OTHER";
    case GenerateContentResponse_PromptFeedback_BlockedReason.BLOCKLIST:
      return "BLOCKLIST";
    case GenerateContentResponse_PromptFeedback_BlockedReason.PROHIBITED_CONTENT:
      return "PROHIBITED_CONTENT";
    case GenerateContentResponse_PromptFeedback_BlockedReason.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Usage metadata about response(s). */
export interface GenerateContentResponse_UsageMetadata {
  /**
   * Number of tokens in the request. When `cached_content` is set, this is
   * still the total effective prompt size meaning this includes the number of
   * tokens in the cached content.
   */
  promptTokenCount: number;
  /** Number of tokens in the response(s). */
  candidatesTokenCount: number;
  /** Total token count for prompt and response candidates. */
  totalTokenCount: number;
}

function createBasePredictRequest(): PredictRequest {
  return { endpoint: "", instances: [], parameters: undefined };
}

export const PredictRequest: MessageFns<PredictRequest> = {
  encode(message: PredictRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.endpoint !== "") {
      writer.uint32(10).string(message.endpoint);
    }
    for (const v of message.instances) {
      Value.encode(Value.wrap(v!), writer.uint32(18).fork()).join();
    }
    if (message.parameters !== undefined) {
      Value.encode(Value.wrap(message.parameters), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PredictRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePredictRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.endpoint = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.instances.push(Value.unwrap(Value.decode(reader, reader.uint32())));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.parameters = Value.unwrap(Value.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PredictRequest {
    return {
      endpoint: isSet(object.endpoint) ? globalThis.String(object.endpoint) : "",
      instances: globalThis.Array.isArray(object?.instances) ? [...object.instances] : [],
      parameters: isSet(object?.parameters) ? object.parameters : undefined,
    };
  },

  toJSON(message: PredictRequest): unknown {
    const obj: any = {};
    if (message.endpoint !== "") {
      obj.endpoint = message.endpoint;
    }
    if (message.instances?.length) {
      obj.instances = message.instances;
    }
    if (message.parameters !== undefined) {
      obj.parameters = message.parameters;
    }
    return obj;
  },

  create(base?: DeepPartial<PredictRequest>): PredictRequest {
    return PredictRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PredictRequest>): PredictRequest {
    const message = createBasePredictRequest();
    message.endpoint = object.endpoint ?? "";
    message.instances = object.instances?.map((e) => e) || [];
    message.parameters = object.parameters ?? undefined;
    return message;
  },
};

function createBasePredictResponse(): PredictResponse {
  return {
    predictions: [],
    deployedModelId: "",
    model: "",
    modelVersionId: "",
    modelDisplayName: "",
    metadata: undefined,
  };
}

export const PredictResponse: MessageFns<PredictResponse> = {
  encode(message: PredictResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.predictions) {
      Value.encode(Value.wrap(v!), writer.uint32(10).fork()).join();
    }
    if (message.deployedModelId !== "") {
      writer.uint32(18).string(message.deployedModelId);
    }
    if (message.model !== "") {
      writer.uint32(26).string(message.model);
    }
    if (message.modelVersionId !== "") {
      writer.uint32(42).string(message.modelVersionId);
    }
    if (message.modelDisplayName !== "") {
      writer.uint32(34).string(message.modelDisplayName);
    }
    if (message.metadata !== undefined) {
      Value.encode(Value.wrap(message.metadata), writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PredictResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePredictResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.predictions.push(Value.unwrap(Value.decode(reader, reader.uint32())));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.deployedModelId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.model = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.modelVersionId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.modelDisplayName = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.metadata = Value.unwrap(Value.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PredictResponse {
    return {
      predictions: globalThis.Array.isArray(object?.predictions) ? [...object.predictions] : [],
      deployedModelId: isSet(object.deployedModelId) ? globalThis.String(object.deployedModelId) : "",
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      modelVersionId: isSet(object.modelVersionId) ? globalThis.String(object.modelVersionId) : "",
      modelDisplayName: isSet(object.modelDisplayName) ? globalThis.String(object.modelDisplayName) : "",
      metadata: isSet(object?.metadata) ? object.metadata : undefined,
    };
  },

  toJSON(message: PredictResponse): unknown {
    const obj: any = {};
    if (message.predictions?.length) {
      obj.predictions = message.predictions;
    }
    if (message.deployedModelId !== "") {
      obj.deployedModelId = message.deployedModelId;
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.modelVersionId !== "") {
      obj.modelVersionId = message.modelVersionId;
    }
    if (message.modelDisplayName !== "") {
      obj.modelDisplayName = message.modelDisplayName;
    }
    if (message.metadata !== undefined) {
      obj.metadata = message.metadata;
    }
    return obj;
  },

  create(base?: DeepPartial<PredictResponse>): PredictResponse {
    return PredictResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PredictResponse>): PredictResponse {
    const message = createBasePredictResponse();
    message.predictions = object.predictions?.map((e) => e) || [];
    message.deployedModelId = object.deployedModelId ?? "";
    message.model = object.model ?? "";
    message.modelVersionId = object.modelVersionId ?? "";
    message.modelDisplayName = object.modelDisplayName ?? "";
    message.metadata = object.metadata ?? undefined;
    return message;
  },
};

function createBaseRawPredictRequest(): RawPredictRequest {
  return { endpoint: "", httpBody: undefined };
}

export const RawPredictRequest: MessageFns<RawPredictRequest> = {
  encode(message: RawPredictRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.endpoint !== "") {
      writer.uint32(10).string(message.endpoint);
    }
    if (message.httpBody !== undefined) {
      HttpBody.encode(message.httpBody, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RawPredictRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRawPredictRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.endpoint = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.httpBody = HttpBody.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RawPredictRequest {
    return {
      endpoint: isSet(object.endpoint) ? globalThis.String(object.endpoint) : "",
      httpBody: isSet(object.httpBody) ? HttpBody.fromJSON(object.httpBody) : undefined,
    };
  },

  toJSON(message: RawPredictRequest): unknown {
    const obj: any = {};
    if (message.endpoint !== "") {
      obj.endpoint = message.endpoint;
    }
    if (message.httpBody !== undefined) {
      obj.httpBody = HttpBody.toJSON(message.httpBody);
    }
    return obj;
  },

  create(base?: DeepPartial<RawPredictRequest>): RawPredictRequest {
    return RawPredictRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RawPredictRequest>): RawPredictRequest {
    const message = createBaseRawPredictRequest();
    message.endpoint = object.endpoint ?? "";
    message.httpBody = (object.httpBody !== undefined && object.httpBody !== null)
      ? HttpBody.fromPartial(object.httpBody)
      : undefined;
    return message;
  },
};

function createBaseStreamRawPredictRequest(): StreamRawPredictRequest {
  return { endpoint: "", httpBody: undefined };
}

export const StreamRawPredictRequest: MessageFns<StreamRawPredictRequest> = {
  encode(message: StreamRawPredictRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.endpoint !== "") {
      writer.uint32(10).string(message.endpoint);
    }
    if (message.httpBody !== undefined) {
      HttpBody.encode(message.httpBody, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamRawPredictRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamRawPredictRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.endpoint = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.httpBody = HttpBody.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamRawPredictRequest {
    return {
      endpoint: isSet(object.endpoint) ? globalThis.String(object.endpoint) : "",
      httpBody: isSet(object.httpBody) ? HttpBody.fromJSON(object.httpBody) : undefined,
    };
  },

  toJSON(message: StreamRawPredictRequest): unknown {
    const obj: any = {};
    if (message.endpoint !== "") {
      obj.endpoint = message.endpoint;
    }
    if (message.httpBody !== undefined) {
      obj.httpBody = HttpBody.toJSON(message.httpBody);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamRawPredictRequest>): StreamRawPredictRequest {
    return StreamRawPredictRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamRawPredictRequest>): StreamRawPredictRequest {
    const message = createBaseStreamRawPredictRequest();
    message.endpoint = object.endpoint ?? "";
    message.httpBody = (object.httpBody !== undefined && object.httpBody !== null)
      ? HttpBody.fromPartial(object.httpBody)
      : undefined;
    return message;
  },
};

function createBaseDirectPredictRequest(): DirectPredictRequest {
  return { endpoint: "", inputs: [], parameters: undefined };
}

export const DirectPredictRequest: MessageFns<DirectPredictRequest> = {
  encode(message: DirectPredictRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.endpoint !== "") {
      writer.uint32(10).string(message.endpoint);
    }
    for (const v of message.inputs) {
      Tensor.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.parameters !== undefined) {
      Tensor.encode(message.parameters, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DirectPredictRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDirectPredictRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.endpoint = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inputs.push(Tensor.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.parameters = Tensor.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DirectPredictRequest {
    return {
      endpoint: isSet(object.endpoint) ? globalThis.String(object.endpoint) : "",
      inputs: globalThis.Array.isArray(object?.inputs) ? object.inputs.map((e: any) => Tensor.fromJSON(e)) : [],
      parameters: isSet(object.parameters) ? Tensor.fromJSON(object.parameters) : undefined,
    };
  },

  toJSON(message: DirectPredictRequest): unknown {
    const obj: any = {};
    if (message.endpoint !== "") {
      obj.endpoint = message.endpoint;
    }
    if (message.inputs?.length) {
      obj.inputs = message.inputs.map((e) => Tensor.toJSON(e));
    }
    if (message.parameters !== undefined) {
      obj.parameters = Tensor.toJSON(message.parameters);
    }
    return obj;
  },

  create(base?: DeepPartial<DirectPredictRequest>): DirectPredictRequest {
    return DirectPredictRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DirectPredictRequest>): DirectPredictRequest {
    const message = createBaseDirectPredictRequest();
    message.endpoint = object.endpoint ?? "";
    message.inputs = object.inputs?.map((e) => Tensor.fromPartial(e)) || [];
    message.parameters = (object.parameters !== undefined && object.parameters !== null)
      ? Tensor.fromPartial(object.parameters)
      : undefined;
    return message;
  },
};

function createBaseDirectPredictResponse(): DirectPredictResponse {
  return { outputs: [], parameters: undefined };
}

export const DirectPredictResponse: MessageFns<DirectPredictResponse> = {
  encode(message: DirectPredictResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.outputs) {
      Tensor.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.parameters !== undefined) {
      Tensor.encode(message.parameters, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DirectPredictResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDirectPredictResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.outputs.push(Tensor.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.parameters = Tensor.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DirectPredictResponse {
    return {
      outputs: globalThis.Array.isArray(object?.outputs) ? object.outputs.map((e: any) => Tensor.fromJSON(e)) : [],
      parameters: isSet(object.parameters) ? Tensor.fromJSON(object.parameters) : undefined,
    };
  },

  toJSON(message: DirectPredictResponse): unknown {
    const obj: any = {};
    if (message.outputs?.length) {
      obj.outputs = message.outputs.map((e) => Tensor.toJSON(e));
    }
    if (message.parameters !== undefined) {
      obj.parameters = Tensor.toJSON(message.parameters);
    }
    return obj;
  },

  create(base?: DeepPartial<DirectPredictResponse>): DirectPredictResponse {
    return DirectPredictResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DirectPredictResponse>): DirectPredictResponse {
    const message = createBaseDirectPredictResponse();
    message.outputs = object.outputs?.map((e) => Tensor.fromPartial(e)) || [];
    message.parameters = (object.parameters !== undefined && object.parameters !== null)
      ? Tensor.fromPartial(object.parameters)
      : undefined;
    return message;
  },
};

function createBaseDirectRawPredictRequest(): DirectRawPredictRequest {
  return { endpoint: "", methodName: "", input: Buffer.alloc(0) };
}

export const DirectRawPredictRequest: MessageFns<DirectRawPredictRequest> = {
  encode(message: DirectRawPredictRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.endpoint !== "") {
      writer.uint32(10).string(message.endpoint);
    }
    if (message.methodName !== "") {
      writer.uint32(18).string(message.methodName);
    }
    if (message.input.length !== 0) {
      writer.uint32(26).bytes(message.input);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DirectRawPredictRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDirectRawPredictRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.endpoint = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.methodName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.input = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DirectRawPredictRequest {
    return {
      endpoint: isSet(object.endpoint) ? globalThis.String(object.endpoint) : "",
      methodName: isSet(object.methodName) ? globalThis.String(object.methodName) : "",
      input: isSet(object.input) ? Buffer.from(bytesFromBase64(object.input)) : Buffer.alloc(0),
    };
  },

  toJSON(message: DirectRawPredictRequest): unknown {
    const obj: any = {};
    if (message.endpoint !== "") {
      obj.endpoint = message.endpoint;
    }
    if (message.methodName !== "") {
      obj.methodName = message.methodName;
    }
    if (message.input.length !== 0) {
      obj.input = base64FromBytes(message.input);
    }
    return obj;
  },

  create(base?: DeepPartial<DirectRawPredictRequest>): DirectRawPredictRequest {
    return DirectRawPredictRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DirectRawPredictRequest>): DirectRawPredictRequest {
    const message = createBaseDirectRawPredictRequest();
    message.endpoint = object.endpoint ?? "";
    message.methodName = object.methodName ?? "";
    message.input = object.input ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseDirectRawPredictResponse(): DirectRawPredictResponse {
  return { output: Buffer.alloc(0) };
}

export const DirectRawPredictResponse: MessageFns<DirectRawPredictResponse> = {
  encode(message: DirectRawPredictResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.output.length !== 0) {
      writer.uint32(10).bytes(message.output);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DirectRawPredictResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDirectRawPredictResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.output = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DirectRawPredictResponse {
    return { output: isSet(object.output) ? Buffer.from(bytesFromBase64(object.output)) : Buffer.alloc(0) };
  },

  toJSON(message: DirectRawPredictResponse): unknown {
    const obj: any = {};
    if (message.output.length !== 0) {
      obj.output = base64FromBytes(message.output);
    }
    return obj;
  },

  create(base?: DeepPartial<DirectRawPredictResponse>): DirectRawPredictResponse {
    return DirectRawPredictResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DirectRawPredictResponse>): DirectRawPredictResponse {
    const message = createBaseDirectRawPredictResponse();
    message.output = object.output ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseStreamDirectPredictRequest(): StreamDirectPredictRequest {
  return { endpoint: "", inputs: [], parameters: undefined };
}

export const StreamDirectPredictRequest: MessageFns<StreamDirectPredictRequest> = {
  encode(message: StreamDirectPredictRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.endpoint !== "") {
      writer.uint32(10).string(message.endpoint);
    }
    for (const v of message.inputs) {
      Tensor.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.parameters !== undefined) {
      Tensor.encode(message.parameters, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamDirectPredictRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamDirectPredictRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.endpoint = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inputs.push(Tensor.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.parameters = Tensor.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamDirectPredictRequest {
    return {
      endpoint: isSet(object.endpoint) ? globalThis.String(object.endpoint) : "",
      inputs: globalThis.Array.isArray(object?.inputs) ? object.inputs.map((e: any) => Tensor.fromJSON(e)) : [],
      parameters: isSet(object.parameters) ? Tensor.fromJSON(object.parameters) : undefined,
    };
  },

  toJSON(message: StreamDirectPredictRequest): unknown {
    const obj: any = {};
    if (message.endpoint !== "") {
      obj.endpoint = message.endpoint;
    }
    if (message.inputs?.length) {
      obj.inputs = message.inputs.map((e) => Tensor.toJSON(e));
    }
    if (message.parameters !== undefined) {
      obj.parameters = Tensor.toJSON(message.parameters);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamDirectPredictRequest>): StreamDirectPredictRequest {
    return StreamDirectPredictRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamDirectPredictRequest>): StreamDirectPredictRequest {
    const message = createBaseStreamDirectPredictRequest();
    message.endpoint = object.endpoint ?? "";
    message.inputs = object.inputs?.map((e) => Tensor.fromPartial(e)) || [];
    message.parameters = (object.parameters !== undefined && object.parameters !== null)
      ? Tensor.fromPartial(object.parameters)
      : undefined;
    return message;
  },
};

function createBaseStreamDirectPredictResponse(): StreamDirectPredictResponse {
  return { outputs: [], parameters: undefined };
}

export const StreamDirectPredictResponse: MessageFns<StreamDirectPredictResponse> = {
  encode(message: StreamDirectPredictResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.outputs) {
      Tensor.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.parameters !== undefined) {
      Tensor.encode(message.parameters, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamDirectPredictResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamDirectPredictResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.outputs.push(Tensor.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.parameters = Tensor.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamDirectPredictResponse {
    return {
      outputs: globalThis.Array.isArray(object?.outputs) ? object.outputs.map((e: any) => Tensor.fromJSON(e)) : [],
      parameters: isSet(object.parameters) ? Tensor.fromJSON(object.parameters) : undefined,
    };
  },

  toJSON(message: StreamDirectPredictResponse): unknown {
    const obj: any = {};
    if (message.outputs?.length) {
      obj.outputs = message.outputs.map((e) => Tensor.toJSON(e));
    }
    if (message.parameters !== undefined) {
      obj.parameters = Tensor.toJSON(message.parameters);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamDirectPredictResponse>): StreamDirectPredictResponse {
    return StreamDirectPredictResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamDirectPredictResponse>): StreamDirectPredictResponse {
    const message = createBaseStreamDirectPredictResponse();
    message.outputs = object.outputs?.map((e) => Tensor.fromPartial(e)) || [];
    message.parameters = (object.parameters !== undefined && object.parameters !== null)
      ? Tensor.fromPartial(object.parameters)
      : undefined;
    return message;
  },
};

function createBaseStreamDirectRawPredictRequest(): StreamDirectRawPredictRequest {
  return { endpoint: "", methodName: "", input: Buffer.alloc(0) };
}

export const StreamDirectRawPredictRequest: MessageFns<StreamDirectRawPredictRequest> = {
  encode(message: StreamDirectRawPredictRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.endpoint !== "") {
      writer.uint32(10).string(message.endpoint);
    }
    if (message.methodName !== "") {
      writer.uint32(18).string(message.methodName);
    }
    if (message.input.length !== 0) {
      writer.uint32(26).bytes(message.input);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamDirectRawPredictRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamDirectRawPredictRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.endpoint = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.methodName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.input = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamDirectRawPredictRequest {
    return {
      endpoint: isSet(object.endpoint) ? globalThis.String(object.endpoint) : "",
      methodName: isSet(object.methodName) ? globalThis.String(object.methodName) : "",
      input: isSet(object.input) ? Buffer.from(bytesFromBase64(object.input)) : Buffer.alloc(0),
    };
  },

  toJSON(message: StreamDirectRawPredictRequest): unknown {
    const obj: any = {};
    if (message.endpoint !== "") {
      obj.endpoint = message.endpoint;
    }
    if (message.methodName !== "") {
      obj.methodName = message.methodName;
    }
    if (message.input.length !== 0) {
      obj.input = base64FromBytes(message.input);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamDirectRawPredictRequest>): StreamDirectRawPredictRequest {
    return StreamDirectRawPredictRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamDirectRawPredictRequest>): StreamDirectRawPredictRequest {
    const message = createBaseStreamDirectRawPredictRequest();
    message.endpoint = object.endpoint ?? "";
    message.methodName = object.methodName ?? "";
    message.input = object.input ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseStreamDirectRawPredictResponse(): StreamDirectRawPredictResponse {
  return { output: Buffer.alloc(0) };
}

export const StreamDirectRawPredictResponse: MessageFns<StreamDirectRawPredictResponse> = {
  encode(message: StreamDirectRawPredictResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.output.length !== 0) {
      writer.uint32(10).bytes(message.output);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamDirectRawPredictResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamDirectRawPredictResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.output = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamDirectRawPredictResponse {
    return { output: isSet(object.output) ? Buffer.from(bytesFromBase64(object.output)) : Buffer.alloc(0) };
  },

  toJSON(message: StreamDirectRawPredictResponse): unknown {
    const obj: any = {};
    if (message.output.length !== 0) {
      obj.output = base64FromBytes(message.output);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamDirectRawPredictResponse>): StreamDirectRawPredictResponse {
    return StreamDirectRawPredictResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamDirectRawPredictResponse>): StreamDirectRawPredictResponse {
    const message = createBaseStreamDirectRawPredictResponse();
    message.output = object.output ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseStreamingPredictRequest(): StreamingPredictRequest {
  return { endpoint: "", inputs: [], parameters: undefined };
}

export const StreamingPredictRequest: MessageFns<StreamingPredictRequest> = {
  encode(message: StreamingPredictRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.endpoint !== "") {
      writer.uint32(10).string(message.endpoint);
    }
    for (const v of message.inputs) {
      Tensor.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.parameters !== undefined) {
      Tensor.encode(message.parameters, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingPredictRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingPredictRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.endpoint = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inputs.push(Tensor.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.parameters = Tensor.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingPredictRequest {
    return {
      endpoint: isSet(object.endpoint) ? globalThis.String(object.endpoint) : "",
      inputs: globalThis.Array.isArray(object?.inputs) ? object.inputs.map((e: any) => Tensor.fromJSON(e)) : [],
      parameters: isSet(object.parameters) ? Tensor.fromJSON(object.parameters) : undefined,
    };
  },

  toJSON(message: StreamingPredictRequest): unknown {
    const obj: any = {};
    if (message.endpoint !== "") {
      obj.endpoint = message.endpoint;
    }
    if (message.inputs?.length) {
      obj.inputs = message.inputs.map((e) => Tensor.toJSON(e));
    }
    if (message.parameters !== undefined) {
      obj.parameters = Tensor.toJSON(message.parameters);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamingPredictRequest>): StreamingPredictRequest {
    return StreamingPredictRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamingPredictRequest>): StreamingPredictRequest {
    const message = createBaseStreamingPredictRequest();
    message.endpoint = object.endpoint ?? "";
    message.inputs = object.inputs?.map((e) => Tensor.fromPartial(e)) || [];
    message.parameters = (object.parameters !== undefined && object.parameters !== null)
      ? Tensor.fromPartial(object.parameters)
      : undefined;
    return message;
  },
};

function createBaseStreamingPredictResponse(): StreamingPredictResponse {
  return { outputs: [], parameters: undefined };
}

export const StreamingPredictResponse: MessageFns<StreamingPredictResponse> = {
  encode(message: StreamingPredictResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.outputs) {
      Tensor.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.parameters !== undefined) {
      Tensor.encode(message.parameters, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingPredictResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingPredictResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.outputs.push(Tensor.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.parameters = Tensor.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingPredictResponse {
    return {
      outputs: globalThis.Array.isArray(object?.outputs) ? object.outputs.map((e: any) => Tensor.fromJSON(e)) : [],
      parameters: isSet(object.parameters) ? Tensor.fromJSON(object.parameters) : undefined,
    };
  },

  toJSON(message: StreamingPredictResponse): unknown {
    const obj: any = {};
    if (message.outputs?.length) {
      obj.outputs = message.outputs.map((e) => Tensor.toJSON(e));
    }
    if (message.parameters !== undefined) {
      obj.parameters = Tensor.toJSON(message.parameters);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamingPredictResponse>): StreamingPredictResponse {
    return StreamingPredictResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamingPredictResponse>): StreamingPredictResponse {
    const message = createBaseStreamingPredictResponse();
    message.outputs = object.outputs?.map((e) => Tensor.fromPartial(e)) || [];
    message.parameters = (object.parameters !== undefined && object.parameters !== null)
      ? Tensor.fromPartial(object.parameters)
      : undefined;
    return message;
  },
};

function createBaseStreamingRawPredictRequest(): StreamingRawPredictRequest {
  return { endpoint: "", methodName: "", input: Buffer.alloc(0) };
}

export const StreamingRawPredictRequest: MessageFns<StreamingRawPredictRequest> = {
  encode(message: StreamingRawPredictRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.endpoint !== "") {
      writer.uint32(10).string(message.endpoint);
    }
    if (message.methodName !== "") {
      writer.uint32(18).string(message.methodName);
    }
    if (message.input.length !== 0) {
      writer.uint32(26).bytes(message.input);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingRawPredictRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingRawPredictRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.endpoint = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.methodName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.input = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingRawPredictRequest {
    return {
      endpoint: isSet(object.endpoint) ? globalThis.String(object.endpoint) : "",
      methodName: isSet(object.methodName) ? globalThis.String(object.methodName) : "",
      input: isSet(object.input) ? Buffer.from(bytesFromBase64(object.input)) : Buffer.alloc(0),
    };
  },

  toJSON(message: StreamingRawPredictRequest): unknown {
    const obj: any = {};
    if (message.endpoint !== "") {
      obj.endpoint = message.endpoint;
    }
    if (message.methodName !== "") {
      obj.methodName = message.methodName;
    }
    if (message.input.length !== 0) {
      obj.input = base64FromBytes(message.input);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamingRawPredictRequest>): StreamingRawPredictRequest {
    return StreamingRawPredictRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamingRawPredictRequest>): StreamingRawPredictRequest {
    const message = createBaseStreamingRawPredictRequest();
    message.endpoint = object.endpoint ?? "";
    message.methodName = object.methodName ?? "";
    message.input = object.input ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseStreamingRawPredictResponse(): StreamingRawPredictResponse {
  return { output: Buffer.alloc(0) };
}

export const StreamingRawPredictResponse: MessageFns<StreamingRawPredictResponse> = {
  encode(message: StreamingRawPredictResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.output.length !== 0) {
      writer.uint32(10).bytes(message.output);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingRawPredictResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingRawPredictResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.output = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingRawPredictResponse {
    return { output: isSet(object.output) ? Buffer.from(bytesFromBase64(object.output)) : Buffer.alloc(0) };
  },

  toJSON(message: StreamingRawPredictResponse): unknown {
    const obj: any = {};
    if (message.output.length !== 0) {
      obj.output = base64FromBytes(message.output);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamingRawPredictResponse>): StreamingRawPredictResponse {
    return StreamingRawPredictResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamingRawPredictResponse>): StreamingRawPredictResponse {
    const message = createBaseStreamingRawPredictResponse();
    message.output = object.output ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseExplainRequest(): ExplainRequest {
  return {
    endpoint: "",
    instances: [],
    parameters: undefined,
    explanationSpecOverride: undefined,
    deployedModelId: "",
  };
}

export const ExplainRequest: MessageFns<ExplainRequest> = {
  encode(message: ExplainRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.endpoint !== "") {
      writer.uint32(10).string(message.endpoint);
    }
    for (const v of message.instances) {
      Value.encode(Value.wrap(v!), writer.uint32(18).fork()).join();
    }
    if (message.parameters !== undefined) {
      Value.encode(Value.wrap(message.parameters), writer.uint32(34).fork()).join();
    }
    if (message.explanationSpecOverride !== undefined) {
      ExplanationSpecOverride.encode(message.explanationSpecOverride, writer.uint32(42).fork()).join();
    }
    if (message.deployedModelId !== "") {
      writer.uint32(26).string(message.deployedModelId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplainRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplainRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.endpoint = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.instances.push(Value.unwrap(Value.decode(reader, reader.uint32())));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.parameters = Value.unwrap(Value.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.explanationSpecOverride = ExplanationSpecOverride.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.deployedModelId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplainRequest {
    return {
      endpoint: isSet(object.endpoint) ? globalThis.String(object.endpoint) : "",
      instances: globalThis.Array.isArray(object?.instances) ? [...object.instances] : [],
      parameters: isSet(object?.parameters) ? object.parameters : undefined,
      explanationSpecOverride: isSet(object.explanationSpecOverride)
        ? ExplanationSpecOverride.fromJSON(object.explanationSpecOverride)
        : undefined,
      deployedModelId: isSet(object.deployedModelId) ? globalThis.String(object.deployedModelId) : "",
    };
  },

  toJSON(message: ExplainRequest): unknown {
    const obj: any = {};
    if (message.endpoint !== "") {
      obj.endpoint = message.endpoint;
    }
    if (message.instances?.length) {
      obj.instances = message.instances;
    }
    if (message.parameters !== undefined) {
      obj.parameters = message.parameters;
    }
    if (message.explanationSpecOverride !== undefined) {
      obj.explanationSpecOverride = ExplanationSpecOverride.toJSON(message.explanationSpecOverride);
    }
    if (message.deployedModelId !== "") {
      obj.deployedModelId = message.deployedModelId;
    }
    return obj;
  },

  create(base?: DeepPartial<ExplainRequest>): ExplainRequest {
    return ExplainRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplainRequest>): ExplainRequest {
    const message = createBaseExplainRequest();
    message.endpoint = object.endpoint ?? "";
    message.instances = object.instances?.map((e) => e) || [];
    message.parameters = object.parameters ?? undefined;
    message.explanationSpecOverride =
      (object.explanationSpecOverride !== undefined && object.explanationSpecOverride !== null)
        ? ExplanationSpecOverride.fromPartial(object.explanationSpecOverride)
        : undefined;
    message.deployedModelId = object.deployedModelId ?? "";
    return message;
  },
};

function createBaseExplainResponse(): ExplainResponse {
  return { explanations: [], deployedModelId: "", predictions: [] };
}

export const ExplainResponse: MessageFns<ExplainResponse> = {
  encode(message: ExplainResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.explanations) {
      Explanation.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.deployedModelId !== "") {
      writer.uint32(18).string(message.deployedModelId);
    }
    for (const v of message.predictions) {
      Value.encode(Value.wrap(v!), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplainResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplainResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.explanations.push(Explanation.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.deployedModelId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.predictions.push(Value.unwrap(Value.decode(reader, reader.uint32())));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplainResponse {
    return {
      explanations: globalThis.Array.isArray(object?.explanations)
        ? object.explanations.map((e: any) => Explanation.fromJSON(e))
        : [],
      deployedModelId: isSet(object.deployedModelId) ? globalThis.String(object.deployedModelId) : "",
      predictions: globalThis.Array.isArray(object?.predictions) ? [...object.predictions] : [],
    };
  },

  toJSON(message: ExplainResponse): unknown {
    const obj: any = {};
    if (message.explanations?.length) {
      obj.explanations = message.explanations.map((e) => Explanation.toJSON(e));
    }
    if (message.deployedModelId !== "") {
      obj.deployedModelId = message.deployedModelId;
    }
    if (message.predictions?.length) {
      obj.predictions = message.predictions;
    }
    return obj;
  },

  create(base?: DeepPartial<ExplainResponse>): ExplainResponse {
    return ExplainResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplainResponse>): ExplainResponse {
    const message = createBaseExplainResponse();
    message.explanations = object.explanations?.map((e) => Explanation.fromPartial(e)) || [];
    message.deployedModelId = object.deployedModelId ?? "";
    message.predictions = object.predictions?.map((e) => e) || [];
    return message;
  },
};

function createBaseCountTokensRequest(): CountTokensRequest {
  return {
    endpoint: "",
    model: "",
    instances: [],
    contents: [],
    systemInstruction: undefined,
    tools: [],
    generationConfig: undefined,
  };
}

export const CountTokensRequest: MessageFns<CountTokensRequest> = {
  encode(message: CountTokensRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.endpoint !== "") {
      writer.uint32(10).string(message.endpoint);
    }
    if (message.model !== "") {
      writer.uint32(26).string(message.model);
    }
    for (const v of message.instances) {
      Value.encode(Value.wrap(v!), writer.uint32(18).fork()).join();
    }
    for (const v of message.contents) {
      Content.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.systemInstruction !== undefined) {
      Content.encode(message.systemInstruction, writer.uint32(42).fork()).join();
    }
    for (const v of message.tools) {
      Tool.encode(v!, writer.uint32(50).fork()).join();
    }
    if (message.generationConfig !== undefined) {
      GenerationConfig.encode(message.generationConfig, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CountTokensRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCountTokensRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.endpoint = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.model = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.instances.push(Value.unwrap(Value.decode(reader, reader.uint32())));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.contents.push(Content.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.systemInstruction = Content.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.tools.push(Tool.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.generationConfig = GenerationConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CountTokensRequest {
    return {
      endpoint: isSet(object.endpoint) ? globalThis.String(object.endpoint) : "",
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      instances: globalThis.Array.isArray(object?.instances) ? [...object.instances] : [],
      contents: globalThis.Array.isArray(object?.contents) ? object.contents.map((e: any) => Content.fromJSON(e)) : [],
      systemInstruction: isSet(object.systemInstruction) ? Content.fromJSON(object.systemInstruction) : undefined,
      tools: globalThis.Array.isArray(object?.tools) ? object.tools.map((e: any) => Tool.fromJSON(e)) : [],
      generationConfig: isSet(object.generationConfig) ? GenerationConfig.fromJSON(object.generationConfig) : undefined,
    };
  },

  toJSON(message: CountTokensRequest): unknown {
    const obj: any = {};
    if (message.endpoint !== "") {
      obj.endpoint = message.endpoint;
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.instances?.length) {
      obj.instances = message.instances;
    }
    if (message.contents?.length) {
      obj.contents = message.contents.map((e) => Content.toJSON(e));
    }
    if (message.systemInstruction !== undefined) {
      obj.systemInstruction = Content.toJSON(message.systemInstruction);
    }
    if (message.tools?.length) {
      obj.tools = message.tools.map((e) => Tool.toJSON(e));
    }
    if (message.generationConfig !== undefined) {
      obj.generationConfig = GenerationConfig.toJSON(message.generationConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<CountTokensRequest>): CountTokensRequest {
    return CountTokensRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CountTokensRequest>): CountTokensRequest {
    const message = createBaseCountTokensRequest();
    message.endpoint = object.endpoint ?? "";
    message.model = object.model ?? "";
    message.instances = object.instances?.map((e) => e) || [];
    message.contents = object.contents?.map((e) => Content.fromPartial(e)) || [];
    message.systemInstruction = (object.systemInstruction !== undefined && object.systemInstruction !== null)
      ? Content.fromPartial(object.systemInstruction)
      : undefined;
    message.tools = object.tools?.map((e) => Tool.fromPartial(e)) || [];
    message.generationConfig = (object.generationConfig !== undefined && object.generationConfig !== null)
      ? GenerationConfig.fromPartial(object.generationConfig)
      : undefined;
    return message;
  },
};

function createBaseCountTokensResponse(): CountTokensResponse {
  return { totalTokens: 0, totalBillableCharacters: 0 };
}

export const CountTokensResponse: MessageFns<CountTokensResponse> = {
  encode(message: CountTokensResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.totalTokens !== 0) {
      writer.uint32(8).int32(message.totalTokens);
    }
    if (message.totalBillableCharacters !== 0) {
      writer.uint32(16).int32(message.totalBillableCharacters);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CountTokensResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCountTokensResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.totalTokens = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.totalBillableCharacters = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CountTokensResponse {
    return {
      totalTokens: isSet(object.totalTokens) ? globalThis.Number(object.totalTokens) : 0,
      totalBillableCharacters: isSet(object.totalBillableCharacters)
        ? globalThis.Number(object.totalBillableCharacters)
        : 0,
    };
  },

  toJSON(message: CountTokensResponse): unknown {
    const obj: any = {};
    if (message.totalTokens !== 0) {
      obj.totalTokens = Math.round(message.totalTokens);
    }
    if (message.totalBillableCharacters !== 0) {
      obj.totalBillableCharacters = Math.round(message.totalBillableCharacters);
    }
    return obj;
  },

  create(base?: DeepPartial<CountTokensResponse>): CountTokensResponse {
    return CountTokensResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CountTokensResponse>): CountTokensResponse {
    const message = createBaseCountTokensResponse();
    message.totalTokens = object.totalTokens ?? 0;
    message.totalBillableCharacters = object.totalBillableCharacters ?? 0;
    return message;
  },
};

function createBaseGenerateContentRequest(): GenerateContentRequest {
  return {
    model: "",
    contents: [],
    systemInstruction: undefined,
    tools: [],
    toolConfig: undefined,
    labels: {},
    safetySettings: [],
    generationConfig: undefined,
  };
}

export const GenerateContentRequest: MessageFns<GenerateContentRequest> = {
  encode(message: GenerateContentRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(42).string(message.model);
    }
    for (const v of message.contents) {
      Content.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.systemInstruction !== undefined) {
      Content.encode(message.systemInstruction, writer.uint32(66).fork()).join();
    }
    for (const v of message.tools) {
      Tool.encode(v!, writer.uint32(50).fork()).join();
    }
    if (message.toolConfig !== undefined) {
      ToolConfig.encode(message.toolConfig, writer.uint32(58).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      GenerateContentRequest_LabelsEntry.encode({ key: key as any, value }, writer.uint32(82).fork()).join();
    });
    for (const v of message.safetySettings) {
      SafetySetting.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.generationConfig !== undefined) {
      GenerationConfig.encode(message.generationConfig, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerateContentRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerateContentRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 5:
          if (tag !== 42) {
            break;
          }

          message.model = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.contents.push(Content.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.systemInstruction = Content.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.tools.push(Tool.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.toolConfig = ToolConfig.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          const entry10 = GenerateContentRequest_LabelsEntry.decode(reader, reader.uint32());
          if (entry10.value !== undefined) {
            message.labels[entry10.key] = entry10.value;
          }
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.safetySettings.push(SafetySetting.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.generationConfig = GenerationConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerateContentRequest {
    return {
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      contents: globalThis.Array.isArray(object?.contents) ? object.contents.map((e: any) => Content.fromJSON(e)) : [],
      systemInstruction: isSet(object.systemInstruction) ? Content.fromJSON(object.systemInstruction) : undefined,
      tools: globalThis.Array.isArray(object?.tools) ? object.tools.map((e: any) => Tool.fromJSON(e)) : [],
      toolConfig: isSet(object.toolConfig) ? ToolConfig.fromJSON(object.toolConfig) : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      safetySettings: globalThis.Array.isArray(object?.safetySettings)
        ? object.safetySettings.map((e: any) => SafetySetting.fromJSON(e))
        : [],
      generationConfig: isSet(object.generationConfig) ? GenerationConfig.fromJSON(object.generationConfig) : undefined,
    };
  },

  toJSON(message: GenerateContentRequest): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.contents?.length) {
      obj.contents = message.contents.map((e) => Content.toJSON(e));
    }
    if (message.systemInstruction !== undefined) {
      obj.systemInstruction = Content.toJSON(message.systemInstruction);
    }
    if (message.tools?.length) {
      obj.tools = message.tools.map((e) => Tool.toJSON(e));
    }
    if (message.toolConfig !== undefined) {
      obj.toolConfig = ToolConfig.toJSON(message.toolConfig);
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.safetySettings?.length) {
      obj.safetySettings = message.safetySettings.map((e) => SafetySetting.toJSON(e));
    }
    if (message.generationConfig !== undefined) {
      obj.generationConfig = GenerationConfig.toJSON(message.generationConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<GenerateContentRequest>): GenerateContentRequest {
    return GenerateContentRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerateContentRequest>): GenerateContentRequest {
    const message = createBaseGenerateContentRequest();
    message.model = object.model ?? "";
    message.contents = object.contents?.map((e) => Content.fromPartial(e)) || [];
    message.systemInstruction = (object.systemInstruction !== undefined && object.systemInstruction !== null)
      ? Content.fromPartial(object.systemInstruction)
      : undefined;
    message.tools = object.tools?.map((e) => Tool.fromPartial(e)) || [];
    message.toolConfig = (object.toolConfig !== undefined && object.toolConfig !== null)
      ? ToolConfig.fromPartial(object.toolConfig)
      : undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.safetySettings = object.safetySettings?.map((e) => SafetySetting.fromPartial(e)) || [];
    message.generationConfig = (object.generationConfig !== undefined && object.generationConfig !== null)
      ? GenerationConfig.fromPartial(object.generationConfig)
      : undefined;
    return message;
  },
};

function createBaseGenerateContentRequest_LabelsEntry(): GenerateContentRequest_LabelsEntry {
  return { key: "", value: "" };
}

export const GenerateContentRequest_LabelsEntry: MessageFns<GenerateContentRequest_LabelsEntry> = {
  encode(message: GenerateContentRequest_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerateContentRequest_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerateContentRequest_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerateContentRequest_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: GenerateContentRequest_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<GenerateContentRequest_LabelsEntry>): GenerateContentRequest_LabelsEntry {
    return GenerateContentRequest_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerateContentRequest_LabelsEntry>): GenerateContentRequest_LabelsEntry {
    const message = createBaseGenerateContentRequest_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseGenerateContentResponse(): GenerateContentResponse {
  return { candidates: [], modelVersion: "", promptFeedback: undefined, usageMetadata: undefined };
}

export const GenerateContentResponse: MessageFns<GenerateContentResponse> = {
  encode(message: GenerateContentResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.candidates) {
      Candidate.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.modelVersion !== "") {
      writer.uint32(90).string(message.modelVersion);
    }
    if (message.promptFeedback !== undefined) {
      GenerateContentResponse_PromptFeedback.encode(message.promptFeedback, writer.uint32(26).fork()).join();
    }
    if (message.usageMetadata !== undefined) {
      GenerateContentResponse_UsageMetadata.encode(message.usageMetadata, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerateContentResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerateContentResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.candidates.push(Candidate.decode(reader, reader.uint32()));
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.modelVersion = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.promptFeedback = GenerateContentResponse_PromptFeedback.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.usageMetadata = GenerateContentResponse_UsageMetadata.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerateContentResponse {
    return {
      candidates: globalThis.Array.isArray(object?.candidates)
        ? object.candidates.map((e: any) => Candidate.fromJSON(e))
        : [],
      modelVersion: isSet(object.modelVersion) ? globalThis.String(object.modelVersion) : "",
      promptFeedback: isSet(object.promptFeedback)
        ? GenerateContentResponse_PromptFeedback.fromJSON(object.promptFeedback)
        : undefined,
      usageMetadata: isSet(object.usageMetadata)
        ? GenerateContentResponse_UsageMetadata.fromJSON(object.usageMetadata)
        : undefined,
    };
  },

  toJSON(message: GenerateContentResponse): unknown {
    const obj: any = {};
    if (message.candidates?.length) {
      obj.candidates = message.candidates.map((e) => Candidate.toJSON(e));
    }
    if (message.modelVersion !== "") {
      obj.modelVersion = message.modelVersion;
    }
    if (message.promptFeedback !== undefined) {
      obj.promptFeedback = GenerateContentResponse_PromptFeedback.toJSON(message.promptFeedback);
    }
    if (message.usageMetadata !== undefined) {
      obj.usageMetadata = GenerateContentResponse_UsageMetadata.toJSON(message.usageMetadata);
    }
    return obj;
  },

  create(base?: DeepPartial<GenerateContentResponse>): GenerateContentResponse {
    return GenerateContentResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerateContentResponse>): GenerateContentResponse {
    const message = createBaseGenerateContentResponse();
    message.candidates = object.candidates?.map((e) => Candidate.fromPartial(e)) || [];
    message.modelVersion = object.modelVersion ?? "";
    message.promptFeedback = (object.promptFeedback !== undefined && object.promptFeedback !== null)
      ? GenerateContentResponse_PromptFeedback.fromPartial(object.promptFeedback)
      : undefined;
    message.usageMetadata = (object.usageMetadata !== undefined && object.usageMetadata !== null)
      ? GenerateContentResponse_UsageMetadata.fromPartial(object.usageMetadata)
      : undefined;
    return message;
  },
};

function createBaseGenerateContentResponse_PromptFeedback(): GenerateContentResponse_PromptFeedback {
  return { blockReason: 0, safetyRatings: [], blockReasonMessage: "" };
}

export const GenerateContentResponse_PromptFeedback: MessageFns<GenerateContentResponse_PromptFeedback> = {
  encode(message: GenerateContentResponse_PromptFeedback, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.blockReason !== 0) {
      writer.uint32(8).int32(message.blockReason);
    }
    for (const v of message.safetyRatings) {
      SafetyRating.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.blockReasonMessage !== "") {
      writer.uint32(26).string(message.blockReasonMessage);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerateContentResponse_PromptFeedback {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerateContentResponse_PromptFeedback();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.blockReason = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.safetyRatings.push(SafetyRating.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.blockReasonMessage = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerateContentResponse_PromptFeedback {
    return {
      blockReason: isSet(object.blockReason)
        ? generateContentResponse_PromptFeedback_BlockedReasonFromJSON(object.blockReason)
        : 0,
      safetyRatings: globalThis.Array.isArray(object?.safetyRatings)
        ? object.safetyRatings.map((e: any) => SafetyRating.fromJSON(e))
        : [],
      blockReasonMessage: isSet(object.blockReasonMessage) ? globalThis.String(object.blockReasonMessage) : "",
    };
  },

  toJSON(message: GenerateContentResponse_PromptFeedback): unknown {
    const obj: any = {};
    if (message.blockReason !== 0) {
      obj.blockReason = generateContentResponse_PromptFeedback_BlockedReasonToJSON(message.blockReason);
    }
    if (message.safetyRatings?.length) {
      obj.safetyRatings = message.safetyRatings.map((e) => SafetyRating.toJSON(e));
    }
    if (message.blockReasonMessage !== "") {
      obj.blockReasonMessage = message.blockReasonMessage;
    }
    return obj;
  },

  create(base?: DeepPartial<GenerateContentResponse_PromptFeedback>): GenerateContentResponse_PromptFeedback {
    return GenerateContentResponse_PromptFeedback.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerateContentResponse_PromptFeedback>): GenerateContentResponse_PromptFeedback {
    const message = createBaseGenerateContentResponse_PromptFeedback();
    message.blockReason = object.blockReason ?? 0;
    message.safetyRatings = object.safetyRatings?.map((e) => SafetyRating.fromPartial(e)) || [];
    message.blockReasonMessage = object.blockReasonMessage ?? "";
    return message;
  },
};

function createBaseGenerateContentResponse_UsageMetadata(): GenerateContentResponse_UsageMetadata {
  return { promptTokenCount: 0, candidatesTokenCount: 0, totalTokenCount: 0 };
}

export const GenerateContentResponse_UsageMetadata: MessageFns<GenerateContentResponse_UsageMetadata> = {
  encode(message: GenerateContentResponse_UsageMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.promptTokenCount !== 0) {
      writer.uint32(8).int32(message.promptTokenCount);
    }
    if (message.candidatesTokenCount !== 0) {
      writer.uint32(16).int32(message.candidatesTokenCount);
    }
    if (message.totalTokenCount !== 0) {
      writer.uint32(24).int32(message.totalTokenCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerateContentResponse_UsageMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerateContentResponse_UsageMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.promptTokenCount = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.candidatesTokenCount = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.totalTokenCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerateContentResponse_UsageMetadata {
    return {
      promptTokenCount: isSet(object.promptTokenCount) ? globalThis.Number(object.promptTokenCount) : 0,
      candidatesTokenCount: isSet(object.candidatesTokenCount) ? globalThis.Number(object.candidatesTokenCount) : 0,
      totalTokenCount: isSet(object.totalTokenCount) ? globalThis.Number(object.totalTokenCount) : 0,
    };
  },

  toJSON(message: GenerateContentResponse_UsageMetadata): unknown {
    const obj: any = {};
    if (message.promptTokenCount !== 0) {
      obj.promptTokenCount = Math.round(message.promptTokenCount);
    }
    if (message.candidatesTokenCount !== 0) {
      obj.candidatesTokenCount = Math.round(message.candidatesTokenCount);
    }
    if (message.totalTokenCount !== 0) {
      obj.totalTokenCount = Math.round(message.totalTokenCount);
    }
    return obj;
  },

  create(base?: DeepPartial<GenerateContentResponse_UsageMetadata>): GenerateContentResponse_UsageMetadata {
    return GenerateContentResponse_UsageMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerateContentResponse_UsageMetadata>): GenerateContentResponse_UsageMetadata {
    const message = createBaseGenerateContentResponse_UsageMetadata();
    message.promptTokenCount = object.promptTokenCount ?? 0;
    message.candidatesTokenCount = object.candidatesTokenCount ?? 0;
    message.totalTokenCount = object.totalTokenCount ?? 0;
    return message;
  },
};

/** A service for online predictions and explanations. */
export type PredictionServiceDefinition = typeof PredictionServiceDefinition;
export const PredictionServiceDefinition = {
  name: "PredictionService",
  fullName: "google.cloud.aiplatform.v1.PredictionService",
  methods: {
    /** Perform an online prediction. */
    predict: {
      name: "Predict",
      requestType: PredictRequest,
      requestStream: false,
      responseType: PredictResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              29,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              44,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              44,
              112,
              97,
              114,
              97,
              109,
              101,
              116,
              101,
              114,
              115,
            ]),
          ],
          578365826: [
            Buffer.from([
              136,
              1,
              58,
              1,
              42,
              90,
              72,
              58,
              1,
              42,
              34,
              67,
              47,
              118,
              49,
              47,
              123,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              112,
              117,
              98,
              108,
              105,
              115,
              104,
              101,
              114,
              115,
              47,
              42,
              47,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              112,
              114,
              101,
              100,
              105,
              99,
              116,
              34,
              57,
              47,
              118,
              49,
              47,
              123,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              115,
              47,
              42,
              125,
              58,
              112,
              114,
              101,
              100,
              105,
              99,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * Perform an online prediction with an arbitrary HTTP payload.
     *
     * The response includes the following HTTP headers:
     *
     * * `X-Vertex-AI-Endpoint-Id`: ID of the
     * [Endpoint][google.cloud.aiplatform.v1.Endpoint] that served this
     * prediction.
     *
     * * `X-Vertex-AI-Deployed-Model-Id`: ID of the Endpoint's
     * [DeployedModel][google.cloud.aiplatform.v1.DeployedModel] that served this
     * prediction.
     */
    rawPredict: {
      name: "RawPredict",
      requestType: RawPredictRequest,
      requestStream: false,
      responseType: HttpBody,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([18, 101, 110, 100, 112, 111, 105, 110, 116, 44, 104, 116, 116, 112, 95, 98, 111, 100, 121]),
          ],
          578365826: [
            Buffer.from([
              142,
              1,
              58,
              1,
              42,
              90,
              75,
              58,
              1,
              42,
              34,
              70,
              47,
              118,
              49,
              47,
              123,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              112,
              117,
              98,
              108,
              105,
              115,
              104,
              101,
              114,
              115,
              47,
              42,
              47,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              114,
              97,
              119,
              80,
              114,
              101,
              100,
              105,
              99,
              116,
              34,
              60,
              47,
              118,
              49,
              47,
              123,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              115,
              47,
              42,
              125,
              58,
              114,
              97,
              119,
              80,
              114,
              101,
              100,
              105,
              99,
              116,
            ]),
          ],
        },
      },
    },
    /** Perform a streaming online prediction with an arbitrary HTTP payload. */
    streamRawPredict: {
      name: "StreamRawPredict",
      requestType: StreamRawPredictRequest,
      requestStream: false,
      responseType: HttpBody,
      responseStream: true,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([18, 101, 110, 100, 112, 111, 105, 110, 116, 44, 104, 116, 116, 112, 95, 98, 111, 100, 121]),
          ],
          578365826: [
            Buffer.from([
              154,
              1,
              58,
              1,
              42,
              90,
              81,
              58,
              1,
              42,
              34,
              76,
              47,
              118,
              49,
              47,
              123,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              112,
              117,
              98,
              108,
              105,
              115,
              104,
              101,
              114,
              115,
              47,
              42,
              47,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              115,
              116,
              114,
              101,
              97,
              109,
              82,
              97,
              119,
              80,
              114,
              101,
              100,
              105,
              99,
              116,
              34,
              66,
              47,
              118,
              49,
              47,
              123,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              115,
              47,
              42,
              125,
              58,
              115,
              116,
              114,
              101,
              97,
              109,
              82,
              97,
              119,
              80,
              114,
              101,
              100,
              105,
              99,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * Perform an unary online prediction request to a gRPC model server for
     * Vertex first-party products and frameworks.
     */
    directPredict: {
      name: "DirectPredict",
      requestType: DirectPredictRequest,
      requestStream: false,
      responseType: DirectPredictResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              68,
              58,
              1,
              42,
              34,
              63,
              47,
              118,
              49,
              47,
              123,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              115,
              47,
              42,
              125,
              58,
              100,
              105,
              114,
              101,
              99,
              116,
              80,
              114,
              101,
              100,
              105,
              99,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * Perform an unary online prediction request to a gRPC model server for
     * custom containers.
     */
    directRawPredict: {
      name: "DirectRawPredict",
      requestType: DirectRawPredictRequest,
      requestStream: false,
      responseType: DirectRawPredictResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              71,
              58,
              1,
              42,
              34,
              66,
              47,
              118,
              49,
              47,
              123,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              115,
              47,
              42,
              125,
              58,
              100,
              105,
              114,
              101,
              99,
              116,
              82,
              97,
              119,
              80,
              114,
              101,
              100,
              105,
              99,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * Perform a streaming online prediction request to a gRPC model server for
     * Vertex first-party products and frameworks.
     */
    streamDirectPredict: {
      name: "StreamDirectPredict",
      requestType: StreamDirectPredictRequest,
      requestStream: true,
      responseType: StreamDirectPredictResponse,
      responseStream: true,
      options: {},
    },
    /**
     * Perform a streaming online prediction request to a gRPC model server for
     * custom containers.
     */
    streamDirectRawPredict: {
      name: "StreamDirectRawPredict",
      requestType: StreamDirectRawPredictRequest,
      requestStream: true,
      responseType: StreamDirectRawPredictResponse,
      responseStream: true,
      options: {},
    },
    /**
     * Perform a streaming online prediction request for Vertex first-party
     * products and frameworks.
     */
    streamingPredict: {
      name: "StreamingPredict",
      requestType: StreamingPredictRequest,
      requestStream: true,
      responseType: StreamingPredictResponse,
      responseStream: true,
      options: {},
    },
    /**
     * Perform a server-side streaming online prediction request for Vertex
     * LLM streaming.
     */
    serverStreamingPredict: {
      name: "ServerStreamingPredict",
      requestType: StreamingPredictRequest,
      requestStream: false,
      responseType: StreamingPredictResponse,
      responseStream: true,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              166,
              1,
              58,
              1,
              42,
              90,
              87,
              58,
              1,
              42,
              34,
              82,
              47,
              118,
              49,
              47,
              123,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              112,
              117,
              98,
              108,
              105,
              115,
              104,
              101,
              114,
              115,
              47,
              42,
              47,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              115,
              101,
              114,
              118,
              101,
              114,
              83,
              116,
              114,
              101,
              97,
              109,
              105,
              110,
              103,
              80,
              114,
              101,
              100,
              105,
              99,
              116,
              34,
              72,
              47,
              118,
              49,
              47,
              123,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              115,
              47,
              42,
              125,
              58,
              115,
              101,
              114,
              118,
              101,
              114,
              83,
              116,
              114,
              101,
              97,
              109,
              105,
              110,
              103,
              80,
              114,
              101,
              100,
              105,
              99,
              116,
            ]),
          ],
        },
      },
    },
    /** Perform a streaming online prediction request through gRPC. */
    streamingRawPredict: {
      name: "StreamingRawPredict",
      requestType: StreamingRawPredictRequest,
      requestStream: true,
      responseType: StreamingRawPredictResponse,
      responseStream: true,
      options: {},
    },
    /**
     * Perform an online explanation.
     *
     * If
     * [deployed_model_id][google.cloud.aiplatform.v1.ExplainRequest.deployed_model_id]
     * is specified, the corresponding DeployModel must have
     * [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
     * populated. If
     * [deployed_model_id][google.cloud.aiplatform.v1.ExplainRequest.deployed_model_id]
     * is not specified, all DeployedModels must have
     * [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
     * populated.
     */
    explain: {
      name: "Explain",
      requestType: ExplainRequest,
      requestStream: false,
      responseType: ExplainResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              47,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              44,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              44,
              112,
              97,
              114,
              97,
              109,
              101,
              116,
              101,
              114,
              115,
              44,
              100,
              101,
              112,
              108,
              111,
              121,
              101,
              100,
              95,
              109,
              111,
              100,
              101,
              108,
              95,
              105,
              100,
            ]),
          ],
          578365826: [
            Buffer.from([
              62,
              58,
              1,
              42,
              34,
              57,
              47,
              118,
              49,
              47,
              123,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              115,
              47,
              42,
              125,
              58,
              101,
              120,
              112,
              108,
              97,
              105,
              110,
            ]),
          ],
        },
      },
    },
    /** Generate content with multimodal inputs. */
    generateContent: {
      name: "GenerateContent",
      requestType: GenerateContentRequest,
      requestStream: false,
      responseType: GenerateContentResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([14, 109, 111, 100, 101, 108, 44, 99, 111, 110, 116, 101, 110, 116, 115])],
          578365826: [
            Buffer.from([
              248,
              1,
              58,
              1,
              42,
              90,
              77,
              58,
              1,
              42,
              34,
              72,
              47,
              118,
              49,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              112,
              117,
              98,
              108,
              105,
              115,
              104,
              101,
              114,
              115,
              47,
              42,
              47,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              103,
              101,
              110,
              101,
              114,
              97,
              116,
              101,
              67,
              111,
              110,
              116,
              101,
              110,
              116,
              90,
              44,
              58,
              1,
              42,
              34,
              39,
              47,
              118,
              49,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              115,
              47,
              42,
              125,
              58,
              103,
              101,
              110,
              101,
              114,
              97,
              116,
              101,
              67,
              111,
              110,
              116,
              101,
              110,
              116,
              90,
              54,
              58,
              1,
              42,
              34,
              49,
              47,
              118,
              49,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              112,
              117,
              98,
              108,
              105,
              115,
              104,
              101,
              114,
              115,
              47,
              42,
              47,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              103,
              101,
              110,
              101,
              114,
              97,
              116,
              101,
              67,
              111,
              110,
              116,
              101,
              110,
              116,
              34,
              62,
              47,
              118,
              49,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              115,
              47,
              42,
              125,
              58,
              103,
              101,
              110,
              101,
              114,
              97,
              116,
              101,
              67,
              111,
              110,
              116,
              101,
              110,
              116,
            ]),
          ],
        },
      },
    },
    /** Generate content with multimodal inputs with streaming support. */
    streamGenerateContent: {
      name: "StreamGenerateContent",
      requestType: GenerateContentRequest,
      requestStream: false,
      responseType: GenerateContentResponse,
      responseStream: true,
      options: {
        _unknownFields: {
          8410: [Buffer.from([14, 109, 111, 100, 101, 108, 44, 99, 111, 110, 116, 101, 110, 116, 115])],
          578365826: [
            Buffer.from([
              144,
              2,
              58,
              1,
              42,
              90,
              83,
              58,
              1,
              42,
              34,
              78,
              47,
              118,
              49,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              112,
              117,
              98,
              108,
              105,
              115,
              104,
              101,
              114,
              115,
              47,
              42,
              47,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              115,
              116,
              114,
              101,
              97,
              109,
              71,
              101,
              110,
              101,
              114,
              97,
              116,
              101,
              67,
              111,
              110,
              116,
              101,
              110,
              116,
              90,
              50,
              58,
              1,
              42,
              34,
              45,
              47,
              118,
              49,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              115,
              47,
              42,
              125,
              58,
              115,
              116,
              114,
              101,
              97,
              109,
              71,
              101,
              110,
              101,
              114,
              97,
              116,
              101,
              67,
              111,
              110,
              116,
              101,
              110,
              116,
              90,
              60,
              58,
              1,
              42,
              34,
              55,
              47,
              118,
              49,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              112,
              117,
              98,
              108,
              105,
              115,
              104,
              101,
              114,
              115,
              47,
              42,
              47,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              115,
              116,
              114,
              101,
              97,
              109,
              71,
              101,
              110,
              101,
              114,
              97,
              116,
              101,
              67,
              111,
              110,
              116,
              101,
              110,
              116,
              34,
              68,
              47,
              118,
              49,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              115,
              47,
              42,
              125,
              58,
              115,
              116,
              114,
              101,
              97,
              109,
              71,
              101,
              110,
              101,
              114,
              97,
              116,
              101,
              67,
              111,
              110,
              116,
              101,
              110,
              116,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface PredictionServiceImplementation<CallContextExt = {}> {
  /** Perform an online prediction. */
  predict(request: PredictRequest, context: CallContext & CallContextExt): Promise<DeepPartial<PredictResponse>>;
  /**
   * Perform an online prediction with an arbitrary HTTP payload.
   *
   * The response includes the following HTTP headers:
   *
   * * `X-Vertex-AI-Endpoint-Id`: ID of the
   * [Endpoint][google.cloud.aiplatform.v1.Endpoint] that served this
   * prediction.
   *
   * * `X-Vertex-AI-Deployed-Model-Id`: ID of the Endpoint's
   * [DeployedModel][google.cloud.aiplatform.v1.DeployedModel] that served this
   * prediction.
   */
  rawPredict(request: RawPredictRequest, context: CallContext & CallContextExt): Promise<DeepPartial<HttpBody>>;
  /** Perform a streaming online prediction with an arbitrary HTTP payload. */
  streamRawPredict(
    request: StreamRawPredictRequest,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<HttpBody>>;
  /**
   * Perform an unary online prediction request to a gRPC model server for
   * Vertex first-party products and frameworks.
   */
  directPredict(
    request: DirectPredictRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<DirectPredictResponse>>;
  /**
   * Perform an unary online prediction request to a gRPC model server for
   * custom containers.
   */
  directRawPredict(
    request: DirectRawPredictRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<DirectRawPredictResponse>>;
  /**
   * Perform a streaming online prediction request to a gRPC model server for
   * Vertex first-party products and frameworks.
   */
  streamDirectPredict(
    request: AsyncIterable<StreamDirectPredictRequest>,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<StreamDirectPredictResponse>>;
  /**
   * Perform a streaming online prediction request to a gRPC model server for
   * custom containers.
   */
  streamDirectRawPredict(
    request: AsyncIterable<StreamDirectRawPredictRequest>,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<StreamDirectRawPredictResponse>>;
  /**
   * Perform a streaming online prediction request for Vertex first-party
   * products and frameworks.
   */
  streamingPredict(
    request: AsyncIterable<StreamingPredictRequest>,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<StreamingPredictResponse>>;
  /**
   * Perform a server-side streaming online prediction request for Vertex
   * LLM streaming.
   */
  serverStreamingPredict(
    request: StreamingPredictRequest,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<StreamingPredictResponse>>;
  /** Perform a streaming online prediction request through gRPC. */
  streamingRawPredict(
    request: AsyncIterable<StreamingRawPredictRequest>,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<StreamingRawPredictResponse>>;
  /**
   * Perform an online explanation.
   *
   * If
   * [deployed_model_id][google.cloud.aiplatform.v1.ExplainRequest.deployed_model_id]
   * is specified, the corresponding DeployModel must have
   * [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
   * populated. If
   * [deployed_model_id][google.cloud.aiplatform.v1.ExplainRequest.deployed_model_id]
   * is not specified, all DeployedModels must have
   * [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
   * populated.
   */
  explain(request: ExplainRequest, context: CallContext & CallContextExt): Promise<DeepPartial<ExplainResponse>>;
  /** Generate content with multimodal inputs. */
  generateContent(
    request: GenerateContentRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<GenerateContentResponse>>;
  /** Generate content with multimodal inputs with streaming support. */
  streamGenerateContent(
    request: GenerateContentRequest,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<GenerateContentResponse>>;
}

export interface PredictionServiceClient<CallOptionsExt = {}> {
  /** Perform an online prediction. */
  predict(request: DeepPartial<PredictRequest>, options?: CallOptions & CallOptionsExt): Promise<PredictResponse>;
  /**
   * Perform an online prediction with an arbitrary HTTP payload.
   *
   * The response includes the following HTTP headers:
   *
   * * `X-Vertex-AI-Endpoint-Id`: ID of the
   * [Endpoint][google.cloud.aiplatform.v1.Endpoint] that served this
   * prediction.
   *
   * * `X-Vertex-AI-Deployed-Model-Id`: ID of the Endpoint's
   * [DeployedModel][google.cloud.aiplatform.v1.DeployedModel] that served this
   * prediction.
   */
  rawPredict(request: DeepPartial<RawPredictRequest>, options?: CallOptions & CallOptionsExt): Promise<HttpBody>;
  /** Perform a streaming online prediction with an arbitrary HTTP payload. */
  streamRawPredict(
    request: DeepPartial<StreamRawPredictRequest>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<HttpBody>;
  /**
   * Perform an unary online prediction request to a gRPC model server for
   * Vertex first-party products and frameworks.
   */
  directPredict(
    request: DeepPartial<DirectPredictRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<DirectPredictResponse>;
  /**
   * Perform an unary online prediction request to a gRPC model server for
   * custom containers.
   */
  directRawPredict(
    request: DeepPartial<DirectRawPredictRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<DirectRawPredictResponse>;
  /**
   * Perform a streaming online prediction request to a gRPC model server for
   * Vertex first-party products and frameworks.
   */
  streamDirectPredict(
    request: AsyncIterable<DeepPartial<StreamDirectPredictRequest>>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<StreamDirectPredictResponse>;
  /**
   * Perform a streaming online prediction request to a gRPC model server for
   * custom containers.
   */
  streamDirectRawPredict(
    request: AsyncIterable<DeepPartial<StreamDirectRawPredictRequest>>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<StreamDirectRawPredictResponse>;
  /**
   * Perform a streaming online prediction request for Vertex first-party
   * products and frameworks.
   */
  streamingPredict(
    request: AsyncIterable<DeepPartial<StreamingPredictRequest>>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<StreamingPredictResponse>;
  /**
   * Perform a server-side streaming online prediction request for Vertex
   * LLM streaming.
   */
  serverStreamingPredict(
    request: DeepPartial<StreamingPredictRequest>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<StreamingPredictResponse>;
  /** Perform a streaming online prediction request through gRPC. */
  streamingRawPredict(
    request: AsyncIterable<DeepPartial<StreamingRawPredictRequest>>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<StreamingRawPredictResponse>;
  /**
   * Perform an online explanation.
   *
   * If
   * [deployed_model_id][google.cloud.aiplatform.v1.ExplainRequest.deployed_model_id]
   * is specified, the corresponding DeployModel must have
   * [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
   * populated. If
   * [deployed_model_id][google.cloud.aiplatform.v1.ExplainRequest.deployed_model_id]
   * is not specified, all DeployedModels must have
   * [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
   * populated.
   */
  explain(request: DeepPartial<ExplainRequest>, options?: CallOptions & CallOptionsExt): Promise<ExplainResponse>;
  /** Generate content with multimodal inputs. */
  generateContent(
    request: DeepPartial<GenerateContentRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<GenerateContentResponse>;
  /** Generate content with multimodal inputs with streaming support. */
  streamGenerateContent(
    request: DeepPartial<GenerateContentRequest>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<GenerateContentResponse>;
}

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export type ServerStreamingMethodResult<Response> = { [Symbol.asyncIterator](): AsyncIterator<Response, void> };

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
