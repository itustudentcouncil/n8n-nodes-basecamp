// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/aiplatform/v1/explanation_metadata.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Value } from "../../../protobuf/struct.js";

export const protobufPackage = "google.cloud.aiplatform.v1";

/** Metadata describing the Model's input and output for explanation. */
export interface ExplanationMetadata {
  /**
   * Required. Map from feature names to feature input metadata. Keys are the
   * name of the features. Values are the specification of the feature.
   *
   * An empty InputMetadata is valid. It describes a text feature which has the
   * name specified as the key in
   * [ExplanationMetadata.inputs][google.cloud.aiplatform.v1.ExplanationMetadata.inputs].
   * The baseline of the empty feature is chosen by Vertex AI.
   *
   * For Vertex AI-provided Tensorflow images, the key can be any friendly
   * name of the feature. Once specified,
   * [featureAttributions][google.cloud.aiplatform.v1.Attribution.feature_attributions]
   * are keyed by this key (if not grouped with another feature).
   *
   * For custom images, the key must match with the key in
   * [instance][google.cloud.aiplatform.v1.ExplainRequest.instances].
   */
  inputs: { [key: string]: ExplanationMetadata_InputMetadata };
  /**
   * Required. Map from output names to output metadata.
   *
   * For Vertex AI-provided Tensorflow images, keys can be any user defined
   * string that consists of any UTF-8 characters.
   *
   * For custom images, keys are the name of the output field in the prediction
   * to be explained.
   *
   * Currently only one key is allowed.
   */
  outputs: { [key: string]: ExplanationMetadata_OutputMetadata };
  /**
   * Points to a YAML file stored on Google Cloud Storage describing the format
   * of the [feature
   * attributions][google.cloud.aiplatform.v1.Attribution.feature_attributions].
   * The schema is defined as an OpenAPI 3.0.2 [Schema
   * Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject).
   * AutoML tabular Models always have this field populated by Vertex AI.
   * Note: The URI given on output may be different, including the URI scheme,
   * than the one given on input. The output URI will point to a location where
   * the user only has a read access.
   */
  featureAttributionsSchemaUri: string;
  /** Name of the source to generate embeddings for example based explanations. */
  latentSpaceSource: string;
}

/**
 * Metadata of the input of a feature.
 *
 * Fields other than
 * [InputMetadata.input_baselines][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.input_baselines]
 * are applicable only for Models that are using Vertex AI-provided images for
 * Tensorflow.
 */
export interface ExplanationMetadata_InputMetadata {
  /**
   * Baseline inputs for this feature.
   *
   * If no baseline is specified, Vertex AI chooses the baseline for this
   * feature. If multiple baselines are specified, Vertex AI returns the
   * average attributions across them in
   * [Attribution.feature_attributions][google.cloud.aiplatform.v1.Attribution.feature_attributions].
   *
   * For Vertex AI-provided Tensorflow images (both 1.x and 2.x), the shape
   * of each baseline must match the shape of the input tensor. If a scalar is
   * provided, we broadcast to the same shape as the input tensor.
   *
   * For custom images, the element of the baselines must be in the same
   * format as the feature's input in the
   * [instance][google.cloud.aiplatform.v1.ExplainRequest.instances][]. The
   * schema of any single instance may be specified via Endpoint's
   * DeployedModels' [Model's][google.cloud.aiplatform.v1.DeployedModel.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
   * [instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri].
   */
  inputBaselines: any[];
  /**
   * Name of the input tensor for this feature. Required and is only
   * applicable to Vertex AI-provided images for Tensorflow.
   */
  inputTensorName: string;
  /**
   * Defines how the feature is encoded into the input tensor. Defaults to
   * IDENTITY.
   */
  encoding: ExplanationMetadata_InputMetadata_Encoding;
  /**
   * Modality of the feature. Valid values are: numeric, image. Defaults to
   * numeric.
   */
  modality: string;
  /**
   * The domain details of the input feature value. Like min/max, original
   * mean or standard deviation if normalized.
   */
  featureValueDomain:
    | ExplanationMetadata_InputMetadata_FeatureValueDomain
    | undefined;
  /**
   * Specifies the index of the values of the input tensor.
   * Required when the input tensor is a sparse representation. Refer to
   * Tensorflow documentation for more details:
   * https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor.
   */
  indicesTensorName: string;
  /**
   * Specifies the shape of the values of the input if the input is a sparse
   * representation. Refer to Tensorflow documentation for more details:
   * https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor.
   */
  denseShapeTensorName: string;
  /**
   * A list of feature names for each index in the input tensor.
   * Required when the input
   * [InputMetadata.encoding][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.encoding]
   * is BAG_OF_FEATURES, BAG_OF_FEATURES_SPARSE, INDICATOR.
   */
  indexFeatureMapping: string[];
  /**
   * Encoded tensor is a transformation of the input tensor. Must be provided
   * if choosing
   * [Integrated Gradients
   * attribution][google.cloud.aiplatform.v1.ExplanationParameters.integrated_gradients_attribution]
   * or [XRAI
   * attribution][google.cloud.aiplatform.v1.ExplanationParameters.xrai_attribution]
   * and the input tensor is not differentiable.
   *
   * An encoded tensor is generated if the input tensor is encoded by a lookup
   * table.
   */
  encodedTensorName: string;
  /**
   * A list of baselines for the encoded tensor.
   *
   * The shape of each baseline should match the shape of the encoded tensor.
   * If a scalar is provided, Vertex AI broadcasts to the same shape as the
   * encoded tensor.
   */
  encodedBaselines: any[];
  /** Visualization configurations for image explanation. */
  visualization:
    | ExplanationMetadata_InputMetadata_Visualization
    | undefined;
  /**
   * Name of the group that the input belongs to. Features with the same group
   * name will be treated as one feature when computing attributions. Features
   * grouped together can have different shapes in value. If provided, there
   * will be one single attribution generated in
   * [Attribution.feature_attributions][google.cloud.aiplatform.v1.Attribution.feature_attributions],
   * keyed by the group name.
   */
  groupName: string;
}

/** Defines how a feature is encoded. Defaults to IDENTITY. */
export enum ExplanationMetadata_InputMetadata_Encoding {
  /** ENCODING_UNSPECIFIED - Default value. This is the same as IDENTITY. */
  ENCODING_UNSPECIFIED = 0,
  /** IDENTITY - The tensor represents one feature. */
  IDENTITY = 1,
  /**
   * BAG_OF_FEATURES - The tensor represents a bag of features where each index maps to
   * a feature.
   * [InputMetadata.index_feature_mapping][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.index_feature_mapping]
   * must be provided for this encoding. For example:
   * ```
   * input = [27, 6.0, 150]
   * index_feature_mapping = ["age", "height", "weight"]
   * ```
   */
  BAG_OF_FEATURES = 2,
  /**
   * BAG_OF_FEATURES_SPARSE - The tensor represents a bag of features where each index maps to a
   * feature. Zero values in the tensor indicates feature being
   * non-existent.
   * [InputMetadata.index_feature_mapping][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.index_feature_mapping]
   * must be provided for this encoding. For example:
   * ```
   * input = [2, 0, 5, 0, 1]
   * index_feature_mapping = ["a", "b", "c", "d", "e"]
   * ```
   */
  BAG_OF_FEATURES_SPARSE = 3,
  /**
   * INDICATOR - The tensor is a list of binaries representing whether a feature exists
   * or not (1 indicates existence).
   * [InputMetadata.index_feature_mapping][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.index_feature_mapping]
   * must be provided for this encoding. For example:
   * ```
   * input = [1, 0, 1, 0, 1]
   * index_feature_mapping = ["a", "b", "c", "d", "e"]
   * ```
   */
  INDICATOR = 4,
  /**
   * COMBINED_EMBEDDING - The tensor is encoded into a 1-dimensional array represented by an
   * encoded tensor.
   * [InputMetadata.encoded_tensor_name][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.encoded_tensor_name]
   * must be provided for this encoding. For example:
   * ```
   * input = ["This", "is", "a", "test", "."]
   * encoded = [0.1, 0.2, 0.3, 0.4, 0.5]
   * ```
   */
  COMBINED_EMBEDDING = 5,
  /**
   * CONCAT_EMBEDDING - Select this encoding when the input tensor is encoded into a
   * 2-dimensional array represented by an encoded tensor.
   * [InputMetadata.encoded_tensor_name][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.encoded_tensor_name]
   * must be provided for this encoding. The first dimension of the encoded
   * tensor's shape is the same as the input tensor's shape. For example:
   * ```
   * input = ["This", "is", "a", "test", "."]
   * encoded = [[0.1, 0.2, 0.3, 0.4, 0.5],
   *            [0.2, 0.1, 0.4, 0.3, 0.5],
   *            [0.5, 0.1, 0.3, 0.5, 0.4],
   *            [0.5, 0.3, 0.1, 0.2, 0.4],
   *            [0.4, 0.3, 0.2, 0.5, 0.1]]
   * ```
   */
  CONCAT_EMBEDDING = 6,
  UNRECOGNIZED = -1,
}

export function explanationMetadata_InputMetadata_EncodingFromJSON(
  object: any,
): ExplanationMetadata_InputMetadata_Encoding {
  switch (object) {
    case 0:
    case "ENCODING_UNSPECIFIED":
      return ExplanationMetadata_InputMetadata_Encoding.ENCODING_UNSPECIFIED;
    case 1:
    case "IDENTITY":
      return ExplanationMetadata_InputMetadata_Encoding.IDENTITY;
    case 2:
    case "BAG_OF_FEATURES":
      return ExplanationMetadata_InputMetadata_Encoding.BAG_OF_FEATURES;
    case 3:
    case "BAG_OF_FEATURES_SPARSE":
      return ExplanationMetadata_InputMetadata_Encoding.BAG_OF_FEATURES_SPARSE;
    case 4:
    case "INDICATOR":
      return ExplanationMetadata_InputMetadata_Encoding.INDICATOR;
    case 5:
    case "COMBINED_EMBEDDING":
      return ExplanationMetadata_InputMetadata_Encoding.COMBINED_EMBEDDING;
    case 6:
    case "CONCAT_EMBEDDING":
      return ExplanationMetadata_InputMetadata_Encoding.CONCAT_EMBEDDING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ExplanationMetadata_InputMetadata_Encoding.UNRECOGNIZED;
  }
}

export function explanationMetadata_InputMetadata_EncodingToJSON(
  object: ExplanationMetadata_InputMetadata_Encoding,
): string {
  switch (object) {
    case ExplanationMetadata_InputMetadata_Encoding.ENCODING_UNSPECIFIED:
      return "ENCODING_UNSPECIFIED";
    case ExplanationMetadata_InputMetadata_Encoding.IDENTITY:
      return "IDENTITY";
    case ExplanationMetadata_InputMetadata_Encoding.BAG_OF_FEATURES:
      return "BAG_OF_FEATURES";
    case ExplanationMetadata_InputMetadata_Encoding.BAG_OF_FEATURES_SPARSE:
      return "BAG_OF_FEATURES_SPARSE";
    case ExplanationMetadata_InputMetadata_Encoding.INDICATOR:
      return "INDICATOR";
    case ExplanationMetadata_InputMetadata_Encoding.COMBINED_EMBEDDING:
      return "COMBINED_EMBEDDING";
    case ExplanationMetadata_InputMetadata_Encoding.CONCAT_EMBEDDING:
      return "CONCAT_EMBEDDING";
    case ExplanationMetadata_InputMetadata_Encoding.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Domain details of the input feature value. Provides numeric information
 * about the feature, such as its range (min, max). If the feature has been
 * pre-processed, for example with z-scoring, then it provides information
 * about how to recover the original feature. For example, if the input
 * feature is an image and it has been pre-processed to obtain 0-mean and
 * stddev = 1 values, then original_mean, and original_stddev refer to the
 * mean and stddev of the original feature (e.g. image tensor) from which
 * input feature (with mean = 0 and stddev = 1) was obtained.
 */
export interface ExplanationMetadata_InputMetadata_FeatureValueDomain {
  /** The minimum permissible value for this feature. */
  minValue: number;
  /** The maximum permissible value for this feature. */
  maxValue: number;
  /**
   * If this input feature has been normalized to a mean value of 0,
   * the original_mean specifies the mean value of the domain prior to
   * normalization.
   */
  originalMean: number;
  /**
   * If this input feature has been normalized to a standard deviation of
   * 1.0, the original_stddev specifies the standard deviation of the domain
   * prior to normalization.
   */
  originalStddev: number;
}

/** Visualization configurations for image explanation. */
export interface ExplanationMetadata_InputMetadata_Visualization {
  /**
   * Type of the image visualization. Only applicable to
   * [Integrated Gradients
   * attribution][google.cloud.aiplatform.v1.ExplanationParameters.integrated_gradients_attribution].
   * OUTLINES shows regions of attribution, while PIXELS shows per-pixel
   * attribution. Defaults to OUTLINES.
   */
  type: ExplanationMetadata_InputMetadata_Visualization_Type;
  /**
   * Whether to only highlight pixels with positive contributions, negative
   * or both. Defaults to POSITIVE.
   */
  polarity: ExplanationMetadata_InputMetadata_Visualization_Polarity;
  /**
   * The color scheme used for the highlighted areas.
   *
   * Defaults to PINK_GREEN for
   * [Integrated Gradients
   * attribution][google.cloud.aiplatform.v1.ExplanationParameters.integrated_gradients_attribution],
   * which shows positive attributions in green and negative in pink.
   *
   * Defaults to VIRIDIS for
   * [XRAI
   * attribution][google.cloud.aiplatform.v1.ExplanationParameters.xrai_attribution],
   * which highlights the most influential regions in yellow and the least
   * influential in blue.
   */
  colorMap: ExplanationMetadata_InputMetadata_Visualization_ColorMap;
  /**
   * Excludes attributions above the specified percentile from the
   * highlighted areas. Using the clip_percent_upperbound and
   * clip_percent_lowerbound together can be useful for filtering out noise
   * and making it easier to see areas of strong attribution. Defaults to
   * 99.9.
   */
  clipPercentUpperbound: number;
  /**
   * Excludes attributions below the specified percentile, from the
   * highlighted areas. Defaults to 62.
   */
  clipPercentLowerbound: number;
  /**
   * How the original image is displayed in the visualization.
   * Adjusting the overlay can help increase visual clarity if the original
   * image makes it difficult to view the visualization. Defaults to NONE.
   */
  overlayType: ExplanationMetadata_InputMetadata_Visualization_OverlayType;
}

/**
 * Type of the image visualization. Only applicable to
 * [Integrated Gradients
 * attribution][google.cloud.aiplatform.v1.ExplanationParameters.integrated_gradients_attribution].
 */
export enum ExplanationMetadata_InputMetadata_Visualization_Type {
  /** TYPE_UNSPECIFIED - Should not be used. */
  TYPE_UNSPECIFIED = 0,
  /** PIXELS - Shows which pixel contributed to the image prediction. */
  PIXELS = 1,
  /**
   * OUTLINES - Shows which region contributed to the image prediction by outlining
   * the region.
   */
  OUTLINES = 2,
  UNRECOGNIZED = -1,
}

export function explanationMetadata_InputMetadata_Visualization_TypeFromJSON(
  object: any,
): ExplanationMetadata_InputMetadata_Visualization_Type {
  switch (object) {
    case 0:
    case "TYPE_UNSPECIFIED":
      return ExplanationMetadata_InputMetadata_Visualization_Type.TYPE_UNSPECIFIED;
    case 1:
    case "PIXELS":
      return ExplanationMetadata_InputMetadata_Visualization_Type.PIXELS;
    case 2:
    case "OUTLINES":
      return ExplanationMetadata_InputMetadata_Visualization_Type.OUTLINES;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ExplanationMetadata_InputMetadata_Visualization_Type.UNRECOGNIZED;
  }
}

export function explanationMetadata_InputMetadata_Visualization_TypeToJSON(
  object: ExplanationMetadata_InputMetadata_Visualization_Type,
): string {
  switch (object) {
    case ExplanationMetadata_InputMetadata_Visualization_Type.TYPE_UNSPECIFIED:
      return "TYPE_UNSPECIFIED";
    case ExplanationMetadata_InputMetadata_Visualization_Type.PIXELS:
      return "PIXELS";
    case ExplanationMetadata_InputMetadata_Visualization_Type.OUTLINES:
      return "OUTLINES";
    case ExplanationMetadata_InputMetadata_Visualization_Type.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Whether to only highlight pixels with positive contributions, negative
 * or both. Defaults to POSITIVE.
 */
export enum ExplanationMetadata_InputMetadata_Visualization_Polarity {
  /** POLARITY_UNSPECIFIED - Default value. This is the same as POSITIVE. */
  POLARITY_UNSPECIFIED = 0,
  /**
   * POSITIVE - Highlights the pixels/outlines that were most influential to the
   * model's prediction.
   */
  POSITIVE = 1,
  /**
   * NEGATIVE - Setting polarity to negative highlights areas that does not lead to
   * the models's current prediction.
   */
  NEGATIVE = 2,
  /** BOTH - Shows both positive and negative attributions. */
  BOTH = 3,
  UNRECOGNIZED = -1,
}

export function explanationMetadata_InputMetadata_Visualization_PolarityFromJSON(
  object: any,
): ExplanationMetadata_InputMetadata_Visualization_Polarity {
  switch (object) {
    case 0:
    case "POLARITY_UNSPECIFIED":
      return ExplanationMetadata_InputMetadata_Visualization_Polarity.POLARITY_UNSPECIFIED;
    case 1:
    case "POSITIVE":
      return ExplanationMetadata_InputMetadata_Visualization_Polarity.POSITIVE;
    case 2:
    case "NEGATIVE":
      return ExplanationMetadata_InputMetadata_Visualization_Polarity.NEGATIVE;
    case 3:
    case "BOTH":
      return ExplanationMetadata_InputMetadata_Visualization_Polarity.BOTH;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ExplanationMetadata_InputMetadata_Visualization_Polarity.UNRECOGNIZED;
  }
}

export function explanationMetadata_InputMetadata_Visualization_PolarityToJSON(
  object: ExplanationMetadata_InputMetadata_Visualization_Polarity,
): string {
  switch (object) {
    case ExplanationMetadata_InputMetadata_Visualization_Polarity.POLARITY_UNSPECIFIED:
      return "POLARITY_UNSPECIFIED";
    case ExplanationMetadata_InputMetadata_Visualization_Polarity.POSITIVE:
      return "POSITIVE";
    case ExplanationMetadata_InputMetadata_Visualization_Polarity.NEGATIVE:
      return "NEGATIVE";
    case ExplanationMetadata_InputMetadata_Visualization_Polarity.BOTH:
      return "BOTH";
    case ExplanationMetadata_InputMetadata_Visualization_Polarity.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The color scheme used for highlighting areas. */
export enum ExplanationMetadata_InputMetadata_Visualization_ColorMap {
  /** COLOR_MAP_UNSPECIFIED - Should not be used. */
  COLOR_MAP_UNSPECIFIED = 0,
  /** PINK_GREEN - Positive: green. Negative: pink. */
  PINK_GREEN = 1,
  /**
   * VIRIDIS - Viridis color map: A perceptually uniform color mapping which is
   * easier to see by those with colorblindness and progresses from yellow
   * to green to blue. Positive: yellow. Negative: blue.
   */
  VIRIDIS = 2,
  /** RED - Positive: red. Negative: red. */
  RED = 3,
  /** GREEN - Positive: green. Negative: green. */
  GREEN = 4,
  /** RED_GREEN - Positive: green. Negative: red. */
  RED_GREEN = 6,
  /** PINK_WHITE_GREEN - PiYG palette. */
  PINK_WHITE_GREEN = 5,
  UNRECOGNIZED = -1,
}

export function explanationMetadata_InputMetadata_Visualization_ColorMapFromJSON(
  object: any,
): ExplanationMetadata_InputMetadata_Visualization_ColorMap {
  switch (object) {
    case 0:
    case "COLOR_MAP_UNSPECIFIED":
      return ExplanationMetadata_InputMetadata_Visualization_ColorMap.COLOR_MAP_UNSPECIFIED;
    case 1:
    case "PINK_GREEN":
      return ExplanationMetadata_InputMetadata_Visualization_ColorMap.PINK_GREEN;
    case 2:
    case "VIRIDIS":
      return ExplanationMetadata_InputMetadata_Visualization_ColorMap.VIRIDIS;
    case 3:
    case "RED":
      return ExplanationMetadata_InputMetadata_Visualization_ColorMap.RED;
    case 4:
    case "GREEN":
      return ExplanationMetadata_InputMetadata_Visualization_ColorMap.GREEN;
    case 6:
    case "RED_GREEN":
      return ExplanationMetadata_InputMetadata_Visualization_ColorMap.RED_GREEN;
    case 5:
    case "PINK_WHITE_GREEN":
      return ExplanationMetadata_InputMetadata_Visualization_ColorMap.PINK_WHITE_GREEN;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ExplanationMetadata_InputMetadata_Visualization_ColorMap.UNRECOGNIZED;
  }
}

export function explanationMetadata_InputMetadata_Visualization_ColorMapToJSON(
  object: ExplanationMetadata_InputMetadata_Visualization_ColorMap,
): string {
  switch (object) {
    case ExplanationMetadata_InputMetadata_Visualization_ColorMap.COLOR_MAP_UNSPECIFIED:
      return "COLOR_MAP_UNSPECIFIED";
    case ExplanationMetadata_InputMetadata_Visualization_ColorMap.PINK_GREEN:
      return "PINK_GREEN";
    case ExplanationMetadata_InputMetadata_Visualization_ColorMap.VIRIDIS:
      return "VIRIDIS";
    case ExplanationMetadata_InputMetadata_Visualization_ColorMap.RED:
      return "RED";
    case ExplanationMetadata_InputMetadata_Visualization_ColorMap.GREEN:
      return "GREEN";
    case ExplanationMetadata_InputMetadata_Visualization_ColorMap.RED_GREEN:
      return "RED_GREEN";
    case ExplanationMetadata_InputMetadata_Visualization_ColorMap.PINK_WHITE_GREEN:
      return "PINK_WHITE_GREEN";
    case ExplanationMetadata_InputMetadata_Visualization_ColorMap.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** How the original image is displayed in the visualization. */
export enum ExplanationMetadata_InputMetadata_Visualization_OverlayType {
  /** OVERLAY_TYPE_UNSPECIFIED - Default value. This is the same as NONE. */
  OVERLAY_TYPE_UNSPECIFIED = 0,
  /** NONE - No overlay. */
  NONE = 1,
  /** ORIGINAL - The attributions are shown on top of the original image. */
  ORIGINAL = 2,
  /**
   * GRAYSCALE - The attributions are shown on top of grayscaled version of the
   * original image.
   */
  GRAYSCALE = 3,
  /**
   * MASK_BLACK - The attributions are used as a mask to reveal predictive parts of
   * the image and hide the un-predictive parts.
   */
  MASK_BLACK = 4,
  UNRECOGNIZED = -1,
}

export function explanationMetadata_InputMetadata_Visualization_OverlayTypeFromJSON(
  object: any,
): ExplanationMetadata_InputMetadata_Visualization_OverlayType {
  switch (object) {
    case 0:
    case "OVERLAY_TYPE_UNSPECIFIED":
      return ExplanationMetadata_InputMetadata_Visualization_OverlayType.OVERLAY_TYPE_UNSPECIFIED;
    case 1:
    case "NONE":
      return ExplanationMetadata_InputMetadata_Visualization_OverlayType.NONE;
    case 2:
    case "ORIGINAL":
      return ExplanationMetadata_InputMetadata_Visualization_OverlayType.ORIGINAL;
    case 3:
    case "GRAYSCALE":
      return ExplanationMetadata_InputMetadata_Visualization_OverlayType.GRAYSCALE;
    case 4:
    case "MASK_BLACK":
      return ExplanationMetadata_InputMetadata_Visualization_OverlayType.MASK_BLACK;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ExplanationMetadata_InputMetadata_Visualization_OverlayType.UNRECOGNIZED;
  }
}

export function explanationMetadata_InputMetadata_Visualization_OverlayTypeToJSON(
  object: ExplanationMetadata_InputMetadata_Visualization_OverlayType,
): string {
  switch (object) {
    case ExplanationMetadata_InputMetadata_Visualization_OverlayType.OVERLAY_TYPE_UNSPECIFIED:
      return "OVERLAY_TYPE_UNSPECIFIED";
    case ExplanationMetadata_InputMetadata_Visualization_OverlayType.NONE:
      return "NONE";
    case ExplanationMetadata_InputMetadata_Visualization_OverlayType.ORIGINAL:
      return "ORIGINAL";
    case ExplanationMetadata_InputMetadata_Visualization_OverlayType.GRAYSCALE:
      return "GRAYSCALE";
    case ExplanationMetadata_InputMetadata_Visualization_OverlayType.MASK_BLACK:
      return "MASK_BLACK";
    case ExplanationMetadata_InputMetadata_Visualization_OverlayType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Metadata of the prediction output to be explained. */
export interface ExplanationMetadata_OutputMetadata {
  /**
   * Static mapping between the index and display name.
   *
   * Use this if the outputs are a deterministic n-dimensional array, e.g. a
   * list of scores of all the classes in a pre-defined order for a
   * multi-classification Model. It's not feasible if the outputs are
   * non-deterministic, e.g. the Model produces top-k classes or sort the
   * outputs by their values.
   *
   * The shape of the value must be an n-dimensional array of strings. The
   * number of dimensions must match that of the outputs to be explained.
   * The
   * [Attribution.output_display_name][google.cloud.aiplatform.v1.Attribution.output_display_name]
   * is populated by locating in the mapping with
   * [Attribution.output_index][google.cloud.aiplatform.v1.Attribution.output_index].
   */
  indexDisplayNameMapping?:
    | any
    | undefined;
  /**
   * Specify a field name in the prediction to look for the display name.
   *
   * Use this if the prediction contains the display names for the outputs.
   *
   * The display names in the prediction must have the same shape of the
   * outputs, so that it can be located by
   * [Attribution.output_index][google.cloud.aiplatform.v1.Attribution.output_index]
   * for a specific output.
   */
  displayNameMappingKey?:
    | string
    | undefined;
  /**
   * Name of the output tensor. Required and is only applicable to Vertex
   * AI provided images for Tensorflow.
   */
  outputTensorName: string;
}

export interface ExplanationMetadata_InputsEntry {
  key: string;
  value: ExplanationMetadata_InputMetadata | undefined;
}

export interface ExplanationMetadata_OutputsEntry {
  key: string;
  value: ExplanationMetadata_OutputMetadata | undefined;
}

function createBaseExplanationMetadata(): ExplanationMetadata {
  return { inputs: {}, outputs: {}, featureAttributionsSchemaUri: "", latentSpaceSource: "" };
}

export const ExplanationMetadata: MessageFns<ExplanationMetadata> = {
  encode(message: ExplanationMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.inputs).forEach(([key, value]) => {
      ExplanationMetadata_InputsEntry.encode({ key: key as any, value }, writer.uint32(10).fork()).join();
    });
    Object.entries(message.outputs).forEach(([key, value]) => {
      ExplanationMetadata_OutputsEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    if (message.featureAttributionsSchemaUri !== "") {
      writer.uint32(26).string(message.featureAttributionsSchemaUri);
    }
    if (message.latentSpaceSource !== "") {
      writer.uint32(42).string(message.latentSpaceSource);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplanationMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplanationMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          const entry1 = ExplanationMetadata_InputsEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.inputs[entry1.key] = entry1.value;
          }
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = ExplanationMetadata_OutputsEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.outputs[entry2.key] = entry2.value;
          }
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.featureAttributionsSchemaUri = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.latentSpaceSource = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplanationMetadata {
    return {
      inputs: isObject(object.inputs)
        ? Object.entries(object.inputs).reduce<{ [key: string]: ExplanationMetadata_InputMetadata }>(
          (acc, [key, value]) => {
            acc[key] = ExplanationMetadata_InputMetadata.fromJSON(value);
            return acc;
          },
          {},
        )
        : {},
      outputs: isObject(object.outputs)
        ? Object.entries(object.outputs).reduce<{ [key: string]: ExplanationMetadata_OutputMetadata }>(
          (acc, [key, value]) => {
            acc[key] = ExplanationMetadata_OutputMetadata.fromJSON(value);
            return acc;
          },
          {},
        )
        : {},
      featureAttributionsSchemaUri: isSet(object.featureAttributionsSchemaUri)
        ? globalThis.String(object.featureAttributionsSchemaUri)
        : "",
      latentSpaceSource: isSet(object.latentSpaceSource) ? globalThis.String(object.latentSpaceSource) : "",
    };
  },

  toJSON(message: ExplanationMetadata): unknown {
    const obj: any = {};
    if (message.inputs) {
      const entries = Object.entries(message.inputs);
      if (entries.length > 0) {
        obj.inputs = {};
        entries.forEach(([k, v]) => {
          obj.inputs[k] = ExplanationMetadata_InputMetadata.toJSON(v);
        });
      }
    }
    if (message.outputs) {
      const entries = Object.entries(message.outputs);
      if (entries.length > 0) {
        obj.outputs = {};
        entries.forEach(([k, v]) => {
          obj.outputs[k] = ExplanationMetadata_OutputMetadata.toJSON(v);
        });
      }
    }
    if (message.featureAttributionsSchemaUri !== "") {
      obj.featureAttributionsSchemaUri = message.featureAttributionsSchemaUri;
    }
    if (message.latentSpaceSource !== "") {
      obj.latentSpaceSource = message.latentSpaceSource;
    }
    return obj;
  },

  create(base?: DeepPartial<ExplanationMetadata>): ExplanationMetadata {
    return ExplanationMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplanationMetadata>): ExplanationMetadata {
    const message = createBaseExplanationMetadata();
    message.inputs = Object.entries(object.inputs ?? {}).reduce<{ [key: string]: ExplanationMetadata_InputMetadata }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = ExplanationMetadata_InputMetadata.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    message.outputs = Object.entries(object.outputs ?? {}).reduce<
      { [key: string]: ExplanationMetadata_OutputMetadata }
    >((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = ExplanationMetadata_OutputMetadata.fromPartial(value);
      }
      return acc;
    }, {});
    message.featureAttributionsSchemaUri = object.featureAttributionsSchemaUri ?? "";
    message.latentSpaceSource = object.latentSpaceSource ?? "";
    return message;
  },
};

function createBaseExplanationMetadata_InputMetadata(): ExplanationMetadata_InputMetadata {
  return {
    inputBaselines: [],
    inputTensorName: "",
    encoding: 0,
    modality: "",
    featureValueDomain: undefined,
    indicesTensorName: "",
    denseShapeTensorName: "",
    indexFeatureMapping: [],
    encodedTensorName: "",
    encodedBaselines: [],
    visualization: undefined,
    groupName: "",
  };
}

export const ExplanationMetadata_InputMetadata: MessageFns<ExplanationMetadata_InputMetadata> = {
  encode(message: ExplanationMetadata_InputMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.inputBaselines) {
      Value.encode(Value.wrap(v!), writer.uint32(10).fork()).join();
    }
    if (message.inputTensorName !== "") {
      writer.uint32(18).string(message.inputTensorName);
    }
    if (message.encoding !== 0) {
      writer.uint32(24).int32(message.encoding);
    }
    if (message.modality !== "") {
      writer.uint32(34).string(message.modality);
    }
    if (message.featureValueDomain !== undefined) {
      ExplanationMetadata_InputMetadata_FeatureValueDomain.encode(message.featureValueDomain, writer.uint32(42).fork())
        .join();
    }
    if (message.indicesTensorName !== "") {
      writer.uint32(50).string(message.indicesTensorName);
    }
    if (message.denseShapeTensorName !== "") {
      writer.uint32(58).string(message.denseShapeTensorName);
    }
    for (const v of message.indexFeatureMapping) {
      writer.uint32(66).string(v!);
    }
    if (message.encodedTensorName !== "") {
      writer.uint32(74).string(message.encodedTensorName);
    }
    for (const v of message.encodedBaselines) {
      Value.encode(Value.wrap(v!), writer.uint32(82).fork()).join();
    }
    if (message.visualization !== undefined) {
      ExplanationMetadata_InputMetadata_Visualization.encode(message.visualization, writer.uint32(90).fork()).join();
    }
    if (message.groupName !== "") {
      writer.uint32(98).string(message.groupName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplanationMetadata_InputMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplanationMetadata_InputMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputBaselines.push(Value.unwrap(Value.decode(reader, reader.uint32())));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inputTensorName = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.encoding = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.modality = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.featureValueDomain = ExplanationMetadata_InputMetadata_FeatureValueDomain.decode(
            reader,
            reader.uint32(),
          );
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.indicesTensorName = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.denseShapeTensorName = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.indexFeatureMapping.push(reader.string());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.encodedTensorName = reader.string();
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.encodedBaselines.push(Value.unwrap(Value.decode(reader, reader.uint32())));
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.visualization = ExplanationMetadata_InputMetadata_Visualization.decode(reader, reader.uint32());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.groupName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplanationMetadata_InputMetadata {
    return {
      inputBaselines: globalThis.Array.isArray(object?.inputBaselines) ? [...object.inputBaselines] : [],
      inputTensorName: isSet(object.inputTensorName) ? globalThis.String(object.inputTensorName) : "",
      encoding: isSet(object.encoding) ? explanationMetadata_InputMetadata_EncodingFromJSON(object.encoding) : 0,
      modality: isSet(object.modality) ? globalThis.String(object.modality) : "",
      featureValueDomain: isSet(object.featureValueDomain)
        ? ExplanationMetadata_InputMetadata_FeatureValueDomain.fromJSON(object.featureValueDomain)
        : undefined,
      indicesTensorName: isSet(object.indicesTensorName) ? globalThis.String(object.indicesTensorName) : "",
      denseShapeTensorName: isSet(object.denseShapeTensorName) ? globalThis.String(object.denseShapeTensorName) : "",
      indexFeatureMapping: globalThis.Array.isArray(object?.indexFeatureMapping)
        ? object.indexFeatureMapping.map((e: any) => globalThis.String(e))
        : [],
      encodedTensorName: isSet(object.encodedTensorName) ? globalThis.String(object.encodedTensorName) : "",
      encodedBaselines: globalThis.Array.isArray(object?.encodedBaselines) ? [...object.encodedBaselines] : [],
      visualization: isSet(object.visualization)
        ? ExplanationMetadata_InputMetadata_Visualization.fromJSON(object.visualization)
        : undefined,
      groupName: isSet(object.groupName) ? globalThis.String(object.groupName) : "",
    };
  },

  toJSON(message: ExplanationMetadata_InputMetadata): unknown {
    const obj: any = {};
    if (message.inputBaselines?.length) {
      obj.inputBaselines = message.inputBaselines;
    }
    if (message.inputTensorName !== "") {
      obj.inputTensorName = message.inputTensorName;
    }
    if (message.encoding !== 0) {
      obj.encoding = explanationMetadata_InputMetadata_EncodingToJSON(message.encoding);
    }
    if (message.modality !== "") {
      obj.modality = message.modality;
    }
    if (message.featureValueDomain !== undefined) {
      obj.featureValueDomain = ExplanationMetadata_InputMetadata_FeatureValueDomain.toJSON(message.featureValueDomain);
    }
    if (message.indicesTensorName !== "") {
      obj.indicesTensorName = message.indicesTensorName;
    }
    if (message.denseShapeTensorName !== "") {
      obj.denseShapeTensorName = message.denseShapeTensorName;
    }
    if (message.indexFeatureMapping?.length) {
      obj.indexFeatureMapping = message.indexFeatureMapping;
    }
    if (message.encodedTensorName !== "") {
      obj.encodedTensorName = message.encodedTensorName;
    }
    if (message.encodedBaselines?.length) {
      obj.encodedBaselines = message.encodedBaselines;
    }
    if (message.visualization !== undefined) {
      obj.visualization = ExplanationMetadata_InputMetadata_Visualization.toJSON(message.visualization);
    }
    if (message.groupName !== "") {
      obj.groupName = message.groupName;
    }
    return obj;
  },

  create(base?: DeepPartial<ExplanationMetadata_InputMetadata>): ExplanationMetadata_InputMetadata {
    return ExplanationMetadata_InputMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplanationMetadata_InputMetadata>): ExplanationMetadata_InputMetadata {
    const message = createBaseExplanationMetadata_InputMetadata();
    message.inputBaselines = object.inputBaselines?.map((e) => e) || [];
    message.inputTensorName = object.inputTensorName ?? "";
    message.encoding = object.encoding ?? 0;
    message.modality = object.modality ?? "";
    message.featureValueDomain = (object.featureValueDomain !== undefined && object.featureValueDomain !== null)
      ? ExplanationMetadata_InputMetadata_FeatureValueDomain.fromPartial(object.featureValueDomain)
      : undefined;
    message.indicesTensorName = object.indicesTensorName ?? "";
    message.denseShapeTensorName = object.denseShapeTensorName ?? "";
    message.indexFeatureMapping = object.indexFeatureMapping?.map((e) => e) || [];
    message.encodedTensorName = object.encodedTensorName ?? "";
    message.encodedBaselines = object.encodedBaselines?.map((e) => e) || [];
    message.visualization = (object.visualization !== undefined && object.visualization !== null)
      ? ExplanationMetadata_InputMetadata_Visualization.fromPartial(object.visualization)
      : undefined;
    message.groupName = object.groupName ?? "";
    return message;
  },
};

function createBaseExplanationMetadata_InputMetadata_FeatureValueDomain(): ExplanationMetadata_InputMetadata_FeatureValueDomain {
  return { minValue: 0, maxValue: 0, originalMean: 0, originalStddev: 0 };
}

export const ExplanationMetadata_InputMetadata_FeatureValueDomain: MessageFns<
  ExplanationMetadata_InputMetadata_FeatureValueDomain
> = {
  encode(
    message: ExplanationMetadata_InputMetadata_FeatureValueDomain,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.minValue !== 0) {
      writer.uint32(13).float(message.minValue);
    }
    if (message.maxValue !== 0) {
      writer.uint32(21).float(message.maxValue);
    }
    if (message.originalMean !== 0) {
      writer.uint32(29).float(message.originalMean);
    }
    if (message.originalStddev !== 0) {
      writer.uint32(37).float(message.originalStddev);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplanationMetadata_InputMetadata_FeatureValueDomain {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplanationMetadata_InputMetadata_FeatureValueDomain();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 13) {
            break;
          }

          message.minValue = reader.float();
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.maxValue = reader.float();
          continue;
        case 3:
          if (tag !== 29) {
            break;
          }

          message.originalMean = reader.float();
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.originalStddev = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplanationMetadata_InputMetadata_FeatureValueDomain {
    return {
      minValue: isSet(object.minValue) ? globalThis.Number(object.minValue) : 0,
      maxValue: isSet(object.maxValue) ? globalThis.Number(object.maxValue) : 0,
      originalMean: isSet(object.originalMean) ? globalThis.Number(object.originalMean) : 0,
      originalStddev: isSet(object.originalStddev) ? globalThis.Number(object.originalStddev) : 0,
    };
  },

  toJSON(message: ExplanationMetadata_InputMetadata_FeatureValueDomain): unknown {
    const obj: any = {};
    if (message.minValue !== 0) {
      obj.minValue = message.minValue;
    }
    if (message.maxValue !== 0) {
      obj.maxValue = message.maxValue;
    }
    if (message.originalMean !== 0) {
      obj.originalMean = message.originalMean;
    }
    if (message.originalStddev !== 0) {
      obj.originalStddev = message.originalStddev;
    }
    return obj;
  },

  create(
    base?: DeepPartial<ExplanationMetadata_InputMetadata_FeatureValueDomain>,
  ): ExplanationMetadata_InputMetadata_FeatureValueDomain {
    return ExplanationMetadata_InputMetadata_FeatureValueDomain.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ExplanationMetadata_InputMetadata_FeatureValueDomain>,
  ): ExplanationMetadata_InputMetadata_FeatureValueDomain {
    const message = createBaseExplanationMetadata_InputMetadata_FeatureValueDomain();
    message.minValue = object.minValue ?? 0;
    message.maxValue = object.maxValue ?? 0;
    message.originalMean = object.originalMean ?? 0;
    message.originalStddev = object.originalStddev ?? 0;
    return message;
  },
};

function createBaseExplanationMetadata_InputMetadata_Visualization(): ExplanationMetadata_InputMetadata_Visualization {
  return { type: 0, polarity: 0, colorMap: 0, clipPercentUpperbound: 0, clipPercentLowerbound: 0, overlayType: 0 };
}

export const ExplanationMetadata_InputMetadata_Visualization: MessageFns<
  ExplanationMetadata_InputMetadata_Visualization
> = {
  encode(
    message: ExplanationMetadata_InputMetadata_Visualization,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.type !== 0) {
      writer.uint32(8).int32(message.type);
    }
    if (message.polarity !== 0) {
      writer.uint32(16).int32(message.polarity);
    }
    if (message.colorMap !== 0) {
      writer.uint32(24).int32(message.colorMap);
    }
    if (message.clipPercentUpperbound !== 0) {
      writer.uint32(37).float(message.clipPercentUpperbound);
    }
    if (message.clipPercentLowerbound !== 0) {
      writer.uint32(45).float(message.clipPercentLowerbound);
    }
    if (message.overlayType !== 0) {
      writer.uint32(48).int32(message.overlayType);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplanationMetadata_InputMetadata_Visualization {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplanationMetadata_InputMetadata_Visualization();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.polarity = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.colorMap = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.clipPercentUpperbound = reader.float();
          continue;
        case 5:
          if (tag !== 45) {
            break;
          }

          message.clipPercentLowerbound = reader.float();
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.overlayType = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplanationMetadata_InputMetadata_Visualization {
    return {
      type: isSet(object.type) ? explanationMetadata_InputMetadata_Visualization_TypeFromJSON(object.type) : 0,
      polarity: isSet(object.polarity)
        ? explanationMetadata_InputMetadata_Visualization_PolarityFromJSON(object.polarity)
        : 0,
      colorMap: isSet(object.colorMap)
        ? explanationMetadata_InputMetadata_Visualization_ColorMapFromJSON(object.colorMap)
        : 0,
      clipPercentUpperbound: isSet(object.clipPercentUpperbound) ? globalThis.Number(object.clipPercentUpperbound) : 0,
      clipPercentLowerbound: isSet(object.clipPercentLowerbound) ? globalThis.Number(object.clipPercentLowerbound) : 0,
      overlayType: isSet(object.overlayType)
        ? explanationMetadata_InputMetadata_Visualization_OverlayTypeFromJSON(object.overlayType)
        : 0,
    };
  },

  toJSON(message: ExplanationMetadata_InputMetadata_Visualization): unknown {
    const obj: any = {};
    if (message.type !== 0) {
      obj.type = explanationMetadata_InputMetadata_Visualization_TypeToJSON(message.type);
    }
    if (message.polarity !== 0) {
      obj.polarity = explanationMetadata_InputMetadata_Visualization_PolarityToJSON(message.polarity);
    }
    if (message.colorMap !== 0) {
      obj.colorMap = explanationMetadata_InputMetadata_Visualization_ColorMapToJSON(message.colorMap);
    }
    if (message.clipPercentUpperbound !== 0) {
      obj.clipPercentUpperbound = message.clipPercentUpperbound;
    }
    if (message.clipPercentLowerbound !== 0) {
      obj.clipPercentLowerbound = message.clipPercentLowerbound;
    }
    if (message.overlayType !== 0) {
      obj.overlayType = explanationMetadata_InputMetadata_Visualization_OverlayTypeToJSON(message.overlayType);
    }
    return obj;
  },

  create(
    base?: DeepPartial<ExplanationMetadata_InputMetadata_Visualization>,
  ): ExplanationMetadata_InputMetadata_Visualization {
    return ExplanationMetadata_InputMetadata_Visualization.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ExplanationMetadata_InputMetadata_Visualization>,
  ): ExplanationMetadata_InputMetadata_Visualization {
    const message = createBaseExplanationMetadata_InputMetadata_Visualization();
    message.type = object.type ?? 0;
    message.polarity = object.polarity ?? 0;
    message.colorMap = object.colorMap ?? 0;
    message.clipPercentUpperbound = object.clipPercentUpperbound ?? 0;
    message.clipPercentLowerbound = object.clipPercentLowerbound ?? 0;
    message.overlayType = object.overlayType ?? 0;
    return message;
  },
};

function createBaseExplanationMetadata_OutputMetadata(): ExplanationMetadata_OutputMetadata {
  return { indexDisplayNameMapping: undefined, displayNameMappingKey: undefined, outputTensorName: "" };
}

export const ExplanationMetadata_OutputMetadata: MessageFns<ExplanationMetadata_OutputMetadata> = {
  encode(message: ExplanationMetadata_OutputMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.indexDisplayNameMapping !== undefined) {
      Value.encode(Value.wrap(message.indexDisplayNameMapping), writer.uint32(10).fork()).join();
    }
    if (message.displayNameMappingKey !== undefined) {
      writer.uint32(18).string(message.displayNameMappingKey);
    }
    if (message.outputTensorName !== "") {
      writer.uint32(26).string(message.outputTensorName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplanationMetadata_OutputMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplanationMetadata_OutputMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.indexDisplayNameMapping = Value.unwrap(Value.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.displayNameMappingKey = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.outputTensorName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplanationMetadata_OutputMetadata {
    return {
      indexDisplayNameMapping: isSet(object?.indexDisplayNameMapping) ? object.indexDisplayNameMapping : undefined,
      displayNameMappingKey: isSet(object.displayNameMappingKey)
        ? globalThis.String(object.displayNameMappingKey)
        : undefined,
      outputTensorName: isSet(object.outputTensorName) ? globalThis.String(object.outputTensorName) : "",
    };
  },

  toJSON(message: ExplanationMetadata_OutputMetadata): unknown {
    const obj: any = {};
    if (message.indexDisplayNameMapping !== undefined) {
      obj.indexDisplayNameMapping = message.indexDisplayNameMapping;
    }
    if (message.displayNameMappingKey !== undefined) {
      obj.displayNameMappingKey = message.displayNameMappingKey;
    }
    if (message.outputTensorName !== "") {
      obj.outputTensorName = message.outputTensorName;
    }
    return obj;
  },

  create(base?: DeepPartial<ExplanationMetadata_OutputMetadata>): ExplanationMetadata_OutputMetadata {
    return ExplanationMetadata_OutputMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplanationMetadata_OutputMetadata>): ExplanationMetadata_OutputMetadata {
    const message = createBaseExplanationMetadata_OutputMetadata();
    message.indexDisplayNameMapping = object.indexDisplayNameMapping ?? undefined;
    message.displayNameMappingKey = object.displayNameMappingKey ?? undefined;
    message.outputTensorName = object.outputTensorName ?? "";
    return message;
  },
};

function createBaseExplanationMetadata_InputsEntry(): ExplanationMetadata_InputsEntry {
  return { key: "", value: undefined };
}

export const ExplanationMetadata_InputsEntry: MessageFns<ExplanationMetadata_InputsEntry> = {
  encode(message: ExplanationMetadata_InputsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      ExplanationMetadata_InputMetadata.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplanationMetadata_InputsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplanationMetadata_InputsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = ExplanationMetadata_InputMetadata.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplanationMetadata_InputsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? ExplanationMetadata_InputMetadata.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: ExplanationMetadata_InputsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = ExplanationMetadata_InputMetadata.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<ExplanationMetadata_InputsEntry>): ExplanationMetadata_InputsEntry {
    return ExplanationMetadata_InputsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplanationMetadata_InputsEntry>): ExplanationMetadata_InputsEntry {
    const message = createBaseExplanationMetadata_InputsEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? ExplanationMetadata_InputMetadata.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseExplanationMetadata_OutputsEntry(): ExplanationMetadata_OutputsEntry {
  return { key: "", value: undefined };
}

export const ExplanationMetadata_OutputsEntry: MessageFns<ExplanationMetadata_OutputsEntry> = {
  encode(message: ExplanationMetadata_OutputsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      ExplanationMetadata_OutputMetadata.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplanationMetadata_OutputsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplanationMetadata_OutputsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = ExplanationMetadata_OutputMetadata.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplanationMetadata_OutputsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? ExplanationMetadata_OutputMetadata.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: ExplanationMetadata_OutputsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = ExplanationMetadata_OutputMetadata.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<ExplanationMetadata_OutputsEntry>): ExplanationMetadata_OutputsEntry {
    return ExplanationMetadata_OutputsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplanationMetadata_OutputsEntry>): ExplanationMetadata_OutputsEntry {
    const message = createBaseExplanationMetadata_OutputsEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? ExplanationMetadata_OutputMetadata.fromPartial(object.value)
      : undefined;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
