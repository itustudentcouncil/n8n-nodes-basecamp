// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/aiplatform/v1/model.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Duration } from "../../../protobuf/duration.js";
import { Value } from "../../../protobuf/struct.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { DeployedModelRef } from "./deployed_model_ref.js";
import { EncryptionSpec } from "./encryption_spec.js";
import { EnvVar } from "./env_var.js";
import { ExplanationSpec } from "./explanation.js";

export const protobufPackage = "google.cloud.aiplatform.v1";

/** A trained machine learning Model. */
export interface Model {
  /** The resource name of the Model. */
  name: string;
  /**
   * Output only. Immutable. The version ID of the model.
   * A new version is committed when a new model version is uploaded or
   * trained under an existing model id. It is an auto-incrementing decimal
   * number in string representation.
   */
  versionId: string;
  /**
   * User provided version aliases so that a model version can be referenced via
   * alias (i.e.
   * `projects/{project}/locations/{location}/models/{model_id}@{version_alias}`
   * instead of auto-generated version id (i.e.
   * `projects/{project}/locations/{location}/models/{model_id}@{version_id})`.
   * The format is [a-z][a-zA-Z0-9-]{0,126}[a-z0-9] to distinguish from
   * version_id. A default version alias will be created for the first version
   * of the model, and there must be exactly one default version alias for a
   * model.
   */
  versionAliases: string[];
  /** Output only. Timestamp when this version was created. */
  versionCreateTime:
    | Date
    | undefined;
  /** Output only. Timestamp when this version was most recently updated. */
  versionUpdateTime:
    | Date
    | undefined;
  /**
   * Required. The display name of the Model.
   * The name can be up to 128 characters long and can consist of any UTF-8
   * characters.
   */
  displayName: string;
  /** The description of the Model. */
  description: string;
  /** The description of this version. */
  versionDescription: string;
  /**
   * The schemata that describe formats of the Model's predictions and
   * explanations as given and returned via
   * [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict]
   * and
   * [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain].
   */
  predictSchemata:
    | PredictSchemata
    | undefined;
  /**
   * Immutable. Points to a YAML file stored on Google Cloud Storage describing
   * additional information about the Model, that is specific to it. Unset if
   * the Model does not have any additional information. The schema is defined
   * as an OpenAPI 3.0.2 [Schema
   * Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject).
   * AutoML Models always have this field populated by Vertex AI, if no
   * additional metadata is needed, this field is set to an empty string.
   * Note: The URI given on output will be immutable and probably different,
   * including the URI scheme, than the one given on input. The output URI will
   * point to a location where the user only has a read access.
   */
  metadataSchemaUri: string;
  /**
   * Immutable. An additional information about the Model; the schema of the
   * metadata can be found in
   * [metadata_schema][google.cloud.aiplatform.v1.Model.metadata_schema_uri].
   * Unset if the Model does not have any additional information.
   */
  metadata:
    | any
    | undefined;
  /**
   * Output only. The formats in which this Model may be exported. If empty,
   * this Model is not available for export.
   */
  supportedExportFormats: Model_ExportFormat[];
  /**
   * Output only. The resource name of the TrainingPipeline that uploaded this
   * Model, if any.
   */
  trainingPipeline: string;
  /**
   * Optional. This field is populated if the model is produced by a pipeline
   * job.
   */
  pipelineJob: string;
  /**
   * Input only. The specification of the container that is to be used when
   * deploying this Model. The specification is ingested upon
   * [ModelService.UploadModel][google.cloud.aiplatform.v1.ModelService.UploadModel],
   * and all binaries it contains are copied and stored internally by Vertex AI.
   * Not required for AutoML Models.
   */
  containerSpec:
    | ModelContainerSpec
    | undefined;
  /**
   * Immutable. The path to the directory containing the Model artifact and any
   * of its supporting files. Not required for AutoML Models.
   */
  artifactUri: string;
  /**
   * Output only. When this Model is deployed, its prediction resources are
   * described by the `prediction_resources` field of the
   * [Endpoint.deployed_models][google.cloud.aiplatform.v1.Endpoint.deployed_models]
   * object. Because not all Models support all resource configuration types,
   * the configuration types this Model supports are listed here. If no
   * configuration types are listed, the Model cannot be deployed to an
   * [Endpoint][google.cloud.aiplatform.v1.Endpoint] and does not support
   * online predictions
   * ([PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict]
   * or
   * [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain]).
   * Such a Model can serve predictions by using a
   * [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob], if it
   * has at least one entry each in
   * [supported_input_storage_formats][google.cloud.aiplatform.v1.Model.supported_input_storage_formats]
   * and
   * [supported_output_storage_formats][google.cloud.aiplatform.v1.Model.supported_output_storage_formats].
   */
  supportedDeploymentResourcesTypes: Model_DeploymentResourcesType[];
  /**
   * Output only. The formats this Model supports in
   * [BatchPredictionJob.input_config][google.cloud.aiplatform.v1.BatchPredictionJob.input_config].
   * If
   * [PredictSchemata.instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
   * exists, the instances should be given as per that schema.
   *
   * The possible formats are:
   *
   * * `jsonl`
   * The JSON Lines format, where each instance is a single line. Uses
   * [GcsSource][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.gcs_source].
   *
   * * `csv`
   * The CSV format, where each instance is a single comma-separated line.
   * The first line in the file is the header, containing comma-separated field
   * names. Uses
   * [GcsSource][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.gcs_source].
   *
   * * `tf-record`
   * The TFRecord format, where each instance is a single record in tfrecord
   * syntax. Uses
   * [GcsSource][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.gcs_source].
   *
   * * `tf-record-gzip`
   * Similar to `tf-record`, but the file is gzipped. Uses
   * [GcsSource][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.gcs_source].
   *
   * * `bigquery`
   * Each instance is a single row in BigQuery. Uses
   * [BigQuerySource][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.bigquery_source].
   *
   * * `file-list`
   * Each line of the file is the location of an instance to process, uses
   * `gcs_source` field of the
   * [InputConfig][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig]
   * object.
   *
   * If this Model doesn't support any of these formats it means it cannot be
   * used with a
   * [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob].
   * However, if it has
   * [supported_deployment_resources_types][google.cloud.aiplatform.v1.Model.supported_deployment_resources_types],
   * it could serve online predictions by using
   * [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict]
   * or
   * [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain].
   */
  supportedInputStorageFormats: string[];
  /**
   * Output only. The formats this Model supports in
   * [BatchPredictionJob.output_config][google.cloud.aiplatform.v1.BatchPredictionJob.output_config].
   * If both
   * [PredictSchemata.instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
   * and
   * [PredictSchemata.prediction_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.prediction_schema_uri]
   * exist, the predictions are returned together with their instances. In other
   * words, the prediction has the original instance data first, followed by the
   * actual prediction content (as per the schema).
   *
   * The possible formats are:
   *
   * * `jsonl`
   * The JSON Lines format, where each prediction is a single line. Uses
   * [GcsDestination][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.gcs_destination].
   *
   * * `csv`
   * The CSV format, where each prediction is a single comma-separated line.
   * The first line in the file is the header, containing comma-separated field
   * names. Uses
   * [GcsDestination][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.gcs_destination].
   *
   * * `bigquery`
   * Each prediction is a single row in a BigQuery table, uses
   * [BigQueryDestination][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.bigquery_destination]
   * .
   *
   * If this Model doesn't support any of these formats it means it cannot be
   * used with a
   * [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob].
   * However, if it has
   * [supported_deployment_resources_types][google.cloud.aiplatform.v1.Model.supported_deployment_resources_types],
   * it could serve online predictions by using
   * [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict]
   * or
   * [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain].
   */
  supportedOutputStorageFormats: string[];
  /** Output only. Timestamp when this Model was uploaded into Vertex AI. */
  createTime:
    | Date
    | undefined;
  /** Output only. Timestamp when this Model was most recently updated. */
  updateTime:
    | Date
    | undefined;
  /**
   * Output only. The pointers to DeployedModels created from this Model. Note
   * that Model could have been deployed to Endpoints in different Locations.
   */
  deployedModels: DeployedModelRef[];
  /**
   * The default explanation specification for this Model.
   *
   * The Model can be used for
   * [requesting
   * explanation][google.cloud.aiplatform.v1.PredictionService.Explain] after
   * being [deployed][google.cloud.aiplatform.v1.EndpointService.DeployModel] if
   * it is populated. The Model can be used for [batch
   * explanation][google.cloud.aiplatform.v1.BatchPredictionJob.generate_explanation]
   * if it is populated.
   *
   * All fields of the explanation_spec can be overridden by
   * [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
   * of
   * [DeployModelRequest.deployed_model][google.cloud.aiplatform.v1.DeployModelRequest.deployed_model],
   * or
   * [explanation_spec][google.cloud.aiplatform.v1.BatchPredictionJob.explanation_spec]
   * of [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob].
   *
   * If the default explanation specification is not set for this Model, this
   * Model can still be used for
   * [requesting
   * explanation][google.cloud.aiplatform.v1.PredictionService.Explain] by
   * setting
   * [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
   * of
   * [DeployModelRequest.deployed_model][google.cloud.aiplatform.v1.DeployModelRequest.deployed_model]
   * and for [batch
   * explanation][google.cloud.aiplatform.v1.BatchPredictionJob.generate_explanation]
   * by setting
   * [explanation_spec][google.cloud.aiplatform.v1.BatchPredictionJob.explanation_spec]
   * of [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob].
   */
  explanationSpec:
    | ExplanationSpec
    | undefined;
  /**
   * Used to perform consistent read-modify-write updates. If not set, a blind
   * "overwrite" update happens.
   */
  etag: string;
  /**
   * The labels with user-defined metadata to organize your Models.
   *
   * Label keys and values can be no longer than 64 characters
   * (Unicode codepoints), can only contain lowercase letters, numeric
   * characters, underscores and dashes. International characters are allowed.
   *
   * See https://goo.gl/xmQnxf for more information and examples of labels.
   */
  labels: { [key: string]: string };
  /**
   * Stats of data used for training or evaluating the Model.
   *
   * Only populated when the Model is trained by a TrainingPipeline with
   * [data_input_config][TrainingPipeline.data_input_config].
   */
  dataStats:
    | Model_DataStats
    | undefined;
  /**
   * Customer-managed encryption key spec for a Model. If set, this
   * Model and all sub-resources of this Model will be secured by this key.
   */
  encryptionSpec:
    | EncryptionSpec
    | undefined;
  /**
   * Output only. Source of a model. It can either be automl training pipeline,
   * custom training pipeline, BigQuery ML, or saved and tuned from Genie or
   * Model Garden.
   */
  modelSourceInfo:
    | ModelSourceInfo
    | undefined;
  /**
   * Output only. If this Model is a copy of another Model, this contains info
   * about the original.
   */
  originalModelInfo:
    | Model_OriginalModelInfo
    | undefined;
  /**
   * Output only. The resource name of the Artifact that was created in
   * MetadataStore when creating the Model. The Artifact resource name pattern
   * is
   * `projects/{project}/locations/{location}/metadataStores/{metadata_store}/artifacts/{artifact}`.
   */
  metadataArtifact: string;
  /**
   * Optional. User input field to specify the base model source. Currently it
   * only supports specifing the Model Garden models and Genie models.
   */
  baseModelSource:
    | Model_BaseModelSource
    | undefined;
  /** Output only. Reserved for future use. */
  satisfiesPzs: boolean;
  /** Output only. Reserved for future use. */
  satisfiesPzi: boolean;
}

/** Identifies a type of Model's prediction resources. */
export enum Model_DeploymentResourcesType {
  /** DEPLOYMENT_RESOURCES_TYPE_UNSPECIFIED - Should not be used. */
  DEPLOYMENT_RESOURCES_TYPE_UNSPECIFIED = 0,
  /**
   * DEDICATED_RESOURCES - Resources that are dedicated to the
   * [DeployedModel][google.cloud.aiplatform.v1.DeployedModel], and that need
   * a higher degree of manual configuration.
   */
  DEDICATED_RESOURCES = 1,
  /**
   * AUTOMATIC_RESOURCES - Resources that to large degree are decided by Vertex AI, and require
   * only a modest additional configuration.
   */
  AUTOMATIC_RESOURCES = 2,
  /**
   * SHARED_RESOURCES - Resources that can be shared by multiple
   * [DeployedModels][google.cloud.aiplatform.v1.DeployedModel]. A
   * pre-configured
   * [DeploymentResourcePool][google.cloud.aiplatform.v1.DeploymentResourcePool]
   * is required.
   */
  SHARED_RESOURCES = 3,
  UNRECOGNIZED = -1,
}

export function model_DeploymentResourcesTypeFromJSON(object: any): Model_DeploymentResourcesType {
  switch (object) {
    case 0:
    case "DEPLOYMENT_RESOURCES_TYPE_UNSPECIFIED":
      return Model_DeploymentResourcesType.DEPLOYMENT_RESOURCES_TYPE_UNSPECIFIED;
    case 1:
    case "DEDICATED_RESOURCES":
      return Model_DeploymentResourcesType.DEDICATED_RESOURCES;
    case 2:
    case "AUTOMATIC_RESOURCES":
      return Model_DeploymentResourcesType.AUTOMATIC_RESOURCES;
    case 3:
    case "SHARED_RESOURCES":
      return Model_DeploymentResourcesType.SHARED_RESOURCES;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Model_DeploymentResourcesType.UNRECOGNIZED;
  }
}

export function model_DeploymentResourcesTypeToJSON(object: Model_DeploymentResourcesType): string {
  switch (object) {
    case Model_DeploymentResourcesType.DEPLOYMENT_RESOURCES_TYPE_UNSPECIFIED:
      return "DEPLOYMENT_RESOURCES_TYPE_UNSPECIFIED";
    case Model_DeploymentResourcesType.DEDICATED_RESOURCES:
      return "DEDICATED_RESOURCES";
    case Model_DeploymentResourcesType.AUTOMATIC_RESOURCES:
      return "AUTOMATIC_RESOURCES";
    case Model_DeploymentResourcesType.SHARED_RESOURCES:
      return "SHARED_RESOURCES";
    case Model_DeploymentResourcesType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Represents export format supported by the Model.
 * All formats export to Google Cloud Storage.
 */
export interface Model_ExportFormat {
  /**
   * Output only. The ID of the export format.
   * The possible format IDs are:
   *
   * * `tflite`
   * Used for Android mobile devices.
   *
   * * `edgetpu-tflite`
   * Used for [Edge TPU](https://cloud.google.com/edge-tpu/) devices.
   *
   * * `tf-saved-model`
   * A tensorflow model in SavedModel format.
   *
   * * `tf-js`
   * A [TensorFlow.js](https://www.tensorflow.org/js) model that can be used
   * in the browser and in Node.js using JavaScript.
   *
   * * `core-ml`
   * Used for iOS mobile devices.
   *
   * * `custom-trained`
   * A Model that was uploaded or trained by custom code.
   */
  id: string;
  /** Output only. The content of this Model that may be exported. */
  exportableContents: Model_ExportFormat_ExportableContent[];
}

/** The Model content that can be exported. */
export enum Model_ExportFormat_ExportableContent {
  /** EXPORTABLE_CONTENT_UNSPECIFIED - Should not be used. */
  EXPORTABLE_CONTENT_UNSPECIFIED = 0,
  /**
   * ARTIFACT - Model artifact and any of its supported files. Will be exported to the
   * location specified by the `artifactDestination` field of the
   * [ExportModelRequest.output_config][google.cloud.aiplatform.v1.ExportModelRequest.output_config]
   * object.
   */
  ARTIFACT = 1,
  /**
   * IMAGE - The container image that is to be used when deploying this Model. Will
   * be exported to the location specified by the `imageDestination` field
   * of the
   * [ExportModelRequest.output_config][google.cloud.aiplatform.v1.ExportModelRequest.output_config]
   * object.
   */
  IMAGE = 2,
  UNRECOGNIZED = -1,
}

export function model_ExportFormat_ExportableContentFromJSON(object: any): Model_ExportFormat_ExportableContent {
  switch (object) {
    case 0:
    case "EXPORTABLE_CONTENT_UNSPECIFIED":
      return Model_ExportFormat_ExportableContent.EXPORTABLE_CONTENT_UNSPECIFIED;
    case 1:
    case "ARTIFACT":
      return Model_ExportFormat_ExportableContent.ARTIFACT;
    case 2:
    case "IMAGE":
      return Model_ExportFormat_ExportableContent.IMAGE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Model_ExportFormat_ExportableContent.UNRECOGNIZED;
  }
}

export function model_ExportFormat_ExportableContentToJSON(object: Model_ExportFormat_ExportableContent): string {
  switch (object) {
    case Model_ExportFormat_ExportableContent.EXPORTABLE_CONTENT_UNSPECIFIED:
      return "EXPORTABLE_CONTENT_UNSPECIFIED";
    case Model_ExportFormat_ExportableContent.ARTIFACT:
      return "ARTIFACT";
    case Model_ExportFormat_ExportableContent.IMAGE:
      return "IMAGE";
    case Model_ExportFormat_ExportableContent.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Stats of data used for train or evaluate the Model. */
export interface Model_DataStats {
  /** Number of DataItems that were used for training this Model. */
  trainingDataItemsCount: Long;
  /**
   * Number of DataItems that were used for validating this Model during
   * training.
   */
  validationDataItemsCount: Long;
  /**
   * Number of DataItems that were used for evaluating this Model. If the
   * Model is evaluated multiple times, this will be the number of test
   * DataItems used by the first evaluation. If the Model is not evaluated,
   * the number is 0.
   */
  testDataItemsCount: Long;
  /** Number of Annotations that are used for training this Model. */
  trainingAnnotationsCount: Long;
  /**
   * Number of Annotations that are used for validating this Model during
   * training.
   */
  validationAnnotationsCount: Long;
  /**
   * Number of Annotations that are used for evaluating this Model. If the
   * Model is evaluated multiple times, this will be the number of test
   * Annotations used by the first evaluation. If the Model is not evaluated,
   * the number is 0.
   */
  testAnnotationsCount: Long;
}

/** Contains information about the original Model if this Model is a copy. */
export interface Model_OriginalModelInfo {
  /**
   * Output only. The resource name of the Model this Model is a copy of,
   * including the revision. Format:
   * `projects/{project}/locations/{location}/models/{model_id}@{version_id}`
   */
  model: string;
}

/**
 * User input field to specify the base model source. Currently it only
 * supports specifing the Model Garden models and Genie models.
 */
export interface Model_BaseModelSource {
  /** Source information of Model Garden models. */
  modelGardenSource?:
    | ModelGardenSource
    | undefined;
  /** Information about the base model of Genie models. */
  genieSource?: GenieSource | undefined;
}

export interface Model_LabelsEntry {
  key: string;
  value: string;
}

/** Contains information about the Large Model. */
export interface LargeModelReference {
  /**
   * Required. The unique name of the large Foundation or pre-built model. Like
   * "chat-bison", "text-bison". Or model name with version ID, like
   * "chat-bison@001", "text-bison@005", etc.
   */
  name: string;
}

/**
 * Contains information about the source of the models generated from Model
 * Garden.
 */
export interface ModelGardenSource {
  /** Required. The model garden source model resource name. */
  publicModelName: string;
}

/**
 * Contains information about the source of the models generated from Generative
 * AI Studio.
 */
export interface GenieSource {
  /** Required. The public base model URI. */
  baseModelUri: string;
}

/**
 * Contains the schemata used in Model's predictions and explanations via
 * [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict],
 * [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain]
 * and [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob].
 */
export interface PredictSchemata {
  /**
   * Immutable. Points to a YAML file stored on Google Cloud Storage describing
   * the format of a single instance, which are used in
   * [PredictRequest.instances][google.cloud.aiplatform.v1.PredictRequest.instances],
   * [ExplainRequest.instances][google.cloud.aiplatform.v1.ExplainRequest.instances]
   * and
   * [BatchPredictionJob.input_config][google.cloud.aiplatform.v1.BatchPredictionJob.input_config].
   * The schema is defined as an OpenAPI 3.0.2 [Schema
   * Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject).
   * AutoML Models always have this field populated by Vertex AI.
   * Note: The URI given on output will be immutable and probably different,
   * including the URI scheme, than the one given on input. The output URI will
   * point to a location where the user only has a read access.
   */
  instanceSchemaUri: string;
  /**
   * Immutable. Points to a YAML file stored on Google Cloud Storage describing
   * the parameters of prediction and explanation via
   * [PredictRequest.parameters][google.cloud.aiplatform.v1.PredictRequest.parameters],
   * [ExplainRequest.parameters][google.cloud.aiplatform.v1.ExplainRequest.parameters]
   * and
   * [BatchPredictionJob.model_parameters][google.cloud.aiplatform.v1.BatchPredictionJob.model_parameters].
   * The schema is defined as an OpenAPI 3.0.2 [Schema
   * Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject).
   * AutoML Models always have this field populated by Vertex AI, if no
   * parameters are supported, then it is set to an empty string.
   * Note: The URI given on output will be immutable and probably different,
   * including the URI scheme, than the one given on input. The output URI will
   * point to a location where the user only has a read access.
   */
  parametersSchemaUri: string;
  /**
   * Immutable. Points to a YAML file stored on Google Cloud Storage describing
   * the format of a single prediction produced by this Model, which are
   * returned via
   * [PredictResponse.predictions][google.cloud.aiplatform.v1.PredictResponse.predictions],
   * [ExplainResponse.explanations][google.cloud.aiplatform.v1.ExplainResponse.explanations],
   * and
   * [BatchPredictionJob.output_config][google.cloud.aiplatform.v1.BatchPredictionJob.output_config].
   * The schema is defined as an OpenAPI 3.0.2 [Schema
   * Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject).
   * AutoML Models always have this field populated by Vertex AI.
   * Note: The URI given on output will be immutable and probably different,
   * including the URI scheme, than the one given on input. The output URI will
   * point to a location where the user only has a read access.
   */
  predictionSchemaUri: string;
}

/**
 * Specification of a container for serving predictions. Some fields in this
 * message correspond to fields in the [Kubernetes Container v1 core
 * specification](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
 */
export interface ModelContainerSpec {
  /**
   * Required. Immutable. URI of the Docker image to be used as the custom
   * container for serving predictions. This URI must identify an image in
   * Artifact Registry or Container Registry. Learn more about the [container
   * publishing
   * requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#publishing),
   * including permissions requirements for the Vertex AI Service Agent.
   *
   * The container image is ingested upon
   * [ModelService.UploadModel][google.cloud.aiplatform.v1.ModelService.UploadModel],
   * stored internally, and this original path is afterwards not used.
   *
   * To learn about the requirements for the Docker image itself, see
   * [Custom container
   * requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#).
   *
   * You can use the URI to one of Vertex AI's [pre-built container images for
   * prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)
   * in this field.
   */
  imageUri: string;
  /**
   * Immutable. Specifies the command that runs when the container starts. This
   * overrides the container's
   * [ENTRYPOINT](https://docs.docker.com/engine/reference/builder/#entrypoint).
   * Specify this field as an array of executable and arguments, similar to a
   * Docker `ENTRYPOINT`'s "exec" form, not its "shell" form.
   *
   * If you do not specify this field, then the container's `ENTRYPOINT` runs,
   * in conjunction with the
   * [args][google.cloud.aiplatform.v1.ModelContainerSpec.args] field or the
   * container's [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd),
   * if either exists. If this field is not specified and the container does not
   * have an `ENTRYPOINT`, then refer to the Docker documentation about [how
   * `CMD` and `ENTRYPOINT`
   * interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact).
   *
   * If you specify this field, then you can also specify the `args` field to
   * provide additional arguments for this command. However, if you specify this
   * field, then the container's `CMD` is ignored. See the
   * [Kubernetes documentation about how the
   * `command` and `args` fields interact with a container's `ENTRYPOINT` and
   * `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes).
   *
   * In this field, you can reference [environment variables set by Vertex
   * AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables)
   * and environment variables set in the
   * [env][google.cloud.aiplatform.v1.ModelContainerSpec.env] field. You cannot
   * reference environment variables set in the Docker image. In order for
   * environment variables to be expanded, reference them by using the following
   * syntax: <code>$(<var>VARIABLE_NAME</var>)</code> Note that this differs
   * from Bash variable expansion, which does not use parentheses. If a variable
   * cannot be resolved, the reference in the input string is used unchanged. To
   * avoid variable expansion, you can escape this syntax with `$$`; for
   * example: <code>$$(<var>VARIABLE_NAME</var>)</code> This field corresponds
   * to the `command` field of the Kubernetes Containers [v1 core
   * API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
   */
  command: string[];
  /**
   * Immutable. Specifies arguments for the command that runs when the container
   * starts. This overrides the container's
   * [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd). Specify
   * this field as an array of executable and arguments, similar to a Docker
   * `CMD`'s "default parameters" form.
   *
   * If you don't specify this field but do specify the
   * [command][google.cloud.aiplatform.v1.ModelContainerSpec.command] field,
   * then the command from the `command` field runs without any additional
   * arguments. See the [Kubernetes documentation about how the `command` and
   * `args` fields interact with a container's `ENTRYPOINT` and
   * `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes).
   *
   * If you don't specify this field and don't specify the `command` field,
   * then the container's
   * [`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#cmd) and
   * `CMD` determine what runs based on their default behavior. See the Docker
   * documentation about [how `CMD` and `ENTRYPOINT`
   * interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact).
   *
   * In this field, you can reference [environment variables
   * set by Vertex
   * AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables)
   * and environment variables set in the
   * [env][google.cloud.aiplatform.v1.ModelContainerSpec.env] field. You cannot
   * reference environment variables set in the Docker image. In order for
   * environment variables to be expanded, reference them by using the following
   * syntax: <code>$(<var>VARIABLE_NAME</var>)</code> Note that this differs
   * from Bash variable expansion, which does not use parentheses. If a variable
   * cannot be resolved, the reference in the input string is used unchanged. To
   * avoid variable expansion, you can escape this syntax with `$$`; for
   * example: <code>$$(<var>VARIABLE_NAME</var>)</code> This field corresponds
   * to the `args` field of the Kubernetes Containers [v1 core
   * API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
   */
  args: string[];
  /**
   * Immutable. List of environment variables to set in the container. After the
   * container starts running, code running in the container can read these
   * environment variables.
   *
   * Additionally, the
   * [command][google.cloud.aiplatform.v1.ModelContainerSpec.command] and
   * [args][google.cloud.aiplatform.v1.ModelContainerSpec.args] fields can
   * reference these variables. Later entries in this list can also reference
   * earlier entries. For example, the following example sets the variable
   * `VAR_2` to have the value `foo bar`:
   *
   * ```json
   * [
   *   {
   *     "name": "VAR_1",
   *     "value": "foo"
   *   },
   *   {
   *     "name": "VAR_2",
   *     "value": "$(VAR_1) bar"
   *   }
   * ]
   * ```
   *
   * If you switch the order of the variables in the example, then the expansion
   * does not occur.
   *
   * This field corresponds to the `env` field of the Kubernetes Containers
   * [v1 core
   * API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
   */
  env: EnvVar[];
  /**
   * Immutable. List of ports to expose from the container. Vertex AI sends any
   * prediction requests that it receives to the first port on this list. Vertex
   * AI also sends
   * [liveness and health
   * checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#liveness)
   * to this port.
   *
   * If you do not specify this field, it defaults to following value:
   *
   * ```json
   * [
   *   {
   *     "containerPort": 8080
   *   }
   * ]
   * ```
   *
   * Vertex AI does not use ports other than the first one listed. This field
   * corresponds to the `ports` field of the Kubernetes Containers
   * [v1 core
   * API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
   */
  ports: Port[];
  /**
   * Immutable. HTTP path on the container to send prediction requests to.
   * Vertex AI forwards requests sent using
   * [projects.locations.endpoints.predict][google.cloud.aiplatform.v1.PredictionService.Predict]
   * to this path on the container's IP address and port. Vertex AI then returns
   * the container's response in the API response.
   *
   * For example, if you set this field to `/foo`, then when Vertex AI
   * receives a prediction request, it forwards the request body in a POST
   * request to the `/foo` path on the port of your container specified by the
   * first value of this `ModelContainerSpec`'s
   * [ports][google.cloud.aiplatform.v1.ModelContainerSpec.ports] field.
   *
   * If you don't specify this field, it defaults to the following value when
   * you [deploy this Model to an
   * Endpoint][google.cloud.aiplatform.v1.EndpointService.DeployModel]:
   * <code>/v1/endpoints/<var>ENDPOINT</var>/deployedModels/<var>DEPLOYED_MODEL</var>:predict</code>
   * The placeholders in this value are replaced as follows:
   *
   * * <var>ENDPOINT</var>: The last segment (following `endpoints/`)of the
   *   Endpoint.name][] field of the Endpoint where this Model has been
   *   deployed. (Vertex AI makes this value available to your container code
   *   as the [`AIP_ENDPOINT_ID` environment
   *  variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
   *
   * * <var>DEPLOYED_MODEL</var>:
   * [DeployedModel.id][google.cloud.aiplatform.v1.DeployedModel.id] of the
   * `DeployedModel`.
   *   (Vertex AI makes this value available to your container code
   *   as the [`AIP_DEPLOYED_MODEL_ID` environment
   *   variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
   */
  predictRoute: string;
  /**
   * Immutable. HTTP path on the container to send health checks to. Vertex AI
   * intermittently sends GET requests to this path on the container's IP
   * address and port to check that the container is healthy. Read more about
   * [health
   * checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#health).
   *
   * For example, if you set this field to `/bar`, then Vertex AI
   * intermittently sends a GET request to the `/bar` path on the port of your
   * container specified by the first value of this `ModelContainerSpec`'s
   * [ports][google.cloud.aiplatform.v1.ModelContainerSpec.ports] field.
   *
   * If you don't specify this field, it defaults to the following value when
   * you [deploy this Model to an
   * Endpoint][google.cloud.aiplatform.v1.EndpointService.DeployModel]:
   * <code>/v1/endpoints/<var>ENDPOINT</var>/deployedModels/<var>DEPLOYED_MODEL</var>:predict</code>
   * The placeholders in this value are replaced as follows:
   *
   * * <var>ENDPOINT</var>: The last segment (following `endpoints/`)of the
   *   Endpoint.name][] field of the Endpoint where this Model has been
   *   deployed. (Vertex AI makes this value available to your container code
   *   as the [`AIP_ENDPOINT_ID` environment
   *   variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
   *
   * * <var>DEPLOYED_MODEL</var>:
   * [DeployedModel.id][google.cloud.aiplatform.v1.DeployedModel.id] of the
   * `DeployedModel`.
   *   (Vertex AI makes this value available to your container code as the
   *   [`AIP_DEPLOYED_MODEL_ID` environment
   *   variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
   */
  healthRoute: string;
  /**
   * Immutable. List of ports to expose from the container. Vertex AI sends gRPC
   * prediction requests that it receives to the first port on this list. Vertex
   * AI also sends liveness and health checks to this port.
   *
   * If you do not specify this field, gRPC requests to the container will be
   * disabled.
   *
   * Vertex AI does not use ports other than the first one listed. This field
   * corresponds to the `ports` field of the Kubernetes Containers v1 core API.
   */
  grpcPorts: Port[];
  /**
   * Immutable. Deployment timeout.
   * Limit for deployment timeout is 2 hours.
   */
  deploymentTimeout:
    | Duration
    | undefined;
  /**
   * Immutable. The amount of the VM memory to reserve as the shared memory for
   * the model in megabytes.
   */
  sharedMemorySizeMb: Long;
  /** Immutable. Specification for Kubernetes startup probe. */
  startupProbe:
    | Probe
    | undefined;
  /** Immutable. Specification for Kubernetes readiness probe. */
  healthProbe: Probe | undefined;
}

/** Represents a network port in a container. */
export interface Port {
  /**
   * The number of the port to expose on the pod's IP address.
   * Must be a valid port number, between 1 and 65535 inclusive.
   */
  containerPort: number;
}

/** Detail description of the source information of the model. */
export interface ModelSourceInfo {
  /** Type of the model source. */
  sourceType: ModelSourceInfo_ModelSourceType;
  /**
   * If this Model is copy of another Model. If true then
   * [source_type][google.cloud.aiplatform.v1.ModelSourceInfo.source_type]
   * pertains to the original.
   */
  copy: boolean;
}

/**
 * Source of the model.
 * Different from `objective` field, this `ModelSourceType` enum
 * indicates the source from which the model was accessed or obtained,
 * whereas the `objective` indicates the overall aim or function of this
 * model.
 */
export enum ModelSourceInfo_ModelSourceType {
  /** MODEL_SOURCE_TYPE_UNSPECIFIED - Should not be used. */
  MODEL_SOURCE_TYPE_UNSPECIFIED = 0,
  /** AUTOML - The Model is uploaded by automl training pipeline. */
  AUTOML = 1,
  /** CUSTOM - The Model is uploaded by user or custom training pipeline. */
  CUSTOM = 2,
  /** BQML - The Model is registered and sync'ed from BigQuery ML. */
  BQML = 3,
  /** MODEL_GARDEN - The Model is saved or tuned from Model Garden. */
  MODEL_GARDEN = 4,
  /** GENIE - The Model is saved or tuned from Genie. */
  GENIE = 5,
  /** CUSTOM_TEXT_EMBEDDING - The Model is uploaded by text embedding finetuning pipeline. */
  CUSTOM_TEXT_EMBEDDING = 6,
  /** MARKETPLACE - The Model is saved or tuned from Marketplace. */
  MARKETPLACE = 7,
  UNRECOGNIZED = -1,
}

export function modelSourceInfo_ModelSourceTypeFromJSON(object: any): ModelSourceInfo_ModelSourceType {
  switch (object) {
    case 0:
    case "MODEL_SOURCE_TYPE_UNSPECIFIED":
      return ModelSourceInfo_ModelSourceType.MODEL_SOURCE_TYPE_UNSPECIFIED;
    case 1:
    case "AUTOML":
      return ModelSourceInfo_ModelSourceType.AUTOML;
    case 2:
    case "CUSTOM":
      return ModelSourceInfo_ModelSourceType.CUSTOM;
    case 3:
    case "BQML":
      return ModelSourceInfo_ModelSourceType.BQML;
    case 4:
    case "MODEL_GARDEN":
      return ModelSourceInfo_ModelSourceType.MODEL_GARDEN;
    case 5:
    case "GENIE":
      return ModelSourceInfo_ModelSourceType.GENIE;
    case 6:
    case "CUSTOM_TEXT_EMBEDDING":
      return ModelSourceInfo_ModelSourceType.CUSTOM_TEXT_EMBEDDING;
    case 7:
    case "MARKETPLACE":
      return ModelSourceInfo_ModelSourceType.MARKETPLACE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ModelSourceInfo_ModelSourceType.UNRECOGNIZED;
  }
}

export function modelSourceInfo_ModelSourceTypeToJSON(object: ModelSourceInfo_ModelSourceType): string {
  switch (object) {
    case ModelSourceInfo_ModelSourceType.MODEL_SOURCE_TYPE_UNSPECIFIED:
      return "MODEL_SOURCE_TYPE_UNSPECIFIED";
    case ModelSourceInfo_ModelSourceType.AUTOML:
      return "AUTOML";
    case ModelSourceInfo_ModelSourceType.CUSTOM:
      return "CUSTOM";
    case ModelSourceInfo_ModelSourceType.BQML:
      return "BQML";
    case ModelSourceInfo_ModelSourceType.MODEL_GARDEN:
      return "MODEL_GARDEN";
    case ModelSourceInfo_ModelSourceType.GENIE:
      return "GENIE";
    case ModelSourceInfo_ModelSourceType.CUSTOM_TEXT_EMBEDDING:
      return "CUSTOM_TEXT_EMBEDDING";
    case ModelSourceInfo_ModelSourceType.MARKETPLACE:
      return "MARKETPLACE";
    case ModelSourceInfo_ModelSourceType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Probe describes a health check to be performed against a container to
 * determine whether it is alive or ready to receive traffic.
 */
export interface Probe {
  /** ExecAction probes the health of a container by executing a command. */
  exec?:
    | Probe_ExecAction
    | undefined;
  /**
   * How often (in seconds) to perform the probe. Default to 10 seconds.
   * Minimum value is 1. Must be less than timeout_seconds.
   *
   * Maps to Kubernetes probe argument 'periodSeconds'.
   */
  periodSeconds: number;
  /**
   * Number of seconds after which the probe times out. Defaults to 1 second.
   * Minimum value is 1. Must be greater or equal to period_seconds.
   *
   * Maps to Kubernetes probe argument 'timeoutSeconds'.
   */
  timeoutSeconds: number;
}

/** ExecAction specifies a command to execute. */
export interface Probe_ExecAction {
  /**
   * Command is the command line to execute inside the container, the working
   * directory for the command is root ('/') in the container's filesystem.
   * The command is simply exec'd, it is not run inside a shell, so
   * traditional shell instructions ('|', etc) won't work. To use a shell, you
   * need to explicitly call out to that shell. Exit status of 0 is treated as
   * live/healthy and non-zero is unhealthy.
   */
  command: string[];
}

function createBaseModel(): Model {
  return {
    name: "",
    versionId: "",
    versionAliases: [],
    versionCreateTime: undefined,
    versionUpdateTime: undefined,
    displayName: "",
    description: "",
    versionDescription: "",
    predictSchemata: undefined,
    metadataSchemaUri: "",
    metadata: undefined,
    supportedExportFormats: [],
    trainingPipeline: "",
    pipelineJob: "",
    containerSpec: undefined,
    artifactUri: "",
    supportedDeploymentResourcesTypes: [],
    supportedInputStorageFormats: [],
    supportedOutputStorageFormats: [],
    createTime: undefined,
    updateTime: undefined,
    deployedModels: [],
    explanationSpec: undefined,
    etag: "",
    labels: {},
    dataStats: undefined,
    encryptionSpec: undefined,
    modelSourceInfo: undefined,
    originalModelInfo: undefined,
    metadataArtifact: "",
    baseModelSource: undefined,
    satisfiesPzs: false,
    satisfiesPzi: false,
  };
}

export const Model: MessageFns<Model> = {
  encode(message: Model, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.versionId !== "") {
      writer.uint32(226).string(message.versionId);
    }
    for (const v of message.versionAliases) {
      writer.uint32(234).string(v!);
    }
    if (message.versionCreateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.versionCreateTime), writer.uint32(250).fork()).join();
    }
    if (message.versionUpdateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.versionUpdateTime), writer.uint32(258).fork()).join();
    }
    if (message.displayName !== "") {
      writer.uint32(18).string(message.displayName);
    }
    if (message.description !== "") {
      writer.uint32(26).string(message.description);
    }
    if (message.versionDescription !== "") {
      writer.uint32(242).string(message.versionDescription);
    }
    if (message.predictSchemata !== undefined) {
      PredictSchemata.encode(message.predictSchemata, writer.uint32(34).fork()).join();
    }
    if (message.metadataSchemaUri !== "") {
      writer.uint32(42).string(message.metadataSchemaUri);
    }
    if (message.metadata !== undefined) {
      Value.encode(Value.wrap(message.metadata), writer.uint32(50).fork()).join();
    }
    for (const v of message.supportedExportFormats) {
      Model_ExportFormat.encode(v!, writer.uint32(162).fork()).join();
    }
    if (message.trainingPipeline !== "") {
      writer.uint32(58).string(message.trainingPipeline);
    }
    if (message.pipelineJob !== "") {
      writer.uint32(378).string(message.pipelineJob);
    }
    if (message.containerSpec !== undefined) {
      ModelContainerSpec.encode(message.containerSpec, writer.uint32(74).fork()).join();
    }
    if (message.artifactUri !== "") {
      writer.uint32(210).string(message.artifactUri);
    }
    writer.uint32(82).fork();
    for (const v of message.supportedDeploymentResourcesTypes) {
      writer.int32(v);
    }
    writer.join();
    for (const v of message.supportedInputStorageFormats) {
      writer.uint32(90).string(v!);
    }
    for (const v of message.supportedOutputStorageFormats) {
      writer.uint32(98).string(v!);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(106).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(114).fork()).join();
    }
    for (const v of message.deployedModels) {
      DeployedModelRef.encode(v!, writer.uint32(122).fork()).join();
    }
    if (message.explanationSpec !== undefined) {
      ExplanationSpec.encode(message.explanationSpec, writer.uint32(186).fork()).join();
    }
    if (message.etag !== "") {
      writer.uint32(130).string(message.etag);
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      Model_LabelsEntry.encode({ key: key as any, value }, writer.uint32(138).fork()).join();
    });
    if (message.dataStats !== undefined) {
      Model_DataStats.encode(message.dataStats, writer.uint32(170).fork()).join();
    }
    if (message.encryptionSpec !== undefined) {
      EncryptionSpec.encode(message.encryptionSpec, writer.uint32(194).fork()).join();
    }
    if (message.modelSourceInfo !== undefined) {
      ModelSourceInfo.encode(message.modelSourceInfo, writer.uint32(306).fork()).join();
    }
    if (message.originalModelInfo !== undefined) {
      Model_OriginalModelInfo.encode(message.originalModelInfo, writer.uint32(274).fork()).join();
    }
    if (message.metadataArtifact !== "") {
      writer.uint32(354).string(message.metadataArtifact);
    }
    if (message.baseModelSource !== undefined) {
      Model_BaseModelSource.encode(message.baseModelSource, writer.uint32(402).fork()).join();
    }
    if (message.satisfiesPzs !== false) {
      writer.uint32(408).bool(message.satisfiesPzs);
    }
    if (message.satisfiesPzi !== false) {
      writer.uint32(416).bool(message.satisfiesPzi);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Model {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseModel();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 28:
          if (tag !== 226) {
            break;
          }

          message.versionId = reader.string();
          continue;
        case 29:
          if (tag !== 234) {
            break;
          }

          message.versionAliases.push(reader.string());
          continue;
        case 31:
          if (tag !== 250) {
            break;
          }

          message.versionCreateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 32:
          if (tag !== 258) {
            break;
          }

          message.versionUpdateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.description = reader.string();
          continue;
        case 30:
          if (tag !== 242) {
            break;
          }

          message.versionDescription = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.predictSchemata = PredictSchemata.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.metadataSchemaUri = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.metadata = Value.unwrap(Value.decode(reader, reader.uint32()));
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.supportedExportFormats.push(Model_ExportFormat.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.trainingPipeline = reader.string();
          continue;
        case 47:
          if (tag !== 378) {
            break;
          }

          message.pipelineJob = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.containerSpec = ModelContainerSpec.decode(reader, reader.uint32());
          continue;
        case 26:
          if (tag !== 210) {
            break;
          }

          message.artifactUri = reader.string();
          continue;
        case 10:
          if (tag === 80) {
            message.supportedDeploymentResourcesTypes.push(reader.int32() as any);

            continue;
          }

          if (tag === 82) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.supportedDeploymentResourcesTypes.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.supportedInputStorageFormats.push(reader.string());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.supportedOutputStorageFormats.push(reader.string());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.deployedModels.push(DeployedModelRef.decode(reader, reader.uint32()));
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.explanationSpec = ExplanationSpec.decode(reader, reader.uint32());
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.etag = reader.string();
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          const entry17 = Model_LabelsEntry.decode(reader, reader.uint32());
          if (entry17.value !== undefined) {
            message.labels[entry17.key] = entry17.value;
          }
          continue;
        case 21:
          if (tag !== 170) {
            break;
          }

          message.dataStats = Model_DataStats.decode(reader, reader.uint32());
          continue;
        case 24:
          if (tag !== 194) {
            break;
          }

          message.encryptionSpec = EncryptionSpec.decode(reader, reader.uint32());
          continue;
        case 38:
          if (tag !== 306) {
            break;
          }

          message.modelSourceInfo = ModelSourceInfo.decode(reader, reader.uint32());
          continue;
        case 34:
          if (tag !== 274) {
            break;
          }

          message.originalModelInfo = Model_OriginalModelInfo.decode(reader, reader.uint32());
          continue;
        case 44:
          if (tag !== 354) {
            break;
          }

          message.metadataArtifact = reader.string();
          continue;
        case 50:
          if (tag !== 402) {
            break;
          }

          message.baseModelSource = Model_BaseModelSource.decode(reader, reader.uint32());
          continue;
        case 51:
          if (tag !== 408) {
            break;
          }

          message.satisfiesPzs = reader.bool();
          continue;
        case 52:
          if (tag !== 416) {
            break;
          }

          message.satisfiesPzi = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Model {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      versionId: isSet(object.versionId) ? globalThis.String(object.versionId) : "",
      versionAliases: globalThis.Array.isArray(object?.versionAliases)
        ? object.versionAliases.map((e: any) => globalThis.String(e))
        : [],
      versionCreateTime: isSet(object.versionCreateTime) ? fromJsonTimestamp(object.versionCreateTime) : undefined,
      versionUpdateTime: isSet(object.versionUpdateTime) ? fromJsonTimestamp(object.versionUpdateTime) : undefined,
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      versionDescription: isSet(object.versionDescription) ? globalThis.String(object.versionDescription) : "",
      predictSchemata: isSet(object.predictSchemata) ? PredictSchemata.fromJSON(object.predictSchemata) : undefined,
      metadataSchemaUri: isSet(object.metadataSchemaUri) ? globalThis.String(object.metadataSchemaUri) : "",
      metadata: isSet(object?.metadata) ? object.metadata : undefined,
      supportedExportFormats: globalThis.Array.isArray(object?.supportedExportFormats)
        ? object.supportedExportFormats.map((e: any) => Model_ExportFormat.fromJSON(e))
        : [],
      trainingPipeline: isSet(object.trainingPipeline) ? globalThis.String(object.trainingPipeline) : "",
      pipelineJob: isSet(object.pipelineJob) ? globalThis.String(object.pipelineJob) : "",
      containerSpec: isSet(object.containerSpec) ? ModelContainerSpec.fromJSON(object.containerSpec) : undefined,
      artifactUri: isSet(object.artifactUri) ? globalThis.String(object.artifactUri) : "",
      supportedDeploymentResourcesTypes: globalThis.Array.isArray(object?.supportedDeploymentResourcesTypes)
        ? object.supportedDeploymentResourcesTypes.map((e: any) => model_DeploymentResourcesTypeFromJSON(e))
        : [],
      supportedInputStorageFormats: globalThis.Array.isArray(object?.supportedInputStorageFormats)
        ? object.supportedInputStorageFormats.map((e: any) => globalThis.String(e))
        : [],
      supportedOutputStorageFormats: globalThis.Array.isArray(object?.supportedOutputStorageFormats)
        ? object.supportedOutputStorageFormats.map((e: any) => globalThis.String(e))
        : [],
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      deployedModels: globalThis.Array.isArray(object?.deployedModels)
        ? object.deployedModels.map((e: any) => DeployedModelRef.fromJSON(e))
        : [],
      explanationSpec: isSet(object.explanationSpec) ? ExplanationSpec.fromJSON(object.explanationSpec) : undefined,
      etag: isSet(object.etag) ? globalThis.String(object.etag) : "",
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      dataStats: isSet(object.dataStats) ? Model_DataStats.fromJSON(object.dataStats) : undefined,
      encryptionSpec: isSet(object.encryptionSpec) ? EncryptionSpec.fromJSON(object.encryptionSpec) : undefined,
      modelSourceInfo: isSet(object.modelSourceInfo) ? ModelSourceInfo.fromJSON(object.modelSourceInfo) : undefined,
      originalModelInfo: isSet(object.originalModelInfo)
        ? Model_OriginalModelInfo.fromJSON(object.originalModelInfo)
        : undefined,
      metadataArtifact: isSet(object.metadataArtifact) ? globalThis.String(object.metadataArtifact) : "",
      baseModelSource: isSet(object.baseModelSource)
        ? Model_BaseModelSource.fromJSON(object.baseModelSource)
        : undefined,
      satisfiesPzs: isSet(object.satisfiesPzs) ? globalThis.Boolean(object.satisfiesPzs) : false,
      satisfiesPzi: isSet(object.satisfiesPzi) ? globalThis.Boolean(object.satisfiesPzi) : false,
    };
  },

  toJSON(message: Model): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.versionId !== "") {
      obj.versionId = message.versionId;
    }
    if (message.versionAliases?.length) {
      obj.versionAliases = message.versionAliases;
    }
    if (message.versionCreateTime !== undefined) {
      obj.versionCreateTime = message.versionCreateTime.toISOString();
    }
    if (message.versionUpdateTime !== undefined) {
      obj.versionUpdateTime = message.versionUpdateTime.toISOString();
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.versionDescription !== "") {
      obj.versionDescription = message.versionDescription;
    }
    if (message.predictSchemata !== undefined) {
      obj.predictSchemata = PredictSchemata.toJSON(message.predictSchemata);
    }
    if (message.metadataSchemaUri !== "") {
      obj.metadataSchemaUri = message.metadataSchemaUri;
    }
    if (message.metadata !== undefined) {
      obj.metadata = message.metadata;
    }
    if (message.supportedExportFormats?.length) {
      obj.supportedExportFormats = message.supportedExportFormats.map((e) => Model_ExportFormat.toJSON(e));
    }
    if (message.trainingPipeline !== "") {
      obj.trainingPipeline = message.trainingPipeline;
    }
    if (message.pipelineJob !== "") {
      obj.pipelineJob = message.pipelineJob;
    }
    if (message.containerSpec !== undefined) {
      obj.containerSpec = ModelContainerSpec.toJSON(message.containerSpec);
    }
    if (message.artifactUri !== "") {
      obj.artifactUri = message.artifactUri;
    }
    if (message.supportedDeploymentResourcesTypes?.length) {
      obj.supportedDeploymentResourcesTypes = message.supportedDeploymentResourcesTypes.map((e) =>
        model_DeploymentResourcesTypeToJSON(e)
      );
    }
    if (message.supportedInputStorageFormats?.length) {
      obj.supportedInputStorageFormats = message.supportedInputStorageFormats;
    }
    if (message.supportedOutputStorageFormats?.length) {
      obj.supportedOutputStorageFormats = message.supportedOutputStorageFormats;
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (message.deployedModels?.length) {
      obj.deployedModels = message.deployedModels.map((e) => DeployedModelRef.toJSON(e));
    }
    if (message.explanationSpec !== undefined) {
      obj.explanationSpec = ExplanationSpec.toJSON(message.explanationSpec);
    }
    if (message.etag !== "") {
      obj.etag = message.etag;
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.dataStats !== undefined) {
      obj.dataStats = Model_DataStats.toJSON(message.dataStats);
    }
    if (message.encryptionSpec !== undefined) {
      obj.encryptionSpec = EncryptionSpec.toJSON(message.encryptionSpec);
    }
    if (message.modelSourceInfo !== undefined) {
      obj.modelSourceInfo = ModelSourceInfo.toJSON(message.modelSourceInfo);
    }
    if (message.originalModelInfo !== undefined) {
      obj.originalModelInfo = Model_OriginalModelInfo.toJSON(message.originalModelInfo);
    }
    if (message.metadataArtifact !== "") {
      obj.metadataArtifact = message.metadataArtifact;
    }
    if (message.baseModelSource !== undefined) {
      obj.baseModelSource = Model_BaseModelSource.toJSON(message.baseModelSource);
    }
    if (message.satisfiesPzs !== false) {
      obj.satisfiesPzs = message.satisfiesPzs;
    }
    if (message.satisfiesPzi !== false) {
      obj.satisfiesPzi = message.satisfiesPzi;
    }
    return obj;
  },

  create(base?: DeepPartial<Model>): Model {
    return Model.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Model>): Model {
    const message = createBaseModel();
    message.name = object.name ?? "";
    message.versionId = object.versionId ?? "";
    message.versionAliases = object.versionAliases?.map((e) => e) || [];
    message.versionCreateTime = object.versionCreateTime ?? undefined;
    message.versionUpdateTime = object.versionUpdateTime ?? undefined;
    message.displayName = object.displayName ?? "";
    message.description = object.description ?? "";
    message.versionDescription = object.versionDescription ?? "";
    message.predictSchemata = (object.predictSchemata !== undefined && object.predictSchemata !== null)
      ? PredictSchemata.fromPartial(object.predictSchemata)
      : undefined;
    message.metadataSchemaUri = object.metadataSchemaUri ?? "";
    message.metadata = object.metadata ?? undefined;
    message.supportedExportFormats = object.supportedExportFormats?.map((e) => Model_ExportFormat.fromPartial(e)) || [];
    message.trainingPipeline = object.trainingPipeline ?? "";
    message.pipelineJob = object.pipelineJob ?? "";
    message.containerSpec = (object.containerSpec !== undefined && object.containerSpec !== null)
      ? ModelContainerSpec.fromPartial(object.containerSpec)
      : undefined;
    message.artifactUri = object.artifactUri ?? "";
    message.supportedDeploymentResourcesTypes = object.supportedDeploymentResourcesTypes?.map((e) => e) || [];
    message.supportedInputStorageFormats = object.supportedInputStorageFormats?.map((e) => e) || [];
    message.supportedOutputStorageFormats = object.supportedOutputStorageFormats?.map((e) => e) || [];
    message.createTime = object.createTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.deployedModels = object.deployedModels?.map((e) => DeployedModelRef.fromPartial(e)) || [];
    message.explanationSpec = (object.explanationSpec !== undefined && object.explanationSpec !== null)
      ? ExplanationSpec.fromPartial(object.explanationSpec)
      : undefined;
    message.etag = object.etag ?? "";
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.dataStats = (object.dataStats !== undefined && object.dataStats !== null)
      ? Model_DataStats.fromPartial(object.dataStats)
      : undefined;
    message.encryptionSpec = (object.encryptionSpec !== undefined && object.encryptionSpec !== null)
      ? EncryptionSpec.fromPartial(object.encryptionSpec)
      : undefined;
    message.modelSourceInfo = (object.modelSourceInfo !== undefined && object.modelSourceInfo !== null)
      ? ModelSourceInfo.fromPartial(object.modelSourceInfo)
      : undefined;
    message.originalModelInfo = (object.originalModelInfo !== undefined && object.originalModelInfo !== null)
      ? Model_OriginalModelInfo.fromPartial(object.originalModelInfo)
      : undefined;
    message.metadataArtifact = object.metadataArtifact ?? "";
    message.baseModelSource = (object.baseModelSource !== undefined && object.baseModelSource !== null)
      ? Model_BaseModelSource.fromPartial(object.baseModelSource)
      : undefined;
    message.satisfiesPzs = object.satisfiesPzs ?? false;
    message.satisfiesPzi = object.satisfiesPzi ?? false;
    return message;
  },
};

function createBaseModel_ExportFormat(): Model_ExportFormat {
  return { id: "", exportableContents: [] };
}

export const Model_ExportFormat: MessageFns<Model_ExportFormat> = {
  encode(message: Model_ExportFormat, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.id !== "") {
      writer.uint32(10).string(message.id);
    }
    writer.uint32(18).fork();
    for (const v of message.exportableContents) {
      writer.int32(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Model_ExportFormat {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseModel_ExportFormat();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.id = reader.string();
          continue;
        case 2:
          if (tag === 16) {
            message.exportableContents.push(reader.int32() as any);

            continue;
          }

          if (tag === 18) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.exportableContents.push(reader.int32() as any);
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Model_ExportFormat {
    return {
      id: isSet(object.id) ? globalThis.String(object.id) : "",
      exportableContents: globalThis.Array.isArray(object?.exportableContents)
        ? object.exportableContents.map((e: any) => model_ExportFormat_ExportableContentFromJSON(e))
        : [],
    };
  },

  toJSON(message: Model_ExportFormat): unknown {
    const obj: any = {};
    if (message.id !== "") {
      obj.id = message.id;
    }
    if (message.exportableContents?.length) {
      obj.exportableContents = message.exportableContents.map((e) => model_ExportFormat_ExportableContentToJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<Model_ExportFormat>): Model_ExportFormat {
    return Model_ExportFormat.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Model_ExportFormat>): Model_ExportFormat {
    const message = createBaseModel_ExportFormat();
    message.id = object.id ?? "";
    message.exportableContents = object.exportableContents?.map((e) => e) || [];
    return message;
  },
};

function createBaseModel_DataStats(): Model_DataStats {
  return {
    trainingDataItemsCount: Long.ZERO,
    validationDataItemsCount: Long.ZERO,
    testDataItemsCount: Long.ZERO,
    trainingAnnotationsCount: Long.ZERO,
    validationAnnotationsCount: Long.ZERO,
    testAnnotationsCount: Long.ZERO,
  };
}

export const Model_DataStats: MessageFns<Model_DataStats> = {
  encode(message: Model_DataStats, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.trainingDataItemsCount.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.trainingDataItemsCount.toString());
    }
    if (!message.validationDataItemsCount.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.validationDataItemsCount.toString());
    }
    if (!message.testDataItemsCount.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.testDataItemsCount.toString());
    }
    if (!message.trainingAnnotationsCount.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.trainingAnnotationsCount.toString());
    }
    if (!message.validationAnnotationsCount.equals(Long.ZERO)) {
      writer.uint32(40).int64(message.validationAnnotationsCount.toString());
    }
    if (!message.testAnnotationsCount.equals(Long.ZERO)) {
      writer.uint32(48).int64(message.testAnnotationsCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Model_DataStats {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseModel_DataStats();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.trainingDataItemsCount = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.validationDataItemsCount = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.testDataItemsCount = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.trainingAnnotationsCount = Long.fromString(reader.int64().toString());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.validationAnnotationsCount = Long.fromString(reader.int64().toString());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.testAnnotationsCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Model_DataStats {
    return {
      trainingDataItemsCount: isSet(object.trainingDataItemsCount)
        ? Long.fromValue(object.trainingDataItemsCount)
        : Long.ZERO,
      validationDataItemsCount: isSet(object.validationDataItemsCount)
        ? Long.fromValue(object.validationDataItemsCount)
        : Long.ZERO,
      testDataItemsCount: isSet(object.testDataItemsCount) ? Long.fromValue(object.testDataItemsCount) : Long.ZERO,
      trainingAnnotationsCount: isSet(object.trainingAnnotationsCount)
        ? Long.fromValue(object.trainingAnnotationsCount)
        : Long.ZERO,
      validationAnnotationsCount: isSet(object.validationAnnotationsCount)
        ? Long.fromValue(object.validationAnnotationsCount)
        : Long.ZERO,
      testAnnotationsCount: isSet(object.testAnnotationsCount)
        ? Long.fromValue(object.testAnnotationsCount)
        : Long.ZERO,
    };
  },

  toJSON(message: Model_DataStats): unknown {
    const obj: any = {};
    if (!message.trainingDataItemsCount.equals(Long.ZERO)) {
      obj.trainingDataItemsCount = (message.trainingDataItemsCount || Long.ZERO).toString();
    }
    if (!message.validationDataItemsCount.equals(Long.ZERO)) {
      obj.validationDataItemsCount = (message.validationDataItemsCount || Long.ZERO).toString();
    }
    if (!message.testDataItemsCount.equals(Long.ZERO)) {
      obj.testDataItemsCount = (message.testDataItemsCount || Long.ZERO).toString();
    }
    if (!message.trainingAnnotationsCount.equals(Long.ZERO)) {
      obj.trainingAnnotationsCount = (message.trainingAnnotationsCount || Long.ZERO).toString();
    }
    if (!message.validationAnnotationsCount.equals(Long.ZERO)) {
      obj.validationAnnotationsCount = (message.validationAnnotationsCount || Long.ZERO).toString();
    }
    if (!message.testAnnotationsCount.equals(Long.ZERO)) {
      obj.testAnnotationsCount = (message.testAnnotationsCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<Model_DataStats>): Model_DataStats {
    return Model_DataStats.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Model_DataStats>): Model_DataStats {
    const message = createBaseModel_DataStats();
    message.trainingDataItemsCount =
      (object.trainingDataItemsCount !== undefined && object.trainingDataItemsCount !== null)
        ? Long.fromValue(object.trainingDataItemsCount)
        : Long.ZERO;
    message.validationDataItemsCount =
      (object.validationDataItemsCount !== undefined && object.validationDataItemsCount !== null)
        ? Long.fromValue(object.validationDataItemsCount)
        : Long.ZERO;
    message.testDataItemsCount = (object.testDataItemsCount !== undefined && object.testDataItemsCount !== null)
      ? Long.fromValue(object.testDataItemsCount)
      : Long.ZERO;
    message.trainingAnnotationsCount =
      (object.trainingAnnotationsCount !== undefined && object.trainingAnnotationsCount !== null)
        ? Long.fromValue(object.trainingAnnotationsCount)
        : Long.ZERO;
    message.validationAnnotationsCount =
      (object.validationAnnotationsCount !== undefined && object.validationAnnotationsCount !== null)
        ? Long.fromValue(object.validationAnnotationsCount)
        : Long.ZERO;
    message.testAnnotationsCount = (object.testAnnotationsCount !== undefined && object.testAnnotationsCount !== null)
      ? Long.fromValue(object.testAnnotationsCount)
      : Long.ZERO;
    return message;
  },
};

function createBaseModel_OriginalModelInfo(): Model_OriginalModelInfo {
  return { model: "" };
}

export const Model_OriginalModelInfo: MessageFns<Model_OriginalModelInfo> = {
  encode(message: Model_OriginalModelInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Model_OriginalModelInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseModel_OriginalModelInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Model_OriginalModelInfo {
    return { model: isSet(object.model) ? globalThis.String(object.model) : "" };
  },

  toJSON(message: Model_OriginalModelInfo): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    return obj;
  },

  create(base?: DeepPartial<Model_OriginalModelInfo>): Model_OriginalModelInfo {
    return Model_OriginalModelInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Model_OriginalModelInfo>): Model_OriginalModelInfo {
    const message = createBaseModel_OriginalModelInfo();
    message.model = object.model ?? "";
    return message;
  },
};

function createBaseModel_BaseModelSource(): Model_BaseModelSource {
  return { modelGardenSource: undefined, genieSource: undefined };
}

export const Model_BaseModelSource: MessageFns<Model_BaseModelSource> = {
  encode(message: Model_BaseModelSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.modelGardenSource !== undefined) {
      ModelGardenSource.encode(message.modelGardenSource, writer.uint32(10).fork()).join();
    }
    if (message.genieSource !== undefined) {
      GenieSource.encode(message.genieSource, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Model_BaseModelSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseModel_BaseModelSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.modelGardenSource = ModelGardenSource.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.genieSource = GenieSource.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Model_BaseModelSource {
    return {
      modelGardenSource: isSet(object.modelGardenSource)
        ? ModelGardenSource.fromJSON(object.modelGardenSource)
        : undefined,
      genieSource: isSet(object.genieSource) ? GenieSource.fromJSON(object.genieSource) : undefined,
    };
  },

  toJSON(message: Model_BaseModelSource): unknown {
    const obj: any = {};
    if (message.modelGardenSource !== undefined) {
      obj.modelGardenSource = ModelGardenSource.toJSON(message.modelGardenSource);
    }
    if (message.genieSource !== undefined) {
      obj.genieSource = GenieSource.toJSON(message.genieSource);
    }
    return obj;
  },

  create(base?: DeepPartial<Model_BaseModelSource>): Model_BaseModelSource {
    return Model_BaseModelSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Model_BaseModelSource>): Model_BaseModelSource {
    const message = createBaseModel_BaseModelSource();
    message.modelGardenSource = (object.modelGardenSource !== undefined && object.modelGardenSource !== null)
      ? ModelGardenSource.fromPartial(object.modelGardenSource)
      : undefined;
    message.genieSource = (object.genieSource !== undefined && object.genieSource !== null)
      ? GenieSource.fromPartial(object.genieSource)
      : undefined;
    return message;
  },
};

function createBaseModel_LabelsEntry(): Model_LabelsEntry {
  return { key: "", value: "" };
}

export const Model_LabelsEntry: MessageFns<Model_LabelsEntry> = {
  encode(message: Model_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Model_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseModel_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Model_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Model_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Model_LabelsEntry>): Model_LabelsEntry {
    return Model_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Model_LabelsEntry>): Model_LabelsEntry {
    const message = createBaseModel_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseLargeModelReference(): LargeModelReference {
  return { name: "" };
}

export const LargeModelReference: MessageFns<LargeModelReference> = {
  encode(message: LargeModelReference, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LargeModelReference {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLargeModelReference();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LargeModelReference {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: LargeModelReference): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<LargeModelReference>): LargeModelReference {
    return LargeModelReference.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LargeModelReference>): LargeModelReference {
    const message = createBaseLargeModelReference();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseModelGardenSource(): ModelGardenSource {
  return { publicModelName: "" };
}

export const ModelGardenSource: MessageFns<ModelGardenSource> = {
  encode(message: ModelGardenSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.publicModelName !== "") {
      writer.uint32(10).string(message.publicModelName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ModelGardenSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseModelGardenSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.publicModelName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ModelGardenSource {
    return { publicModelName: isSet(object.publicModelName) ? globalThis.String(object.publicModelName) : "" };
  },

  toJSON(message: ModelGardenSource): unknown {
    const obj: any = {};
    if (message.publicModelName !== "") {
      obj.publicModelName = message.publicModelName;
    }
    return obj;
  },

  create(base?: DeepPartial<ModelGardenSource>): ModelGardenSource {
    return ModelGardenSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ModelGardenSource>): ModelGardenSource {
    const message = createBaseModelGardenSource();
    message.publicModelName = object.publicModelName ?? "";
    return message;
  },
};

function createBaseGenieSource(): GenieSource {
  return { baseModelUri: "" };
}

export const GenieSource: MessageFns<GenieSource> = {
  encode(message: GenieSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.baseModelUri !== "") {
      writer.uint32(10).string(message.baseModelUri);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenieSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenieSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.baseModelUri = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenieSource {
    return { baseModelUri: isSet(object.baseModelUri) ? globalThis.String(object.baseModelUri) : "" };
  },

  toJSON(message: GenieSource): unknown {
    const obj: any = {};
    if (message.baseModelUri !== "") {
      obj.baseModelUri = message.baseModelUri;
    }
    return obj;
  },

  create(base?: DeepPartial<GenieSource>): GenieSource {
    return GenieSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenieSource>): GenieSource {
    const message = createBaseGenieSource();
    message.baseModelUri = object.baseModelUri ?? "";
    return message;
  },
};

function createBasePredictSchemata(): PredictSchemata {
  return { instanceSchemaUri: "", parametersSchemaUri: "", predictionSchemaUri: "" };
}

export const PredictSchemata: MessageFns<PredictSchemata> = {
  encode(message: PredictSchemata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.instanceSchemaUri !== "") {
      writer.uint32(10).string(message.instanceSchemaUri);
    }
    if (message.parametersSchemaUri !== "") {
      writer.uint32(18).string(message.parametersSchemaUri);
    }
    if (message.predictionSchemaUri !== "") {
      writer.uint32(26).string(message.predictionSchemaUri);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PredictSchemata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePredictSchemata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.instanceSchemaUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.parametersSchemaUri = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.predictionSchemaUri = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PredictSchemata {
    return {
      instanceSchemaUri: isSet(object.instanceSchemaUri) ? globalThis.String(object.instanceSchemaUri) : "",
      parametersSchemaUri: isSet(object.parametersSchemaUri) ? globalThis.String(object.parametersSchemaUri) : "",
      predictionSchemaUri: isSet(object.predictionSchemaUri) ? globalThis.String(object.predictionSchemaUri) : "",
    };
  },

  toJSON(message: PredictSchemata): unknown {
    const obj: any = {};
    if (message.instanceSchemaUri !== "") {
      obj.instanceSchemaUri = message.instanceSchemaUri;
    }
    if (message.parametersSchemaUri !== "") {
      obj.parametersSchemaUri = message.parametersSchemaUri;
    }
    if (message.predictionSchemaUri !== "") {
      obj.predictionSchemaUri = message.predictionSchemaUri;
    }
    return obj;
  },

  create(base?: DeepPartial<PredictSchemata>): PredictSchemata {
    return PredictSchemata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PredictSchemata>): PredictSchemata {
    const message = createBasePredictSchemata();
    message.instanceSchemaUri = object.instanceSchemaUri ?? "";
    message.parametersSchemaUri = object.parametersSchemaUri ?? "";
    message.predictionSchemaUri = object.predictionSchemaUri ?? "";
    return message;
  },
};

function createBaseModelContainerSpec(): ModelContainerSpec {
  return {
    imageUri: "",
    command: [],
    args: [],
    env: [],
    ports: [],
    predictRoute: "",
    healthRoute: "",
    grpcPorts: [],
    deploymentTimeout: undefined,
    sharedMemorySizeMb: Long.ZERO,
    startupProbe: undefined,
    healthProbe: undefined,
  };
}

export const ModelContainerSpec: MessageFns<ModelContainerSpec> = {
  encode(message: ModelContainerSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.imageUri !== "") {
      writer.uint32(10).string(message.imageUri);
    }
    for (const v of message.command) {
      writer.uint32(18).string(v!);
    }
    for (const v of message.args) {
      writer.uint32(26).string(v!);
    }
    for (const v of message.env) {
      EnvVar.encode(v!, writer.uint32(34).fork()).join();
    }
    for (const v of message.ports) {
      Port.encode(v!, writer.uint32(42).fork()).join();
    }
    if (message.predictRoute !== "") {
      writer.uint32(50).string(message.predictRoute);
    }
    if (message.healthRoute !== "") {
      writer.uint32(58).string(message.healthRoute);
    }
    for (const v of message.grpcPorts) {
      Port.encode(v!, writer.uint32(74).fork()).join();
    }
    if (message.deploymentTimeout !== undefined) {
      Duration.encode(message.deploymentTimeout, writer.uint32(82).fork()).join();
    }
    if (!message.sharedMemorySizeMb.equals(Long.ZERO)) {
      writer.uint32(88).int64(message.sharedMemorySizeMb.toString());
    }
    if (message.startupProbe !== undefined) {
      Probe.encode(message.startupProbe, writer.uint32(98).fork()).join();
    }
    if (message.healthProbe !== undefined) {
      Probe.encode(message.healthProbe, writer.uint32(106).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ModelContainerSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseModelContainerSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.imageUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.command.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.args.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.env.push(EnvVar.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.ports.push(Port.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.predictRoute = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.healthRoute = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.grpcPorts.push(Port.decode(reader, reader.uint32()));
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.deploymentTimeout = Duration.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.sharedMemorySizeMb = Long.fromString(reader.int64().toString());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.startupProbe = Probe.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.healthProbe = Probe.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ModelContainerSpec {
    return {
      imageUri: isSet(object.imageUri) ? globalThis.String(object.imageUri) : "",
      command: globalThis.Array.isArray(object?.command) ? object.command.map((e: any) => globalThis.String(e)) : [],
      args: globalThis.Array.isArray(object?.args) ? object.args.map((e: any) => globalThis.String(e)) : [],
      env: globalThis.Array.isArray(object?.env) ? object.env.map((e: any) => EnvVar.fromJSON(e)) : [],
      ports: globalThis.Array.isArray(object?.ports) ? object.ports.map((e: any) => Port.fromJSON(e)) : [],
      predictRoute: isSet(object.predictRoute) ? globalThis.String(object.predictRoute) : "",
      healthRoute: isSet(object.healthRoute) ? globalThis.String(object.healthRoute) : "",
      grpcPorts: globalThis.Array.isArray(object?.grpcPorts) ? object.grpcPorts.map((e: any) => Port.fromJSON(e)) : [],
      deploymentTimeout: isSet(object.deploymentTimeout) ? Duration.fromJSON(object.deploymentTimeout) : undefined,
      sharedMemorySizeMb: isSet(object.sharedMemorySizeMb) ? Long.fromValue(object.sharedMemorySizeMb) : Long.ZERO,
      startupProbe: isSet(object.startupProbe) ? Probe.fromJSON(object.startupProbe) : undefined,
      healthProbe: isSet(object.healthProbe) ? Probe.fromJSON(object.healthProbe) : undefined,
    };
  },

  toJSON(message: ModelContainerSpec): unknown {
    const obj: any = {};
    if (message.imageUri !== "") {
      obj.imageUri = message.imageUri;
    }
    if (message.command?.length) {
      obj.command = message.command;
    }
    if (message.args?.length) {
      obj.args = message.args;
    }
    if (message.env?.length) {
      obj.env = message.env.map((e) => EnvVar.toJSON(e));
    }
    if (message.ports?.length) {
      obj.ports = message.ports.map((e) => Port.toJSON(e));
    }
    if (message.predictRoute !== "") {
      obj.predictRoute = message.predictRoute;
    }
    if (message.healthRoute !== "") {
      obj.healthRoute = message.healthRoute;
    }
    if (message.grpcPorts?.length) {
      obj.grpcPorts = message.grpcPorts.map((e) => Port.toJSON(e));
    }
    if (message.deploymentTimeout !== undefined) {
      obj.deploymentTimeout = Duration.toJSON(message.deploymentTimeout);
    }
    if (!message.sharedMemorySizeMb.equals(Long.ZERO)) {
      obj.sharedMemorySizeMb = (message.sharedMemorySizeMb || Long.ZERO).toString();
    }
    if (message.startupProbe !== undefined) {
      obj.startupProbe = Probe.toJSON(message.startupProbe);
    }
    if (message.healthProbe !== undefined) {
      obj.healthProbe = Probe.toJSON(message.healthProbe);
    }
    return obj;
  },

  create(base?: DeepPartial<ModelContainerSpec>): ModelContainerSpec {
    return ModelContainerSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ModelContainerSpec>): ModelContainerSpec {
    const message = createBaseModelContainerSpec();
    message.imageUri = object.imageUri ?? "";
    message.command = object.command?.map((e) => e) || [];
    message.args = object.args?.map((e) => e) || [];
    message.env = object.env?.map((e) => EnvVar.fromPartial(e)) || [];
    message.ports = object.ports?.map((e) => Port.fromPartial(e)) || [];
    message.predictRoute = object.predictRoute ?? "";
    message.healthRoute = object.healthRoute ?? "";
    message.grpcPorts = object.grpcPorts?.map((e) => Port.fromPartial(e)) || [];
    message.deploymentTimeout = (object.deploymentTimeout !== undefined && object.deploymentTimeout !== null)
      ? Duration.fromPartial(object.deploymentTimeout)
      : undefined;
    message.sharedMemorySizeMb = (object.sharedMemorySizeMb !== undefined && object.sharedMemorySizeMb !== null)
      ? Long.fromValue(object.sharedMemorySizeMb)
      : Long.ZERO;
    message.startupProbe = (object.startupProbe !== undefined && object.startupProbe !== null)
      ? Probe.fromPartial(object.startupProbe)
      : undefined;
    message.healthProbe = (object.healthProbe !== undefined && object.healthProbe !== null)
      ? Probe.fromPartial(object.healthProbe)
      : undefined;
    return message;
  },
};

function createBasePort(): Port {
  return { containerPort: 0 };
}

export const Port: MessageFns<Port> = {
  encode(message: Port, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.containerPort !== 0) {
      writer.uint32(24).int32(message.containerPort);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Port {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePort();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 24) {
            break;
          }

          message.containerPort = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Port {
    return { containerPort: isSet(object.containerPort) ? globalThis.Number(object.containerPort) : 0 };
  },

  toJSON(message: Port): unknown {
    const obj: any = {};
    if (message.containerPort !== 0) {
      obj.containerPort = Math.round(message.containerPort);
    }
    return obj;
  },

  create(base?: DeepPartial<Port>): Port {
    return Port.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Port>): Port {
    const message = createBasePort();
    message.containerPort = object.containerPort ?? 0;
    return message;
  },
};

function createBaseModelSourceInfo(): ModelSourceInfo {
  return { sourceType: 0, copy: false };
}

export const ModelSourceInfo: MessageFns<ModelSourceInfo> = {
  encode(message: ModelSourceInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.sourceType !== 0) {
      writer.uint32(8).int32(message.sourceType);
    }
    if (message.copy !== false) {
      writer.uint32(16).bool(message.copy);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ModelSourceInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseModelSourceInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.sourceType = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.copy = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ModelSourceInfo {
    return {
      sourceType: isSet(object.sourceType) ? modelSourceInfo_ModelSourceTypeFromJSON(object.sourceType) : 0,
      copy: isSet(object.copy) ? globalThis.Boolean(object.copy) : false,
    };
  },

  toJSON(message: ModelSourceInfo): unknown {
    const obj: any = {};
    if (message.sourceType !== 0) {
      obj.sourceType = modelSourceInfo_ModelSourceTypeToJSON(message.sourceType);
    }
    if (message.copy !== false) {
      obj.copy = message.copy;
    }
    return obj;
  },

  create(base?: DeepPartial<ModelSourceInfo>): ModelSourceInfo {
    return ModelSourceInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ModelSourceInfo>): ModelSourceInfo {
    const message = createBaseModelSourceInfo();
    message.sourceType = object.sourceType ?? 0;
    message.copy = object.copy ?? false;
    return message;
  },
};

function createBaseProbe(): Probe {
  return { exec: undefined, periodSeconds: 0, timeoutSeconds: 0 };
}

export const Probe: MessageFns<Probe> = {
  encode(message: Probe, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.exec !== undefined) {
      Probe_ExecAction.encode(message.exec, writer.uint32(10).fork()).join();
    }
    if (message.periodSeconds !== 0) {
      writer.uint32(16).int32(message.periodSeconds);
    }
    if (message.timeoutSeconds !== 0) {
      writer.uint32(24).int32(message.timeoutSeconds);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Probe {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProbe();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.exec = Probe_ExecAction.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.periodSeconds = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.timeoutSeconds = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Probe {
    return {
      exec: isSet(object.exec) ? Probe_ExecAction.fromJSON(object.exec) : undefined,
      periodSeconds: isSet(object.periodSeconds) ? globalThis.Number(object.periodSeconds) : 0,
      timeoutSeconds: isSet(object.timeoutSeconds) ? globalThis.Number(object.timeoutSeconds) : 0,
    };
  },

  toJSON(message: Probe): unknown {
    const obj: any = {};
    if (message.exec !== undefined) {
      obj.exec = Probe_ExecAction.toJSON(message.exec);
    }
    if (message.periodSeconds !== 0) {
      obj.periodSeconds = Math.round(message.periodSeconds);
    }
    if (message.timeoutSeconds !== 0) {
      obj.timeoutSeconds = Math.round(message.timeoutSeconds);
    }
    return obj;
  },

  create(base?: DeepPartial<Probe>): Probe {
    return Probe.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Probe>): Probe {
    const message = createBaseProbe();
    message.exec = (object.exec !== undefined && object.exec !== null)
      ? Probe_ExecAction.fromPartial(object.exec)
      : undefined;
    message.periodSeconds = object.periodSeconds ?? 0;
    message.timeoutSeconds = object.timeoutSeconds ?? 0;
    return message;
  },
};

function createBaseProbe_ExecAction(): Probe_ExecAction {
  return { command: [] };
}

export const Probe_ExecAction: MessageFns<Probe_ExecAction> = {
  encode(message: Probe_ExecAction, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.command) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Probe_ExecAction {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProbe_ExecAction();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.command.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Probe_ExecAction {
    return {
      command: globalThis.Array.isArray(object?.command) ? object.command.map((e: any) => globalThis.String(e)) : [],
    };
  },

  toJSON(message: Probe_ExecAction): unknown {
    const obj: any = {};
    if (message.command?.length) {
      obj.command = message.command;
    }
    return obj;
  },

  create(base?: DeepPartial<Probe_ExecAction>): Probe_ExecAction {
    return Probe_ExecAction.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Probe_ExecAction>): Probe_ExecAction {
    const message = createBaseProbe_ExecAction();
    message.command = object.command?.map((e) => e) || [];
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
