// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/retail/v2/import_config.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { FieldMask } from "../../../protobuf/field_mask.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { Status } from "../../../rpc/status.js";
import { DateMessage } from "../../../type/date.js";
import { Product } from "./product.js";
import { UserEvent } from "./user_event.js";

export const protobufPackage = "google.cloud.retail.v2";

/** Google Cloud Storage location for input content. */
export interface GcsSource {
  /**
   * Required. Google Cloud Storage URIs to input files. URI can be up to
   * 2000 characters long. URIs can match the full object path (for example,
   * `gs://bucket/directory/object.json`) or a pattern matching one or more
   * files, such as `gs://bucket/directory/*.json`. A request can
   * contain at most 100 files, and each file can be up to 2 GB. See
   * [Importing product
   * information](https://cloud.google.com/retail/recommendations-ai/docs/upload-catalog)
   * for the expected file format and setup instructions.
   */
  inputUris: string[];
  /**
   * The schema to use when parsing the data from the source.
   *
   * Supported values for product imports:
   *
   * * `product` (default): One JSON [Product][google.cloud.retail.v2.Product]
   * per line. Each product must
   *   have a valid [Product.id][google.cloud.retail.v2.Product.id].
   * * `product_merchant_center`: See [Importing catalog data from Merchant
   *   Center](https://cloud.google.com/retail/recommendations-ai/docs/upload-catalog#mc).
   *
   * Supported values for user events imports:
   *
   * * `user_event` (default): One JSON
   * [UserEvent][google.cloud.retail.v2.UserEvent] per line.
   * * `user_event_ga360`: Using
   *   https://support.google.com/analytics/answer/3437719.
   *
   * Supported values for control imports:
   *
   * * `control` (default): One JSON [Control][google.cloud.retail.v2.Control]
   * per line.
   *
   * Supported values for catalog attribute imports:
   *
   * * `catalog_attribute` (default): One CSV
   * [CatalogAttribute][google.cloud.retail.v2.CatalogAttribute] per line.
   */
  dataSchema: string;
}

/** BigQuery source import data from. */
export interface BigQuerySource {
  /**
   * BigQuery time partitioned table's _PARTITIONDATE in YYYY-MM-DD format.
   *
   * Only supported in
   * [ImportProductsRequest][google.cloud.retail.v2.ImportProductsRequest].
   */
  partitionDate?:
    | DateMessage
    | undefined;
  /**
   * The project ID (can be project # or ID) that the BigQuery source is in with
   * a length limit of 128 characters. If not specified, inherits the project
   * ID from the parent request.
   */
  projectId: string;
  /**
   * Required. The BigQuery data set to copy the data from with a length limit
   * of 1,024 characters.
   */
  datasetId: string;
  /**
   * Required. The BigQuery table to copy the data from with a length limit of
   * 1,024 characters.
   */
  tableId: string;
  /**
   * Intermediate Cloud Storage directory used for the import with a length
   * limit of 2,000 characters. Can be specified if one wants to have the
   * BigQuery export to a specific Cloud Storage directory.
   */
  gcsStagingDir: string;
  /**
   * The schema to use when parsing the data from the source.
   *
   * Supported values for product imports:
   *
   * * `product` (default): One JSON [Product][google.cloud.retail.v2.Product]
   * per line. Each product must
   *   have a valid [Product.id][google.cloud.retail.v2.Product.id].
   * * `product_merchant_center`: See [Importing catalog data from Merchant
   *   Center](https://cloud.google.com/retail/recommendations-ai/docs/upload-catalog#mc).
   *
   * Supported values for user events imports:
   *
   * * `user_event` (default): One JSON
   * [UserEvent][google.cloud.retail.v2.UserEvent] per line.
   * * `user_event_ga360`:
   *   The schema is available here:
   *   https://support.google.com/analytics/answer/3437719.
   * * `user_event_ga4`:
   *   The schema is available here:
   *   https://support.google.com/analytics/answer/7029846.
   *
   * Supported values for autocomplete imports:
   *
   * * `suggestions` (default): One JSON completion suggestion per line.
   * * `denylist`:  One JSON deny suggestion per line.
   * * `allowlist`:  One JSON allow suggestion per line.
   */
  dataSchema: string;
}

/** The inline source for the input config for ImportProducts method. */
export interface ProductInlineSource {
  /**
   * Required. A list of products to update/create. Each product must have a
   * valid [Product.id][google.cloud.retail.v2.Product.id]. Recommended max of
   * 100 items.
   */
  products: Product[];
}

/** The inline source for the input config for ImportUserEvents method. */
export interface UserEventInlineSource {
  /** Required. A list of user events to import. Recommended max of 10k items. */
  userEvents: UserEvent[];
}

/** Configuration of destination for Import related errors. */
export interface ImportErrorsConfig {
  /**
   * Google Cloud Storage prefix for import errors. This must be an empty,
   * existing Cloud Storage directory. Import errors are written to
   * sharded files in this directory, one per line, as a JSON-encoded
   * `google.rpc.Status` message.
   */
  gcsPrefix?: string | undefined;
}

/** Request message for Import methods. */
export interface ImportProductsRequest {
  /**
   * Required.
   * `projects/1234/locations/global/catalogs/default_catalog/branches/default_branch`
   *
   * If no updateMask is specified, requires products.create permission.
   * If updateMask is specified, requires products.update permission.
   */
  parent: string;
  /**
   * Deprecated. This field has no effect.
   *
   * @deprecated
   */
  requestId: string;
  /** Required. The desired input location of the data. */
  inputConfig:
    | ProductInputConfig
    | undefined;
  /** The desired location of errors incurred during the Import. */
  errorsConfig:
    | ImportErrorsConfig
    | undefined;
  /**
   * Indicates which fields in the provided imported `products` to update. If
   * not set, all fields are updated. If provided, only the existing product
   * fields are updated. Missing products will not be created.
   */
  updateMask:
    | string[]
    | undefined;
  /**
   * The mode of reconciliation between existing products and the products to be
   * imported. Defaults to
   * [ReconciliationMode.INCREMENTAL][google.cloud.retail.v2.ImportProductsRequest.ReconciliationMode.INCREMENTAL].
   */
  reconciliationMode: ImportProductsRequest_ReconciliationMode;
  /**
   * Full Pub/Sub topic name for receiving notification. If this field is set,
   * when the import is finished, a notification is sent to
   * specified Pub/Sub topic. The message data is JSON string of a
   * [Operation][google.longrunning.Operation].
   *
   * Format of the Pub/Sub topic is `projects/{project}/topics/{topic}`. It has
   * to be within the same project as
   * [ImportProductsRequest.parent][google.cloud.retail.v2.ImportProductsRequest.parent].
   * Make sure that both
   * `cloud-retail-customer-data-access@system.gserviceaccount.com` and
   * `service-<project number>@gcp-sa-retail.iam.gserviceaccount.com`
   * have the `pubsub.topics.publish` IAM permission on the topic.
   *
   * Only supported when
   * [ImportProductsRequest.reconciliation_mode][google.cloud.retail.v2.ImportProductsRequest.reconciliation_mode]
   * is set to `FULL`.
   */
  notificationPubsubTopic: string;
}

/**
 * Indicates how imported products are reconciled with the existing products
 * created or imported before.
 */
export enum ImportProductsRequest_ReconciliationMode {
  /** RECONCILIATION_MODE_UNSPECIFIED - Defaults to INCREMENTAL. */
  RECONCILIATION_MODE_UNSPECIFIED = 0,
  /** INCREMENTAL - Inserts new products or updates existing products. */
  INCREMENTAL = 1,
  /**
   * FULL - Calculates diff and replaces the entire product dataset. Existing
   * products may be deleted if they are not present in the source location.
   */
  FULL = 2,
  UNRECOGNIZED = -1,
}

export function importProductsRequest_ReconciliationModeFromJSON(
  object: any,
): ImportProductsRequest_ReconciliationMode {
  switch (object) {
    case 0:
    case "RECONCILIATION_MODE_UNSPECIFIED":
      return ImportProductsRequest_ReconciliationMode.RECONCILIATION_MODE_UNSPECIFIED;
    case 1:
    case "INCREMENTAL":
      return ImportProductsRequest_ReconciliationMode.INCREMENTAL;
    case 2:
    case "FULL":
      return ImportProductsRequest_ReconciliationMode.FULL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ImportProductsRequest_ReconciliationMode.UNRECOGNIZED;
  }
}

export function importProductsRequest_ReconciliationModeToJSON(
  object: ImportProductsRequest_ReconciliationMode,
): string {
  switch (object) {
    case ImportProductsRequest_ReconciliationMode.RECONCILIATION_MODE_UNSPECIFIED:
      return "RECONCILIATION_MODE_UNSPECIFIED";
    case ImportProductsRequest_ReconciliationMode.INCREMENTAL:
      return "INCREMENTAL";
    case ImportProductsRequest_ReconciliationMode.FULL:
      return "FULL";
    case ImportProductsRequest_ReconciliationMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Request message for the ImportUserEvents request. */
export interface ImportUserEventsRequest {
  /** Required. `projects/1234/locations/global/catalogs/default_catalog` */
  parent: string;
  /** Required. The desired input location of the data. */
  inputConfig:
    | UserEventInputConfig
    | undefined;
  /**
   * The desired location of errors incurred during the Import. Cannot be set
   * for inline user event imports.
   */
  errorsConfig: ImportErrorsConfig | undefined;
}

/** Request message for ImportCompletionData methods. */
export interface ImportCompletionDataRequest {
  /**
   * Required. The catalog which the suggestions dataset belongs to.
   *
   * Format: `projects/1234/locations/global/catalogs/default_catalog`.
   */
  parent: string;
  /** Required. The desired input location of the data. */
  inputConfig:
    | CompletionDataInputConfig
    | undefined;
  /**
   * Pub/Sub topic for receiving notification. If this field is set,
   * when the import is finished, a notification is sent to
   * specified Pub/Sub topic. The message data is JSON string of a
   * [Operation][google.longrunning.Operation].
   * Format of the Pub/Sub topic is `projects/{project}/topics/{topic}`.
   */
  notificationPubsubTopic: string;
}

/** The input config source for products. */
export interface ProductInputConfig {
  /** The Inline source for the input content for products. */
  productInlineSource?:
    | ProductInlineSource
    | undefined;
  /** Google Cloud Storage location for the input content. */
  gcsSource?:
    | GcsSource
    | undefined;
  /** BigQuery input source. */
  bigQuerySource?: BigQuerySource | undefined;
}

/** The input config source for user events. */
export interface UserEventInputConfig {
  /** Required. The Inline source for the input content for UserEvents. */
  userEventInlineSource?:
    | UserEventInlineSource
    | undefined;
  /** Required. Google Cloud Storage location for the input content. */
  gcsSource?:
    | GcsSource
    | undefined;
  /** Required. BigQuery input source. */
  bigQuerySource?: BigQuerySource | undefined;
}

/** The input config source for completion data. */
export interface CompletionDataInputConfig {
  /**
   * Required. BigQuery input source.
   *
   * Add the IAM permission "BigQuery Data Viewer" for
   * cloud-retail-customer-data-access@system.gserviceaccount.com before
   * using this feature otherwise an error is thrown.
   */
  bigQuerySource?: BigQuerySource | undefined;
}

/**
 * Metadata related to the progress of the Import operation. This is
 * returned by the google.longrunning.Operation.metadata field.
 */
export interface ImportMetadata {
  /** Operation create time. */
  createTime:
    | Date
    | undefined;
  /**
   * Operation last update time. If the operation is done, this is also the
   * finish time.
   */
  updateTime:
    | Date
    | undefined;
  /** Count of entries that were processed successfully. */
  successCount: Long;
  /** Count of entries that encountered errors while processing. */
  failureCount: Long;
  /**
   * Deprecated. This field is never set.
   *
   * @deprecated
   */
  requestId: string;
  /**
   * Pub/Sub topic for receiving notification. If this field is set,
   * when the import is finished, a notification is sent to
   * specified Pub/Sub topic. The message data is JSON string of a
   * [Operation][google.longrunning.Operation].
   * Format of the Pub/Sub topic is `projects/{project}/topics/{topic}`.
   */
  notificationPubsubTopic: string;
}

/**
 * Response of the
 * [ImportProductsRequest][google.cloud.retail.v2.ImportProductsRequest]. If the
 * long running operation is done, then this message is returned by the
 * google.longrunning.Operations.response field if the operation was successful.
 */
export interface ImportProductsResponse {
  /** A sample of errors encountered while processing the request. */
  errorSamples: Status[];
  /** Echoes the destination for the complete errors in the request if set. */
  errorsConfig: ImportErrorsConfig | undefined;
}

/**
 * Response of the ImportUserEventsRequest. If the long running
 * operation was successful, then this message is returned by the
 * google.longrunning.Operations.response field if the operation was successful.
 */
export interface ImportUserEventsResponse {
  /** A sample of errors encountered while processing the request. */
  errorSamples: Status[];
  /**
   * Echoes the destination for the complete errors if this field was set in
   * the request.
   */
  errorsConfig:
    | ImportErrorsConfig
    | undefined;
  /** Aggregated statistics of user event import status. */
  importSummary: UserEventImportSummary | undefined;
}

/**
 * A summary of import result. The UserEventImportSummary summarizes
 * the import status for user events.
 */
export interface UserEventImportSummary {
  /** Count of user events imported with complete existing catalog information. */
  joinedEventsCount: Long;
  /**
   * Count of user events imported, but with catalog information not found
   * in the imported catalog.
   */
  unjoinedEventsCount: Long;
}

/**
 * Response of the
 * [ImportCompletionDataRequest][google.cloud.retail.v2.ImportCompletionDataRequest].
 * If the long running operation is done, this message is returned by the
 * google.longrunning.Operations.response field if the operation is successful.
 */
export interface ImportCompletionDataResponse {
  /** A sample of errors encountered while processing the request. */
  errorSamples: Status[];
}

function createBaseGcsSource(): GcsSource {
  return { inputUris: [], dataSchema: "" };
}

export const GcsSource: MessageFns<GcsSource> = {
  encode(message: GcsSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.inputUris) {
      writer.uint32(10).string(v!);
    }
    if (message.dataSchema !== "") {
      writer.uint32(18).string(message.dataSchema);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GcsSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGcsSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputUris.push(reader.string());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.dataSchema = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GcsSource {
    return {
      inputUris: globalThis.Array.isArray(object?.inputUris)
        ? object.inputUris.map((e: any) => globalThis.String(e))
        : [],
      dataSchema: isSet(object.dataSchema) ? globalThis.String(object.dataSchema) : "",
    };
  },

  toJSON(message: GcsSource): unknown {
    const obj: any = {};
    if (message.inputUris?.length) {
      obj.inputUris = message.inputUris;
    }
    if (message.dataSchema !== "") {
      obj.dataSchema = message.dataSchema;
    }
    return obj;
  },

  create(base?: DeepPartial<GcsSource>): GcsSource {
    return GcsSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GcsSource>): GcsSource {
    const message = createBaseGcsSource();
    message.inputUris = object.inputUris?.map((e) => e) || [];
    message.dataSchema = object.dataSchema ?? "";
    return message;
  },
};

function createBaseBigQuerySource(): BigQuerySource {
  return { partitionDate: undefined, projectId: "", datasetId: "", tableId: "", gcsStagingDir: "", dataSchema: "" };
}

export const BigQuerySource: MessageFns<BigQuerySource> = {
  encode(message: BigQuerySource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.partitionDate !== undefined) {
      DateMessage.encode(message.partitionDate, writer.uint32(50).fork()).join();
    }
    if (message.projectId !== "") {
      writer.uint32(42).string(message.projectId);
    }
    if (message.datasetId !== "") {
      writer.uint32(10).string(message.datasetId);
    }
    if (message.tableId !== "") {
      writer.uint32(18).string(message.tableId);
    }
    if (message.gcsStagingDir !== "") {
      writer.uint32(26).string(message.gcsStagingDir);
    }
    if (message.dataSchema !== "") {
      writer.uint32(34).string(message.dataSchema);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigQuerySource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigQuerySource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 6:
          if (tag !== 50) {
            break;
          }

          message.partitionDate = DateMessage.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.tableId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.gcsStagingDir = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.dataSchema = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigQuerySource {
    return {
      partitionDate: isSet(object.partitionDate) ? DateMessage.fromJSON(object.partitionDate) : undefined,
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      datasetId: isSet(object.datasetId) ? globalThis.String(object.datasetId) : "",
      tableId: isSet(object.tableId) ? globalThis.String(object.tableId) : "",
      gcsStagingDir: isSet(object.gcsStagingDir) ? globalThis.String(object.gcsStagingDir) : "",
      dataSchema: isSet(object.dataSchema) ? globalThis.String(object.dataSchema) : "",
    };
  },

  toJSON(message: BigQuerySource): unknown {
    const obj: any = {};
    if (message.partitionDate !== undefined) {
      obj.partitionDate = DateMessage.toJSON(message.partitionDate);
    }
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    if (message.tableId !== "") {
      obj.tableId = message.tableId;
    }
    if (message.gcsStagingDir !== "") {
      obj.gcsStagingDir = message.gcsStagingDir;
    }
    if (message.dataSchema !== "") {
      obj.dataSchema = message.dataSchema;
    }
    return obj;
  },

  create(base?: DeepPartial<BigQuerySource>): BigQuerySource {
    return BigQuerySource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigQuerySource>): BigQuerySource {
    const message = createBaseBigQuerySource();
    message.partitionDate = (object.partitionDate !== undefined && object.partitionDate !== null)
      ? DateMessage.fromPartial(object.partitionDate)
      : undefined;
    message.projectId = object.projectId ?? "";
    message.datasetId = object.datasetId ?? "";
    message.tableId = object.tableId ?? "";
    message.gcsStagingDir = object.gcsStagingDir ?? "";
    message.dataSchema = object.dataSchema ?? "";
    return message;
  },
};

function createBaseProductInlineSource(): ProductInlineSource {
  return { products: [] };
}

export const ProductInlineSource: MessageFns<ProductInlineSource> = {
  encode(message: ProductInlineSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.products) {
      Product.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ProductInlineSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProductInlineSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.products.push(Product.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ProductInlineSource {
    return {
      products: globalThis.Array.isArray(object?.products) ? object.products.map((e: any) => Product.fromJSON(e)) : [],
    };
  },

  toJSON(message: ProductInlineSource): unknown {
    const obj: any = {};
    if (message.products?.length) {
      obj.products = message.products.map((e) => Product.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ProductInlineSource>): ProductInlineSource {
    return ProductInlineSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ProductInlineSource>): ProductInlineSource {
    const message = createBaseProductInlineSource();
    message.products = object.products?.map((e) => Product.fromPartial(e)) || [];
    return message;
  },
};

function createBaseUserEventInlineSource(): UserEventInlineSource {
  return { userEvents: [] };
}

export const UserEventInlineSource: MessageFns<UserEventInlineSource> = {
  encode(message: UserEventInlineSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.userEvents) {
      UserEvent.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UserEventInlineSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUserEventInlineSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.userEvents.push(UserEvent.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UserEventInlineSource {
    return {
      userEvents: globalThis.Array.isArray(object?.userEvents)
        ? object.userEvents.map((e: any) => UserEvent.fromJSON(e))
        : [],
    };
  },

  toJSON(message: UserEventInlineSource): unknown {
    const obj: any = {};
    if (message.userEvents?.length) {
      obj.userEvents = message.userEvents.map((e) => UserEvent.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<UserEventInlineSource>): UserEventInlineSource {
    return UserEventInlineSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UserEventInlineSource>): UserEventInlineSource {
    const message = createBaseUserEventInlineSource();
    message.userEvents = object.userEvents?.map((e) => UserEvent.fromPartial(e)) || [];
    return message;
  },
};

function createBaseImportErrorsConfig(): ImportErrorsConfig {
  return { gcsPrefix: undefined };
}

export const ImportErrorsConfig: MessageFns<ImportErrorsConfig> = {
  encode(message: ImportErrorsConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcsPrefix !== undefined) {
      writer.uint32(10).string(message.gcsPrefix);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportErrorsConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportErrorsConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.gcsPrefix = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportErrorsConfig {
    return { gcsPrefix: isSet(object.gcsPrefix) ? globalThis.String(object.gcsPrefix) : undefined };
  },

  toJSON(message: ImportErrorsConfig): unknown {
    const obj: any = {};
    if (message.gcsPrefix !== undefined) {
      obj.gcsPrefix = message.gcsPrefix;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportErrorsConfig>): ImportErrorsConfig {
    return ImportErrorsConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportErrorsConfig>): ImportErrorsConfig {
    const message = createBaseImportErrorsConfig();
    message.gcsPrefix = object.gcsPrefix ?? undefined;
    return message;
  },
};

function createBaseImportProductsRequest(): ImportProductsRequest {
  return {
    parent: "",
    requestId: "",
    inputConfig: undefined,
    errorsConfig: undefined,
    updateMask: undefined,
    reconciliationMode: 0,
    notificationPubsubTopic: "",
  };
}

export const ImportProductsRequest: MessageFns<ImportProductsRequest> = {
  encode(message: ImportProductsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.requestId !== "") {
      writer.uint32(50).string(message.requestId);
    }
    if (message.inputConfig !== undefined) {
      ProductInputConfig.encode(message.inputConfig, writer.uint32(18).fork()).join();
    }
    if (message.errorsConfig !== undefined) {
      ImportErrorsConfig.encode(message.errorsConfig, writer.uint32(26).fork()).join();
    }
    if (message.updateMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.updateMask), writer.uint32(34).fork()).join();
    }
    if (message.reconciliationMode !== 0) {
      writer.uint32(40).int32(message.reconciliationMode);
    }
    if (message.notificationPubsubTopic !== "") {
      writer.uint32(58).string(message.notificationPubsubTopic);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportProductsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportProductsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.requestId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inputConfig = ProductInputConfig.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.errorsConfig = ImportErrorsConfig.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.updateMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.reconciliationMode = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.notificationPubsubTopic = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportProductsRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      requestId: isSet(object.requestId) ? globalThis.String(object.requestId) : "",
      inputConfig: isSet(object.inputConfig) ? ProductInputConfig.fromJSON(object.inputConfig) : undefined,
      errorsConfig: isSet(object.errorsConfig) ? ImportErrorsConfig.fromJSON(object.errorsConfig) : undefined,
      updateMask: isSet(object.updateMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.updateMask)) : undefined,
      reconciliationMode: isSet(object.reconciliationMode)
        ? importProductsRequest_ReconciliationModeFromJSON(object.reconciliationMode)
        : 0,
      notificationPubsubTopic: isSet(object.notificationPubsubTopic)
        ? globalThis.String(object.notificationPubsubTopic)
        : "",
    };
  },

  toJSON(message: ImportProductsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.requestId !== "") {
      obj.requestId = message.requestId;
    }
    if (message.inputConfig !== undefined) {
      obj.inputConfig = ProductInputConfig.toJSON(message.inputConfig);
    }
    if (message.errorsConfig !== undefined) {
      obj.errorsConfig = ImportErrorsConfig.toJSON(message.errorsConfig);
    }
    if (message.updateMask !== undefined) {
      obj.updateMask = FieldMask.toJSON(FieldMask.wrap(message.updateMask));
    }
    if (message.reconciliationMode !== 0) {
      obj.reconciliationMode = importProductsRequest_ReconciliationModeToJSON(message.reconciliationMode);
    }
    if (message.notificationPubsubTopic !== "") {
      obj.notificationPubsubTopic = message.notificationPubsubTopic;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportProductsRequest>): ImportProductsRequest {
    return ImportProductsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportProductsRequest>): ImportProductsRequest {
    const message = createBaseImportProductsRequest();
    message.parent = object.parent ?? "";
    message.requestId = object.requestId ?? "";
    message.inputConfig = (object.inputConfig !== undefined && object.inputConfig !== null)
      ? ProductInputConfig.fromPartial(object.inputConfig)
      : undefined;
    message.errorsConfig = (object.errorsConfig !== undefined && object.errorsConfig !== null)
      ? ImportErrorsConfig.fromPartial(object.errorsConfig)
      : undefined;
    message.updateMask = object.updateMask ?? undefined;
    message.reconciliationMode = object.reconciliationMode ?? 0;
    message.notificationPubsubTopic = object.notificationPubsubTopic ?? "";
    return message;
  },
};

function createBaseImportUserEventsRequest(): ImportUserEventsRequest {
  return { parent: "", inputConfig: undefined, errorsConfig: undefined };
}

export const ImportUserEventsRequest: MessageFns<ImportUserEventsRequest> = {
  encode(message: ImportUserEventsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.inputConfig !== undefined) {
      UserEventInputConfig.encode(message.inputConfig, writer.uint32(18).fork()).join();
    }
    if (message.errorsConfig !== undefined) {
      ImportErrorsConfig.encode(message.errorsConfig, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportUserEventsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportUserEventsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inputConfig = UserEventInputConfig.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.errorsConfig = ImportErrorsConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportUserEventsRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      inputConfig: isSet(object.inputConfig) ? UserEventInputConfig.fromJSON(object.inputConfig) : undefined,
      errorsConfig: isSet(object.errorsConfig) ? ImportErrorsConfig.fromJSON(object.errorsConfig) : undefined,
    };
  },

  toJSON(message: ImportUserEventsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.inputConfig !== undefined) {
      obj.inputConfig = UserEventInputConfig.toJSON(message.inputConfig);
    }
    if (message.errorsConfig !== undefined) {
      obj.errorsConfig = ImportErrorsConfig.toJSON(message.errorsConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<ImportUserEventsRequest>): ImportUserEventsRequest {
    return ImportUserEventsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportUserEventsRequest>): ImportUserEventsRequest {
    const message = createBaseImportUserEventsRequest();
    message.parent = object.parent ?? "";
    message.inputConfig = (object.inputConfig !== undefined && object.inputConfig !== null)
      ? UserEventInputConfig.fromPartial(object.inputConfig)
      : undefined;
    message.errorsConfig = (object.errorsConfig !== undefined && object.errorsConfig !== null)
      ? ImportErrorsConfig.fromPartial(object.errorsConfig)
      : undefined;
    return message;
  },
};

function createBaseImportCompletionDataRequest(): ImportCompletionDataRequest {
  return { parent: "", inputConfig: undefined, notificationPubsubTopic: "" };
}

export const ImportCompletionDataRequest: MessageFns<ImportCompletionDataRequest> = {
  encode(message: ImportCompletionDataRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.inputConfig !== undefined) {
      CompletionDataInputConfig.encode(message.inputConfig, writer.uint32(18).fork()).join();
    }
    if (message.notificationPubsubTopic !== "") {
      writer.uint32(26).string(message.notificationPubsubTopic);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportCompletionDataRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportCompletionDataRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inputConfig = CompletionDataInputConfig.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.notificationPubsubTopic = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportCompletionDataRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      inputConfig: isSet(object.inputConfig) ? CompletionDataInputConfig.fromJSON(object.inputConfig) : undefined,
      notificationPubsubTopic: isSet(object.notificationPubsubTopic)
        ? globalThis.String(object.notificationPubsubTopic)
        : "",
    };
  },

  toJSON(message: ImportCompletionDataRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.inputConfig !== undefined) {
      obj.inputConfig = CompletionDataInputConfig.toJSON(message.inputConfig);
    }
    if (message.notificationPubsubTopic !== "") {
      obj.notificationPubsubTopic = message.notificationPubsubTopic;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportCompletionDataRequest>): ImportCompletionDataRequest {
    return ImportCompletionDataRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportCompletionDataRequest>): ImportCompletionDataRequest {
    const message = createBaseImportCompletionDataRequest();
    message.parent = object.parent ?? "";
    message.inputConfig = (object.inputConfig !== undefined && object.inputConfig !== null)
      ? CompletionDataInputConfig.fromPartial(object.inputConfig)
      : undefined;
    message.notificationPubsubTopic = object.notificationPubsubTopic ?? "";
    return message;
  },
};

function createBaseProductInputConfig(): ProductInputConfig {
  return { productInlineSource: undefined, gcsSource: undefined, bigQuerySource: undefined };
}

export const ProductInputConfig: MessageFns<ProductInputConfig> = {
  encode(message: ProductInputConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.productInlineSource !== undefined) {
      ProductInlineSource.encode(message.productInlineSource, writer.uint32(10).fork()).join();
    }
    if (message.gcsSource !== undefined) {
      GcsSource.encode(message.gcsSource, writer.uint32(18).fork()).join();
    }
    if (message.bigQuerySource !== undefined) {
      BigQuerySource.encode(message.bigQuerySource, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ProductInputConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProductInputConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.productInlineSource = ProductInlineSource.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.gcsSource = GcsSource.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.bigQuerySource = BigQuerySource.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ProductInputConfig {
    return {
      productInlineSource: isSet(object.productInlineSource)
        ? ProductInlineSource.fromJSON(object.productInlineSource)
        : undefined,
      gcsSource: isSet(object.gcsSource) ? GcsSource.fromJSON(object.gcsSource) : undefined,
      bigQuerySource: isSet(object.bigQuerySource) ? BigQuerySource.fromJSON(object.bigQuerySource) : undefined,
    };
  },

  toJSON(message: ProductInputConfig): unknown {
    const obj: any = {};
    if (message.productInlineSource !== undefined) {
      obj.productInlineSource = ProductInlineSource.toJSON(message.productInlineSource);
    }
    if (message.gcsSource !== undefined) {
      obj.gcsSource = GcsSource.toJSON(message.gcsSource);
    }
    if (message.bigQuerySource !== undefined) {
      obj.bigQuerySource = BigQuerySource.toJSON(message.bigQuerySource);
    }
    return obj;
  },

  create(base?: DeepPartial<ProductInputConfig>): ProductInputConfig {
    return ProductInputConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ProductInputConfig>): ProductInputConfig {
    const message = createBaseProductInputConfig();
    message.productInlineSource = (object.productInlineSource !== undefined && object.productInlineSource !== null)
      ? ProductInlineSource.fromPartial(object.productInlineSource)
      : undefined;
    message.gcsSource = (object.gcsSource !== undefined && object.gcsSource !== null)
      ? GcsSource.fromPartial(object.gcsSource)
      : undefined;
    message.bigQuerySource = (object.bigQuerySource !== undefined && object.bigQuerySource !== null)
      ? BigQuerySource.fromPartial(object.bigQuerySource)
      : undefined;
    return message;
  },
};

function createBaseUserEventInputConfig(): UserEventInputConfig {
  return { userEventInlineSource: undefined, gcsSource: undefined, bigQuerySource: undefined };
}

export const UserEventInputConfig: MessageFns<UserEventInputConfig> = {
  encode(message: UserEventInputConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.userEventInlineSource !== undefined) {
      UserEventInlineSource.encode(message.userEventInlineSource, writer.uint32(10).fork()).join();
    }
    if (message.gcsSource !== undefined) {
      GcsSource.encode(message.gcsSource, writer.uint32(18).fork()).join();
    }
    if (message.bigQuerySource !== undefined) {
      BigQuerySource.encode(message.bigQuerySource, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UserEventInputConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUserEventInputConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.userEventInlineSource = UserEventInlineSource.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.gcsSource = GcsSource.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.bigQuerySource = BigQuerySource.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UserEventInputConfig {
    return {
      userEventInlineSource: isSet(object.userEventInlineSource)
        ? UserEventInlineSource.fromJSON(object.userEventInlineSource)
        : undefined,
      gcsSource: isSet(object.gcsSource) ? GcsSource.fromJSON(object.gcsSource) : undefined,
      bigQuerySource: isSet(object.bigQuerySource) ? BigQuerySource.fromJSON(object.bigQuerySource) : undefined,
    };
  },

  toJSON(message: UserEventInputConfig): unknown {
    const obj: any = {};
    if (message.userEventInlineSource !== undefined) {
      obj.userEventInlineSource = UserEventInlineSource.toJSON(message.userEventInlineSource);
    }
    if (message.gcsSource !== undefined) {
      obj.gcsSource = GcsSource.toJSON(message.gcsSource);
    }
    if (message.bigQuerySource !== undefined) {
      obj.bigQuerySource = BigQuerySource.toJSON(message.bigQuerySource);
    }
    return obj;
  },

  create(base?: DeepPartial<UserEventInputConfig>): UserEventInputConfig {
    return UserEventInputConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UserEventInputConfig>): UserEventInputConfig {
    const message = createBaseUserEventInputConfig();
    message.userEventInlineSource =
      (object.userEventInlineSource !== undefined && object.userEventInlineSource !== null)
        ? UserEventInlineSource.fromPartial(object.userEventInlineSource)
        : undefined;
    message.gcsSource = (object.gcsSource !== undefined && object.gcsSource !== null)
      ? GcsSource.fromPartial(object.gcsSource)
      : undefined;
    message.bigQuerySource = (object.bigQuerySource !== undefined && object.bigQuerySource !== null)
      ? BigQuerySource.fromPartial(object.bigQuerySource)
      : undefined;
    return message;
  },
};

function createBaseCompletionDataInputConfig(): CompletionDataInputConfig {
  return { bigQuerySource: undefined };
}

export const CompletionDataInputConfig: MessageFns<CompletionDataInputConfig> = {
  encode(message: CompletionDataInputConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.bigQuerySource !== undefined) {
      BigQuerySource.encode(message.bigQuerySource, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CompletionDataInputConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCompletionDataInputConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.bigQuerySource = BigQuerySource.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CompletionDataInputConfig {
    return {
      bigQuerySource: isSet(object.bigQuerySource) ? BigQuerySource.fromJSON(object.bigQuerySource) : undefined,
    };
  },

  toJSON(message: CompletionDataInputConfig): unknown {
    const obj: any = {};
    if (message.bigQuerySource !== undefined) {
      obj.bigQuerySource = BigQuerySource.toJSON(message.bigQuerySource);
    }
    return obj;
  },

  create(base?: DeepPartial<CompletionDataInputConfig>): CompletionDataInputConfig {
    return CompletionDataInputConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CompletionDataInputConfig>): CompletionDataInputConfig {
    const message = createBaseCompletionDataInputConfig();
    message.bigQuerySource = (object.bigQuerySource !== undefined && object.bigQuerySource !== null)
      ? BigQuerySource.fromPartial(object.bigQuerySource)
      : undefined;
    return message;
  },
};

function createBaseImportMetadata(): ImportMetadata {
  return {
    createTime: undefined,
    updateTime: undefined,
    successCount: Long.ZERO,
    failureCount: Long.ZERO,
    requestId: "",
    notificationPubsubTopic: "",
  };
}

export const ImportMetadata: MessageFns<ImportMetadata> = {
  encode(message: ImportMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(10).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(18).fork()).join();
    }
    if (!message.successCount.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.successCount.toString());
    }
    if (!message.failureCount.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.failureCount.toString());
    }
    if (message.requestId !== "") {
      writer.uint32(42).string(message.requestId);
    }
    if (message.notificationPubsubTopic !== "") {
      writer.uint32(50).string(message.notificationPubsubTopic);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.successCount = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.failureCount = Long.fromString(reader.int64().toString());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.requestId = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.notificationPubsubTopic = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportMetadata {
    return {
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      successCount: isSet(object.successCount) ? Long.fromValue(object.successCount) : Long.ZERO,
      failureCount: isSet(object.failureCount) ? Long.fromValue(object.failureCount) : Long.ZERO,
      requestId: isSet(object.requestId) ? globalThis.String(object.requestId) : "",
      notificationPubsubTopic: isSet(object.notificationPubsubTopic)
        ? globalThis.String(object.notificationPubsubTopic)
        : "",
    };
  },

  toJSON(message: ImportMetadata): unknown {
    const obj: any = {};
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (!message.successCount.equals(Long.ZERO)) {
      obj.successCount = (message.successCount || Long.ZERO).toString();
    }
    if (!message.failureCount.equals(Long.ZERO)) {
      obj.failureCount = (message.failureCount || Long.ZERO).toString();
    }
    if (message.requestId !== "") {
      obj.requestId = message.requestId;
    }
    if (message.notificationPubsubTopic !== "") {
      obj.notificationPubsubTopic = message.notificationPubsubTopic;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportMetadata>): ImportMetadata {
    return ImportMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportMetadata>): ImportMetadata {
    const message = createBaseImportMetadata();
    message.createTime = object.createTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.successCount = (object.successCount !== undefined && object.successCount !== null)
      ? Long.fromValue(object.successCount)
      : Long.ZERO;
    message.failureCount = (object.failureCount !== undefined && object.failureCount !== null)
      ? Long.fromValue(object.failureCount)
      : Long.ZERO;
    message.requestId = object.requestId ?? "";
    message.notificationPubsubTopic = object.notificationPubsubTopic ?? "";
    return message;
  },
};

function createBaseImportProductsResponse(): ImportProductsResponse {
  return { errorSamples: [], errorsConfig: undefined };
}

export const ImportProductsResponse: MessageFns<ImportProductsResponse> = {
  encode(message: ImportProductsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.errorSamples) {
      Status.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.errorsConfig !== undefined) {
      ImportErrorsConfig.encode(message.errorsConfig, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportProductsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportProductsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.errorSamples.push(Status.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.errorsConfig = ImportErrorsConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportProductsResponse {
    return {
      errorSamples: globalThis.Array.isArray(object?.errorSamples)
        ? object.errorSamples.map((e: any) => Status.fromJSON(e))
        : [],
      errorsConfig: isSet(object.errorsConfig) ? ImportErrorsConfig.fromJSON(object.errorsConfig) : undefined,
    };
  },

  toJSON(message: ImportProductsResponse): unknown {
    const obj: any = {};
    if (message.errorSamples?.length) {
      obj.errorSamples = message.errorSamples.map((e) => Status.toJSON(e));
    }
    if (message.errorsConfig !== undefined) {
      obj.errorsConfig = ImportErrorsConfig.toJSON(message.errorsConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<ImportProductsResponse>): ImportProductsResponse {
    return ImportProductsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportProductsResponse>): ImportProductsResponse {
    const message = createBaseImportProductsResponse();
    message.errorSamples = object.errorSamples?.map((e) => Status.fromPartial(e)) || [];
    message.errorsConfig = (object.errorsConfig !== undefined && object.errorsConfig !== null)
      ? ImportErrorsConfig.fromPartial(object.errorsConfig)
      : undefined;
    return message;
  },
};

function createBaseImportUserEventsResponse(): ImportUserEventsResponse {
  return { errorSamples: [], errorsConfig: undefined, importSummary: undefined };
}

export const ImportUserEventsResponse: MessageFns<ImportUserEventsResponse> = {
  encode(message: ImportUserEventsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.errorSamples) {
      Status.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.errorsConfig !== undefined) {
      ImportErrorsConfig.encode(message.errorsConfig, writer.uint32(18).fork()).join();
    }
    if (message.importSummary !== undefined) {
      UserEventImportSummary.encode(message.importSummary, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportUserEventsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportUserEventsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.errorSamples.push(Status.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.errorsConfig = ImportErrorsConfig.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.importSummary = UserEventImportSummary.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportUserEventsResponse {
    return {
      errorSamples: globalThis.Array.isArray(object?.errorSamples)
        ? object.errorSamples.map((e: any) => Status.fromJSON(e))
        : [],
      errorsConfig: isSet(object.errorsConfig) ? ImportErrorsConfig.fromJSON(object.errorsConfig) : undefined,
      importSummary: isSet(object.importSummary) ? UserEventImportSummary.fromJSON(object.importSummary) : undefined,
    };
  },

  toJSON(message: ImportUserEventsResponse): unknown {
    const obj: any = {};
    if (message.errorSamples?.length) {
      obj.errorSamples = message.errorSamples.map((e) => Status.toJSON(e));
    }
    if (message.errorsConfig !== undefined) {
      obj.errorsConfig = ImportErrorsConfig.toJSON(message.errorsConfig);
    }
    if (message.importSummary !== undefined) {
      obj.importSummary = UserEventImportSummary.toJSON(message.importSummary);
    }
    return obj;
  },

  create(base?: DeepPartial<ImportUserEventsResponse>): ImportUserEventsResponse {
    return ImportUserEventsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportUserEventsResponse>): ImportUserEventsResponse {
    const message = createBaseImportUserEventsResponse();
    message.errorSamples = object.errorSamples?.map((e) => Status.fromPartial(e)) || [];
    message.errorsConfig = (object.errorsConfig !== undefined && object.errorsConfig !== null)
      ? ImportErrorsConfig.fromPartial(object.errorsConfig)
      : undefined;
    message.importSummary = (object.importSummary !== undefined && object.importSummary !== null)
      ? UserEventImportSummary.fromPartial(object.importSummary)
      : undefined;
    return message;
  },
};

function createBaseUserEventImportSummary(): UserEventImportSummary {
  return { joinedEventsCount: Long.ZERO, unjoinedEventsCount: Long.ZERO };
}

export const UserEventImportSummary: MessageFns<UserEventImportSummary> = {
  encode(message: UserEventImportSummary, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.joinedEventsCount.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.joinedEventsCount.toString());
    }
    if (!message.unjoinedEventsCount.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.unjoinedEventsCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UserEventImportSummary {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUserEventImportSummary();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.joinedEventsCount = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.unjoinedEventsCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UserEventImportSummary {
    return {
      joinedEventsCount: isSet(object.joinedEventsCount) ? Long.fromValue(object.joinedEventsCount) : Long.ZERO,
      unjoinedEventsCount: isSet(object.unjoinedEventsCount) ? Long.fromValue(object.unjoinedEventsCount) : Long.ZERO,
    };
  },

  toJSON(message: UserEventImportSummary): unknown {
    const obj: any = {};
    if (!message.joinedEventsCount.equals(Long.ZERO)) {
      obj.joinedEventsCount = (message.joinedEventsCount || Long.ZERO).toString();
    }
    if (!message.unjoinedEventsCount.equals(Long.ZERO)) {
      obj.unjoinedEventsCount = (message.unjoinedEventsCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<UserEventImportSummary>): UserEventImportSummary {
    return UserEventImportSummary.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UserEventImportSummary>): UserEventImportSummary {
    const message = createBaseUserEventImportSummary();
    message.joinedEventsCount = (object.joinedEventsCount !== undefined && object.joinedEventsCount !== null)
      ? Long.fromValue(object.joinedEventsCount)
      : Long.ZERO;
    message.unjoinedEventsCount = (object.unjoinedEventsCount !== undefined && object.unjoinedEventsCount !== null)
      ? Long.fromValue(object.unjoinedEventsCount)
      : Long.ZERO;
    return message;
  },
};

function createBaseImportCompletionDataResponse(): ImportCompletionDataResponse {
  return { errorSamples: [] };
}

export const ImportCompletionDataResponse: MessageFns<ImportCompletionDataResponse> = {
  encode(message: ImportCompletionDataResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.errorSamples) {
      Status.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportCompletionDataResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportCompletionDataResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.errorSamples.push(Status.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportCompletionDataResponse {
    return {
      errorSamples: globalThis.Array.isArray(object?.errorSamples)
        ? object.errorSamples.map((e: any) => Status.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ImportCompletionDataResponse): unknown {
    const obj: any = {};
    if (message.errorSamples?.length) {
      obj.errorSamples = message.errorSamples.map((e) => Status.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ImportCompletionDataResponse>): ImportCompletionDataResponse {
    return ImportCompletionDataResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportCompletionDataResponse>): ImportCompletionDataResponse {
    const message = createBaseImportCompletionDataResponse();
    message.errorSamples = object.errorSamples?.map((e) => Status.fromPartial(e)) || [];
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
