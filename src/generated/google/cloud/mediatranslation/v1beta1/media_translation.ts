// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/mediatranslation/v1beta1/media_translation.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Status } from "../../../rpc/status.js";

export const protobufPackage = "google.cloud.mediatranslation.v1beta1";

/**
 * Provides information to the speech translation that specifies how to process
 * the request.
 */
export interface TranslateSpeechConfig {
  /**
   * Required. Encoding of audio data.
   * Supported formats:
   *
   * - `linear16`
   *
   *   Uncompressed 16-bit signed little-endian samples (Linear PCM).
   *
   * - `flac`
   *
   *   `flac` (Free Lossless Audio Codec) is the recommended encoding
   *   because it is lossless--therefore recognition is not compromised--and
   *   requires only about half the bandwidth of `linear16`.
   *
   * - `mulaw`
   *
   *   8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
   *
   * - `amr`
   *
   *   Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000.
   *
   * - `amr-wb`
   *
   *   Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000.
   *
   * - `ogg-opus`
   *
   *   Opus encoded audio frames in [Ogg](https://wikipedia.org/wiki/Ogg)
   *   container. `sample_rate_hertz` must be one of 8000, 12000, 16000, 24000,
   *   or 48000.
   *
   * - `mp3`
   *
   *   MP3 audio. Support all standard MP3 bitrates (which range from 32-320
   *   kbps). When using this encoding, `sample_rate_hertz` has to match the
   *   sample rate of the file being used.
   */
  audioEncoding: string;
  /** Required. Source language code (BCP-47) of the input audio. */
  sourceLanguageCode: string;
  /** Required. Target language code (BCP-47) of the output. */
  targetLanguageCode: string;
  /**
   * Optional. Sample rate in Hertz of the audio data. Valid values are:
   * 8000-48000. 16000 is optimal. For best results, set the sampling rate of
   * the audio source to 16000 Hz. If that's not possible, use the native sample
   * rate of the audio source (instead of re-sampling).
   */
  sampleRateHertz: number;
  /**
   * Optional. `google-provided-model/video` and
   * `google-provided-model/enhanced-phone-call` are premium models.
   * `google-provided-model/phone-call` is not premium model.
   */
  model: string;
}

/** Config used for streaming translation. */
export interface StreamingTranslateSpeechConfig {
  /** Required. The common config for all the following audio contents. */
  audioConfig:
    | TranslateSpeechConfig
    | undefined;
  /**
   * Optional. If `false` or omitted, the system performs
   * continuous translation (continuing to wait for and process audio even if
   * the user pauses speaking) until the client closes the input stream (gRPC
   * API) or until the maximum time limit has been reached. May return multiple
   * `StreamingTranslateSpeechResult`s with the `is_final` flag set to `true`.
   *
   * If `true`, the speech translator will detect a single spoken utterance.
   * When it detects that the user has paused or stopped speaking, it will
   * return an `END_OF_SINGLE_UTTERANCE` event and cease translation.
   * When the client receives 'END_OF_SINGLE_UTTERANCE' event, the client should
   * stop sending the requests. However, clients should keep receiving remaining
   * responses until the stream is terminated. To construct the complete
   * sentence in a streaming way, one should override (if 'is_final' of previous
   * response is false), or append (if 'is_final' of previous response is true).
   */
  singleUtterance: boolean;
}

/**
 * The top-level message sent by the client for the `StreamingTranslateSpeech`
 * method. Multiple `StreamingTranslateSpeechRequest` messages are sent. The
 * first message must contain a `streaming_config` message and must not contain
 * `audio_content` data. All subsequent messages must contain `audio_content`
 * data and must not contain a `streaming_config` message.
 */
export interface StreamingTranslateSpeechRequest {
  /**
   * Provides information to the recognizer that specifies how to process the
   * request. The first `StreamingTranslateSpeechRequest` message must contain
   * a `streaming_config` message.
   */
  streamingConfig?:
    | StreamingTranslateSpeechConfig
    | undefined;
  /**
   * The audio data to be translated. Sequential chunks of audio data are sent
   * in sequential `StreamingTranslateSpeechRequest` messages. The first
   * `StreamingTranslateSpeechRequest` message must not contain
   * `audio_content` data and all subsequent `StreamingTranslateSpeechRequest`
   * messages must contain `audio_content` data. The audio bytes must be
   * encoded as specified in `StreamingTranslateSpeechConfig`. Note: as with
   * all bytes fields, protobuffers use a pure binary representation (not
   * base64).
   */
  audioContent?: Buffer | undefined;
}

/**
 * A streaming speech translation result corresponding to a portion of the audio
 * that is currently being processed.
 */
export interface StreamingTranslateSpeechResult {
  /** Text translation result. */
  textTranslationResult?: StreamingTranslateSpeechResult_TextTranslationResult | undefined;
}

/** Text translation result. */
export interface StreamingTranslateSpeechResult_TextTranslationResult {
  /** Output only. The translated sentence. */
  translation: string;
  /**
   * Output only. If `false`, this `StreamingTranslateSpeechResult` represents
   * an interim result that may change. If `true`, this is the final time the
   * translation service will return this particular
   * `StreamingTranslateSpeechResult`, the streaming translator will not
   * return any further hypotheses for this portion of the transcript and
   * corresponding audio.
   */
  isFinal: boolean;
}

/**
 * A streaming speech translation response corresponding to a portion of
 * the audio currently processed.
 */
export interface StreamingTranslateSpeechResponse {
  /**
   * Output only. If set, returns a [google.rpc.Status][google.rpc.Status] message that
   * specifies the error for the operation.
   */
  error:
    | Status
    | undefined;
  /**
   * Output only. The translation result that is currently being processed (is_final could be
   * true or false).
   */
  result:
    | StreamingTranslateSpeechResult
    | undefined;
  /** Output only. Indicates the type of speech event. */
  speechEventType: StreamingTranslateSpeechResponse_SpeechEventType;
}

/** Indicates the type of speech event. */
export enum StreamingTranslateSpeechResponse_SpeechEventType {
  /** SPEECH_EVENT_TYPE_UNSPECIFIED - No speech event specified. */
  SPEECH_EVENT_TYPE_UNSPECIFIED = 0,
  /**
   * END_OF_SINGLE_UTTERANCE - This event indicates that the server has detected the end of the user's
   * speech utterance and expects no additional speech. Therefore, the server
   * will not process additional audio (although it may subsequently return
   * additional results). When the client receives 'END_OF_SINGLE_UTTERANCE'
   * event, the client should stop sending the requests. However, clients
   * should keep receiving remaining responses until the stream is terminated.
   * To construct the complete sentence in a streaming way, one should
   * override (if 'is_final' of previous response is false), or append (if
   * 'is_final' of previous response is true). This event is only sent if
   * `single_utterance` was set to `true`, and is not used otherwise.
   */
  END_OF_SINGLE_UTTERANCE = 1,
  UNRECOGNIZED = -1,
}

export function streamingTranslateSpeechResponse_SpeechEventTypeFromJSON(
  object: any,
): StreamingTranslateSpeechResponse_SpeechEventType {
  switch (object) {
    case 0:
    case "SPEECH_EVENT_TYPE_UNSPECIFIED":
      return StreamingTranslateSpeechResponse_SpeechEventType.SPEECH_EVENT_TYPE_UNSPECIFIED;
    case 1:
    case "END_OF_SINGLE_UTTERANCE":
      return StreamingTranslateSpeechResponse_SpeechEventType.END_OF_SINGLE_UTTERANCE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return StreamingTranslateSpeechResponse_SpeechEventType.UNRECOGNIZED;
  }
}

export function streamingTranslateSpeechResponse_SpeechEventTypeToJSON(
  object: StreamingTranslateSpeechResponse_SpeechEventType,
): string {
  switch (object) {
    case StreamingTranslateSpeechResponse_SpeechEventType.SPEECH_EVENT_TYPE_UNSPECIFIED:
      return "SPEECH_EVENT_TYPE_UNSPECIFIED";
    case StreamingTranslateSpeechResponse_SpeechEventType.END_OF_SINGLE_UTTERANCE:
      return "END_OF_SINGLE_UTTERANCE";
    case StreamingTranslateSpeechResponse_SpeechEventType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseTranslateSpeechConfig(): TranslateSpeechConfig {
  return { audioEncoding: "", sourceLanguageCode: "", targetLanguageCode: "", sampleRateHertz: 0, model: "" };
}

export const TranslateSpeechConfig: MessageFns<TranslateSpeechConfig> = {
  encode(message: TranslateSpeechConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.audioEncoding !== "") {
      writer.uint32(10).string(message.audioEncoding);
    }
    if (message.sourceLanguageCode !== "") {
      writer.uint32(18).string(message.sourceLanguageCode);
    }
    if (message.targetLanguageCode !== "") {
      writer.uint32(26).string(message.targetLanguageCode);
    }
    if (message.sampleRateHertz !== 0) {
      writer.uint32(32).int32(message.sampleRateHertz);
    }
    if (message.model !== "") {
      writer.uint32(42).string(message.model);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TranslateSpeechConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTranslateSpeechConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.audioEncoding = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.sourceLanguageCode = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.targetLanguageCode = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.sampleRateHertz = reader.int32();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.model = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TranslateSpeechConfig {
    return {
      audioEncoding: isSet(object.audioEncoding) ? globalThis.String(object.audioEncoding) : "",
      sourceLanguageCode: isSet(object.sourceLanguageCode) ? globalThis.String(object.sourceLanguageCode) : "",
      targetLanguageCode: isSet(object.targetLanguageCode) ? globalThis.String(object.targetLanguageCode) : "",
      sampleRateHertz: isSet(object.sampleRateHertz) ? globalThis.Number(object.sampleRateHertz) : 0,
      model: isSet(object.model) ? globalThis.String(object.model) : "",
    };
  },

  toJSON(message: TranslateSpeechConfig): unknown {
    const obj: any = {};
    if (message.audioEncoding !== "") {
      obj.audioEncoding = message.audioEncoding;
    }
    if (message.sourceLanguageCode !== "") {
      obj.sourceLanguageCode = message.sourceLanguageCode;
    }
    if (message.targetLanguageCode !== "") {
      obj.targetLanguageCode = message.targetLanguageCode;
    }
    if (message.sampleRateHertz !== 0) {
      obj.sampleRateHertz = Math.round(message.sampleRateHertz);
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    return obj;
  },

  create(base?: DeepPartial<TranslateSpeechConfig>): TranslateSpeechConfig {
    return TranslateSpeechConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TranslateSpeechConfig>): TranslateSpeechConfig {
    const message = createBaseTranslateSpeechConfig();
    message.audioEncoding = object.audioEncoding ?? "";
    message.sourceLanguageCode = object.sourceLanguageCode ?? "";
    message.targetLanguageCode = object.targetLanguageCode ?? "";
    message.sampleRateHertz = object.sampleRateHertz ?? 0;
    message.model = object.model ?? "";
    return message;
  },
};

function createBaseStreamingTranslateSpeechConfig(): StreamingTranslateSpeechConfig {
  return { audioConfig: undefined, singleUtterance: false };
}

export const StreamingTranslateSpeechConfig: MessageFns<StreamingTranslateSpeechConfig> = {
  encode(message: StreamingTranslateSpeechConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.audioConfig !== undefined) {
      TranslateSpeechConfig.encode(message.audioConfig, writer.uint32(10).fork()).join();
    }
    if (message.singleUtterance !== false) {
      writer.uint32(16).bool(message.singleUtterance);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingTranslateSpeechConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingTranslateSpeechConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.audioConfig = TranslateSpeechConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.singleUtterance = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingTranslateSpeechConfig {
    return {
      audioConfig: isSet(object.audioConfig) ? TranslateSpeechConfig.fromJSON(object.audioConfig) : undefined,
      singleUtterance: isSet(object.singleUtterance) ? globalThis.Boolean(object.singleUtterance) : false,
    };
  },

  toJSON(message: StreamingTranslateSpeechConfig): unknown {
    const obj: any = {};
    if (message.audioConfig !== undefined) {
      obj.audioConfig = TranslateSpeechConfig.toJSON(message.audioConfig);
    }
    if (message.singleUtterance !== false) {
      obj.singleUtterance = message.singleUtterance;
    }
    return obj;
  },

  create(base?: DeepPartial<StreamingTranslateSpeechConfig>): StreamingTranslateSpeechConfig {
    return StreamingTranslateSpeechConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamingTranslateSpeechConfig>): StreamingTranslateSpeechConfig {
    const message = createBaseStreamingTranslateSpeechConfig();
    message.audioConfig = (object.audioConfig !== undefined && object.audioConfig !== null)
      ? TranslateSpeechConfig.fromPartial(object.audioConfig)
      : undefined;
    message.singleUtterance = object.singleUtterance ?? false;
    return message;
  },
};

function createBaseStreamingTranslateSpeechRequest(): StreamingTranslateSpeechRequest {
  return { streamingConfig: undefined, audioContent: undefined };
}

export const StreamingTranslateSpeechRequest: MessageFns<StreamingTranslateSpeechRequest> = {
  encode(message: StreamingTranslateSpeechRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.streamingConfig !== undefined) {
      StreamingTranslateSpeechConfig.encode(message.streamingConfig, writer.uint32(10).fork()).join();
    }
    if (message.audioContent !== undefined) {
      writer.uint32(18).bytes(message.audioContent);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingTranslateSpeechRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingTranslateSpeechRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.streamingConfig = StreamingTranslateSpeechConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.audioContent = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingTranslateSpeechRequest {
    return {
      streamingConfig: isSet(object.streamingConfig)
        ? StreamingTranslateSpeechConfig.fromJSON(object.streamingConfig)
        : undefined,
      audioContent: isSet(object.audioContent) ? Buffer.from(bytesFromBase64(object.audioContent)) : undefined,
    };
  },

  toJSON(message: StreamingTranslateSpeechRequest): unknown {
    const obj: any = {};
    if (message.streamingConfig !== undefined) {
      obj.streamingConfig = StreamingTranslateSpeechConfig.toJSON(message.streamingConfig);
    }
    if (message.audioContent !== undefined) {
      obj.audioContent = base64FromBytes(message.audioContent);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamingTranslateSpeechRequest>): StreamingTranslateSpeechRequest {
    return StreamingTranslateSpeechRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamingTranslateSpeechRequest>): StreamingTranslateSpeechRequest {
    const message = createBaseStreamingTranslateSpeechRequest();
    message.streamingConfig = (object.streamingConfig !== undefined && object.streamingConfig !== null)
      ? StreamingTranslateSpeechConfig.fromPartial(object.streamingConfig)
      : undefined;
    message.audioContent = object.audioContent ?? undefined;
    return message;
  },
};

function createBaseStreamingTranslateSpeechResult(): StreamingTranslateSpeechResult {
  return { textTranslationResult: undefined };
}

export const StreamingTranslateSpeechResult: MessageFns<StreamingTranslateSpeechResult> = {
  encode(message: StreamingTranslateSpeechResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.textTranslationResult !== undefined) {
      StreamingTranslateSpeechResult_TextTranslationResult.encode(
        message.textTranslationResult,
        writer.uint32(10).fork(),
      ).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingTranslateSpeechResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingTranslateSpeechResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.textTranslationResult = StreamingTranslateSpeechResult_TextTranslationResult.decode(
            reader,
            reader.uint32(),
          );
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingTranslateSpeechResult {
    return {
      textTranslationResult: isSet(object.textTranslationResult)
        ? StreamingTranslateSpeechResult_TextTranslationResult.fromJSON(object.textTranslationResult)
        : undefined,
    };
  },

  toJSON(message: StreamingTranslateSpeechResult): unknown {
    const obj: any = {};
    if (message.textTranslationResult !== undefined) {
      obj.textTranslationResult = StreamingTranslateSpeechResult_TextTranslationResult.toJSON(
        message.textTranslationResult,
      );
    }
    return obj;
  },

  create(base?: DeepPartial<StreamingTranslateSpeechResult>): StreamingTranslateSpeechResult {
    return StreamingTranslateSpeechResult.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamingTranslateSpeechResult>): StreamingTranslateSpeechResult {
    const message = createBaseStreamingTranslateSpeechResult();
    message.textTranslationResult =
      (object.textTranslationResult !== undefined && object.textTranslationResult !== null)
        ? StreamingTranslateSpeechResult_TextTranslationResult.fromPartial(object.textTranslationResult)
        : undefined;
    return message;
  },
};

function createBaseStreamingTranslateSpeechResult_TextTranslationResult(): StreamingTranslateSpeechResult_TextTranslationResult {
  return { translation: "", isFinal: false };
}

export const StreamingTranslateSpeechResult_TextTranslationResult: MessageFns<
  StreamingTranslateSpeechResult_TextTranslationResult
> = {
  encode(
    message: StreamingTranslateSpeechResult_TextTranslationResult,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.translation !== "") {
      writer.uint32(10).string(message.translation);
    }
    if (message.isFinal !== false) {
      writer.uint32(16).bool(message.isFinal);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingTranslateSpeechResult_TextTranslationResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingTranslateSpeechResult_TextTranslationResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.translation = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.isFinal = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingTranslateSpeechResult_TextTranslationResult {
    return {
      translation: isSet(object.translation) ? globalThis.String(object.translation) : "",
      isFinal: isSet(object.isFinal) ? globalThis.Boolean(object.isFinal) : false,
    };
  },

  toJSON(message: StreamingTranslateSpeechResult_TextTranslationResult): unknown {
    const obj: any = {};
    if (message.translation !== "") {
      obj.translation = message.translation;
    }
    if (message.isFinal !== false) {
      obj.isFinal = message.isFinal;
    }
    return obj;
  },

  create(
    base?: DeepPartial<StreamingTranslateSpeechResult_TextTranslationResult>,
  ): StreamingTranslateSpeechResult_TextTranslationResult {
    return StreamingTranslateSpeechResult_TextTranslationResult.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<StreamingTranslateSpeechResult_TextTranslationResult>,
  ): StreamingTranslateSpeechResult_TextTranslationResult {
    const message = createBaseStreamingTranslateSpeechResult_TextTranslationResult();
    message.translation = object.translation ?? "";
    message.isFinal = object.isFinal ?? false;
    return message;
  },
};

function createBaseStreamingTranslateSpeechResponse(): StreamingTranslateSpeechResponse {
  return { error: undefined, result: undefined, speechEventType: 0 };
}

export const StreamingTranslateSpeechResponse: MessageFns<StreamingTranslateSpeechResponse> = {
  encode(message: StreamingTranslateSpeechResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.error !== undefined) {
      Status.encode(message.error, writer.uint32(10).fork()).join();
    }
    if (message.result !== undefined) {
      StreamingTranslateSpeechResult.encode(message.result, writer.uint32(18).fork()).join();
    }
    if (message.speechEventType !== 0) {
      writer.uint32(24).int32(message.speechEventType);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingTranslateSpeechResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingTranslateSpeechResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.error = Status.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.result = StreamingTranslateSpeechResult.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.speechEventType = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingTranslateSpeechResponse {
    return {
      error: isSet(object.error) ? Status.fromJSON(object.error) : undefined,
      result: isSet(object.result) ? StreamingTranslateSpeechResult.fromJSON(object.result) : undefined,
      speechEventType: isSet(object.speechEventType)
        ? streamingTranslateSpeechResponse_SpeechEventTypeFromJSON(object.speechEventType)
        : 0,
    };
  },

  toJSON(message: StreamingTranslateSpeechResponse): unknown {
    const obj: any = {};
    if (message.error !== undefined) {
      obj.error = Status.toJSON(message.error);
    }
    if (message.result !== undefined) {
      obj.result = StreamingTranslateSpeechResult.toJSON(message.result);
    }
    if (message.speechEventType !== 0) {
      obj.speechEventType = streamingTranslateSpeechResponse_SpeechEventTypeToJSON(message.speechEventType);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamingTranslateSpeechResponse>): StreamingTranslateSpeechResponse {
    return StreamingTranslateSpeechResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamingTranslateSpeechResponse>): StreamingTranslateSpeechResponse {
    const message = createBaseStreamingTranslateSpeechResponse();
    message.error = (object.error !== undefined && object.error !== null)
      ? Status.fromPartial(object.error)
      : undefined;
    message.result = (object.result !== undefined && object.result !== null)
      ? StreamingTranslateSpeechResult.fromPartial(object.result)
      : undefined;
    message.speechEventType = object.speechEventType ?? 0;
    return message;
  },
};

/** Provides translation from/to media types. */
export type SpeechTranslationServiceDefinition = typeof SpeechTranslationServiceDefinition;
export const SpeechTranslationServiceDefinition = {
  name: "SpeechTranslationService",
  fullName: "google.cloud.mediatranslation.v1beta1.SpeechTranslationService",
  methods: {
    /**
     * Performs bidirectional streaming speech translation: receive results while
     * sending audio. This method is only available via the gRPC API (not REST).
     */
    streamingTranslateSpeech: {
      name: "StreamingTranslateSpeech",
      requestType: StreamingTranslateSpeechRequest,
      requestStream: true,
      responseType: StreamingTranslateSpeechResponse,
      responseStream: true,
      options: {},
    },
  },
} as const;

export interface SpeechTranslationServiceImplementation<CallContextExt = {}> {
  /**
   * Performs bidirectional streaming speech translation: receive results while
   * sending audio. This method is only available via the gRPC API (not REST).
   */
  streamingTranslateSpeech(
    request: AsyncIterable<StreamingTranslateSpeechRequest>,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<StreamingTranslateSpeechResponse>>;
}

export interface SpeechTranslationServiceClient<CallOptionsExt = {}> {
  /**
   * Performs bidirectional streaming speech translation: receive results while
   * sending audio. This method is only available via the gRPC API (not REST).
   */
  streamingTranslateSpeech(
    request: AsyncIterable<DeepPartial<StreamingTranslateSpeechRequest>>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<StreamingTranslateSpeechResponse>;
}

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export type ServerStreamingMethodResult<Response> = { [Symbol.asyncIterator](): AsyncIterator<Response, void> };

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
