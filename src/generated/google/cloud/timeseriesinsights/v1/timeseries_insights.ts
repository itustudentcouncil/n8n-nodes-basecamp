// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/timeseriesinsights/v1/timeseries_insights.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Duration } from "../../../protobuf/duration.js";
import { Empty } from "../../../protobuf/empty.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { Status } from "../../../rpc/status.js";

export const protobufPackage = "google.cloud.timeseriesinsights.v1";

/** Mapping of BigQuery columns to timestamp, group_id and dimensions. */
export interface BigqueryMapping {
  /**
   * The column which should be used as the event timestamps. If not specified
   * 'Timestamp' is used by default. The column may have TIMESTAMP or INT64
   * type (the latter is interpreted as microseconds since the Unix epoch).
   */
  timestampColumn: string;
  /**
   * The column which should be used as the group ID (grouping events into
   * sessions). If not specified 'GroupId' is used by default, if the input
   * table does not have such a column, random unique group IDs are
   * generated automatically (different group ID per input row).
   */
  groupIdColumn: string;
  /**
   * The list of columns that should be translated to dimensions. If empty,
   * all columns are translated to dimensions. The timestamp and group_id
   * columns should not be listed here again. Columns are expected to have
   * primitive types (STRING, INT64, FLOAT64 or NUMERIC).
   */
  dimensionColumn: string[];
}

/**
 * A data source consists of multiple [Event][google.cloud.timeseriesinsights.v1.Event] objects stored on
 * Cloud Storage.  Each Event should be in JSON format, with one Event
 * per line, also known as JSON Lines format.
 */
export interface DataSource {
  /**
   * Data source URI.
   *
   * 1) Google Cloud Storage files (JSON) are defined in the following form.
   * `gs://bucket_name/object_name`. For more information on Cloud Storage URIs,
   * please see https://cloud.google.com/storage/docs/reference-uris.
   */
  uri: string;
  /**
   * For BigQuery inputs defines the columns that should be used for dimensions
   * (including time and group ID).
   */
  bqMapping: BigqueryMapping | undefined;
}

/** A collection of data sources sent for processing. */
export interface DataSet {
  /**
   * The dataset name, which will be used for querying, status and unload
   * requests. This must be unique within a project.
   */
  name: string;
  /**
   * [Data dimension names][google.cloud.timeseriesinsights.v1.EventDimension.name] allowed for this `DataSet`.
   *
   * If left empty, all dimension names are included. This field works as a
   * filter to avoid regenerating the data.
   */
  dataNames: string[];
  /** Input data. */
  dataSources: DataSource[];
  /** Dataset state in the system. */
  state: DataSet_State;
  /** Dataset processing status. */
  status:
    | Status
    | undefined;
  /**
   * Periodically we discard dataset [Event][google.cloud.timeseriesinsights.v1.Event] objects that have
   * timestamps older than 'ttl'.  Omitting this field or a zero value means no
   * events are discarded.
   */
  ttl: Duration | undefined;
}

/** DataSet state. */
export enum DataSet_State {
  /** STATE_UNSPECIFIED - Unspecified / undefined state. */
  STATE_UNSPECIFIED = 0,
  /**
   * UNKNOWN - Dataset is unknown to the system; we have never seen this dataset before
   * or we have seen this dataset but have fully GC-ed it.
   */
  UNKNOWN = 1,
  /** PENDING - Dataset processing is pending. */
  PENDING = 2,
  /** LOADING - Dataset is loading. */
  LOADING = 3,
  /** LOADED - Dataset is loaded and can be queried. */
  LOADED = 4,
  /** UNLOADING - Dataset is unloading. */
  UNLOADING = 5,
  /** UNLOADED - Dataset is unloaded and is removed from the system. */
  UNLOADED = 6,
  /** FAILED - Dataset processing failed. */
  FAILED = 7,
  UNRECOGNIZED = -1,
}

export function dataSet_StateFromJSON(object: any): DataSet_State {
  switch (object) {
    case 0:
    case "STATE_UNSPECIFIED":
      return DataSet_State.STATE_UNSPECIFIED;
    case 1:
    case "UNKNOWN":
      return DataSet_State.UNKNOWN;
    case 2:
    case "PENDING":
      return DataSet_State.PENDING;
    case 3:
    case "LOADING":
      return DataSet_State.LOADING;
    case 4:
    case "LOADED":
      return DataSet_State.LOADED;
    case 5:
    case "UNLOADING":
      return DataSet_State.UNLOADING;
    case 6:
    case "UNLOADED":
      return DataSet_State.UNLOADED;
    case 7:
    case "FAILED":
      return DataSet_State.FAILED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DataSet_State.UNRECOGNIZED;
  }
}

export function dataSet_StateToJSON(object: DataSet_State): string {
  switch (object) {
    case DataSet_State.STATE_UNSPECIFIED:
      return "STATE_UNSPECIFIED";
    case DataSet_State.UNKNOWN:
      return "UNKNOWN";
    case DataSet_State.PENDING:
      return "PENDING";
    case DataSet_State.LOADING:
      return "LOADING";
    case DataSet_State.LOADED:
      return "LOADED";
    case DataSet_State.UNLOADING:
      return "UNLOADING";
    case DataSet_State.UNLOADED:
      return "UNLOADED";
    case DataSet_State.FAILED:
      return "FAILED";
    case DataSet_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Represents an event dimension. */
export interface EventDimension {
  /**
   * Dimension name.
   *
   * **NOTE**: `EventDimension` names must be composed of alphanumeric
   * characters only, and are case insensitive. Unicode characters are *not*
   * supported. The underscore '_' is also allowed.
   */
  name: string;
  /**
   * String representation.
   *
   * **NOTE**: String values are case insensitive. Unicode characters are
   * supported.
   */
  stringVal?:
    | string
    | undefined;
  /** Long representation. */
  longVal?:
    | Long
    | undefined;
  /** Bool representation. */
  boolVal?:
    | boolean
    | undefined;
  /** Double representation. */
  doubleVal?: number | undefined;
}

/**
 * Represents an entry in a data source.
 *
 * Each Event has:
 *
 * * A timestamp at which the event occurs.
 * * One or multiple dimensions.
 * * Optionally, a group ID that allows clients to group logically related
 *   events (for example, all events representing payments transactions done by
 *   a user in a day have the same group ID).  If a group ID is not provided, an
 *   internal one will be generated based on the content and `eventTime`.
 *
 * **NOTE**:
 *
 * * Internally, we discretize time in equal-sized chunks and we assume an
 *   event has a 0
 *   [TimeseriesPoint.value][google.cloud.timeseriesinsights.v1.TimeseriesPoint.value]
 *   in a chunk that does not contain any occurrences of an event in the input.
 * * The number of Events with the same group ID should be limited.
 * * Group ID *cannot* be queried.
 * * Group ID does *not* correspond to a user ID or the like. If a user ID is of
 *   interest to be queried, use a user ID `dimension` instead.
 */
export interface Event {
  /** Event dimensions. */
  dimensions: EventDimension[];
  /**
   * Event group ID.
   *
   * **NOTE**: JSON encoding should use a string to hold a 64-bit integer value,
   * because a native JSON number holds only 53 binary bits for an integer.
   */
  groupId: Long;
  /** Event timestamp. */
  eventTime: Date | undefined;
}

/** Appends events to an existing DataSet. */
export interface AppendEventsRequest {
  /**
   * Events to be appended.
   *
   * Note:
   *
   * 0. The [DataSet][google.cloud.timeseriesinsights.v1.DataSet] must be shown in a `LOADED` state
   *    in the results of `list` method; otherwise, all events from
   *    the append request will be dropped, and a `NOT_FOUND` status will be
   *    returned.
   * 0. All events in a single request must have the same
   *    [groupId][google.cloud.timeseriesinsights.v1.Event.group_id] if set; otherwise, an
   *    `INVALID_ARGUMENT` status will be returned.
   * 0. If [groupId][google.cloud.timeseriesinsights.v1.Event.group_id] is not set (or 0), there
   *    should be only 1 event; otherwise, an `INVALID_ARGUMENT` status will be
   *    returned.
   * 0. The events must be newer than the current time minus
   *    [DataSet TTL][google.cloud.timeseriesinsights.v1.DataSet.ttl] or they will be dropped.
   */
  events: Event[];
  /**
   * Required. The DataSet to which we want to append to in the format of
   * "projects/{project}/datasets/{dataset}"
   */
  dataset: string;
}

/** Response for an AppendEvents RPC. */
export interface AppendEventsResponse {
  /** Dropped events; empty if all events are successfully added. */
  droppedEvents: Event[];
}

/** Create a DataSet request. */
export interface CreateDataSetRequest {
  /**
   * Required. Client project name which will own this DataSet in the format of
   * 'projects/{project}'.
   */
  parent: string;
  /** Required. Dataset to be loaded. */
  dataset: DataSet | undefined;
}

/** Unload DataSet request from the serving system. */
export interface DeleteDataSetRequest {
  /** Required. Dataset name in the format of "projects/{project}/datasets/{dataset}" */
  name: string;
}

/** List the DataSets created by the current project. */
export interface ListDataSetsRequest {
  /** Required. Project owning the DataSet in the format of "projects/{project}". */
  parent: string;
  /** Number of results to return in the list. */
  pageSize: number;
  /** Token to provide to skip to a particular spot in the list. */
  pageToken: string;
}

/** Created DataSets list response. */
export interface ListDataSetsResponse {
  /** The list of created DataSets. */
  datasets: DataSet[];
  /** Token to receive the next page of results. */
  nextPageToken: string;
}

/** A categorical dimension fixed to a certain value. */
export interface PinnedDimension {
  /** The name of the dimension for which we are fixing its value. */
  name: string;
  /**
   * A string value. This can be used for [dimensions][google.cloud.timeseriesinsights.v1.EventDimension], which
   * have their value field set to [string_val][google.cloud.timeseriesinsights.v1.EventDimension.string_val].
   */
  stringVal?:
    | string
    | undefined;
  /**
   * A bool value. This can be used for [dimensions][google.cloud.timeseriesinsights.v1.EventDimension], which
   * have their value field set to [bool_val][google.cloud.timeseriesinsights.v1.EventDimension.bool_val].
   */
  boolVal?: boolean | undefined;
}

/**
 * Parameters that control the sensitivity and other options for the time series
 * forecast.
 */
export interface ForecastParams {
  /**
   * Optional. Penalize variations between the actual and forecasted values smaller than
   * this. For more information about how this parameter affects the score, see
   * the [anomalyScore](EvaluatedSlice.anomaly_score) formula.
   *
   * Intuitively, anomaly scores summarize how statistically significant the
   * change between the actual and forecasted value is compared with what we
   * expect the change to be (see
   * [expectedDeviation](EvaluatedSlice.expected_deviation)). However, in
   * practice, depending on the application, changes smaller than certain
   * absolute values, while statistically significant, may not be important.
   *
   * This parameter allows us to penalize such low absolute value changes.
   *
   * Must be in the (0.0, inf) range.
   *
   * If unspecified, it defaults to 0.000001.
   */
  noiseThreshold?:
    | number
    | undefined;
  /**
   * Optional. Specifying any known seasonality/periodicity in the time series
   * for the slices we will analyze can improve the quality of the results.
   *
   * If unsure, simply leave it unspecified by not setting a value for this
   * field.
   *
   * If your time series has multiple seasonal patterns, then set it to the most
   * granular one (e.g. if it has daily and weekly patterns, set this to DAILY).
   */
  seasonalityHint: ForecastParams_Period;
  /**
   * Optional. The length of the returned [forecasted
   * timeseries][EvaluatedSlice.forecast].
   *
   * This duration is currently capped at 100 x
   * [granularity][google.cloud.timeseriesinsights.v1.TimeseriesParams.granularity].
   *
   * Example: If the detection point is set to "2020-12-27T00:00:00Z", the
   * [granularity][google.cloud.timeseriesinsights.v1.TimeseriesParams.granularity] to "3600s" and the
   * horizon_duration to "10800s", then we will generate 3 time
   * series points (from "2020-12-27T01:00:00Z" to "2020-12-27T04:00:00Z"), for
   * which we will return their forecasted values.
   *
   * Note: The horizon time is only used for forecasting not for anormaly
   * detection. To detect anomalies for multiple points of time,
   * simply send multiple queries with those as
   * [detectionTime][google.cloud.timeseriesinsights.v1.QueryDataSetRequest.detection_time].
   */
  horizonDuration: Duration | undefined;
}

/** A time period of a fixed interval. */
export enum ForecastParams_Period {
  /** PERIOD_UNSPECIFIED - Unknown or simply not given. */
  PERIOD_UNSPECIFIED = 0,
  /** HOURLY - 1 hour */
  HOURLY = 5,
  /** DAILY - 24 hours */
  DAILY = 1,
  /** WEEKLY - 7 days */
  WEEKLY = 2,
  /** MONTHLY - 30 days */
  MONTHLY = 3,
  /** YEARLY - 365 days */
  YEARLY = 4,
  UNRECOGNIZED = -1,
}

export function forecastParams_PeriodFromJSON(object: any): ForecastParams_Period {
  switch (object) {
    case 0:
    case "PERIOD_UNSPECIFIED":
      return ForecastParams_Period.PERIOD_UNSPECIFIED;
    case 5:
    case "HOURLY":
      return ForecastParams_Period.HOURLY;
    case 1:
    case "DAILY":
      return ForecastParams_Period.DAILY;
    case 2:
    case "WEEKLY":
      return ForecastParams_Period.WEEKLY;
    case 3:
    case "MONTHLY":
      return ForecastParams_Period.MONTHLY;
    case 4:
    case "YEARLY":
      return ForecastParams_Period.YEARLY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ForecastParams_Period.UNRECOGNIZED;
  }
}

export function forecastParams_PeriodToJSON(object: ForecastParams_Period): string {
  switch (object) {
    case ForecastParams_Period.PERIOD_UNSPECIFIED:
      return "PERIOD_UNSPECIFIED";
    case ForecastParams_Period.HOURLY:
      return "HOURLY";
    case ForecastParams_Period.DAILY:
      return "DAILY";
    case ForecastParams_Period.WEEKLY:
      return "WEEKLY";
    case ForecastParams_Period.MONTHLY:
      return "MONTHLY";
    case ForecastParams_Period.YEARLY:
      return "YEARLY";
    case ForecastParams_Period.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** A point in a time series. */
export interface TimeseriesPoint {
  /** The timestamp of this point. */
  time:
    | Date
    | undefined;
  /**
   * The value for this point.
   *
   * It is computed by aggregating all events in the associated slice that are
   * in the `[time, time + granularity]` range (see
   * [granularity][google.cloud.timeseriesinsights.v1.TimeseriesParams.granularity]) using the specified
   * [metric][google.cloud.timeseriesinsights.v1.TimeseriesParams.metric].
   */
  value?: number | undefined;
}

/** A time series. */
export interface Timeseries {
  /** The points in this time series, ordered by their timestamp. */
  point: TimeseriesPoint[];
}

/** Forecast result for a given slice. */
export interface EvaluatedSlice {
  /** Values for all categorical dimensions that uniquely identify this slice. */
  dimensions: PinnedDimension[];
  /**
   * The actual value at the detection time (see
   * [detectionTime][google.cloud.timeseriesinsights.v1.QueryDataSetRequest.detection_time]).
   *
   * **NOTE**: This value can be an estimate, so it should not be used as a
   * source of truth.
   */
  detectionPointActual?:
    | number
    | undefined;
  /**
   * The expected value at the detection time, which is obtained by forecasting
   * on the historical time series.
   */
  detectionPointForecast?:
    | number
    | undefined;
  /**
   * How much our forecast model expects the detection point actual will
   * deviate from its forecasted value based on how well it fit the input time
   * series.
   *
   * In general, we expect the `detectionPointActual` to
   * be in the `[detectionPointForecast - expectedDeviation,
   * detectionPointForecast + expectedDeviation]` range. The more the actual
   * value is outside this range, the more statistically significant the
   * anomaly is.
   *
   * The expected deviation is always positive.
   */
  expectedDeviation?:
    | number
    | undefined;
  /**
   * Summarizes how significant the change between the actual and forecasted
   * detection points are compared with the historical patterns observed on the
   * [history][google.cloud.timeseriesinsights.v1.EvaluatedSlice.history] time series.
   *
   * Defined as *|a - f| / (e + nt)*, where:
   *
   * - *a* is the [detectionPointActual][google.cloud.timeseriesinsights.v1.EvaluatedSlice.detection_point_actual].
   * - *f* is the [detectionPointForecast][google.cloud.timeseriesinsights.v1.EvaluatedSlice.detection_point_forecast].
   * - *e* is the [expectedDeviation][google.cloud.timeseriesinsights.v1.EvaluatedSlice.expected_deviation].
   * - *nt` is the [noiseThreshold][google.cloud.timeseriesinsights.v1.ForecastParams.noise_threshold].
   *
   * Anomaly scores between different requests and datasets are comparable. As
   * a guideline, the risk of a slice being an anomaly based on the anomaly
   * score is:
   *
   * - **Very High** if `anomalyScore` > 5.
   * - **High** if the `anomalyScore` is in the [2, 5] range.
   * - **Medium** if the `anomalyScore` is in the [1, 2) range.
   * - **Low** if the `anomalyScore` is < 1.
   *
   * If there were issues evaluating this slice, then the anomaly score will be
   * set to -1.0 and the [status][google.cloud.timeseriesinsights.v1.EvaluatedSlice.status] field will contain details on what
   * went wrong.
   */
  anomalyScore?:
    | number
    | undefined;
  /**
   * The actual values in the `[`
   * [detectionTime][google.cloud.timeseriesinsights.v1.QueryDataSetRequest.detection_time] `-`
   * [forecastHistory][google.cloud.timeseriesinsights.v1.TimeseriesParams.forecast_history]`,`
   * [detectionTime][google.cloud.timeseriesinsights.v1.QueryDataSetRequest.detection_time] `]` time
   * range.
   *
   * **NOTE**: This field is only populated if
   * [returnTimeseries][google.cloud.timeseriesinsights.v1.QueryDataSetRequest.return_timeseries] is true.
   */
  history:
    | Timeseries
    | undefined;
  /**
   * The forecasted values in the `[`
   * [detectionTime][google.cloud.timeseriesinsights.v1.QueryDataSetRequest.detection_time] `+`
   * [granularity][google.cloud.timeseriesinsights.v1.TimeseriesParams.granularity]`,`
   * [forecastParams.horizonTime][google.cloud.timeseriesinsights.v1.QueryDataSetRequest.forecast_params] `]` time
   * range.
   *
   * **NOTE**: This field is only populated if
   * [returnTimeseries][google.cloud.timeseriesinsights.v1.QueryDataSetRequest.return_timeseries] is true.
   */
  forecast:
    | Timeseries
    | undefined;
  /**
   * Evaluation status. Contains an error message if the `anomalyScore` is < 0.
   *
   * Possible error messages:
   *
   * - **"Time series too sparse"**: The returned time series for this slice did
   * not contain enough data points (we require a minimum of 10).
   * - **"Not enough recent time series points"**: The time series contains the
   * minimum of 10 points, but there are not enough close in time to the
   * detection point.
   * - **"Missing detection point data"**: There were not events to be
   * aggregated within the `[detectionTime, detectionTime + granularity]` time
   * interval, so we don't have an actual value with which we can compare our
   * prediction.
   * - **"Data retrieval error"**: We failed to retrieve the time series data
   * for this slice and could not evaluate it successfully. Should be a
   * transient error.
   * - **"Internal server error"**: Internal unexpected error.
   */
  status: Status | undefined;
}

/**
 * Parameters that control how we slice the dataset and, optionally, filter
 * slices that have some specific values on some dimensions (pinned dimensions).
 */
export interface SlicingParams {
  /**
   * Required. Dimensions over which we will group the events in slices. The names
   * specified here come from the
   * [EventDimension.name][google.cloud.timeseriesinsights.v1.EventDimension.name] field. At least
   * one dimension name must be specified. All dimension names that do not exist
   * in the queried `DataSet` will be ignored.
   *
   * Currently only dimensions that hold string values can be specified here.
   */
  dimensionNames: string[];
  /**
   * Optional. We will only analyze slices for which
   * [EvaluatedSlice.dimensions][google.cloud.timeseriesinsights.v1.EvaluatedSlice.dimensions] contain all of the
   * following pinned dimensions. A query with a pinned dimension `{ name: "d3"
   * stringVal: "v3" }` will only analyze events which contain the dimension `{
   * name: "d3" stringVal: "v3" }`.
   * The [pinnedDimensions][google.cloud.timeseriesinsights.v1.SlicingParams.pinned_dimensions] and
   * [dimensionNames][google.cloud.timeseriesinsights.v1.SlicingParams.dimension_names] fields can **not**
   * share the same dimension names.
   *
   * Example a valid specification:
   *
   * ```json
   * {
   *   dimensionNames: ["d1", "d2"],
   *   pinnedDimensions: [
   *     { name: "d3" stringVal: "v3" },
   *     { name: "d4" stringVal: "v4" }
   *   ]
   * }
   * ```
   *
   * In the previous example we will slice the dataset by dimensions "d1",
   * "d2", "d3" and "d4", but we will only analyze slices for which "d3=v3" and
   * "d4=v4".
   *
   * The following example is **invalid** as "d2" is present in both
   * dimensionNames and pinnedDimensions:
   *
   * ```json
   * {
   *   dimensionNames: ["d1", "d2"],
   *   pinnedDimensions: [
   *     { name: "d2" stringVal: "v2" },
   *     { name: "d4" stringVal: "v4" }
   *   ]
   * }
   * ```
   */
  pinnedDimensions: PinnedDimension[];
}

/** Parameters that control how we construct the time series for each slice. */
export interface TimeseriesParams {
  /**
   * Required. How long should we go in the past when fetching the timeline used for
   * forecasting each slice.
   *
   * This is used in combination with the
   * [detectionTime][google.cloud.timeseriesinsights.v1.QueryDataSetRequest.detection_time] parameter.
   * The time series we construct will have the following time range:
   * `[detectionTime - forecastHistory, detectionTime + granularity]`.
   *
   * The forecast history might be rounded up, so that a multiple of
   * `granularity` is used to process the query.
   *
   * Note: If there are not enough events in the
   * `[detectionTime - forecastHistory, detectionTime + granularity]` time
   * interval, the slice evaluation can fail. For more information, see
   * [EvaluatedSlice.status][google.cloud.timeseriesinsights.v1.EvaluatedSlice.status].
   */
  forecastHistory:
    | Duration
    | undefined;
  /**
   * Required. The time granularity of the time series (on the x-axis). Each time series
   * point starting at time T will aggregate all events for a particular slice
   * in *[T, T + granularity)* time windows.
   *
   * Note: The aggregation is decided based on the
   * [metric][google.cloud.timeseriesinsights.v1.TimeseriesParams.metric] parameter.
   *
   * This granularity defines the query-time aggregation windows and is not
   * necessarily related to any event time granularity in the raw data (though
   * we do recommend that the query-time granularity is not finer than the
   * ingestion-time one).
   *
   * Currently, the minimal supported granularity is 10 seconds.
   */
  granularity:
    | Duration
    | undefined;
  /**
   * Optional. Denotes the [name][google.cloud.timeseriesinsights.v1.EventDimension.name] of a numerical
   * dimension that will have its values aggregated to compute the y-axis of the
   * time series.
   *
   * The aggregation method must also be specified by setting the
   * [metricAggregationMethod][google.cloud.timeseriesinsights.v1.TimeseriesParams.metric_aggregation_method]
   * field.
   *
   * Note: Currently, if the aggregation method is unspecified, we will
   * default to SUM for backward compatibility reasons, but new implementations
   * should set the
   * [metricAggregationMethod][google.cloud.timeseriesinsights.v1.TimeseriesParams.metric_aggregation_method]
   * explicitly.
   *
   * If the metric is unspecified, we will use the number of events that each
   * time series point contains as the point value.
   *
   * Example: Let's assume we have the following three events in our dataset:
   * ```json
   * {
   *   eventTime: "2020-12-27T00:00:00Z",
   *   dimensions: [
   *     { name: "d1" stringVal: "v1" },
   *     { name: "d2" stringVal: "v2" }
   *     { name: "m1" longVal: 100 }
   *     { name: "m2" longVal: 11 }
   *   ]
   * },
   * {
   *   eventTime: "2020-12-27T00:10:00Z",
   *   dimensions: [
   *     { name: "d1" stringVal: "v1" },
   *     { name: "d2" stringVal: "v2" }
   *     { name: "m1" longVal: 200 }
   *     { name: "m2" longVal: 22 }
   *   ]
   * },
   * {
   *   eventTime: "2020-12-27T00:20:00Z",
   *   dimensions: [
   *     { name: "d1" stringVal: "v1" },
   *     { name: "d2" stringVal: "v2" }
   *     { name: "m1" longVal: 300 }
   *     { name: "m2" longVal: 33 }
   *   ]
   * }
   * ```
   *
   * These events are all within the same hour, spaced 10 minutes between each
   * of them. Assuming our [QueryDataSetRequest][google.cloud.timeseriesinsights.v1.QueryDataSetRequest] had set
   * [slicingParams.dimensionNames][google.cloud.timeseriesinsights.v1.SlicingParams.dimension_names] to ["d1",
   * "d2"] and [timeseries_params.granularity][google.cloud.timeseriesinsights.v1.TimeseriesParams.granularity] to
   * "3600s", then all the previous events will be aggregated into the same
   * [timeseries point][google.cloud.timeseriesinsights.v1.TimeseriesPoint].
   *
   * The time series point that they're all part of will have the
   * [time][google.cloud.timeseriesinsights.v1.TimeseriesPoint.time] set to "2020-12-27T00:00:00Z" and the
   * [value][google.cloud.timeseriesinsights.v1.TimeseriesPoint.value] populated based on this metric field:
   *
   * - If the metric is set to "m1" and metric_aggregation_method to SUM, then
   * the value of the point will be 600.
   * - If the metric is set to "m2" and metric_aggregation_method to SUM, then
   * the value of the point will be 66.
   * - If the metric is set to "m1" and metric_aggregation_method to AVERAGE,
   * then the value of the point will be 200.
   * - If the metric is set to "m2" and metric_aggregation_method to AVERAGE,
   * then the value of the point will be 22.
   * - If the metric field is "" or unspecified, then the value of the point
   * will be 3, as we will simply count the events.
   */
  metric?:
    | string
    | undefined;
  /**
   * Optional. Together with the [metric][google.cloud.timeseriesinsights.v1.TimeseriesParams.metric] field, specifies how
   * we will aggregate multiple events to obtain the value of a time series
   * point. See the [metric][google.cloud.timeseriesinsights.v1.TimeseriesParams.metric] documentation for more
   * details.
   *
   * If the metric is not specified or "", then this field will be ignored.
   */
  metricAggregationMethod: TimeseriesParams_AggregationMethod;
}

/**
 * Methods by which we can aggregate multiple events by a given
 * [metric][google.cloud.timeseriesinsights.v1.TimeseriesParams.metric].
 */
export enum TimeseriesParams_AggregationMethod {
  /** AGGREGATION_METHOD_UNSPECIFIED - Unspecified. */
  AGGREGATION_METHOD_UNSPECIFIED = 0,
  /**
   * SUM - Aggregate multiple events by summing up the values found in the
   * [metric][google.cloud.timeseriesinsights.v1.TimeseriesParams.metric] dimension.
   */
  SUM = 1,
  /**
   * AVERAGE - Aggregate multiple events by averaging out the values found in the
   * [metric][google.cloud.timeseriesinsights.v1.TimeseriesParams.metric] dimension.
   */
  AVERAGE = 2,
  UNRECOGNIZED = -1,
}

export function timeseriesParams_AggregationMethodFromJSON(object: any): TimeseriesParams_AggregationMethod {
  switch (object) {
    case 0:
    case "AGGREGATION_METHOD_UNSPECIFIED":
      return TimeseriesParams_AggregationMethod.AGGREGATION_METHOD_UNSPECIFIED;
    case 1:
    case "SUM":
      return TimeseriesParams_AggregationMethod.SUM;
    case 2:
    case "AVERAGE":
      return TimeseriesParams_AggregationMethod.AVERAGE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return TimeseriesParams_AggregationMethod.UNRECOGNIZED;
  }
}

export function timeseriesParams_AggregationMethodToJSON(object: TimeseriesParams_AggregationMethod): string {
  switch (object) {
    case TimeseriesParams_AggregationMethod.AGGREGATION_METHOD_UNSPECIFIED:
      return "AGGREGATION_METHOD_UNSPECIFIED";
    case TimeseriesParams_AggregationMethod.SUM:
      return "SUM";
    case TimeseriesParams_AggregationMethod.AVERAGE:
      return "AVERAGE";
    case TimeseriesParams_AggregationMethod.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Request for performing a query against a loaded DataSet. */
export interface QueryDataSetRequest {
  /**
   * Required. Loaded DataSet to be queried in the format of
   * "projects/{project}/datasets/{dataset}"
   */
  name: string;
  /**
   * Required. This is the point in time that we want to probe for anomalies.
   *
   * The corresponding [TimeseriesPoint][google.cloud.timeseriesinsights.v1.TimeseriesPoint] is referred to as the
   * detection point.
   *
   * **NOTE**: As with any other time series point, the value is given by
   * aggregating all events in the slice that are in the
   * [detectionTime, detectionTime + granularity) time interval, where
   * the granularity is specified in the
   * [timeseriesParams.granularity][google.cloud.timeseriesinsights.v1.TimeseriesParams.granularity] field.
   */
  detectionTime:
    | Date
    | undefined;
  /**
   * How many slices are returned in
   * [QueryDataSetResponse.slices][google.cloud.timeseriesinsights.v1.QueryDataSetResponse.slices].
   *
   * The returned slices are tentatively the ones with the highest
   * [anomaly scores][google.cloud.timeseriesinsights.v1.EvaluatedSlice.anomaly_score] in the dataset that match
   * the query, but it is not guaranteed.
   *
   * Reducing this number will improve query performance, both in terms of
   * latency and resource usage.
   *
   * Defaults to 50.
   */
  numReturnedSlices?:
    | number
    | undefined;
  /**
   * Parameters controlling how we will split the dataset into the slices that
   * we will analyze.
   */
  slicingParams:
    | SlicingParams
    | undefined;
  /**
   * Parameters controlling how we will build the time series used to predict
   * the [detectionTime][google.cloud.timeseriesinsights.v1.QueryDataSetRequest.detection_time] value for each slice.
   */
  timeseriesParams:
    | TimeseriesParams
    | undefined;
  /**
   * Parameters that control the time series forecasting models, such as the
   * sensitivity of the anomaly detection.
   */
  forecastParams:
    | ForecastParams
    | undefined;
  /**
   * If specified, we will return the actual and forecasted time for all
   * returned slices.
   *
   * The time series are returned in the
   * [EvaluatedSlice.history][google.cloud.timeseriesinsights.v1.EvaluatedSlice.history] and
   * [EvaluatedSlice.forecast][google.cloud.timeseriesinsights.v1.EvaluatedSlice.forecast] fields.
   */
  returnTimeseries: boolean;
}

/** Response for a query executed by the system. */
export interface QueryDataSetResponse {
  /** Loaded DataSet that was queried. */
  name: string;
  /**
   * Slices sorted in descending order by their
   * [anomalyScore][google.cloud.timeseriesinsights.v1.EvaluatedSlice.anomaly_score].
   *
   * At most [numReturnedSlices][google.cloud.timeseriesinsights.v1.QueryDataSetRequest.num_returned_slices]
   * slices are present in this field.
   */
  slices: EvaluatedSlice[];
}

/** Request for evaluateSlice. */
export interface EvaluateSliceRequest {
  /**
   * Required. Loaded DataSet to be queried in the format of
   * "projects/{project}/datasets/{dataset}"
   */
  dataset: string;
  /**
   * Required. Dimensions with pinned values that specify the slice for which we will
   * fetch the time series.
   */
  pinnedDimensions: PinnedDimension[];
  /**
   * Required. This is the point in time that we want to probe for anomalies.
   *
   * See documentation for
   * [QueryDataSetRequest.detectionTime][google.cloud.timeseriesinsights.v1.QueryDataSetRequest.detection_time].
   */
  detectionTime:
    | Date
    | undefined;
  /**
   * Parameters controlling how we will build the time series used to predict
   * the [detectionTime][google.cloud.timeseriesinsights.v1.EvaluateSliceRequest.detection_time] value for this slice.
   */
  timeseriesParams:
    | TimeseriesParams
    | undefined;
  /**
   * Parameters that control the time series forecasting models, such as the
   * sensitivity of the anomaly detection.
   */
  forecastParams: ForecastParams | undefined;
}

/** Request for evaluateTimeseries. */
export interface EvaluateTimeseriesRequest {
  /** Required. Client project name in the format of 'projects/{project}'. */
  parent: string;
  /**
   * Evaluate this time series without requiring it was previously loaded in
   * a data set.
   *
   * The evaluated time series point is the last one, analogous to calling
   * evaluateSlice or query with
   * [detectionTime][google.cloud.timeseriesinsights.v1.EvaluateSliceRequest.detection_time] set to
   * `timeseries.point(timeseries.point_size() - 1).time`.
   *
   * The length of the time series must be at least 10.
   *
   * All points must have the same time offset relative to the granularity. For
   * example, if the [granularity][google.cloud.timeseriesinsights.v1.EvaluateTimeseriesRequest.granularity] is "5s", then the following
   * point.time sequences are valid:
   * - "100s", "105s", "120s", "125s" (no offset)
   * - "102s", "107s", "122s", "127s" (offset is "2s")
   * However, the following sequence is invalid as it has inconsistent offsets:
   * - "100s", "105s", "122s", "127s" (offsets are either "0s" or "2s")
   */
  timeseries:
    | Timeseries
    | undefined;
  /**
   * The granularity of the time series (time distance between two consecutive
   * points).
   */
  granularity:
    | Duration
    | undefined;
  /** The forecast parameters. */
  forecastParams: ForecastParams | undefined;
}

function createBaseBigqueryMapping(): BigqueryMapping {
  return { timestampColumn: "", groupIdColumn: "", dimensionColumn: [] };
}

export const BigqueryMapping: MessageFns<BigqueryMapping> = {
  encode(message: BigqueryMapping, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.timestampColumn !== "") {
      writer.uint32(10).string(message.timestampColumn);
    }
    if (message.groupIdColumn !== "") {
      writer.uint32(18).string(message.groupIdColumn);
    }
    for (const v of message.dimensionColumn) {
      writer.uint32(26).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigqueryMapping {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigqueryMapping();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.timestampColumn = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.groupIdColumn = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.dimensionColumn.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigqueryMapping {
    return {
      timestampColumn: isSet(object.timestampColumn) ? globalThis.String(object.timestampColumn) : "",
      groupIdColumn: isSet(object.groupIdColumn) ? globalThis.String(object.groupIdColumn) : "",
      dimensionColumn: globalThis.Array.isArray(object?.dimensionColumn)
        ? object.dimensionColumn.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: BigqueryMapping): unknown {
    const obj: any = {};
    if (message.timestampColumn !== "") {
      obj.timestampColumn = message.timestampColumn;
    }
    if (message.groupIdColumn !== "") {
      obj.groupIdColumn = message.groupIdColumn;
    }
    if (message.dimensionColumn?.length) {
      obj.dimensionColumn = message.dimensionColumn;
    }
    return obj;
  },

  create(base?: DeepPartial<BigqueryMapping>): BigqueryMapping {
    return BigqueryMapping.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigqueryMapping>): BigqueryMapping {
    const message = createBaseBigqueryMapping();
    message.timestampColumn = object.timestampColumn ?? "";
    message.groupIdColumn = object.groupIdColumn ?? "";
    message.dimensionColumn = object.dimensionColumn?.map((e) => e) || [];
    return message;
  },
};

function createBaseDataSource(): DataSource {
  return { uri: "", bqMapping: undefined };
}

export const DataSource: MessageFns<DataSource> = {
  encode(message: DataSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.uri !== "") {
      writer.uint32(10).string(message.uri);
    }
    if (message.bqMapping !== undefined) {
      BigqueryMapping.encode(message.bqMapping, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DataSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDataSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.uri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.bqMapping = BigqueryMapping.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DataSource {
    return {
      uri: isSet(object.uri) ? globalThis.String(object.uri) : "",
      bqMapping: isSet(object.bqMapping) ? BigqueryMapping.fromJSON(object.bqMapping) : undefined,
    };
  },

  toJSON(message: DataSource): unknown {
    const obj: any = {};
    if (message.uri !== "") {
      obj.uri = message.uri;
    }
    if (message.bqMapping !== undefined) {
      obj.bqMapping = BigqueryMapping.toJSON(message.bqMapping);
    }
    return obj;
  },

  create(base?: DeepPartial<DataSource>): DataSource {
    return DataSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DataSource>): DataSource {
    const message = createBaseDataSource();
    message.uri = object.uri ?? "";
    message.bqMapping = (object.bqMapping !== undefined && object.bqMapping !== null)
      ? BigqueryMapping.fromPartial(object.bqMapping)
      : undefined;
    return message;
  },
};

function createBaseDataSet(): DataSet {
  return { name: "", dataNames: [], dataSources: [], state: 0, status: undefined, ttl: undefined };
}

export const DataSet: MessageFns<DataSet> = {
  encode(message: DataSet, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    for (const v of message.dataNames) {
      writer.uint32(18).string(v!);
    }
    for (const v of message.dataSources) {
      DataSource.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.state !== 0) {
      writer.uint32(32).int32(message.state);
    }
    if (message.status !== undefined) {
      Status.encode(message.status, writer.uint32(42).fork()).join();
    }
    if (message.ttl !== undefined) {
      Duration.encode(message.ttl, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DataSet {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDataSet();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.dataNames.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.dataSources.push(DataSource.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.status = Status.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.ttl = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DataSet {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      dataNames: globalThis.Array.isArray(object?.dataNames)
        ? object.dataNames.map((e: any) => globalThis.String(e))
        : [],
      dataSources: globalThis.Array.isArray(object?.dataSources)
        ? object.dataSources.map((e: any) => DataSource.fromJSON(e))
        : [],
      state: isSet(object.state) ? dataSet_StateFromJSON(object.state) : 0,
      status: isSet(object.status) ? Status.fromJSON(object.status) : undefined,
      ttl: isSet(object.ttl) ? Duration.fromJSON(object.ttl) : undefined,
    };
  },

  toJSON(message: DataSet): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.dataNames?.length) {
      obj.dataNames = message.dataNames;
    }
    if (message.dataSources?.length) {
      obj.dataSources = message.dataSources.map((e) => DataSource.toJSON(e));
    }
    if (message.state !== 0) {
      obj.state = dataSet_StateToJSON(message.state);
    }
    if (message.status !== undefined) {
      obj.status = Status.toJSON(message.status);
    }
    if (message.ttl !== undefined) {
      obj.ttl = Duration.toJSON(message.ttl);
    }
    return obj;
  },

  create(base?: DeepPartial<DataSet>): DataSet {
    return DataSet.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DataSet>): DataSet {
    const message = createBaseDataSet();
    message.name = object.name ?? "";
    message.dataNames = object.dataNames?.map((e) => e) || [];
    message.dataSources = object.dataSources?.map((e) => DataSource.fromPartial(e)) || [];
    message.state = object.state ?? 0;
    message.status = (object.status !== undefined && object.status !== null)
      ? Status.fromPartial(object.status)
      : undefined;
    message.ttl = (object.ttl !== undefined && object.ttl !== null) ? Duration.fromPartial(object.ttl) : undefined;
    return message;
  },
};

function createBaseEventDimension(): EventDimension {
  return { name: "", stringVal: undefined, longVal: undefined, boolVal: undefined, doubleVal: undefined };
}

export const EventDimension: MessageFns<EventDimension> = {
  encode(message: EventDimension, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.stringVal !== undefined) {
      writer.uint32(18).string(message.stringVal);
    }
    if (message.longVal !== undefined) {
      writer.uint32(24).int64(message.longVal.toString());
    }
    if (message.boolVal !== undefined) {
      writer.uint32(32).bool(message.boolVal);
    }
    if (message.doubleVal !== undefined) {
      writer.uint32(41).double(message.doubleVal);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EventDimension {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEventDimension();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.stringVal = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.longVal = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.boolVal = reader.bool();
          continue;
        case 5:
          if (tag !== 41) {
            break;
          }

          message.doubleVal = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EventDimension {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      stringVal: isSet(object.stringVal) ? globalThis.String(object.stringVal) : undefined,
      longVal: isSet(object.longVal) ? Long.fromValue(object.longVal) : undefined,
      boolVal: isSet(object.boolVal) ? globalThis.Boolean(object.boolVal) : undefined,
      doubleVal: isSet(object.doubleVal) ? globalThis.Number(object.doubleVal) : undefined,
    };
  },

  toJSON(message: EventDimension): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.stringVal !== undefined) {
      obj.stringVal = message.stringVal;
    }
    if (message.longVal !== undefined) {
      obj.longVal = (message.longVal || Long.ZERO).toString();
    }
    if (message.boolVal !== undefined) {
      obj.boolVal = message.boolVal;
    }
    if (message.doubleVal !== undefined) {
      obj.doubleVal = message.doubleVal;
    }
    return obj;
  },

  create(base?: DeepPartial<EventDimension>): EventDimension {
    return EventDimension.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EventDimension>): EventDimension {
    const message = createBaseEventDimension();
    message.name = object.name ?? "";
    message.stringVal = object.stringVal ?? undefined;
    message.longVal = (object.longVal !== undefined && object.longVal !== null)
      ? Long.fromValue(object.longVal)
      : undefined;
    message.boolVal = object.boolVal ?? undefined;
    message.doubleVal = object.doubleVal ?? undefined;
    return message;
  },
};

function createBaseEvent(): Event {
  return { dimensions: [], groupId: Long.ZERO, eventTime: undefined };
}

export const Event: MessageFns<Event> = {
  encode(message: Event, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.dimensions) {
      EventDimension.encode(v!, writer.uint32(10).fork()).join();
    }
    if (!message.groupId.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.groupId.toString());
    }
    if (message.eventTime !== undefined) {
      Timestamp.encode(toTimestamp(message.eventTime), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Event {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEvent();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dimensions.push(EventDimension.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.groupId = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.eventTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Event {
    return {
      dimensions: globalThis.Array.isArray(object?.dimensions)
        ? object.dimensions.map((e: any) => EventDimension.fromJSON(e))
        : [],
      groupId: isSet(object.groupId) ? Long.fromValue(object.groupId) : Long.ZERO,
      eventTime: isSet(object.eventTime) ? fromJsonTimestamp(object.eventTime) : undefined,
    };
  },

  toJSON(message: Event): unknown {
    const obj: any = {};
    if (message.dimensions?.length) {
      obj.dimensions = message.dimensions.map((e) => EventDimension.toJSON(e));
    }
    if (!message.groupId.equals(Long.ZERO)) {
      obj.groupId = (message.groupId || Long.ZERO).toString();
    }
    if (message.eventTime !== undefined) {
      obj.eventTime = message.eventTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<Event>): Event {
    return Event.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Event>): Event {
    const message = createBaseEvent();
    message.dimensions = object.dimensions?.map((e) => EventDimension.fromPartial(e)) || [];
    message.groupId = (object.groupId !== undefined && object.groupId !== null)
      ? Long.fromValue(object.groupId)
      : Long.ZERO;
    message.eventTime = object.eventTime ?? undefined;
    return message;
  },
};

function createBaseAppendEventsRequest(): AppendEventsRequest {
  return { events: [], dataset: "" };
}

export const AppendEventsRequest: MessageFns<AppendEventsRequest> = {
  encode(message: AppendEventsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.events) {
      Event.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.dataset !== "") {
      writer.uint32(18).string(message.dataset);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppendEventsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppendEventsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.events.push(Event.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.dataset = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AppendEventsRequest {
    return {
      events: globalThis.Array.isArray(object?.events) ? object.events.map((e: any) => Event.fromJSON(e)) : [],
      dataset: isSet(object.dataset) ? globalThis.String(object.dataset) : "",
    };
  },

  toJSON(message: AppendEventsRequest): unknown {
    const obj: any = {};
    if (message.events?.length) {
      obj.events = message.events.map((e) => Event.toJSON(e));
    }
    if (message.dataset !== "") {
      obj.dataset = message.dataset;
    }
    return obj;
  },

  create(base?: DeepPartial<AppendEventsRequest>): AppendEventsRequest {
    return AppendEventsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AppendEventsRequest>): AppendEventsRequest {
    const message = createBaseAppendEventsRequest();
    message.events = object.events?.map((e) => Event.fromPartial(e)) || [];
    message.dataset = object.dataset ?? "";
    return message;
  },
};

function createBaseAppendEventsResponse(): AppendEventsResponse {
  return { droppedEvents: [] };
}

export const AppendEventsResponse: MessageFns<AppendEventsResponse> = {
  encode(message: AppendEventsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.droppedEvents) {
      Event.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppendEventsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppendEventsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.droppedEvents.push(Event.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AppendEventsResponse {
    return {
      droppedEvents: globalThis.Array.isArray(object?.droppedEvents)
        ? object.droppedEvents.map((e: any) => Event.fromJSON(e))
        : [],
    };
  },

  toJSON(message: AppendEventsResponse): unknown {
    const obj: any = {};
    if (message.droppedEvents?.length) {
      obj.droppedEvents = message.droppedEvents.map((e) => Event.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<AppendEventsResponse>): AppendEventsResponse {
    return AppendEventsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AppendEventsResponse>): AppendEventsResponse {
    const message = createBaseAppendEventsResponse();
    message.droppedEvents = object.droppedEvents?.map((e) => Event.fromPartial(e)) || [];
    return message;
  },
};

function createBaseCreateDataSetRequest(): CreateDataSetRequest {
  return { parent: "", dataset: undefined };
}

export const CreateDataSetRequest: MessageFns<CreateDataSetRequest> = {
  encode(message: CreateDataSetRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.dataset !== undefined) {
      DataSet.encode(message.dataset, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateDataSetRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateDataSetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.dataset = DataSet.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateDataSetRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      dataset: isSet(object.dataset) ? DataSet.fromJSON(object.dataset) : undefined,
    };
  },

  toJSON(message: CreateDataSetRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.dataset !== undefined) {
      obj.dataset = DataSet.toJSON(message.dataset);
    }
    return obj;
  },

  create(base?: DeepPartial<CreateDataSetRequest>): CreateDataSetRequest {
    return CreateDataSetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateDataSetRequest>): CreateDataSetRequest {
    const message = createBaseCreateDataSetRequest();
    message.parent = object.parent ?? "";
    message.dataset = (object.dataset !== undefined && object.dataset !== null)
      ? DataSet.fromPartial(object.dataset)
      : undefined;
    return message;
  },
};

function createBaseDeleteDataSetRequest(): DeleteDataSetRequest {
  return { name: "" };
}

export const DeleteDataSetRequest: MessageFns<DeleteDataSetRequest> = {
  encode(message: DeleteDataSetRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeleteDataSetRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeleteDataSetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeleteDataSetRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: DeleteDataSetRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<DeleteDataSetRequest>): DeleteDataSetRequest {
    return DeleteDataSetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeleteDataSetRequest>): DeleteDataSetRequest {
    const message = createBaseDeleteDataSetRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseListDataSetsRequest(): ListDataSetsRequest {
  return { parent: "", pageSize: 0, pageToken: "" };
}

export const ListDataSetsRequest: MessageFns<ListDataSetsRequest> = {
  encode(message: ListDataSetsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.pageSize !== 0) {
      writer.uint32(16).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(26).string(message.pageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListDataSetsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListDataSetsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListDataSetsRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
    };
  },

  toJSON(message: ListDataSetsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListDataSetsRequest>): ListDataSetsRequest {
    return ListDataSetsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListDataSetsRequest>): ListDataSetsRequest {
    const message = createBaseListDataSetsRequest();
    message.parent = object.parent ?? "";
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    return message;
  },
};

function createBaseListDataSetsResponse(): ListDataSetsResponse {
  return { datasets: [], nextPageToken: "" };
}

export const ListDataSetsResponse: MessageFns<ListDataSetsResponse> = {
  encode(message: ListDataSetsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.datasets) {
      DataSet.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListDataSetsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListDataSetsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.datasets.push(DataSet.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListDataSetsResponse {
    return {
      datasets: globalThis.Array.isArray(object?.datasets) ? object.datasets.map((e: any) => DataSet.fromJSON(e)) : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: ListDataSetsResponse): unknown {
    const obj: any = {};
    if (message.datasets?.length) {
      obj.datasets = message.datasets.map((e) => DataSet.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListDataSetsResponse>): ListDataSetsResponse {
    return ListDataSetsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListDataSetsResponse>): ListDataSetsResponse {
    const message = createBaseListDataSetsResponse();
    message.datasets = object.datasets?.map((e) => DataSet.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBasePinnedDimension(): PinnedDimension {
  return { name: "", stringVal: undefined, boolVal: undefined };
}

export const PinnedDimension: MessageFns<PinnedDimension> = {
  encode(message: PinnedDimension, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.stringVal !== undefined) {
      writer.uint32(18).string(message.stringVal);
    }
    if (message.boolVal !== undefined) {
      writer.uint32(24).bool(message.boolVal);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PinnedDimension {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePinnedDimension();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.stringVal = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.boolVal = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PinnedDimension {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      stringVal: isSet(object.stringVal) ? globalThis.String(object.stringVal) : undefined,
      boolVal: isSet(object.boolVal) ? globalThis.Boolean(object.boolVal) : undefined,
    };
  },

  toJSON(message: PinnedDimension): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.stringVal !== undefined) {
      obj.stringVal = message.stringVal;
    }
    if (message.boolVal !== undefined) {
      obj.boolVal = message.boolVal;
    }
    return obj;
  },

  create(base?: DeepPartial<PinnedDimension>): PinnedDimension {
    return PinnedDimension.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PinnedDimension>): PinnedDimension {
    const message = createBasePinnedDimension();
    message.name = object.name ?? "";
    message.stringVal = object.stringVal ?? undefined;
    message.boolVal = object.boolVal ?? undefined;
    return message;
  },
};

function createBaseForecastParams(): ForecastParams {
  return { noiseThreshold: undefined, seasonalityHint: 0, horizonDuration: undefined };
}

export const ForecastParams: MessageFns<ForecastParams> = {
  encode(message: ForecastParams, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.noiseThreshold !== undefined) {
      writer.uint32(97).double(message.noiseThreshold);
    }
    if (message.seasonalityHint !== 0) {
      writer.uint32(80).int32(message.seasonalityHint);
    }
    if (message.horizonDuration !== undefined) {
      Duration.encode(message.horizonDuration, writer.uint32(106).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ForecastParams {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseForecastParams();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 12:
          if (tag !== 97) {
            break;
          }

          message.noiseThreshold = reader.double();
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.seasonalityHint = reader.int32() as any;
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.horizonDuration = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ForecastParams {
    return {
      noiseThreshold: isSet(object.noiseThreshold) ? globalThis.Number(object.noiseThreshold) : undefined,
      seasonalityHint: isSet(object.seasonalityHint) ? forecastParams_PeriodFromJSON(object.seasonalityHint) : 0,
      horizonDuration: isSet(object.horizonDuration) ? Duration.fromJSON(object.horizonDuration) : undefined,
    };
  },

  toJSON(message: ForecastParams): unknown {
    const obj: any = {};
    if (message.noiseThreshold !== undefined) {
      obj.noiseThreshold = message.noiseThreshold;
    }
    if (message.seasonalityHint !== 0) {
      obj.seasonalityHint = forecastParams_PeriodToJSON(message.seasonalityHint);
    }
    if (message.horizonDuration !== undefined) {
      obj.horizonDuration = Duration.toJSON(message.horizonDuration);
    }
    return obj;
  },

  create(base?: DeepPartial<ForecastParams>): ForecastParams {
    return ForecastParams.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ForecastParams>): ForecastParams {
    const message = createBaseForecastParams();
    message.noiseThreshold = object.noiseThreshold ?? undefined;
    message.seasonalityHint = object.seasonalityHint ?? 0;
    message.horizonDuration = (object.horizonDuration !== undefined && object.horizonDuration !== null)
      ? Duration.fromPartial(object.horizonDuration)
      : undefined;
    return message;
  },
};

function createBaseTimeseriesPoint(): TimeseriesPoint {
  return { time: undefined, value: undefined };
}

export const TimeseriesPoint: MessageFns<TimeseriesPoint> = {
  encode(message: TimeseriesPoint, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.time !== undefined) {
      Timestamp.encode(toTimestamp(message.time), writer.uint32(10).fork()).join();
    }
    if (message.value !== undefined) {
      writer.uint32(17).double(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TimeseriesPoint {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTimeseriesPoint();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.time = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.value = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TimeseriesPoint {
    return {
      time: isSet(object.time) ? fromJsonTimestamp(object.time) : undefined,
      value: isSet(object.value) ? globalThis.Number(object.value) : undefined,
    };
  },

  toJSON(message: TimeseriesPoint): unknown {
    const obj: any = {};
    if (message.time !== undefined) {
      obj.time = message.time.toISOString();
    }
    if (message.value !== undefined) {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<TimeseriesPoint>): TimeseriesPoint {
    return TimeseriesPoint.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TimeseriesPoint>): TimeseriesPoint {
    const message = createBaseTimeseriesPoint();
    message.time = object.time ?? undefined;
    message.value = object.value ?? undefined;
    return message;
  },
};

function createBaseTimeseries(): Timeseries {
  return { point: [] };
}

export const Timeseries: MessageFns<Timeseries> = {
  encode(message: Timeseries, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.point) {
      TimeseriesPoint.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Timeseries {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTimeseries();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.point.push(TimeseriesPoint.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Timeseries {
    return {
      point: globalThis.Array.isArray(object?.point) ? object.point.map((e: any) => TimeseriesPoint.fromJSON(e)) : [],
    };
  },

  toJSON(message: Timeseries): unknown {
    const obj: any = {};
    if (message.point?.length) {
      obj.point = message.point.map((e) => TimeseriesPoint.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<Timeseries>): Timeseries {
    return Timeseries.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Timeseries>): Timeseries {
    const message = createBaseTimeseries();
    message.point = object.point?.map((e) => TimeseriesPoint.fromPartial(e)) || [];
    return message;
  },
};

function createBaseEvaluatedSlice(): EvaluatedSlice {
  return {
    dimensions: [],
    detectionPointActual: undefined,
    detectionPointForecast: undefined,
    expectedDeviation: undefined,
    anomalyScore: undefined,
    history: undefined,
    forecast: undefined,
    status: undefined,
  };
}

export const EvaluatedSlice: MessageFns<EvaluatedSlice> = {
  encode(message: EvaluatedSlice, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.dimensions) {
      PinnedDimension.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.detectionPointActual !== undefined) {
      writer.uint32(89).double(message.detectionPointActual);
    }
    if (message.detectionPointForecast !== undefined) {
      writer.uint32(97).double(message.detectionPointForecast);
    }
    if (message.expectedDeviation !== undefined) {
      writer.uint32(129).double(message.expectedDeviation);
    }
    if (message.anomalyScore !== undefined) {
      writer.uint32(137).double(message.anomalyScore);
    }
    if (message.history !== undefined) {
      Timeseries.encode(message.history, writer.uint32(42).fork()).join();
    }
    if (message.forecast !== undefined) {
      Timeseries.encode(message.forecast, writer.uint32(82).fork()).join();
    }
    if (message.status !== undefined) {
      Status.encode(message.status, writer.uint32(146).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EvaluatedSlice {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEvaluatedSlice();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dimensions.push(PinnedDimension.decode(reader, reader.uint32()));
          continue;
        case 11:
          if (tag !== 89) {
            break;
          }

          message.detectionPointActual = reader.double();
          continue;
        case 12:
          if (tag !== 97) {
            break;
          }

          message.detectionPointForecast = reader.double();
          continue;
        case 16:
          if (tag !== 129) {
            break;
          }

          message.expectedDeviation = reader.double();
          continue;
        case 17:
          if (tag !== 137) {
            break;
          }

          message.anomalyScore = reader.double();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.history = Timeseries.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.forecast = Timeseries.decode(reader, reader.uint32());
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.status = Status.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EvaluatedSlice {
    return {
      dimensions: globalThis.Array.isArray(object?.dimensions)
        ? object.dimensions.map((e: any) => PinnedDimension.fromJSON(e))
        : [],
      detectionPointActual: isSet(object.detectionPointActual)
        ? globalThis.Number(object.detectionPointActual)
        : undefined,
      detectionPointForecast: isSet(object.detectionPointForecast)
        ? globalThis.Number(object.detectionPointForecast)
        : undefined,
      expectedDeviation: isSet(object.expectedDeviation) ? globalThis.Number(object.expectedDeviation) : undefined,
      anomalyScore: isSet(object.anomalyScore) ? globalThis.Number(object.anomalyScore) : undefined,
      history: isSet(object.history) ? Timeseries.fromJSON(object.history) : undefined,
      forecast: isSet(object.forecast) ? Timeseries.fromJSON(object.forecast) : undefined,
      status: isSet(object.status) ? Status.fromJSON(object.status) : undefined,
    };
  },

  toJSON(message: EvaluatedSlice): unknown {
    const obj: any = {};
    if (message.dimensions?.length) {
      obj.dimensions = message.dimensions.map((e) => PinnedDimension.toJSON(e));
    }
    if (message.detectionPointActual !== undefined) {
      obj.detectionPointActual = message.detectionPointActual;
    }
    if (message.detectionPointForecast !== undefined) {
      obj.detectionPointForecast = message.detectionPointForecast;
    }
    if (message.expectedDeviation !== undefined) {
      obj.expectedDeviation = message.expectedDeviation;
    }
    if (message.anomalyScore !== undefined) {
      obj.anomalyScore = message.anomalyScore;
    }
    if (message.history !== undefined) {
      obj.history = Timeseries.toJSON(message.history);
    }
    if (message.forecast !== undefined) {
      obj.forecast = Timeseries.toJSON(message.forecast);
    }
    if (message.status !== undefined) {
      obj.status = Status.toJSON(message.status);
    }
    return obj;
  },

  create(base?: DeepPartial<EvaluatedSlice>): EvaluatedSlice {
    return EvaluatedSlice.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EvaluatedSlice>): EvaluatedSlice {
    const message = createBaseEvaluatedSlice();
    message.dimensions = object.dimensions?.map((e) => PinnedDimension.fromPartial(e)) || [];
    message.detectionPointActual = object.detectionPointActual ?? undefined;
    message.detectionPointForecast = object.detectionPointForecast ?? undefined;
    message.expectedDeviation = object.expectedDeviation ?? undefined;
    message.anomalyScore = object.anomalyScore ?? undefined;
    message.history = (object.history !== undefined && object.history !== null)
      ? Timeseries.fromPartial(object.history)
      : undefined;
    message.forecast = (object.forecast !== undefined && object.forecast !== null)
      ? Timeseries.fromPartial(object.forecast)
      : undefined;
    message.status = (object.status !== undefined && object.status !== null)
      ? Status.fromPartial(object.status)
      : undefined;
    return message;
  },
};

function createBaseSlicingParams(): SlicingParams {
  return { dimensionNames: [], pinnedDimensions: [] };
}

export const SlicingParams: MessageFns<SlicingParams> = {
  encode(message: SlicingParams, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.dimensionNames) {
      writer.uint32(10).string(v!);
    }
    for (const v of message.pinnedDimensions) {
      PinnedDimension.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SlicingParams {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSlicingParams();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dimensionNames.push(reader.string());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.pinnedDimensions.push(PinnedDimension.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SlicingParams {
    return {
      dimensionNames: globalThis.Array.isArray(object?.dimensionNames)
        ? object.dimensionNames.map((e: any) => globalThis.String(e))
        : [],
      pinnedDimensions: globalThis.Array.isArray(object?.pinnedDimensions)
        ? object.pinnedDimensions.map((e: any) => PinnedDimension.fromJSON(e))
        : [],
    };
  },

  toJSON(message: SlicingParams): unknown {
    const obj: any = {};
    if (message.dimensionNames?.length) {
      obj.dimensionNames = message.dimensionNames;
    }
    if (message.pinnedDimensions?.length) {
      obj.pinnedDimensions = message.pinnedDimensions.map((e) => PinnedDimension.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<SlicingParams>): SlicingParams {
    return SlicingParams.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SlicingParams>): SlicingParams {
    const message = createBaseSlicingParams();
    message.dimensionNames = object.dimensionNames?.map((e) => e) || [];
    message.pinnedDimensions = object.pinnedDimensions?.map((e) => PinnedDimension.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTimeseriesParams(): TimeseriesParams {
  return { forecastHistory: undefined, granularity: undefined, metric: undefined, metricAggregationMethod: 0 };
}

export const TimeseriesParams: MessageFns<TimeseriesParams> = {
  encode(message: TimeseriesParams, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.forecastHistory !== undefined) {
      Duration.encode(message.forecastHistory, writer.uint32(10).fork()).join();
    }
    if (message.granularity !== undefined) {
      Duration.encode(message.granularity, writer.uint32(18).fork()).join();
    }
    if (message.metric !== undefined) {
      writer.uint32(34).string(message.metric);
    }
    if (message.metricAggregationMethod !== 0) {
      writer.uint32(40).int32(message.metricAggregationMethod);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TimeseriesParams {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTimeseriesParams();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.forecastHistory = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.granularity = Duration.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.metric = reader.string();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.metricAggregationMethod = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TimeseriesParams {
    return {
      forecastHistory: isSet(object.forecastHistory) ? Duration.fromJSON(object.forecastHistory) : undefined,
      granularity: isSet(object.granularity) ? Duration.fromJSON(object.granularity) : undefined,
      metric: isSet(object.metric) ? globalThis.String(object.metric) : undefined,
      metricAggregationMethod: isSet(object.metricAggregationMethod)
        ? timeseriesParams_AggregationMethodFromJSON(object.metricAggregationMethod)
        : 0,
    };
  },

  toJSON(message: TimeseriesParams): unknown {
    const obj: any = {};
    if (message.forecastHistory !== undefined) {
      obj.forecastHistory = Duration.toJSON(message.forecastHistory);
    }
    if (message.granularity !== undefined) {
      obj.granularity = Duration.toJSON(message.granularity);
    }
    if (message.metric !== undefined) {
      obj.metric = message.metric;
    }
    if (message.metricAggregationMethod !== 0) {
      obj.metricAggregationMethod = timeseriesParams_AggregationMethodToJSON(message.metricAggregationMethod);
    }
    return obj;
  },

  create(base?: DeepPartial<TimeseriesParams>): TimeseriesParams {
    return TimeseriesParams.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TimeseriesParams>): TimeseriesParams {
    const message = createBaseTimeseriesParams();
    message.forecastHistory = (object.forecastHistory !== undefined && object.forecastHistory !== null)
      ? Duration.fromPartial(object.forecastHistory)
      : undefined;
    message.granularity = (object.granularity !== undefined && object.granularity !== null)
      ? Duration.fromPartial(object.granularity)
      : undefined;
    message.metric = object.metric ?? undefined;
    message.metricAggregationMethod = object.metricAggregationMethod ?? 0;
    return message;
  },
};

function createBaseQueryDataSetRequest(): QueryDataSetRequest {
  return {
    name: "",
    detectionTime: undefined,
    numReturnedSlices: undefined,
    slicingParams: undefined,
    timeseriesParams: undefined,
    forecastParams: undefined,
    returnTimeseries: false,
  };
}

export const QueryDataSetRequest: MessageFns<QueryDataSetRequest> = {
  encode(message: QueryDataSetRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.detectionTime !== undefined) {
      Timestamp.encode(toTimestamp(message.detectionTime), writer.uint32(90).fork()).join();
    }
    if (message.numReturnedSlices !== undefined) {
      writer.uint32(104).int32(message.numReturnedSlices);
    }
    if (message.slicingParams !== undefined) {
      SlicingParams.encode(message.slicingParams, writer.uint32(74).fork()).join();
    }
    if (message.timeseriesParams !== undefined) {
      TimeseriesParams.encode(message.timeseriesParams, writer.uint32(82).fork()).join();
    }
    if (message.forecastParams !== undefined) {
      ForecastParams.encode(message.forecastParams, writer.uint32(42).fork()).join();
    }
    if (message.returnTimeseries !== false) {
      writer.uint32(64).bool(message.returnTimeseries);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryDataSetRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryDataSetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.detectionTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 13:
          if (tag !== 104) {
            break;
          }

          message.numReturnedSlices = reader.int32();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.slicingParams = SlicingParams.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.timeseriesParams = TimeseriesParams.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.forecastParams = ForecastParams.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.returnTimeseries = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryDataSetRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      detectionTime: isSet(object.detectionTime) ? fromJsonTimestamp(object.detectionTime) : undefined,
      numReturnedSlices: isSet(object.numReturnedSlices) ? globalThis.Number(object.numReturnedSlices) : undefined,
      slicingParams: isSet(object.slicingParams) ? SlicingParams.fromJSON(object.slicingParams) : undefined,
      timeseriesParams: isSet(object.timeseriesParams) ? TimeseriesParams.fromJSON(object.timeseriesParams) : undefined,
      forecastParams: isSet(object.forecastParams) ? ForecastParams.fromJSON(object.forecastParams) : undefined,
      returnTimeseries: isSet(object.returnTimeseries) ? globalThis.Boolean(object.returnTimeseries) : false,
    };
  },

  toJSON(message: QueryDataSetRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.detectionTime !== undefined) {
      obj.detectionTime = message.detectionTime.toISOString();
    }
    if (message.numReturnedSlices !== undefined) {
      obj.numReturnedSlices = Math.round(message.numReturnedSlices);
    }
    if (message.slicingParams !== undefined) {
      obj.slicingParams = SlicingParams.toJSON(message.slicingParams);
    }
    if (message.timeseriesParams !== undefined) {
      obj.timeseriesParams = TimeseriesParams.toJSON(message.timeseriesParams);
    }
    if (message.forecastParams !== undefined) {
      obj.forecastParams = ForecastParams.toJSON(message.forecastParams);
    }
    if (message.returnTimeseries !== false) {
      obj.returnTimeseries = message.returnTimeseries;
    }
    return obj;
  },

  create(base?: DeepPartial<QueryDataSetRequest>): QueryDataSetRequest {
    return QueryDataSetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<QueryDataSetRequest>): QueryDataSetRequest {
    const message = createBaseQueryDataSetRequest();
    message.name = object.name ?? "";
    message.detectionTime = object.detectionTime ?? undefined;
    message.numReturnedSlices = object.numReturnedSlices ?? undefined;
    message.slicingParams = (object.slicingParams !== undefined && object.slicingParams !== null)
      ? SlicingParams.fromPartial(object.slicingParams)
      : undefined;
    message.timeseriesParams = (object.timeseriesParams !== undefined && object.timeseriesParams !== null)
      ? TimeseriesParams.fromPartial(object.timeseriesParams)
      : undefined;
    message.forecastParams = (object.forecastParams !== undefined && object.forecastParams !== null)
      ? ForecastParams.fromPartial(object.forecastParams)
      : undefined;
    message.returnTimeseries = object.returnTimeseries ?? false;
    return message;
  },
};

function createBaseQueryDataSetResponse(): QueryDataSetResponse {
  return { name: "", slices: [] };
}

export const QueryDataSetResponse: MessageFns<QueryDataSetResponse> = {
  encode(message: QueryDataSetResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    for (const v of message.slices) {
      EvaluatedSlice.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryDataSetResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryDataSetResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.slices.push(EvaluatedSlice.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryDataSetResponse {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      slices: globalThis.Array.isArray(object?.slices) ? object.slices.map((e: any) => EvaluatedSlice.fromJSON(e)) : [],
    };
  },

  toJSON(message: QueryDataSetResponse): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.slices?.length) {
      obj.slices = message.slices.map((e) => EvaluatedSlice.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<QueryDataSetResponse>): QueryDataSetResponse {
    return QueryDataSetResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<QueryDataSetResponse>): QueryDataSetResponse {
    const message = createBaseQueryDataSetResponse();
    message.name = object.name ?? "";
    message.slices = object.slices?.map((e) => EvaluatedSlice.fromPartial(e)) || [];
    return message;
  },
};

function createBaseEvaluateSliceRequest(): EvaluateSliceRequest {
  return {
    dataset: "",
    pinnedDimensions: [],
    detectionTime: undefined,
    timeseriesParams: undefined,
    forecastParams: undefined,
  };
}

export const EvaluateSliceRequest: MessageFns<EvaluateSliceRequest> = {
  encode(message: EvaluateSliceRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dataset !== "") {
      writer.uint32(10).string(message.dataset);
    }
    for (const v of message.pinnedDimensions) {
      PinnedDimension.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.detectionTime !== undefined) {
      Timestamp.encode(toTimestamp(message.detectionTime), writer.uint32(26).fork()).join();
    }
    if (message.timeseriesParams !== undefined) {
      TimeseriesParams.encode(message.timeseriesParams, writer.uint32(34).fork()).join();
    }
    if (message.forecastParams !== undefined) {
      ForecastParams.encode(message.forecastParams, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EvaluateSliceRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEvaluateSliceRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dataset = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.pinnedDimensions.push(PinnedDimension.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.detectionTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.timeseriesParams = TimeseriesParams.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.forecastParams = ForecastParams.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EvaluateSliceRequest {
    return {
      dataset: isSet(object.dataset) ? globalThis.String(object.dataset) : "",
      pinnedDimensions: globalThis.Array.isArray(object?.pinnedDimensions)
        ? object.pinnedDimensions.map((e: any) => PinnedDimension.fromJSON(e))
        : [],
      detectionTime: isSet(object.detectionTime) ? fromJsonTimestamp(object.detectionTime) : undefined,
      timeseriesParams: isSet(object.timeseriesParams) ? TimeseriesParams.fromJSON(object.timeseriesParams) : undefined,
      forecastParams: isSet(object.forecastParams) ? ForecastParams.fromJSON(object.forecastParams) : undefined,
    };
  },

  toJSON(message: EvaluateSliceRequest): unknown {
    const obj: any = {};
    if (message.dataset !== "") {
      obj.dataset = message.dataset;
    }
    if (message.pinnedDimensions?.length) {
      obj.pinnedDimensions = message.pinnedDimensions.map((e) => PinnedDimension.toJSON(e));
    }
    if (message.detectionTime !== undefined) {
      obj.detectionTime = message.detectionTime.toISOString();
    }
    if (message.timeseriesParams !== undefined) {
      obj.timeseriesParams = TimeseriesParams.toJSON(message.timeseriesParams);
    }
    if (message.forecastParams !== undefined) {
      obj.forecastParams = ForecastParams.toJSON(message.forecastParams);
    }
    return obj;
  },

  create(base?: DeepPartial<EvaluateSliceRequest>): EvaluateSliceRequest {
    return EvaluateSliceRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EvaluateSliceRequest>): EvaluateSliceRequest {
    const message = createBaseEvaluateSliceRequest();
    message.dataset = object.dataset ?? "";
    message.pinnedDimensions = object.pinnedDimensions?.map((e) => PinnedDimension.fromPartial(e)) || [];
    message.detectionTime = object.detectionTime ?? undefined;
    message.timeseriesParams = (object.timeseriesParams !== undefined && object.timeseriesParams !== null)
      ? TimeseriesParams.fromPartial(object.timeseriesParams)
      : undefined;
    message.forecastParams = (object.forecastParams !== undefined && object.forecastParams !== null)
      ? ForecastParams.fromPartial(object.forecastParams)
      : undefined;
    return message;
  },
};

function createBaseEvaluateTimeseriesRequest(): EvaluateTimeseriesRequest {
  return { parent: "", timeseries: undefined, granularity: undefined, forecastParams: undefined };
}

export const EvaluateTimeseriesRequest: MessageFns<EvaluateTimeseriesRequest> = {
  encode(message: EvaluateTimeseriesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.timeseries !== undefined) {
      Timeseries.encode(message.timeseries, writer.uint32(18).fork()).join();
    }
    if (message.granularity !== undefined) {
      Duration.encode(message.granularity, writer.uint32(26).fork()).join();
    }
    if (message.forecastParams !== undefined) {
      ForecastParams.encode(message.forecastParams, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EvaluateTimeseriesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEvaluateTimeseriesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.timeseries = Timeseries.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.granularity = Duration.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.forecastParams = ForecastParams.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EvaluateTimeseriesRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      timeseries: isSet(object.timeseries) ? Timeseries.fromJSON(object.timeseries) : undefined,
      granularity: isSet(object.granularity) ? Duration.fromJSON(object.granularity) : undefined,
      forecastParams: isSet(object.forecastParams) ? ForecastParams.fromJSON(object.forecastParams) : undefined,
    };
  },

  toJSON(message: EvaluateTimeseriesRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.timeseries !== undefined) {
      obj.timeseries = Timeseries.toJSON(message.timeseries);
    }
    if (message.granularity !== undefined) {
      obj.granularity = Duration.toJSON(message.granularity);
    }
    if (message.forecastParams !== undefined) {
      obj.forecastParams = ForecastParams.toJSON(message.forecastParams);
    }
    return obj;
  },

  create(base?: DeepPartial<EvaluateTimeseriesRequest>): EvaluateTimeseriesRequest {
    return EvaluateTimeseriesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EvaluateTimeseriesRequest>): EvaluateTimeseriesRequest {
    const message = createBaseEvaluateTimeseriesRequest();
    message.parent = object.parent ?? "";
    message.timeseries = (object.timeseries !== undefined && object.timeseries !== null)
      ? Timeseries.fromPartial(object.timeseries)
      : undefined;
    message.granularity = (object.granularity !== undefined && object.granularity !== null)
      ? Duration.fromPartial(object.granularity)
      : undefined;
    message.forecastParams = (object.forecastParams !== undefined && object.forecastParams !== null)
      ? ForecastParams.fromPartial(object.forecastParams)
      : undefined;
    return message;
  },
};

export type TimeseriesInsightsControllerDefinition = typeof TimeseriesInsightsControllerDefinition;
export const TimeseriesInsightsControllerDefinition = {
  name: "TimeseriesInsightsController",
  fullName: "google.cloud.timeseriesinsights.v1.TimeseriesInsightsController",
  methods: {
    /**
     * Lists [DataSets][google.cloud.timeseriesinsights.v1.DataSet] under the project.
     *
     * The order of the results is unspecified but deterministic. Newly created
     * [DataSets][google.cloud.timeseriesinsights.v1.DataSet] will not necessarily be added to the end
     * of this list.
     */
    listDataSets: {
      name: "ListDataSets",
      requestType: ListDataSetsRequest,
      requestStream: false,
      responseType: ListDataSetsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([6, 112, 97, 114, 101, 110, 116])],
          578365826: [
            Buffer.from([
              82,
              90,
              34,
              18,
              32,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              125,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              18,
              44,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Create a [DataSet][google.cloud.timeseriesinsights.v1.DataSet] from data stored on Cloud
     * Storage.
     *
     * The data must stay immutable while we process the
     * [DataSet][google.cloud.timeseriesinsights.v1.DataSet] creation; otherwise, undefined outcomes
     * might result.  For more information, see [DataSet][google.cloud.timeseriesinsights.v1.DataSet].
     */
    createDataSet: {
      name: "CreateDataSet",
      requestType: CreateDataSetRequest,
      requestStream: false,
      responseType: DataSet,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([14, 112, 97, 114, 101, 110, 116, 44, 100, 97, 116, 97, 115, 101, 116])],
          578365826: [
            Buffer.from([
              100,
              58,
              7,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              90,
              43,
              58,
              7,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              34,
              32,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              125,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              34,
              44,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Delete a [DataSet][google.cloud.timeseriesinsights.v1.DataSet] from the system.
     *
     * **NOTE**: If the [DataSet][google.cloud.timeseriesinsights.v1.DataSet] is still being
     * processed, it will be aborted and deleted.
     */
    deleteDataSet: {
      name: "DeleteDataSet",
      requestType: DeleteDataSetRequest,
      requestStream: false,
      responseType: Empty,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              82,
              90,
              34,
              42,
              32,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              125,
              42,
              44,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /** Append events to a `LOADED` [DataSet][google.cloud.timeseriesinsights.v1.DataSet]. */
    appendEvents: {
      name: "AppendEvents",
      requestType: AppendEventsRequest,
      requestStream: false,
      responseType: AppendEventsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([14, 100, 97, 116, 97, 115, 101, 116, 44, 101, 118, 101, 110, 116, 115])],
          578365826: [
            Buffer.from([
              120,
              58,
              1,
              42,
              90,
              53,
              58,
              1,
              42,
              34,
              48,
              47,
              118,
              49,
              47,
              123,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              125,
              58,
              97,
              112,
              112,
              101,
              110,
              100,
              69,
              118,
              101,
              110,
              116,
              115,
              34,
              60,
              47,
              118,
              49,
              47,
              123,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              125,
              58,
              97,
              112,
              112,
              101,
              110,
              100,
              69,
              118,
              101,
              110,
              116,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Execute a Timeseries Insights query over a loaded
     * [DataSet][google.cloud.timeseriesinsights.v1.DataSet].
     */
    queryDataSet: {
      name: "QueryDataSet",
      requestType: QueryDataSetRequest,
      requestStream: false,
      responseType: QueryDataSetResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              100,
              58,
              1,
              42,
              90,
              43,
              58,
              1,
              42,
              34,
              38,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              125,
              58,
              113,
              117,
              101,
              114,
              121,
              34,
              50,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              125,
              58,
              113,
              117,
              101,
              114,
              121,
            ]),
          ],
        },
      },
    },
    /** Evaluate an explicit slice from a loaded [DataSet][google.cloud.timeseriesinsights.v1.DataSet]. */
    evaluateSlice: {
      name: "EvaluateSlice",
      requestType: EvaluateSliceRequest,
      requestStream: false,
      responseType: EvaluatedSlice,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              122,
              58,
              1,
              42,
              90,
              54,
              58,
              1,
              42,
              34,
              49,
              47,
              118,
              49,
              47,
              123,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              125,
              58,
              101,
              118,
              97,
              108,
              117,
              97,
              116,
              101,
              83,
              108,
              105,
              99,
              101,
              34,
              61,
              47,
              118,
              49,
              47,
              123,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              125,
              58,
              101,
              118,
              97,
              108,
              117,
              97,
              116,
              101,
              83,
              108,
              105,
              99,
              101,
            ]),
          ],
        },
      },
    },
    /** Evaluate an explicit timeseries. */
    evaluateTimeseries: {
      name: "EvaluateTimeseries",
      requestType: EvaluateTimeseriesRequest,
      requestStream: false,
      responseType: EvaluatedSlice,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              126,
              58,
              1,
              42,
              90,
              56,
              58,
              1,
              42,
              34,
              51,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              125,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              58,
              101,
              118,
              97,
              108,
              117,
              97,
              116,
              101,
              84,
              105,
              109,
              101,
              115,
              101,
              114,
              105,
              101,
              115,
              34,
              63,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              58,
              101,
              118,
              97,
              108,
              117,
              97,
              116,
              101,
              84,
              105,
              109,
              101,
              115,
              101,
              114,
              105,
              101,
              115,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface TimeseriesInsightsControllerServiceImplementation<CallContextExt = {}> {
  /**
   * Lists [DataSets][google.cloud.timeseriesinsights.v1.DataSet] under the project.
   *
   * The order of the results is unspecified but deterministic. Newly created
   * [DataSets][google.cloud.timeseriesinsights.v1.DataSet] will not necessarily be added to the end
   * of this list.
   */
  listDataSets(
    request: ListDataSetsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ListDataSetsResponse>>;
  /**
   * Create a [DataSet][google.cloud.timeseriesinsights.v1.DataSet] from data stored on Cloud
   * Storage.
   *
   * The data must stay immutable while we process the
   * [DataSet][google.cloud.timeseriesinsights.v1.DataSet] creation; otherwise, undefined outcomes
   * might result.  For more information, see [DataSet][google.cloud.timeseriesinsights.v1.DataSet].
   */
  createDataSet(request: CreateDataSetRequest, context: CallContext & CallContextExt): Promise<DeepPartial<DataSet>>;
  /**
   * Delete a [DataSet][google.cloud.timeseriesinsights.v1.DataSet] from the system.
   *
   * **NOTE**: If the [DataSet][google.cloud.timeseriesinsights.v1.DataSet] is still being
   * processed, it will be aborted and deleted.
   */
  deleteDataSet(request: DeleteDataSetRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Empty>>;
  /** Append events to a `LOADED` [DataSet][google.cloud.timeseriesinsights.v1.DataSet]. */
  appendEvents(
    request: AppendEventsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<AppendEventsResponse>>;
  /**
   * Execute a Timeseries Insights query over a loaded
   * [DataSet][google.cloud.timeseriesinsights.v1.DataSet].
   */
  queryDataSet(
    request: QueryDataSetRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<QueryDataSetResponse>>;
  /** Evaluate an explicit slice from a loaded [DataSet][google.cloud.timeseriesinsights.v1.DataSet]. */
  evaluateSlice(
    request: EvaluateSliceRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<EvaluatedSlice>>;
  /** Evaluate an explicit timeseries. */
  evaluateTimeseries(
    request: EvaluateTimeseriesRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<EvaluatedSlice>>;
}

export interface TimeseriesInsightsControllerClient<CallOptionsExt = {}> {
  /**
   * Lists [DataSets][google.cloud.timeseriesinsights.v1.DataSet] under the project.
   *
   * The order of the results is unspecified but deterministic. Newly created
   * [DataSets][google.cloud.timeseriesinsights.v1.DataSet] will not necessarily be added to the end
   * of this list.
   */
  listDataSets(
    request: DeepPartial<ListDataSetsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ListDataSetsResponse>;
  /**
   * Create a [DataSet][google.cloud.timeseriesinsights.v1.DataSet] from data stored on Cloud
   * Storage.
   *
   * The data must stay immutable while we process the
   * [DataSet][google.cloud.timeseriesinsights.v1.DataSet] creation; otherwise, undefined outcomes
   * might result.  For more information, see [DataSet][google.cloud.timeseriesinsights.v1.DataSet].
   */
  createDataSet(request: DeepPartial<CreateDataSetRequest>, options?: CallOptions & CallOptionsExt): Promise<DataSet>;
  /**
   * Delete a [DataSet][google.cloud.timeseriesinsights.v1.DataSet] from the system.
   *
   * **NOTE**: If the [DataSet][google.cloud.timeseriesinsights.v1.DataSet] is still being
   * processed, it will be aborted and deleted.
   */
  deleteDataSet(request: DeepPartial<DeleteDataSetRequest>, options?: CallOptions & CallOptionsExt): Promise<Empty>;
  /** Append events to a `LOADED` [DataSet][google.cloud.timeseriesinsights.v1.DataSet]. */
  appendEvents(
    request: DeepPartial<AppendEventsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<AppendEventsResponse>;
  /**
   * Execute a Timeseries Insights query over a loaded
   * [DataSet][google.cloud.timeseriesinsights.v1.DataSet].
   */
  queryDataSet(
    request: DeepPartial<QueryDataSetRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<QueryDataSetResponse>;
  /** Evaluate an explicit slice from a loaded [DataSet][google.cloud.timeseriesinsights.v1.DataSet]. */
  evaluateSlice(
    request: DeepPartial<EvaluateSliceRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<EvaluatedSlice>;
  /** Evaluate an explicit timeseries. */
  evaluateTimeseries(
    request: DeepPartial<EvaluateTimeseriesRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<EvaluatedSlice>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
