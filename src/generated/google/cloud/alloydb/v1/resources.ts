// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/alloydb/v1/resources.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Duration } from "../../../protobuf/duration.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { Int64Value } from "../../../protobuf/wrappers.js";
import { DayOfWeek, dayOfWeekFromJSON, dayOfWeekToJSON } from "../../../type/dayofweek.js";
import { TimeOfDay } from "../../../type/timeofday.js";

export const protobufPackage = "google.cloud.alloydb.v1";

/**
 * View on Instance. Pass this enum to rpcs that returns an Instance message to
 * control which subsets of fields to get.
 */
export enum InstanceView {
  /** INSTANCE_VIEW_UNSPECIFIED - INSTANCE_VIEW_UNSPECIFIED Not specified, equivalent to BASIC. */
  INSTANCE_VIEW_UNSPECIFIED = 0,
  /**
   * INSTANCE_VIEW_BASIC - BASIC server responses for a primary or read instance include all the
   * relevant instance details, excluding the details of each node in the
   * instance. The default value.
   */
  INSTANCE_VIEW_BASIC = 1,
  /**
   * INSTANCE_VIEW_FULL - FULL response is equivalent to BASIC for primary instance (for now).
   * For read pool instance, this includes details of each node in the pool.
   */
  INSTANCE_VIEW_FULL = 2,
  UNRECOGNIZED = -1,
}

export function instanceViewFromJSON(object: any): InstanceView {
  switch (object) {
    case 0:
    case "INSTANCE_VIEW_UNSPECIFIED":
      return InstanceView.INSTANCE_VIEW_UNSPECIFIED;
    case 1:
    case "INSTANCE_VIEW_BASIC":
      return InstanceView.INSTANCE_VIEW_BASIC;
    case 2:
    case "INSTANCE_VIEW_FULL":
      return InstanceView.INSTANCE_VIEW_FULL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return InstanceView.UNRECOGNIZED;
  }
}

export function instanceViewToJSON(object: InstanceView): string {
  switch (object) {
    case InstanceView.INSTANCE_VIEW_UNSPECIFIED:
      return "INSTANCE_VIEW_UNSPECIFIED";
    case InstanceView.INSTANCE_VIEW_BASIC:
      return "INSTANCE_VIEW_BASIC";
    case InstanceView.INSTANCE_VIEW_FULL:
      return "INSTANCE_VIEW_FULL";
    case InstanceView.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * View on Cluster. Pass this enum to rpcs that returns a cluster message to
 * control which subsets of fields to get.
 */
export enum ClusterView {
  /** CLUSTER_VIEW_UNSPECIFIED - CLUSTER_VIEW_UNSPECIFIED Not specified, equivalent to BASIC. */
  CLUSTER_VIEW_UNSPECIFIED = 0,
  /**
   * CLUSTER_VIEW_BASIC - BASIC server responses include all the relevant cluster details, excluding
   * Cluster.ContinuousBackupInfo.EarliestRestorableTime and other view-specific
   * fields. The default value.
   */
  CLUSTER_VIEW_BASIC = 1,
  /**
   * CLUSTER_VIEW_CONTINUOUS_BACKUP - CONTINUOUS_BACKUP response returns all the fields from BASIC plus
   * the earliest restorable time if continuous backups are enabled.
   * May increase latency.
   */
  CLUSTER_VIEW_CONTINUOUS_BACKUP = 2,
  UNRECOGNIZED = -1,
}

export function clusterViewFromJSON(object: any): ClusterView {
  switch (object) {
    case 0:
    case "CLUSTER_VIEW_UNSPECIFIED":
      return ClusterView.CLUSTER_VIEW_UNSPECIFIED;
    case 1:
    case "CLUSTER_VIEW_BASIC":
      return ClusterView.CLUSTER_VIEW_BASIC;
    case 2:
    case "CLUSTER_VIEW_CONTINUOUS_BACKUP":
      return ClusterView.CLUSTER_VIEW_CONTINUOUS_BACKUP;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ClusterView.UNRECOGNIZED;
  }
}

export function clusterViewToJSON(object: ClusterView): string {
  switch (object) {
    case ClusterView.CLUSTER_VIEW_UNSPECIFIED:
      return "CLUSTER_VIEW_UNSPECIFIED";
    case ClusterView.CLUSTER_VIEW_BASIC:
      return "CLUSTER_VIEW_BASIC";
    case ClusterView.CLUSTER_VIEW_CONTINUOUS_BACKUP:
      return "CLUSTER_VIEW_CONTINUOUS_BACKUP";
    case ClusterView.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The supported database engine versions. */
export enum DatabaseVersion {
  /** DATABASE_VERSION_UNSPECIFIED - This is an unknown database version. */
  DATABASE_VERSION_UNSPECIFIED = 0,
  /**
   * POSTGRES_13 - DEPRECATED - The database version is Postgres 13.
   *
   * @deprecated
   */
  POSTGRES_13 = 1,
  /** POSTGRES_14 - The database version is Postgres 14. */
  POSTGRES_14 = 2,
  UNRECOGNIZED = -1,
}

export function databaseVersionFromJSON(object: any): DatabaseVersion {
  switch (object) {
    case 0:
    case "DATABASE_VERSION_UNSPECIFIED":
      return DatabaseVersion.DATABASE_VERSION_UNSPECIFIED;
    case 1:
    case "POSTGRES_13":
      return DatabaseVersion.POSTGRES_13;
    case 2:
    case "POSTGRES_14":
      return DatabaseVersion.POSTGRES_14;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DatabaseVersion.UNRECOGNIZED;
  }
}

export function databaseVersionToJSON(object: DatabaseVersion): string {
  switch (object) {
    case DatabaseVersion.DATABASE_VERSION_UNSPECIFIED:
      return "DATABASE_VERSION_UNSPECIFIED";
    case DatabaseVersion.POSTGRES_13:
      return "POSTGRES_13";
    case DatabaseVersion.POSTGRES_14:
      return "POSTGRES_14";
    case DatabaseVersion.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * The username/password for a database user. Used for specifying initial
 * users at cluster creation time.
 */
export interface UserPassword {
  /** The database username. */
  user: string;
  /** The initial password for the user. */
  password: string;
}

/**
 * Subset of the source instance configuration that is available when reading
 * the cluster resource.
 */
export interface MigrationSource {
  /**
   * Output only. The host and port of the on-premises instance in host:port
   * format
   */
  hostPort: string;
  /**
   * Output only. Place holder for the external source identifier(e.g DMS job
   * name) that created the cluster.
   */
  referenceId: string;
  /** Output only. Type of migration source. */
  sourceType: MigrationSource_MigrationSourceType;
}

/** Denote the type of migration source that created this cluster. */
export enum MigrationSource_MigrationSourceType {
  /** MIGRATION_SOURCE_TYPE_UNSPECIFIED - Migration source is unknown. */
  MIGRATION_SOURCE_TYPE_UNSPECIFIED = 0,
  /** DMS - DMS source means the cluster was created via DMS migration job. */
  DMS = 1,
  UNRECOGNIZED = -1,
}

export function migrationSource_MigrationSourceTypeFromJSON(object: any): MigrationSource_MigrationSourceType {
  switch (object) {
    case 0:
    case "MIGRATION_SOURCE_TYPE_UNSPECIFIED":
      return MigrationSource_MigrationSourceType.MIGRATION_SOURCE_TYPE_UNSPECIFIED;
    case 1:
    case "DMS":
      return MigrationSource_MigrationSourceType.DMS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return MigrationSource_MigrationSourceType.UNRECOGNIZED;
  }
}

export function migrationSource_MigrationSourceTypeToJSON(object: MigrationSource_MigrationSourceType): string {
  switch (object) {
    case MigrationSource_MigrationSourceType.MIGRATION_SOURCE_TYPE_UNSPECIFIED:
      return "MIGRATION_SOURCE_TYPE_UNSPECIFIED";
    case MigrationSource_MigrationSourceType.DMS:
      return "DMS";
    case MigrationSource_MigrationSourceType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * EncryptionConfig describes the encryption config of a cluster or a backup
 * that is encrypted with a CMEK (customer-managed encryption key).
 */
export interface EncryptionConfig {
  /**
   * The fully-qualified resource name of the KMS key.
   * Each Cloud KMS key is regionalized and has the following format:
   * projects/[PROJECT]/locations/[REGION]/keyRings/[RING]/cryptoKeys/[KEY_NAME]
   */
  kmsKeyName: string;
}

/** EncryptionInfo describes the encryption information of a cluster or a backup. */
export interface EncryptionInfo {
  /** Output only. Type of encryption. */
  encryptionType: EncryptionInfo_Type;
  /**
   * Output only. Cloud KMS key versions that are being used to protect the
   * database or the backup.
   */
  kmsKeyVersions: string[];
}

/** Possible encryption types. */
export enum EncryptionInfo_Type {
  /** TYPE_UNSPECIFIED - Encryption type not specified. Defaults to GOOGLE_DEFAULT_ENCRYPTION. */
  TYPE_UNSPECIFIED = 0,
  /**
   * GOOGLE_DEFAULT_ENCRYPTION - The data is encrypted at rest with a key that is fully managed by Google.
   * No key version will be populated. This is the default state.
   */
  GOOGLE_DEFAULT_ENCRYPTION = 1,
  /**
   * CUSTOMER_MANAGED_ENCRYPTION - The data is encrypted at rest with a key that is managed by the customer.
   * KMS key versions will be populated.
   */
  CUSTOMER_MANAGED_ENCRYPTION = 2,
  UNRECOGNIZED = -1,
}

export function encryptionInfo_TypeFromJSON(object: any): EncryptionInfo_Type {
  switch (object) {
    case 0:
    case "TYPE_UNSPECIFIED":
      return EncryptionInfo_Type.TYPE_UNSPECIFIED;
    case 1:
    case "GOOGLE_DEFAULT_ENCRYPTION":
      return EncryptionInfo_Type.GOOGLE_DEFAULT_ENCRYPTION;
    case 2:
    case "CUSTOMER_MANAGED_ENCRYPTION":
      return EncryptionInfo_Type.CUSTOMER_MANAGED_ENCRYPTION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return EncryptionInfo_Type.UNRECOGNIZED;
  }
}

export function encryptionInfo_TypeToJSON(object: EncryptionInfo_Type): string {
  switch (object) {
    case EncryptionInfo_Type.TYPE_UNSPECIFIED:
      return "TYPE_UNSPECIFIED";
    case EncryptionInfo_Type.GOOGLE_DEFAULT_ENCRYPTION:
      return "GOOGLE_DEFAULT_ENCRYPTION";
    case EncryptionInfo_Type.CUSTOMER_MANAGED_ENCRYPTION:
      return "CUSTOMER_MANAGED_ENCRYPTION";
    case EncryptionInfo_Type.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** SSL configuration. */
export interface SslConfig {
  /** Optional. SSL mode. Specifies client-server SSL/TLS connection behavior. */
  sslMode: SslConfig_SslMode;
  /**
   * Optional. Certificate Authority (CA) source. Only CA_SOURCE_MANAGED is
   * supported currently, and is the default value.
   */
  caSource: SslConfig_CaSource;
}

/** SSL mode options. */
export enum SslConfig_SslMode {
  /** SSL_MODE_UNSPECIFIED - SSL mode not specified. Defaults to ENCRYPTED_ONLY. */
  SSL_MODE_UNSPECIFIED = 0,
  /**
   * SSL_MODE_ALLOW - SSL connections are optional. CA verification not enforced.
   *
   * @deprecated
   */
  SSL_MODE_ALLOW = 1,
  /**
   * SSL_MODE_REQUIRE - SSL connections are required. CA verification not enforced.
   * Clients may use locally self-signed certificates (default psql client
   * behavior).
   *
   * @deprecated
   */
  SSL_MODE_REQUIRE = 2,
  /**
   * SSL_MODE_VERIFY_CA - SSL connections are required. CA verification enforced.
   * Clients must have certificates signed by a Cluster CA, e.g. via
   * GenerateClientCertificate.
   *
   * @deprecated
   */
  SSL_MODE_VERIFY_CA = 3,
  /** ALLOW_UNENCRYPTED_AND_ENCRYPTED - SSL connections are optional. CA verification not enforced. */
  ALLOW_UNENCRYPTED_AND_ENCRYPTED = 4,
  /** ENCRYPTED_ONLY - SSL connections are required. CA verification not enforced. */
  ENCRYPTED_ONLY = 5,
  UNRECOGNIZED = -1,
}

export function sslConfig_SslModeFromJSON(object: any): SslConfig_SslMode {
  switch (object) {
    case 0:
    case "SSL_MODE_UNSPECIFIED":
      return SslConfig_SslMode.SSL_MODE_UNSPECIFIED;
    case 1:
    case "SSL_MODE_ALLOW":
      return SslConfig_SslMode.SSL_MODE_ALLOW;
    case 2:
    case "SSL_MODE_REQUIRE":
      return SslConfig_SslMode.SSL_MODE_REQUIRE;
    case 3:
    case "SSL_MODE_VERIFY_CA":
      return SslConfig_SslMode.SSL_MODE_VERIFY_CA;
    case 4:
    case "ALLOW_UNENCRYPTED_AND_ENCRYPTED":
      return SslConfig_SslMode.ALLOW_UNENCRYPTED_AND_ENCRYPTED;
    case 5:
    case "ENCRYPTED_ONLY":
      return SslConfig_SslMode.ENCRYPTED_ONLY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SslConfig_SslMode.UNRECOGNIZED;
  }
}

export function sslConfig_SslModeToJSON(object: SslConfig_SslMode): string {
  switch (object) {
    case SslConfig_SslMode.SSL_MODE_UNSPECIFIED:
      return "SSL_MODE_UNSPECIFIED";
    case SslConfig_SslMode.SSL_MODE_ALLOW:
      return "SSL_MODE_ALLOW";
    case SslConfig_SslMode.SSL_MODE_REQUIRE:
      return "SSL_MODE_REQUIRE";
    case SslConfig_SslMode.SSL_MODE_VERIFY_CA:
      return "SSL_MODE_VERIFY_CA";
    case SslConfig_SslMode.ALLOW_UNENCRYPTED_AND_ENCRYPTED:
      return "ALLOW_UNENCRYPTED_AND_ENCRYPTED";
    case SslConfig_SslMode.ENCRYPTED_ONLY:
      return "ENCRYPTED_ONLY";
    case SslConfig_SslMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Certificate Authority (CA) source for SSL/TLS certificates. */
export enum SslConfig_CaSource {
  /**
   * CA_SOURCE_UNSPECIFIED - Certificate Authority (CA) source not specified. Defaults to
   * CA_SOURCE_MANAGED.
   */
  CA_SOURCE_UNSPECIFIED = 0,
  /** CA_SOURCE_MANAGED - Certificate Authority (CA) managed by the AlloyDB Cluster. */
  CA_SOURCE_MANAGED = 1,
  UNRECOGNIZED = -1,
}

export function sslConfig_CaSourceFromJSON(object: any): SslConfig_CaSource {
  switch (object) {
    case 0:
    case "CA_SOURCE_UNSPECIFIED":
      return SslConfig_CaSource.CA_SOURCE_UNSPECIFIED;
    case 1:
    case "CA_SOURCE_MANAGED":
      return SslConfig_CaSource.CA_SOURCE_MANAGED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SslConfig_CaSource.UNRECOGNIZED;
  }
}

export function sslConfig_CaSourceToJSON(object: SslConfig_CaSource): string {
  switch (object) {
    case SslConfig_CaSource.CA_SOURCE_UNSPECIFIED:
      return "CA_SOURCE_UNSPECIFIED";
    case SslConfig_CaSource.CA_SOURCE_MANAGED:
      return "CA_SOURCE_MANAGED";
    case SslConfig_CaSource.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Message describing the user-specified automated backup policy.
 *
 * All fields in the automated backup policy are optional. Defaults for each
 * field are provided if they are not set.
 */
export interface AutomatedBackupPolicy {
  /** Weekly schedule for the Backup. */
  weeklySchedule?:
    | AutomatedBackupPolicy_WeeklySchedule
    | undefined;
  /** Time-based Backup retention policy. */
  timeBasedRetention?:
    | AutomatedBackupPolicy_TimeBasedRetention
    | undefined;
  /** Quantity-based Backup retention policy to retain recent backups. */
  quantityBasedRetention?:
    | AutomatedBackupPolicy_QuantityBasedRetention
    | undefined;
  /**
   * Whether automated automated backups are enabled. If not set, defaults to
   * true.
   */
  enabled?:
    | boolean
    | undefined;
  /**
   * The length of the time window during which a backup can be
   * taken. If a backup does not succeed within this time window, it will be
   * canceled and considered failed.
   *
   * The backup window must be at least 5 minutes long. There is no upper bound
   * on the window. If not set, it defaults to 1 hour.
   */
  backupWindow:
    | Duration
    | undefined;
  /**
   * Optional. The encryption config can be specified to encrypt the
   * backups with a customer-managed encryption key (CMEK). When this field is
   * not specified, the backup will then use default encryption scheme to
   * protect the user data.
   */
  encryptionConfig:
    | EncryptionConfig
    | undefined;
  /**
   * The location where the backup will be stored. Currently, the only supported
   * option is to store the backup in the same region as the cluster.
   *
   * If empty, defaults to the region of the cluster.
   */
  location: string;
  /** Labels to apply to backups created using this configuration. */
  labels: { [key: string]: string };
}

/**
 * A weekly schedule starts a backup at prescribed start times within a
 * day, for the specified days of the week.
 *
 * The weekly schedule message is flexible and can be used to create many
 * types of schedules. For example, to have a daily backup that starts at
 * 22:00, configure the `start_times` field to have one element "22:00" and
 * the `days_of_week` field to have all seven days of the week.
 */
export interface AutomatedBackupPolicy_WeeklySchedule {
  /**
   * The times during the day to start a backup. The start times are assumed
   * to be in UTC and to be an exact hour (e.g., 04:00:00).
   *
   * If no start times are provided, a single fixed start time is chosen
   * arbitrarily.
   */
  startTimes: TimeOfDay[];
  /**
   * The days of the week to perform a backup.
   *
   * If this field is left empty, the default of every day of the week is
   * used.
   */
  daysOfWeek: DayOfWeek[];
}

/**
 * A time based retention policy specifies that all backups within a certain
 * time period should be retained.
 */
export interface AutomatedBackupPolicy_TimeBasedRetention {
  /** The retention period. */
  retentionPeriod: Duration | undefined;
}

/**
 * A quantity based policy specifies that a certain number of the most recent
 * successful backups should be retained.
 */
export interface AutomatedBackupPolicy_QuantityBasedRetention {
  /** The number of backups to retain. */
  count: number;
}

export interface AutomatedBackupPolicy_LabelsEntry {
  key: string;
  value: string;
}

/**
 * ContinuousBackupConfig describes the continuous backups recovery
 * configurations of a cluster.
 */
export interface ContinuousBackupConfig {
  /** Whether ContinuousBackup is enabled. */
  enabled?:
    | boolean
    | undefined;
  /**
   * The number of days that are eligible to restore from using PITR. To support
   * the entire recovery window, backups and logs are retained for one day more
   * than the recovery window. If not set, defaults to 14 days.
   */
  recoveryWindowDays: number;
  /**
   * The encryption config can be specified to encrypt the
   * backups with a customer-managed encryption key (CMEK). When this field is
   * not specified, the backup will then use default encryption scheme to
   * protect the user data.
   */
  encryptionConfig: EncryptionConfig | undefined;
}

/**
 * ContinuousBackupInfo describes the continuous backup properties of a
 * cluster.
 */
export interface ContinuousBackupInfo {
  /**
   * Output only. The encryption information for the WALs and backups required
   * for ContinuousBackup.
   */
  encryptionInfo:
    | EncryptionInfo
    | undefined;
  /**
   * Output only. When ContinuousBackup was most recently enabled. Set to null
   * if ContinuousBackup is not enabled.
   */
  enabledTime:
    | Date
    | undefined;
  /**
   * Output only. Days of the week on which a continuous backup is taken. Output
   * only field. Ignored if passed into the request.
   */
  schedule: DayOfWeek[];
  /**
   * Output only. The earliest restorable time that can be restored to. Output
   * only field.
   */
  earliestRestorableTime: Date | undefined;
}

/** Message describing a BackupSource. */
export interface BackupSource {
  /**
   * Output only. The system-generated UID of the backup which was used to
   * create this resource. The UID is generated when the backup is created, and
   * it is retained until the backup is deleted.
   */
  backupUid: string;
  /**
   * Required. The name of the backup resource with the format:
   *  * projects/{project}/locations/{region}/backups/{backup_id}
   */
  backupName: string;
}

/** Message describing a ContinuousBackupSource. */
export interface ContinuousBackupSource {
  /**
   * Required. The source cluster from which to restore. This cluster must have
   * continuous backup enabled for this operation to succeed. For the required
   * format, see the comment on the Cluster.name field.
   */
  cluster: string;
  /** Required. The point in time to restore to. */
  pointInTime: Date | undefined;
}

/**
 * A cluster is a collection of regional AlloyDB resources. It can include a
 * primary instance and one or more read pool instances.
 * All cluster resources share a storage layer, which scales as needed.
 */
export interface Cluster {
  /** Output only. Cluster created from backup. */
  backupSource?:
    | BackupSource
    | undefined;
  /** Output only. Cluster created via DMS migration. */
  migrationSource?:
    | MigrationSource
    | undefined;
  /**
   * Output only. The name of the cluster resource with the format:
   *  * projects/{project}/locations/{region}/clusters/{cluster_id}
   * where the cluster ID segment should satisfy the regex expression
   * `[a-z0-9-]+`. For more details see https://google.aip.dev/122.
   * The prefix of the cluster resource name is the name of the parent resource:
   *  * projects/{project}/locations/{region}
   */
  name: string;
  /** User-settable and human-readable display name for the Cluster. */
  displayName: string;
  /**
   * Output only. The system-generated UID of the resource. The UID is assigned
   * when the resource is created, and it is retained until it is deleted.
   */
  uid: string;
  /** Output only. Create time stamp */
  createTime:
    | Date
    | undefined;
  /** Output only. Update time stamp */
  updateTime:
    | Date
    | undefined;
  /** Output only. Delete time stamp */
  deleteTime:
    | Date
    | undefined;
  /** Labels as key value pairs */
  labels: { [key: string]: string };
  /** Output only. The current serving state of the cluster. */
  state: Cluster_State;
  /**
   * Output only. The type of the cluster. This is an output-only field and it's
   * populated at the Cluster creation time or the Cluster promotion
   * time. The cluster type is determined by which RPC was used to create
   * the cluster (i.e. `CreateCluster` vs. `CreateSecondaryCluster`
   */
  clusterType: Cluster_ClusterType;
  /**
   * Optional. The database engine major version. This is an optional field and
   * it is populated at the Cluster creation time. If a database version is not
   * supplied at cluster creation time, then a default database version will
   * be used.
   */
  databaseVersion: DatabaseVersion;
  networkConfig:
    | Cluster_NetworkConfig
    | undefined;
  /**
   * Required. The resource link for the VPC network in which cluster resources
   * are created and from which they are accessible via Private IP. The network
   * must belong to the same project as the cluster. It is specified in the
   * form: "projects/{project}/global/networks/{network_id}". This is required
   * to create a cluster. Deprecated, use network_config.network instead.
   *
   * @deprecated
   */
  network: string;
  /** For Resource freshness validation (https://google.aip.dev/154) */
  etag: string;
  /**
   * Annotations to allow client tools to store small amount of arbitrary data.
   * This is distinct from labels.
   * https://google.aip.dev/128
   */
  annotations: { [key: string]: string };
  /**
   * Output only. Reconciling (https://google.aip.dev/128#reconciliation).
   * Set to true if the current state of Cluster does not match the user's
   * intended state, and the service is actively updating the resource to
   * reconcile them. This can happen due to user-triggered updates or
   * system actions like failover or maintenance.
   */
  reconciling: boolean;
  /**
   * Input only. Initial user to setup during cluster creation. Required.
   * If used in `RestoreCluster` this is ignored.
   */
  initialUser:
    | UserPassword
    | undefined;
  /**
   * The automated backup policy for this cluster.
   *
   * If no policy is provided then the default policy will be used. If backups
   * are supported for the cluster, the default policy takes one backup a day,
   * has a backup window of 1 hour, and retains backups for 14 days.
   * For more information on the defaults, consult the
   * documentation for the message type.
   */
  automatedBackupPolicy:
    | AutomatedBackupPolicy
    | undefined;
  /**
   * SSL configuration for this AlloyDB cluster.
   *
   * @deprecated
   */
  sslConfig:
    | SslConfig
    | undefined;
  /**
   * Optional. The encryption config can be specified to encrypt the data disks
   * and other persistent data resources of a cluster with a
   * customer-managed encryption key (CMEK). When this field is not
   * specified, the cluster will then use default encryption scheme to
   * protect the user data.
   */
  encryptionConfig:
    | EncryptionConfig
    | undefined;
  /** Output only. The encryption information for the cluster. */
  encryptionInfo:
    | EncryptionInfo
    | undefined;
  /** Optional. Continuous backup configuration for this cluster. */
  continuousBackupConfig:
    | ContinuousBackupConfig
    | undefined;
  /** Output only. Continuous backup properties for this cluster. */
  continuousBackupInfo:
    | ContinuousBackupInfo
    | undefined;
  /** Cross Region replication config specific to SECONDARY cluster. */
  secondaryConfig:
    | Cluster_SecondaryConfig
    | undefined;
  /** Output only. Cross Region replication config specific to PRIMARY cluster. */
  primaryConfig: Cluster_PrimaryConfig | undefined;
}

/** Cluster State */
export enum Cluster_State {
  /** STATE_UNSPECIFIED - The state of the cluster is unknown. */
  STATE_UNSPECIFIED = 0,
  /** READY - The cluster is active and running. */
  READY = 1,
  /**
   * STOPPED - The cluster is stopped. All instances in the cluster are stopped.
   * Customers can start a stopped cluster at any point and all their
   * instances will come back to life with same names and IP resources. In
   * this state, customer pays for storage.
   * Associated backups could also be present in a stopped cluster.
   */
  STOPPED = 2,
  /**
   * EMPTY - The cluster is empty and has no associated resources.
   * All instances, associated storage and backups have been deleted.
   */
  EMPTY = 3,
  /** CREATING - The cluster is being created. */
  CREATING = 4,
  /** DELETING - The cluster is being deleted. */
  DELETING = 5,
  /** FAILED - The creation of the cluster failed. */
  FAILED = 6,
  /**
   * BOOTSTRAPPING - The cluster is bootstrapping with data from some other source.
   * Direct mutations to the cluster (e.g. adding read pool) are not allowed.
   */
  BOOTSTRAPPING = 7,
  /**
   * MAINTENANCE - The cluster is under maintenance. AlloyDB regularly performs maintenance
   * and upgrades on customer clusters. Updates on the cluster are
   * not allowed while the cluster is in this state.
   */
  MAINTENANCE = 8,
  /** PROMOTING - The cluster is being promoted. */
  PROMOTING = 9,
  UNRECOGNIZED = -1,
}

export function cluster_StateFromJSON(object: any): Cluster_State {
  switch (object) {
    case 0:
    case "STATE_UNSPECIFIED":
      return Cluster_State.STATE_UNSPECIFIED;
    case 1:
    case "READY":
      return Cluster_State.READY;
    case 2:
    case "STOPPED":
      return Cluster_State.STOPPED;
    case 3:
    case "EMPTY":
      return Cluster_State.EMPTY;
    case 4:
    case "CREATING":
      return Cluster_State.CREATING;
    case 5:
    case "DELETING":
      return Cluster_State.DELETING;
    case 6:
    case "FAILED":
      return Cluster_State.FAILED;
    case 7:
    case "BOOTSTRAPPING":
      return Cluster_State.BOOTSTRAPPING;
    case 8:
    case "MAINTENANCE":
      return Cluster_State.MAINTENANCE;
    case 9:
    case "PROMOTING":
      return Cluster_State.PROMOTING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Cluster_State.UNRECOGNIZED;
  }
}

export function cluster_StateToJSON(object: Cluster_State): string {
  switch (object) {
    case Cluster_State.STATE_UNSPECIFIED:
      return "STATE_UNSPECIFIED";
    case Cluster_State.READY:
      return "READY";
    case Cluster_State.STOPPED:
      return "STOPPED";
    case Cluster_State.EMPTY:
      return "EMPTY";
    case Cluster_State.CREATING:
      return "CREATING";
    case Cluster_State.DELETING:
      return "DELETING";
    case Cluster_State.FAILED:
      return "FAILED";
    case Cluster_State.BOOTSTRAPPING:
      return "BOOTSTRAPPING";
    case Cluster_State.MAINTENANCE:
      return "MAINTENANCE";
    case Cluster_State.PROMOTING:
      return "PROMOTING";
    case Cluster_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Type of Cluster */
export enum Cluster_ClusterType {
  /** CLUSTER_TYPE_UNSPECIFIED - The type of the cluster is unknown. */
  CLUSTER_TYPE_UNSPECIFIED = 0,
  /** PRIMARY - Primary cluster that support read and write operations. */
  PRIMARY = 1,
  /**
   * SECONDARY - Secondary cluster that is replicating from another region.
   * This only supports read.
   */
  SECONDARY = 2,
  UNRECOGNIZED = -1,
}

export function cluster_ClusterTypeFromJSON(object: any): Cluster_ClusterType {
  switch (object) {
    case 0:
    case "CLUSTER_TYPE_UNSPECIFIED":
      return Cluster_ClusterType.CLUSTER_TYPE_UNSPECIFIED;
    case 1:
    case "PRIMARY":
      return Cluster_ClusterType.PRIMARY;
    case 2:
    case "SECONDARY":
      return Cluster_ClusterType.SECONDARY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Cluster_ClusterType.UNRECOGNIZED;
  }
}

export function cluster_ClusterTypeToJSON(object: Cluster_ClusterType): string {
  switch (object) {
    case Cluster_ClusterType.CLUSTER_TYPE_UNSPECIFIED:
      return "CLUSTER_TYPE_UNSPECIFIED";
    case Cluster_ClusterType.PRIMARY:
      return "PRIMARY";
    case Cluster_ClusterType.SECONDARY:
      return "SECONDARY";
    case Cluster_ClusterType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Metadata related to network configuration. */
export interface Cluster_NetworkConfig {
  /**
   * Required. The resource link for the VPC network in which cluster
   * resources are created and from which they are accessible via Private IP.
   * The network must belong to the same project as the cluster. It is
   * specified in the form:
   * "projects/{project_number}/global/networks/{network_id}". This is
   * required to create a cluster.
   */
  network: string;
  /**
   * Optional. Name of the allocated IP range for the private IP AlloyDB
   * cluster, for example: "google-managed-services-default". If set, the
   * instance IPs for this cluster will be created in the allocated range. The
   * range name must comply with RFC 1035. Specifically, the name must be 1-63
   * characters long and match the regular expression
   * `[a-z]([-a-z0-9]*[a-z0-9])?`.
   * Field name is intended to be consistent with Cloud SQL.
   */
  allocatedIpRange: string;
}

/**
 * Configuration information for the secondary cluster. This should be set
 * if and only if the cluster is of type SECONDARY.
 */
export interface Cluster_SecondaryConfig {
  /**
   * The name of the primary cluster name with the format:
   * * projects/{project}/locations/{region}/clusters/{cluster_id}
   */
  primaryClusterName: string;
}

/**
 * Configuration for the primary cluster. It has the list of clusters that are
 * replicating from this cluster. This should be set if and only if the
 * cluster is of type PRIMARY.
 */
export interface Cluster_PrimaryConfig {
  /**
   * Output only. Names of the clusters that are replicating from this
   * cluster.
   */
  secondaryClusterNames: string[];
}

export interface Cluster_LabelsEntry {
  key: string;
  value: string;
}

export interface Cluster_AnnotationsEntry {
  key: string;
  value: string;
}

/**
 * An Instance is a computing unit that an end customer can connect to.
 * It's the main unit of computing resources in AlloyDB.
 */
export interface Instance {
  /**
   * Output only. The name of the instance resource with the format:
   *  * projects/{project}/locations/{region}/clusters/{cluster_id}/instances/{instance_id}
   * where the cluster and instance ID segments should satisfy the regex
   * expression `[a-z]([a-z0-9-]{0,61}[a-z0-9])?`, e.g. 1-63 characters of
   * lowercase letters, numbers, and dashes, starting with a letter, and ending
   * with a letter or number. For more details see https://google.aip.dev/122.
   * The prefix of the instance resource name is the name of the parent
   * resource:
   *  * projects/{project}/locations/{region}/clusters/{cluster_id}
   */
  name: string;
  /** User-settable and human-readable display name for the Instance. */
  displayName: string;
  /**
   * Output only. The system-generated UID of the resource. The UID is assigned
   * when the resource is created, and it is retained until it is deleted.
   */
  uid: string;
  /** Output only. Create time stamp */
  createTime:
    | Date
    | undefined;
  /** Output only. Update time stamp */
  updateTime:
    | Date
    | undefined;
  /** Output only. Delete time stamp */
  deleteTime:
    | Date
    | undefined;
  /** Labels as key value pairs */
  labels: { [key: string]: string };
  /** Output only. The current serving state of the instance. */
  state: Instance_State;
  /** Required. The type of the instance. Specified at creation time. */
  instanceType: Instance_InstanceType;
  /**
   * Configurations for the machines that host the underlying
   * database engine.
   */
  machineConfig:
    | Instance_MachineConfig
    | undefined;
  /**
   * Availability type of an Instance.
   * If empty, defaults to REGIONAL for primary instances.
   * For read pools, availability_type is always UNSPECIFIED. Instances in the
   * read pools are evenly distributed across available zones within the region
   * (i.e. read pools with more than one node will have a node in at
   * least two zones).
   */
  availabilityType: Instance_AvailabilityType;
  /**
   * The Compute Engine zone that the instance should serve from, per
   * https://cloud.google.com/compute/docs/regions-zones
   * This can ONLY be specified for ZONAL instances.
   * If present for a REGIONAL instance, an error will be thrown.
   * If this is absent for a ZONAL instance, instance is created in a random
   * zone with available capacity.
   */
  gceZone: string;
  /**
   * Database flags. Set at instance level.
   *  * They are copied from primary instance on read instance creation.
   *  * Read instances can set new or override existing flags that are relevant
   *    for reads, e.g. for enabling columnar cache on a read instance. Flags
   *    set on read instance may or may not be present on primary.
   *
   * This is a list of "key": "value" pairs.
   * "key": The name of the flag. These flags are passed at instance setup time,
   * so include both server options and system variables for Postgres. Flags are
   * specified with underscores, not hyphens.
   * "value": The value of the flag. Booleans are set to **on** for true
   * and **off** for false. This field must be omitted if the flag
   * doesn't take a value.
   */
  databaseFlags: { [key: string]: string };
  /**
   * Output only. This is set for the read-write VM of the PRIMARY instance
   * only.
   */
  writableNode:
    | Instance_Node
    | undefined;
  /**
   * Output only. List of available read-only VMs in this instance, including
   * the standby for a PRIMARY instance.
   */
  nodes: Instance_Node[];
  /** Configuration for query insights. */
  queryInsightsConfig:
    | Instance_QueryInsightsInstanceConfig
    | undefined;
  /**
   * Read pool instance configuration.
   * This is required if the value of instanceType is READ_POOL.
   */
  readPoolConfig:
    | Instance_ReadPoolConfig
    | undefined;
  /**
   * Output only. The IP address for the Instance.
   * This is the connection endpoint for an end-user application.
   */
  ipAddress: string;
  /**
   * Output only. Reconciling (https://google.aip.dev/128#reconciliation).
   * Set to true if the current state of Instance does not match the user's
   * intended state, and the service is actively updating the resource to
   * reconcile them. This can happen due to user-triggered updates or
   * system actions like failover or maintenance.
   */
  reconciling: boolean;
  /** For Resource freshness validation (https://google.aip.dev/154) */
  etag: string;
  /**
   * Annotations to allow client tools to store small amount of arbitrary data.
   * This is distinct from labels.
   * https://google.aip.dev/128
   */
  annotations: { [key: string]: string };
  /** Optional. Client connection specific configurations */
  clientConnectionConfig: Instance_ClientConnectionConfig | undefined;
}

/** Instance State */
export enum Instance_State {
  /** STATE_UNSPECIFIED - The state of the instance is unknown. */
  STATE_UNSPECIFIED = 0,
  /** READY - The instance is active and running. */
  READY = 1,
  /** STOPPED - The instance is stopped. Instance name and IP resources are preserved. */
  STOPPED = 2,
  /** CREATING - The instance is being created. */
  CREATING = 3,
  /** DELETING - The instance is being deleted. */
  DELETING = 4,
  /** MAINTENANCE - The instance is down for maintenance. */
  MAINTENANCE = 5,
  /**
   * FAILED - The creation of the instance failed or a fatal error occurred during
   * an operation on the instance.
   * Note: Instances in this state would tried to be auto-repaired. And
   * Customers should be able to restart, update or delete these instances.
   */
  FAILED = 6,
  /**
   * BOOTSTRAPPING - Index 7 is used in the producer apis for ROLLED_BACK state. Keeping that
   * index unused in case that state also needs to exposed via consumer apis
   * in future.
   * The instance has been configured to sync data from some other source.
   */
  BOOTSTRAPPING = 8,
  /** PROMOTING - The instance is being promoted. */
  PROMOTING = 9,
  UNRECOGNIZED = -1,
}

export function instance_StateFromJSON(object: any): Instance_State {
  switch (object) {
    case 0:
    case "STATE_UNSPECIFIED":
      return Instance_State.STATE_UNSPECIFIED;
    case 1:
    case "READY":
      return Instance_State.READY;
    case 2:
    case "STOPPED":
      return Instance_State.STOPPED;
    case 3:
    case "CREATING":
      return Instance_State.CREATING;
    case 4:
    case "DELETING":
      return Instance_State.DELETING;
    case 5:
    case "MAINTENANCE":
      return Instance_State.MAINTENANCE;
    case 6:
    case "FAILED":
      return Instance_State.FAILED;
    case 8:
    case "BOOTSTRAPPING":
      return Instance_State.BOOTSTRAPPING;
    case 9:
    case "PROMOTING":
      return Instance_State.PROMOTING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Instance_State.UNRECOGNIZED;
  }
}

export function instance_StateToJSON(object: Instance_State): string {
  switch (object) {
    case Instance_State.STATE_UNSPECIFIED:
      return "STATE_UNSPECIFIED";
    case Instance_State.READY:
      return "READY";
    case Instance_State.STOPPED:
      return "STOPPED";
    case Instance_State.CREATING:
      return "CREATING";
    case Instance_State.DELETING:
      return "DELETING";
    case Instance_State.MAINTENANCE:
      return "MAINTENANCE";
    case Instance_State.FAILED:
      return "FAILED";
    case Instance_State.BOOTSTRAPPING:
      return "BOOTSTRAPPING";
    case Instance_State.PROMOTING:
      return "PROMOTING";
    case Instance_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Type of an Instance */
export enum Instance_InstanceType {
  /** INSTANCE_TYPE_UNSPECIFIED - The type of the instance is unknown. */
  INSTANCE_TYPE_UNSPECIFIED = 0,
  /** PRIMARY - PRIMARY instances support read and write operations. */
  PRIMARY = 1,
  /**
   * READ_POOL - READ POOL instances support read operations only. Each read pool instance
   * consists of one or more homogeneous nodes.
   *  * Read pool of size 1 can only have zonal availability.
   *  * Read pools with node count of 2 or more can have regional
   *    availability (nodes are present in 2 or more zones in a region).
   */
  READ_POOL = 2,
  /**
   * SECONDARY - SECONDARY instances support read operations only. SECONDARY instance
   * is a cross-region read replica
   */
  SECONDARY = 3,
  UNRECOGNIZED = -1,
}

export function instance_InstanceTypeFromJSON(object: any): Instance_InstanceType {
  switch (object) {
    case 0:
    case "INSTANCE_TYPE_UNSPECIFIED":
      return Instance_InstanceType.INSTANCE_TYPE_UNSPECIFIED;
    case 1:
    case "PRIMARY":
      return Instance_InstanceType.PRIMARY;
    case 2:
    case "READ_POOL":
      return Instance_InstanceType.READ_POOL;
    case 3:
    case "SECONDARY":
      return Instance_InstanceType.SECONDARY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Instance_InstanceType.UNRECOGNIZED;
  }
}

export function instance_InstanceTypeToJSON(object: Instance_InstanceType): string {
  switch (object) {
    case Instance_InstanceType.INSTANCE_TYPE_UNSPECIFIED:
      return "INSTANCE_TYPE_UNSPECIFIED";
    case Instance_InstanceType.PRIMARY:
      return "PRIMARY";
    case Instance_InstanceType.READ_POOL:
      return "READ_POOL";
    case Instance_InstanceType.SECONDARY:
      return "SECONDARY";
    case Instance_InstanceType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * The Availability type of an instance. Potential values:
 *
 * - ZONAL: The instance serves data from only one zone. Outages in that
 *     zone affect instance availability.
 * - REGIONAL: The instance can serve data from more than one zone in a
 *     region (it is highly available).
 */
export enum Instance_AvailabilityType {
  /** AVAILABILITY_TYPE_UNSPECIFIED - This is an unknown Availability type. */
  AVAILABILITY_TYPE_UNSPECIFIED = 0,
  /** ZONAL - Zonal available instance. */
  ZONAL = 1,
  /** REGIONAL - Regional (or Highly) available instance. */
  REGIONAL = 2,
  UNRECOGNIZED = -1,
}

export function instance_AvailabilityTypeFromJSON(object: any): Instance_AvailabilityType {
  switch (object) {
    case 0:
    case "AVAILABILITY_TYPE_UNSPECIFIED":
      return Instance_AvailabilityType.AVAILABILITY_TYPE_UNSPECIFIED;
    case 1:
    case "ZONAL":
      return Instance_AvailabilityType.ZONAL;
    case 2:
    case "REGIONAL":
      return Instance_AvailabilityType.REGIONAL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Instance_AvailabilityType.UNRECOGNIZED;
  }
}

export function instance_AvailabilityTypeToJSON(object: Instance_AvailabilityType): string {
  switch (object) {
    case Instance_AvailabilityType.AVAILABILITY_TYPE_UNSPECIFIED:
      return "AVAILABILITY_TYPE_UNSPECIFIED";
    case Instance_AvailabilityType.ZONAL:
      return "ZONAL";
    case Instance_AvailabilityType.REGIONAL:
      return "REGIONAL";
    case Instance_AvailabilityType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** MachineConfig describes the configuration of a machine. */
export interface Instance_MachineConfig {
  /** The number of CPU's in the VM instance. */
  cpuCount: number;
}

/**
 * Details of a single node in the instance.
 * Nodes in an AlloyDB instance are ephemereal, they can change during
 * update, failover, autohealing and resize operations.
 */
export interface Instance_Node {
  /** The Compute Engine zone of the VM e.g. "us-central1-b". */
  zoneId: string;
  /** The identifier of the VM e.g. "test-read-0601-407e52be-ms3l". */
  id: string;
  /** The private IP address of the VM e.g. "10.57.0.34". */
  ip: string;
  /**
   * Determined by state of the compute VM and postgres-service health.
   * Compute VM state can have values listed in
   * https://cloud.google.com/compute/docs/instances/instance-life-cycle and
   * postgres-service health can have values: HEALTHY and UNHEALTHY.
   */
  state: string;
}

/** QueryInsights Instance specific configuration. */
export interface Instance_QueryInsightsInstanceConfig {
  /**
   * Record application tags for an instance.
   * This flag is turned "on" by default.
   */
  recordApplicationTags?:
    | boolean
    | undefined;
  /**
   * Record client address for an instance. Client address is PII information.
   * This flag is turned "on" by default.
   */
  recordClientAddress?:
    | boolean
    | undefined;
  /**
   * Query string length. The default value is 1024.
   * Any integer between 256 and 4500 is considered valid.
   */
  queryStringLength: number;
  /**
   * Number of query execution plans captured by Insights per minute
   * for all queries combined. The default value is 5.
   * Any integer between 0 and 20 is considered valid.
   */
  queryPlansPerMinute?: number | undefined;
}

/** Configuration for a read pool instance. */
export interface Instance_ReadPoolConfig {
  /** Read capacity, i.e. number of nodes in a read pool instance. */
  nodeCount: number;
}

/** Client connection configuration */
export interface Instance_ClientConnectionConfig {
  /**
   * Optional. Configuration to enforce connectors only (ex: AuthProxy)
   * connections to the database.
   */
  requireConnectors: boolean;
  /** Optional. SSL config option for this instance. */
  sslConfig: SslConfig | undefined;
}

export interface Instance_LabelsEntry {
  key: string;
  value: string;
}

export interface Instance_DatabaseFlagsEntry {
  key: string;
  value: string;
}

export interface Instance_AnnotationsEntry {
  key: string;
  value: string;
}

/**
 * ConnectionInfo singleton resource.
 * https://google.aip.dev/156
 */
export interface ConnectionInfo {
  /**
   * The name of the ConnectionInfo singleton resource, e.g.:
   * projects/{project}/locations/{location}/clusters/* /instances/* /connectionInfo
   * This field currently has no semantic meaning.
   */
  name: string;
  /**
   * Output only. The private network IP address for the Instance. This is the
   * default IP for the instance and is always created (even if enable_public_ip
   * is set). This is the connection endpoint for an end-user application.
   */
  ipAddress: string;
  /** Output only. The unique ID of the Instance. */
  instanceUid: string;
}

/** Message describing Backup object */
export interface Backup {
  /**
   * Output only. The name of the backup resource with the format:
   *  * projects/{project}/locations/{region}/backups/{backup_id}
   * where the cluster and backup ID segments should satisfy the regex
   * expression `[a-z]([a-z0-9-]{0,61}[a-z0-9])?`, e.g. 1-63 characters of
   * lowercase letters, numbers, and dashes, starting with a letter, and ending
   * with a letter or number. For more details see https://google.aip.dev/122.
   * The prefix of the backup resource name is the name of the parent
   * resource:
   *  * projects/{project}/locations/{region}
   */
  name: string;
  /** User-settable and human-readable display name for the Backup. */
  displayName: string;
  /**
   * Output only. The system-generated UID of the resource. The UID is assigned
   * when the resource is created, and it is retained until it is deleted.
   */
  uid: string;
  /** Output only. Create time stamp */
  createTime:
    | Date
    | undefined;
  /** Output only. Update time stamp */
  updateTime:
    | Date
    | undefined;
  /** Output only. Delete time stamp */
  deleteTime:
    | Date
    | undefined;
  /** Labels as key value pairs */
  labels: { [key: string]: string };
  /** Output only. The current state of the backup. */
  state: Backup_State;
  /** The backup type, which suggests the trigger for the backup. */
  type: Backup_Type;
  /** User-provided description of the backup. */
  description: string;
  /**
   * Output only. The system-generated UID of the cluster which was used to
   * create this resource.
   */
  clusterUid: string;
  /**
   * Required. The full resource name of the backup source cluster
   * (e.g., projects/{project}/locations/{region}/clusters/{cluster_id}).
   */
  clusterName: string;
  /**
   * Output only. Reconciling (https://google.aip.dev/128#reconciliation), if
   * true, indicates that the service is actively updating the resource. This
   * can happen due to user-triggered updates or system actions like failover or
   * maintenance.
   */
  reconciling: boolean;
  /**
   * Optional. The encryption config can be specified to encrypt the
   * backup with a customer-managed encryption key (CMEK). When this field is
   * not specified, the backup will then use default encryption scheme to
   * protect the user data.
   */
  encryptionConfig:
    | EncryptionConfig
    | undefined;
  /** Output only. The encryption information for the backup. */
  encryptionInfo:
    | EncryptionInfo
    | undefined;
  /** For Resource freshness validation (https://google.aip.dev/154) */
  etag: string;
  /**
   * Annotations to allow client tools to store small amount of arbitrary data.
   * This is distinct from labels.
   * https://google.aip.dev/128
   */
  annotations: { [key: string]: string };
  /** Output only. The size of the backup in bytes. */
  sizeBytes: Long;
  /**
   * Output only. The time at which after the backup is eligible to be garbage
   * collected. It is the duration specified by the backup's retention policy,
   * added to the backup's create_time.
   */
  expiryTime:
    | Date
    | undefined;
  /**
   * Output only. The QuantityBasedExpiry of the backup, specified by the
   * backup's retention policy. Once the expiry quantity is over retention, the
   * backup is eligible to be garbage collected.
   */
  expiryQuantity:
    | Backup_QuantityBasedExpiry
    | undefined;
  /**
   * Output only. The database engine major version of the cluster this backup
   * was created from. Any restored cluster created from this backup will have
   * the same database version.
   */
  databaseVersion: DatabaseVersion;
}

/** Backup State */
export enum Backup_State {
  /** STATE_UNSPECIFIED - The state of the backup is unknown. */
  STATE_UNSPECIFIED = 0,
  /** READY - The backup is ready. */
  READY = 1,
  /** CREATING - The backup is creating. */
  CREATING = 2,
  /** FAILED - The backup failed. */
  FAILED = 3,
  /** DELETING - The backup is being deleted. */
  DELETING = 4,
  UNRECOGNIZED = -1,
}

export function backup_StateFromJSON(object: any): Backup_State {
  switch (object) {
    case 0:
    case "STATE_UNSPECIFIED":
      return Backup_State.STATE_UNSPECIFIED;
    case 1:
    case "READY":
      return Backup_State.READY;
    case 2:
    case "CREATING":
      return Backup_State.CREATING;
    case 3:
    case "FAILED":
      return Backup_State.FAILED;
    case 4:
    case "DELETING":
      return Backup_State.DELETING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Backup_State.UNRECOGNIZED;
  }
}

export function backup_StateToJSON(object: Backup_State): string {
  switch (object) {
    case Backup_State.STATE_UNSPECIFIED:
      return "STATE_UNSPECIFIED";
    case Backup_State.READY:
      return "READY";
    case Backup_State.CREATING:
      return "CREATING";
    case Backup_State.FAILED:
      return "FAILED";
    case Backup_State.DELETING:
      return "DELETING";
    case Backup_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Backup Type */
export enum Backup_Type {
  /** TYPE_UNSPECIFIED - Backup Type is unknown. */
  TYPE_UNSPECIFIED = 0,
  /**
   * ON_DEMAND - ON_DEMAND backups that were triggered by the customer (e.g., not
   * AUTOMATED).
   */
  ON_DEMAND = 1,
  /**
   * AUTOMATED - AUTOMATED backups triggered by the automated backups scheduler pursuant
   * to an automated backup policy.
   */
  AUTOMATED = 2,
  /**
   * CONTINUOUS - CONTINUOUS backups triggered by the automated backups scheduler
   * due to a continuous backup policy.
   */
  CONTINUOUS = 3,
  UNRECOGNIZED = -1,
}

export function backup_TypeFromJSON(object: any): Backup_Type {
  switch (object) {
    case 0:
    case "TYPE_UNSPECIFIED":
      return Backup_Type.TYPE_UNSPECIFIED;
    case 1:
    case "ON_DEMAND":
      return Backup_Type.ON_DEMAND;
    case 2:
    case "AUTOMATED":
      return Backup_Type.AUTOMATED;
    case 3:
    case "CONTINUOUS":
      return Backup_Type.CONTINUOUS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Backup_Type.UNRECOGNIZED;
  }
}

export function backup_TypeToJSON(object: Backup_Type): string {
  switch (object) {
    case Backup_Type.TYPE_UNSPECIFIED:
      return "TYPE_UNSPECIFIED";
    case Backup_Type.ON_DEMAND:
      return "ON_DEMAND";
    case Backup_Type.AUTOMATED:
      return "AUTOMATED";
    case Backup_Type.CONTINUOUS:
      return "CONTINUOUS";
    case Backup_Type.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * A backup's position in a quantity-based retention queue, of backups with
 * the same source cluster and type, with length, retention, specified by the
 * backup's retention policy.
 * Once the position is greater than the retention, the backup is eligible to
 * be garbage collected.
 *
 * Example: 5 backups from the same source cluster and type with a
 * quantity-based retention of 3 and denoted by backup_id (position,
 * retention).
 *
 * Safe: backup_5 (1, 3), backup_4, (2, 3), backup_3 (3, 3).
 * Awaiting garbage collection: backup_2 (4, 3), backup_1 (5, 3)
 */
export interface Backup_QuantityBasedExpiry {
  /**
   * Output only. The backup's position among its backups with the same source
   * cluster and type, by descending chronological order create time(i.e.
   * newest first).
   */
  retentionCount: number;
  /**
   * Output only. The length of the quantity-based queue, specified by the
   * backup's retention policy.
   */
  totalRetentionCount: number;
}

export interface Backup_LabelsEntry {
  key: string;
  value: string;
}

export interface Backup_AnnotationsEntry {
  key: string;
  value: string;
}

/**
 * SupportedDatabaseFlag gives general information about a database flag,
 * like type and allowed values. This is a static value that is defined
 * on the server side, and it cannot be modified by callers.
 * To set the Database flags on a particular Instance, a caller should modify
 * the Instance.database_flags field.
 */
export interface SupportedDatabaseFlag {
  /** Restriction on STRING type value. */
  stringRestrictions?:
    | SupportedDatabaseFlag_StringRestrictions
    | undefined;
  /** Restriction on INTEGER type value. */
  integerRestrictions?:
    | SupportedDatabaseFlag_IntegerRestrictions
    | undefined;
  /**
   * The name of the flag resource, following Google Cloud conventions, e.g.:
   *  * projects/{project}/locations/{location}/flags/{flag}
   * This field currently has no semantic meaning.
   */
  name: string;
  /**
   * The name of the database flag, e.g. "max_allowed_packets".
   * The is a possibly key for the Instance.database_flags map field.
   */
  flagName: string;
  valueType: SupportedDatabaseFlag_ValueType;
  /**
   * Whether the database flag accepts multiple values. If true,
   * a comma-separated list of stringified values may be specified.
   */
  acceptsMultipleValues: boolean;
  /** Major database engine versions for which this flag is supported. */
  supportedDbVersions: DatabaseVersion[];
  /**
   * Whether setting or updating this flag on an Instance requires a database
   * restart. If a flag that requires database restart is set, the backend
   * will automatically restart the database (making sure to satisfy any
   * availability SLO's).
   */
  requiresDbRestart: boolean;
}

/**
 * ValueType describes the semantic type of the value that the flag accepts.
 * Regardless of the ValueType, the Instance.database_flags field accepts the
 * stringified version of the value, i.e. "20" or "3.14".
 */
export enum SupportedDatabaseFlag_ValueType {
  /** VALUE_TYPE_UNSPECIFIED - This is an unknown flag type. */
  VALUE_TYPE_UNSPECIFIED = 0,
  /** STRING - String type flag. */
  STRING = 1,
  /** INTEGER - Integer type flag. */
  INTEGER = 2,
  /** FLOAT - Float type flag. */
  FLOAT = 3,
  /** NONE - Denotes that the flag does not accept any values. */
  NONE = 4,
  UNRECOGNIZED = -1,
}

export function supportedDatabaseFlag_ValueTypeFromJSON(object: any): SupportedDatabaseFlag_ValueType {
  switch (object) {
    case 0:
    case "VALUE_TYPE_UNSPECIFIED":
      return SupportedDatabaseFlag_ValueType.VALUE_TYPE_UNSPECIFIED;
    case 1:
    case "STRING":
      return SupportedDatabaseFlag_ValueType.STRING;
    case 2:
    case "INTEGER":
      return SupportedDatabaseFlag_ValueType.INTEGER;
    case 3:
    case "FLOAT":
      return SupportedDatabaseFlag_ValueType.FLOAT;
    case 4:
    case "NONE":
      return SupportedDatabaseFlag_ValueType.NONE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SupportedDatabaseFlag_ValueType.UNRECOGNIZED;
  }
}

export function supportedDatabaseFlag_ValueTypeToJSON(object: SupportedDatabaseFlag_ValueType): string {
  switch (object) {
    case SupportedDatabaseFlag_ValueType.VALUE_TYPE_UNSPECIFIED:
      return "VALUE_TYPE_UNSPECIFIED";
    case SupportedDatabaseFlag_ValueType.STRING:
      return "STRING";
    case SupportedDatabaseFlag_ValueType.INTEGER:
      return "INTEGER";
    case SupportedDatabaseFlag_ValueType.FLOAT:
      return "FLOAT";
    case SupportedDatabaseFlag_ValueType.NONE:
      return "NONE";
    case SupportedDatabaseFlag_ValueType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Restrictions on STRING type values */
export interface SupportedDatabaseFlag_StringRestrictions {
  /**
   * The list of allowed values, if bounded. This field will be empty
   * if there is a unbounded number of allowed values.
   */
  allowedValues: string[];
}

/** Restrictions on INTEGER type values. */
export interface SupportedDatabaseFlag_IntegerRestrictions {
  /** The minimum value that can be specified, if applicable. */
  minValue:
    | Long
    | undefined;
  /** The maximum value that can be specified, if applicable. */
  maxValue: Long | undefined;
}

/** Message describing User object. */
export interface User {
  /**
   * Output only. Name of the resource in the form of
   * projects/{project}/locations/{location}/cluster/{cluster}/users/{user}.
   */
  name: string;
  /** Input only. Password for the user. */
  password: string;
  /**
   * Optional. List of database roles this user has.
   * The database role strings are subject to the PostgreSQL naming conventions.
   */
  databaseRoles: string[];
  /** Optional. Type of this user. */
  userType: User_UserType;
}

/** Enum that details the user type. */
export enum User_UserType {
  /** USER_TYPE_UNSPECIFIED - Unspecified user type. */
  USER_TYPE_UNSPECIFIED = 0,
  /**
   * ALLOYDB_BUILT_IN - The default user type that authenticates via password-based
   * authentication.
   */
  ALLOYDB_BUILT_IN = 1,
  /** ALLOYDB_IAM_USER - Database user that can authenticate via IAM-Based authentication. */
  ALLOYDB_IAM_USER = 2,
  UNRECOGNIZED = -1,
}

export function user_UserTypeFromJSON(object: any): User_UserType {
  switch (object) {
    case 0:
    case "USER_TYPE_UNSPECIFIED":
      return User_UserType.USER_TYPE_UNSPECIFIED;
    case 1:
    case "ALLOYDB_BUILT_IN":
      return User_UserType.ALLOYDB_BUILT_IN;
    case 2:
    case "ALLOYDB_IAM_USER":
      return User_UserType.ALLOYDB_IAM_USER;
    case -1:
    case "UNRECOGNIZED":
    default:
      return User_UserType.UNRECOGNIZED;
  }
}

export function user_UserTypeToJSON(object: User_UserType): string {
  switch (object) {
    case User_UserType.USER_TYPE_UNSPECIFIED:
      return "USER_TYPE_UNSPECIFIED";
    case User_UserType.ALLOYDB_BUILT_IN:
      return "ALLOYDB_BUILT_IN";
    case User_UserType.ALLOYDB_IAM_USER:
      return "ALLOYDB_IAM_USER";
    case User_UserType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseUserPassword(): UserPassword {
  return { user: "", password: "" };
}

export const UserPassword: MessageFns<UserPassword> = {
  encode(message: UserPassword, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.user !== "") {
      writer.uint32(10).string(message.user);
    }
    if (message.password !== "") {
      writer.uint32(18).string(message.password);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UserPassword {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUserPassword();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.user = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.password = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UserPassword {
    return {
      user: isSet(object.user) ? globalThis.String(object.user) : "",
      password: isSet(object.password) ? globalThis.String(object.password) : "",
    };
  },

  toJSON(message: UserPassword): unknown {
    const obj: any = {};
    if (message.user !== "") {
      obj.user = message.user;
    }
    if (message.password !== "") {
      obj.password = message.password;
    }
    return obj;
  },

  create(base?: DeepPartial<UserPassword>): UserPassword {
    return UserPassword.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UserPassword>): UserPassword {
    const message = createBaseUserPassword();
    message.user = object.user ?? "";
    message.password = object.password ?? "";
    return message;
  },
};

function createBaseMigrationSource(): MigrationSource {
  return { hostPort: "", referenceId: "", sourceType: 0 };
}

export const MigrationSource: MessageFns<MigrationSource> = {
  encode(message: MigrationSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.hostPort !== "") {
      writer.uint32(10).string(message.hostPort);
    }
    if (message.referenceId !== "") {
      writer.uint32(18).string(message.referenceId);
    }
    if (message.sourceType !== 0) {
      writer.uint32(24).int32(message.sourceType);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MigrationSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMigrationSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.hostPort = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.referenceId = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.sourceType = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MigrationSource {
    return {
      hostPort: isSet(object.hostPort) ? globalThis.String(object.hostPort) : "",
      referenceId: isSet(object.referenceId) ? globalThis.String(object.referenceId) : "",
      sourceType: isSet(object.sourceType) ? migrationSource_MigrationSourceTypeFromJSON(object.sourceType) : 0,
    };
  },

  toJSON(message: MigrationSource): unknown {
    const obj: any = {};
    if (message.hostPort !== "") {
      obj.hostPort = message.hostPort;
    }
    if (message.referenceId !== "") {
      obj.referenceId = message.referenceId;
    }
    if (message.sourceType !== 0) {
      obj.sourceType = migrationSource_MigrationSourceTypeToJSON(message.sourceType);
    }
    return obj;
  },

  create(base?: DeepPartial<MigrationSource>): MigrationSource {
    return MigrationSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MigrationSource>): MigrationSource {
    const message = createBaseMigrationSource();
    message.hostPort = object.hostPort ?? "";
    message.referenceId = object.referenceId ?? "";
    message.sourceType = object.sourceType ?? 0;
    return message;
  },
};

function createBaseEncryptionConfig(): EncryptionConfig {
  return { kmsKeyName: "" };
}

export const EncryptionConfig: MessageFns<EncryptionConfig> = {
  encode(message: EncryptionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kmsKeyName !== "") {
      writer.uint32(10).string(message.kmsKeyName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EncryptionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEncryptionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kmsKeyName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EncryptionConfig {
    return { kmsKeyName: isSet(object.kmsKeyName) ? globalThis.String(object.kmsKeyName) : "" };
  },

  toJSON(message: EncryptionConfig): unknown {
    const obj: any = {};
    if (message.kmsKeyName !== "") {
      obj.kmsKeyName = message.kmsKeyName;
    }
    return obj;
  },

  create(base?: DeepPartial<EncryptionConfig>): EncryptionConfig {
    return EncryptionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EncryptionConfig>): EncryptionConfig {
    const message = createBaseEncryptionConfig();
    message.kmsKeyName = object.kmsKeyName ?? "";
    return message;
  },
};

function createBaseEncryptionInfo(): EncryptionInfo {
  return { encryptionType: 0, kmsKeyVersions: [] };
}

export const EncryptionInfo: MessageFns<EncryptionInfo> = {
  encode(message: EncryptionInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.encryptionType !== 0) {
      writer.uint32(8).int32(message.encryptionType);
    }
    for (const v of message.kmsKeyVersions) {
      writer.uint32(18).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EncryptionInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEncryptionInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.encryptionType = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.kmsKeyVersions.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EncryptionInfo {
    return {
      encryptionType: isSet(object.encryptionType) ? encryptionInfo_TypeFromJSON(object.encryptionType) : 0,
      kmsKeyVersions: globalThis.Array.isArray(object?.kmsKeyVersions)
        ? object.kmsKeyVersions.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: EncryptionInfo): unknown {
    const obj: any = {};
    if (message.encryptionType !== 0) {
      obj.encryptionType = encryptionInfo_TypeToJSON(message.encryptionType);
    }
    if (message.kmsKeyVersions?.length) {
      obj.kmsKeyVersions = message.kmsKeyVersions;
    }
    return obj;
  },

  create(base?: DeepPartial<EncryptionInfo>): EncryptionInfo {
    return EncryptionInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EncryptionInfo>): EncryptionInfo {
    const message = createBaseEncryptionInfo();
    message.encryptionType = object.encryptionType ?? 0;
    message.kmsKeyVersions = object.kmsKeyVersions?.map((e) => e) || [];
    return message;
  },
};

function createBaseSslConfig(): SslConfig {
  return { sslMode: 0, caSource: 0 };
}

export const SslConfig: MessageFns<SslConfig> = {
  encode(message: SslConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.sslMode !== 0) {
      writer.uint32(8).int32(message.sslMode);
    }
    if (message.caSource !== 0) {
      writer.uint32(16).int32(message.caSource);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SslConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSslConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.sslMode = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.caSource = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SslConfig {
    return {
      sslMode: isSet(object.sslMode) ? sslConfig_SslModeFromJSON(object.sslMode) : 0,
      caSource: isSet(object.caSource) ? sslConfig_CaSourceFromJSON(object.caSource) : 0,
    };
  },

  toJSON(message: SslConfig): unknown {
    const obj: any = {};
    if (message.sslMode !== 0) {
      obj.sslMode = sslConfig_SslModeToJSON(message.sslMode);
    }
    if (message.caSource !== 0) {
      obj.caSource = sslConfig_CaSourceToJSON(message.caSource);
    }
    return obj;
  },

  create(base?: DeepPartial<SslConfig>): SslConfig {
    return SslConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SslConfig>): SslConfig {
    const message = createBaseSslConfig();
    message.sslMode = object.sslMode ?? 0;
    message.caSource = object.caSource ?? 0;
    return message;
  },
};

function createBaseAutomatedBackupPolicy(): AutomatedBackupPolicy {
  return {
    weeklySchedule: undefined,
    timeBasedRetention: undefined,
    quantityBasedRetention: undefined,
    enabled: undefined,
    backupWindow: undefined,
    encryptionConfig: undefined,
    location: "",
    labels: {},
  };
}

export const AutomatedBackupPolicy: MessageFns<AutomatedBackupPolicy> = {
  encode(message: AutomatedBackupPolicy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.weeklySchedule !== undefined) {
      AutomatedBackupPolicy_WeeklySchedule.encode(message.weeklySchedule, writer.uint32(18).fork()).join();
    }
    if (message.timeBasedRetention !== undefined) {
      AutomatedBackupPolicy_TimeBasedRetention.encode(message.timeBasedRetention, writer.uint32(34).fork()).join();
    }
    if (message.quantityBasedRetention !== undefined) {
      AutomatedBackupPolicy_QuantityBasedRetention.encode(message.quantityBasedRetention, writer.uint32(42).fork())
        .join();
    }
    if (message.enabled !== undefined) {
      writer.uint32(8).bool(message.enabled);
    }
    if (message.backupWindow !== undefined) {
      Duration.encode(message.backupWindow, writer.uint32(26).fork()).join();
    }
    if (message.encryptionConfig !== undefined) {
      EncryptionConfig.encode(message.encryptionConfig, writer.uint32(66).fork()).join();
    }
    if (message.location !== "") {
      writer.uint32(50).string(message.location);
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      AutomatedBackupPolicy_LabelsEntry.encode({ key: key as any, value }, writer.uint32(58).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutomatedBackupPolicy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutomatedBackupPolicy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.weeklySchedule = AutomatedBackupPolicy_WeeklySchedule.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.timeBasedRetention = AutomatedBackupPolicy_TimeBasedRetention.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.quantityBasedRetention = AutomatedBackupPolicy_QuantityBasedRetention.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 8) {
            break;
          }

          message.enabled = reader.bool();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.backupWindow = Duration.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.encryptionConfig = EncryptionConfig.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.location = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          const entry7 = AutomatedBackupPolicy_LabelsEntry.decode(reader, reader.uint32());
          if (entry7.value !== undefined) {
            message.labels[entry7.key] = entry7.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutomatedBackupPolicy {
    return {
      weeklySchedule: isSet(object.weeklySchedule)
        ? AutomatedBackupPolicy_WeeklySchedule.fromJSON(object.weeklySchedule)
        : undefined,
      timeBasedRetention: isSet(object.timeBasedRetention)
        ? AutomatedBackupPolicy_TimeBasedRetention.fromJSON(object.timeBasedRetention)
        : undefined,
      quantityBasedRetention: isSet(object.quantityBasedRetention)
        ? AutomatedBackupPolicy_QuantityBasedRetention.fromJSON(object.quantityBasedRetention)
        : undefined,
      enabled: isSet(object.enabled) ? globalThis.Boolean(object.enabled) : undefined,
      backupWindow: isSet(object.backupWindow) ? Duration.fromJSON(object.backupWindow) : undefined,
      encryptionConfig: isSet(object.encryptionConfig) ? EncryptionConfig.fromJSON(object.encryptionConfig) : undefined,
      location: isSet(object.location) ? globalThis.String(object.location) : "",
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: AutomatedBackupPolicy): unknown {
    const obj: any = {};
    if (message.weeklySchedule !== undefined) {
      obj.weeklySchedule = AutomatedBackupPolicy_WeeklySchedule.toJSON(message.weeklySchedule);
    }
    if (message.timeBasedRetention !== undefined) {
      obj.timeBasedRetention = AutomatedBackupPolicy_TimeBasedRetention.toJSON(message.timeBasedRetention);
    }
    if (message.quantityBasedRetention !== undefined) {
      obj.quantityBasedRetention = AutomatedBackupPolicy_QuantityBasedRetention.toJSON(message.quantityBasedRetention);
    }
    if (message.enabled !== undefined) {
      obj.enabled = message.enabled;
    }
    if (message.backupWindow !== undefined) {
      obj.backupWindow = Duration.toJSON(message.backupWindow);
    }
    if (message.encryptionConfig !== undefined) {
      obj.encryptionConfig = EncryptionConfig.toJSON(message.encryptionConfig);
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<AutomatedBackupPolicy>): AutomatedBackupPolicy {
    return AutomatedBackupPolicy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutomatedBackupPolicy>): AutomatedBackupPolicy {
    const message = createBaseAutomatedBackupPolicy();
    message.weeklySchedule = (object.weeklySchedule !== undefined && object.weeklySchedule !== null)
      ? AutomatedBackupPolicy_WeeklySchedule.fromPartial(object.weeklySchedule)
      : undefined;
    message.timeBasedRetention = (object.timeBasedRetention !== undefined && object.timeBasedRetention !== null)
      ? AutomatedBackupPolicy_TimeBasedRetention.fromPartial(object.timeBasedRetention)
      : undefined;
    message.quantityBasedRetention =
      (object.quantityBasedRetention !== undefined && object.quantityBasedRetention !== null)
        ? AutomatedBackupPolicy_QuantityBasedRetention.fromPartial(object.quantityBasedRetention)
        : undefined;
    message.enabled = object.enabled ?? undefined;
    message.backupWindow = (object.backupWindow !== undefined && object.backupWindow !== null)
      ? Duration.fromPartial(object.backupWindow)
      : undefined;
    message.encryptionConfig = (object.encryptionConfig !== undefined && object.encryptionConfig !== null)
      ? EncryptionConfig.fromPartial(object.encryptionConfig)
      : undefined;
    message.location = object.location ?? "";
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseAutomatedBackupPolicy_WeeklySchedule(): AutomatedBackupPolicy_WeeklySchedule {
  return { startTimes: [], daysOfWeek: [] };
}

export const AutomatedBackupPolicy_WeeklySchedule: MessageFns<AutomatedBackupPolicy_WeeklySchedule> = {
  encode(message: AutomatedBackupPolicy_WeeklySchedule, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.startTimes) {
      TimeOfDay.encode(v!, writer.uint32(10).fork()).join();
    }
    writer.uint32(18).fork();
    for (const v of message.daysOfWeek) {
      writer.int32(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutomatedBackupPolicy_WeeklySchedule {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutomatedBackupPolicy_WeeklySchedule();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startTimes.push(TimeOfDay.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag === 16) {
            message.daysOfWeek.push(reader.int32() as any);

            continue;
          }

          if (tag === 18) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.daysOfWeek.push(reader.int32() as any);
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutomatedBackupPolicy_WeeklySchedule {
    return {
      startTimes: globalThis.Array.isArray(object?.startTimes)
        ? object.startTimes.map((e: any) => TimeOfDay.fromJSON(e))
        : [],
      daysOfWeek: globalThis.Array.isArray(object?.daysOfWeek)
        ? object.daysOfWeek.map((e: any) => dayOfWeekFromJSON(e))
        : [],
    };
  },

  toJSON(message: AutomatedBackupPolicy_WeeklySchedule): unknown {
    const obj: any = {};
    if (message.startTimes?.length) {
      obj.startTimes = message.startTimes.map((e) => TimeOfDay.toJSON(e));
    }
    if (message.daysOfWeek?.length) {
      obj.daysOfWeek = message.daysOfWeek.map((e) => dayOfWeekToJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<AutomatedBackupPolicy_WeeklySchedule>): AutomatedBackupPolicy_WeeklySchedule {
    return AutomatedBackupPolicy_WeeklySchedule.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutomatedBackupPolicy_WeeklySchedule>): AutomatedBackupPolicy_WeeklySchedule {
    const message = createBaseAutomatedBackupPolicy_WeeklySchedule();
    message.startTimes = object.startTimes?.map((e) => TimeOfDay.fromPartial(e)) || [];
    message.daysOfWeek = object.daysOfWeek?.map((e) => e) || [];
    return message;
  },
};

function createBaseAutomatedBackupPolicy_TimeBasedRetention(): AutomatedBackupPolicy_TimeBasedRetention {
  return { retentionPeriod: undefined };
}

export const AutomatedBackupPolicy_TimeBasedRetention: MessageFns<AutomatedBackupPolicy_TimeBasedRetention> = {
  encode(message: AutomatedBackupPolicy_TimeBasedRetention, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.retentionPeriod !== undefined) {
      Duration.encode(message.retentionPeriod, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutomatedBackupPolicy_TimeBasedRetention {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutomatedBackupPolicy_TimeBasedRetention();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.retentionPeriod = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutomatedBackupPolicy_TimeBasedRetention {
    return { retentionPeriod: isSet(object.retentionPeriod) ? Duration.fromJSON(object.retentionPeriod) : undefined };
  },

  toJSON(message: AutomatedBackupPolicy_TimeBasedRetention): unknown {
    const obj: any = {};
    if (message.retentionPeriod !== undefined) {
      obj.retentionPeriod = Duration.toJSON(message.retentionPeriod);
    }
    return obj;
  },

  create(base?: DeepPartial<AutomatedBackupPolicy_TimeBasedRetention>): AutomatedBackupPolicy_TimeBasedRetention {
    return AutomatedBackupPolicy_TimeBasedRetention.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutomatedBackupPolicy_TimeBasedRetention>): AutomatedBackupPolicy_TimeBasedRetention {
    const message = createBaseAutomatedBackupPolicy_TimeBasedRetention();
    message.retentionPeriod = (object.retentionPeriod !== undefined && object.retentionPeriod !== null)
      ? Duration.fromPartial(object.retentionPeriod)
      : undefined;
    return message;
  },
};

function createBaseAutomatedBackupPolicy_QuantityBasedRetention(): AutomatedBackupPolicy_QuantityBasedRetention {
  return { count: 0 };
}

export const AutomatedBackupPolicy_QuantityBasedRetention: MessageFns<AutomatedBackupPolicy_QuantityBasedRetention> = {
  encode(
    message: AutomatedBackupPolicy_QuantityBasedRetention,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.count !== 0) {
      writer.uint32(8).int32(message.count);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutomatedBackupPolicy_QuantityBasedRetention {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutomatedBackupPolicy_QuantityBasedRetention();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.count = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutomatedBackupPolicy_QuantityBasedRetention {
    return { count: isSet(object.count) ? globalThis.Number(object.count) : 0 };
  },

  toJSON(message: AutomatedBackupPolicy_QuantityBasedRetention): unknown {
    const obj: any = {};
    if (message.count !== 0) {
      obj.count = Math.round(message.count);
    }
    return obj;
  },

  create(
    base?: DeepPartial<AutomatedBackupPolicy_QuantityBasedRetention>,
  ): AutomatedBackupPolicy_QuantityBasedRetention {
    return AutomatedBackupPolicy_QuantityBasedRetention.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<AutomatedBackupPolicy_QuantityBasedRetention>,
  ): AutomatedBackupPolicy_QuantityBasedRetention {
    const message = createBaseAutomatedBackupPolicy_QuantityBasedRetention();
    message.count = object.count ?? 0;
    return message;
  },
};

function createBaseAutomatedBackupPolicy_LabelsEntry(): AutomatedBackupPolicy_LabelsEntry {
  return { key: "", value: "" };
}

export const AutomatedBackupPolicy_LabelsEntry: MessageFns<AutomatedBackupPolicy_LabelsEntry> = {
  encode(message: AutomatedBackupPolicy_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutomatedBackupPolicy_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutomatedBackupPolicy_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutomatedBackupPolicy_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: AutomatedBackupPolicy_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<AutomatedBackupPolicy_LabelsEntry>): AutomatedBackupPolicy_LabelsEntry {
    return AutomatedBackupPolicy_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutomatedBackupPolicy_LabelsEntry>): AutomatedBackupPolicy_LabelsEntry {
    const message = createBaseAutomatedBackupPolicy_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseContinuousBackupConfig(): ContinuousBackupConfig {
  return { enabled: undefined, recoveryWindowDays: 0, encryptionConfig: undefined };
}

export const ContinuousBackupConfig: MessageFns<ContinuousBackupConfig> = {
  encode(message: ContinuousBackupConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.enabled !== undefined) {
      writer.uint32(8).bool(message.enabled);
    }
    if (message.recoveryWindowDays !== 0) {
      writer.uint32(32).int32(message.recoveryWindowDays);
    }
    if (message.encryptionConfig !== undefined) {
      EncryptionConfig.encode(message.encryptionConfig, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ContinuousBackupConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseContinuousBackupConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.enabled = reader.bool();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.recoveryWindowDays = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.encryptionConfig = EncryptionConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ContinuousBackupConfig {
    return {
      enabled: isSet(object.enabled) ? globalThis.Boolean(object.enabled) : undefined,
      recoveryWindowDays: isSet(object.recoveryWindowDays) ? globalThis.Number(object.recoveryWindowDays) : 0,
      encryptionConfig: isSet(object.encryptionConfig) ? EncryptionConfig.fromJSON(object.encryptionConfig) : undefined,
    };
  },

  toJSON(message: ContinuousBackupConfig): unknown {
    const obj: any = {};
    if (message.enabled !== undefined) {
      obj.enabled = message.enabled;
    }
    if (message.recoveryWindowDays !== 0) {
      obj.recoveryWindowDays = Math.round(message.recoveryWindowDays);
    }
    if (message.encryptionConfig !== undefined) {
      obj.encryptionConfig = EncryptionConfig.toJSON(message.encryptionConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<ContinuousBackupConfig>): ContinuousBackupConfig {
    return ContinuousBackupConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ContinuousBackupConfig>): ContinuousBackupConfig {
    const message = createBaseContinuousBackupConfig();
    message.enabled = object.enabled ?? undefined;
    message.recoveryWindowDays = object.recoveryWindowDays ?? 0;
    message.encryptionConfig = (object.encryptionConfig !== undefined && object.encryptionConfig !== null)
      ? EncryptionConfig.fromPartial(object.encryptionConfig)
      : undefined;
    return message;
  },
};

function createBaseContinuousBackupInfo(): ContinuousBackupInfo {
  return { encryptionInfo: undefined, enabledTime: undefined, schedule: [], earliestRestorableTime: undefined };
}

export const ContinuousBackupInfo: MessageFns<ContinuousBackupInfo> = {
  encode(message: ContinuousBackupInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.encryptionInfo !== undefined) {
      EncryptionInfo.encode(message.encryptionInfo, writer.uint32(10).fork()).join();
    }
    if (message.enabledTime !== undefined) {
      Timestamp.encode(toTimestamp(message.enabledTime), writer.uint32(18).fork()).join();
    }
    writer.uint32(26).fork();
    for (const v of message.schedule) {
      writer.int32(v);
    }
    writer.join();
    if (message.earliestRestorableTime !== undefined) {
      Timestamp.encode(toTimestamp(message.earliestRestorableTime), writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ContinuousBackupInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseContinuousBackupInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.encryptionInfo = EncryptionInfo.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.enabledTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag === 24) {
            message.schedule.push(reader.int32() as any);

            continue;
          }

          if (tag === 26) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.schedule.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.earliestRestorableTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ContinuousBackupInfo {
    return {
      encryptionInfo: isSet(object.encryptionInfo) ? EncryptionInfo.fromJSON(object.encryptionInfo) : undefined,
      enabledTime: isSet(object.enabledTime) ? fromJsonTimestamp(object.enabledTime) : undefined,
      schedule: globalThis.Array.isArray(object?.schedule) ? object.schedule.map((e: any) => dayOfWeekFromJSON(e)) : [],
      earliestRestorableTime: isSet(object.earliestRestorableTime)
        ? fromJsonTimestamp(object.earliestRestorableTime)
        : undefined,
    };
  },

  toJSON(message: ContinuousBackupInfo): unknown {
    const obj: any = {};
    if (message.encryptionInfo !== undefined) {
      obj.encryptionInfo = EncryptionInfo.toJSON(message.encryptionInfo);
    }
    if (message.enabledTime !== undefined) {
      obj.enabledTime = message.enabledTime.toISOString();
    }
    if (message.schedule?.length) {
      obj.schedule = message.schedule.map((e) => dayOfWeekToJSON(e));
    }
    if (message.earliestRestorableTime !== undefined) {
      obj.earliestRestorableTime = message.earliestRestorableTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<ContinuousBackupInfo>): ContinuousBackupInfo {
    return ContinuousBackupInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ContinuousBackupInfo>): ContinuousBackupInfo {
    const message = createBaseContinuousBackupInfo();
    message.encryptionInfo = (object.encryptionInfo !== undefined && object.encryptionInfo !== null)
      ? EncryptionInfo.fromPartial(object.encryptionInfo)
      : undefined;
    message.enabledTime = object.enabledTime ?? undefined;
    message.schedule = object.schedule?.map((e) => e) || [];
    message.earliestRestorableTime = object.earliestRestorableTime ?? undefined;
    return message;
  },
};

function createBaseBackupSource(): BackupSource {
  return { backupUid: "", backupName: "" };
}

export const BackupSource: MessageFns<BackupSource> = {
  encode(message: BackupSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.backupUid !== "") {
      writer.uint32(18).string(message.backupUid);
    }
    if (message.backupName !== "") {
      writer.uint32(10).string(message.backupName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BackupSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackupSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.backupUid = reader.string();
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.backupName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BackupSource {
    return {
      backupUid: isSet(object.backupUid) ? globalThis.String(object.backupUid) : "",
      backupName: isSet(object.backupName) ? globalThis.String(object.backupName) : "",
    };
  },

  toJSON(message: BackupSource): unknown {
    const obj: any = {};
    if (message.backupUid !== "") {
      obj.backupUid = message.backupUid;
    }
    if (message.backupName !== "") {
      obj.backupName = message.backupName;
    }
    return obj;
  },

  create(base?: DeepPartial<BackupSource>): BackupSource {
    return BackupSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BackupSource>): BackupSource {
    const message = createBaseBackupSource();
    message.backupUid = object.backupUid ?? "";
    message.backupName = object.backupName ?? "";
    return message;
  },
};

function createBaseContinuousBackupSource(): ContinuousBackupSource {
  return { cluster: "", pointInTime: undefined };
}

export const ContinuousBackupSource: MessageFns<ContinuousBackupSource> = {
  encode(message: ContinuousBackupSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.cluster !== "") {
      writer.uint32(10).string(message.cluster);
    }
    if (message.pointInTime !== undefined) {
      Timestamp.encode(toTimestamp(message.pointInTime), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ContinuousBackupSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseContinuousBackupSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.cluster = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.pointInTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ContinuousBackupSource {
    return {
      cluster: isSet(object.cluster) ? globalThis.String(object.cluster) : "",
      pointInTime: isSet(object.pointInTime) ? fromJsonTimestamp(object.pointInTime) : undefined,
    };
  },

  toJSON(message: ContinuousBackupSource): unknown {
    const obj: any = {};
    if (message.cluster !== "") {
      obj.cluster = message.cluster;
    }
    if (message.pointInTime !== undefined) {
      obj.pointInTime = message.pointInTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<ContinuousBackupSource>): ContinuousBackupSource {
    return ContinuousBackupSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ContinuousBackupSource>): ContinuousBackupSource {
    const message = createBaseContinuousBackupSource();
    message.cluster = object.cluster ?? "";
    message.pointInTime = object.pointInTime ?? undefined;
    return message;
  },
};

function createBaseCluster(): Cluster {
  return {
    backupSource: undefined,
    migrationSource: undefined,
    name: "",
    displayName: "",
    uid: "",
    createTime: undefined,
    updateTime: undefined,
    deleteTime: undefined,
    labels: {},
    state: 0,
    clusterType: 0,
    databaseVersion: 0,
    networkConfig: undefined,
    network: "",
    etag: "",
    annotations: {},
    reconciling: false,
    initialUser: undefined,
    automatedBackupPolicy: undefined,
    sslConfig: undefined,
    encryptionConfig: undefined,
    encryptionInfo: undefined,
    continuousBackupConfig: undefined,
    continuousBackupInfo: undefined,
    secondaryConfig: undefined,
    primaryConfig: undefined,
  };
}

export const Cluster: MessageFns<Cluster> = {
  encode(message: Cluster, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.backupSource !== undefined) {
      BackupSource.encode(message.backupSource, writer.uint32(122).fork()).join();
    }
    if (message.migrationSource !== undefined) {
      MigrationSource.encode(message.migrationSource, writer.uint32(130).fork()).join();
    }
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.displayName !== "") {
      writer.uint32(18).string(message.displayName);
    }
    if (message.uid !== "") {
      writer.uint32(26).string(message.uid);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(34).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(42).fork()).join();
    }
    if (message.deleteTime !== undefined) {
      Timestamp.encode(toTimestamp(message.deleteTime), writer.uint32(50).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      Cluster_LabelsEntry.encode({ key: key as any, value }, writer.uint32(58).fork()).join();
    });
    if (message.state !== 0) {
      writer.uint32(64).int32(message.state);
    }
    if (message.clusterType !== 0) {
      writer.uint32(192).int32(message.clusterType);
    }
    if (message.databaseVersion !== 0) {
      writer.uint32(72).int32(message.databaseVersion);
    }
    if (message.networkConfig !== undefined) {
      Cluster_NetworkConfig.encode(message.networkConfig, writer.uint32(234).fork()).join();
    }
    if (message.network !== "") {
      writer.uint32(82).string(message.network);
    }
    if (message.etag !== "") {
      writer.uint32(90).string(message.etag);
    }
    Object.entries(message.annotations).forEach(([key, value]) => {
      Cluster_AnnotationsEntry.encode({ key: key as any, value }, writer.uint32(98).fork()).join();
    });
    if (message.reconciling !== false) {
      writer.uint32(104).bool(message.reconciling);
    }
    if (message.initialUser !== undefined) {
      UserPassword.encode(message.initialUser, writer.uint32(114).fork()).join();
    }
    if (message.automatedBackupPolicy !== undefined) {
      AutomatedBackupPolicy.encode(message.automatedBackupPolicy, writer.uint32(138).fork()).join();
    }
    if (message.sslConfig !== undefined) {
      SslConfig.encode(message.sslConfig, writer.uint32(146).fork()).join();
    }
    if (message.encryptionConfig !== undefined) {
      EncryptionConfig.encode(message.encryptionConfig, writer.uint32(154).fork()).join();
    }
    if (message.encryptionInfo !== undefined) {
      EncryptionInfo.encode(message.encryptionInfo, writer.uint32(162).fork()).join();
    }
    if (message.continuousBackupConfig !== undefined) {
      ContinuousBackupConfig.encode(message.continuousBackupConfig, writer.uint32(218).fork()).join();
    }
    if (message.continuousBackupInfo !== undefined) {
      ContinuousBackupInfo.encode(message.continuousBackupInfo, writer.uint32(226).fork()).join();
    }
    if (message.secondaryConfig !== undefined) {
      Cluster_SecondaryConfig.encode(message.secondaryConfig, writer.uint32(178).fork()).join();
    }
    if (message.primaryConfig !== undefined) {
      Cluster_PrimaryConfig.encode(message.primaryConfig, writer.uint32(186).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 15:
          if (tag !== 122) {
            break;
          }

          message.backupSource = BackupSource.decode(reader, reader.uint32());
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.migrationSource = MigrationSource.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.uid = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.deleteTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          const entry7 = Cluster_LabelsEntry.decode(reader, reader.uint32());
          if (entry7.value !== undefined) {
            message.labels[entry7.key] = entry7.value;
          }
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 24:
          if (tag !== 192) {
            break;
          }

          message.clusterType = reader.int32() as any;
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.databaseVersion = reader.int32() as any;
          continue;
        case 29:
          if (tag !== 234) {
            break;
          }

          message.networkConfig = Cluster_NetworkConfig.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.network = reader.string();
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.etag = reader.string();
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          const entry12 = Cluster_AnnotationsEntry.decode(reader, reader.uint32());
          if (entry12.value !== undefined) {
            message.annotations[entry12.key] = entry12.value;
          }
          continue;
        case 13:
          if (tag !== 104) {
            break;
          }

          message.reconciling = reader.bool();
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.initialUser = UserPassword.decode(reader, reader.uint32());
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.automatedBackupPolicy = AutomatedBackupPolicy.decode(reader, reader.uint32());
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.sslConfig = SslConfig.decode(reader, reader.uint32());
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.encryptionConfig = EncryptionConfig.decode(reader, reader.uint32());
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.encryptionInfo = EncryptionInfo.decode(reader, reader.uint32());
          continue;
        case 27:
          if (tag !== 218) {
            break;
          }

          message.continuousBackupConfig = ContinuousBackupConfig.decode(reader, reader.uint32());
          continue;
        case 28:
          if (tag !== 226) {
            break;
          }

          message.continuousBackupInfo = ContinuousBackupInfo.decode(reader, reader.uint32());
          continue;
        case 22:
          if (tag !== 178) {
            break;
          }

          message.secondaryConfig = Cluster_SecondaryConfig.decode(reader, reader.uint32());
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.primaryConfig = Cluster_PrimaryConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster {
    return {
      backupSource: isSet(object.backupSource) ? BackupSource.fromJSON(object.backupSource) : undefined,
      migrationSource: isSet(object.migrationSource) ? MigrationSource.fromJSON(object.migrationSource) : undefined,
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      uid: isSet(object.uid) ? globalThis.String(object.uid) : "",
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      deleteTime: isSet(object.deleteTime) ? fromJsonTimestamp(object.deleteTime) : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      state: isSet(object.state) ? cluster_StateFromJSON(object.state) : 0,
      clusterType: isSet(object.clusterType) ? cluster_ClusterTypeFromJSON(object.clusterType) : 0,
      databaseVersion: isSet(object.databaseVersion) ? databaseVersionFromJSON(object.databaseVersion) : 0,
      networkConfig: isSet(object.networkConfig) ? Cluster_NetworkConfig.fromJSON(object.networkConfig) : undefined,
      network: isSet(object.network) ? globalThis.String(object.network) : "",
      etag: isSet(object.etag) ? globalThis.String(object.etag) : "",
      annotations: isObject(object.annotations)
        ? Object.entries(object.annotations).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      reconciling: isSet(object.reconciling) ? globalThis.Boolean(object.reconciling) : false,
      initialUser: isSet(object.initialUser) ? UserPassword.fromJSON(object.initialUser) : undefined,
      automatedBackupPolicy: isSet(object.automatedBackupPolicy)
        ? AutomatedBackupPolicy.fromJSON(object.automatedBackupPolicy)
        : undefined,
      sslConfig: isSet(object.sslConfig) ? SslConfig.fromJSON(object.sslConfig) : undefined,
      encryptionConfig: isSet(object.encryptionConfig) ? EncryptionConfig.fromJSON(object.encryptionConfig) : undefined,
      encryptionInfo: isSet(object.encryptionInfo) ? EncryptionInfo.fromJSON(object.encryptionInfo) : undefined,
      continuousBackupConfig: isSet(object.continuousBackupConfig)
        ? ContinuousBackupConfig.fromJSON(object.continuousBackupConfig)
        : undefined,
      continuousBackupInfo: isSet(object.continuousBackupInfo)
        ? ContinuousBackupInfo.fromJSON(object.continuousBackupInfo)
        : undefined,
      secondaryConfig: isSet(object.secondaryConfig)
        ? Cluster_SecondaryConfig.fromJSON(object.secondaryConfig)
        : undefined,
      primaryConfig: isSet(object.primaryConfig) ? Cluster_PrimaryConfig.fromJSON(object.primaryConfig) : undefined,
    };
  },

  toJSON(message: Cluster): unknown {
    const obj: any = {};
    if (message.backupSource !== undefined) {
      obj.backupSource = BackupSource.toJSON(message.backupSource);
    }
    if (message.migrationSource !== undefined) {
      obj.migrationSource = MigrationSource.toJSON(message.migrationSource);
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.uid !== "") {
      obj.uid = message.uid;
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (message.deleteTime !== undefined) {
      obj.deleteTime = message.deleteTime.toISOString();
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.state !== 0) {
      obj.state = cluster_StateToJSON(message.state);
    }
    if (message.clusterType !== 0) {
      obj.clusterType = cluster_ClusterTypeToJSON(message.clusterType);
    }
    if (message.databaseVersion !== 0) {
      obj.databaseVersion = databaseVersionToJSON(message.databaseVersion);
    }
    if (message.networkConfig !== undefined) {
      obj.networkConfig = Cluster_NetworkConfig.toJSON(message.networkConfig);
    }
    if (message.network !== "") {
      obj.network = message.network;
    }
    if (message.etag !== "") {
      obj.etag = message.etag;
    }
    if (message.annotations) {
      const entries = Object.entries(message.annotations);
      if (entries.length > 0) {
        obj.annotations = {};
        entries.forEach(([k, v]) => {
          obj.annotations[k] = v;
        });
      }
    }
    if (message.reconciling !== false) {
      obj.reconciling = message.reconciling;
    }
    if (message.initialUser !== undefined) {
      obj.initialUser = UserPassword.toJSON(message.initialUser);
    }
    if (message.automatedBackupPolicy !== undefined) {
      obj.automatedBackupPolicy = AutomatedBackupPolicy.toJSON(message.automatedBackupPolicy);
    }
    if (message.sslConfig !== undefined) {
      obj.sslConfig = SslConfig.toJSON(message.sslConfig);
    }
    if (message.encryptionConfig !== undefined) {
      obj.encryptionConfig = EncryptionConfig.toJSON(message.encryptionConfig);
    }
    if (message.encryptionInfo !== undefined) {
      obj.encryptionInfo = EncryptionInfo.toJSON(message.encryptionInfo);
    }
    if (message.continuousBackupConfig !== undefined) {
      obj.continuousBackupConfig = ContinuousBackupConfig.toJSON(message.continuousBackupConfig);
    }
    if (message.continuousBackupInfo !== undefined) {
      obj.continuousBackupInfo = ContinuousBackupInfo.toJSON(message.continuousBackupInfo);
    }
    if (message.secondaryConfig !== undefined) {
      obj.secondaryConfig = Cluster_SecondaryConfig.toJSON(message.secondaryConfig);
    }
    if (message.primaryConfig !== undefined) {
      obj.primaryConfig = Cluster_PrimaryConfig.toJSON(message.primaryConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster>): Cluster {
    return Cluster.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster>): Cluster {
    const message = createBaseCluster();
    message.backupSource = (object.backupSource !== undefined && object.backupSource !== null)
      ? BackupSource.fromPartial(object.backupSource)
      : undefined;
    message.migrationSource = (object.migrationSource !== undefined && object.migrationSource !== null)
      ? MigrationSource.fromPartial(object.migrationSource)
      : undefined;
    message.name = object.name ?? "";
    message.displayName = object.displayName ?? "";
    message.uid = object.uid ?? "";
    message.createTime = object.createTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.deleteTime = object.deleteTime ?? undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.state = object.state ?? 0;
    message.clusterType = object.clusterType ?? 0;
    message.databaseVersion = object.databaseVersion ?? 0;
    message.networkConfig = (object.networkConfig !== undefined && object.networkConfig !== null)
      ? Cluster_NetworkConfig.fromPartial(object.networkConfig)
      : undefined;
    message.network = object.network ?? "";
    message.etag = object.etag ?? "";
    message.annotations = Object.entries(object.annotations ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.reconciling = object.reconciling ?? false;
    message.initialUser = (object.initialUser !== undefined && object.initialUser !== null)
      ? UserPassword.fromPartial(object.initialUser)
      : undefined;
    message.automatedBackupPolicy =
      (object.automatedBackupPolicy !== undefined && object.automatedBackupPolicy !== null)
        ? AutomatedBackupPolicy.fromPartial(object.automatedBackupPolicy)
        : undefined;
    message.sslConfig = (object.sslConfig !== undefined && object.sslConfig !== null)
      ? SslConfig.fromPartial(object.sslConfig)
      : undefined;
    message.encryptionConfig = (object.encryptionConfig !== undefined && object.encryptionConfig !== null)
      ? EncryptionConfig.fromPartial(object.encryptionConfig)
      : undefined;
    message.encryptionInfo = (object.encryptionInfo !== undefined && object.encryptionInfo !== null)
      ? EncryptionInfo.fromPartial(object.encryptionInfo)
      : undefined;
    message.continuousBackupConfig =
      (object.continuousBackupConfig !== undefined && object.continuousBackupConfig !== null)
        ? ContinuousBackupConfig.fromPartial(object.continuousBackupConfig)
        : undefined;
    message.continuousBackupInfo = (object.continuousBackupInfo !== undefined && object.continuousBackupInfo !== null)
      ? ContinuousBackupInfo.fromPartial(object.continuousBackupInfo)
      : undefined;
    message.secondaryConfig = (object.secondaryConfig !== undefined && object.secondaryConfig !== null)
      ? Cluster_SecondaryConfig.fromPartial(object.secondaryConfig)
      : undefined;
    message.primaryConfig = (object.primaryConfig !== undefined && object.primaryConfig !== null)
      ? Cluster_PrimaryConfig.fromPartial(object.primaryConfig)
      : undefined;
    return message;
  },
};

function createBaseCluster_NetworkConfig(): Cluster_NetworkConfig {
  return { network: "", allocatedIpRange: "" };
}

export const Cluster_NetworkConfig: MessageFns<Cluster_NetworkConfig> = {
  encode(message: Cluster_NetworkConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.network !== "") {
      writer.uint32(10).string(message.network);
    }
    if (message.allocatedIpRange !== "") {
      writer.uint32(18).string(message.allocatedIpRange);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster_NetworkConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster_NetworkConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.network = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.allocatedIpRange = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster_NetworkConfig {
    return {
      network: isSet(object.network) ? globalThis.String(object.network) : "",
      allocatedIpRange: isSet(object.allocatedIpRange) ? globalThis.String(object.allocatedIpRange) : "",
    };
  },

  toJSON(message: Cluster_NetworkConfig): unknown {
    const obj: any = {};
    if (message.network !== "") {
      obj.network = message.network;
    }
    if (message.allocatedIpRange !== "") {
      obj.allocatedIpRange = message.allocatedIpRange;
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster_NetworkConfig>): Cluster_NetworkConfig {
    return Cluster_NetworkConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster_NetworkConfig>): Cluster_NetworkConfig {
    const message = createBaseCluster_NetworkConfig();
    message.network = object.network ?? "";
    message.allocatedIpRange = object.allocatedIpRange ?? "";
    return message;
  },
};

function createBaseCluster_SecondaryConfig(): Cluster_SecondaryConfig {
  return { primaryClusterName: "" };
}

export const Cluster_SecondaryConfig: MessageFns<Cluster_SecondaryConfig> = {
  encode(message: Cluster_SecondaryConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.primaryClusterName !== "") {
      writer.uint32(10).string(message.primaryClusterName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster_SecondaryConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster_SecondaryConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.primaryClusterName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster_SecondaryConfig {
    return { primaryClusterName: isSet(object.primaryClusterName) ? globalThis.String(object.primaryClusterName) : "" };
  },

  toJSON(message: Cluster_SecondaryConfig): unknown {
    const obj: any = {};
    if (message.primaryClusterName !== "") {
      obj.primaryClusterName = message.primaryClusterName;
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster_SecondaryConfig>): Cluster_SecondaryConfig {
    return Cluster_SecondaryConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster_SecondaryConfig>): Cluster_SecondaryConfig {
    const message = createBaseCluster_SecondaryConfig();
    message.primaryClusterName = object.primaryClusterName ?? "";
    return message;
  },
};

function createBaseCluster_PrimaryConfig(): Cluster_PrimaryConfig {
  return { secondaryClusterNames: [] };
}

export const Cluster_PrimaryConfig: MessageFns<Cluster_PrimaryConfig> = {
  encode(message: Cluster_PrimaryConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.secondaryClusterNames) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster_PrimaryConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster_PrimaryConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.secondaryClusterNames.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster_PrimaryConfig {
    return {
      secondaryClusterNames: globalThis.Array.isArray(object?.secondaryClusterNames)
        ? object.secondaryClusterNames.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: Cluster_PrimaryConfig): unknown {
    const obj: any = {};
    if (message.secondaryClusterNames?.length) {
      obj.secondaryClusterNames = message.secondaryClusterNames;
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster_PrimaryConfig>): Cluster_PrimaryConfig {
    return Cluster_PrimaryConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster_PrimaryConfig>): Cluster_PrimaryConfig {
    const message = createBaseCluster_PrimaryConfig();
    message.secondaryClusterNames = object.secondaryClusterNames?.map((e) => e) || [];
    return message;
  },
};

function createBaseCluster_LabelsEntry(): Cluster_LabelsEntry {
  return { key: "", value: "" };
}

export const Cluster_LabelsEntry: MessageFns<Cluster_LabelsEntry> = {
  encode(message: Cluster_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Cluster_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster_LabelsEntry>): Cluster_LabelsEntry {
    return Cluster_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster_LabelsEntry>): Cluster_LabelsEntry {
    const message = createBaseCluster_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseCluster_AnnotationsEntry(): Cluster_AnnotationsEntry {
  return { key: "", value: "" };
}

export const Cluster_AnnotationsEntry: MessageFns<Cluster_AnnotationsEntry> = {
  encode(message: Cluster_AnnotationsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster_AnnotationsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster_AnnotationsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster_AnnotationsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Cluster_AnnotationsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster_AnnotationsEntry>): Cluster_AnnotationsEntry {
    return Cluster_AnnotationsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster_AnnotationsEntry>): Cluster_AnnotationsEntry {
    const message = createBaseCluster_AnnotationsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseInstance(): Instance {
  return {
    name: "",
    displayName: "",
    uid: "",
    createTime: undefined,
    updateTime: undefined,
    deleteTime: undefined,
    labels: {},
    state: 0,
    instanceType: 0,
    machineConfig: undefined,
    availabilityType: 0,
    gceZone: "",
    databaseFlags: {},
    writableNode: undefined,
    nodes: [],
    queryInsightsConfig: undefined,
    readPoolConfig: undefined,
    ipAddress: "",
    reconciling: false,
    etag: "",
    annotations: {},
    clientConnectionConfig: undefined,
  };
}

export const Instance: MessageFns<Instance> = {
  encode(message: Instance, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.displayName !== "") {
      writer.uint32(18).string(message.displayName);
    }
    if (message.uid !== "") {
      writer.uint32(26).string(message.uid);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(34).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(42).fork()).join();
    }
    if (message.deleteTime !== undefined) {
      Timestamp.encode(toTimestamp(message.deleteTime), writer.uint32(50).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      Instance_LabelsEntry.encode({ key: key as any, value }, writer.uint32(58).fork()).join();
    });
    if (message.state !== 0) {
      writer.uint32(64).int32(message.state);
    }
    if (message.instanceType !== 0) {
      writer.uint32(72).int32(message.instanceType);
    }
    if (message.machineConfig !== undefined) {
      Instance_MachineConfig.encode(message.machineConfig, writer.uint32(82).fork()).join();
    }
    if (message.availabilityType !== 0) {
      writer.uint32(88).int32(message.availabilityType);
    }
    if (message.gceZone !== "") {
      writer.uint32(98).string(message.gceZone);
    }
    Object.entries(message.databaseFlags).forEach(([key, value]) => {
      Instance_DatabaseFlagsEntry.encode({ key: key as any, value }, writer.uint32(106).fork()).join();
    });
    if (message.writableNode !== undefined) {
      Instance_Node.encode(message.writableNode, writer.uint32(154).fork()).join();
    }
    for (const v of message.nodes) {
      Instance_Node.encode(v!, writer.uint32(162).fork()).join();
    }
    if (message.queryInsightsConfig !== undefined) {
      Instance_QueryInsightsInstanceConfig.encode(message.queryInsightsConfig, writer.uint32(170).fork()).join();
    }
    if (message.readPoolConfig !== undefined) {
      Instance_ReadPoolConfig.encode(message.readPoolConfig, writer.uint32(114).fork()).join();
    }
    if (message.ipAddress !== "") {
      writer.uint32(122).string(message.ipAddress);
    }
    if (message.reconciling !== false) {
      writer.uint32(128).bool(message.reconciling);
    }
    if (message.etag !== "") {
      writer.uint32(138).string(message.etag);
    }
    Object.entries(message.annotations).forEach(([key, value]) => {
      Instance_AnnotationsEntry.encode({ key: key as any, value }, writer.uint32(146).fork()).join();
    });
    if (message.clientConnectionConfig !== undefined) {
      Instance_ClientConnectionConfig.encode(message.clientConnectionConfig, writer.uint32(186).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Instance {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstance();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.uid = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.deleteTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          const entry7 = Instance_LabelsEntry.decode(reader, reader.uint32());
          if (entry7.value !== undefined) {
            message.labels[entry7.key] = entry7.value;
          }
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.instanceType = reader.int32() as any;
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.machineConfig = Instance_MachineConfig.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.availabilityType = reader.int32() as any;
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.gceZone = reader.string();
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          const entry13 = Instance_DatabaseFlagsEntry.decode(reader, reader.uint32());
          if (entry13.value !== undefined) {
            message.databaseFlags[entry13.key] = entry13.value;
          }
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.writableNode = Instance_Node.decode(reader, reader.uint32());
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.nodes.push(Instance_Node.decode(reader, reader.uint32()));
          continue;
        case 21:
          if (tag !== 170) {
            break;
          }

          message.queryInsightsConfig = Instance_QueryInsightsInstanceConfig.decode(reader, reader.uint32());
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.readPoolConfig = Instance_ReadPoolConfig.decode(reader, reader.uint32());
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.ipAddress = reader.string();
          continue;
        case 16:
          if (tag !== 128) {
            break;
          }

          message.reconciling = reader.bool();
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.etag = reader.string();
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          const entry18 = Instance_AnnotationsEntry.decode(reader, reader.uint32());
          if (entry18.value !== undefined) {
            message.annotations[entry18.key] = entry18.value;
          }
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.clientConnectionConfig = Instance_ClientConnectionConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Instance {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      uid: isSet(object.uid) ? globalThis.String(object.uid) : "",
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      deleteTime: isSet(object.deleteTime) ? fromJsonTimestamp(object.deleteTime) : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      state: isSet(object.state) ? instance_StateFromJSON(object.state) : 0,
      instanceType: isSet(object.instanceType) ? instance_InstanceTypeFromJSON(object.instanceType) : 0,
      machineConfig: isSet(object.machineConfig) ? Instance_MachineConfig.fromJSON(object.machineConfig) : undefined,
      availabilityType: isSet(object.availabilityType) ? instance_AvailabilityTypeFromJSON(object.availabilityType) : 0,
      gceZone: isSet(object.gceZone) ? globalThis.String(object.gceZone) : "",
      databaseFlags: isObject(object.databaseFlags)
        ? Object.entries(object.databaseFlags).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      writableNode: isSet(object.writableNode) ? Instance_Node.fromJSON(object.writableNode) : undefined,
      nodes: globalThis.Array.isArray(object?.nodes) ? object.nodes.map((e: any) => Instance_Node.fromJSON(e)) : [],
      queryInsightsConfig: isSet(object.queryInsightsConfig)
        ? Instance_QueryInsightsInstanceConfig.fromJSON(object.queryInsightsConfig)
        : undefined,
      readPoolConfig: isSet(object.readPoolConfig)
        ? Instance_ReadPoolConfig.fromJSON(object.readPoolConfig)
        : undefined,
      ipAddress: isSet(object.ipAddress) ? globalThis.String(object.ipAddress) : "",
      reconciling: isSet(object.reconciling) ? globalThis.Boolean(object.reconciling) : false,
      etag: isSet(object.etag) ? globalThis.String(object.etag) : "",
      annotations: isObject(object.annotations)
        ? Object.entries(object.annotations).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      clientConnectionConfig: isSet(object.clientConnectionConfig)
        ? Instance_ClientConnectionConfig.fromJSON(object.clientConnectionConfig)
        : undefined,
    };
  },

  toJSON(message: Instance): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.uid !== "") {
      obj.uid = message.uid;
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (message.deleteTime !== undefined) {
      obj.deleteTime = message.deleteTime.toISOString();
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.state !== 0) {
      obj.state = instance_StateToJSON(message.state);
    }
    if (message.instanceType !== 0) {
      obj.instanceType = instance_InstanceTypeToJSON(message.instanceType);
    }
    if (message.machineConfig !== undefined) {
      obj.machineConfig = Instance_MachineConfig.toJSON(message.machineConfig);
    }
    if (message.availabilityType !== 0) {
      obj.availabilityType = instance_AvailabilityTypeToJSON(message.availabilityType);
    }
    if (message.gceZone !== "") {
      obj.gceZone = message.gceZone;
    }
    if (message.databaseFlags) {
      const entries = Object.entries(message.databaseFlags);
      if (entries.length > 0) {
        obj.databaseFlags = {};
        entries.forEach(([k, v]) => {
          obj.databaseFlags[k] = v;
        });
      }
    }
    if (message.writableNode !== undefined) {
      obj.writableNode = Instance_Node.toJSON(message.writableNode);
    }
    if (message.nodes?.length) {
      obj.nodes = message.nodes.map((e) => Instance_Node.toJSON(e));
    }
    if (message.queryInsightsConfig !== undefined) {
      obj.queryInsightsConfig = Instance_QueryInsightsInstanceConfig.toJSON(message.queryInsightsConfig);
    }
    if (message.readPoolConfig !== undefined) {
      obj.readPoolConfig = Instance_ReadPoolConfig.toJSON(message.readPoolConfig);
    }
    if (message.ipAddress !== "") {
      obj.ipAddress = message.ipAddress;
    }
    if (message.reconciling !== false) {
      obj.reconciling = message.reconciling;
    }
    if (message.etag !== "") {
      obj.etag = message.etag;
    }
    if (message.annotations) {
      const entries = Object.entries(message.annotations);
      if (entries.length > 0) {
        obj.annotations = {};
        entries.forEach(([k, v]) => {
          obj.annotations[k] = v;
        });
      }
    }
    if (message.clientConnectionConfig !== undefined) {
      obj.clientConnectionConfig = Instance_ClientConnectionConfig.toJSON(message.clientConnectionConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<Instance>): Instance {
    return Instance.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Instance>): Instance {
    const message = createBaseInstance();
    message.name = object.name ?? "";
    message.displayName = object.displayName ?? "";
    message.uid = object.uid ?? "";
    message.createTime = object.createTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.deleteTime = object.deleteTime ?? undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.state = object.state ?? 0;
    message.instanceType = object.instanceType ?? 0;
    message.machineConfig = (object.machineConfig !== undefined && object.machineConfig !== null)
      ? Instance_MachineConfig.fromPartial(object.machineConfig)
      : undefined;
    message.availabilityType = object.availabilityType ?? 0;
    message.gceZone = object.gceZone ?? "";
    message.databaseFlags = Object.entries(object.databaseFlags ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.writableNode = (object.writableNode !== undefined && object.writableNode !== null)
      ? Instance_Node.fromPartial(object.writableNode)
      : undefined;
    message.nodes = object.nodes?.map((e) => Instance_Node.fromPartial(e)) || [];
    message.queryInsightsConfig = (object.queryInsightsConfig !== undefined && object.queryInsightsConfig !== null)
      ? Instance_QueryInsightsInstanceConfig.fromPartial(object.queryInsightsConfig)
      : undefined;
    message.readPoolConfig = (object.readPoolConfig !== undefined && object.readPoolConfig !== null)
      ? Instance_ReadPoolConfig.fromPartial(object.readPoolConfig)
      : undefined;
    message.ipAddress = object.ipAddress ?? "";
    message.reconciling = object.reconciling ?? false;
    message.etag = object.etag ?? "";
    message.annotations = Object.entries(object.annotations ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.clientConnectionConfig =
      (object.clientConnectionConfig !== undefined && object.clientConnectionConfig !== null)
        ? Instance_ClientConnectionConfig.fromPartial(object.clientConnectionConfig)
        : undefined;
    return message;
  },
};

function createBaseInstance_MachineConfig(): Instance_MachineConfig {
  return { cpuCount: 0 };
}

export const Instance_MachineConfig: MessageFns<Instance_MachineConfig> = {
  encode(message: Instance_MachineConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.cpuCount !== 0) {
      writer.uint32(8).int32(message.cpuCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Instance_MachineConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstance_MachineConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.cpuCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Instance_MachineConfig {
    return { cpuCount: isSet(object.cpuCount) ? globalThis.Number(object.cpuCount) : 0 };
  },

  toJSON(message: Instance_MachineConfig): unknown {
    const obj: any = {};
    if (message.cpuCount !== 0) {
      obj.cpuCount = Math.round(message.cpuCount);
    }
    return obj;
  },

  create(base?: DeepPartial<Instance_MachineConfig>): Instance_MachineConfig {
    return Instance_MachineConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Instance_MachineConfig>): Instance_MachineConfig {
    const message = createBaseInstance_MachineConfig();
    message.cpuCount = object.cpuCount ?? 0;
    return message;
  },
};

function createBaseInstance_Node(): Instance_Node {
  return { zoneId: "", id: "", ip: "", state: "" };
}

export const Instance_Node: MessageFns<Instance_Node> = {
  encode(message: Instance_Node, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.zoneId !== "") {
      writer.uint32(10).string(message.zoneId);
    }
    if (message.id !== "") {
      writer.uint32(18).string(message.id);
    }
    if (message.ip !== "") {
      writer.uint32(26).string(message.ip);
    }
    if (message.state !== "") {
      writer.uint32(34).string(message.state);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Instance_Node {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstance_Node();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.zoneId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.id = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.ip = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.state = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Instance_Node {
    return {
      zoneId: isSet(object.zoneId) ? globalThis.String(object.zoneId) : "",
      id: isSet(object.id) ? globalThis.String(object.id) : "",
      ip: isSet(object.ip) ? globalThis.String(object.ip) : "",
      state: isSet(object.state) ? globalThis.String(object.state) : "",
    };
  },

  toJSON(message: Instance_Node): unknown {
    const obj: any = {};
    if (message.zoneId !== "") {
      obj.zoneId = message.zoneId;
    }
    if (message.id !== "") {
      obj.id = message.id;
    }
    if (message.ip !== "") {
      obj.ip = message.ip;
    }
    if (message.state !== "") {
      obj.state = message.state;
    }
    return obj;
  },

  create(base?: DeepPartial<Instance_Node>): Instance_Node {
    return Instance_Node.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Instance_Node>): Instance_Node {
    const message = createBaseInstance_Node();
    message.zoneId = object.zoneId ?? "";
    message.id = object.id ?? "";
    message.ip = object.ip ?? "";
    message.state = object.state ?? "";
    return message;
  },
};

function createBaseInstance_QueryInsightsInstanceConfig(): Instance_QueryInsightsInstanceConfig {
  return {
    recordApplicationTags: undefined,
    recordClientAddress: undefined,
    queryStringLength: 0,
    queryPlansPerMinute: undefined,
  };
}

export const Instance_QueryInsightsInstanceConfig: MessageFns<Instance_QueryInsightsInstanceConfig> = {
  encode(message: Instance_QueryInsightsInstanceConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.recordApplicationTags !== undefined) {
      writer.uint32(16).bool(message.recordApplicationTags);
    }
    if (message.recordClientAddress !== undefined) {
      writer.uint32(24).bool(message.recordClientAddress);
    }
    if (message.queryStringLength !== 0) {
      writer.uint32(32).uint32(message.queryStringLength);
    }
    if (message.queryPlansPerMinute !== undefined) {
      writer.uint32(40).uint32(message.queryPlansPerMinute);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Instance_QueryInsightsInstanceConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstance_QueryInsightsInstanceConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 16) {
            break;
          }

          message.recordApplicationTags = reader.bool();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.recordClientAddress = reader.bool();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.queryStringLength = reader.uint32();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.queryPlansPerMinute = reader.uint32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Instance_QueryInsightsInstanceConfig {
    return {
      recordApplicationTags: isSet(object.recordApplicationTags)
        ? globalThis.Boolean(object.recordApplicationTags)
        : undefined,
      recordClientAddress: isSet(object.recordClientAddress)
        ? globalThis.Boolean(object.recordClientAddress)
        : undefined,
      queryStringLength: isSet(object.queryStringLength) ? globalThis.Number(object.queryStringLength) : 0,
      queryPlansPerMinute: isSet(object.queryPlansPerMinute)
        ? globalThis.Number(object.queryPlansPerMinute)
        : undefined,
    };
  },

  toJSON(message: Instance_QueryInsightsInstanceConfig): unknown {
    const obj: any = {};
    if (message.recordApplicationTags !== undefined) {
      obj.recordApplicationTags = message.recordApplicationTags;
    }
    if (message.recordClientAddress !== undefined) {
      obj.recordClientAddress = message.recordClientAddress;
    }
    if (message.queryStringLength !== 0) {
      obj.queryStringLength = Math.round(message.queryStringLength);
    }
    if (message.queryPlansPerMinute !== undefined) {
      obj.queryPlansPerMinute = Math.round(message.queryPlansPerMinute);
    }
    return obj;
  },

  create(base?: DeepPartial<Instance_QueryInsightsInstanceConfig>): Instance_QueryInsightsInstanceConfig {
    return Instance_QueryInsightsInstanceConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Instance_QueryInsightsInstanceConfig>): Instance_QueryInsightsInstanceConfig {
    const message = createBaseInstance_QueryInsightsInstanceConfig();
    message.recordApplicationTags = object.recordApplicationTags ?? undefined;
    message.recordClientAddress = object.recordClientAddress ?? undefined;
    message.queryStringLength = object.queryStringLength ?? 0;
    message.queryPlansPerMinute = object.queryPlansPerMinute ?? undefined;
    return message;
  },
};

function createBaseInstance_ReadPoolConfig(): Instance_ReadPoolConfig {
  return { nodeCount: 0 };
}

export const Instance_ReadPoolConfig: MessageFns<Instance_ReadPoolConfig> = {
  encode(message: Instance_ReadPoolConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.nodeCount !== 0) {
      writer.uint32(8).int32(message.nodeCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Instance_ReadPoolConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstance_ReadPoolConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.nodeCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Instance_ReadPoolConfig {
    return { nodeCount: isSet(object.nodeCount) ? globalThis.Number(object.nodeCount) : 0 };
  },

  toJSON(message: Instance_ReadPoolConfig): unknown {
    const obj: any = {};
    if (message.nodeCount !== 0) {
      obj.nodeCount = Math.round(message.nodeCount);
    }
    return obj;
  },

  create(base?: DeepPartial<Instance_ReadPoolConfig>): Instance_ReadPoolConfig {
    return Instance_ReadPoolConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Instance_ReadPoolConfig>): Instance_ReadPoolConfig {
    const message = createBaseInstance_ReadPoolConfig();
    message.nodeCount = object.nodeCount ?? 0;
    return message;
  },
};

function createBaseInstance_ClientConnectionConfig(): Instance_ClientConnectionConfig {
  return { requireConnectors: false, sslConfig: undefined };
}

export const Instance_ClientConnectionConfig: MessageFns<Instance_ClientConnectionConfig> = {
  encode(message: Instance_ClientConnectionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.requireConnectors !== false) {
      writer.uint32(8).bool(message.requireConnectors);
    }
    if (message.sslConfig !== undefined) {
      SslConfig.encode(message.sslConfig, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Instance_ClientConnectionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstance_ClientConnectionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.requireConnectors = reader.bool();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.sslConfig = SslConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Instance_ClientConnectionConfig {
    return {
      requireConnectors: isSet(object.requireConnectors) ? globalThis.Boolean(object.requireConnectors) : false,
      sslConfig: isSet(object.sslConfig) ? SslConfig.fromJSON(object.sslConfig) : undefined,
    };
  },

  toJSON(message: Instance_ClientConnectionConfig): unknown {
    const obj: any = {};
    if (message.requireConnectors !== false) {
      obj.requireConnectors = message.requireConnectors;
    }
    if (message.sslConfig !== undefined) {
      obj.sslConfig = SslConfig.toJSON(message.sslConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<Instance_ClientConnectionConfig>): Instance_ClientConnectionConfig {
    return Instance_ClientConnectionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Instance_ClientConnectionConfig>): Instance_ClientConnectionConfig {
    const message = createBaseInstance_ClientConnectionConfig();
    message.requireConnectors = object.requireConnectors ?? false;
    message.sslConfig = (object.sslConfig !== undefined && object.sslConfig !== null)
      ? SslConfig.fromPartial(object.sslConfig)
      : undefined;
    return message;
  },
};

function createBaseInstance_LabelsEntry(): Instance_LabelsEntry {
  return { key: "", value: "" };
}

export const Instance_LabelsEntry: MessageFns<Instance_LabelsEntry> = {
  encode(message: Instance_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Instance_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstance_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Instance_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Instance_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Instance_LabelsEntry>): Instance_LabelsEntry {
    return Instance_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Instance_LabelsEntry>): Instance_LabelsEntry {
    const message = createBaseInstance_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseInstance_DatabaseFlagsEntry(): Instance_DatabaseFlagsEntry {
  return { key: "", value: "" };
}

export const Instance_DatabaseFlagsEntry: MessageFns<Instance_DatabaseFlagsEntry> = {
  encode(message: Instance_DatabaseFlagsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Instance_DatabaseFlagsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstance_DatabaseFlagsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Instance_DatabaseFlagsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Instance_DatabaseFlagsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Instance_DatabaseFlagsEntry>): Instance_DatabaseFlagsEntry {
    return Instance_DatabaseFlagsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Instance_DatabaseFlagsEntry>): Instance_DatabaseFlagsEntry {
    const message = createBaseInstance_DatabaseFlagsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseInstance_AnnotationsEntry(): Instance_AnnotationsEntry {
  return { key: "", value: "" };
}

export const Instance_AnnotationsEntry: MessageFns<Instance_AnnotationsEntry> = {
  encode(message: Instance_AnnotationsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Instance_AnnotationsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstance_AnnotationsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Instance_AnnotationsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Instance_AnnotationsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Instance_AnnotationsEntry>): Instance_AnnotationsEntry {
    return Instance_AnnotationsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Instance_AnnotationsEntry>): Instance_AnnotationsEntry {
    const message = createBaseInstance_AnnotationsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseConnectionInfo(): ConnectionInfo {
  return { name: "", ipAddress: "", instanceUid: "" };
}

export const ConnectionInfo: MessageFns<ConnectionInfo> = {
  encode(message: ConnectionInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.ipAddress !== "") {
      writer.uint32(18).string(message.ipAddress);
    }
    if (message.instanceUid !== "") {
      writer.uint32(34).string(message.instanceUid);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ConnectionInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseConnectionInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.ipAddress = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.instanceUid = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ConnectionInfo {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      ipAddress: isSet(object.ipAddress) ? globalThis.String(object.ipAddress) : "",
      instanceUid: isSet(object.instanceUid) ? globalThis.String(object.instanceUid) : "",
    };
  },

  toJSON(message: ConnectionInfo): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.ipAddress !== "") {
      obj.ipAddress = message.ipAddress;
    }
    if (message.instanceUid !== "") {
      obj.instanceUid = message.instanceUid;
    }
    return obj;
  },

  create(base?: DeepPartial<ConnectionInfo>): ConnectionInfo {
    return ConnectionInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ConnectionInfo>): ConnectionInfo {
    const message = createBaseConnectionInfo();
    message.name = object.name ?? "";
    message.ipAddress = object.ipAddress ?? "";
    message.instanceUid = object.instanceUid ?? "";
    return message;
  },
};

function createBaseBackup(): Backup {
  return {
    name: "",
    displayName: "",
    uid: "",
    createTime: undefined,
    updateTime: undefined,
    deleteTime: undefined,
    labels: {},
    state: 0,
    type: 0,
    description: "",
    clusterUid: "",
    clusterName: "",
    reconciling: false,
    encryptionConfig: undefined,
    encryptionInfo: undefined,
    etag: "",
    annotations: {},
    sizeBytes: Long.ZERO,
    expiryTime: undefined,
    expiryQuantity: undefined,
    databaseVersion: 0,
  };
}

export const Backup: MessageFns<Backup> = {
  encode(message: Backup, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.displayName !== "") {
      writer.uint32(18).string(message.displayName);
    }
    if (message.uid !== "") {
      writer.uint32(26).string(message.uid);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(34).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(42).fork()).join();
    }
    if (message.deleteTime !== undefined) {
      Timestamp.encode(toTimestamp(message.deleteTime), writer.uint32(122).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      Backup_LabelsEntry.encode({ key: key as any, value }, writer.uint32(50).fork()).join();
    });
    if (message.state !== 0) {
      writer.uint32(56).int32(message.state);
    }
    if (message.type !== 0) {
      writer.uint32(64).int32(message.type);
    }
    if (message.description !== "") {
      writer.uint32(74).string(message.description);
    }
    if (message.clusterUid !== "") {
      writer.uint32(146).string(message.clusterUid);
    }
    if (message.clusterName !== "") {
      writer.uint32(82).string(message.clusterName);
    }
    if (message.reconciling !== false) {
      writer.uint32(88).bool(message.reconciling);
    }
    if (message.encryptionConfig !== undefined) {
      EncryptionConfig.encode(message.encryptionConfig, writer.uint32(98).fork()).join();
    }
    if (message.encryptionInfo !== undefined) {
      EncryptionInfo.encode(message.encryptionInfo, writer.uint32(106).fork()).join();
    }
    if (message.etag !== "") {
      writer.uint32(114).string(message.etag);
    }
    Object.entries(message.annotations).forEach(([key, value]) => {
      Backup_AnnotationsEntry.encode({ key: key as any, value }, writer.uint32(130).fork()).join();
    });
    if (!message.sizeBytes.equals(Long.ZERO)) {
      writer.uint32(136).int64(message.sizeBytes.toString());
    }
    if (message.expiryTime !== undefined) {
      Timestamp.encode(toTimestamp(message.expiryTime), writer.uint32(154).fork()).join();
    }
    if (message.expiryQuantity !== undefined) {
      Backup_QuantityBasedExpiry.encode(message.expiryQuantity, writer.uint32(162).fork()).join();
    }
    if (message.databaseVersion !== 0) {
      writer.uint32(176).int32(message.databaseVersion);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Backup {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackup();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.uid = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.deleteTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          const entry6 = Backup_LabelsEntry.decode(reader, reader.uint32());
          if (entry6.value !== undefined) {
            message.labels[entry6.key] = entry6.value;
          }
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.description = reader.string();
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.clusterUid = reader.string();
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.clusterName = reader.string();
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.reconciling = reader.bool();
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.encryptionConfig = EncryptionConfig.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.encryptionInfo = EncryptionInfo.decode(reader, reader.uint32());
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.etag = reader.string();
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          const entry16 = Backup_AnnotationsEntry.decode(reader, reader.uint32());
          if (entry16.value !== undefined) {
            message.annotations[entry16.key] = entry16.value;
          }
          continue;
        case 17:
          if (tag !== 136) {
            break;
          }

          message.sizeBytes = Long.fromString(reader.int64().toString());
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.expiryTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.expiryQuantity = Backup_QuantityBasedExpiry.decode(reader, reader.uint32());
          continue;
        case 22:
          if (tag !== 176) {
            break;
          }

          message.databaseVersion = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Backup {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      uid: isSet(object.uid) ? globalThis.String(object.uid) : "",
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      deleteTime: isSet(object.deleteTime) ? fromJsonTimestamp(object.deleteTime) : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      state: isSet(object.state) ? backup_StateFromJSON(object.state) : 0,
      type: isSet(object.type) ? backup_TypeFromJSON(object.type) : 0,
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      clusterUid: isSet(object.clusterUid) ? globalThis.String(object.clusterUid) : "",
      clusterName: isSet(object.clusterName) ? globalThis.String(object.clusterName) : "",
      reconciling: isSet(object.reconciling) ? globalThis.Boolean(object.reconciling) : false,
      encryptionConfig: isSet(object.encryptionConfig) ? EncryptionConfig.fromJSON(object.encryptionConfig) : undefined,
      encryptionInfo: isSet(object.encryptionInfo) ? EncryptionInfo.fromJSON(object.encryptionInfo) : undefined,
      etag: isSet(object.etag) ? globalThis.String(object.etag) : "",
      annotations: isObject(object.annotations)
        ? Object.entries(object.annotations).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      sizeBytes: isSet(object.sizeBytes) ? Long.fromValue(object.sizeBytes) : Long.ZERO,
      expiryTime: isSet(object.expiryTime) ? fromJsonTimestamp(object.expiryTime) : undefined,
      expiryQuantity: isSet(object.expiryQuantity)
        ? Backup_QuantityBasedExpiry.fromJSON(object.expiryQuantity)
        : undefined,
      databaseVersion: isSet(object.databaseVersion) ? databaseVersionFromJSON(object.databaseVersion) : 0,
    };
  },

  toJSON(message: Backup): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.uid !== "") {
      obj.uid = message.uid;
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (message.deleteTime !== undefined) {
      obj.deleteTime = message.deleteTime.toISOString();
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.state !== 0) {
      obj.state = backup_StateToJSON(message.state);
    }
    if (message.type !== 0) {
      obj.type = backup_TypeToJSON(message.type);
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.clusterUid !== "") {
      obj.clusterUid = message.clusterUid;
    }
    if (message.clusterName !== "") {
      obj.clusterName = message.clusterName;
    }
    if (message.reconciling !== false) {
      obj.reconciling = message.reconciling;
    }
    if (message.encryptionConfig !== undefined) {
      obj.encryptionConfig = EncryptionConfig.toJSON(message.encryptionConfig);
    }
    if (message.encryptionInfo !== undefined) {
      obj.encryptionInfo = EncryptionInfo.toJSON(message.encryptionInfo);
    }
    if (message.etag !== "") {
      obj.etag = message.etag;
    }
    if (message.annotations) {
      const entries = Object.entries(message.annotations);
      if (entries.length > 0) {
        obj.annotations = {};
        entries.forEach(([k, v]) => {
          obj.annotations[k] = v;
        });
      }
    }
    if (!message.sizeBytes.equals(Long.ZERO)) {
      obj.sizeBytes = (message.sizeBytes || Long.ZERO).toString();
    }
    if (message.expiryTime !== undefined) {
      obj.expiryTime = message.expiryTime.toISOString();
    }
    if (message.expiryQuantity !== undefined) {
      obj.expiryQuantity = Backup_QuantityBasedExpiry.toJSON(message.expiryQuantity);
    }
    if (message.databaseVersion !== 0) {
      obj.databaseVersion = databaseVersionToJSON(message.databaseVersion);
    }
    return obj;
  },

  create(base?: DeepPartial<Backup>): Backup {
    return Backup.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Backup>): Backup {
    const message = createBaseBackup();
    message.name = object.name ?? "";
    message.displayName = object.displayName ?? "";
    message.uid = object.uid ?? "";
    message.createTime = object.createTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.deleteTime = object.deleteTime ?? undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.state = object.state ?? 0;
    message.type = object.type ?? 0;
    message.description = object.description ?? "";
    message.clusterUid = object.clusterUid ?? "";
    message.clusterName = object.clusterName ?? "";
    message.reconciling = object.reconciling ?? false;
    message.encryptionConfig = (object.encryptionConfig !== undefined && object.encryptionConfig !== null)
      ? EncryptionConfig.fromPartial(object.encryptionConfig)
      : undefined;
    message.encryptionInfo = (object.encryptionInfo !== undefined && object.encryptionInfo !== null)
      ? EncryptionInfo.fromPartial(object.encryptionInfo)
      : undefined;
    message.etag = object.etag ?? "";
    message.annotations = Object.entries(object.annotations ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.sizeBytes = (object.sizeBytes !== undefined && object.sizeBytes !== null)
      ? Long.fromValue(object.sizeBytes)
      : Long.ZERO;
    message.expiryTime = object.expiryTime ?? undefined;
    message.expiryQuantity = (object.expiryQuantity !== undefined && object.expiryQuantity !== null)
      ? Backup_QuantityBasedExpiry.fromPartial(object.expiryQuantity)
      : undefined;
    message.databaseVersion = object.databaseVersion ?? 0;
    return message;
  },
};

function createBaseBackup_QuantityBasedExpiry(): Backup_QuantityBasedExpiry {
  return { retentionCount: 0, totalRetentionCount: 0 };
}

export const Backup_QuantityBasedExpiry: MessageFns<Backup_QuantityBasedExpiry> = {
  encode(message: Backup_QuantityBasedExpiry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.retentionCount !== 0) {
      writer.uint32(8).int32(message.retentionCount);
    }
    if (message.totalRetentionCount !== 0) {
      writer.uint32(16).int32(message.totalRetentionCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Backup_QuantityBasedExpiry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackup_QuantityBasedExpiry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.retentionCount = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.totalRetentionCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Backup_QuantityBasedExpiry {
    return {
      retentionCount: isSet(object.retentionCount) ? globalThis.Number(object.retentionCount) : 0,
      totalRetentionCount: isSet(object.totalRetentionCount) ? globalThis.Number(object.totalRetentionCount) : 0,
    };
  },

  toJSON(message: Backup_QuantityBasedExpiry): unknown {
    const obj: any = {};
    if (message.retentionCount !== 0) {
      obj.retentionCount = Math.round(message.retentionCount);
    }
    if (message.totalRetentionCount !== 0) {
      obj.totalRetentionCount = Math.round(message.totalRetentionCount);
    }
    return obj;
  },

  create(base?: DeepPartial<Backup_QuantityBasedExpiry>): Backup_QuantityBasedExpiry {
    return Backup_QuantityBasedExpiry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Backup_QuantityBasedExpiry>): Backup_QuantityBasedExpiry {
    const message = createBaseBackup_QuantityBasedExpiry();
    message.retentionCount = object.retentionCount ?? 0;
    message.totalRetentionCount = object.totalRetentionCount ?? 0;
    return message;
  },
};

function createBaseBackup_LabelsEntry(): Backup_LabelsEntry {
  return { key: "", value: "" };
}

export const Backup_LabelsEntry: MessageFns<Backup_LabelsEntry> = {
  encode(message: Backup_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Backup_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackup_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Backup_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Backup_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Backup_LabelsEntry>): Backup_LabelsEntry {
    return Backup_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Backup_LabelsEntry>): Backup_LabelsEntry {
    const message = createBaseBackup_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseBackup_AnnotationsEntry(): Backup_AnnotationsEntry {
  return { key: "", value: "" };
}

export const Backup_AnnotationsEntry: MessageFns<Backup_AnnotationsEntry> = {
  encode(message: Backup_AnnotationsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Backup_AnnotationsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackup_AnnotationsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Backup_AnnotationsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Backup_AnnotationsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Backup_AnnotationsEntry>): Backup_AnnotationsEntry {
    return Backup_AnnotationsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Backup_AnnotationsEntry>): Backup_AnnotationsEntry {
    const message = createBaseBackup_AnnotationsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseSupportedDatabaseFlag(): SupportedDatabaseFlag {
  return {
    stringRestrictions: undefined,
    integerRestrictions: undefined,
    name: "",
    flagName: "",
    valueType: 0,
    acceptsMultipleValues: false,
    supportedDbVersions: [],
    requiresDbRestart: false,
  };
}

export const SupportedDatabaseFlag: MessageFns<SupportedDatabaseFlag> = {
  encode(message: SupportedDatabaseFlag, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.stringRestrictions !== undefined) {
      SupportedDatabaseFlag_StringRestrictions.encode(message.stringRestrictions, writer.uint32(58).fork()).join();
    }
    if (message.integerRestrictions !== undefined) {
      SupportedDatabaseFlag_IntegerRestrictions.encode(message.integerRestrictions, writer.uint32(66).fork()).join();
    }
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.flagName !== "") {
      writer.uint32(18).string(message.flagName);
    }
    if (message.valueType !== 0) {
      writer.uint32(24).int32(message.valueType);
    }
    if (message.acceptsMultipleValues !== false) {
      writer.uint32(32).bool(message.acceptsMultipleValues);
    }
    writer.uint32(42).fork();
    for (const v of message.supportedDbVersions) {
      writer.int32(v);
    }
    writer.join();
    if (message.requiresDbRestart !== false) {
      writer.uint32(48).bool(message.requiresDbRestart);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SupportedDatabaseFlag {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSupportedDatabaseFlag();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 7:
          if (tag !== 58) {
            break;
          }

          message.stringRestrictions = SupportedDatabaseFlag_StringRestrictions.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.integerRestrictions = SupportedDatabaseFlag_IntegerRestrictions.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.flagName = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.valueType = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.acceptsMultipleValues = reader.bool();
          continue;
        case 5:
          if (tag === 40) {
            message.supportedDbVersions.push(reader.int32() as any);

            continue;
          }

          if (tag === 42) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.supportedDbVersions.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.requiresDbRestart = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SupportedDatabaseFlag {
    return {
      stringRestrictions: isSet(object.stringRestrictions)
        ? SupportedDatabaseFlag_StringRestrictions.fromJSON(object.stringRestrictions)
        : undefined,
      integerRestrictions: isSet(object.integerRestrictions)
        ? SupportedDatabaseFlag_IntegerRestrictions.fromJSON(object.integerRestrictions)
        : undefined,
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      flagName: isSet(object.flagName) ? globalThis.String(object.flagName) : "",
      valueType: isSet(object.valueType) ? supportedDatabaseFlag_ValueTypeFromJSON(object.valueType) : 0,
      acceptsMultipleValues: isSet(object.acceptsMultipleValues)
        ? globalThis.Boolean(object.acceptsMultipleValues)
        : false,
      supportedDbVersions: globalThis.Array.isArray(object?.supportedDbVersions)
        ? object.supportedDbVersions.map((e: any) => databaseVersionFromJSON(e))
        : [],
      requiresDbRestart: isSet(object.requiresDbRestart) ? globalThis.Boolean(object.requiresDbRestart) : false,
    };
  },

  toJSON(message: SupportedDatabaseFlag): unknown {
    const obj: any = {};
    if (message.stringRestrictions !== undefined) {
      obj.stringRestrictions = SupportedDatabaseFlag_StringRestrictions.toJSON(message.stringRestrictions);
    }
    if (message.integerRestrictions !== undefined) {
      obj.integerRestrictions = SupportedDatabaseFlag_IntegerRestrictions.toJSON(message.integerRestrictions);
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.flagName !== "") {
      obj.flagName = message.flagName;
    }
    if (message.valueType !== 0) {
      obj.valueType = supportedDatabaseFlag_ValueTypeToJSON(message.valueType);
    }
    if (message.acceptsMultipleValues !== false) {
      obj.acceptsMultipleValues = message.acceptsMultipleValues;
    }
    if (message.supportedDbVersions?.length) {
      obj.supportedDbVersions = message.supportedDbVersions.map((e) => databaseVersionToJSON(e));
    }
    if (message.requiresDbRestart !== false) {
      obj.requiresDbRestart = message.requiresDbRestart;
    }
    return obj;
  },

  create(base?: DeepPartial<SupportedDatabaseFlag>): SupportedDatabaseFlag {
    return SupportedDatabaseFlag.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SupportedDatabaseFlag>): SupportedDatabaseFlag {
    const message = createBaseSupportedDatabaseFlag();
    message.stringRestrictions = (object.stringRestrictions !== undefined && object.stringRestrictions !== null)
      ? SupportedDatabaseFlag_StringRestrictions.fromPartial(object.stringRestrictions)
      : undefined;
    message.integerRestrictions = (object.integerRestrictions !== undefined && object.integerRestrictions !== null)
      ? SupportedDatabaseFlag_IntegerRestrictions.fromPartial(object.integerRestrictions)
      : undefined;
    message.name = object.name ?? "";
    message.flagName = object.flagName ?? "";
    message.valueType = object.valueType ?? 0;
    message.acceptsMultipleValues = object.acceptsMultipleValues ?? false;
    message.supportedDbVersions = object.supportedDbVersions?.map((e) => e) || [];
    message.requiresDbRestart = object.requiresDbRestart ?? false;
    return message;
  },
};

function createBaseSupportedDatabaseFlag_StringRestrictions(): SupportedDatabaseFlag_StringRestrictions {
  return { allowedValues: [] };
}

export const SupportedDatabaseFlag_StringRestrictions: MessageFns<SupportedDatabaseFlag_StringRestrictions> = {
  encode(message: SupportedDatabaseFlag_StringRestrictions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.allowedValues) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SupportedDatabaseFlag_StringRestrictions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSupportedDatabaseFlag_StringRestrictions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.allowedValues.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SupportedDatabaseFlag_StringRestrictions {
    return {
      allowedValues: globalThis.Array.isArray(object?.allowedValues)
        ? object.allowedValues.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: SupportedDatabaseFlag_StringRestrictions): unknown {
    const obj: any = {};
    if (message.allowedValues?.length) {
      obj.allowedValues = message.allowedValues;
    }
    return obj;
  },

  create(base?: DeepPartial<SupportedDatabaseFlag_StringRestrictions>): SupportedDatabaseFlag_StringRestrictions {
    return SupportedDatabaseFlag_StringRestrictions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SupportedDatabaseFlag_StringRestrictions>): SupportedDatabaseFlag_StringRestrictions {
    const message = createBaseSupportedDatabaseFlag_StringRestrictions();
    message.allowedValues = object.allowedValues?.map((e) => e) || [];
    return message;
  },
};

function createBaseSupportedDatabaseFlag_IntegerRestrictions(): SupportedDatabaseFlag_IntegerRestrictions {
  return { minValue: undefined, maxValue: undefined };
}

export const SupportedDatabaseFlag_IntegerRestrictions: MessageFns<SupportedDatabaseFlag_IntegerRestrictions> = {
  encode(message: SupportedDatabaseFlag_IntegerRestrictions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.minValue !== undefined) {
      Int64Value.encode({ value: message.minValue! }, writer.uint32(10).fork()).join();
    }
    if (message.maxValue !== undefined) {
      Int64Value.encode({ value: message.maxValue! }, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SupportedDatabaseFlag_IntegerRestrictions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSupportedDatabaseFlag_IntegerRestrictions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.minValue = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.maxValue = Int64Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SupportedDatabaseFlag_IntegerRestrictions {
    return {
      minValue: isSet(object.minValue) ? Long.fromValue(object.minValue) : undefined,
      maxValue: isSet(object.maxValue) ? Long.fromValue(object.maxValue) : undefined,
    };
  },

  toJSON(message: SupportedDatabaseFlag_IntegerRestrictions): unknown {
    const obj: any = {};
    if (message.minValue !== undefined) {
      obj.minValue = message.minValue;
    }
    if (message.maxValue !== undefined) {
      obj.maxValue = message.maxValue;
    }
    return obj;
  },

  create(base?: DeepPartial<SupportedDatabaseFlag_IntegerRestrictions>): SupportedDatabaseFlag_IntegerRestrictions {
    return SupportedDatabaseFlag_IntegerRestrictions.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<SupportedDatabaseFlag_IntegerRestrictions>,
  ): SupportedDatabaseFlag_IntegerRestrictions {
    const message = createBaseSupportedDatabaseFlag_IntegerRestrictions();
    message.minValue = (object.minValue !== undefined && object.minValue !== null)
      ? Long.fromValue(object.minValue)
      : undefined;
    message.maxValue = (object.maxValue !== undefined && object.maxValue !== null)
      ? Long.fromValue(object.maxValue)
      : undefined;
    return message;
  },
};

function createBaseUser(): User {
  return { name: "", password: "", databaseRoles: [], userType: 0 };
}

export const User: MessageFns<User> = {
  encode(message: User, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.password !== "") {
      writer.uint32(18).string(message.password);
    }
    for (const v of message.databaseRoles) {
      writer.uint32(34).string(v!);
    }
    if (message.userType !== 0) {
      writer.uint32(40).int32(message.userType);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): User {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUser();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.password = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.databaseRoles.push(reader.string());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.userType = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): User {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      password: isSet(object.password) ? globalThis.String(object.password) : "",
      databaseRoles: globalThis.Array.isArray(object?.databaseRoles)
        ? object.databaseRoles.map((e: any) => globalThis.String(e))
        : [],
      userType: isSet(object.userType) ? user_UserTypeFromJSON(object.userType) : 0,
    };
  },

  toJSON(message: User): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.password !== "") {
      obj.password = message.password;
    }
    if (message.databaseRoles?.length) {
      obj.databaseRoles = message.databaseRoles;
    }
    if (message.userType !== 0) {
      obj.userType = user_UserTypeToJSON(message.userType);
    }
    return obj;
  },

  create(base?: DeepPartial<User>): User {
    return User.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<User>): User {
    const message = createBaseUser();
    message.name = object.name ?? "";
    message.password = object.password ?? "";
    message.databaseRoles = object.databaseRoles?.map((e) => e) || [];
    message.userType = object.userType ?? 0;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
