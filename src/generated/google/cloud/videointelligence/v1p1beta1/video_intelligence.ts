// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/videointelligence/v1p1beta1/video_intelligence.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Operation } from "../../../longrunning/operations.js";
import { Duration } from "../../../protobuf/duration.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { Status } from "../../../rpc/status.js";

export const protobufPackage = "google.cloud.videointelligence.v1p1beta1";

/** Video annotation feature. */
export enum Feature {
  /** FEATURE_UNSPECIFIED - Unspecified. */
  FEATURE_UNSPECIFIED = 0,
  /** LABEL_DETECTION - Label detection. Detect objects, such as dog or flower. */
  LABEL_DETECTION = 1,
  /** SHOT_CHANGE_DETECTION - Shot change detection. */
  SHOT_CHANGE_DETECTION = 2,
  /** EXPLICIT_CONTENT_DETECTION - Explicit content detection. */
  EXPLICIT_CONTENT_DETECTION = 3,
  /** SPEECH_TRANSCRIPTION - Speech transcription. */
  SPEECH_TRANSCRIPTION = 6,
  UNRECOGNIZED = -1,
}

export function featureFromJSON(object: any): Feature {
  switch (object) {
    case 0:
    case "FEATURE_UNSPECIFIED":
      return Feature.FEATURE_UNSPECIFIED;
    case 1:
    case "LABEL_DETECTION":
      return Feature.LABEL_DETECTION;
    case 2:
    case "SHOT_CHANGE_DETECTION":
      return Feature.SHOT_CHANGE_DETECTION;
    case 3:
    case "EXPLICIT_CONTENT_DETECTION":
      return Feature.EXPLICIT_CONTENT_DETECTION;
    case 6:
    case "SPEECH_TRANSCRIPTION":
      return Feature.SPEECH_TRANSCRIPTION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Feature.UNRECOGNIZED;
  }
}

export function featureToJSON(object: Feature): string {
  switch (object) {
    case Feature.FEATURE_UNSPECIFIED:
      return "FEATURE_UNSPECIFIED";
    case Feature.LABEL_DETECTION:
      return "LABEL_DETECTION";
    case Feature.SHOT_CHANGE_DETECTION:
      return "SHOT_CHANGE_DETECTION";
    case Feature.EXPLICIT_CONTENT_DETECTION:
      return "EXPLICIT_CONTENT_DETECTION";
    case Feature.SPEECH_TRANSCRIPTION:
      return "SPEECH_TRANSCRIPTION";
    case Feature.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Label detection mode. */
export enum LabelDetectionMode {
  /** LABEL_DETECTION_MODE_UNSPECIFIED - Unspecified. */
  LABEL_DETECTION_MODE_UNSPECIFIED = 0,
  /** SHOT_MODE - Detect shot-level labels. */
  SHOT_MODE = 1,
  /** FRAME_MODE - Detect frame-level labels. */
  FRAME_MODE = 2,
  /** SHOT_AND_FRAME_MODE - Detect both shot-level and frame-level labels. */
  SHOT_AND_FRAME_MODE = 3,
  UNRECOGNIZED = -1,
}

export function labelDetectionModeFromJSON(object: any): LabelDetectionMode {
  switch (object) {
    case 0:
    case "LABEL_DETECTION_MODE_UNSPECIFIED":
      return LabelDetectionMode.LABEL_DETECTION_MODE_UNSPECIFIED;
    case 1:
    case "SHOT_MODE":
      return LabelDetectionMode.SHOT_MODE;
    case 2:
    case "FRAME_MODE":
      return LabelDetectionMode.FRAME_MODE;
    case 3:
    case "SHOT_AND_FRAME_MODE":
      return LabelDetectionMode.SHOT_AND_FRAME_MODE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return LabelDetectionMode.UNRECOGNIZED;
  }
}

export function labelDetectionModeToJSON(object: LabelDetectionMode): string {
  switch (object) {
    case LabelDetectionMode.LABEL_DETECTION_MODE_UNSPECIFIED:
      return "LABEL_DETECTION_MODE_UNSPECIFIED";
    case LabelDetectionMode.SHOT_MODE:
      return "SHOT_MODE";
    case LabelDetectionMode.FRAME_MODE:
      return "FRAME_MODE";
    case LabelDetectionMode.SHOT_AND_FRAME_MODE:
      return "SHOT_AND_FRAME_MODE";
    case LabelDetectionMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Bucketized representation of likelihood. */
export enum Likelihood {
  /** LIKELIHOOD_UNSPECIFIED - Unspecified likelihood. */
  LIKELIHOOD_UNSPECIFIED = 0,
  /** VERY_UNLIKELY - Very unlikely. */
  VERY_UNLIKELY = 1,
  /** UNLIKELY - Unlikely. */
  UNLIKELY = 2,
  /** POSSIBLE - Possible. */
  POSSIBLE = 3,
  /** LIKELY - Likely. */
  LIKELY = 4,
  /** VERY_LIKELY - Very likely. */
  VERY_LIKELY = 5,
  UNRECOGNIZED = -1,
}

export function likelihoodFromJSON(object: any): Likelihood {
  switch (object) {
    case 0:
    case "LIKELIHOOD_UNSPECIFIED":
      return Likelihood.LIKELIHOOD_UNSPECIFIED;
    case 1:
    case "VERY_UNLIKELY":
      return Likelihood.VERY_UNLIKELY;
    case 2:
    case "UNLIKELY":
      return Likelihood.UNLIKELY;
    case 3:
    case "POSSIBLE":
      return Likelihood.POSSIBLE;
    case 4:
    case "LIKELY":
      return Likelihood.LIKELY;
    case 5:
    case "VERY_LIKELY":
      return Likelihood.VERY_LIKELY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Likelihood.UNRECOGNIZED;
  }
}

export function likelihoodToJSON(object: Likelihood): string {
  switch (object) {
    case Likelihood.LIKELIHOOD_UNSPECIFIED:
      return "LIKELIHOOD_UNSPECIFIED";
    case Likelihood.VERY_UNLIKELY:
      return "VERY_UNLIKELY";
    case Likelihood.UNLIKELY:
      return "UNLIKELY";
    case Likelihood.POSSIBLE:
      return "POSSIBLE";
    case Likelihood.LIKELY:
      return "LIKELY";
    case Likelihood.VERY_LIKELY:
      return "VERY_LIKELY";
    case Likelihood.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Video annotation request. */
export interface AnnotateVideoRequest {
  /**
   * Input video location. Currently, only
   * [Google Cloud Storage](https://cloud.google.com/storage/) URIs are
   * supported, which must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
   * more information, see [Request
   * URIs](https://cloud.google.com/storage/docs/request-endpoints). A video URI
   * may include wildcards in `object-id`, and thus identify multiple videos.
   * Supported wildcards: '*' to match 0 or more characters;
   * '?' to match 1 character. If unset, the input video should be embedded
   * in the request as `input_content`. If set, `input_content` should be unset.
   */
  inputUri: string;
  /**
   * The video data bytes.
   * If unset, the input video(s) should be specified via `input_uri`.
   * If set, `input_uri` should be unset.
   */
  inputContent: Buffer;
  /** Required. Requested video annotation features. */
  features: Feature[];
  /** Additional video context and/or feature-specific parameters. */
  videoContext:
    | VideoContext
    | undefined;
  /**
   * Optional. Location where the output (in JSON format) should be stored.
   * Currently, only [Google Cloud Storage](https://cloud.google.com/storage/)
   * URIs are supported, which must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
   * more information, see [Request
   * URIs](https://cloud.google.com/storage/docs/request-endpoints).
   */
  outputUri: string;
  /**
   * Optional. Cloud region where annotation should take place. Supported cloud
   * regions: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no region
   * is specified, a region will be determined based on video file location.
   */
  locationId: string;
}

/** Video context and/or feature-specific parameters. */
export interface VideoContext {
  /**
   * Video segments to annotate. The segments may overlap and are not required
   * to be contiguous or span the whole video. If unspecified, each video is
   * treated as a single segment.
   */
  segments: VideoSegment[];
  /** Config for LABEL_DETECTION. */
  labelDetectionConfig:
    | LabelDetectionConfig
    | undefined;
  /** Config for SHOT_CHANGE_DETECTION. */
  shotChangeDetectionConfig:
    | ShotChangeDetectionConfig
    | undefined;
  /** Config for EXPLICIT_CONTENT_DETECTION. */
  explicitContentDetectionConfig:
    | ExplicitContentDetectionConfig
    | undefined;
  /** Config for SPEECH_TRANSCRIPTION. */
  speechTranscriptionConfig: SpeechTranscriptionConfig | undefined;
}

/** Config for LABEL_DETECTION. */
export interface LabelDetectionConfig {
  /**
   * What labels should be detected with LABEL_DETECTION, in addition to
   * video-level labels or segment-level labels.
   * If unspecified, defaults to `SHOT_MODE`.
   */
  labelDetectionMode: LabelDetectionMode;
  /**
   * Whether the video has been shot from a stationary (i.e. non-moving) camera.
   * When set to true, might improve detection accuracy for moving objects.
   * Should be used with `SHOT_AND_FRAME_MODE` enabled.
   */
  stationaryCamera: boolean;
  /**
   * Model to use for label detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   */
  model: string;
}

/** Config for SHOT_CHANGE_DETECTION. */
export interface ShotChangeDetectionConfig {
  /**
   * Model to use for shot change detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   */
  model: string;
}

/** Config for EXPLICIT_CONTENT_DETECTION. */
export interface ExplicitContentDetectionConfig {
  /**
   * Model to use for explicit content detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   */
  model: string;
}

/** Video segment. */
export interface VideoSegment {
  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the start of the segment (inclusive).
   */
  startTimeOffset:
    | Duration
    | undefined;
  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the end of the segment (inclusive).
   */
  endTimeOffset: Duration | undefined;
}

/** Video segment level annotation results for label detection. */
export interface LabelSegment {
  /** Video segment where a label was detected. */
  segment:
    | VideoSegment
    | undefined;
  /** Confidence that the label is accurate. Range: [0, 1]. */
  confidence: number;
}

/** Video frame level annotation results for label detection. */
export interface LabelFrame {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   */
  timeOffset:
    | Duration
    | undefined;
  /** Confidence that the label is accurate. Range: [0, 1]. */
  confidence: number;
}

/** Detected entity from video analysis. */
export interface Entity {
  /**
   * Opaque entity ID. Some IDs may be available in
   * [Google Knowledge Graph Search
   * API](https://developers.google.com/knowledge-graph/).
   */
  entityId: string;
  /** Textual description, e.g. `Fixed-gear bicycle`. */
  description: string;
  /** Language code for `description` in BCP-47 format. */
  languageCode: string;
}

/** Label annotation. */
export interface LabelAnnotation {
  /** Detected entity. */
  entity:
    | Entity
    | undefined;
  /**
   * Common categories for the detected entity.
   * E.g. when the label is `Terrier` the category is likely `dog`. And in some
   * cases there might be more than one categories e.g. `Terrier` could also be
   * a `pet`.
   */
  categoryEntities: Entity[];
  /** All video segments where a label was detected. */
  segments: LabelSegment[];
  /** All video frames where a label was detected. */
  frames: LabelFrame[];
}

/** Video frame level annotation results for explicit content. */
export interface ExplicitContentFrame {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   */
  timeOffset:
    | Duration
    | undefined;
  /** Likelihood of the pornography content.. */
  pornographyLikelihood: Likelihood;
}

/**
 * Explicit content annotation (based on per-frame visual signals only).
 * If no explicit content has been detected in a frame, no annotations are
 * present for that frame.
 */
export interface ExplicitContentAnnotation {
  /** All video frames where explicit content was detected. */
  frames: ExplicitContentFrame[];
}

/** Annotation results for a single video. */
export interface VideoAnnotationResults {
  /**
   * Output only. Video file location in
   * [Google Cloud Storage](https://cloud.google.com/storage/).
   */
  inputUri: string;
  /**
   * Label annotations on video level or user specified segment level.
   * There is exactly one element for each unique label.
   */
  segmentLabelAnnotations: LabelAnnotation[];
  /**
   * Label annotations on shot level.
   * There is exactly one element for each unique label.
   */
  shotLabelAnnotations: LabelAnnotation[];
  /**
   * Label annotations on frame level.
   * There is exactly one element for each unique label.
   */
  frameLabelAnnotations: LabelAnnotation[];
  /** Shot annotations. Each shot is represented as a video segment. */
  shotAnnotations: VideoSegment[];
  /** Explicit content annotation. */
  explicitAnnotation:
    | ExplicitContentAnnotation
    | undefined;
  /** Speech transcription. */
  speechTranscriptions: SpeechTranscription[];
  /**
   * Output only. If set, indicates an error. Note that for a single
   * `AnnotateVideoRequest` some videos may succeed and some may fail.
   */
  error: Status | undefined;
}

/**
 * Video annotation response. Included in the `response`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 */
export interface AnnotateVideoResponse {
  /** Annotation results for all videos specified in `AnnotateVideoRequest`. */
  annotationResults: VideoAnnotationResults[];
}

/** Annotation progress for a single video. */
export interface VideoAnnotationProgress {
  /**
   * Output only. Video file location in
   * [Google Cloud Storage](https://cloud.google.com/storage/).
   */
  inputUri: string;
  /**
   * Output only. Approximate percentage processed thus far. Guaranteed to be
   * 100 when fully processed.
   */
  progressPercent: number;
  /** Output only. Time when the request was received. */
  startTime:
    | Date
    | undefined;
  /** Output only. Time of the most recent update. */
  updateTime: Date | undefined;
}

/**
 * Video annotation progress. Included in the `metadata`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 */
export interface AnnotateVideoProgress {
  /** Progress metadata for all videos specified in `AnnotateVideoRequest`. */
  annotationProgress: VideoAnnotationProgress[];
}

/** Config for SPEECH_TRANSCRIPTION. */
export interface SpeechTranscriptionConfig {
  /**
   * Required. *Required* The language of the supplied audio as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Example: "en-US".
   * See [Language Support](https://cloud.google.com/speech/docs/languages)
   * for a list of the currently supported language codes.
   */
  languageCode: string;
  /**
   * Optional. Maximum number of recognition hypotheses to be returned.
   * Specifically, the maximum number of `SpeechRecognitionAlternative` messages
   * within each `SpeechTranscription`. The server may return fewer than
   * `max_alternatives`. Valid values are `0`-`30`. A value of `0` or `1` will
   * return a maximum of one. If omitted, will return a maximum of one.
   */
  maxAlternatives: number;
  /**
   * Optional. If set to `true`, the server will attempt to filter out
   * profanities, replacing all but the initial character in each filtered word
   * with asterisks, e.g. "f***". If set to `false` or omitted, profanities
   * won't be filtered out.
   */
  filterProfanity: boolean;
  /** Optional. A means to provide context to assist the speech recognition. */
  speechContexts: SpeechContext[];
  /**
   * Optional. If 'true', adds punctuation to recognition result hypotheses.
   * This feature is only available in select languages. Setting this for
   * requests in other languages has no effect at all. The default 'false' value
   * does not add punctuation to result hypotheses. NOTE: "This is currently
   * offered as an experimental service, complimentary to all users. In the
   * future this may be exclusively available as a premium feature."
   */
  enableAutomaticPunctuation: boolean;
  /**
   * Optional. For file formats, such as MXF or MKV, supporting multiple audio
   * tracks, specify up to two tracks. Default: track 0.
   */
  audioTracks: number[];
}

/**
 * Provides "hints" to the speech recognizer to favor specific words and phrases
 * in the results.
 */
export interface SpeechContext {
  /**
   * Optional. A list of strings containing words and phrases "hints" so that
   * the speech recognition is more likely to recognize them. This can be used
   * to improve the accuracy for specific words and phrases, for example, if
   * specific commands are typically spoken by the user. This can also be used
   * to add additional words to the vocabulary of the recognizer. See
   * [usage limits](https://cloud.google.com/speech/limits#content).
   */
  phrases: string[];
}

/** A speech recognition result corresponding to a portion of the audio. */
export interface SpeechTranscription {
  /**
   * May contain one or more recognition hypotheses (up to the maximum specified
   * in `max_alternatives`).  These alternatives are ordered in terms of
   * accuracy, with the top (first) alternative being the most probable, as
   * ranked by the recognizer.
   */
  alternatives: SpeechRecognitionAlternative[];
}

/** Alternative hypotheses (a.k.a. n-best list). */
export interface SpeechRecognitionAlternative {
  /** Output only. Transcript text representing the words that the user spoke. */
  transcript: string;
  /**
   * Output only. The confidence estimate between 0.0 and 1.0. A higher number
   * indicates an estimated greater likelihood that the recognized words are
   * correct. This field is set only for the top alternative.
   * This field is not guaranteed to be accurate and users should not rely on it
   * to be always provided.
   * The default of 0.0 is a sentinel value indicating `confidence` was not set.
   */
  confidence: number;
  /** Output only. A list of word-specific information for each recognized word. */
  words: WordInfo[];
}

/**
 * Word-specific information for recognized words. Word information is only
 * included in the response when certain request parameters are set, such
 * as `enable_word_time_offsets`.
 */
export interface WordInfo {
  /**
   * Output only. Time offset relative to the beginning of the audio, and
   * corresponding to the start of the spoken word. This field is only set if
   * `enable_word_time_offsets=true` and only in the top hypothesis. This is an
   * experimental feature and the accuracy of the time offset can vary.
   */
  startTime:
    | Duration
    | undefined;
  /**
   * Output only. Time offset relative to the beginning of the audio, and
   * corresponding to the end of the spoken word. This field is only set if
   * `enable_word_time_offsets=true` and only in the top hypothesis. This is an
   * experimental feature and the accuracy of the time offset can vary.
   */
  endTime:
    | Duration
    | undefined;
  /** Output only. The word corresponding to this set of information. */
  word: string;
}

function createBaseAnnotateVideoRequest(): AnnotateVideoRequest {
  return {
    inputUri: "",
    inputContent: Buffer.alloc(0),
    features: [],
    videoContext: undefined,
    outputUri: "",
    locationId: "",
  };
}

export const AnnotateVideoRequest: MessageFns<AnnotateVideoRequest> = {
  encode(message: AnnotateVideoRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputUri !== "") {
      writer.uint32(10).string(message.inputUri);
    }
    if (message.inputContent.length !== 0) {
      writer.uint32(50).bytes(message.inputContent);
    }
    writer.uint32(18).fork();
    for (const v of message.features) {
      writer.int32(v);
    }
    writer.join();
    if (message.videoContext !== undefined) {
      VideoContext.encode(message.videoContext, writer.uint32(26).fork()).join();
    }
    if (message.outputUri !== "") {
      writer.uint32(34).string(message.outputUri);
    }
    if (message.locationId !== "") {
      writer.uint32(42).string(message.locationId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnnotateVideoRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnnotateVideoRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputUri = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.inputContent = Buffer.from(reader.bytes());
          continue;
        case 2:
          if (tag === 16) {
            message.features.push(reader.int32() as any);

            continue;
          }

          if (tag === 18) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.features.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.videoContext = VideoContext.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.outputUri = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.locationId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnnotateVideoRequest {
    return {
      inputUri: isSet(object.inputUri) ? globalThis.String(object.inputUri) : "",
      inputContent: isSet(object.inputContent) ? Buffer.from(bytesFromBase64(object.inputContent)) : Buffer.alloc(0),
      features: globalThis.Array.isArray(object?.features) ? object.features.map((e: any) => featureFromJSON(e)) : [],
      videoContext: isSet(object.videoContext) ? VideoContext.fromJSON(object.videoContext) : undefined,
      outputUri: isSet(object.outputUri) ? globalThis.String(object.outputUri) : "",
      locationId: isSet(object.locationId) ? globalThis.String(object.locationId) : "",
    };
  },

  toJSON(message: AnnotateVideoRequest): unknown {
    const obj: any = {};
    if (message.inputUri !== "") {
      obj.inputUri = message.inputUri;
    }
    if (message.inputContent.length !== 0) {
      obj.inputContent = base64FromBytes(message.inputContent);
    }
    if (message.features?.length) {
      obj.features = message.features.map((e) => featureToJSON(e));
    }
    if (message.videoContext !== undefined) {
      obj.videoContext = VideoContext.toJSON(message.videoContext);
    }
    if (message.outputUri !== "") {
      obj.outputUri = message.outputUri;
    }
    if (message.locationId !== "") {
      obj.locationId = message.locationId;
    }
    return obj;
  },

  create(base?: DeepPartial<AnnotateVideoRequest>): AnnotateVideoRequest {
    return AnnotateVideoRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AnnotateVideoRequest>): AnnotateVideoRequest {
    const message = createBaseAnnotateVideoRequest();
    message.inputUri = object.inputUri ?? "";
    message.inputContent = object.inputContent ?? Buffer.alloc(0);
    message.features = object.features?.map((e) => e) || [];
    message.videoContext = (object.videoContext !== undefined && object.videoContext !== null)
      ? VideoContext.fromPartial(object.videoContext)
      : undefined;
    message.outputUri = object.outputUri ?? "";
    message.locationId = object.locationId ?? "";
    return message;
  },
};

function createBaseVideoContext(): VideoContext {
  return {
    segments: [],
    labelDetectionConfig: undefined,
    shotChangeDetectionConfig: undefined,
    explicitContentDetectionConfig: undefined,
    speechTranscriptionConfig: undefined,
  };
}

export const VideoContext: MessageFns<VideoContext> = {
  encode(message: VideoContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.segments) {
      VideoSegment.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.labelDetectionConfig !== undefined) {
      LabelDetectionConfig.encode(message.labelDetectionConfig, writer.uint32(18).fork()).join();
    }
    if (message.shotChangeDetectionConfig !== undefined) {
      ShotChangeDetectionConfig.encode(message.shotChangeDetectionConfig, writer.uint32(26).fork()).join();
    }
    if (message.explicitContentDetectionConfig !== undefined) {
      ExplicitContentDetectionConfig.encode(message.explicitContentDetectionConfig, writer.uint32(34).fork()).join();
    }
    if (message.speechTranscriptionConfig !== undefined) {
      SpeechTranscriptionConfig.encode(message.speechTranscriptionConfig, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VideoContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVideoContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.segments.push(VideoSegment.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.labelDetectionConfig = LabelDetectionConfig.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.shotChangeDetectionConfig = ShotChangeDetectionConfig.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.explicitContentDetectionConfig = ExplicitContentDetectionConfig.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.speechTranscriptionConfig = SpeechTranscriptionConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VideoContext {
    return {
      segments: globalThis.Array.isArray(object?.segments)
        ? object.segments.map((e: any) => VideoSegment.fromJSON(e))
        : [],
      labelDetectionConfig: isSet(object.labelDetectionConfig)
        ? LabelDetectionConfig.fromJSON(object.labelDetectionConfig)
        : undefined,
      shotChangeDetectionConfig: isSet(object.shotChangeDetectionConfig)
        ? ShotChangeDetectionConfig.fromJSON(object.shotChangeDetectionConfig)
        : undefined,
      explicitContentDetectionConfig: isSet(object.explicitContentDetectionConfig)
        ? ExplicitContentDetectionConfig.fromJSON(object.explicitContentDetectionConfig)
        : undefined,
      speechTranscriptionConfig: isSet(object.speechTranscriptionConfig)
        ? SpeechTranscriptionConfig.fromJSON(object.speechTranscriptionConfig)
        : undefined,
    };
  },

  toJSON(message: VideoContext): unknown {
    const obj: any = {};
    if (message.segments?.length) {
      obj.segments = message.segments.map((e) => VideoSegment.toJSON(e));
    }
    if (message.labelDetectionConfig !== undefined) {
      obj.labelDetectionConfig = LabelDetectionConfig.toJSON(message.labelDetectionConfig);
    }
    if (message.shotChangeDetectionConfig !== undefined) {
      obj.shotChangeDetectionConfig = ShotChangeDetectionConfig.toJSON(message.shotChangeDetectionConfig);
    }
    if (message.explicitContentDetectionConfig !== undefined) {
      obj.explicitContentDetectionConfig = ExplicitContentDetectionConfig.toJSON(
        message.explicitContentDetectionConfig,
      );
    }
    if (message.speechTranscriptionConfig !== undefined) {
      obj.speechTranscriptionConfig = SpeechTranscriptionConfig.toJSON(message.speechTranscriptionConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<VideoContext>): VideoContext {
    return VideoContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VideoContext>): VideoContext {
    const message = createBaseVideoContext();
    message.segments = object.segments?.map((e) => VideoSegment.fromPartial(e)) || [];
    message.labelDetectionConfig = (object.labelDetectionConfig !== undefined && object.labelDetectionConfig !== null)
      ? LabelDetectionConfig.fromPartial(object.labelDetectionConfig)
      : undefined;
    message.shotChangeDetectionConfig =
      (object.shotChangeDetectionConfig !== undefined && object.shotChangeDetectionConfig !== null)
        ? ShotChangeDetectionConfig.fromPartial(object.shotChangeDetectionConfig)
        : undefined;
    message.explicitContentDetectionConfig =
      (object.explicitContentDetectionConfig !== undefined && object.explicitContentDetectionConfig !== null)
        ? ExplicitContentDetectionConfig.fromPartial(object.explicitContentDetectionConfig)
        : undefined;
    message.speechTranscriptionConfig =
      (object.speechTranscriptionConfig !== undefined && object.speechTranscriptionConfig !== null)
        ? SpeechTranscriptionConfig.fromPartial(object.speechTranscriptionConfig)
        : undefined;
    return message;
  },
};

function createBaseLabelDetectionConfig(): LabelDetectionConfig {
  return { labelDetectionMode: 0, stationaryCamera: false, model: "" };
}

export const LabelDetectionConfig: MessageFns<LabelDetectionConfig> = {
  encode(message: LabelDetectionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.labelDetectionMode !== 0) {
      writer.uint32(8).int32(message.labelDetectionMode);
    }
    if (message.stationaryCamera !== false) {
      writer.uint32(16).bool(message.stationaryCamera);
    }
    if (message.model !== "") {
      writer.uint32(26).string(message.model);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LabelDetectionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLabelDetectionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.labelDetectionMode = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.stationaryCamera = reader.bool();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.model = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LabelDetectionConfig {
    return {
      labelDetectionMode: isSet(object.labelDetectionMode) ? labelDetectionModeFromJSON(object.labelDetectionMode) : 0,
      stationaryCamera: isSet(object.stationaryCamera) ? globalThis.Boolean(object.stationaryCamera) : false,
      model: isSet(object.model) ? globalThis.String(object.model) : "",
    };
  },

  toJSON(message: LabelDetectionConfig): unknown {
    const obj: any = {};
    if (message.labelDetectionMode !== 0) {
      obj.labelDetectionMode = labelDetectionModeToJSON(message.labelDetectionMode);
    }
    if (message.stationaryCamera !== false) {
      obj.stationaryCamera = message.stationaryCamera;
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    return obj;
  },

  create(base?: DeepPartial<LabelDetectionConfig>): LabelDetectionConfig {
    return LabelDetectionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LabelDetectionConfig>): LabelDetectionConfig {
    const message = createBaseLabelDetectionConfig();
    message.labelDetectionMode = object.labelDetectionMode ?? 0;
    message.stationaryCamera = object.stationaryCamera ?? false;
    message.model = object.model ?? "";
    return message;
  },
};

function createBaseShotChangeDetectionConfig(): ShotChangeDetectionConfig {
  return { model: "" };
}

export const ShotChangeDetectionConfig: MessageFns<ShotChangeDetectionConfig> = {
  encode(message: ShotChangeDetectionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ShotChangeDetectionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseShotChangeDetectionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ShotChangeDetectionConfig {
    return { model: isSet(object.model) ? globalThis.String(object.model) : "" };
  },

  toJSON(message: ShotChangeDetectionConfig): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    return obj;
  },

  create(base?: DeepPartial<ShotChangeDetectionConfig>): ShotChangeDetectionConfig {
    return ShotChangeDetectionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ShotChangeDetectionConfig>): ShotChangeDetectionConfig {
    const message = createBaseShotChangeDetectionConfig();
    message.model = object.model ?? "";
    return message;
  },
};

function createBaseExplicitContentDetectionConfig(): ExplicitContentDetectionConfig {
  return { model: "" };
}

export const ExplicitContentDetectionConfig: MessageFns<ExplicitContentDetectionConfig> = {
  encode(message: ExplicitContentDetectionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplicitContentDetectionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplicitContentDetectionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplicitContentDetectionConfig {
    return { model: isSet(object.model) ? globalThis.String(object.model) : "" };
  },

  toJSON(message: ExplicitContentDetectionConfig): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    return obj;
  },

  create(base?: DeepPartial<ExplicitContentDetectionConfig>): ExplicitContentDetectionConfig {
    return ExplicitContentDetectionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplicitContentDetectionConfig>): ExplicitContentDetectionConfig {
    const message = createBaseExplicitContentDetectionConfig();
    message.model = object.model ?? "";
    return message;
  },
};

function createBaseVideoSegment(): VideoSegment {
  return { startTimeOffset: undefined, endTimeOffset: undefined };
}

export const VideoSegment: MessageFns<VideoSegment> = {
  encode(message: VideoSegment, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startTimeOffset !== undefined) {
      Duration.encode(message.startTimeOffset, writer.uint32(10).fork()).join();
    }
    if (message.endTimeOffset !== undefined) {
      Duration.encode(message.endTimeOffset, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VideoSegment {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVideoSegment();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startTimeOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.endTimeOffset = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VideoSegment {
    return {
      startTimeOffset: isSet(object.startTimeOffset) ? Duration.fromJSON(object.startTimeOffset) : undefined,
      endTimeOffset: isSet(object.endTimeOffset) ? Duration.fromJSON(object.endTimeOffset) : undefined,
    };
  },

  toJSON(message: VideoSegment): unknown {
    const obj: any = {};
    if (message.startTimeOffset !== undefined) {
      obj.startTimeOffset = Duration.toJSON(message.startTimeOffset);
    }
    if (message.endTimeOffset !== undefined) {
      obj.endTimeOffset = Duration.toJSON(message.endTimeOffset);
    }
    return obj;
  },

  create(base?: DeepPartial<VideoSegment>): VideoSegment {
    return VideoSegment.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VideoSegment>): VideoSegment {
    const message = createBaseVideoSegment();
    message.startTimeOffset = (object.startTimeOffset !== undefined && object.startTimeOffset !== null)
      ? Duration.fromPartial(object.startTimeOffset)
      : undefined;
    message.endTimeOffset = (object.endTimeOffset !== undefined && object.endTimeOffset !== null)
      ? Duration.fromPartial(object.endTimeOffset)
      : undefined;
    return message;
  },
};

function createBaseLabelSegment(): LabelSegment {
  return { segment: undefined, confidence: 0 };
}

export const LabelSegment: MessageFns<LabelSegment> = {
  encode(message: LabelSegment, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.segment !== undefined) {
      VideoSegment.encode(message.segment, writer.uint32(10).fork()).join();
    }
    if (message.confidence !== 0) {
      writer.uint32(21).float(message.confidence);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LabelSegment {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLabelSegment();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.segment = VideoSegment.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.confidence = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LabelSegment {
    return {
      segment: isSet(object.segment) ? VideoSegment.fromJSON(object.segment) : undefined,
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
    };
  },

  toJSON(message: LabelSegment): unknown {
    const obj: any = {};
    if (message.segment !== undefined) {
      obj.segment = VideoSegment.toJSON(message.segment);
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    return obj;
  },

  create(base?: DeepPartial<LabelSegment>): LabelSegment {
    return LabelSegment.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LabelSegment>): LabelSegment {
    const message = createBaseLabelSegment();
    message.segment = (object.segment !== undefined && object.segment !== null)
      ? VideoSegment.fromPartial(object.segment)
      : undefined;
    message.confidence = object.confidence ?? 0;
    return message;
  },
};

function createBaseLabelFrame(): LabelFrame {
  return { timeOffset: undefined, confidence: 0 };
}

export const LabelFrame: MessageFns<LabelFrame> = {
  encode(message: LabelFrame, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.timeOffset !== undefined) {
      Duration.encode(message.timeOffset, writer.uint32(10).fork()).join();
    }
    if (message.confidence !== 0) {
      writer.uint32(21).float(message.confidence);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LabelFrame {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLabelFrame();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.timeOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.confidence = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LabelFrame {
    return {
      timeOffset: isSet(object.timeOffset) ? Duration.fromJSON(object.timeOffset) : undefined,
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
    };
  },

  toJSON(message: LabelFrame): unknown {
    const obj: any = {};
    if (message.timeOffset !== undefined) {
      obj.timeOffset = Duration.toJSON(message.timeOffset);
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    return obj;
  },

  create(base?: DeepPartial<LabelFrame>): LabelFrame {
    return LabelFrame.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LabelFrame>): LabelFrame {
    const message = createBaseLabelFrame();
    message.timeOffset = (object.timeOffset !== undefined && object.timeOffset !== null)
      ? Duration.fromPartial(object.timeOffset)
      : undefined;
    message.confidence = object.confidence ?? 0;
    return message;
  },
};

function createBaseEntity(): Entity {
  return { entityId: "", description: "", languageCode: "" };
}

export const Entity: MessageFns<Entity> = {
  encode(message: Entity, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.entityId !== "") {
      writer.uint32(10).string(message.entityId);
    }
    if (message.description !== "") {
      writer.uint32(18).string(message.description);
    }
    if (message.languageCode !== "") {
      writer.uint32(26).string(message.languageCode);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Entity {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEntity();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.entityId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.description = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.languageCode = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Entity {
    return {
      entityId: isSet(object.entityId) ? globalThis.String(object.entityId) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
    };
  },

  toJSON(message: Entity): unknown {
    const obj: any = {};
    if (message.entityId !== "") {
      obj.entityId = message.entityId;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    return obj;
  },

  create(base?: DeepPartial<Entity>): Entity {
    return Entity.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Entity>): Entity {
    const message = createBaseEntity();
    message.entityId = object.entityId ?? "";
    message.description = object.description ?? "";
    message.languageCode = object.languageCode ?? "";
    return message;
  },
};

function createBaseLabelAnnotation(): LabelAnnotation {
  return { entity: undefined, categoryEntities: [], segments: [], frames: [] };
}

export const LabelAnnotation: MessageFns<LabelAnnotation> = {
  encode(message: LabelAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.entity !== undefined) {
      Entity.encode(message.entity, writer.uint32(10).fork()).join();
    }
    for (const v of message.categoryEntities) {
      Entity.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.segments) {
      LabelSegment.encode(v!, writer.uint32(26).fork()).join();
    }
    for (const v of message.frames) {
      LabelFrame.encode(v!, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LabelAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLabelAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.entity = Entity.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.categoryEntities.push(Entity.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.segments.push(LabelSegment.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.frames.push(LabelFrame.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LabelAnnotation {
    return {
      entity: isSet(object.entity) ? Entity.fromJSON(object.entity) : undefined,
      categoryEntities: globalThis.Array.isArray(object?.categoryEntities)
        ? object.categoryEntities.map((e: any) => Entity.fromJSON(e))
        : [],
      segments: globalThis.Array.isArray(object?.segments)
        ? object.segments.map((e: any) => LabelSegment.fromJSON(e))
        : [],
      frames: globalThis.Array.isArray(object?.frames) ? object.frames.map((e: any) => LabelFrame.fromJSON(e)) : [],
    };
  },

  toJSON(message: LabelAnnotation): unknown {
    const obj: any = {};
    if (message.entity !== undefined) {
      obj.entity = Entity.toJSON(message.entity);
    }
    if (message.categoryEntities?.length) {
      obj.categoryEntities = message.categoryEntities.map((e) => Entity.toJSON(e));
    }
    if (message.segments?.length) {
      obj.segments = message.segments.map((e) => LabelSegment.toJSON(e));
    }
    if (message.frames?.length) {
      obj.frames = message.frames.map((e) => LabelFrame.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<LabelAnnotation>): LabelAnnotation {
    return LabelAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LabelAnnotation>): LabelAnnotation {
    const message = createBaseLabelAnnotation();
    message.entity = (object.entity !== undefined && object.entity !== null)
      ? Entity.fromPartial(object.entity)
      : undefined;
    message.categoryEntities = object.categoryEntities?.map((e) => Entity.fromPartial(e)) || [];
    message.segments = object.segments?.map((e) => LabelSegment.fromPartial(e)) || [];
    message.frames = object.frames?.map((e) => LabelFrame.fromPartial(e)) || [];
    return message;
  },
};

function createBaseExplicitContentFrame(): ExplicitContentFrame {
  return { timeOffset: undefined, pornographyLikelihood: 0 };
}

export const ExplicitContentFrame: MessageFns<ExplicitContentFrame> = {
  encode(message: ExplicitContentFrame, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.timeOffset !== undefined) {
      Duration.encode(message.timeOffset, writer.uint32(10).fork()).join();
    }
    if (message.pornographyLikelihood !== 0) {
      writer.uint32(16).int32(message.pornographyLikelihood);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplicitContentFrame {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplicitContentFrame();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.timeOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pornographyLikelihood = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplicitContentFrame {
    return {
      timeOffset: isSet(object.timeOffset) ? Duration.fromJSON(object.timeOffset) : undefined,
      pornographyLikelihood: isSet(object.pornographyLikelihood) ? likelihoodFromJSON(object.pornographyLikelihood) : 0,
    };
  },

  toJSON(message: ExplicitContentFrame): unknown {
    const obj: any = {};
    if (message.timeOffset !== undefined) {
      obj.timeOffset = Duration.toJSON(message.timeOffset);
    }
    if (message.pornographyLikelihood !== 0) {
      obj.pornographyLikelihood = likelihoodToJSON(message.pornographyLikelihood);
    }
    return obj;
  },

  create(base?: DeepPartial<ExplicitContentFrame>): ExplicitContentFrame {
    return ExplicitContentFrame.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplicitContentFrame>): ExplicitContentFrame {
    const message = createBaseExplicitContentFrame();
    message.timeOffset = (object.timeOffset !== undefined && object.timeOffset !== null)
      ? Duration.fromPartial(object.timeOffset)
      : undefined;
    message.pornographyLikelihood = object.pornographyLikelihood ?? 0;
    return message;
  },
};

function createBaseExplicitContentAnnotation(): ExplicitContentAnnotation {
  return { frames: [] };
}

export const ExplicitContentAnnotation: MessageFns<ExplicitContentAnnotation> = {
  encode(message: ExplicitContentAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.frames) {
      ExplicitContentFrame.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplicitContentAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplicitContentAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.frames.push(ExplicitContentFrame.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplicitContentAnnotation {
    return {
      frames: globalThis.Array.isArray(object?.frames)
        ? object.frames.map((e: any) => ExplicitContentFrame.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ExplicitContentAnnotation): unknown {
    const obj: any = {};
    if (message.frames?.length) {
      obj.frames = message.frames.map((e) => ExplicitContentFrame.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ExplicitContentAnnotation>): ExplicitContentAnnotation {
    return ExplicitContentAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplicitContentAnnotation>): ExplicitContentAnnotation {
    const message = createBaseExplicitContentAnnotation();
    message.frames = object.frames?.map((e) => ExplicitContentFrame.fromPartial(e)) || [];
    return message;
  },
};

function createBaseVideoAnnotationResults(): VideoAnnotationResults {
  return {
    inputUri: "",
    segmentLabelAnnotations: [],
    shotLabelAnnotations: [],
    frameLabelAnnotations: [],
    shotAnnotations: [],
    explicitAnnotation: undefined,
    speechTranscriptions: [],
    error: undefined,
  };
}

export const VideoAnnotationResults: MessageFns<VideoAnnotationResults> = {
  encode(message: VideoAnnotationResults, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputUri !== "") {
      writer.uint32(10).string(message.inputUri);
    }
    for (const v of message.segmentLabelAnnotations) {
      LabelAnnotation.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.shotLabelAnnotations) {
      LabelAnnotation.encode(v!, writer.uint32(26).fork()).join();
    }
    for (const v of message.frameLabelAnnotations) {
      LabelAnnotation.encode(v!, writer.uint32(34).fork()).join();
    }
    for (const v of message.shotAnnotations) {
      VideoSegment.encode(v!, writer.uint32(50).fork()).join();
    }
    if (message.explicitAnnotation !== undefined) {
      ExplicitContentAnnotation.encode(message.explicitAnnotation, writer.uint32(58).fork()).join();
    }
    for (const v of message.speechTranscriptions) {
      SpeechTranscription.encode(v!, writer.uint32(90).fork()).join();
    }
    if (message.error !== undefined) {
      Status.encode(message.error, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VideoAnnotationResults {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVideoAnnotationResults();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.segmentLabelAnnotations.push(LabelAnnotation.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.shotLabelAnnotations.push(LabelAnnotation.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.frameLabelAnnotations.push(LabelAnnotation.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.shotAnnotations.push(VideoSegment.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.explicitAnnotation = ExplicitContentAnnotation.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.speechTranscriptions.push(SpeechTranscription.decode(reader, reader.uint32()));
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.error = Status.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VideoAnnotationResults {
    return {
      inputUri: isSet(object.inputUri) ? globalThis.String(object.inputUri) : "",
      segmentLabelAnnotations: globalThis.Array.isArray(object?.segmentLabelAnnotations)
        ? object.segmentLabelAnnotations.map((e: any) => LabelAnnotation.fromJSON(e))
        : [],
      shotLabelAnnotations: globalThis.Array.isArray(object?.shotLabelAnnotations)
        ? object.shotLabelAnnotations.map((e: any) => LabelAnnotation.fromJSON(e))
        : [],
      frameLabelAnnotations: globalThis.Array.isArray(object?.frameLabelAnnotations)
        ? object.frameLabelAnnotations.map((e: any) => LabelAnnotation.fromJSON(e))
        : [],
      shotAnnotations: globalThis.Array.isArray(object?.shotAnnotations)
        ? object.shotAnnotations.map((e: any) => VideoSegment.fromJSON(e))
        : [],
      explicitAnnotation: isSet(object.explicitAnnotation)
        ? ExplicitContentAnnotation.fromJSON(object.explicitAnnotation)
        : undefined,
      speechTranscriptions: globalThis.Array.isArray(object?.speechTranscriptions)
        ? object.speechTranscriptions.map((e: any) => SpeechTranscription.fromJSON(e))
        : [],
      error: isSet(object.error) ? Status.fromJSON(object.error) : undefined,
    };
  },

  toJSON(message: VideoAnnotationResults): unknown {
    const obj: any = {};
    if (message.inputUri !== "") {
      obj.inputUri = message.inputUri;
    }
    if (message.segmentLabelAnnotations?.length) {
      obj.segmentLabelAnnotations = message.segmentLabelAnnotations.map((e) => LabelAnnotation.toJSON(e));
    }
    if (message.shotLabelAnnotations?.length) {
      obj.shotLabelAnnotations = message.shotLabelAnnotations.map((e) => LabelAnnotation.toJSON(e));
    }
    if (message.frameLabelAnnotations?.length) {
      obj.frameLabelAnnotations = message.frameLabelAnnotations.map((e) => LabelAnnotation.toJSON(e));
    }
    if (message.shotAnnotations?.length) {
      obj.shotAnnotations = message.shotAnnotations.map((e) => VideoSegment.toJSON(e));
    }
    if (message.explicitAnnotation !== undefined) {
      obj.explicitAnnotation = ExplicitContentAnnotation.toJSON(message.explicitAnnotation);
    }
    if (message.speechTranscriptions?.length) {
      obj.speechTranscriptions = message.speechTranscriptions.map((e) => SpeechTranscription.toJSON(e));
    }
    if (message.error !== undefined) {
      obj.error = Status.toJSON(message.error);
    }
    return obj;
  },

  create(base?: DeepPartial<VideoAnnotationResults>): VideoAnnotationResults {
    return VideoAnnotationResults.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VideoAnnotationResults>): VideoAnnotationResults {
    const message = createBaseVideoAnnotationResults();
    message.inputUri = object.inputUri ?? "";
    message.segmentLabelAnnotations = object.segmentLabelAnnotations?.map((e) => LabelAnnotation.fromPartial(e)) || [];
    message.shotLabelAnnotations = object.shotLabelAnnotations?.map((e) => LabelAnnotation.fromPartial(e)) || [];
    message.frameLabelAnnotations = object.frameLabelAnnotations?.map((e) => LabelAnnotation.fromPartial(e)) || [];
    message.shotAnnotations = object.shotAnnotations?.map((e) => VideoSegment.fromPartial(e)) || [];
    message.explicitAnnotation = (object.explicitAnnotation !== undefined && object.explicitAnnotation !== null)
      ? ExplicitContentAnnotation.fromPartial(object.explicitAnnotation)
      : undefined;
    message.speechTranscriptions = object.speechTranscriptions?.map((e) => SpeechTranscription.fromPartial(e)) || [];
    message.error = (object.error !== undefined && object.error !== null)
      ? Status.fromPartial(object.error)
      : undefined;
    return message;
  },
};

function createBaseAnnotateVideoResponse(): AnnotateVideoResponse {
  return { annotationResults: [] };
}

export const AnnotateVideoResponse: MessageFns<AnnotateVideoResponse> = {
  encode(message: AnnotateVideoResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.annotationResults) {
      VideoAnnotationResults.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnnotateVideoResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnnotateVideoResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.annotationResults.push(VideoAnnotationResults.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnnotateVideoResponse {
    return {
      annotationResults: globalThis.Array.isArray(object?.annotationResults)
        ? object.annotationResults.map((e: any) => VideoAnnotationResults.fromJSON(e))
        : [],
    };
  },

  toJSON(message: AnnotateVideoResponse): unknown {
    const obj: any = {};
    if (message.annotationResults?.length) {
      obj.annotationResults = message.annotationResults.map((e) => VideoAnnotationResults.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<AnnotateVideoResponse>): AnnotateVideoResponse {
    return AnnotateVideoResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AnnotateVideoResponse>): AnnotateVideoResponse {
    const message = createBaseAnnotateVideoResponse();
    message.annotationResults = object.annotationResults?.map((e) => VideoAnnotationResults.fromPartial(e)) || [];
    return message;
  },
};

function createBaseVideoAnnotationProgress(): VideoAnnotationProgress {
  return { inputUri: "", progressPercent: 0, startTime: undefined, updateTime: undefined };
}

export const VideoAnnotationProgress: MessageFns<VideoAnnotationProgress> = {
  encode(message: VideoAnnotationProgress, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputUri !== "") {
      writer.uint32(10).string(message.inputUri);
    }
    if (message.progressPercent !== 0) {
      writer.uint32(16).int32(message.progressPercent);
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(26).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VideoAnnotationProgress {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVideoAnnotationProgress();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputUri = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.progressPercent = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VideoAnnotationProgress {
    return {
      inputUri: isSet(object.inputUri) ? globalThis.String(object.inputUri) : "",
      progressPercent: isSet(object.progressPercent) ? globalThis.Number(object.progressPercent) : 0,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
    };
  },

  toJSON(message: VideoAnnotationProgress): unknown {
    const obj: any = {};
    if (message.inputUri !== "") {
      obj.inputUri = message.inputUri;
    }
    if (message.progressPercent !== 0) {
      obj.progressPercent = Math.round(message.progressPercent);
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<VideoAnnotationProgress>): VideoAnnotationProgress {
    return VideoAnnotationProgress.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VideoAnnotationProgress>): VideoAnnotationProgress {
    const message = createBaseVideoAnnotationProgress();
    message.inputUri = object.inputUri ?? "";
    message.progressPercent = object.progressPercent ?? 0;
    message.startTime = object.startTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    return message;
  },
};

function createBaseAnnotateVideoProgress(): AnnotateVideoProgress {
  return { annotationProgress: [] };
}

export const AnnotateVideoProgress: MessageFns<AnnotateVideoProgress> = {
  encode(message: AnnotateVideoProgress, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.annotationProgress) {
      VideoAnnotationProgress.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnnotateVideoProgress {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnnotateVideoProgress();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.annotationProgress.push(VideoAnnotationProgress.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnnotateVideoProgress {
    return {
      annotationProgress: globalThis.Array.isArray(object?.annotationProgress)
        ? object.annotationProgress.map((e: any) => VideoAnnotationProgress.fromJSON(e))
        : [],
    };
  },

  toJSON(message: AnnotateVideoProgress): unknown {
    const obj: any = {};
    if (message.annotationProgress?.length) {
      obj.annotationProgress = message.annotationProgress.map((e) => VideoAnnotationProgress.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<AnnotateVideoProgress>): AnnotateVideoProgress {
    return AnnotateVideoProgress.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AnnotateVideoProgress>): AnnotateVideoProgress {
    const message = createBaseAnnotateVideoProgress();
    message.annotationProgress = object.annotationProgress?.map((e) => VideoAnnotationProgress.fromPartial(e)) || [];
    return message;
  },
};

function createBaseSpeechTranscriptionConfig(): SpeechTranscriptionConfig {
  return {
    languageCode: "",
    maxAlternatives: 0,
    filterProfanity: false,
    speechContexts: [],
    enableAutomaticPunctuation: false,
    audioTracks: [],
  };
}

export const SpeechTranscriptionConfig: MessageFns<SpeechTranscriptionConfig> = {
  encode(message: SpeechTranscriptionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.languageCode !== "") {
      writer.uint32(10).string(message.languageCode);
    }
    if (message.maxAlternatives !== 0) {
      writer.uint32(16).int32(message.maxAlternatives);
    }
    if (message.filterProfanity !== false) {
      writer.uint32(24).bool(message.filterProfanity);
    }
    for (const v of message.speechContexts) {
      SpeechContext.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.enableAutomaticPunctuation !== false) {
      writer.uint32(40).bool(message.enableAutomaticPunctuation);
    }
    writer.uint32(50).fork();
    for (const v of message.audioTracks) {
      writer.int32(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechTranscriptionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechTranscriptionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.languageCode = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.maxAlternatives = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.filterProfanity = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.speechContexts.push(SpeechContext.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.enableAutomaticPunctuation = reader.bool();
          continue;
        case 6:
          if (tag === 48) {
            message.audioTracks.push(reader.int32());

            continue;
          }

          if (tag === 50) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.audioTracks.push(reader.int32());
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechTranscriptionConfig {
    return {
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
      maxAlternatives: isSet(object.maxAlternatives) ? globalThis.Number(object.maxAlternatives) : 0,
      filterProfanity: isSet(object.filterProfanity) ? globalThis.Boolean(object.filterProfanity) : false,
      speechContexts: globalThis.Array.isArray(object?.speechContexts)
        ? object.speechContexts.map((e: any) => SpeechContext.fromJSON(e))
        : [],
      enableAutomaticPunctuation: isSet(object.enableAutomaticPunctuation)
        ? globalThis.Boolean(object.enableAutomaticPunctuation)
        : false,
      audioTracks: globalThis.Array.isArray(object?.audioTracks)
        ? object.audioTracks.map((e: any) => globalThis.Number(e))
        : [],
    };
  },

  toJSON(message: SpeechTranscriptionConfig): unknown {
    const obj: any = {};
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    if (message.maxAlternatives !== 0) {
      obj.maxAlternatives = Math.round(message.maxAlternatives);
    }
    if (message.filterProfanity !== false) {
      obj.filterProfanity = message.filterProfanity;
    }
    if (message.speechContexts?.length) {
      obj.speechContexts = message.speechContexts.map((e) => SpeechContext.toJSON(e));
    }
    if (message.enableAutomaticPunctuation !== false) {
      obj.enableAutomaticPunctuation = message.enableAutomaticPunctuation;
    }
    if (message.audioTracks?.length) {
      obj.audioTracks = message.audioTracks.map((e) => Math.round(e));
    }
    return obj;
  },

  create(base?: DeepPartial<SpeechTranscriptionConfig>): SpeechTranscriptionConfig {
    return SpeechTranscriptionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SpeechTranscriptionConfig>): SpeechTranscriptionConfig {
    const message = createBaseSpeechTranscriptionConfig();
    message.languageCode = object.languageCode ?? "";
    message.maxAlternatives = object.maxAlternatives ?? 0;
    message.filterProfanity = object.filterProfanity ?? false;
    message.speechContexts = object.speechContexts?.map((e) => SpeechContext.fromPartial(e)) || [];
    message.enableAutomaticPunctuation = object.enableAutomaticPunctuation ?? false;
    message.audioTracks = object.audioTracks?.map((e) => e) || [];
    return message;
  },
};

function createBaseSpeechContext(): SpeechContext {
  return { phrases: [] };
}

export const SpeechContext: MessageFns<SpeechContext> = {
  encode(message: SpeechContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.phrases) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.phrases.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechContext {
    return {
      phrases: globalThis.Array.isArray(object?.phrases) ? object.phrases.map((e: any) => globalThis.String(e)) : [],
    };
  },

  toJSON(message: SpeechContext): unknown {
    const obj: any = {};
    if (message.phrases?.length) {
      obj.phrases = message.phrases;
    }
    return obj;
  },

  create(base?: DeepPartial<SpeechContext>): SpeechContext {
    return SpeechContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SpeechContext>): SpeechContext {
    const message = createBaseSpeechContext();
    message.phrases = object.phrases?.map((e) => e) || [];
    return message;
  },
};

function createBaseSpeechTranscription(): SpeechTranscription {
  return { alternatives: [] };
}

export const SpeechTranscription: MessageFns<SpeechTranscription> = {
  encode(message: SpeechTranscription, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.alternatives) {
      SpeechRecognitionAlternative.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechTranscription {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechTranscription();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.alternatives.push(SpeechRecognitionAlternative.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechTranscription {
    return {
      alternatives: globalThis.Array.isArray(object?.alternatives)
        ? object.alternatives.map((e: any) => SpeechRecognitionAlternative.fromJSON(e))
        : [],
    };
  },

  toJSON(message: SpeechTranscription): unknown {
    const obj: any = {};
    if (message.alternatives?.length) {
      obj.alternatives = message.alternatives.map((e) => SpeechRecognitionAlternative.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<SpeechTranscription>): SpeechTranscription {
    return SpeechTranscription.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SpeechTranscription>): SpeechTranscription {
    const message = createBaseSpeechTranscription();
    message.alternatives = object.alternatives?.map((e) => SpeechRecognitionAlternative.fromPartial(e)) || [];
    return message;
  },
};

function createBaseSpeechRecognitionAlternative(): SpeechRecognitionAlternative {
  return { transcript: "", confidence: 0, words: [] };
}

export const SpeechRecognitionAlternative: MessageFns<SpeechRecognitionAlternative> = {
  encode(message: SpeechRecognitionAlternative, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.transcript !== "") {
      writer.uint32(10).string(message.transcript);
    }
    if (message.confidence !== 0) {
      writer.uint32(21).float(message.confidence);
    }
    for (const v of message.words) {
      WordInfo.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechRecognitionAlternative {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechRecognitionAlternative();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.transcript = reader.string();
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.confidence = reader.float();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.words.push(WordInfo.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechRecognitionAlternative {
    return {
      transcript: isSet(object.transcript) ? globalThis.String(object.transcript) : "",
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
      words: globalThis.Array.isArray(object?.words) ? object.words.map((e: any) => WordInfo.fromJSON(e)) : [],
    };
  },

  toJSON(message: SpeechRecognitionAlternative): unknown {
    const obj: any = {};
    if (message.transcript !== "") {
      obj.transcript = message.transcript;
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    if (message.words?.length) {
      obj.words = message.words.map((e) => WordInfo.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<SpeechRecognitionAlternative>): SpeechRecognitionAlternative {
    return SpeechRecognitionAlternative.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SpeechRecognitionAlternative>): SpeechRecognitionAlternative {
    const message = createBaseSpeechRecognitionAlternative();
    message.transcript = object.transcript ?? "";
    message.confidence = object.confidence ?? 0;
    message.words = object.words?.map((e) => WordInfo.fromPartial(e)) || [];
    return message;
  },
};

function createBaseWordInfo(): WordInfo {
  return { startTime: undefined, endTime: undefined, word: "" };
}

export const WordInfo: MessageFns<WordInfo> = {
  encode(message: WordInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startTime !== undefined) {
      Duration.encode(message.startTime, writer.uint32(10).fork()).join();
    }
    if (message.endTime !== undefined) {
      Duration.encode(message.endTime, writer.uint32(18).fork()).join();
    }
    if (message.word !== "") {
      writer.uint32(26).string(message.word);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): WordInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseWordInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startTime = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.endTime = Duration.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.word = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): WordInfo {
    return {
      startTime: isSet(object.startTime) ? Duration.fromJSON(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? Duration.fromJSON(object.endTime) : undefined,
      word: isSet(object.word) ? globalThis.String(object.word) : "",
    };
  },

  toJSON(message: WordInfo): unknown {
    const obj: any = {};
    if (message.startTime !== undefined) {
      obj.startTime = Duration.toJSON(message.startTime);
    }
    if (message.endTime !== undefined) {
      obj.endTime = Duration.toJSON(message.endTime);
    }
    if (message.word !== "") {
      obj.word = message.word;
    }
    return obj;
  },

  create(base?: DeepPartial<WordInfo>): WordInfo {
    return WordInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<WordInfo>): WordInfo {
    const message = createBaseWordInfo();
    message.startTime = (object.startTime !== undefined && object.startTime !== null)
      ? Duration.fromPartial(object.startTime)
      : undefined;
    message.endTime = (object.endTime !== undefined && object.endTime !== null)
      ? Duration.fromPartial(object.endTime)
      : undefined;
    message.word = object.word ?? "";
    return message;
  },
};

/** Service that implements Google Cloud Video Intelligence API. */
export type VideoIntelligenceServiceDefinition = typeof VideoIntelligenceServiceDefinition;
export const VideoIntelligenceServiceDefinition = {
  name: "VideoIntelligenceService",
  fullName: "google.cloud.videointelligence.v1p1beta1.VideoIntelligenceService",
  methods: {
    /**
     * Performs asynchronous video annotation. Progress and results can be
     * retrieved through the `google.longrunning.Operations` interface.
     * `Operation.metadata` contains `AnnotateVideoProgress` (progress).
     * `Operation.response` contains `AnnotateVideoResponse` (results).
     */
    annotateVideo: {
      name: "AnnotateVideo",
      requestType: AnnotateVideoRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              46,
              10,
              21,
              65,
              110,
              110,
              111,
              116,
              97,
              116,
              101,
              86,
              105,
              100,
              101,
              111,
              82,
              101,
              115,
              112,
              111,
              110,
              115,
              101,
              18,
              21,
              65,
              110,
              110,
              111,
              116,
              97,
              116,
              101,
              86,
              105,
              100,
              101,
              111,
              80,
              114,
              111,
              103,
              114,
              101,
              115,
              115,
            ]),
          ],
          8410: [
            Buffer.from([18, 105, 110, 112, 117, 116, 95, 117, 114, 105, 44, 102, 101, 97, 116, 117, 114, 101, 115]),
          ],
          578365826: [
            Buffer.from([
              31,
              58,
              1,
              42,
              34,
              26,
              47,
              118,
              49,
              112,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              118,
              105,
              100,
              101,
              111,
              115,
              58,
              97,
              110,
              110,
              111,
              116,
              97,
              116,
              101,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface VideoIntelligenceServiceImplementation<CallContextExt = {}> {
  /**
   * Performs asynchronous video annotation. Progress and results can be
   * retrieved through the `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `AnnotateVideoProgress` (progress).
   * `Operation.response` contains `AnnotateVideoResponse` (results).
   */
  annotateVideo(request: AnnotateVideoRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
}

export interface VideoIntelligenceServiceClient<CallOptionsExt = {}> {
  /**
   * Performs asynchronous video annotation. Progress and results can be
   * retrieved through the `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `AnnotateVideoProgress` (progress).
   * `Operation.response` contains `AnnotateVideoResponse` (results).
   */
  annotateVideo(request: DeepPartial<AnnotateVideoRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
}

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
