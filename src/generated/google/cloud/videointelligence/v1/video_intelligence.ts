// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/videointelligence/v1/video_intelligence.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Operation } from "../../../longrunning/operations.js";
import { Duration } from "../../../protobuf/duration.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { Status } from "../../../rpc/status.js";

export const protobufPackage = "google.cloud.videointelligence.v1";

/** Video annotation feature. */
export enum Feature {
  /** FEATURE_UNSPECIFIED - Unspecified. */
  FEATURE_UNSPECIFIED = 0,
  /** LABEL_DETECTION - Label detection. Detect objects, such as dog or flower. */
  LABEL_DETECTION = 1,
  /** SHOT_CHANGE_DETECTION - Shot change detection. */
  SHOT_CHANGE_DETECTION = 2,
  /** EXPLICIT_CONTENT_DETECTION - Explicit content detection. */
  EXPLICIT_CONTENT_DETECTION = 3,
  /** FACE_DETECTION - Human face detection. */
  FACE_DETECTION = 4,
  /** SPEECH_TRANSCRIPTION - Speech transcription. */
  SPEECH_TRANSCRIPTION = 6,
  /** TEXT_DETECTION - OCR text detection and tracking. */
  TEXT_DETECTION = 7,
  /** OBJECT_TRACKING - Object detection and tracking. */
  OBJECT_TRACKING = 9,
  /** LOGO_RECOGNITION - Logo detection, tracking, and recognition. */
  LOGO_RECOGNITION = 12,
  /** PERSON_DETECTION - Person detection. */
  PERSON_DETECTION = 14,
  UNRECOGNIZED = -1,
}

export function featureFromJSON(object: any): Feature {
  switch (object) {
    case 0:
    case "FEATURE_UNSPECIFIED":
      return Feature.FEATURE_UNSPECIFIED;
    case 1:
    case "LABEL_DETECTION":
      return Feature.LABEL_DETECTION;
    case 2:
    case "SHOT_CHANGE_DETECTION":
      return Feature.SHOT_CHANGE_DETECTION;
    case 3:
    case "EXPLICIT_CONTENT_DETECTION":
      return Feature.EXPLICIT_CONTENT_DETECTION;
    case 4:
    case "FACE_DETECTION":
      return Feature.FACE_DETECTION;
    case 6:
    case "SPEECH_TRANSCRIPTION":
      return Feature.SPEECH_TRANSCRIPTION;
    case 7:
    case "TEXT_DETECTION":
      return Feature.TEXT_DETECTION;
    case 9:
    case "OBJECT_TRACKING":
      return Feature.OBJECT_TRACKING;
    case 12:
    case "LOGO_RECOGNITION":
      return Feature.LOGO_RECOGNITION;
    case 14:
    case "PERSON_DETECTION":
      return Feature.PERSON_DETECTION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Feature.UNRECOGNIZED;
  }
}

export function featureToJSON(object: Feature): string {
  switch (object) {
    case Feature.FEATURE_UNSPECIFIED:
      return "FEATURE_UNSPECIFIED";
    case Feature.LABEL_DETECTION:
      return "LABEL_DETECTION";
    case Feature.SHOT_CHANGE_DETECTION:
      return "SHOT_CHANGE_DETECTION";
    case Feature.EXPLICIT_CONTENT_DETECTION:
      return "EXPLICIT_CONTENT_DETECTION";
    case Feature.FACE_DETECTION:
      return "FACE_DETECTION";
    case Feature.SPEECH_TRANSCRIPTION:
      return "SPEECH_TRANSCRIPTION";
    case Feature.TEXT_DETECTION:
      return "TEXT_DETECTION";
    case Feature.OBJECT_TRACKING:
      return "OBJECT_TRACKING";
    case Feature.LOGO_RECOGNITION:
      return "LOGO_RECOGNITION";
    case Feature.PERSON_DETECTION:
      return "PERSON_DETECTION";
    case Feature.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Label detection mode. */
export enum LabelDetectionMode {
  /** LABEL_DETECTION_MODE_UNSPECIFIED - Unspecified. */
  LABEL_DETECTION_MODE_UNSPECIFIED = 0,
  /** SHOT_MODE - Detect shot-level labels. */
  SHOT_MODE = 1,
  /** FRAME_MODE - Detect frame-level labels. */
  FRAME_MODE = 2,
  /** SHOT_AND_FRAME_MODE - Detect both shot-level and frame-level labels. */
  SHOT_AND_FRAME_MODE = 3,
  UNRECOGNIZED = -1,
}

export function labelDetectionModeFromJSON(object: any): LabelDetectionMode {
  switch (object) {
    case 0:
    case "LABEL_DETECTION_MODE_UNSPECIFIED":
      return LabelDetectionMode.LABEL_DETECTION_MODE_UNSPECIFIED;
    case 1:
    case "SHOT_MODE":
      return LabelDetectionMode.SHOT_MODE;
    case 2:
    case "FRAME_MODE":
      return LabelDetectionMode.FRAME_MODE;
    case 3:
    case "SHOT_AND_FRAME_MODE":
      return LabelDetectionMode.SHOT_AND_FRAME_MODE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return LabelDetectionMode.UNRECOGNIZED;
  }
}

export function labelDetectionModeToJSON(object: LabelDetectionMode): string {
  switch (object) {
    case LabelDetectionMode.LABEL_DETECTION_MODE_UNSPECIFIED:
      return "LABEL_DETECTION_MODE_UNSPECIFIED";
    case LabelDetectionMode.SHOT_MODE:
      return "SHOT_MODE";
    case LabelDetectionMode.FRAME_MODE:
      return "FRAME_MODE";
    case LabelDetectionMode.SHOT_AND_FRAME_MODE:
      return "SHOT_AND_FRAME_MODE";
    case LabelDetectionMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Bucketized representation of likelihood. */
export enum Likelihood {
  /** LIKELIHOOD_UNSPECIFIED - Unspecified likelihood. */
  LIKELIHOOD_UNSPECIFIED = 0,
  /** VERY_UNLIKELY - Very unlikely. */
  VERY_UNLIKELY = 1,
  /** UNLIKELY - Unlikely. */
  UNLIKELY = 2,
  /** POSSIBLE - Possible. */
  POSSIBLE = 3,
  /** LIKELY - Likely. */
  LIKELY = 4,
  /** VERY_LIKELY - Very likely. */
  VERY_LIKELY = 5,
  UNRECOGNIZED = -1,
}

export function likelihoodFromJSON(object: any): Likelihood {
  switch (object) {
    case 0:
    case "LIKELIHOOD_UNSPECIFIED":
      return Likelihood.LIKELIHOOD_UNSPECIFIED;
    case 1:
    case "VERY_UNLIKELY":
      return Likelihood.VERY_UNLIKELY;
    case 2:
    case "UNLIKELY":
      return Likelihood.UNLIKELY;
    case 3:
    case "POSSIBLE":
      return Likelihood.POSSIBLE;
    case 4:
    case "LIKELY":
      return Likelihood.LIKELY;
    case 5:
    case "VERY_LIKELY":
      return Likelihood.VERY_LIKELY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Likelihood.UNRECOGNIZED;
  }
}

export function likelihoodToJSON(object: Likelihood): string {
  switch (object) {
    case Likelihood.LIKELIHOOD_UNSPECIFIED:
      return "LIKELIHOOD_UNSPECIFIED";
    case Likelihood.VERY_UNLIKELY:
      return "VERY_UNLIKELY";
    case Likelihood.UNLIKELY:
      return "UNLIKELY";
    case Likelihood.POSSIBLE:
      return "POSSIBLE";
    case Likelihood.LIKELY:
      return "LIKELY";
    case Likelihood.VERY_LIKELY:
      return "VERY_LIKELY";
    case Likelihood.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Video annotation request. */
export interface AnnotateVideoRequest {
  /**
   * Input video location. Currently, only
   * [Cloud Storage](https://cloud.google.com/storage/) URIs are
   * supported. URIs must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
   * more information, see [Request
   * URIs](https://cloud.google.com/storage/docs/request-endpoints). To identify
   * multiple videos, a video URI may include wildcards in the `object-id`.
   * Supported wildcards: '*' to match 0 or more characters;
   * '?' to match 1 character. If unset, the input video should be embedded
   * in the request as `input_content`. If set, `input_content` must be unset.
   */
  inputUri: string;
  /**
   * The video data bytes.
   * If unset, the input video(s) should be specified via the `input_uri`.
   * If set, `input_uri` must be unset.
   */
  inputContent: Buffer;
  /** Required. Requested video annotation features. */
  features: Feature[];
  /** Additional video context and/or feature-specific parameters. */
  videoContext:
    | VideoContext
    | undefined;
  /**
   * Optional. Location where the output (in JSON format) should be stored.
   * Currently, only [Cloud Storage](https://cloud.google.com/storage/)
   * URIs are supported. These must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
   * more information, see [Request
   * URIs](https://cloud.google.com/storage/docs/request-endpoints).
   */
  outputUri: string;
  /**
   * Optional. Cloud region where annotation should take place. Supported cloud
   * regions are: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no
   * region is specified, the region will be determined based on video file
   * location.
   */
  locationId: string;
}

/** Video context and/or feature-specific parameters. */
export interface VideoContext {
  /**
   * Video segments to annotate. The segments may overlap and are not required
   * to be contiguous or span the whole video. If unspecified, each video is
   * treated as a single segment.
   */
  segments: VideoSegment[];
  /** Config for LABEL_DETECTION. */
  labelDetectionConfig:
    | LabelDetectionConfig
    | undefined;
  /** Config for SHOT_CHANGE_DETECTION. */
  shotChangeDetectionConfig:
    | ShotChangeDetectionConfig
    | undefined;
  /** Config for EXPLICIT_CONTENT_DETECTION. */
  explicitContentDetectionConfig:
    | ExplicitContentDetectionConfig
    | undefined;
  /** Config for FACE_DETECTION. */
  faceDetectionConfig:
    | FaceDetectionConfig
    | undefined;
  /** Config for SPEECH_TRANSCRIPTION. */
  speechTranscriptionConfig:
    | SpeechTranscriptionConfig
    | undefined;
  /** Config for TEXT_DETECTION. */
  textDetectionConfig:
    | TextDetectionConfig
    | undefined;
  /** Config for PERSON_DETECTION. */
  personDetectionConfig:
    | PersonDetectionConfig
    | undefined;
  /** Config for OBJECT_TRACKING. */
  objectTrackingConfig: ObjectTrackingConfig | undefined;
}

/** Config for LABEL_DETECTION. */
export interface LabelDetectionConfig {
  /**
   * What labels should be detected with LABEL_DETECTION, in addition to
   * video-level labels or segment-level labels.
   * If unspecified, defaults to `SHOT_MODE`.
   */
  labelDetectionMode: LabelDetectionMode;
  /**
   * Whether the video has been shot from a stationary (i.e., non-moving)
   * camera. When set to true, might improve detection accuracy for moving
   * objects. Should be used with `SHOT_AND_FRAME_MODE` enabled.
   */
  stationaryCamera: boolean;
  /**
   * Model to use for label detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   */
  model: string;
  /**
   * The confidence threshold we perform filtering on the labels from
   * frame-level detection. If not set, it is set to 0.4 by default. The valid
   * range for this threshold is [0.1, 0.9]. Any value set outside of this
   * range will be clipped.
   * Note: For best results, follow the default threshold. We will update
   * the default threshold everytime when we release a new model.
   */
  frameConfidenceThreshold: number;
  /**
   * The confidence threshold we perform filtering on the labels from
   * video-level and shot-level detections. If not set, it's set to 0.3 by
   * default. The valid range for this threshold is [0.1, 0.9]. Any value set
   * outside of this range will be clipped.
   * Note: For best results, follow the default threshold. We will update
   * the default threshold everytime when we release a new model.
   */
  videoConfidenceThreshold: number;
}

/** Config for SHOT_CHANGE_DETECTION. */
export interface ShotChangeDetectionConfig {
  /**
   * Model to use for shot change detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   */
  model: string;
}

/** Config for OBJECT_TRACKING. */
export interface ObjectTrackingConfig {
  /**
   * Model to use for object tracking.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   */
  model: string;
}

/** Config for FACE_DETECTION. */
export interface FaceDetectionConfig {
  /**
   * Model to use for face detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   */
  model: string;
  /** Whether bounding boxes are included in the face annotation output. */
  includeBoundingBoxes: boolean;
  /**
   * Whether to enable face attributes detection, such as glasses, dark_glasses,
   * mouth_open etc. Ignored if 'include_bounding_boxes' is set to false.
   */
  includeAttributes: boolean;
}

/** Config for PERSON_DETECTION. */
export interface PersonDetectionConfig {
  /**
   * Whether bounding boxes are included in the person detection annotation
   * output.
   */
  includeBoundingBoxes: boolean;
  /**
   * Whether to enable pose landmarks detection. Ignored if
   * 'include_bounding_boxes' is set to false.
   */
  includePoseLandmarks: boolean;
  /**
   * Whether to enable person attributes detection, such as cloth color (black,
   * blue, etc), type (coat, dress, etc), pattern (plain, floral, etc), hair,
   * etc.
   * Ignored if 'include_bounding_boxes' is set to false.
   */
  includeAttributes: boolean;
}

/** Config for EXPLICIT_CONTENT_DETECTION. */
export interface ExplicitContentDetectionConfig {
  /**
   * Model to use for explicit content detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   */
  model: string;
}

/** Config for TEXT_DETECTION. */
export interface TextDetectionConfig {
  /**
   * Language hint can be specified if the language to be detected is known a
   * priori. It can increase the accuracy of the detection. Language hint must
   * be language code in BCP-47 format.
   *
   * Automatic language detection is performed if no hint is provided.
   */
  languageHints: string[];
  /**
   * Model to use for text detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   */
  model: string;
}

/** Video segment. */
export interface VideoSegment {
  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the start of the segment (inclusive).
   */
  startTimeOffset:
    | Duration
    | undefined;
  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the end of the segment (inclusive).
   */
  endTimeOffset: Duration | undefined;
}

/** Video segment level annotation results for label detection. */
export interface LabelSegment {
  /** Video segment where a label was detected. */
  segment:
    | VideoSegment
    | undefined;
  /** Confidence that the label is accurate. Range: [0, 1]. */
  confidence: number;
}

/** Video frame level annotation results for label detection. */
export interface LabelFrame {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   */
  timeOffset:
    | Duration
    | undefined;
  /** Confidence that the label is accurate. Range: [0, 1]. */
  confidence: number;
}

/** Detected entity from video analysis. */
export interface Entity {
  /**
   * Opaque entity ID. Some IDs may be available in
   * [Google Knowledge Graph Search
   * API](https://developers.google.com/knowledge-graph/).
   */
  entityId: string;
  /** Textual description, e.g., `Fixed-gear bicycle`. */
  description: string;
  /** Language code for `description` in BCP-47 format. */
  languageCode: string;
}

/** Label annotation. */
export interface LabelAnnotation {
  /** Detected entity. */
  entity:
    | Entity
    | undefined;
  /**
   * Common categories for the detected entity.
   * For example, when the label is `Terrier`, the category is likely `dog`. And
   * in some cases there might be more than one categories e.g., `Terrier` could
   * also be a `pet`.
   */
  categoryEntities: Entity[];
  /** All video segments where a label was detected. */
  segments: LabelSegment[];
  /** All video frames where a label was detected. */
  frames: LabelFrame[];
  /** Feature version. */
  version: string;
}

/** Video frame level annotation results for explicit content. */
export interface ExplicitContentFrame {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   */
  timeOffset:
    | Duration
    | undefined;
  /** Likelihood of the pornography content.. */
  pornographyLikelihood: Likelihood;
}

/**
 * Explicit content annotation (based on per-frame visual signals only).
 * If no explicit content has been detected in a frame, no annotations are
 * present for that frame.
 */
export interface ExplicitContentAnnotation {
  /** All video frames where explicit content was detected. */
  frames: ExplicitContentFrame[];
  /** Feature version. */
  version: string;
}

/**
 * Normalized bounding box.
 * The normalized vertex coordinates are relative to the original image.
 * Range: [0, 1].
 */
export interface NormalizedBoundingBox {
  /** Left X coordinate. */
  left: number;
  /** Top Y coordinate. */
  top: number;
  /** Right X coordinate. */
  right: number;
  /** Bottom Y coordinate. */
  bottom: number;
}

/** Face detection annotation. */
export interface FaceDetectionAnnotation {
  /** The face tracks with attributes. */
  tracks: Track[];
  /** The thumbnail of a person's face. */
  thumbnail: Buffer;
  /** Feature version. */
  version: string;
}

/** Person detection annotation per video. */
export interface PersonDetectionAnnotation {
  /** The detected tracks of a person. */
  tracks: Track[];
  /** Feature version. */
  version: string;
}

/** Video segment level annotation results for face detection. */
export interface FaceSegment {
  /** Video segment where a face was detected. */
  segment: VideoSegment | undefined;
}

/**
 * Deprecated. No effect.
 *
 * @deprecated
 */
export interface FaceFrame {
  /**
   * Normalized Bounding boxes in a frame.
   * There can be more than one boxes if the same face is detected in multiple
   * locations within the current frame.
   */
  normalizedBoundingBoxes: NormalizedBoundingBox[];
  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the video frame for this location.
   */
  timeOffset: Duration | undefined;
}

/**
 * Deprecated. No effect.
 *
 * @deprecated
 */
export interface FaceAnnotation {
  /** Thumbnail of a representative face view (in JPEG format). */
  thumbnail: Buffer;
  /** All video segments where a face was detected. */
  segments: FaceSegment[];
  /** All video frames where a face was detected. */
  frames: FaceFrame[];
}

/**
 * For tracking related features.
 * An object at time_offset with attributes, and located with
 * normalized_bounding_box.
 */
export interface TimestampedObject {
  /** Normalized Bounding box in a frame, where the object is located. */
  normalizedBoundingBox:
    | NormalizedBoundingBox
    | undefined;
  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the video frame for this object.
   */
  timeOffset:
    | Duration
    | undefined;
  /** Optional. The attributes of the object in the bounding box. */
  attributes: DetectedAttribute[];
  /** Optional. The detected landmarks. */
  landmarks: DetectedLandmark[];
}

/** A track of an object instance. */
export interface Track {
  /** Video segment of a track. */
  segment:
    | VideoSegment
    | undefined;
  /** The object with timestamp and attributes per frame in the track. */
  timestampedObjects: TimestampedObject[];
  /** Optional. Attributes in the track level. */
  attributes: DetectedAttribute[];
  /** Optional. The confidence score of the tracked object. */
  confidence: number;
}

/** A generic detected attribute represented by name in string format. */
export interface DetectedAttribute {
  /**
   * The name of the attribute, for example, glasses, dark_glasses, mouth_open.
   * A full list of supported type names will be provided in the document.
   */
  name: string;
  /** Detected attribute confidence. Range [0, 1]. */
  confidence: number;
  /**
   * Text value of the detection result. For example, the value for "HairColor"
   * can be "black", "blonde", etc.
   */
  value: string;
}

/**
 * A generic detected landmark represented by name in string format and a 2D
 * location.
 */
export interface DetectedLandmark {
  /** The name of this landmark, for example, left_hand, right_shoulder. */
  name: string;
  /**
   * The 2D point of the detected landmark using the normalized image
   * coordindate system. The normalized coordinates have the range from 0 to 1.
   */
  point:
    | NormalizedVertex
    | undefined;
  /** The confidence score of the detected landmark. Range [0, 1]. */
  confidence: number;
}

/** Annotation results for a single video. */
export interface VideoAnnotationResults {
  /**
   * Video file location in
   * [Cloud Storage](https://cloud.google.com/storage/).
   */
  inputUri: string;
  /** Video segment on which the annotation is run. */
  segment:
    | VideoSegment
    | undefined;
  /**
   * Topical label annotations on video level or user-specified segment level.
   * There is exactly one element for each unique label.
   */
  segmentLabelAnnotations: LabelAnnotation[];
  /**
   * Presence label annotations on video level or user-specified segment level.
   * There is exactly one element for each unique label. Compared to the
   * existing topical `segment_label_annotations`, this field presents more
   * fine-grained, segment-level labels detected in video content and is made
   * available only when the client sets `LabelDetectionConfig.model` to
   * "builtin/latest" in the request.
   */
  segmentPresenceLabelAnnotations: LabelAnnotation[];
  /**
   * Topical label annotations on shot level.
   * There is exactly one element for each unique label.
   */
  shotLabelAnnotations: LabelAnnotation[];
  /**
   * Presence label annotations on shot level. There is exactly one element for
   * each unique label. Compared to the existing topical
   * `shot_label_annotations`, this field presents more fine-grained, shot-level
   * labels detected in video content and is made available only when the client
   * sets `LabelDetectionConfig.model` to "builtin/latest" in the request.
   */
  shotPresenceLabelAnnotations: LabelAnnotation[];
  /**
   * Label annotations on frame level.
   * There is exactly one element for each unique label.
   */
  frameLabelAnnotations: LabelAnnotation[];
  /**
   * Deprecated. Please use `face_detection_annotations` instead.
   *
   * @deprecated
   */
  faceAnnotations: FaceAnnotation[];
  /** Face detection annotations. */
  faceDetectionAnnotations: FaceDetectionAnnotation[];
  /** Shot annotations. Each shot is represented as a video segment. */
  shotAnnotations: VideoSegment[];
  /** Explicit content annotation. */
  explicitAnnotation:
    | ExplicitContentAnnotation
    | undefined;
  /** Speech transcription. */
  speechTranscriptions: SpeechTranscription[];
  /**
   * OCR text detection and tracking.
   * Annotations for list of detected text snippets. Each will have list of
   * frame information associated with it.
   */
  textAnnotations: TextAnnotation[];
  /** Annotations for list of objects detected and tracked in video. */
  objectAnnotations: ObjectTrackingAnnotation[];
  /** Annotations for list of logos detected, tracked and recognized in video. */
  logoRecognitionAnnotations: LogoRecognitionAnnotation[];
  /** Person detection annotations. */
  personDetectionAnnotations: PersonDetectionAnnotation[];
  /**
   * If set, indicates an error. Note that for a single `AnnotateVideoRequest`
   * some videos may succeed and some may fail.
   */
  error: Status | undefined;
}

/**
 * Video annotation response. Included in the `response`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 */
export interface AnnotateVideoResponse {
  /** Annotation results for all videos specified in `AnnotateVideoRequest`. */
  annotationResults: VideoAnnotationResults[];
}

/** Annotation progress for a single video. */
export interface VideoAnnotationProgress {
  /**
   * Video file location in
   * [Cloud Storage](https://cloud.google.com/storage/).
   */
  inputUri: string;
  /**
   * Approximate percentage processed thus far. Guaranteed to be
   * 100 when fully processed.
   */
  progressPercent: number;
  /** Time when the request was received. */
  startTime:
    | Date
    | undefined;
  /** Time of the most recent update. */
  updateTime:
    | Date
    | undefined;
  /**
   * Specifies which feature is being tracked if the request contains more than
   * one feature.
   */
  feature: Feature;
  /**
   * Specifies which segment is being tracked if the request contains more than
   * one segment.
   */
  segment: VideoSegment | undefined;
}

/**
 * Video annotation progress. Included in the `metadata`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 */
export interface AnnotateVideoProgress {
  /** Progress metadata for all videos specified in `AnnotateVideoRequest`. */
  annotationProgress: VideoAnnotationProgress[];
}

/** Config for SPEECH_TRANSCRIPTION. */
export interface SpeechTranscriptionConfig {
  /**
   * Required. *Required* The language of the supplied audio as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Example: "en-US".
   * See [Language Support](https://cloud.google.com/speech/docs/languages)
   * for a list of the currently supported language codes.
   */
  languageCode: string;
  /**
   * Optional. Maximum number of recognition hypotheses to be returned.
   * Specifically, the maximum number of `SpeechRecognitionAlternative` messages
   * within each `SpeechTranscription`. The server may return fewer than
   * `max_alternatives`. Valid values are `0`-`30`. A value of `0` or `1` will
   * return a maximum of one. If omitted, will return a maximum of one.
   */
  maxAlternatives: number;
  /**
   * Optional. If set to `true`, the server will attempt to filter out
   * profanities, replacing all but the initial character in each filtered word
   * with asterisks, e.g. "f***". If set to `false` or omitted, profanities
   * won't be filtered out.
   */
  filterProfanity: boolean;
  /** Optional. A means to provide context to assist the speech recognition. */
  speechContexts: SpeechContext[];
  /**
   * Optional. If 'true', adds punctuation to recognition result hypotheses.
   * This feature is only available in select languages. Setting this for
   * requests in other languages has no effect at all. The default 'false' value
   * does not add punctuation to result hypotheses. NOTE: "This is currently
   * offered as an experimental service, complimentary to all users. In the
   * future this may be exclusively available as a premium feature."
   */
  enableAutomaticPunctuation: boolean;
  /**
   * Optional. For file formats, such as MXF or MKV, supporting multiple audio
   * tracks, specify up to two tracks. Default: track 0.
   */
  audioTracks: number[];
  /**
   * Optional. If 'true', enables speaker detection for each recognized word in
   * the top alternative of the recognition result using a speaker_tag provided
   * in the WordInfo.
   * Note: When this is true, we send all the words from the beginning of the
   * audio for the top alternative in every consecutive response.
   * This is done in order to improve our speaker tags as our models learn to
   * identify the speakers in the conversation over time.
   */
  enableSpeakerDiarization: boolean;
  /**
   * Optional. If set, specifies the estimated number of speakers in the
   * conversation. If not set, defaults to '2'. Ignored unless
   * enable_speaker_diarization is set to true.
   */
  diarizationSpeakerCount: number;
  /**
   * Optional. If `true`, the top result includes a list of words and the
   * confidence for those words. If `false`, no word-level confidence
   * information is returned. The default is `false`.
   */
  enableWordConfidence: boolean;
}

/**
 * Provides "hints" to the speech recognizer to favor specific words and phrases
 * in the results.
 */
export interface SpeechContext {
  /**
   * Optional. A list of strings containing words and phrases "hints" so that
   * the speech recognition is more likely to recognize them. This can be used
   * to improve the accuracy for specific words and phrases, for example, if
   * specific commands are typically spoken by the user. This can also be used
   * to add additional words to the vocabulary of the recognizer. See
   * [usage limits](https://cloud.google.com/speech/limits#content).
   */
  phrases: string[];
}

/** A speech recognition result corresponding to a portion of the audio. */
export interface SpeechTranscription {
  /**
   * May contain one or more recognition hypotheses (up to the maximum specified
   * in `max_alternatives`).  These alternatives are ordered in terms of
   * accuracy, with the top (first) alternative being the most probable, as
   * ranked by the recognizer.
   */
  alternatives: SpeechRecognitionAlternative[];
  /**
   * Output only. The [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt)
   * language tag of the language in this result. This language code was
   * detected to have the most likelihood of being spoken in the audio.
   */
  languageCode: string;
}

/** Alternative hypotheses (a.k.a. n-best list). */
export interface SpeechRecognitionAlternative {
  /** Transcript text representing the words that the user spoke. */
  transcript: string;
  /**
   * Output only. The confidence estimate between 0.0 and 1.0. A higher number
   * indicates an estimated greater likelihood that the recognized words are
   * correct. This field is set only for the top alternative.
   * This field is not guaranteed to be accurate and users should not rely on it
   * to be always provided.
   * The default of 0.0 is a sentinel value indicating `confidence` was not set.
   */
  confidence: number;
  /**
   * Output only. A list of word-specific information for each recognized word.
   * Note: When `enable_speaker_diarization` is set to true, you will see all
   * the words from the beginning of the audio.
   */
  words: WordInfo[];
}

/**
 * Word-specific information for recognized words. Word information is only
 * included in the response when certain request parameters are set, such
 * as `enable_word_time_offsets`.
 */
export interface WordInfo {
  /**
   * Time offset relative to the beginning of the audio, and
   * corresponding to the start of the spoken word. This field is only set if
   * `enable_word_time_offsets=true` and only in the top hypothesis. This is an
   * experimental feature and the accuracy of the time offset can vary.
   */
  startTime:
    | Duration
    | undefined;
  /**
   * Time offset relative to the beginning of the audio, and
   * corresponding to the end of the spoken word. This field is only set if
   * `enable_word_time_offsets=true` and only in the top hypothesis. This is an
   * experimental feature and the accuracy of the time offset can vary.
   */
  endTime:
    | Duration
    | undefined;
  /** The word corresponding to this set of information. */
  word: string;
  /**
   * Output only. The confidence estimate between 0.0 and 1.0. A higher number
   * indicates an estimated greater likelihood that the recognized words are
   * correct. This field is set only for the top alternative.
   * This field is not guaranteed to be accurate and users should not rely on it
   * to be always provided.
   * The default of 0.0 is a sentinel value indicating `confidence` was not set.
   */
  confidence: number;
  /**
   * Output only. A distinct integer value is assigned for every speaker within
   * the audio. This field specifies which one of those speakers was detected to
   * have spoken this word. Value ranges from 1 up to diarization_speaker_count,
   * and is only set if speaker diarization is enabled.
   */
  speakerTag: number;
}

/**
 * A vertex represents a 2D point in the image.
 * NOTE: the normalized vertex coordinates are relative to the original image
 * and range from 0 to 1.
 */
export interface NormalizedVertex {
  /** X coordinate. */
  x: number;
  /** Y coordinate. */
  y: number;
}

/**
 * Normalized bounding polygon for text (that might not be aligned with axis).
 * Contains list of the corner points in clockwise order starting from
 * top-left corner. For example, for a rectangular bounding box:
 * When the text is horizontal it might look like:
 *         0----1
 *         |    |
 *         3----2
 *
 * When it's clockwise rotated 180 degrees around the top-left corner it
 * becomes:
 *         2----3
 *         |    |
 *         1----0
 *
 * and the vertex order will still be (0, 1, 2, 3). Note that values can be less
 * than 0, or greater than 1 due to trignometric calculations for location of
 * the box.
 */
export interface NormalizedBoundingPoly {
  /** Normalized vertices of the bounding polygon. */
  vertices: NormalizedVertex[];
}

/** Video segment level annotation results for text detection. */
export interface TextSegment {
  /** Video segment where a text snippet was detected. */
  segment:
    | VideoSegment
    | undefined;
  /**
   * Confidence for the track of detected text. It is calculated as the highest
   * over all frames where OCR detected text appears.
   */
  confidence: number;
  /** Information related to the frames where OCR detected text appears. */
  frames: TextFrame[];
}

/**
 * Video frame level annotation results for text annotation (OCR).
 * Contains information regarding timestamp and bounding box locations for the
 * frames containing detected OCR text snippets.
 */
export interface TextFrame {
  /** Bounding polygon of the detected text for this frame. */
  rotatedBoundingBox:
    | NormalizedBoundingPoly
    | undefined;
  /** Timestamp of this frame. */
  timeOffset: Duration | undefined;
}

/**
 * Annotations related to one detected OCR text snippet. This will contain the
 * corresponding text, confidence value, and frame level information for each
 * detection.
 */
export interface TextAnnotation {
  /** The detected text. */
  text: string;
  /** All video segments where OCR detected text appears. */
  segments: TextSegment[];
  /** Feature version. */
  version: string;
}

/**
 * Video frame level annotations for object detection and tracking. This field
 * stores per frame location, time offset, and confidence.
 */
export interface ObjectTrackingFrame {
  /** The normalized bounding box location of this object track for the frame. */
  normalizedBoundingBox:
    | NormalizedBoundingBox
    | undefined;
  /** The timestamp of the frame in microseconds. */
  timeOffset: Duration | undefined;
}

/** Annotations corresponding to one tracked object. */
export interface ObjectTrackingAnnotation {
  /**
   * Non-streaming batch mode ONLY.
   * Each object track corresponds to one video segment where it appears.
   */
  segment?:
    | VideoSegment
    | undefined;
  /**
   * Streaming mode ONLY.
   * In streaming mode, we do not know the end time of a tracked object
   * before it is completed. Hence, there is no VideoSegment info returned.
   * Instead, we provide a unique identifiable integer track_id so that
   * the customers can correlate the results of the ongoing
   * ObjectTrackAnnotation of the same track_id over time.
   */
  trackId?:
    | Long
    | undefined;
  /** Entity to specify the object category that this track is labeled as. */
  entity:
    | Entity
    | undefined;
  /** Object category's labeling confidence of this track. */
  confidence: number;
  /**
   * Information corresponding to all frames where this object track appears.
   * Non-streaming batch mode: it may be one or multiple ObjectTrackingFrame
   * messages in frames.
   * Streaming mode: it can only be one ObjectTrackingFrame message in frames.
   */
  frames: ObjectTrackingFrame[];
  /** Feature version. */
  version: string;
}

/** Annotation corresponding to one detected, tracked and recognized logo class. */
export interface LogoRecognitionAnnotation {
  /**
   * Entity category information to specify the logo class that all the logo
   * tracks within this LogoRecognitionAnnotation are recognized as.
   */
  entity:
    | Entity
    | undefined;
  /**
   * All logo tracks where the recognized logo appears. Each track corresponds
   * to one logo instance appearing in consecutive frames.
   */
  tracks: Track[];
  /**
   * All video segments where the recognized logo appears. There might be
   * multiple instances of the same logo class appearing in one VideoSegment.
   */
  segments: VideoSegment[];
}

function createBaseAnnotateVideoRequest(): AnnotateVideoRequest {
  return {
    inputUri: "",
    inputContent: Buffer.alloc(0),
    features: [],
    videoContext: undefined,
    outputUri: "",
    locationId: "",
  };
}

export const AnnotateVideoRequest: MessageFns<AnnotateVideoRequest> = {
  encode(message: AnnotateVideoRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputUri !== "") {
      writer.uint32(10).string(message.inputUri);
    }
    if (message.inputContent.length !== 0) {
      writer.uint32(50).bytes(message.inputContent);
    }
    writer.uint32(18).fork();
    for (const v of message.features) {
      writer.int32(v);
    }
    writer.join();
    if (message.videoContext !== undefined) {
      VideoContext.encode(message.videoContext, writer.uint32(26).fork()).join();
    }
    if (message.outputUri !== "") {
      writer.uint32(34).string(message.outputUri);
    }
    if (message.locationId !== "") {
      writer.uint32(42).string(message.locationId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnnotateVideoRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnnotateVideoRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputUri = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.inputContent = Buffer.from(reader.bytes());
          continue;
        case 2:
          if (tag === 16) {
            message.features.push(reader.int32() as any);

            continue;
          }

          if (tag === 18) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.features.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.videoContext = VideoContext.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.outputUri = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.locationId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnnotateVideoRequest {
    return {
      inputUri: isSet(object.inputUri) ? globalThis.String(object.inputUri) : "",
      inputContent: isSet(object.inputContent) ? Buffer.from(bytesFromBase64(object.inputContent)) : Buffer.alloc(0),
      features: globalThis.Array.isArray(object?.features) ? object.features.map((e: any) => featureFromJSON(e)) : [],
      videoContext: isSet(object.videoContext) ? VideoContext.fromJSON(object.videoContext) : undefined,
      outputUri: isSet(object.outputUri) ? globalThis.String(object.outputUri) : "",
      locationId: isSet(object.locationId) ? globalThis.String(object.locationId) : "",
    };
  },

  toJSON(message: AnnotateVideoRequest): unknown {
    const obj: any = {};
    if (message.inputUri !== "") {
      obj.inputUri = message.inputUri;
    }
    if (message.inputContent.length !== 0) {
      obj.inputContent = base64FromBytes(message.inputContent);
    }
    if (message.features?.length) {
      obj.features = message.features.map((e) => featureToJSON(e));
    }
    if (message.videoContext !== undefined) {
      obj.videoContext = VideoContext.toJSON(message.videoContext);
    }
    if (message.outputUri !== "") {
      obj.outputUri = message.outputUri;
    }
    if (message.locationId !== "") {
      obj.locationId = message.locationId;
    }
    return obj;
  },

  create(base?: DeepPartial<AnnotateVideoRequest>): AnnotateVideoRequest {
    return AnnotateVideoRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AnnotateVideoRequest>): AnnotateVideoRequest {
    const message = createBaseAnnotateVideoRequest();
    message.inputUri = object.inputUri ?? "";
    message.inputContent = object.inputContent ?? Buffer.alloc(0);
    message.features = object.features?.map((e) => e) || [];
    message.videoContext = (object.videoContext !== undefined && object.videoContext !== null)
      ? VideoContext.fromPartial(object.videoContext)
      : undefined;
    message.outputUri = object.outputUri ?? "";
    message.locationId = object.locationId ?? "";
    return message;
  },
};

function createBaseVideoContext(): VideoContext {
  return {
    segments: [],
    labelDetectionConfig: undefined,
    shotChangeDetectionConfig: undefined,
    explicitContentDetectionConfig: undefined,
    faceDetectionConfig: undefined,
    speechTranscriptionConfig: undefined,
    textDetectionConfig: undefined,
    personDetectionConfig: undefined,
    objectTrackingConfig: undefined,
  };
}

export const VideoContext: MessageFns<VideoContext> = {
  encode(message: VideoContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.segments) {
      VideoSegment.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.labelDetectionConfig !== undefined) {
      LabelDetectionConfig.encode(message.labelDetectionConfig, writer.uint32(18).fork()).join();
    }
    if (message.shotChangeDetectionConfig !== undefined) {
      ShotChangeDetectionConfig.encode(message.shotChangeDetectionConfig, writer.uint32(26).fork()).join();
    }
    if (message.explicitContentDetectionConfig !== undefined) {
      ExplicitContentDetectionConfig.encode(message.explicitContentDetectionConfig, writer.uint32(34).fork()).join();
    }
    if (message.faceDetectionConfig !== undefined) {
      FaceDetectionConfig.encode(message.faceDetectionConfig, writer.uint32(42).fork()).join();
    }
    if (message.speechTranscriptionConfig !== undefined) {
      SpeechTranscriptionConfig.encode(message.speechTranscriptionConfig, writer.uint32(50).fork()).join();
    }
    if (message.textDetectionConfig !== undefined) {
      TextDetectionConfig.encode(message.textDetectionConfig, writer.uint32(66).fork()).join();
    }
    if (message.personDetectionConfig !== undefined) {
      PersonDetectionConfig.encode(message.personDetectionConfig, writer.uint32(90).fork()).join();
    }
    if (message.objectTrackingConfig !== undefined) {
      ObjectTrackingConfig.encode(message.objectTrackingConfig, writer.uint32(106).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VideoContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVideoContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.segments.push(VideoSegment.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.labelDetectionConfig = LabelDetectionConfig.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.shotChangeDetectionConfig = ShotChangeDetectionConfig.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.explicitContentDetectionConfig = ExplicitContentDetectionConfig.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.faceDetectionConfig = FaceDetectionConfig.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.speechTranscriptionConfig = SpeechTranscriptionConfig.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.textDetectionConfig = TextDetectionConfig.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.personDetectionConfig = PersonDetectionConfig.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.objectTrackingConfig = ObjectTrackingConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VideoContext {
    return {
      segments: globalThis.Array.isArray(object?.segments)
        ? object.segments.map((e: any) => VideoSegment.fromJSON(e))
        : [],
      labelDetectionConfig: isSet(object.labelDetectionConfig)
        ? LabelDetectionConfig.fromJSON(object.labelDetectionConfig)
        : undefined,
      shotChangeDetectionConfig: isSet(object.shotChangeDetectionConfig)
        ? ShotChangeDetectionConfig.fromJSON(object.shotChangeDetectionConfig)
        : undefined,
      explicitContentDetectionConfig: isSet(object.explicitContentDetectionConfig)
        ? ExplicitContentDetectionConfig.fromJSON(object.explicitContentDetectionConfig)
        : undefined,
      faceDetectionConfig: isSet(object.faceDetectionConfig)
        ? FaceDetectionConfig.fromJSON(object.faceDetectionConfig)
        : undefined,
      speechTranscriptionConfig: isSet(object.speechTranscriptionConfig)
        ? SpeechTranscriptionConfig.fromJSON(object.speechTranscriptionConfig)
        : undefined,
      textDetectionConfig: isSet(object.textDetectionConfig)
        ? TextDetectionConfig.fromJSON(object.textDetectionConfig)
        : undefined,
      personDetectionConfig: isSet(object.personDetectionConfig)
        ? PersonDetectionConfig.fromJSON(object.personDetectionConfig)
        : undefined,
      objectTrackingConfig: isSet(object.objectTrackingConfig)
        ? ObjectTrackingConfig.fromJSON(object.objectTrackingConfig)
        : undefined,
    };
  },

  toJSON(message: VideoContext): unknown {
    const obj: any = {};
    if (message.segments?.length) {
      obj.segments = message.segments.map((e) => VideoSegment.toJSON(e));
    }
    if (message.labelDetectionConfig !== undefined) {
      obj.labelDetectionConfig = LabelDetectionConfig.toJSON(message.labelDetectionConfig);
    }
    if (message.shotChangeDetectionConfig !== undefined) {
      obj.shotChangeDetectionConfig = ShotChangeDetectionConfig.toJSON(message.shotChangeDetectionConfig);
    }
    if (message.explicitContentDetectionConfig !== undefined) {
      obj.explicitContentDetectionConfig = ExplicitContentDetectionConfig.toJSON(
        message.explicitContentDetectionConfig,
      );
    }
    if (message.faceDetectionConfig !== undefined) {
      obj.faceDetectionConfig = FaceDetectionConfig.toJSON(message.faceDetectionConfig);
    }
    if (message.speechTranscriptionConfig !== undefined) {
      obj.speechTranscriptionConfig = SpeechTranscriptionConfig.toJSON(message.speechTranscriptionConfig);
    }
    if (message.textDetectionConfig !== undefined) {
      obj.textDetectionConfig = TextDetectionConfig.toJSON(message.textDetectionConfig);
    }
    if (message.personDetectionConfig !== undefined) {
      obj.personDetectionConfig = PersonDetectionConfig.toJSON(message.personDetectionConfig);
    }
    if (message.objectTrackingConfig !== undefined) {
      obj.objectTrackingConfig = ObjectTrackingConfig.toJSON(message.objectTrackingConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<VideoContext>): VideoContext {
    return VideoContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VideoContext>): VideoContext {
    const message = createBaseVideoContext();
    message.segments = object.segments?.map((e) => VideoSegment.fromPartial(e)) || [];
    message.labelDetectionConfig = (object.labelDetectionConfig !== undefined && object.labelDetectionConfig !== null)
      ? LabelDetectionConfig.fromPartial(object.labelDetectionConfig)
      : undefined;
    message.shotChangeDetectionConfig =
      (object.shotChangeDetectionConfig !== undefined && object.shotChangeDetectionConfig !== null)
        ? ShotChangeDetectionConfig.fromPartial(object.shotChangeDetectionConfig)
        : undefined;
    message.explicitContentDetectionConfig =
      (object.explicitContentDetectionConfig !== undefined && object.explicitContentDetectionConfig !== null)
        ? ExplicitContentDetectionConfig.fromPartial(object.explicitContentDetectionConfig)
        : undefined;
    message.faceDetectionConfig = (object.faceDetectionConfig !== undefined && object.faceDetectionConfig !== null)
      ? FaceDetectionConfig.fromPartial(object.faceDetectionConfig)
      : undefined;
    message.speechTranscriptionConfig =
      (object.speechTranscriptionConfig !== undefined && object.speechTranscriptionConfig !== null)
        ? SpeechTranscriptionConfig.fromPartial(object.speechTranscriptionConfig)
        : undefined;
    message.textDetectionConfig = (object.textDetectionConfig !== undefined && object.textDetectionConfig !== null)
      ? TextDetectionConfig.fromPartial(object.textDetectionConfig)
      : undefined;
    message.personDetectionConfig =
      (object.personDetectionConfig !== undefined && object.personDetectionConfig !== null)
        ? PersonDetectionConfig.fromPartial(object.personDetectionConfig)
        : undefined;
    message.objectTrackingConfig = (object.objectTrackingConfig !== undefined && object.objectTrackingConfig !== null)
      ? ObjectTrackingConfig.fromPartial(object.objectTrackingConfig)
      : undefined;
    return message;
  },
};

function createBaseLabelDetectionConfig(): LabelDetectionConfig {
  return {
    labelDetectionMode: 0,
    stationaryCamera: false,
    model: "",
    frameConfidenceThreshold: 0,
    videoConfidenceThreshold: 0,
  };
}

export const LabelDetectionConfig: MessageFns<LabelDetectionConfig> = {
  encode(message: LabelDetectionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.labelDetectionMode !== 0) {
      writer.uint32(8).int32(message.labelDetectionMode);
    }
    if (message.stationaryCamera !== false) {
      writer.uint32(16).bool(message.stationaryCamera);
    }
    if (message.model !== "") {
      writer.uint32(26).string(message.model);
    }
    if (message.frameConfidenceThreshold !== 0) {
      writer.uint32(37).float(message.frameConfidenceThreshold);
    }
    if (message.videoConfidenceThreshold !== 0) {
      writer.uint32(45).float(message.videoConfidenceThreshold);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LabelDetectionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLabelDetectionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.labelDetectionMode = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.stationaryCamera = reader.bool();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.model = reader.string();
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.frameConfidenceThreshold = reader.float();
          continue;
        case 5:
          if (tag !== 45) {
            break;
          }

          message.videoConfidenceThreshold = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LabelDetectionConfig {
    return {
      labelDetectionMode: isSet(object.labelDetectionMode) ? labelDetectionModeFromJSON(object.labelDetectionMode) : 0,
      stationaryCamera: isSet(object.stationaryCamera) ? globalThis.Boolean(object.stationaryCamera) : false,
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      frameConfidenceThreshold: isSet(object.frameConfidenceThreshold)
        ? globalThis.Number(object.frameConfidenceThreshold)
        : 0,
      videoConfidenceThreshold: isSet(object.videoConfidenceThreshold)
        ? globalThis.Number(object.videoConfidenceThreshold)
        : 0,
    };
  },

  toJSON(message: LabelDetectionConfig): unknown {
    const obj: any = {};
    if (message.labelDetectionMode !== 0) {
      obj.labelDetectionMode = labelDetectionModeToJSON(message.labelDetectionMode);
    }
    if (message.stationaryCamera !== false) {
      obj.stationaryCamera = message.stationaryCamera;
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.frameConfidenceThreshold !== 0) {
      obj.frameConfidenceThreshold = message.frameConfidenceThreshold;
    }
    if (message.videoConfidenceThreshold !== 0) {
      obj.videoConfidenceThreshold = message.videoConfidenceThreshold;
    }
    return obj;
  },

  create(base?: DeepPartial<LabelDetectionConfig>): LabelDetectionConfig {
    return LabelDetectionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LabelDetectionConfig>): LabelDetectionConfig {
    const message = createBaseLabelDetectionConfig();
    message.labelDetectionMode = object.labelDetectionMode ?? 0;
    message.stationaryCamera = object.stationaryCamera ?? false;
    message.model = object.model ?? "";
    message.frameConfidenceThreshold = object.frameConfidenceThreshold ?? 0;
    message.videoConfidenceThreshold = object.videoConfidenceThreshold ?? 0;
    return message;
  },
};

function createBaseShotChangeDetectionConfig(): ShotChangeDetectionConfig {
  return { model: "" };
}

export const ShotChangeDetectionConfig: MessageFns<ShotChangeDetectionConfig> = {
  encode(message: ShotChangeDetectionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ShotChangeDetectionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseShotChangeDetectionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ShotChangeDetectionConfig {
    return { model: isSet(object.model) ? globalThis.String(object.model) : "" };
  },

  toJSON(message: ShotChangeDetectionConfig): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    return obj;
  },

  create(base?: DeepPartial<ShotChangeDetectionConfig>): ShotChangeDetectionConfig {
    return ShotChangeDetectionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ShotChangeDetectionConfig>): ShotChangeDetectionConfig {
    const message = createBaseShotChangeDetectionConfig();
    message.model = object.model ?? "";
    return message;
  },
};

function createBaseObjectTrackingConfig(): ObjectTrackingConfig {
  return { model: "" };
}

export const ObjectTrackingConfig: MessageFns<ObjectTrackingConfig> = {
  encode(message: ObjectTrackingConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ObjectTrackingConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseObjectTrackingConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ObjectTrackingConfig {
    return { model: isSet(object.model) ? globalThis.String(object.model) : "" };
  },

  toJSON(message: ObjectTrackingConfig): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    return obj;
  },

  create(base?: DeepPartial<ObjectTrackingConfig>): ObjectTrackingConfig {
    return ObjectTrackingConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ObjectTrackingConfig>): ObjectTrackingConfig {
    const message = createBaseObjectTrackingConfig();
    message.model = object.model ?? "";
    return message;
  },
};

function createBaseFaceDetectionConfig(): FaceDetectionConfig {
  return { model: "", includeBoundingBoxes: false, includeAttributes: false };
}

export const FaceDetectionConfig: MessageFns<FaceDetectionConfig> = {
  encode(message: FaceDetectionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    if (message.includeBoundingBoxes !== false) {
      writer.uint32(16).bool(message.includeBoundingBoxes);
    }
    if (message.includeAttributes !== false) {
      writer.uint32(40).bool(message.includeAttributes);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FaceDetectionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFaceDetectionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.includeBoundingBoxes = reader.bool();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.includeAttributes = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FaceDetectionConfig {
    return {
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      includeBoundingBoxes: isSet(object.includeBoundingBoxes)
        ? globalThis.Boolean(object.includeBoundingBoxes)
        : false,
      includeAttributes: isSet(object.includeAttributes) ? globalThis.Boolean(object.includeAttributes) : false,
    };
  },

  toJSON(message: FaceDetectionConfig): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.includeBoundingBoxes !== false) {
      obj.includeBoundingBoxes = message.includeBoundingBoxes;
    }
    if (message.includeAttributes !== false) {
      obj.includeAttributes = message.includeAttributes;
    }
    return obj;
  },

  create(base?: DeepPartial<FaceDetectionConfig>): FaceDetectionConfig {
    return FaceDetectionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FaceDetectionConfig>): FaceDetectionConfig {
    const message = createBaseFaceDetectionConfig();
    message.model = object.model ?? "";
    message.includeBoundingBoxes = object.includeBoundingBoxes ?? false;
    message.includeAttributes = object.includeAttributes ?? false;
    return message;
  },
};

function createBasePersonDetectionConfig(): PersonDetectionConfig {
  return { includeBoundingBoxes: false, includePoseLandmarks: false, includeAttributes: false };
}

export const PersonDetectionConfig: MessageFns<PersonDetectionConfig> = {
  encode(message: PersonDetectionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.includeBoundingBoxes !== false) {
      writer.uint32(8).bool(message.includeBoundingBoxes);
    }
    if (message.includePoseLandmarks !== false) {
      writer.uint32(16).bool(message.includePoseLandmarks);
    }
    if (message.includeAttributes !== false) {
      writer.uint32(24).bool(message.includeAttributes);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PersonDetectionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePersonDetectionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.includeBoundingBoxes = reader.bool();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.includePoseLandmarks = reader.bool();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.includeAttributes = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PersonDetectionConfig {
    return {
      includeBoundingBoxes: isSet(object.includeBoundingBoxes)
        ? globalThis.Boolean(object.includeBoundingBoxes)
        : false,
      includePoseLandmarks: isSet(object.includePoseLandmarks)
        ? globalThis.Boolean(object.includePoseLandmarks)
        : false,
      includeAttributes: isSet(object.includeAttributes) ? globalThis.Boolean(object.includeAttributes) : false,
    };
  },

  toJSON(message: PersonDetectionConfig): unknown {
    const obj: any = {};
    if (message.includeBoundingBoxes !== false) {
      obj.includeBoundingBoxes = message.includeBoundingBoxes;
    }
    if (message.includePoseLandmarks !== false) {
      obj.includePoseLandmarks = message.includePoseLandmarks;
    }
    if (message.includeAttributes !== false) {
      obj.includeAttributes = message.includeAttributes;
    }
    return obj;
  },

  create(base?: DeepPartial<PersonDetectionConfig>): PersonDetectionConfig {
    return PersonDetectionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PersonDetectionConfig>): PersonDetectionConfig {
    const message = createBasePersonDetectionConfig();
    message.includeBoundingBoxes = object.includeBoundingBoxes ?? false;
    message.includePoseLandmarks = object.includePoseLandmarks ?? false;
    message.includeAttributes = object.includeAttributes ?? false;
    return message;
  },
};

function createBaseExplicitContentDetectionConfig(): ExplicitContentDetectionConfig {
  return { model: "" };
}

export const ExplicitContentDetectionConfig: MessageFns<ExplicitContentDetectionConfig> = {
  encode(message: ExplicitContentDetectionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplicitContentDetectionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplicitContentDetectionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplicitContentDetectionConfig {
    return { model: isSet(object.model) ? globalThis.String(object.model) : "" };
  },

  toJSON(message: ExplicitContentDetectionConfig): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    return obj;
  },

  create(base?: DeepPartial<ExplicitContentDetectionConfig>): ExplicitContentDetectionConfig {
    return ExplicitContentDetectionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplicitContentDetectionConfig>): ExplicitContentDetectionConfig {
    const message = createBaseExplicitContentDetectionConfig();
    message.model = object.model ?? "";
    return message;
  },
};

function createBaseTextDetectionConfig(): TextDetectionConfig {
  return { languageHints: [], model: "" };
}

export const TextDetectionConfig: MessageFns<TextDetectionConfig> = {
  encode(message: TextDetectionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.languageHints) {
      writer.uint32(10).string(v!);
    }
    if (message.model !== "") {
      writer.uint32(18).string(message.model);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TextDetectionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextDetectionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.languageHints.push(reader.string());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.model = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextDetectionConfig {
    return {
      languageHints: globalThis.Array.isArray(object?.languageHints)
        ? object.languageHints.map((e: any) => globalThis.String(e))
        : [],
      model: isSet(object.model) ? globalThis.String(object.model) : "",
    };
  },

  toJSON(message: TextDetectionConfig): unknown {
    const obj: any = {};
    if (message.languageHints?.length) {
      obj.languageHints = message.languageHints;
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    return obj;
  },

  create(base?: DeepPartial<TextDetectionConfig>): TextDetectionConfig {
    return TextDetectionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TextDetectionConfig>): TextDetectionConfig {
    const message = createBaseTextDetectionConfig();
    message.languageHints = object.languageHints?.map((e) => e) || [];
    message.model = object.model ?? "";
    return message;
  },
};

function createBaseVideoSegment(): VideoSegment {
  return { startTimeOffset: undefined, endTimeOffset: undefined };
}

export const VideoSegment: MessageFns<VideoSegment> = {
  encode(message: VideoSegment, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startTimeOffset !== undefined) {
      Duration.encode(message.startTimeOffset, writer.uint32(10).fork()).join();
    }
    if (message.endTimeOffset !== undefined) {
      Duration.encode(message.endTimeOffset, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VideoSegment {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVideoSegment();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startTimeOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.endTimeOffset = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VideoSegment {
    return {
      startTimeOffset: isSet(object.startTimeOffset) ? Duration.fromJSON(object.startTimeOffset) : undefined,
      endTimeOffset: isSet(object.endTimeOffset) ? Duration.fromJSON(object.endTimeOffset) : undefined,
    };
  },

  toJSON(message: VideoSegment): unknown {
    const obj: any = {};
    if (message.startTimeOffset !== undefined) {
      obj.startTimeOffset = Duration.toJSON(message.startTimeOffset);
    }
    if (message.endTimeOffset !== undefined) {
      obj.endTimeOffset = Duration.toJSON(message.endTimeOffset);
    }
    return obj;
  },

  create(base?: DeepPartial<VideoSegment>): VideoSegment {
    return VideoSegment.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VideoSegment>): VideoSegment {
    const message = createBaseVideoSegment();
    message.startTimeOffset = (object.startTimeOffset !== undefined && object.startTimeOffset !== null)
      ? Duration.fromPartial(object.startTimeOffset)
      : undefined;
    message.endTimeOffset = (object.endTimeOffset !== undefined && object.endTimeOffset !== null)
      ? Duration.fromPartial(object.endTimeOffset)
      : undefined;
    return message;
  },
};

function createBaseLabelSegment(): LabelSegment {
  return { segment: undefined, confidence: 0 };
}

export const LabelSegment: MessageFns<LabelSegment> = {
  encode(message: LabelSegment, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.segment !== undefined) {
      VideoSegment.encode(message.segment, writer.uint32(10).fork()).join();
    }
    if (message.confidence !== 0) {
      writer.uint32(21).float(message.confidence);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LabelSegment {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLabelSegment();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.segment = VideoSegment.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.confidence = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LabelSegment {
    return {
      segment: isSet(object.segment) ? VideoSegment.fromJSON(object.segment) : undefined,
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
    };
  },

  toJSON(message: LabelSegment): unknown {
    const obj: any = {};
    if (message.segment !== undefined) {
      obj.segment = VideoSegment.toJSON(message.segment);
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    return obj;
  },

  create(base?: DeepPartial<LabelSegment>): LabelSegment {
    return LabelSegment.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LabelSegment>): LabelSegment {
    const message = createBaseLabelSegment();
    message.segment = (object.segment !== undefined && object.segment !== null)
      ? VideoSegment.fromPartial(object.segment)
      : undefined;
    message.confidence = object.confidence ?? 0;
    return message;
  },
};

function createBaseLabelFrame(): LabelFrame {
  return { timeOffset: undefined, confidence: 0 };
}

export const LabelFrame: MessageFns<LabelFrame> = {
  encode(message: LabelFrame, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.timeOffset !== undefined) {
      Duration.encode(message.timeOffset, writer.uint32(10).fork()).join();
    }
    if (message.confidence !== 0) {
      writer.uint32(21).float(message.confidence);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LabelFrame {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLabelFrame();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.timeOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.confidence = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LabelFrame {
    return {
      timeOffset: isSet(object.timeOffset) ? Duration.fromJSON(object.timeOffset) : undefined,
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
    };
  },

  toJSON(message: LabelFrame): unknown {
    const obj: any = {};
    if (message.timeOffset !== undefined) {
      obj.timeOffset = Duration.toJSON(message.timeOffset);
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    return obj;
  },

  create(base?: DeepPartial<LabelFrame>): LabelFrame {
    return LabelFrame.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LabelFrame>): LabelFrame {
    const message = createBaseLabelFrame();
    message.timeOffset = (object.timeOffset !== undefined && object.timeOffset !== null)
      ? Duration.fromPartial(object.timeOffset)
      : undefined;
    message.confidence = object.confidence ?? 0;
    return message;
  },
};

function createBaseEntity(): Entity {
  return { entityId: "", description: "", languageCode: "" };
}

export const Entity: MessageFns<Entity> = {
  encode(message: Entity, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.entityId !== "") {
      writer.uint32(10).string(message.entityId);
    }
    if (message.description !== "") {
      writer.uint32(18).string(message.description);
    }
    if (message.languageCode !== "") {
      writer.uint32(26).string(message.languageCode);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Entity {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEntity();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.entityId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.description = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.languageCode = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Entity {
    return {
      entityId: isSet(object.entityId) ? globalThis.String(object.entityId) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
    };
  },

  toJSON(message: Entity): unknown {
    const obj: any = {};
    if (message.entityId !== "") {
      obj.entityId = message.entityId;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    return obj;
  },

  create(base?: DeepPartial<Entity>): Entity {
    return Entity.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Entity>): Entity {
    const message = createBaseEntity();
    message.entityId = object.entityId ?? "";
    message.description = object.description ?? "";
    message.languageCode = object.languageCode ?? "";
    return message;
  },
};

function createBaseLabelAnnotation(): LabelAnnotation {
  return { entity: undefined, categoryEntities: [], segments: [], frames: [], version: "" };
}

export const LabelAnnotation: MessageFns<LabelAnnotation> = {
  encode(message: LabelAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.entity !== undefined) {
      Entity.encode(message.entity, writer.uint32(10).fork()).join();
    }
    for (const v of message.categoryEntities) {
      Entity.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.segments) {
      LabelSegment.encode(v!, writer.uint32(26).fork()).join();
    }
    for (const v of message.frames) {
      LabelFrame.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.version !== "") {
      writer.uint32(42).string(message.version);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LabelAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLabelAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.entity = Entity.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.categoryEntities.push(Entity.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.segments.push(LabelSegment.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.frames.push(LabelFrame.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.version = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LabelAnnotation {
    return {
      entity: isSet(object.entity) ? Entity.fromJSON(object.entity) : undefined,
      categoryEntities: globalThis.Array.isArray(object?.categoryEntities)
        ? object.categoryEntities.map((e: any) => Entity.fromJSON(e))
        : [],
      segments: globalThis.Array.isArray(object?.segments)
        ? object.segments.map((e: any) => LabelSegment.fromJSON(e))
        : [],
      frames: globalThis.Array.isArray(object?.frames) ? object.frames.map((e: any) => LabelFrame.fromJSON(e)) : [],
      version: isSet(object.version) ? globalThis.String(object.version) : "",
    };
  },

  toJSON(message: LabelAnnotation): unknown {
    const obj: any = {};
    if (message.entity !== undefined) {
      obj.entity = Entity.toJSON(message.entity);
    }
    if (message.categoryEntities?.length) {
      obj.categoryEntities = message.categoryEntities.map((e) => Entity.toJSON(e));
    }
    if (message.segments?.length) {
      obj.segments = message.segments.map((e) => LabelSegment.toJSON(e));
    }
    if (message.frames?.length) {
      obj.frames = message.frames.map((e) => LabelFrame.toJSON(e));
    }
    if (message.version !== "") {
      obj.version = message.version;
    }
    return obj;
  },

  create(base?: DeepPartial<LabelAnnotation>): LabelAnnotation {
    return LabelAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LabelAnnotation>): LabelAnnotation {
    const message = createBaseLabelAnnotation();
    message.entity = (object.entity !== undefined && object.entity !== null)
      ? Entity.fromPartial(object.entity)
      : undefined;
    message.categoryEntities = object.categoryEntities?.map((e) => Entity.fromPartial(e)) || [];
    message.segments = object.segments?.map((e) => LabelSegment.fromPartial(e)) || [];
    message.frames = object.frames?.map((e) => LabelFrame.fromPartial(e)) || [];
    message.version = object.version ?? "";
    return message;
  },
};

function createBaseExplicitContentFrame(): ExplicitContentFrame {
  return { timeOffset: undefined, pornographyLikelihood: 0 };
}

export const ExplicitContentFrame: MessageFns<ExplicitContentFrame> = {
  encode(message: ExplicitContentFrame, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.timeOffset !== undefined) {
      Duration.encode(message.timeOffset, writer.uint32(10).fork()).join();
    }
    if (message.pornographyLikelihood !== 0) {
      writer.uint32(16).int32(message.pornographyLikelihood);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplicitContentFrame {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplicitContentFrame();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.timeOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pornographyLikelihood = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplicitContentFrame {
    return {
      timeOffset: isSet(object.timeOffset) ? Duration.fromJSON(object.timeOffset) : undefined,
      pornographyLikelihood: isSet(object.pornographyLikelihood) ? likelihoodFromJSON(object.pornographyLikelihood) : 0,
    };
  },

  toJSON(message: ExplicitContentFrame): unknown {
    const obj: any = {};
    if (message.timeOffset !== undefined) {
      obj.timeOffset = Duration.toJSON(message.timeOffset);
    }
    if (message.pornographyLikelihood !== 0) {
      obj.pornographyLikelihood = likelihoodToJSON(message.pornographyLikelihood);
    }
    return obj;
  },

  create(base?: DeepPartial<ExplicitContentFrame>): ExplicitContentFrame {
    return ExplicitContentFrame.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplicitContentFrame>): ExplicitContentFrame {
    const message = createBaseExplicitContentFrame();
    message.timeOffset = (object.timeOffset !== undefined && object.timeOffset !== null)
      ? Duration.fromPartial(object.timeOffset)
      : undefined;
    message.pornographyLikelihood = object.pornographyLikelihood ?? 0;
    return message;
  },
};

function createBaseExplicitContentAnnotation(): ExplicitContentAnnotation {
  return { frames: [], version: "" };
}

export const ExplicitContentAnnotation: MessageFns<ExplicitContentAnnotation> = {
  encode(message: ExplicitContentAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.frames) {
      ExplicitContentFrame.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.version !== "") {
      writer.uint32(18).string(message.version);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplicitContentAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplicitContentAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.frames.push(ExplicitContentFrame.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.version = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplicitContentAnnotation {
    return {
      frames: globalThis.Array.isArray(object?.frames)
        ? object.frames.map((e: any) => ExplicitContentFrame.fromJSON(e))
        : [],
      version: isSet(object.version) ? globalThis.String(object.version) : "",
    };
  },

  toJSON(message: ExplicitContentAnnotation): unknown {
    const obj: any = {};
    if (message.frames?.length) {
      obj.frames = message.frames.map((e) => ExplicitContentFrame.toJSON(e));
    }
    if (message.version !== "") {
      obj.version = message.version;
    }
    return obj;
  },

  create(base?: DeepPartial<ExplicitContentAnnotation>): ExplicitContentAnnotation {
    return ExplicitContentAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplicitContentAnnotation>): ExplicitContentAnnotation {
    const message = createBaseExplicitContentAnnotation();
    message.frames = object.frames?.map((e) => ExplicitContentFrame.fromPartial(e)) || [];
    message.version = object.version ?? "";
    return message;
  },
};

function createBaseNormalizedBoundingBox(): NormalizedBoundingBox {
  return { left: 0, top: 0, right: 0, bottom: 0 };
}

export const NormalizedBoundingBox: MessageFns<NormalizedBoundingBox> = {
  encode(message: NormalizedBoundingBox, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.left !== 0) {
      writer.uint32(13).float(message.left);
    }
    if (message.top !== 0) {
      writer.uint32(21).float(message.top);
    }
    if (message.right !== 0) {
      writer.uint32(29).float(message.right);
    }
    if (message.bottom !== 0) {
      writer.uint32(37).float(message.bottom);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NormalizedBoundingBox {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNormalizedBoundingBox();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 13) {
            break;
          }

          message.left = reader.float();
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.top = reader.float();
          continue;
        case 3:
          if (tag !== 29) {
            break;
          }

          message.right = reader.float();
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.bottom = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NormalizedBoundingBox {
    return {
      left: isSet(object.left) ? globalThis.Number(object.left) : 0,
      top: isSet(object.top) ? globalThis.Number(object.top) : 0,
      right: isSet(object.right) ? globalThis.Number(object.right) : 0,
      bottom: isSet(object.bottom) ? globalThis.Number(object.bottom) : 0,
    };
  },

  toJSON(message: NormalizedBoundingBox): unknown {
    const obj: any = {};
    if (message.left !== 0) {
      obj.left = message.left;
    }
    if (message.top !== 0) {
      obj.top = message.top;
    }
    if (message.right !== 0) {
      obj.right = message.right;
    }
    if (message.bottom !== 0) {
      obj.bottom = message.bottom;
    }
    return obj;
  },

  create(base?: DeepPartial<NormalizedBoundingBox>): NormalizedBoundingBox {
    return NormalizedBoundingBox.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<NormalizedBoundingBox>): NormalizedBoundingBox {
    const message = createBaseNormalizedBoundingBox();
    message.left = object.left ?? 0;
    message.top = object.top ?? 0;
    message.right = object.right ?? 0;
    message.bottom = object.bottom ?? 0;
    return message;
  },
};

function createBaseFaceDetectionAnnotation(): FaceDetectionAnnotation {
  return { tracks: [], thumbnail: Buffer.alloc(0), version: "" };
}

export const FaceDetectionAnnotation: MessageFns<FaceDetectionAnnotation> = {
  encode(message: FaceDetectionAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.tracks) {
      Track.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.thumbnail.length !== 0) {
      writer.uint32(34).bytes(message.thumbnail);
    }
    if (message.version !== "") {
      writer.uint32(42).string(message.version);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FaceDetectionAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFaceDetectionAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 26) {
            break;
          }

          message.tracks.push(Track.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.thumbnail = Buffer.from(reader.bytes());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.version = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FaceDetectionAnnotation {
    return {
      tracks: globalThis.Array.isArray(object?.tracks) ? object.tracks.map((e: any) => Track.fromJSON(e)) : [],
      thumbnail: isSet(object.thumbnail) ? Buffer.from(bytesFromBase64(object.thumbnail)) : Buffer.alloc(0),
      version: isSet(object.version) ? globalThis.String(object.version) : "",
    };
  },

  toJSON(message: FaceDetectionAnnotation): unknown {
    const obj: any = {};
    if (message.tracks?.length) {
      obj.tracks = message.tracks.map((e) => Track.toJSON(e));
    }
    if (message.thumbnail.length !== 0) {
      obj.thumbnail = base64FromBytes(message.thumbnail);
    }
    if (message.version !== "") {
      obj.version = message.version;
    }
    return obj;
  },

  create(base?: DeepPartial<FaceDetectionAnnotation>): FaceDetectionAnnotation {
    return FaceDetectionAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FaceDetectionAnnotation>): FaceDetectionAnnotation {
    const message = createBaseFaceDetectionAnnotation();
    message.tracks = object.tracks?.map((e) => Track.fromPartial(e)) || [];
    message.thumbnail = object.thumbnail ?? Buffer.alloc(0);
    message.version = object.version ?? "";
    return message;
  },
};

function createBasePersonDetectionAnnotation(): PersonDetectionAnnotation {
  return { tracks: [], version: "" };
}

export const PersonDetectionAnnotation: MessageFns<PersonDetectionAnnotation> = {
  encode(message: PersonDetectionAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.tracks) {
      Track.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.version !== "") {
      writer.uint32(18).string(message.version);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PersonDetectionAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePersonDetectionAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.tracks.push(Track.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.version = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PersonDetectionAnnotation {
    return {
      tracks: globalThis.Array.isArray(object?.tracks) ? object.tracks.map((e: any) => Track.fromJSON(e)) : [],
      version: isSet(object.version) ? globalThis.String(object.version) : "",
    };
  },

  toJSON(message: PersonDetectionAnnotation): unknown {
    const obj: any = {};
    if (message.tracks?.length) {
      obj.tracks = message.tracks.map((e) => Track.toJSON(e));
    }
    if (message.version !== "") {
      obj.version = message.version;
    }
    return obj;
  },

  create(base?: DeepPartial<PersonDetectionAnnotation>): PersonDetectionAnnotation {
    return PersonDetectionAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PersonDetectionAnnotation>): PersonDetectionAnnotation {
    const message = createBasePersonDetectionAnnotation();
    message.tracks = object.tracks?.map((e) => Track.fromPartial(e)) || [];
    message.version = object.version ?? "";
    return message;
  },
};

function createBaseFaceSegment(): FaceSegment {
  return { segment: undefined };
}

export const FaceSegment: MessageFns<FaceSegment> = {
  encode(message: FaceSegment, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.segment !== undefined) {
      VideoSegment.encode(message.segment, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FaceSegment {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFaceSegment();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.segment = VideoSegment.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FaceSegment {
    return { segment: isSet(object.segment) ? VideoSegment.fromJSON(object.segment) : undefined };
  },

  toJSON(message: FaceSegment): unknown {
    const obj: any = {};
    if (message.segment !== undefined) {
      obj.segment = VideoSegment.toJSON(message.segment);
    }
    return obj;
  },

  create(base?: DeepPartial<FaceSegment>): FaceSegment {
    return FaceSegment.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FaceSegment>): FaceSegment {
    const message = createBaseFaceSegment();
    message.segment = (object.segment !== undefined && object.segment !== null)
      ? VideoSegment.fromPartial(object.segment)
      : undefined;
    return message;
  },
};

function createBaseFaceFrame(): FaceFrame {
  return { normalizedBoundingBoxes: [], timeOffset: undefined };
}

export const FaceFrame: MessageFns<FaceFrame> = {
  encode(message: FaceFrame, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.normalizedBoundingBoxes) {
      NormalizedBoundingBox.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.timeOffset !== undefined) {
      Duration.encode(message.timeOffset, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FaceFrame {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFaceFrame();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.normalizedBoundingBoxes.push(NormalizedBoundingBox.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.timeOffset = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FaceFrame {
    return {
      normalizedBoundingBoxes: globalThis.Array.isArray(object?.normalizedBoundingBoxes)
        ? object.normalizedBoundingBoxes.map((e: any) => NormalizedBoundingBox.fromJSON(e))
        : [],
      timeOffset: isSet(object.timeOffset) ? Duration.fromJSON(object.timeOffset) : undefined,
    };
  },

  toJSON(message: FaceFrame): unknown {
    const obj: any = {};
    if (message.normalizedBoundingBoxes?.length) {
      obj.normalizedBoundingBoxes = message.normalizedBoundingBoxes.map((e) => NormalizedBoundingBox.toJSON(e));
    }
    if (message.timeOffset !== undefined) {
      obj.timeOffset = Duration.toJSON(message.timeOffset);
    }
    return obj;
  },

  create(base?: DeepPartial<FaceFrame>): FaceFrame {
    return FaceFrame.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FaceFrame>): FaceFrame {
    const message = createBaseFaceFrame();
    message.normalizedBoundingBoxes =
      object.normalizedBoundingBoxes?.map((e) => NormalizedBoundingBox.fromPartial(e)) || [];
    message.timeOffset = (object.timeOffset !== undefined && object.timeOffset !== null)
      ? Duration.fromPartial(object.timeOffset)
      : undefined;
    return message;
  },
};

function createBaseFaceAnnotation(): FaceAnnotation {
  return { thumbnail: Buffer.alloc(0), segments: [], frames: [] };
}

export const FaceAnnotation: MessageFns<FaceAnnotation> = {
  encode(message: FaceAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.thumbnail.length !== 0) {
      writer.uint32(10).bytes(message.thumbnail);
    }
    for (const v of message.segments) {
      FaceSegment.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.frames) {
      FaceFrame.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FaceAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFaceAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.thumbnail = Buffer.from(reader.bytes());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.segments.push(FaceSegment.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.frames.push(FaceFrame.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FaceAnnotation {
    return {
      thumbnail: isSet(object.thumbnail) ? Buffer.from(bytesFromBase64(object.thumbnail)) : Buffer.alloc(0),
      segments: globalThis.Array.isArray(object?.segments)
        ? object.segments.map((e: any) => FaceSegment.fromJSON(e))
        : [],
      frames: globalThis.Array.isArray(object?.frames) ? object.frames.map((e: any) => FaceFrame.fromJSON(e)) : [],
    };
  },

  toJSON(message: FaceAnnotation): unknown {
    const obj: any = {};
    if (message.thumbnail.length !== 0) {
      obj.thumbnail = base64FromBytes(message.thumbnail);
    }
    if (message.segments?.length) {
      obj.segments = message.segments.map((e) => FaceSegment.toJSON(e));
    }
    if (message.frames?.length) {
      obj.frames = message.frames.map((e) => FaceFrame.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<FaceAnnotation>): FaceAnnotation {
    return FaceAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FaceAnnotation>): FaceAnnotation {
    const message = createBaseFaceAnnotation();
    message.thumbnail = object.thumbnail ?? Buffer.alloc(0);
    message.segments = object.segments?.map((e) => FaceSegment.fromPartial(e)) || [];
    message.frames = object.frames?.map((e) => FaceFrame.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTimestampedObject(): TimestampedObject {
  return { normalizedBoundingBox: undefined, timeOffset: undefined, attributes: [], landmarks: [] };
}

export const TimestampedObject: MessageFns<TimestampedObject> = {
  encode(message: TimestampedObject, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.normalizedBoundingBox !== undefined) {
      NormalizedBoundingBox.encode(message.normalizedBoundingBox, writer.uint32(10).fork()).join();
    }
    if (message.timeOffset !== undefined) {
      Duration.encode(message.timeOffset, writer.uint32(18).fork()).join();
    }
    for (const v of message.attributes) {
      DetectedAttribute.encode(v!, writer.uint32(26).fork()).join();
    }
    for (const v of message.landmarks) {
      DetectedLandmark.encode(v!, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TimestampedObject {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTimestampedObject();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.normalizedBoundingBox = NormalizedBoundingBox.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.timeOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.attributes.push(DetectedAttribute.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.landmarks.push(DetectedLandmark.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TimestampedObject {
    return {
      normalizedBoundingBox: isSet(object.normalizedBoundingBox)
        ? NormalizedBoundingBox.fromJSON(object.normalizedBoundingBox)
        : undefined,
      timeOffset: isSet(object.timeOffset) ? Duration.fromJSON(object.timeOffset) : undefined,
      attributes: globalThis.Array.isArray(object?.attributes)
        ? object.attributes.map((e: any) => DetectedAttribute.fromJSON(e))
        : [],
      landmarks: globalThis.Array.isArray(object?.landmarks)
        ? object.landmarks.map((e: any) => DetectedLandmark.fromJSON(e))
        : [],
    };
  },

  toJSON(message: TimestampedObject): unknown {
    const obj: any = {};
    if (message.normalizedBoundingBox !== undefined) {
      obj.normalizedBoundingBox = NormalizedBoundingBox.toJSON(message.normalizedBoundingBox);
    }
    if (message.timeOffset !== undefined) {
      obj.timeOffset = Duration.toJSON(message.timeOffset);
    }
    if (message.attributes?.length) {
      obj.attributes = message.attributes.map((e) => DetectedAttribute.toJSON(e));
    }
    if (message.landmarks?.length) {
      obj.landmarks = message.landmarks.map((e) => DetectedLandmark.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<TimestampedObject>): TimestampedObject {
    return TimestampedObject.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TimestampedObject>): TimestampedObject {
    const message = createBaseTimestampedObject();
    message.normalizedBoundingBox =
      (object.normalizedBoundingBox !== undefined && object.normalizedBoundingBox !== null)
        ? NormalizedBoundingBox.fromPartial(object.normalizedBoundingBox)
        : undefined;
    message.timeOffset = (object.timeOffset !== undefined && object.timeOffset !== null)
      ? Duration.fromPartial(object.timeOffset)
      : undefined;
    message.attributes = object.attributes?.map((e) => DetectedAttribute.fromPartial(e)) || [];
    message.landmarks = object.landmarks?.map((e) => DetectedLandmark.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTrack(): Track {
  return { segment: undefined, timestampedObjects: [], attributes: [], confidence: 0 };
}

export const Track: MessageFns<Track> = {
  encode(message: Track, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.segment !== undefined) {
      VideoSegment.encode(message.segment, writer.uint32(10).fork()).join();
    }
    for (const v of message.timestampedObjects) {
      TimestampedObject.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.attributes) {
      DetectedAttribute.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.confidence !== 0) {
      writer.uint32(37).float(message.confidence);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Track {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTrack();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.segment = VideoSegment.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.timestampedObjects.push(TimestampedObject.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.attributes.push(DetectedAttribute.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.confidence = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Track {
    return {
      segment: isSet(object.segment) ? VideoSegment.fromJSON(object.segment) : undefined,
      timestampedObjects: globalThis.Array.isArray(object?.timestampedObjects)
        ? object.timestampedObjects.map((e: any) => TimestampedObject.fromJSON(e))
        : [],
      attributes: globalThis.Array.isArray(object?.attributes)
        ? object.attributes.map((e: any) => DetectedAttribute.fromJSON(e))
        : [],
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
    };
  },

  toJSON(message: Track): unknown {
    const obj: any = {};
    if (message.segment !== undefined) {
      obj.segment = VideoSegment.toJSON(message.segment);
    }
    if (message.timestampedObjects?.length) {
      obj.timestampedObjects = message.timestampedObjects.map((e) => TimestampedObject.toJSON(e));
    }
    if (message.attributes?.length) {
      obj.attributes = message.attributes.map((e) => DetectedAttribute.toJSON(e));
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    return obj;
  },

  create(base?: DeepPartial<Track>): Track {
    return Track.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Track>): Track {
    const message = createBaseTrack();
    message.segment = (object.segment !== undefined && object.segment !== null)
      ? VideoSegment.fromPartial(object.segment)
      : undefined;
    message.timestampedObjects = object.timestampedObjects?.map((e) => TimestampedObject.fromPartial(e)) || [];
    message.attributes = object.attributes?.map((e) => DetectedAttribute.fromPartial(e)) || [];
    message.confidence = object.confidence ?? 0;
    return message;
  },
};

function createBaseDetectedAttribute(): DetectedAttribute {
  return { name: "", confidence: 0, value: "" };
}

export const DetectedAttribute: MessageFns<DetectedAttribute> = {
  encode(message: DetectedAttribute, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.confidence !== 0) {
      writer.uint32(21).float(message.confidence);
    }
    if (message.value !== "") {
      writer.uint32(26).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DetectedAttribute {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDetectedAttribute();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.confidence = reader.float();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DetectedAttribute {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: DetectedAttribute): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<DetectedAttribute>): DetectedAttribute {
    return DetectedAttribute.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DetectedAttribute>): DetectedAttribute {
    const message = createBaseDetectedAttribute();
    message.name = object.name ?? "";
    message.confidence = object.confidence ?? 0;
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseDetectedLandmark(): DetectedLandmark {
  return { name: "", point: undefined, confidence: 0 };
}

export const DetectedLandmark: MessageFns<DetectedLandmark> = {
  encode(message: DetectedLandmark, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.point !== undefined) {
      NormalizedVertex.encode(message.point, writer.uint32(18).fork()).join();
    }
    if (message.confidence !== 0) {
      writer.uint32(29).float(message.confidence);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DetectedLandmark {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDetectedLandmark();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.point = NormalizedVertex.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 29) {
            break;
          }

          message.confidence = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DetectedLandmark {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      point: isSet(object.point) ? NormalizedVertex.fromJSON(object.point) : undefined,
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
    };
  },

  toJSON(message: DetectedLandmark): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.point !== undefined) {
      obj.point = NormalizedVertex.toJSON(message.point);
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    return obj;
  },

  create(base?: DeepPartial<DetectedLandmark>): DetectedLandmark {
    return DetectedLandmark.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DetectedLandmark>): DetectedLandmark {
    const message = createBaseDetectedLandmark();
    message.name = object.name ?? "";
    message.point = (object.point !== undefined && object.point !== null)
      ? NormalizedVertex.fromPartial(object.point)
      : undefined;
    message.confidence = object.confidence ?? 0;
    return message;
  },
};

function createBaseVideoAnnotationResults(): VideoAnnotationResults {
  return {
    inputUri: "",
    segment: undefined,
    segmentLabelAnnotations: [],
    segmentPresenceLabelAnnotations: [],
    shotLabelAnnotations: [],
    shotPresenceLabelAnnotations: [],
    frameLabelAnnotations: [],
    faceAnnotations: [],
    faceDetectionAnnotations: [],
    shotAnnotations: [],
    explicitAnnotation: undefined,
    speechTranscriptions: [],
    textAnnotations: [],
    objectAnnotations: [],
    logoRecognitionAnnotations: [],
    personDetectionAnnotations: [],
    error: undefined,
  };
}

export const VideoAnnotationResults: MessageFns<VideoAnnotationResults> = {
  encode(message: VideoAnnotationResults, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputUri !== "") {
      writer.uint32(10).string(message.inputUri);
    }
    if (message.segment !== undefined) {
      VideoSegment.encode(message.segment, writer.uint32(82).fork()).join();
    }
    for (const v of message.segmentLabelAnnotations) {
      LabelAnnotation.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.segmentPresenceLabelAnnotations) {
      LabelAnnotation.encode(v!, writer.uint32(186).fork()).join();
    }
    for (const v of message.shotLabelAnnotations) {
      LabelAnnotation.encode(v!, writer.uint32(26).fork()).join();
    }
    for (const v of message.shotPresenceLabelAnnotations) {
      LabelAnnotation.encode(v!, writer.uint32(194).fork()).join();
    }
    for (const v of message.frameLabelAnnotations) {
      LabelAnnotation.encode(v!, writer.uint32(34).fork()).join();
    }
    for (const v of message.faceAnnotations) {
      FaceAnnotation.encode(v!, writer.uint32(42).fork()).join();
    }
    for (const v of message.faceDetectionAnnotations) {
      FaceDetectionAnnotation.encode(v!, writer.uint32(106).fork()).join();
    }
    for (const v of message.shotAnnotations) {
      VideoSegment.encode(v!, writer.uint32(50).fork()).join();
    }
    if (message.explicitAnnotation !== undefined) {
      ExplicitContentAnnotation.encode(message.explicitAnnotation, writer.uint32(58).fork()).join();
    }
    for (const v of message.speechTranscriptions) {
      SpeechTranscription.encode(v!, writer.uint32(90).fork()).join();
    }
    for (const v of message.textAnnotations) {
      TextAnnotation.encode(v!, writer.uint32(98).fork()).join();
    }
    for (const v of message.objectAnnotations) {
      ObjectTrackingAnnotation.encode(v!, writer.uint32(114).fork()).join();
    }
    for (const v of message.logoRecognitionAnnotations) {
      LogoRecognitionAnnotation.encode(v!, writer.uint32(154).fork()).join();
    }
    for (const v of message.personDetectionAnnotations) {
      PersonDetectionAnnotation.encode(v!, writer.uint32(162).fork()).join();
    }
    if (message.error !== undefined) {
      Status.encode(message.error, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VideoAnnotationResults {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVideoAnnotationResults();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputUri = reader.string();
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.segment = VideoSegment.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.segmentLabelAnnotations.push(LabelAnnotation.decode(reader, reader.uint32()));
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.segmentPresenceLabelAnnotations.push(LabelAnnotation.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.shotLabelAnnotations.push(LabelAnnotation.decode(reader, reader.uint32()));
          continue;
        case 24:
          if (tag !== 194) {
            break;
          }

          message.shotPresenceLabelAnnotations.push(LabelAnnotation.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.frameLabelAnnotations.push(LabelAnnotation.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.faceAnnotations.push(FaceAnnotation.decode(reader, reader.uint32()));
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.faceDetectionAnnotations.push(FaceDetectionAnnotation.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.shotAnnotations.push(VideoSegment.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.explicitAnnotation = ExplicitContentAnnotation.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.speechTranscriptions.push(SpeechTranscription.decode(reader, reader.uint32()));
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.textAnnotations.push(TextAnnotation.decode(reader, reader.uint32()));
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.objectAnnotations.push(ObjectTrackingAnnotation.decode(reader, reader.uint32()));
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.logoRecognitionAnnotations.push(LogoRecognitionAnnotation.decode(reader, reader.uint32()));
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.personDetectionAnnotations.push(PersonDetectionAnnotation.decode(reader, reader.uint32()));
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.error = Status.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VideoAnnotationResults {
    return {
      inputUri: isSet(object.inputUri) ? globalThis.String(object.inputUri) : "",
      segment: isSet(object.segment) ? VideoSegment.fromJSON(object.segment) : undefined,
      segmentLabelAnnotations: globalThis.Array.isArray(object?.segmentLabelAnnotations)
        ? object.segmentLabelAnnotations.map((e: any) => LabelAnnotation.fromJSON(e))
        : [],
      segmentPresenceLabelAnnotations: globalThis.Array.isArray(object?.segmentPresenceLabelAnnotations)
        ? object.segmentPresenceLabelAnnotations.map((e: any) => LabelAnnotation.fromJSON(e))
        : [],
      shotLabelAnnotations: globalThis.Array.isArray(object?.shotLabelAnnotations)
        ? object.shotLabelAnnotations.map((e: any) => LabelAnnotation.fromJSON(e))
        : [],
      shotPresenceLabelAnnotations: globalThis.Array.isArray(object?.shotPresenceLabelAnnotations)
        ? object.shotPresenceLabelAnnotations.map((e: any) => LabelAnnotation.fromJSON(e))
        : [],
      frameLabelAnnotations: globalThis.Array.isArray(object?.frameLabelAnnotations)
        ? object.frameLabelAnnotations.map((e: any) => LabelAnnotation.fromJSON(e))
        : [],
      faceAnnotations: globalThis.Array.isArray(object?.faceAnnotations)
        ? object.faceAnnotations.map((e: any) => FaceAnnotation.fromJSON(e))
        : [],
      faceDetectionAnnotations: globalThis.Array.isArray(object?.faceDetectionAnnotations)
        ? object.faceDetectionAnnotations.map((e: any) => FaceDetectionAnnotation.fromJSON(e))
        : [],
      shotAnnotations: globalThis.Array.isArray(object?.shotAnnotations)
        ? object.shotAnnotations.map((e: any) => VideoSegment.fromJSON(e))
        : [],
      explicitAnnotation: isSet(object.explicitAnnotation)
        ? ExplicitContentAnnotation.fromJSON(object.explicitAnnotation)
        : undefined,
      speechTranscriptions: globalThis.Array.isArray(object?.speechTranscriptions)
        ? object.speechTranscriptions.map((e: any) => SpeechTranscription.fromJSON(e))
        : [],
      textAnnotations: globalThis.Array.isArray(object?.textAnnotations)
        ? object.textAnnotations.map((e: any) => TextAnnotation.fromJSON(e))
        : [],
      objectAnnotations: globalThis.Array.isArray(object?.objectAnnotations)
        ? object.objectAnnotations.map((e: any) => ObjectTrackingAnnotation.fromJSON(e))
        : [],
      logoRecognitionAnnotations: globalThis.Array.isArray(object?.logoRecognitionAnnotations)
        ? object.logoRecognitionAnnotations.map((e: any) => LogoRecognitionAnnotation.fromJSON(e))
        : [],
      personDetectionAnnotations: globalThis.Array.isArray(object?.personDetectionAnnotations)
        ? object.personDetectionAnnotations.map((e: any) => PersonDetectionAnnotation.fromJSON(e))
        : [],
      error: isSet(object.error) ? Status.fromJSON(object.error) : undefined,
    };
  },

  toJSON(message: VideoAnnotationResults): unknown {
    const obj: any = {};
    if (message.inputUri !== "") {
      obj.inputUri = message.inputUri;
    }
    if (message.segment !== undefined) {
      obj.segment = VideoSegment.toJSON(message.segment);
    }
    if (message.segmentLabelAnnotations?.length) {
      obj.segmentLabelAnnotations = message.segmentLabelAnnotations.map((e) => LabelAnnotation.toJSON(e));
    }
    if (message.segmentPresenceLabelAnnotations?.length) {
      obj.segmentPresenceLabelAnnotations = message.segmentPresenceLabelAnnotations.map((e) =>
        LabelAnnotation.toJSON(e)
      );
    }
    if (message.shotLabelAnnotations?.length) {
      obj.shotLabelAnnotations = message.shotLabelAnnotations.map((e) => LabelAnnotation.toJSON(e));
    }
    if (message.shotPresenceLabelAnnotations?.length) {
      obj.shotPresenceLabelAnnotations = message.shotPresenceLabelAnnotations.map((e) => LabelAnnotation.toJSON(e));
    }
    if (message.frameLabelAnnotations?.length) {
      obj.frameLabelAnnotations = message.frameLabelAnnotations.map((e) => LabelAnnotation.toJSON(e));
    }
    if (message.faceAnnotations?.length) {
      obj.faceAnnotations = message.faceAnnotations.map((e) => FaceAnnotation.toJSON(e));
    }
    if (message.faceDetectionAnnotations?.length) {
      obj.faceDetectionAnnotations = message.faceDetectionAnnotations.map((e) => FaceDetectionAnnotation.toJSON(e));
    }
    if (message.shotAnnotations?.length) {
      obj.shotAnnotations = message.shotAnnotations.map((e) => VideoSegment.toJSON(e));
    }
    if (message.explicitAnnotation !== undefined) {
      obj.explicitAnnotation = ExplicitContentAnnotation.toJSON(message.explicitAnnotation);
    }
    if (message.speechTranscriptions?.length) {
      obj.speechTranscriptions = message.speechTranscriptions.map((e) => SpeechTranscription.toJSON(e));
    }
    if (message.textAnnotations?.length) {
      obj.textAnnotations = message.textAnnotations.map((e) => TextAnnotation.toJSON(e));
    }
    if (message.objectAnnotations?.length) {
      obj.objectAnnotations = message.objectAnnotations.map((e) => ObjectTrackingAnnotation.toJSON(e));
    }
    if (message.logoRecognitionAnnotations?.length) {
      obj.logoRecognitionAnnotations = message.logoRecognitionAnnotations.map((e) =>
        LogoRecognitionAnnotation.toJSON(e)
      );
    }
    if (message.personDetectionAnnotations?.length) {
      obj.personDetectionAnnotations = message.personDetectionAnnotations.map((e) =>
        PersonDetectionAnnotation.toJSON(e)
      );
    }
    if (message.error !== undefined) {
      obj.error = Status.toJSON(message.error);
    }
    return obj;
  },

  create(base?: DeepPartial<VideoAnnotationResults>): VideoAnnotationResults {
    return VideoAnnotationResults.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VideoAnnotationResults>): VideoAnnotationResults {
    const message = createBaseVideoAnnotationResults();
    message.inputUri = object.inputUri ?? "";
    message.segment = (object.segment !== undefined && object.segment !== null)
      ? VideoSegment.fromPartial(object.segment)
      : undefined;
    message.segmentLabelAnnotations = object.segmentLabelAnnotations?.map((e) => LabelAnnotation.fromPartial(e)) || [];
    message.segmentPresenceLabelAnnotations =
      object.segmentPresenceLabelAnnotations?.map((e) => LabelAnnotation.fromPartial(e)) || [];
    message.shotLabelAnnotations = object.shotLabelAnnotations?.map((e) => LabelAnnotation.fromPartial(e)) || [];
    message.shotPresenceLabelAnnotations =
      object.shotPresenceLabelAnnotations?.map((e) => LabelAnnotation.fromPartial(e)) || [];
    message.frameLabelAnnotations = object.frameLabelAnnotations?.map((e) => LabelAnnotation.fromPartial(e)) || [];
    message.faceAnnotations = object.faceAnnotations?.map((e) => FaceAnnotation.fromPartial(e)) || [];
    message.faceDetectionAnnotations =
      object.faceDetectionAnnotations?.map((e) => FaceDetectionAnnotation.fromPartial(e)) || [];
    message.shotAnnotations = object.shotAnnotations?.map((e) => VideoSegment.fromPartial(e)) || [];
    message.explicitAnnotation = (object.explicitAnnotation !== undefined && object.explicitAnnotation !== null)
      ? ExplicitContentAnnotation.fromPartial(object.explicitAnnotation)
      : undefined;
    message.speechTranscriptions = object.speechTranscriptions?.map((e) => SpeechTranscription.fromPartial(e)) || [];
    message.textAnnotations = object.textAnnotations?.map((e) => TextAnnotation.fromPartial(e)) || [];
    message.objectAnnotations = object.objectAnnotations?.map((e) => ObjectTrackingAnnotation.fromPartial(e)) || [];
    message.logoRecognitionAnnotations =
      object.logoRecognitionAnnotations?.map((e) => LogoRecognitionAnnotation.fromPartial(e)) || [];
    message.personDetectionAnnotations =
      object.personDetectionAnnotations?.map((e) => PersonDetectionAnnotation.fromPartial(e)) || [];
    message.error = (object.error !== undefined && object.error !== null)
      ? Status.fromPartial(object.error)
      : undefined;
    return message;
  },
};

function createBaseAnnotateVideoResponse(): AnnotateVideoResponse {
  return { annotationResults: [] };
}

export const AnnotateVideoResponse: MessageFns<AnnotateVideoResponse> = {
  encode(message: AnnotateVideoResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.annotationResults) {
      VideoAnnotationResults.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnnotateVideoResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnnotateVideoResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.annotationResults.push(VideoAnnotationResults.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnnotateVideoResponse {
    return {
      annotationResults: globalThis.Array.isArray(object?.annotationResults)
        ? object.annotationResults.map((e: any) => VideoAnnotationResults.fromJSON(e))
        : [],
    };
  },

  toJSON(message: AnnotateVideoResponse): unknown {
    const obj: any = {};
    if (message.annotationResults?.length) {
      obj.annotationResults = message.annotationResults.map((e) => VideoAnnotationResults.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<AnnotateVideoResponse>): AnnotateVideoResponse {
    return AnnotateVideoResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AnnotateVideoResponse>): AnnotateVideoResponse {
    const message = createBaseAnnotateVideoResponse();
    message.annotationResults = object.annotationResults?.map((e) => VideoAnnotationResults.fromPartial(e)) || [];
    return message;
  },
};

function createBaseVideoAnnotationProgress(): VideoAnnotationProgress {
  return {
    inputUri: "",
    progressPercent: 0,
    startTime: undefined,
    updateTime: undefined,
    feature: 0,
    segment: undefined,
  };
}

export const VideoAnnotationProgress: MessageFns<VideoAnnotationProgress> = {
  encode(message: VideoAnnotationProgress, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputUri !== "") {
      writer.uint32(10).string(message.inputUri);
    }
    if (message.progressPercent !== 0) {
      writer.uint32(16).int32(message.progressPercent);
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(26).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(34).fork()).join();
    }
    if (message.feature !== 0) {
      writer.uint32(40).int32(message.feature);
    }
    if (message.segment !== undefined) {
      VideoSegment.encode(message.segment, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VideoAnnotationProgress {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVideoAnnotationProgress();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputUri = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.progressPercent = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.feature = reader.int32() as any;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.segment = VideoSegment.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VideoAnnotationProgress {
    return {
      inputUri: isSet(object.inputUri) ? globalThis.String(object.inputUri) : "",
      progressPercent: isSet(object.progressPercent) ? globalThis.Number(object.progressPercent) : 0,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      feature: isSet(object.feature) ? featureFromJSON(object.feature) : 0,
      segment: isSet(object.segment) ? VideoSegment.fromJSON(object.segment) : undefined,
    };
  },

  toJSON(message: VideoAnnotationProgress): unknown {
    const obj: any = {};
    if (message.inputUri !== "") {
      obj.inputUri = message.inputUri;
    }
    if (message.progressPercent !== 0) {
      obj.progressPercent = Math.round(message.progressPercent);
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (message.feature !== 0) {
      obj.feature = featureToJSON(message.feature);
    }
    if (message.segment !== undefined) {
      obj.segment = VideoSegment.toJSON(message.segment);
    }
    return obj;
  },

  create(base?: DeepPartial<VideoAnnotationProgress>): VideoAnnotationProgress {
    return VideoAnnotationProgress.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VideoAnnotationProgress>): VideoAnnotationProgress {
    const message = createBaseVideoAnnotationProgress();
    message.inputUri = object.inputUri ?? "";
    message.progressPercent = object.progressPercent ?? 0;
    message.startTime = object.startTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.feature = object.feature ?? 0;
    message.segment = (object.segment !== undefined && object.segment !== null)
      ? VideoSegment.fromPartial(object.segment)
      : undefined;
    return message;
  },
};

function createBaseAnnotateVideoProgress(): AnnotateVideoProgress {
  return { annotationProgress: [] };
}

export const AnnotateVideoProgress: MessageFns<AnnotateVideoProgress> = {
  encode(message: AnnotateVideoProgress, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.annotationProgress) {
      VideoAnnotationProgress.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnnotateVideoProgress {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnnotateVideoProgress();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.annotationProgress.push(VideoAnnotationProgress.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnnotateVideoProgress {
    return {
      annotationProgress: globalThis.Array.isArray(object?.annotationProgress)
        ? object.annotationProgress.map((e: any) => VideoAnnotationProgress.fromJSON(e))
        : [],
    };
  },

  toJSON(message: AnnotateVideoProgress): unknown {
    const obj: any = {};
    if (message.annotationProgress?.length) {
      obj.annotationProgress = message.annotationProgress.map((e) => VideoAnnotationProgress.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<AnnotateVideoProgress>): AnnotateVideoProgress {
    return AnnotateVideoProgress.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AnnotateVideoProgress>): AnnotateVideoProgress {
    const message = createBaseAnnotateVideoProgress();
    message.annotationProgress = object.annotationProgress?.map((e) => VideoAnnotationProgress.fromPartial(e)) || [];
    return message;
  },
};

function createBaseSpeechTranscriptionConfig(): SpeechTranscriptionConfig {
  return {
    languageCode: "",
    maxAlternatives: 0,
    filterProfanity: false,
    speechContexts: [],
    enableAutomaticPunctuation: false,
    audioTracks: [],
    enableSpeakerDiarization: false,
    diarizationSpeakerCount: 0,
    enableWordConfidence: false,
  };
}

export const SpeechTranscriptionConfig: MessageFns<SpeechTranscriptionConfig> = {
  encode(message: SpeechTranscriptionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.languageCode !== "") {
      writer.uint32(10).string(message.languageCode);
    }
    if (message.maxAlternatives !== 0) {
      writer.uint32(16).int32(message.maxAlternatives);
    }
    if (message.filterProfanity !== false) {
      writer.uint32(24).bool(message.filterProfanity);
    }
    for (const v of message.speechContexts) {
      SpeechContext.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.enableAutomaticPunctuation !== false) {
      writer.uint32(40).bool(message.enableAutomaticPunctuation);
    }
    writer.uint32(50).fork();
    for (const v of message.audioTracks) {
      writer.int32(v);
    }
    writer.join();
    if (message.enableSpeakerDiarization !== false) {
      writer.uint32(56).bool(message.enableSpeakerDiarization);
    }
    if (message.diarizationSpeakerCount !== 0) {
      writer.uint32(64).int32(message.diarizationSpeakerCount);
    }
    if (message.enableWordConfidence !== false) {
      writer.uint32(72).bool(message.enableWordConfidence);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechTranscriptionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechTranscriptionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.languageCode = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.maxAlternatives = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.filterProfanity = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.speechContexts.push(SpeechContext.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.enableAutomaticPunctuation = reader.bool();
          continue;
        case 6:
          if (tag === 48) {
            message.audioTracks.push(reader.int32());

            continue;
          }

          if (tag === 50) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.audioTracks.push(reader.int32());
            }

            continue;
          }

          break;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.enableSpeakerDiarization = reader.bool();
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.diarizationSpeakerCount = reader.int32();
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.enableWordConfidence = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechTranscriptionConfig {
    return {
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
      maxAlternatives: isSet(object.maxAlternatives) ? globalThis.Number(object.maxAlternatives) : 0,
      filterProfanity: isSet(object.filterProfanity) ? globalThis.Boolean(object.filterProfanity) : false,
      speechContexts: globalThis.Array.isArray(object?.speechContexts)
        ? object.speechContexts.map((e: any) => SpeechContext.fromJSON(e))
        : [],
      enableAutomaticPunctuation: isSet(object.enableAutomaticPunctuation)
        ? globalThis.Boolean(object.enableAutomaticPunctuation)
        : false,
      audioTracks: globalThis.Array.isArray(object?.audioTracks)
        ? object.audioTracks.map((e: any) => globalThis.Number(e))
        : [],
      enableSpeakerDiarization: isSet(object.enableSpeakerDiarization)
        ? globalThis.Boolean(object.enableSpeakerDiarization)
        : false,
      diarizationSpeakerCount: isSet(object.diarizationSpeakerCount)
        ? globalThis.Number(object.diarizationSpeakerCount)
        : 0,
      enableWordConfidence: isSet(object.enableWordConfidence)
        ? globalThis.Boolean(object.enableWordConfidence)
        : false,
    };
  },

  toJSON(message: SpeechTranscriptionConfig): unknown {
    const obj: any = {};
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    if (message.maxAlternatives !== 0) {
      obj.maxAlternatives = Math.round(message.maxAlternatives);
    }
    if (message.filterProfanity !== false) {
      obj.filterProfanity = message.filterProfanity;
    }
    if (message.speechContexts?.length) {
      obj.speechContexts = message.speechContexts.map((e) => SpeechContext.toJSON(e));
    }
    if (message.enableAutomaticPunctuation !== false) {
      obj.enableAutomaticPunctuation = message.enableAutomaticPunctuation;
    }
    if (message.audioTracks?.length) {
      obj.audioTracks = message.audioTracks.map((e) => Math.round(e));
    }
    if (message.enableSpeakerDiarization !== false) {
      obj.enableSpeakerDiarization = message.enableSpeakerDiarization;
    }
    if (message.diarizationSpeakerCount !== 0) {
      obj.diarizationSpeakerCount = Math.round(message.diarizationSpeakerCount);
    }
    if (message.enableWordConfidence !== false) {
      obj.enableWordConfidence = message.enableWordConfidence;
    }
    return obj;
  },

  create(base?: DeepPartial<SpeechTranscriptionConfig>): SpeechTranscriptionConfig {
    return SpeechTranscriptionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SpeechTranscriptionConfig>): SpeechTranscriptionConfig {
    const message = createBaseSpeechTranscriptionConfig();
    message.languageCode = object.languageCode ?? "";
    message.maxAlternatives = object.maxAlternatives ?? 0;
    message.filterProfanity = object.filterProfanity ?? false;
    message.speechContexts = object.speechContexts?.map((e) => SpeechContext.fromPartial(e)) || [];
    message.enableAutomaticPunctuation = object.enableAutomaticPunctuation ?? false;
    message.audioTracks = object.audioTracks?.map((e) => e) || [];
    message.enableSpeakerDiarization = object.enableSpeakerDiarization ?? false;
    message.diarizationSpeakerCount = object.diarizationSpeakerCount ?? 0;
    message.enableWordConfidence = object.enableWordConfidence ?? false;
    return message;
  },
};

function createBaseSpeechContext(): SpeechContext {
  return { phrases: [] };
}

export const SpeechContext: MessageFns<SpeechContext> = {
  encode(message: SpeechContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.phrases) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.phrases.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechContext {
    return {
      phrases: globalThis.Array.isArray(object?.phrases) ? object.phrases.map((e: any) => globalThis.String(e)) : [],
    };
  },

  toJSON(message: SpeechContext): unknown {
    const obj: any = {};
    if (message.phrases?.length) {
      obj.phrases = message.phrases;
    }
    return obj;
  },

  create(base?: DeepPartial<SpeechContext>): SpeechContext {
    return SpeechContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SpeechContext>): SpeechContext {
    const message = createBaseSpeechContext();
    message.phrases = object.phrases?.map((e) => e) || [];
    return message;
  },
};

function createBaseSpeechTranscription(): SpeechTranscription {
  return { alternatives: [], languageCode: "" };
}

export const SpeechTranscription: MessageFns<SpeechTranscription> = {
  encode(message: SpeechTranscription, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.alternatives) {
      SpeechRecognitionAlternative.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.languageCode !== "") {
      writer.uint32(18).string(message.languageCode);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechTranscription {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechTranscription();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.alternatives.push(SpeechRecognitionAlternative.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.languageCode = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechTranscription {
    return {
      alternatives: globalThis.Array.isArray(object?.alternatives)
        ? object.alternatives.map((e: any) => SpeechRecognitionAlternative.fromJSON(e))
        : [],
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
    };
  },

  toJSON(message: SpeechTranscription): unknown {
    const obj: any = {};
    if (message.alternatives?.length) {
      obj.alternatives = message.alternatives.map((e) => SpeechRecognitionAlternative.toJSON(e));
    }
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    return obj;
  },

  create(base?: DeepPartial<SpeechTranscription>): SpeechTranscription {
    return SpeechTranscription.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SpeechTranscription>): SpeechTranscription {
    const message = createBaseSpeechTranscription();
    message.alternatives = object.alternatives?.map((e) => SpeechRecognitionAlternative.fromPartial(e)) || [];
    message.languageCode = object.languageCode ?? "";
    return message;
  },
};

function createBaseSpeechRecognitionAlternative(): SpeechRecognitionAlternative {
  return { transcript: "", confidence: 0, words: [] };
}

export const SpeechRecognitionAlternative: MessageFns<SpeechRecognitionAlternative> = {
  encode(message: SpeechRecognitionAlternative, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.transcript !== "") {
      writer.uint32(10).string(message.transcript);
    }
    if (message.confidence !== 0) {
      writer.uint32(21).float(message.confidence);
    }
    for (const v of message.words) {
      WordInfo.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechRecognitionAlternative {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechRecognitionAlternative();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.transcript = reader.string();
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.confidence = reader.float();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.words.push(WordInfo.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechRecognitionAlternative {
    return {
      transcript: isSet(object.transcript) ? globalThis.String(object.transcript) : "",
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
      words: globalThis.Array.isArray(object?.words) ? object.words.map((e: any) => WordInfo.fromJSON(e)) : [],
    };
  },

  toJSON(message: SpeechRecognitionAlternative): unknown {
    const obj: any = {};
    if (message.transcript !== "") {
      obj.transcript = message.transcript;
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    if (message.words?.length) {
      obj.words = message.words.map((e) => WordInfo.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<SpeechRecognitionAlternative>): SpeechRecognitionAlternative {
    return SpeechRecognitionAlternative.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SpeechRecognitionAlternative>): SpeechRecognitionAlternative {
    const message = createBaseSpeechRecognitionAlternative();
    message.transcript = object.transcript ?? "";
    message.confidence = object.confidence ?? 0;
    message.words = object.words?.map((e) => WordInfo.fromPartial(e)) || [];
    return message;
  },
};

function createBaseWordInfo(): WordInfo {
  return { startTime: undefined, endTime: undefined, word: "", confidence: 0, speakerTag: 0 };
}

export const WordInfo: MessageFns<WordInfo> = {
  encode(message: WordInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startTime !== undefined) {
      Duration.encode(message.startTime, writer.uint32(10).fork()).join();
    }
    if (message.endTime !== undefined) {
      Duration.encode(message.endTime, writer.uint32(18).fork()).join();
    }
    if (message.word !== "") {
      writer.uint32(26).string(message.word);
    }
    if (message.confidence !== 0) {
      writer.uint32(37).float(message.confidence);
    }
    if (message.speakerTag !== 0) {
      writer.uint32(40).int32(message.speakerTag);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): WordInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseWordInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startTime = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.endTime = Duration.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.word = reader.string();
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.confidence = reader.float();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.speakerTag = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): WordInfo {
    return {
      startTime: isSet(object.startTime) ? Duration.fromJSON(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? Duration.fromJSON(object.endTime) : undefined,
      word: isSet(object.word) ? globalThis.String(object.word) : "",
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
      speakerTag: isSet(object.speakerTag) ? globalThis.Number(object.speakerTag) : 0,
    };
  },

  toJSON(message: WordInfo): unknown {
    const obj: any = {};
    if (message.startTime !== undefined) {
      obj.startTime = Duration.toJSON(message.startTime);
    }
    if (message.endTime !== undefined) {
      obj.endTime = Duration.toJSON(message.endTime);
    }
    if (message.word !== "") {
      obj.word = message.word;
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    if (message.speakerTag !== 0) {
      obj.speakerTag = Math.round(message.speakerTag);
    }
    return obj;
  },

  create(base?: DeepPartial<WordInfo>): WordInfo {
    return WordInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<WordInfo>): WordInfo {
    const message = createBaseWordInfo();
    message.startTime = (object.startTime !== undefined && object.startTime !== null)
      ? Duration.fromPartial(object.startTime)
      : undefined;
    message.endTime = (object.endTime !== undefined && object.endTime !== null)
      ? Duration.fromPartial(object.endTime)
      : undefined;
    message.word = object.word ?? "";
    message.confidence = object.confidence ?? 0;
    message.speakerTag = object.speakerTag ?? 0;
    return message;
  },
};

function createBaseNormalizedVertex(): NormalizedVertex {
  return { x: 0, y: 0 };
}

export const NormalizedVertex: MessageFns<NormalizedVertex> = {
  encode(message: NormalizedVertex, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.x !== 0) {
      writer.uint32(13).float(message.x);
    }
    if (message.y !== 0) {
      writer.uint32(21).float(message.y);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NormalizedVertex {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNormalizedVertex();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 13) {
            break;
          }

          message.x = reader.float();
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.y = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NormalizedVertex {
    return {
      x: isSet(object.x) ? globalThis.Number(object.x) : 0,
      y: isSet(object.y) ? globalThis.Number(object.y) : 0,
    };
  },

  toJSON(message: NormalizedVertex): unknown {
    const obj: any = {};
    if (message.x !== 0) {
      obj.x = message.x;
    }
    if (message.y !== 0) {
      obj.y = message.y;
    }
    return obj;
  },

  create(base?: DeepPartial<NormalizedVertex>): NormalizedVertex {
    return NormalizedVertex.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<NormalizedVertex>): NormalizedVertex {
    const message = createBaseNormalizedVertex();
    message.x = object.x ?? 0;
    message.y = object.y ?? 0;
    return message;
  },
};

function createBaseNormalizedBoundingPoly(): NormalizedBoundingPoly {
  return { vertices: [] };
}

export const NormalizedBoundingPoly: MessageFns<NormalizedBoundingPoly> = {
  encode(message: NormalizedBoundingPoly, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.vertices) {
      NormalizedVertex.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NormalizedBoundingPoly {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNormalizedBoundingPoly();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.vertices.push(NormalizedVertex.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NormalizedBoundingPoly {
    return {
      vertices: globalThis.Array.isArray(object?.vertices)
        ? object.vertices.map((e: any) => NormalizedVertex.fromJSON(e))
        : [],
    };
  },

  toJSON(message: NormalizedBoundingPoly): unknown {
    const obj: any = {};
    if (message.vertices?.length) {
      obj.vertices = message.vertices.map((e) => NormalizedVertex.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<NormalizedBoundingPoly>): NormalizedBoundingPoly {
    return NormalizedBoundingPoly.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<NormalizedBoundingPoly>): NormalizedBoundingPoly {
    const message = createBaseNormalizedBoundingPoly();
    message.vertices = object.vertices?.map((e) => NormalizedVertex.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTextSegment(): TextSegment {
  return { segment: undefined, confidence: 0, frames: [] };
}

export const TextSegment: MessageFns<TextSegment> = {
  encode(message: TextSegment, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.segment !== undefined) {
      VideoSegment.encode(message.segment, writer.uint32(10).fork()).join();
    }
    if (message.confidence !== 0) {
      writer.uint32(21).float(message.confidence);
    }
    for (const v of message.frames) {
      TextFrame.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TextSegment {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextSegment();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.segment = VideoSegment.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.confidence = reader.float();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.frames.push(TextFrame.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextSegment {
    return {
      segment: isSet(object.segment) ? VideoSegment.fromJSON(object.segment) : undefined,
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
      frames: globalThis.Array.isArray(object?.frames) ? object.frames.map((e: any) => TextFrame.fromJSON(e)) : [],
    };
  },

  toJSON(message: TextSegment): unknown {
    const obj: any = {};
    if (message.segment !== undefined) {
      obj.segment = VideoSegment.toJSON(message.segment);
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    if (message.frames?.length) {
      obj.frames = message.frames.map((e) => TextFrame.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<TextSegment>): TextSegment {
    return TextSegment.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TextSegment>): TextSegment {
    const message = createBaseTextSegment();
    message.segment = (object.segment !== undefined && object.segment !== null)
      ? VideoSegment.fromPartial(object.segment)
      : undefined;
    message.confidence = object.confidence ?? 0;
    message.frames = object.frames?.map((e) => TextFrame.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTextFrame(): TextFrame {
  return { rotatedBoundingBox: undefined, timeOffset: undefined };
}

export const TextFrame: MessageFns<TextFrame> = {
  encode(message: TextFrame, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.rotatedBoundingBox !== undefined) {
      NormalizedBoundingPoly.encode(message.rotatedBoundingBox, writer.uint32(10).fork()).join();
    }
    if (message.timeOffset !== undefined) {
      Duration.encode(message.timeOffset, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TextFrame {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextFrame();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.rotatedBoundingBox = NormalizedBoundingPoly.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.timeOffset = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextFrame {
    return {
      rotatedBoundingBox: isSet(object.rotatedBoundingBox)
        ? NormalizedBoundingPoly.fromJSON(object.rotatedBoundingBox)
        : undefined,
      timeOffset: isSet(object.timeOffset) ? Duration.fromJSON(object.timeOffset) : undefined,
    };
  },

  toJSON(message: TextFrame): unknown {
    const obj: any = {};
    if (message.rotatedBoundingBox !== undefined) {
      obj.rotatedBoundingBox = NormalizedBoundingPoly.toJSON(message.rotatedBoundingBox);
    }
    if (message.timeOffset !== undefined) {
      obj.timeOffset = Duration.toJSON(message.timeOffset);
    }
    return obj;
  },

  create(base?: DeepPartial<TextFrame>): TextFrame {
    return TextFrame.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TextFrame>): TextFrame {
    const message = createBaseTextFrame();
    message.rotatedBoundingBox = (object.rotatedBoundingBox !== undefined && object.rotatedBoundingBox !== null)
      ? NormalizedBoundingPoly.fromPartial(object.rotatedBoundingBox)
      : undefined;
    message.timeOffset = (object.timeOffset !== undefined && object.timeOffset !== null)
      ? Duration.fromPartial(object.timeOffset)
      : undefined;
    return message;
  },
};

function createBaseTextAnnotation(): TextAnnotation {
  return { text: "", segments: [], version: "" };
}

export const TextAnnotation: MessageFns<TextAnnotation> = {
  encode(message: TextAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.text !== "") {
      writer.uint32(10).string(message.text);
    }
    for (const v of message.segments) {
      TextSegment.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.version !== "") {
      writer.uint32(26).string(message.version);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TextAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.text = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.segments.push(TextSegment.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.version = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextAnnotation {
    return {
      text: isSet(object.text) ? globalThis.String(object.text) : "",
      segments: globalThis.Array.isArray(object?.segments)
        ? object.segments.map((e: any) => TextSegment.fromJSON(e))
        : [],
      version: isSet(object.version) ? globalThis.String(object.version) : "",
    };
  },

  toJSON(message: TextAnnotation): unknown {
    const obj: any = {};
    if (message.text !== "") {
      obj.text = message.text;
    }
    if (message.segments?.length) {
      obj.segments = message.segments.map((e) => TextSegment.toJSON(e));
    }
    if (message.version !== "") {
      obj.version = message.version;
    }
    return obj;
  },

  create(base?: DeepPartial<TextAnnotation>): TextAnnotation {
    return TextAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TextAnnotation>): TextAnnotation {
    const message = createBaseTextAnnotation();
    message.text = object.text ?? "";
    message.segments = object.segments?.map((e) => TextSegment.fromPartial(e)) || [];
    message.version = object.version ?? "";
    return message;
  },
};

function createBaseObjectTrackingFrame(): ObjectTrackingFrame {
  return { normalizedBoundingBox: undefined, timeOffset: undefined };
}

export const ObjectTrackingFrame: MessageFns<ObjectTrackingFrame> = {
  encode(message: ObjectTrackingFrame, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.normalizedBoundingBox !== undefined) {
      NormalizedBoundingBox.encode(message.normalizedBoundingBox, writer.uint32(10).fork()).join();
    }
    if (message.timeOffset !== undefined) {
      Duration.encode(message.timeOffset, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ObjectTrackingFrame {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseObjectTrackingFrame();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.normalizedBoundingBox = NormalizedBoundingBox.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.timeOffset = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ObjectTrackingFrame {
    return {
      normalizedBoundingBox: isSet(object.normalizedBoundingBox)
        ? NormalizedBoundingBox.fromJSON(object.normalizedBoundingBox)
        : undefined,
      timeOffset: isSet(object.timeOffset) ? Duration.fromJSON(object.timeOffset) : undefined,
    };
  },

  toJSON(message: ObjectTrackingFrame): unknown {
    const obj: any = {};
    if (message.normalizedBoundingBox !== undefined) {
      obj.normalizedBoundingBox = NormalizedBoundingBox.toJSON(message.normalizedBoundingBox);
    }
    if (message.timeOffset !== undefined) {
      obj.timeOffset = Duration.toJSON(message.timeOffset);
    }
    return obj;
  },

  create(base?: DeepPartial<ObjectTrackingFrame>): ObjectTrackingFrame {
    return ObjectTrackingFrame.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ObjectTrackingFrame>): ObjectTrackingFrame {
    const message = createBaseObjectTrackingFrame();
    message.normalizedBoundingBox =
      (object.normalizedBoundingBox !== undefined && object.normalizedBoundingBox !== null)
        ? NormalizedBoundingBox.fromPartial(object.normalizedBoundingBox)
        : undefined;
    message.timeOffset = (object.timeOffset !== undefined && object.timeOffset !== null)
      ? Duration.fromPartial(object.timeOffset)
      : undefined;
    return message;
  },
};

function createBaseObjectTrackingAnnotation(): ObjectTrackingAnnotation {
  return { segment: undefined, trackId: undefined, entity: undefined, confidence: 0, frames: [], version: "" };
}

export const ObjectTrackingAnnotation: MessageFns<ObjectTrackingAnnotation> = {
  encode(message: ObjectTrackingAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.segment !== undefined) {
      VideoSegment.encode(message.segment, writer.uint32(26).fork()).join();
    }
    if (message.trackId !== undefined) {
      writer.uint32(40).int64(message.trackId.toString());
    }
    if (message.entity !== undefined) {
      Entity.encode(message.entity, writer.uint32(10).fork()).join();
    }
    if (message.confidence !== 0) {
      writer.uint32(37).float(message.confidence);
    }
    for (const v of message.frames) {
      ObjectTrackingFrame.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.version !== "") {
      writer.uint32(50).string(message.version);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ObjectTrackingAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseObjectTrackingAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 26) {
            break;
          }

          message.segment = VideoSegment.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.trackId = Long.fromString(reader.int64().toString());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.entity = Entity.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.confidence = reader.float();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.frames.push(ObjectTrackingFrame.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.version = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ObjectTrackingAnnotation {
    return {
      segment: isSet(object.segment) ? VideoSegment.fromJSON(object.segment) : undefined,
      trackId: isSet(object.trackId) ? Long.fromValue(object.trackId) : undefined,
      entity: isSet(object.entity) ? Entity.fromJSON(object.entity) : undefined,
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
      frames: globalThis.Array.isArray(object?.frames)
        ? object.frames.map((e: any) => ObjectTrackingFrame.fromJSON(e))
        : [],
      version: isSet(object.version) ? globalThis.String(object.version) : "",
    };
  },

  toJSON(message: ObjectTrackingAnnotation): unknown {
    const obj: any = {};
    if (message.segment !== undefined) {
      obj.segment = VideoSegment.toJSON(message.segment);
    }
    if (message.trackId !== undefined) {
      obj.trackId = (message.trackId || Long.ZERO).toString();
    }
    if (message.entity !== undefined) {
      obj.entity = Entity.toJSON(message.entity);
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    if (message.frames?.length) {
      obj.frames = message.frames.map((e) => ObjectTrackingFrame.toJSON(e));
    }
    if (message.version !== "") {
      obj.version = message.version;
    }
    return obj;
  },

  create(base?: DeepPartial<ObjectTrackingAnnotation>): ObjectTrackingAnnotation {
    return ObjectTrackingAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ObjectTrackingAnnotation>): ObjectTrackingAnnotation {
    const message = createBaseObjectTrackingAnnotation();
    message.segment = (object.segment !== undefined && object.segment !== null)
      ? VideoSegment.fromPartial(object.segment)
      : undefined;
    message.trackId = (object.trackId !== undefined && object.trackId !== null)
      ? Long.fromValue(object.trackId)
      : undefined;
    message.entity = (object.entity !== undefined && object.entity !== null)
      ? Entity.fromPartial(object.entity)
      : undefined;
    message.confidence = object.confidence ?? 0;
    message.frames = object.frames?.map((e) => ObjectTrackingFrame.fromPartial(e)) || [];
    message.version = object.version ?? "";
    return message;
  },
};

function createBaseLogoRecognitionAnnotation(): LogoRecognitionAnnotation {
  return { entity: undefined, tracks: [], segments: [] };
}

export const LogoRecognitionAnnotation: MessageFns<LogoRecognitionAnnotation> = {
  encode(message: LogoRecognitionAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.entity !== undefined) {
      Entity.encode(message.entity, writer.uint32(10).fork()).join();
    }
    for (const v of message.tracks) {
      Track.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.segments) {
      VideoSegment.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LogoRecognitionAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLogoRecognitionAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.entity = Entity.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.tracks.push(Track.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.segments.push(VideoSegment.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LogoRecognitionAnnotation {
    return {
      entity: isSet(object.entity) ? Entity.fromJSON(object.entity) : undefined,
      tracks: globalThis.Array.isArray(object?.tracks) ? object.tracks.map((e: any) => Track.fromJSON(e)) : [],
      segments: globalThis.Array.isArray(object?.segments)
        ? object.segments.map((e: any) => VideoSegment.fromJSON(e))
        : [],
    };
  },

  toJSON(message: LogoRecognitionAnnotation): unknown {
    const obj: any = {};
    if (message.entity !== undefined) {
      obj.entity = Entity.toJSON(message.entity);
    }
    if (message.tracks?.length) {
      obj.tracks = message.tracks.map((e) => Track.toJSON(e));
    }
    if (message.segments?.length) {
      obj.segments = message.segments.map((e) => VideoSegment.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<LogoRecognitionAnnotation>): LogoRecognitionAnnotation {
    return LogoRecognitionAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LogoRecognitionAnnotation>): LogoRecognitionAnnotation {
    const message = createBaseLogoRecognitionAnnotation();
    message.entity = (object.entity !== undefined && object.entity !== null)
      ? Entity.fromPartial(object.entity)
      : undefined;
    message.tracks = object.tracks?.map((e) => Track.fromPartial(e)) || [];
    message.segments = object.segments?.map((e) => VideoSegment.fromPartial(e)) || [];
    return message;
  },
};

/** Service that implements the Video Intelligence API. */
export type VideoIntelligenceServiceDefinition = typeof VideoIntelligenceServiceDefinition;
export const VideoIntelligenceServiceDefinition = {
  name: "VideoIntelligenceService",
  fullName: "google.cloud.videointelligence.v1.VideoIntelligenceService",
  methods: {
    /**
     * Performs asynchronous video annotation. Progress and results can be
     * retrieved through the `google.longrunning.Operations` interface.
     * `Operation.metadata` contains `AnnotateVideoProgress` (progress).
     * `Operation.response` contains `AnnotateVideoResponse` (results).
     */
    annotateVideo: {
      name: "AnnotateVideo",
      requestType: AnnotateVideoRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              46,
              10,
              21,
              65,
              110,
              110,
              111,
              116,
              97,
              116,
              101,
              86,
              105,
              100,
              101,
              111,
              82,
              101,
              115,
              112,
              111,
              110,
              115,
              101,
              18,
              21,
              65,
              110,
              110,
              111,
              116,
              97,
              116,
              101,
              86,
              105,
              100,
              101,
              111,
              80,
              114,
              111,
              103,
              114,
              101,
              115,
              115,
            ]),
          ],
          8410: [
            Buffer.from([18, 105, 110, 112, 117, 116, 95, 117, 114, 105, 44, 102, 101, 97, 116, 117, 114, 101, 115]),
          ],
          578365826: [
            Buffer.from([
              24,
              58,
              1,
              42,
              34,
              19,
              47,
              118,
              49,
              47,
              118,
              105,
              100,
              101,
              111,
              115,
              58,
              97,
              110,
              110,
              111,
              116,
              97,
              116,
              101,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface VideoIntelligenceServiceImplementation<CallContextExt = {}> {
  /**
   * Performs asynchronous video annotation. Progress and results can be
   * retrieved through the `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `AnnotateVideoProgress` (progress).
   * `Operation.response` contains `AnnotateVideoResponse` (results).
   */
  annotateVideo(request: AnnotateVideoRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
}

export interface VideoIntelligenceServiceClient<CallOptionsExt = {}> {
  /**
   * Performs asynchronous video annotation. Progress and results can be
   * retrieved through the `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `AnnotateVideoProgress` (progress).
   * `Operation.response` contains `AnnotateVideoResponse` (results).
   */
  annotateVideo(request: DeepPartial<AnnotateVideoRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
}

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
