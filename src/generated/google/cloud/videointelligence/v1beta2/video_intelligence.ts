// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/videointelligence/v1beta2/video_intelligence.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Operation } from "../../../longrunning/operations.js";
import { Duration } from "../../../protobuf/duration.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { Status } from "../../../rpc/status.js";

export const protobufPackage = "google.cloud.videointelligence.v1beta2";

/** Video annotation feature. */
export enum Feature {
  /** FEATURE_UNSPECIFIED - Unspecified. */
  FEATURE_UNSPECIFIED = 0,
  /** LABEL_DETECTION - Label detection. Detect objects, such as dog or flower. */
  LABEL_DETECTION = 1,
  /** SHOT_CHANGE_DETECTION - Shot change detection. */
  SHOT_CHANGE_DETECTION = 2,
  /** EXPLICIT_CONTENT_DETECTION - Explicit content detection. */
  EXPLICIT_CONTENT_DETECTION = 3,
  /** FACE_DETECTION - Human face detection and tracking. */
  FACE_DETECTION = 4,
  UNRECOGNIZED = -1,
}

export function featureFromJSON(object: any): Feature {
  switch (object) {
    case 0:
    case "FEATURE_UNSPECIFIED":
      return Feature.FEATURE_UNSPECIFIED;
    case 1:
    case "LABEL_DETECTION":
      return Feature.LABEL_DETECTION;
    case 2:
    case "SHOT_CHANGE_DETECTION":
      return Feature.SHOT_CHANGE_DETECTION;
    case 3:
    case "EXPLICIT_CONTENT_DETECTION":
      return Feature.EXPLICIT_CONTENT_DETECTION;
    case 4:
    case "FACE_DETECTION":
      return Feature.FACE_DETECTION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Feature.UNRECOGNIZED;
  }
}

export function featureToJSON(object: Feature): string {
  switch (object) {
    case Feature.FEATURE_UNSPECIFIED:
      return "FEATURE_UNSPECIFIED";
    case Feature.LABEL_DETECTION:
      return "LABEL_DETECTION";
    case Feature.SHOT_CHANGE_DETECTION:
      return "SHOT_CHANGE_DETECTION";
    case Feature.EXPLICIT_CONTENT_DETECTION:
      return "EXPLICIT_CONTENT_DETECTION";
    case Feature.FACE_DETECTION:
      return "FACE_DETECTION";
    case Feature.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Label detection mode. */
export enum LabelDetectionMode {
  /** LABEL_DETECTION_MODE_UNSPECIFIED - Unspecified. */
  LABEL_DETECTION_MODE_UNSPECIFIED = 0,
  /** SHOT_MODE - Detect shot-level labels. */
  SHOT_MODE = 1,
  /** FRAME_MODE - Detect frame-level labels. */
  FRAME_MODE = 2,
  /** SHOT_AND_FRAME_MODE - Detect both shot-level and frame-level labels. */
  SHOT_AND_FRAME_MODE = 3,
  UNRECOGNIZED = -1,
}

export function labelDetectionModeFromJSON(object: any): LabelDetectionMode {
  switch (object) {
    case 0:
    case "LABEL_DETECTION_MODE_UNSPECIFIED":
      return LabelDetectionMode.LABEL_DETECTION_MODE_UNSPECIFIED;
    case 1:
    case "SHOT_MODE":
      return LabelDetectionMode.SHOT_MODE;
    case 2:
    case "FRAME_MODE":
      return LabelDetectionMode.FRAME_MODE;
    case 3:
    case "SHOT_AND_FRAME_MODE":
      return LabelDetectionMode.SHOT_AND_FRAME_MODE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return LabelDetectionMode.UNRECOGNIZED;
  }
}

export function labelDetectionModeToJSON(object: LabelDetectionMode): string {
  switch (object) {
    case LabelDetectionMode.LABEL_DETECTION_MODE_UNSPECIFIED:
      return "LABEL_DETECTION_MODE_UNSPECIFIED";
    case LabelDetectionMode.SHOT_MODE:
      return "SHOT_MODE";
    case LabelDetectionMode.FRAME_MODE:
      return "FRAME_MODE";
    case LabelDetectionMode.SHOT_AND_FRAME_MODE:
      return "SHOT_AND_FRAME_MODE";
    case LabelDetectionMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Bucketized representation of likelihood. */
export enum Likelihood {
  /** LIKELIHOOD_UNSPECIFIED - Unspecified likelihood. */
  LIKELIHOOD_UNSPECIFIED = 0,
  /** VERY_UNLIKELY - Very unlikely. */
  VERY_UNLIKELY = 1,
  /** UNLIKELY - Unlikely. */
  UNLIKELY = 2,
  /** POSSIBLE - Possible. */
  POSSIBLE = 3,
  /** LIKELY - Likely. */
  LIKELY = 4,
  /** VERY_LIKELY - Very likely. */
  VERY_LIKELY = 5,
  UNRECOGNIZED = -1,
}

export function likelihoodFromJSON(object: any): Likelihood {
  switch (object) {
    case 0:
    case "LIKELIHOOD_UNSPECIFIED":
      return Likelihood.LIKELIHOOD_UNSPECIFIED;
    case 1:
    case "VERY_UNLIKELY":
      return Likelihood.VERY_UNLIKELY;
    case 2:
    case "UNLIKELY":
      return Likelihood.UNLIKELY;
    case 3:
    case "POSSIBLE":
      return Likelihood.POSSIBLE;
    case 4:
    case "LIKELY":
      return Likelihood.LIKELY;
    case 5:
    case "VERY_LIKELY":
      return Likelihood.VERY_LIKELY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Likelihood.UNRECOGNIZED;
  }
}

export function likelihoodToJSON(object: Likelihood): string {
  switch (object) {
    case Likelihood.LIKELIHOOD_UNSPECIFIED:
      return "LIKELIHOOD_UNSPECIFIED";
    case Likelihood.VERY_UNLIKELY:
      return "VERY_UNLIKELY";
    case Likelihood.UNLIKELY:
      return "UNLIKELY";
    case Likelihood.POSSIBLE:
      return "POSSIBLE";
    case Likelihood.LIKELY:
      return "LIKELY";
    case Likelihood.VERY_LIKELY:
      return "VERY_LIKELY";
    case Likelihood.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Video annotation request. */
export interface AnnotateVideoRequest {
  /**
   * Input video location. Currently, only
   * [Google Cloud Storage](https://cloud.google.com/storage/) URIs are
   * supported, which must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
   * more information, see [Request
   * URIs](https://cloud.google.com/storage/docs/request-endpoints). A video URI
   * may include wildcards in `object-id`, and thus identify multiple videos.
   * Supported wildcards: '*' to match 0 or more characters;
   * '?' to match 1 character. If unset, the input video should be embedded
   * in the request as `input_content`. If set, `input_content` should be unset.
   */
  inputUri: string;
  /**
   * The video data bytes.
   * If unset, the input video(s) should be specified via `input_uri`.
   * If set, `input_uri` should be unset.
   */
  inputContent: Buffer;
  /** Required. Requested video annotation features. */
  features: Feature[];
  /** Additional video context and/or feature-specific parameters. */
  videoContext:
    | VideoContext
    | undefined;
  /**
   * Optional. Location where the output (in JSON format) should be stored.
   * Currently, only [Google Cloud Storage](https://cloud.google.com/storage/)
   * URIs are supported, which must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
   * more information, see [Request
   * URIs](https://cloud.google.com/storage/docs/request-endpoints).
   */
  outputUri: string;
  /**
   * Optional. Cloud region where annotation should take place. Supported cloud
   * regions: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no region
   * is specified, a region will be determined based on video file location.
   */
  locationId: string;
}

/** Video context and/or feature-specific parameters. */
export interface VideoContext {
  /**
   * Video segments to annotate. The segments may overlap and are not required
   * to be contiguous or span the whole video. If unspecified, each video is
   * treated as a single segment.
   */
  segments: VideoSegment[];
  /** Config for LABEL_DETECTION. */
  labelDetectionConfig:
    | LabelDetectionConfig
    | undefined;
  /** Config for SHOT_CHANGE_DETECTION. */
  shotChangeDetectionConfig:
    | ShotChangeDetectionConfig
    | undefined;
  /** Config for EXPLICIT_CONTENT_DETECTION. */
  explicitContentDetectionConfig:
    | ExplicitContentDetectionConfig
    | undefined;
  /** Config for FACE_DETECTION. */
  faceDetectionConfig: FaceDetectionConfig | undefined;
}

/** Config for LABEL_DETECTION. */
export interface LabelDetectionConfig {
  /**
   * What labels should be detected with LABEL_DETECTION, in addition to
   * video-level labels or segment-level labels.
   * If unspecified, defaults to `SHOT_MODE`.
   */
  labelDetectionMode: LabelDetectionMode;
  /**
   * Whether the video has been shot from a stationary (i.e. non-moving) camera.
   * When set to true, might improve detection accuracy for moving objects.
   * Should be used with `SHOT_AND_FRAME_MODE` enabled.
   */
  stationaryCamera: boolean;
  /**
   * Model to use for label detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   */
  model: string;
}

/** Config for SHOT_CHANGE_DETECTION. */
export interface ShotChangeDetectionConfig {
  /**
   * Model to use for shot change detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   */
  model: string;
}

/** Config for EXPLICIT_CONTENT_DETECTION. */
export interface ExplicitContentDetectionConfig {
  /**
   * Model to use for explicit content detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   */
  model: string;
}

/** Config for FACE_DETECTION. */
export interface FaceDetectionConfig {
  /**
   * Model to use for face detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   */
  model: string;
  /** Whether bounding boxes be included in the face annotation output. */
  includeBoundingBoxes: boolean;
}

/** Video segment. */
export interface VideoSegment {
  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the start of the segment (inclusive).
   */
  startTimeOffset:
    | Duration
    | undefined;
  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the end of the segment (inclusive).
   */
  endTimeOffset: Duration | undefined;
}

/** Video segment level annotation results for label detection. */
export interface LabelSegment {
  /** Video segment where a label was detected. */
  segment:
    | VideoSegment
    | undefined;
  /** Confidence that the label is accurate. Range: [0, 1]. */
  confidence: number;
}

/** Video frame level annotation results for label detection. */
export interface LabelFrame {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   */
  timeOffset:
    | Duration
    | undefined;
  /** Confidence that the label is accurate. Range: [0, 1]. */
  confidence: number;
}

/** Detected entity from video analysis. */
export interface Entity {
  /**
   * Opaque entity ID. Some IDs may be available in
   * [Google Knowledge Graph Search
   * API](https://developers.google.com/knowledge-graph/).
   */
  entityId: string;
  /** Textual description, e.g. `Fixed-gear bicycle`. */
  description: string;
  /** Language code for `description` in BCP-47 format. */
  languageCode: string;
}

/** Label annotation. */
export interface LabelAnnotation {
  /** Detected entity. */
  entity:
    | Entity
    | undefined;
  /**
   * Common categories for the detected entity.
   * E.g. when the label is `Terrier` the category is likely `dog`. And in some
   * cases there might be more than one categories e.g. `Terrier` could also be
   * a `pet`.
   */
  categoryEntities: Entity[];
  /** All video segments where a label was detected. */
  segments: LabelSegment[];
  /** All video frames where a label was detected. */
  frames: LabelFrame[];
}

/** Video frame level annotation results for explicit content. */
export interface ExplicitContentFrame {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   */
  timeOffset:
    | Duration
    | undefined;
  /** Likelihood of the pornography content.. */
  pornographyLikelihood: Likelihood;
}

/**
 * Explicit content annotation (based on per-frame visual signals only).
 * If no explicit content has been detected in a frame, no annotations are
 * present for that frame.
 */
export interface ExplicitContentAnnotation {
  /** All video frames where explicit content was detected. */
  frames: ExplicitContentFrame[];
}

/**
 * Normalized bounding box.
 * The normalized vertex coordinates are relative to the original image.
 * Range: [0, 1].
 */
export interface NormalizedBoundingBox {
  /** Left X coordinate. */
  left: number;
  /** Top Y coordinate. */
  top: number;
  /** Right X coordinate. */
  right: number;
  /** Bottom Y coordinate. */
  bottom: number;
}

/** Video segment level annotation results for face detection. */
export interface FaceSegment {
  /** Video segment where a face was detected. */
  segment: VideoSegment | undefined;
}

/** Video frame level annotation results for face detection. */
export interface FaceFrame {
  /**
   * Normalized Bounding boxes in a frame.
   * There can be more than one boxes if the same face is detected in multiple
   * locations within the current frame.
   */
  normalizedBoundingBoxes: NormalizedBoundingBox[];
  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the video frame for this location.
   */
  timeOffset: Duration | undefined;
}

/** Face annotation. */
export interface FaceAnnotation {
  /** Thumbnail of a representative face view (in JPEG format). */
  thumbnail: Buffer;
  /** All video segments where a face was detected. */
  segments: FaceSegment[];
  /** All video frames where a face was detected. */
  frames: FaceFrame[];
}

/** Annotation results for a single video. */
export interface VideoAnnotationResults {
  /**
   * Video file location in
   * [Google Cloud Storage](https://cloud.google.com/storage/).
   */
  inputUri: string;
  /**
   * Label annotations on video level or user specified segment level.
   * There is exactly one element for each unique label.
   */
  segmentLabelAnnotations: LabelAnnotation[];
  /**
   * Label annotations on shot level.
   * There is exactly one element for each unique label.
   */
  shotLabelAnnotations: LabelAnnotation[];
  /**
   * Label annotations on frame level.
   * There is exactly one element for each unique label.
   */
  frameLabelAnnotations: LabelAnnotation[];
  /** Face annotations. There is exactly one element for each unique face. */
  faceAnnotations: FaceAnnotation[];
  /** Shot annotations. Each shot is represented as a video segment. */
  shotAnnotations: VideoSegment[];
  /** Explicit content annotation. */
  explicitAnnotation:
    | ExplicitContentAnnotation
    | undefined;
  /**
   * If set, indicates an error. Note that for a single `AnnotateVideoRequest`
   * some videos may succeed and some may fail.
   */
  error: Status | undefined;
}

/**
 * Video annotation response. Included in the `response`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 */
export interface AnnotateVideoResponse {
  /** Annotation results for all videos specified in `AnnotateVideoRequest`. */
  annotationResults: VideoAnnotationResults[];
}

/** Annotation progress for a single video. */
export interface VideoAnnotationProgress {
  /**
   * Video file location in
   * [Google Cloud Storage](https://cloud.google.com/storage/).
   */
  inputUri: string;
  /**
   * Approximate percentage processed thus far.
   * Guaranteed to be 100 when fully processed.
   */
  progressPercent: number;
  /** Time when the request was received. */
  startTime:
    | Date
    | undefined;
  /** Time of the most recent update. */
  updateTime: Date | undefined;
}

/**
 * Video annotation progress. Included in the `metadata`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 */
export interface AnnotateVideoProgress {
  /** Progress metadata for all videos specified in `AnnotateVideoRequest`. */
  annotationProgress: VideoAnnotationProgress[];
}

function createBaseAnnotateVideoRequest(): AnnotateVideoRequest {
  return {
    inputUri: "",
    inputContent: Buffer.alloc(0),
    features: [],
    videoContext: undefined,
    outputUri: "",
    locationId: "",
  };
}

export const AnnotateVideoRequest: MessageFns<AnnotateVideoRequest> = {
  encode(message: AnnotateVideoRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputUri !== "") {
      writer.uint32(10).string(message.inputUri);
    }
    if (message.inputContent.length !== 0) {
      writer.uint32(50).bytes(message.inputContent);
    }
    writer.uint32(18).fork();
    for (const v of message.features) {
      writer.int32(v);
    }
    writer.join();
    if (message.videoContext !== undefined) {
      VideoContext.encode(message.videoContext, writer.uint32(26).fork()).join();
    }
    if (message.outputUri !== "") {
      writer.uint32(34).string(message.outputUri);
    }
    if (message.locationId !== "") {
      writer.uint32(42).string(message.locationId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnnotateVideoRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnnotateVideoRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputUri = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.inputContent = Buffer.from(reader.bytes());
          continue;
        case 2:
          if (tag === 16) {
            message.features.push(reader.int32() as any);

            continue;
          }

          if (tag === 18) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.features.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.videoContext = VideoContext.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.outputUri = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.locationId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnnotateVideoRequest {
    return {
      inputUri: isSet(object.inputUri) ? globalThis.String(object.inputUri) : "",
      inputContent: isSet(object.inputContent) ? Buffer.from(bytesFromBase64(object.inputContent)) : Buffer.alloc(0),
      features: globalThis.Array.isArray(object?.features) ? object.features.map((e: any) => featureFromJSON(e)) : [],
      videoContext: isSet(object.videoContext) ? VideoContext.fromJSON(object.videoContext) : undefined,
      outputUri: isSet(object.outputUri) ? globalThis.String(object.outputUri) : "",
      locationId: isSet(object.locationId) ? globalThis.String(object.locationId) : "",
    };
  },

  toJSON(message: AnnotateVideoRequest): unknown {
    const obj: any = {};
    if (message.inputUri !== "") {
      obj.inputUri = message.inputUri;
    }
    if (message.inputContent.length !== 0) {
      obj.inputContent = base64FromBytes(message.inputContent);
    }
    if (message.features?.length) {
      obj.features = message.features.map((e) => featureToJSON(e));
    }
    if (message.videoContext !== undefined) {
      obj.videoContext = VideoContext.toJSON(message.videoContext);
    }
    if (message.outputUri !== "") {
      obj.outputUri = message.outputUri;
    }
    if (message.locationId !== "") {
      obj.locationId = message.locationId;
    }
    return obj;
  },

  create(base?: DeepPartial<AnnotateVideoRequest>): AnnotateVideoRequest {
    return AnnotateVideoRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AnnotateVideoRequest>): AnnotateVideoRequest {
    const message = createBaseAnnotateVideoRequest();
    message.inputUri = object.inputUri ?? "";
    message.inputContent = object.inputContent ?? Buffer.alloc(0);
    message.features = object.features?.map((e) => e) || [];
    message.videoContext = (object.videoContext !== undefined && object.videoContext !== null)
      ? VideoContext.fromPartial(object.videoContext)
      : undefined;
    message.outputUri = object.outputUri ?? "";
    message.locationId = object.locationId ?? "";
    return message;
  },
};

function createBaseVideoContext(): VideoContext {
  return {
    segments: [],
    labelDetectionConfig: undefined,
    shotChangeDetectionConfig: undefined,
    explicitContentDetectionConfig: undefined,
    faceDetectionConfig: undefined,
  };
}

export const VideoContext: MessageFns<VideoContext> = {
  encode(message: VideoContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.segments) {
      VideoSegment.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.labelDetectionConfig !== undefined) {
      LabelDetectionConfig.encode(message.labelDetectionConfig, writer.uint32(18).fork()).join();
    }
    if (message.shotChangeDetectionConfig !== undefined) {
      ShotChangeDetectionConfig.encode(message.shotChangeDetectionConfig, writer.uint32(26).fork()).join();
    }
    if (message.explicitContentDetectionConfig !== undefined) {
      ExplicitContentDetectionConfig.encode(message.explicitContentDetectionConfig, writer.uint32(34).fork()).join();
    }
    if (message.faceDetectionConfig !== undefined) {
      FaceDetectionConfig.encode(message.faceDetectionConfig, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VideoContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVideoContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.segments.push(VideoSegment.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.labelDetectionConfig = LabelDetectionConfig.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.shotChangeDetectionConfig = ShotChangeDetectionConfig.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.explicitContentDetectionConfig = ExplicitContentDetectionConfig.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.faceDetectionConfig = FaceDetectionConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VideoContext {
    return {
      segments: globalThis.Array.isArray(object?.segments)
        ? object.segments.map((e: any) => VideoSegment.fromJSON(e))
        : [],
      labelDetectionConfig: isSet(object.labelDetectionConfig)
        ? LabelDetectionConfig.fromJSON(object.labelDetectionConfig)
        : undefined,
      shotChangeDetectionConfig: isSet(object.shotChangeDetectionConfig)
        ? ShotChangeDetectionConfig.fromJSON(object.shotChangeDetectionConfig)
        : undefined,
      explicitContentDetectionConfig: isSet(object.explicitContentDetectionConfig)
        ? ExplicitContentDetectionConfig.fromJSON(object.explicitContentDetectionConfig)
        : undefined,
      faceDetectionConfig: isSet(object.faceDetectionConfig)
        ? FaceDetectionConfig.fromJSON(object.faceDetectionConfig)
        : undefined,
    };
  },

  toJSON(message: VideoContext): unknown {
    const obj: any = {};
    if (message.segments?.length) {
      obj.segments = message.segments.map((e) => VideoSegment.toJSON(e));
    }
    if (message.labelDetectionConfig !== undefined) {
      obj.labelDetectionConfig = LabelDetectionConfig.toJSON(message.labelDetectionConfig);
    }
    if (message.shotChangeDetectionConfig !== undefined) {
      obj.shotChangeDetectionConfig = ShotChangeDetectionConfig.toJSON(message.shotChangeDetectionConfig);
    }
    if (message.explicitContentDetectionConfig !== undefined) {
      obj.explicitContentDetectionConfig = ExplicitContentDetectionConfig.toJSON(
        message.explicitContentDetectionConfig,
      );
    }
    if (message.faceDetectionConfig !== undefined) {
      obj.faceDetectionConfig = FaceDetectionConfig.toJSON(message.faceDetectionConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<VideoContext>): VideoContext {
    return VideoContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VideoContext>): VideoContext {
    const message = createBaseVideoContext();
    message.segments = object.segments?.map((e) => VideoSegment.fromPartial(e)) || [];
    message.labelDetectionConfig = (object.labelDetectionConfig !== undefined && object.labelDetectionConfig !== null)
      ? LabelDetectionConfig.fromPartial(object.labelDetectionConfig)
      : undefined;
    message.shotChangeDetectionConfig =
      (object.shotChangeDetectionConfig !== undefined && object.shotChangeDetectionConfig !== null)
        ? ShotChangeDetectionConfig.fromPartial(object.shotChangeDetectionConfig)
        : undefined;
    message.explicitContentDetectionConfig =
      (object.explicitContentDetectionConfig !== undefined && object.explicitContentDetectionConfig !== null)
        ? ExplicitContentDetectionConfig.fromPartial(object.explicitContentDetectionConfig)
        : undefined;
    message.faceDetectionConfig = (object.faceDetectionConfig !== undefined && object.faceDetectionConfig !== null)
      ? FaceDetectionConfig.fromPartial(object.faceDetectionConfig)
      : undefined;
    return message;
  },
};

function createBaseLabelDetectionConfig(): LabelDetectionConfig {
  return { labelDetectionMode: 0, stationaryCamera: false, model: "" };
}

export const LabelDetectionConfig: MessageFns<LabelDetectionConfig> = {
  encode(message: LabelDetectionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.labelDetectionMode !== 0) {
      writer.uint32(8).int32(message.labelDetectionMode);
    }
    if (message.stationaryCamera !== false) {
      writer.uint32(16).bool(message.stationaryCamera);
    }
    if (message.model !== "") {
      writer.uint32(26).string(message.model);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LabelDetectionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLabelDetectionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.labelDetectionMode = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.stationaryCamera = reader.bool();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.model = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LabelDetectionConfig {
    return {
      labelDetectionMode: isSet(object.labelDetectionMode) ? labelDetectionModeFromJSON(object.labelDetectionMode) : 0,
      stationaryCamera: isSet(object.stationaryCamera) ? globalThis.Boolean(object.stationaryCamera) : false,
      model: isSet(object.model) ? globalThis.String(object.model) : "",
    };
  },

  toJSON(message: LabelDetectionConfig): unknown {
    const obj: any = {};
    if (message.labelDetectionMode !== 0) {
      obj.labelDetectionMode = labelDetectionModeToJSON(message.labelDetectionMode);
    }
    if (message.stationaryCamera !== false) {
      obj.stationaryCamera = message.stationaryCamera;
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    return obj;
  },

  create(base?: DeepPartial<LabelDetectionConfig>): LabelDetectionConfig {
    return LabelDetectionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LabelDetectionConfig>): LabelDetectionConfig {
    const message = createBaseLabelDetectionConfig();
    message.labelDetectionMode = object.labelDetectionMode ?? 0;
    message.stationaryCamera = object.stationaryCamera ?? false;
    message.model = object.model ?? "";
    return message;
  },
};

function createBaseShotChangeDetectionConfig(): ShotChangeDetectionConfig {
  return { model: "" };
}

export const ShotChangeDetectionConfig: MessageFns<ShotChangeDetectionConfig> = {
  encode(message: ShotChangeDetectionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ShotChangeDetectionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseShotChangeDetectionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ShotChangeDetectionConfig {
    return { model: isSet(object.model) ? globalThis.String(object.model) : "" };
  },

  toJSON(message: ShotChangeDetectionConfig): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    return obj;
  },

  create(base?: DeepPartial<ShotChangeDetectionConfig>): ShotChangeDetectionConfig {
    return ShotChangeDetectionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ShotChangeDetectionConfig>): ShotChangeDetectionConfig {
    const message = createBaseShotChangeDetectionConfig();
    message.model = object.model ?? "";
    return message;
  },
};

function createBaseExplicitContentDetectionConfig(): ExplicitContentDetectionConfig {
  return { model: "" };
}

export const ExplicitContentDetectionConfig: MessageFns<ExplicitContentDetectionConfig> = {
  encode(message: ExplicitContentDetectionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplicitContentDetectionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplicitContentDetectionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplicitContentDetectionConfig {
    return { model: isSet(object.model) ? globalThis.String(object.model) : "" };
  },

  toJSON(message: ExplicitContentDetectionConfig): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    return obj;
  },

  create(base?: DeepPartial<ExplicitContentDetectionConfig>): ExplicitContentDetectionConfig {
    return ExplicitContentDetectionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplicitContentDetectionConfig>): ExplicitContentDetectionConfig {
    const message = createBaseExplicitContentDetectionConfig();
    message.model = object.model ?? "";
    return message;
  },
};

function createBaseFaceDetectionConfig(): FaceDetectionConfig {
  return { model: "", includeBoundingBoxes: false };
}

export const FaceDetectionConfig: MessageFns<FaceDetectionConfig> = {
  encode(message: FaceDetectionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    if (message.includeBoundingBoxes !== false) {
      writer.uint32(16).bool(message.includeBoundingBoxes);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FaceDetectionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFaceDetectionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.includeBoundingBoxes = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FaceDetectionConfig {
    return {
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      includeBoundingBoxes: isSet(object.includeBoundingBoxes)
        ? globalThis.Boolean(object.includeBoundingBoxes)
        : false,
    };
  },

  toJSON(message: FaceDetectionConfig): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.includeBoundingBoxes !== false) {
      obj.includeBoundingBoxes = message.includeBoundingBoxes;
    }
    return obj;
  },

  create(base?: DeepPartial<FaceDetectionConfig>): FaceDetectionConfig {
    return FaceDetectionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FaceDetectionConfig>): FaceDetectionConfig {
    const message = createBaseFaceDetectionConfig();
    message.model = object.model ?? "";
    message.includeBoundingBoxes = object.includeBoundingBoxes ?? false;
    return message;
  },
};

function createBaseVideoSegment(): VideoSegment {
  return { startTimeOffset: undefined, endTimeOffset: undefined };
}

export const VideoSegment: MessageFns<VideoSegment> = {
  encode(message: VideoSegment, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startTimeOffset !== undefined) {
      Duration.encode(message.startTimeOffset, writer.uint32(10).fork()).join();
    }
    if (message.endTimeOffset !== undefined) {
      Duration.encode(message.endTimeOffset, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VideoSegment {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVideoSegment();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startTimeOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.endTimeOffset = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VideoSegment {
    return {
      startTimeOffset: isSet(object.startTimeOffset) ? Duration.fromJSON(object.startTimeOffset) : undefined,
      endTimeOffset: isSet(object.endTimeOffset) ? Duration.fromJSON(object.endTimeOffset) : undefined,
    };
  },

  toJSON(message: VideoSegment): unknown {
    const obj: any = {};
    if (message.startTimeOffset !== undefined) {
      obj.startTimeOffset = Duration.toJSON(message.startTimeOffset);
    }
    if (message.endTimeOffset !== undefined) {
      obj.endTimeOffset = Duration.toJSON(message.endTimeOffset);
    }
    return obj;
  },

  create(base?: DeepPartial<VideoSegment>): VideoSegment {
    return VideoSegment.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VideoSegment>): VideoSegment {
    const message = createBaseVideoSegment();
    message.startTimeOffset = (object.startTimeOffset !== undefined && object.startTimeOffset !== null)
      ? Duration.fromPartial(object.startTimeOffset)
      : undefined;
    message.endTimeOffset = (object.endTimeOffset !== undefined && object.endTimeOffset !== null)
      ? Duration.fromPartial(object.endTimeOffset)
      : undefined;
    return message;
  },
};

function createBaseLabelSegment(): LabelSegment {
  return { segment: undefined, confidence: 0 };
}

export const LabelSegment: MessageFns<LabelSegment> = {
  encode(message: LabelSegment, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.segment !== undefined) {
      VideoSegment.encode(message.segment, writer.uint32(10).fork()).join();
    }
    if (message.confidence !== 0) {
      writer.uint32(21).float(message.confidence);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LabelSegment {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLabelSegment();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.segment = VideoSegment.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.confidence = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LabelSegment {
    return {
      segment: isSet(object.segment) ? VideoSegment.fromJSON(object.segment) : undefined,
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
    };
  },

  toJSON(message: LabelSegment): unknown {
    const obj: any = {};
    if (message.segment !== undefined) {
      obj.segment = VideoSegment.toJSON(message.segment);
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    return obj;
  },

  create(base?: DeepPartial<LabelSegment>): LabelSegment {
    return LabelSegment.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LabelSegment>): LabelSegment {
    const message = createBaseLabelSegment();
    message.segment = (object.segment !== undefined && object.segment !== null)
      ? VideoSegment.fromPartial(object.segment)
      : undefined;
    message.confidence = object.confidence ?? 0;
    return message;
  },
};

function createBaseLabelFrame(): LabelFrame {
  return { timeOffset: undefined, confidence: 0 };
}

export const LabelFrame: MessageFns<LabelFrame> = {
  encode(message: LabelFrame, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.timeOffset !== undefined) {
      Duration.encode(message.timeOffset, writer.uint32(10).fork()).join();
    }
    if (message.confidence !== 0) {
      writer.uint32(21).float(message.confidence);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LabelFrame {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLabelFrame();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.timeOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.confidence = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LabelFrame {
    return {
      timeOffset: isSet(object.timeOffset) ? Duration.fromJSON(object.timeOffset) : undefined,
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
    };
  },

  toJSON(message: LabelFrame): unknown {
    const obj: any = {};
    if (message.timeOffset !== undefined) {
      obj.timeOffset = Duration.toJSON(message.timeOffset);
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    return obj;
  },

  create(base?: DeepPartial<LabelFrame>): LabelFrame {
    return LabelFrame.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LabelFrame>): LabelFrame {
    const message = createBaseLabelFrame();
    message.timeOffset = (object.timeOffset !== undefined && object.timeOffset !== null)
      ? Duration.fromPartial(object.timeOffset)
      : undefined;
    message.confidence = object.confidence ?? 0;
    return message;
  },
};

function createBaseEntity(): Entity {
  return { entityId: "", description: "", languageCode: "" };
}

export const Entity: MessageFns<Entity> = {
  encode(message: Entity, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.entityId !== "") {
      writer.uint32(10).string(message.entityId);
    }
    if (message.description !== "") {
      writer.uint32(18).string(message.description);
    }
    if (message.languageCode !== "") {
      writer.uint32(26).string(message.languageCode);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Entity {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEntity();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.entityId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.description = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.languageCode = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Entity {
    return {
      entityId: isSet(object.entityId) ? globalThis.String(object.entityId) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
    };
  },

  toJSON(message: Entity): unknown {
    const obj: any = {};
    if (message.entityId !== "") {
      obj.entityId = message.entityId;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    return obj;
  },

  create(base?: DeepPartial<Entity>): Entity {
    return Entity.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Entity>): Entity {
    const message = createBaseEntity();
    message.entityId = object.entityId ?? "";
    message.description = object.description ?? "";
    message.languageCode = object.languageCode ?? "";
    return message;
  },
};

function createBaseLabelAnnotation(): LabelAnnotation {
  return { entity: undefined, categoryEntities: [], segments: [], frames: [] };
}

export const LabelAnnotation: MessageFns<LabelAnnotation> = {
  encode(message: LabelAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.entity !== undefined) {
      Entity.encode(message.entity, writer.uint32(10).fork()).join();
    }
    for (const v of message.categoryEntities) {
      Entity.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.segments) {
      LabelSegment.encode(v!, writer.uint32(26).fork()).join();
    }
    for (const v of message.frames) {
      LabelFrame.encode(v!, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LabelAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLabelAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.entity = Entity.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.categoryEntities.push(Entity.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.segments.push(LabelSegment.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.frames.push(LabelFrame.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LabelAnnotation {
    return {
      entity: isSet(object.entity) ? Entity.fromJSON(object.entity) : undefined,
      categoryEntities: globalThis.Array.isArray(object?.categoryEntities)
        ? object.categoryEntities.map((e: any) => Entity.fromJSON(e))
        : [],
      segments: globalThis.Array.isArray(object?.segments)
        ? object.segments.map((e: any) => LabelSegment.fromJSON(e))
        : [],
      frames: globalThis.Array.isArray(object?.frames) ? object.frames.map((e: any) => LabelFrame.fromJSON(e)) : [],
    };
  },

  toJSON(message: LabelAnnotation): unknown {
    const obj: any = {};
    if (message.entity !== undefined) {
      obj.entity = Entity.toJSON(message.entity);
    }
    if (message.categoryEntities?.length) {
      obj.categoryEntities = message.categoryEntities.map((e) => Entity.toJSON(e));
    }
    if (message.segments?.length) {
      obj.segments = message.segments.map((e) => LabelSegment.toJSON(e));
    }
    if (message.frames?.length) {
      obj.frames = message.frames.map((e) => LabelFrame.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<LabelAnnotation>): LabelAnnotation {
    return LabelAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LabelAnnotation>): LabelAnnotation {
    const message = createBaseLabelAnnotation();
    message.entity = (object.entity !== undefined && object.entity !== null)
      ? Entity.fromPartial(object.entity)
      : undefined;
    message.categoryEntities = object.categoryEntities?.map((e) => Entity.fromPartial(e)) || [];
    message.segments = object.segments?.map((e) => LabelSegment.fromPartial(e)) || [];
    message.frames = object.frames?.map((e) => LabelFrame.fromPartial(e)) || [];
    return message;
  },
};

function createBaseExplicitContentFrame(): ExplicitContentFrame {
  return { timeOffset: undefined, pornographyLikelihood: 0 };
}

export const ExplicitContentFrame: MessageFns<ExplicitContentFrame> = {
  encode(message: ExplicitContentFrame, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.timeOffset !== undefined) {
      Duration.encode(message.timeOffset, writer.uint32(10).fork()).join();
    }
    if (message.pornographyLikelihood !== 0) {
      writer.uint32(16).int32(message.pornographyLikelihood);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplicitContentFrame {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplicitContentFrame();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.timeOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pornographyLikelihood = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplicitContentFrame {
    return {
      timeOffset: isSet(object.timeOffset) ? Duration.fromJSON(object.timeOffset) : undefined,
      pornographyLikelihood: isSet(object.pornographyLikelihood) ? likelihoodFromJSON(object.pornographyLikelihood) : 0,
    };
  },

  toJSON(message: ExplicitContentFrame): unknown {
    const obj: any = {};
    if (message.timeOffset !== undefined) {
      obj.timeOffset = Duration.toJSON(message.timeOffset);
    }
    if (message.pornographyLikelihood !== 0) {
      obj.pornographyLikelihood = likelihoodToJSON(message.pornographyLikelihood);
    }
    return obj;
  },

  create(base?: DeepPartial<ExplicitContentFrame>): ExplicitContentFrame {
    return ExplicitContentFrame.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplicitContentFrame>): ExplicitContentFrame {
    const message = createBaseExplicitContentFrame();
    message.timeOffset = (object.timeOffset !== undefined && object.timeOffset !== null)
      ? Duration.fromPartial(object.timeOffset)
      : undefined;
    message.pornographyLikelihood = object.pornographyLikelihood ?? 0;
    return message;
  },
};

function createBaseExplicitContentAnnotation(): ExplicitContentAnnotation {
  return { frames: [] };
}

export const ExplicitContentAnnotation: MessageFns<ExplicitContentAnnotation> = {
  encode(message: ExplicitContentAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.frames) {
      ExplicitContentFrame.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplicitContentAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplicitContentAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.frames.push(ExplicitContentFrame.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplicitContentAnnotation {
    return {
      frames: globalThis.Array.isArray(object?.frames)
        ? object.frames.map((e: any) => ExplicitContentFrame.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ExplicitContentAnnotation): unknown {
    const obj: any = {};
    if (message.frames?.length) {
      obj.frames = message.frames.map((e) => ExplicitContentFrame.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ExplicitContentAnnotation>): ExplicitContentAnnotation {
    return ExplicitContentAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplicitContentAnnotation>): ExplicitContentAnnotation {
    const message = createBaseExplicitContentAnnotation();
    message.frames = object.frames?.map((e) => ExplicitContentFrame.fromPartial(e)) || [];
    return message;
  },
};

function createBaseNormalizedBoundingBox(): NormalizedBoundingBox {
  return { left: 0, top: 0, right: 0, bottom: 0 };
}

export const NormalizedBoundingBox: MessageFns<NormalizedBoundingBox> = {
  encode(message: NormalizedBoundingBox, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.left !== 0) {
      writer.uint32(13).float(message.left);
    }
    if (message.top !== 0) {
      writer.uint32(21).float(message.top);
    }
    if (message.right !== 0) {
      writer.uint32(29).float(message.right);
    }
    if (message.bottom !== 0) {
      writer.uint32(37).float(message.bottom);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NormalizedBoundingBox {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNormalizedBoundingBox();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 13) {
            break;
          }

          message.left = reader.float();
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.top = reader.float();
          continue;
        case 3:
          if (tag !== 29) {
            break;
          }

          message.right = reader.float();
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.bottom = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NormalizedBoundingBox {
    return {
      left: isSet(object.left) ? globalThis.Number(object.left) : 0,
      top: isSet(object.top) ? globalThis.Number(object.top) : 0,
      right: isSet(object.right) ? globalThis.Number(object.right) : 0,
      bottom: isSet(object.bottom) ? globalThis.Number(object.bottom) : 0,
    };
  },

  toJSON(message: NormalizedBoundingBox): unknown {
    const obj: any = {};
    if (message.left !== 0) {
      obj.left = message.left;
    }
    if (message.top !== 0) {
      obj.top = message.top;
    }
    if (message.right !== 0) {
      obj.right = message.right;
    }
    if (message.bottom !== 0) {
      obj.bottom = message.bottom;
    }
    return obj;
  },

  create(base?: DeepPartial<NormalizedBoundingBox>): NormalizedBoundingBox {
    return NormalizedBoundingBox.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<NormalizedBoundingBox>): NormalizedBoundingBox {
    const message = createBaseNormalizedBoundingBox();
    message.left = object.left ?? 0;
    message.top = object.top ?? 0;
    message.right = object.right ?? 0;
    message.bottom = object.bottom ?? 0;
    return message;
  },
};

function createBaseFaceSegment(): FaceSegment {
  return { segment: undefined };
}

export const FaceSegment: MessageFns<FaceSegment> = {
  encode(message: FaceSegment, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.segment !== undefined) {
      VideoSegment.encode(message.segment, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FaceSegment {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFaceSegment();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.segment = VideoSegment.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FaceSegment {
    return { segment: isSet(object.segment) ? VideoSegment.fromJSON(object.segment) : undefined };
  },

  toJSON(message: FaceSegment): unknown {
    const obj: any = {};
    if (message.segment !== undefined) {
      obj.segment = VideoSegment.toJSON(message.segment);
    }
    return obj;
  },

  create(base?: DeepPartial<FaceSegment>): FaceSegment {
    return FaceSegment.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FaceSegment>): FaceSegment {
    const message = createBaseFaceSegment();
    message.segment = (object.segment !== undefined && object.segment !== null)
      ? VideoSegment.fromPartial(object.segment)
      : undefined;
    return message;
  },
};

function createBaseFaceFrame(): FaceFrame {
  return { normalizedBoundingBoxes: [], timeOffset: undefined };
}

export const FaceFrame: MessageFns<FaceFrame> = {
  encode(message: FaceFrame, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.normalizedBoundingBoxes) {
      NormalizedBoundingBox.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.timeOffset !== undefined) {
      Duration.encode(message.timeOffset, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FaceFrame {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFaceFrame();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.normalizedBoundingBoxes.push(NormalizedBoundingBox.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.timeOffset = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FaceFrame {
    return {
      normalizedBoundingBoxes: globalThis.Array.isArray(object?.normalizedBoundingBoxes)
        ? object.normalizedBoundingBoxes.map((e: any) => NormalizedBoundingBox.fromJSON(e))
        : [],
      timeOffset: isSet(object.timeOffset) ? Duration.fromJSON(object.timeOffset) : undefined,
    };
  },

  toJSON(message: FaceFrame): unknown {
    const obj: any = {};
    if (message.normalizedBoundingBoxes?.length) {
      obj.normalizedBoundingBoxes = message.normalizedBoundingBoxes.map((e) => NormalizedBoundingBox.toJSON(e));
    }
    if (message.timeOffset !== undefined) {
      obj.timeOffset = Duration.toJSON(message.timeOffset);
    }
    return obj;
  },

  create(base?: DeepPartial<FaceFrame>): FaceFrame {
    return FaceFrame.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FaceFrame>): FaceFrame {
    const message = createBaseFaceFrame();
    message.normalizedBoundingBoxes =
      object.normalizedBoundingBoxes?.map((e) => NormalizedBoundingBox.fromPartial(e)) || [];
    message.timeOffset = (object.timeOffset !== undefined && object.timeOffset !== null)
      ? Duration.fromPartial(object.timeOffset)
      : undefined;
    return message;
  },
};

function createBaseFaceAnnotation(): FaceAnnotation {
  return { thumbnail: Buffer.alloc(0), segments: [], frames: [] };
}

export const FaceAnnotation: MessageFns<FaceAnnotation> = {
  encode(message: FaceAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.thumbnail.length !== 0) {
      writer.uint32(10).bytes(message.thumbnail);
    }
    for (const v of message.segments) {
      FaceSegment.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.frames) {
      FaceFrame.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FaceAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFaceAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.thumbnail = Buffer.from(reader.bytes());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.segments.push(FaceSegment.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.frames.push(FaceFrame.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FaceAnnotation {
    return {
      thumbnail: isSet(object.thumbnail) ? Buffer.from(bytesFromBase64(object.thumbnail)) : Buffer.alloc(0),
      segments: globalThis.Array.isArray(object?.segments)
        ? object.segments.map((e: any) => FaceSegment.fromJSON(e))
        : [],
      frames: globalThis.Array.isArray(object?.frames) ? object.frames.map((e: any) => FaceFrame.fromJSON(e)) : [],
    };
  },

  toJSON(message: FaceAnnotation): unknown {
    const obj: any = {};
    if (message.thumbnail.length !== 0) {
      obj.thumbnail = base64FromBytes(message.thumbnail);
    }
    if (message.segments?.length) {
      obj.segments = message.segments.map((e) => FaceSegment.toJSON(e));
    }
    if (message.frames?.length) {
      obj.frames = message.frames.map((e) => FaceFrame.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<FaceAnnotation>): FaceAnnotation {
    return FaceAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FaceAnnotation>): FaceAnnotation {
    const message = createBaseFaceAnnotation();
    message.thumbnail = object.thumbnail ?? Buffer.alloc(0);
    message.segments = object.segments?.map((e) => FaceSegment.fromPartial(e)) || [];
    message.frames = object.frames?.map((e) => FaceFrame.fromPartial(e)) || [];
    return message;
  },
};

function createBaseVideoAnnotationResults(): VideoAnnotationResults {
  return {
    inputUri: "",
    segmentLabelAnnotations: [],
    shotLabelAnnotations: [],
    frameLabelAnnotations: [],
    faceAnnotations: [],
    shotAnnotations: [],
    explicitAnnotation: undefined,
    error: undefined,
  };
}

export const VideoAnnotationResults: MessageFns<VideoAnnotationResults> = {
  encode(message: VideoAnnotationResults, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputUri !== "") {
      writer.uint32(10).string(message.inputUri);
    }
    for (const v of message.segmentLabelAnnotations) {
      LabelAnnotation.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.shotLabelAnnotations) {
      LabelAnnotation.encode(v!, writer.uint32(26).fork()).join();
    }
    for (const v of message.frameLabelAnnotations) {
      LabelAnnotation.encode(v!, writer.uint32(34).fork()).join();
    }
    for (const v of message.faceAnnotations) {
      FaceAnnotation.encode(v!, writer.uint32(42).fork()).join();
    }
    for (const v of message.shotAnnotations) {
      VideoSegment.encode(v!, writer.uint32(50).fork()).join();
    }
    if (message.explicitAnnotation !== undefined) {
      ExplicitContentAnnotation.encode(message.explicitAnnotation, writer.uint32(58).fork()).join();
    }
    if (message.error !== undefined) {
      Status.encode(message.error, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VideoAnnotationResults {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVideoAnnotationResults();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.segmentLabelAnnotations.push(LabelAnnotation.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.shotLabelAnnotations.push(LabelAnnotation.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.frameLabelAnnotations.push(LabelAnnotation.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.faceAnnotations.push(FaceAnnotation.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.shotAnnotations.push(VideoSegment.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.explicitAnnotation = ExplicitContentAnnotation.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.error = Status.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VideoAnnotationResults {
    return {
      inputUri: isSet(object.inputUri) ? globalThis.String(object.inputUri) : "",
      segmentLabelAnnotations: globalThis.Array.isArray(object?.segmentLabelAnnotations)
        ? object.segmentLabelAnnotations.map((e: any) => LabelAnnotation.fromJSON(e))
        : [],
      shotLabelAnnotations: globalThis.Array.isArray(object?.shotLabelAnnotations)
        ? object.shotLabelAnnotations.map((e: any) => LabelAnnotation.fromJSON(e))
        : [],
      frameLabelAnnotations: globalThis.Array.isArray(object?.frameLabelAnnotations)
        ? object.frameLabelAnnotations.map((e: any) => LabelAnnotation.fromJSON(e))
        : [],
      faceAnnotations: globalThis.Array.isArray(object?.faceAnnotations)
        ? object.faceAnnotations.map((e: any) => FaceAnnotation.fromJSON(e))
        : [],
      shotAnnotations: globalThis.Array.isArray(object?.shotAnnotations)
        ? object.shotAnnotations.map((e: any) => VideoSegment.fromJSON(e))
        : [],
      explicitAnnotation: isSet(object.explicitAnnotation)
        ? ExplicitContentAnnotation.fromJSON(object.explicitAnnotation)
        : undefined,
      error: isSet(object.error) ? Status.fromJSON(object.error) : undefined,
    };
  },

  toJSON(message: VideoAnnotationResults): unknown {
    const obj: any = {};
    if (message.inputUri !== "") {
      obj.inputUri = message.inputUri;
    }
    if (message.segmentLabelAnnotations?.length) {
      obj.segmentLabelAnnotations = message.segmentLabelAnnotations.map((e) => LabelAnnotation.toJSON(e));
    }
    if (message.shotLabelAnnotations?.length) {
      obj.shotLabelAnnotations = message.shotLabelAnnotations.map((e) => LabelAnnotation.toJSON(e));
    }
    if (message.frameLabelAnnotations?.length) {
      obj.frameLabelAnnotations = message.frameLabelAnnotations.map((e) => LabelAnnotation.toJSON(e));
    }
    if (message.faceAnnotations?.length) {
      obj.faceAnnotations = message.faceAnnotations.map((e) => FaceAnnotation.toJSON(e));
    }
    if (message.shotAnnotations?.length) {
      obj.shotAnnotations = message.shotAnnotations.map((e) => VideoSegment.toJSON(e));
    }
    if (message.explicitAnnotation !== undefined) {
      obj.explicitAnnotation = ExplicitContentAnnotation.toJSON(message.explicitAnnotation);
    }
    if (message.error !== undefined) {
      obj.error = Status.toJSON(message.error);
    }
    return obj;
  },

  create(base?: DeepPartial<VideoAnnotationResults>): VideoAnnotationResults {
    return VideoAnnotationResults.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VideoAnnotationResults>): VideoAnnotationResults {
    const message = createBaseVideoAnnotationResults();
    message.inputUri = object.inputUri ?? "";
    message.segmentLabelAnnotations = object.segmentLabelAnnotations?.map((e) => LabelAnnotation.fromPartial(e)) || [];
    message.shotLabelAnnotations = object.shotLabelAnnotations?.map((e) => LabelAnnotation.fromPartial(e)) || [];
    message.frameLabelAnnotations = object.frameLabelAnnotations?.map((e) => LabelAnnotation.fromPartial(e)) || [];
    message.faceAnnotations = object.faceAnnotations?.map((e) => FaceAnnotation.fromPartial(e)) || [];
    message.shotAnnotations = object.shotAnnotations?.map((e) => VideoSegment.fromPartial(e)) || [];
    message.explicitAnnotation = (object.explicitAnnotation !== undefined && object.explicitAnnotation !== null)
      ? ExplicitContentAnnotation.fromPartial(object.explicitAnnotation)
      : undefined;
    message.error = (object.error !== undefined && object.error !== null)
      ? Status.fromPartial(object.error)
      : undefined;
    return message;
  },
};

function createBaseAnnotateVideoResponse(): AnnotateVideoResponse {
  return { annotationResults: [] };
}

export const AnnotateVideoResponse: MessageFns<AnnotateVideoResponse> = {
  encode(message: AnnotateVideoResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.annotationResults) {
      VideoAnnotationResults.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnnotateVideoResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnnotateVideoResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.annotationResults.push(VideoAnnotationResults.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnnotateVideoResponse {
    return {
      annotationResults: globalThis.Array.isArray(object?.annotationResults)
        ? object.annotationResults.map((e: any) => VideoAnnotationResults.fromJSON(e))
        : [],
    };
  },

  toJSON(message: AnnotateVideoResponse): unknown {
    const obj: any = {};
    if (message.annotationResults?.length) {
      obj.annotationResults = message.annotationResults.map((e) => VideoAnnotationResults.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<AnnotateVideoResponse>): AnnotateVideoResponse {
    return AnnotateVideoResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AnnotateVideoResponse>): AnnotateVideoResponse {
    const message = createBaseAnnotateVideoResponse();
    message.annotationResults = object.annotationResults?.map((e) => VideoAnnotationResults.fromPartial(e)) || [];
    return message;
  },
};

function createBaseVideoAnnotationProgress(): VideoAnnotationProgress {
  return { inputUri: "", progressPercent: 0, startTime: undefined, updateTime: undefined };
}

export const VideoAnnotationProgress: MessageFns<VideoAnnotationProgress> = {
  encode(message: VideoAnnotationProgress, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputUri !== "") {
      writer.uint32(10).string(message.inputUri);
    }
    if (message.progressPercent !== 0) {
      writer.uint32(16).int32(message.progressPercent);
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(26).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VideoAnnotationProgress {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVideoAnnotationProgress();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputUri = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.progressPercent = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VideoAnnotationProgress {
    return {
      inputUri: isSet(object.inputUri) ? globalThis.String(object.inputUri) : "",
      progressPercent: isSet(object.progressPercent) ? globalThis.Number(object.progressPercent) : 0,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
    };
  },

  toJSON(message: VideoAnnotationProgress): unknown {
    const obj: any = {};
    if (message.inputUri !== "") {
      obj.inputUri = message.inputUri;
    }
    if (message.progressPercent !== 0) {
      obj.progressPercent = Math.round(message.progressPercent);
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<VideoAnnotationProgress>): VideoAnnotationProgress {
    return VideoAnnotationProgress.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VideoAnnotationProgress>): VideoAnnotationProgress {
    const message = createBaseVideoAnnotationProgress();
    message.inputUri = object.inputUri ?? "";
    message.progressPercent = object.progressPercent ?? 0;
    message.startTime = object.startTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    return message;
  },
};

function createBaseAnnotateVideoProgress(): AnnotateVideoProgress {
  return { annotationProgress: [] };
}

export const AnnotateVideoProgress: MessageFns<AnnotateVideoProgress> = {
  encode(message: AnnotateVideoProgress, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.annotationProgress) {
      VideoAnnotationProgress.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnnotateVideoProgress {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnnotateVideoProgress();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.annotationProgress.push(VideoAnnotationProgress.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnnotateVideoProgress {
    return {
      annotationProgress: globalThis.Array.isArray(object?.annotationProgress)
        ? object.annotationProgress.map((e: any) => VideoAnnotationProgress.fromJSON(e))
        : [],
    };
  },

  toJSON(message: AnnotateVideoProgress): unknown {
    const obj: any = {};
    if (message.annotationProgress?.length) {
      obj.annotationProgress = message.annotationProgress.map((e) => VideoAnnotationProgress.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<AnnotateVideoProgress>): AnnotateVideoProgress {
    return AnnotateVideoProgress.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AnnotateVideoProgress>): AnnotateVideoProgress {
    const message = createBaseAnnotateVideoProgress();
    message.annotationProgress = object.annotationProgress?.map((e) => VideoAnnotationProgress.fromPartial(e)) || [];
    return message;
  },
};

/** Service that implements Google Cloud Video Intelligence API. */
export type VideoIntelligenceServiceDefinition = typeof VideoIntelligenceServiceDefinition;
export const VideoIntelligenceServiceDefinition = {
  name: "VideoIntelligenceService",
  fullName: "google.cloud.videointelligence.v1beta2.VideoIntelligenceService",
  methods: {
    /**
     * Performs asynchronous video annotation. Progress and results can be
     * retrieved through the `google.longrunning.Operations` interface.
     * `Operation.metadata` contains `AnnotateVideoProgress` (progress).
     * `Operation.response` contains `AnnotateVideoResponse` (results).
     */
    annotateVideo: {
      name: "AnnotateVideo",
      requestType: AnnotateVideoRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              46,
              10,
              21,
              65,
              110,
              110,
              111,
              116,
              97,
              116,
              101,
              86,
              105,
              100,
              101,
              111,
              82,
              101,
              115,
              112,
              111,
              110,
              115,
              101,
              18,
              21,
              65,
              110,
              110,
              111,
              116,
              97,
              116,
              101,
              86,
              105,
              100,
              101,
              111,
              80,
              114,
              111,
              103,
              114,
              101,
              115,
              115,
            ]),
          ],
          8410: [
            Buffer.from([18, 105, 110, 112, 117, 116, 95, 117, 114, 105, 44, 102, 101, 97, 116, 117, 114, 101, 115]),
          ],
          578365826: [
            Buffer.from([
              29,
              58,
              1,
              42,
              34,
              24,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              50,
              47,
              118,
              105,
              100,
              101,
              111,
              115,
              58,
              97,
              110,
              110,
              111,
              116,
              97,
              116,
              101,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface VideoIntelligenceServiceImplementation<CallContextExt = {}> {
  /**
   * Performs asynchronous video annotation. Progress and results can be
   * retrieved through the `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `AnnotateVideoProgress` (progress).
   * `Operation.response` contains `AnnotateVideoResponse` (results).
   */
  annotateVideo(request: AnnotateVideoRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
}

export interface VideoIntelligenceServiceClient<CallOptionsExt = {}> {
  /**
   * Performs asynchronous video annotation. Progress and results can be
   * retrieved through the `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `AnnotateVideoProgress` (progress).
   * `Operation.response` contains `AnnotateVideoResponse` (results).
   */
  annotateVideo(request: DeepPartial<AnnotateVideoRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
}

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
