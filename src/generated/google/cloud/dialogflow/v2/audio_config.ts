// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/dialogflow/v2/audio_config.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Duration } from "../../../protobuf/duration.js";

export const protobufPackage = "google.cloud.dialogflow.v2";

/**
 * [DTMF](https://en.wikipedia.org/wiki/Dual-tone_multi-frequency_signaling)
 * digit in Telephony Gateway.
 */
export enum TelephonyDtmf {
  /** TELEPHONY_DTMF_UNSPECIFIED - Not specified. This value may be used to indicate an absent digit. */
  TELEPHONY_DTMF_UNSPECIFIED = 0,
  /** DTMF_ONE - Number: '1'. */
  DTMF_ONE = 1,
  /** DTMF_TWO - Number: '2'. */
  DTMF_TWO = 2,
  /** DTMF_THREE - Number: '3'. */
  DTMF_THREE = 3,
  /** DTMF_FOUR - Number: '4'. */
  DTMF_FOUR = 4,
  /** DTMF_FIVE - Number: '5'. */
  DTMF_FIVE = 5,
  /** DTMF_SIX - Number: '6'. */
  DTMF_SIX = 6,
  /** DTMF_SEVEN - Number: '7'. */
  DTMF_SEVEN = 7,
  /** DTMF_EIGHT - Number: '8'. */
  DTMF_EIGHT = 8,
  /** DTMF_NINE - Number: '9'. */
  DTMF_NINE = 9,
  /** DTMF_ZERO - Number: '0'. */
  DTMF_ZERO = 10,
  /** DTMF_A - Letter: 'A'. */
  DTMF_A = 11,
  /** DTMF_B - Letter: 'B'. */
  DTMF_B = 12,
  /** DTMF_C - Letter: 'C'. */
  DTMF_C = 13,
  /** DTMF_D - Letter: 'D'. */
  DTMF_D = 14,
  /** DTMF_STAR - Asterisk/star: '*'. */
  DTMF_STAR = 15,
  /** DTMF_POUND - Pound/diamond/hash/square/gate/octothorpe: '#'. */
  DTMF_POUND = 16,
  UNRECOGNIZED = -1,
}

export function telephonyDtmfFromJSON(object: any): TelephonyDtmf {
  switch (object) {
    case 0:
    case "TELEPHONY_DTMF_UNSPECIFIED":
      return TelephonyDtmf.TELEPHONY_DTMF_UNSPECIFIED;
    case 1:
    case "DTMF_ONE":
      return TelephonyDtmf.DTMF_ONE;
    case 2:
    case "DTMF_TWO":
      return TelephonyDtmf.DTMF_TWO;
    case 3:
    case "DTMF_THREE":
      return TelephonyDtmf.DTMF_THREE;
    case 4:
    case "DTMF_FOUR":
      return TelephonyDtmf.DTMF_FOUR;
    case 5:
    case "DTMF_FIVE":
      return TelephonyDtmf.DTMF_FIVE;
    case 6:
    case "DTMF_SIX":
      return TelephonyDtmf.DTMF_SIX;
    case 7:
    case "DTMF_SEVEN":
      return TelephonyDtmf.DTMF_SEVEN;
    case 8:
    case "DTMF_EIGHT":
      return TelephonyDtmf.DTMF_EIGHT;
    case 9:
    case "DTMF_NINE":
      return TelephonyDtmf.DTMF_NINE;
    case 10:
    case "DTMF_ZERO":
      return TelephonyDtmf.DTMF_ZERO;
    case 11:
    case "DTMF_A":
      return TelephonyDtmf.DTMF_A;
    case 12:
    case "DTMF_B":
      return TelephonyDtmf.DTMF_B;
    case 13:
    case "DTMF_C":
      return TelephonyDtmf.DTMF_C;
    case 14:
    case "DTMF_D":
      return TelephonyDtmf.DTMF_D;
    case 15:
    case "DTMF_STAR":
      return TelephonyDtmf.DTMF_STAR;
    case 16:
    case "DTMF_POUND":
      return TelephonyDtmf.DTMF_POUND;
    case -1:
    case "UNRECOGNIZED":
    default:
      return TelephonyDtmf.UNRECOGNIZED;
  }
}

export function telephonyDtmfToJSON(object: TelephonyDtmf): string {
  switch (object) {
    case TelephonyDtmf.TELEPHONY_DTMF_UNSPECIFIED:
      return "TELEPHONY_DTMF_UNSPECIFIED";
    case TelephonyDtmf.DTMF_ONE:
      return "DTMF_ONE";
    case TelephonyDtmf.DTMF_TWO:
      return "DTMF_TWO";
    case TelephonyDtmf.DTMF_THREE:
      return "DTMF_THREE";
    case TelephonyDtmf.DTMF_FOUR:
      return "DTMF_FOUR";
    case TelephonyDtmf.DTMF_FIVE:
      return "DTMF_FIVE";
    case TelephonyDtmf.DTMF_SIX:
      return "DTMF_SIX";
    case TelephonyDtmf.DTMF_SEVEN:
      return "DTMF_SEVEN";
    case TelephonyDtmf.DTMF_EIGHT:
      return "DTMF_EIGHT";
    case TelephonyDtmf.DTMF_NINE:
      return "DTMF_NINE";
    case TelephonyDtmf.DTMF_ZERO:
      return "DTMF_ZERO";
    case TelephonyDtmf.DTMF_A:
      return "DTMF_A";
    case TelephonyDtmf.DTMF_B:
      return "DTMF_B";
    case TelephonyDtmf.DTMF_C:
      return "DTMF_C";
    case TelephonyDtmf.DTMF_D:
      return "DTMF_D";
    case TelephonyDtmf.DTMF_STAR:
      return "DTMF_STAR";
    case TelephonyDtmf.DTMF_POUND:
      return "DTMF_POUND";
    case TelephonyDtmf.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Audio encoding of the audio content sent in the conversational query request.
 * Refer to the
 * [Cloud Speech API
 * documentation](https://cloud.google.com/speech-to-text/docs/basics) for more
 * details.
 */
export enum AudioEncoding {
  /** AUDIO_ENCODING_UNSPECIFIED - Not specified. */
  AUDIO_ENCODING_UNSPECIFIED = 0,
  /** AUDIO_ENCODING_LINEAR_16 - Uncompressed 16-bit signed little-endian samples (Linear PCM). */
  AUDIO_ENCODING_LINEAR_16 = 1,
  /**
   * AUDIO_ENCODING_FLAC - [`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
   * Codec) is the recommended encoding because it is lossless (therefore
   * recognition is not compromised) and requires only about half the
   * bandwidth of `LINEAR16`. `FLAC` stream encoding supports 16-bit and
   * 24-bit samples, however, not all fields in `STREAMINFO` are supported.
   */
  AUDIO_ENCODING_FLAC = 2,
  /** AUDIO_ENCODING_MULAW - 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law. */
  AUDIO_ENCODING_MULAW = 3,
  /** AUDIO_ENCODING_AMR - Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000. */
  AUDIO_ENCODING_AMR = 4,
  /** AUDIO_ENCODING_AMR_WB - Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000. */
  AUDIO_ENCODING_AMR_WB = 5,
  /**
   * AUDIO_ENCODING_OGG_OPUS - Opus encoded audio frames in Ogg container
   * ([OggOpus](https://wiki.xiph.org/OggOpus)).
   * `sample_rate_hertz` must be 16000.
   */
  AUDIO_ENCODING_OGG_OPUS = 6,
  /**
   * AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE - Although the use of lossy encodings is not recommended, if a very low
   * bitrate encoding is required, `OGG_OPUS` is highly preferred over
   * Speex encoding. The [Speex](https://speex.org/) encoding supported by
   * Dialogflow API has a header byte in each block, as in MIME type
   * `audio/x-speex-with-header-byte`.
   * It is a variant of the RTP Speex encoding defined in
   * [RFC 5574](https://tools.ietf.org/html/rfc5574).
   * The stream is a sequence of blocks, one block per RTP packet. Each block
   * starts with a byte containing the length of the block, in bytes, followed
   * by one or more frames of Speex data, padded to an integral number of
   * bytes (octets) as specified in RFC 5574. In other words, each RTP header
   * is replaced with a single byte containing the block length. Only Speex
   * wideband is supported. `sample_rate_hertz` must be 16000.
   */
  AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE = 7,
  /** AUDIO_ENCODING_ALAW - 8-bit samples that compand 13-bit audio samples using G.711 PCMU/a-law. */
  AUDIO_ENCODING_ALAW = 8,
  UNRECOGNIZED = -1,
}

export function audioEncodingFromJSON(object: any): AudioEncoding {
  switch (object) {
    case 0:
    case "AUDIO_ENCODING_UNSPECIFIED":
      return AudioEncoding.AUDIO_ENCODING_UNSPECIFIED;
    case 1:
    case "AUDIO_ENCODING_LINEAR_16":
      return AudioEncoding.AUDIO_ENCODING_LINEAR_16;
    case 2:
    case "AUDIO_ENCODING_FLAC":
      return AudioEncoding.AUDIO_ENCODING_FLAC;
    case 3:
    case "AUDIO_ENCODING_MULAW":
      return AudioEncoding.AUDIO_ENCODING_MULAW;
    case 4:
    case "AUDIO_ENCODING_AMR":
      return AudioEncoding.AUDIO_ENCODING_AMR;
    case 5:
    case "AUDIO_ENCODING_AMR_WB":
      return AudioEncoding.AUDIO_ENCODING_AMR_WB;
    case 6:
    case "AUDIO_ENCODING_OGG_OPUS":
      return AudioEncoding.AUDIO_ENCODING_OGG_OPUS;
    case 7:
    case "AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE":
      return AudioEncoding.AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE;
    case 8:
    case "AUDIO_ENCODING_ALAW":
      return AudioEncoding.AUDIO_ENCODING_ALAW;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AudioEncoding.UNRECOGNIZED;
  }
}

export function audioEncodingToJSON(object: AudioEncoding): string {
  switch (object) {
    case AudioEncoding.AUDIO_ENCODING_UNSPECIFIED:
      return "AUDIO_ENCODING_UNSPECIFIED";
    case AudioEncoding.AUDIO_ENCODING_LINEAR_16:
      return "AUDIO_ENCODING_LINEAR_16";
    case AudioEncoding.AUDIO_ENCODING_FLAC:
      return "AUDIO_ENCODING_FLAC";
    case AudioEncoding.AUDIO_ENCODING_MULAW:
      return "AUDIO_ENCODING_MULAW";
    case AudioEncoding.AUDIO_ENCODING_AMR:
      return "AUDIO_ENCODING_AMR";
    case AudioEncoding.AUDIO_ENCODING_AMR_WB:
      return "AUDIO_ENCODING_AMR_WB";
    case AudioEncoding.AUDIO_ENCODING_OGG_OPUS:
      return "AUDIO_ENCODING_OGG_OPUS";
    case AudioEncoding.AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE:
      return "AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE";
    case AudioEncoding.AUDIO_ENCODING_ALAW:
      return "AUDIO_ENCODING_ALAW";
    case AudioEncoding.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Variant of the specified [Speech
 * model][google.cloud.dialogflow.v2.InputAudioConfig.model] to use.
 *
 * See the [Cloud Speech
 * documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
 * for which models have different variants. For example, the "phone_call" model
 * has both a standard and an enhanced variant. When you use an enhanced model,
 * you will generally receive higher quality results than for a standard model.
 */
export enum SpeechModelVariant {
  /**
   * SPEECH_MODEL_VARIANT_UNSPECIFIED - No model variant specified. In this case Dialogflow defaults to
   * USE_BEST_AVAILABLE.
   */
  SPEECH_MODEL_VARIANT_UNSPECIFIED = 0,
  /**
   * USE_BEST_AVAILABLE - Use the best available variant of the [Speech
   * model][InputAudioConfig.model] that the caller is eligible for.
   *
   * Please see the [Dialogflow
   * docs](https://cloud.google.com/dialogflow/docs/data-logging) for
   * how to make your project eligible for enhanced models.
   */
  USE_BEST_AVAILABLE = 1,
  /**
   * USE_STANDARD - Use standard model variant even if an enhanced model is available.  See the
   * [Cloud Speech
   * documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
   * for details about enhanced models.
   */
  USE_STANDARD = 2,
  /**
   * USE_ENHANCED - Use an enhanced model variant:
   *
   * * If an enhanced variant does not exist for the given
   *   [model][google.cloud.dialogflow.v2.InputAudioConfig.model] and request
   *   language, Dialogflow falls back to the standard variant.
   *
   *   The [Cloud Speech
   *   documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
   *   describes which models have enhanced variants.
   *
   * * If the API caller isn't eligible for enhanced models, Dialogflow returns
   *   an error. Please see the [Dialogflow
   *   docs](https://cloud.google.com/dialogflow/docs/data-logging)
   *   for how to make your project eligible.
   */
  USE_ENHANCED = 3,
  UNRECOGNIZED = -1,
}

export function speechModelVariantFromJSON(object: any): SpeechModelVariant {
  switch (object) {
    case 0:
    case "SPEECH_MODEL_VARIANT_UNSPECIFIED":
      return SpeechModelVariant.SPEECH_MODEL_VARIANT_UNSPECIFIED;
    case 1:
    case "USE_BEST_AVAILABLE":
      return SpeechModelVariant.USE_BEST_AVAILABLE;
    case 2:
    case "USE_STANDARD":
      return SpeechModelVariant.USE_STANDARD;
    case 3:
    case "USE_ENHANCED":
      return SpeechModelVariant.USE_ENHANCED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SpeechModelVariant.UNRECOGNIZED;
  }
}

export function speechModelVariantToJSON(object: SpeechModelVariant): string {
  switch (object) {
    case SpeechModelVariant.SPEECH_MODEL_VARIANT_UNSPECIFIED:
      return "SPEECH_MODEL_VARIANT_UNSPECIFIED";
    case SpeechModelVariant.USE_BEST_AVAILABLE:
      return "USE_BEST_AVAILABLE";
    case SpeechModelVariant.USE_STANDARD:
      return "USE_STANDARD";
    case SpeechModelVariant.USE_ENHANCED:
      return "USE_ENHANCED";
    case SpeechModelVariant.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Gender of the voice as described in
 * [SSML voice element](https://www.w3.org/TR/speech-synthesis11/#edef_voice).
 */
export enum SsmlVoiceGender {
  /**
   * SSML_VOICE_GENDER_UNSPECIFIED - An unspecified gender, which means that the client doesn't care which
   * gender the selected voice will have.
   */
  SSML_VOICE_GENDER_UNSPECIFIED = 0,
  /** SSML_VOICE_GENDER_MALE - A male voice. */
  SSML_VOICE_GENDER_MALE = 1,
  /** SSML_VOICE_GENDER_FEMALE - A female voice. */
  SSML_VOICE_GENDER_FEMALE = 2,
  /** SSML_VOICE_GENDER_NEUTRAL - A gender-neutral voice. */
  SSML_VOICE_GENDER_NEUTRAL = 3,
  UNRECOGNIZED = -1,
}

export function ssmlVoiceGenderFromJSON(object: any): SsmlVoiceGender {
  switch (object) {
    case 0:
    case "SSML_VOICE_GENDER_UNSPECIFIED":
      return SsmlVoiceGender.SSML_VOICE_GENDER_UNSPECIFIED;
    case 1:
    case "SSML_VOICE_GENDER_MALE":
      return SsmlVoiceGender.SSML_VOICE_GENDER_MALE;
    case 2:
    case "SSML_VOICE_GENDER_FEMALE":
      return SsmlVoiceGender.SSML_VOICE_GENDER_FEMALE;
    case 3:
    case "SSML_VOICE_GENDER_NEUTRAL":
      return SsmlVoiceGender.SSML_VOICE_GENDER_NEUTRAL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SsmlVoiceGender.UNRECOGNIZED;
  }
}

export function ssmlVoiceGenderToJSON(object: SsmlVoiceGender): string {
  switch (object) {
    case SsmlVoiceGender.SSML_VOICE_GENDER_UNSPECIFIED:
      return "SSML_VOICE_GENDER_UNSPECIFIED";
    case SsmlVoiceGender.SSML_VOICE_GENDER_MALE:
      return "SSML_VOICE_GENDER_MALE";
    case SsmlVoiceGender.SSML_VOICE_GENDER_FEMALE:
      return "SSML_VOICE_GENDER_FEMALE";
    case SsmlVoiceGender.SSML_VOICE_GENDER_NEUTRAL:
      return "SSML_VOICE_GENDER_NEUTRAL";
    case SsmlVoiceGender.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Audio encoding of the output audio format in Text-To-Speech. */
export enum OutputAudioEncoding {
  /** OUTPUT_AUDIO_ENCODING_UNSPECIFIED - Not specified. */
  OUTPUT_AUDIO_ENCODING_UNSPECIFIED = 0,
  /**
   * OUTPUT_AUDIO_ENCODING_LINEAR_16 - Uncompressed 16-bit signed little-endian samples (Linear PCM).
   * Audio content returned as LINEAR16 also contains a WAV header.
   */
  OUTPUT_AUDIO_ENCODING_LINEAR_16 = 1,
  /** OUTPUT_AUDIO_ENCODING_MP3 - MP3 audio at 32kbps. */
  OUTPUT_AUDIO_ENCODING_MP3 = 2,
  /** OUTPUT_AUDIO_ENCODING_MP3_64_KBPS - MP3 audio at 64kbps. */
  OUTPUT_AUDIO_ENCODING_MP3_64_KBPS = 4,
  /**
   * OUTPUT_AUDIO_ENCODING_OGG_OPUS - Opus encoded audio wrapped in an ogg container. The result will be a
   * file which can be played natively on Android, and in browsers (at least
   * Chrome and Firefox). The quality of the encoding is considerably higher
   * than MP3 while using approximately the same bitrate.
   */
  OUTPUT_AUDIO_ENCODING_OGG_OPUS = 3,
  /** OUTPUT_AUDIO_ENCODING_MULAW - 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law. */
  OUTPUT_AUDIO_ENCODING_MULAW = 5,
  /** OUTPUT_AUDIO_ENCODING_ALAW - 8-bit samples that compand 13-bit audio samples using G.711 PCMU/a-law. */
  OUTPUT_AUDIO_ENCODING_ALAW = 6,
  UNRECOGNIZED = -1,
}

export function outputAudioEncodingFromJSON(object: any): OutputAudioEncoding {
  switch (object) {
    case 0:
    case "OUTPUT_AUDIO_ENCODING_UNSPECIFIED":
      return OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_UNSPECIFIED;
    case 1:
    case "OUTPUT_AUDIO_ENCODING_LINEAR_16":
      return OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_LINEAR_16;
    case 2:
    case "OUTPUT_AUDIO_ENCODING_MP3":
      return OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_MP3;
    case 4:
    case "OUTPUT_AUDIO_ENCODING_MP3_64_KBPS":
      return OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_MP3_64_KBPS;
    case 3:
    case "OUTPUT_AUDIO_ENCODING_OGG_OPUS":
      return OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_OGG_OPUS;
    case 5:
    case "OUTPUT_AUDIO_ENCODING_MULAW":
      return OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_MULAW;
    case 6:
    case "OUTPUT_AUDIO_ENCODING_ALAW":
      return OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_ALAW;
    case -1:
    case "UNRECOGNIZED":
    default:
      return OutputAudioEncoding.UNRECOGNIZED;
  }
}

export function outputAudioEncodingToJSON(object: OutputAudioEncoding): string {
  switch (object) {
    case OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_UNSPECIFIED:
      return "OUTPUT_AUDIO_ENCODING_UNSPECIFIED";
    case OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_LINEAR_16:
      return "OUTPUT_AUDIO_ENCODING_LINEAR_16";
    case OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_MP3:
      return "OUTPUT_AUDIO_ENCODING_MP3";
    case OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_MP3_64_KBPS:
      return "OUTPUT_AUDIO_ENCODING_MP3_64_KBPS";
    case OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_OGG_OPUS:
      return "OUTPUT_AUDIO_ENCODING_OGG_OPUS";
    case OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_MULAW:
      return "OUTPUT_AUDIO_ENCODING_MULAW";
    case OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_ALAW:
      return "OUTPUT_AUDIO_ENCODING_ALAW";
    case OutputAudioEncoding.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Hints for the speech recognizer to help with recognition in a specific
 * conversation state.
 */
export interface SpeechContext {
  /**
   * Optional. A list of strings containing words and phrases that the speech
   * recognizer should recognize with higher likelihood.
   *
   * This list can be used to:
   *
   * * improve accuracy for words and phrases you expect the user to say,
   *   e.g. typical commands for your Dialogflow agent
   * * add additional words to the speech recognizer vocabulary
   * * ...
   *
   * See the [Cloud Speech
   * documentation](https://cloud.google.com/speech-to-text/quotas) for usage
   * limits.
   */
  phrases: string[];
  /**
   * Optional. Boost for this context compared to other contexts:
   *
   * * If the boost is positive, Dialogflow will increase the probability that
   *   the phrases in this context are recognized over similar sounding phrases.
   * * If the boost is unspecified or non-positive, Dialogflow will not apply
   *   any boost.
   *
   * Dialogflow recommends that you use boosts in the range (0, 20] and that you
   * find a value that fits your use case with binary search.
   */
  boost: number;
}

/** Information for a word recognized by the speech recognizer. */
export interface SpeechWordInfo {
  /** The word this info is for. */
  word: string;
  /**
   * Time offset relative to the beginning of the audio that corresponds to the
   * start of the spoken word. This is an experimental feature and the accuracy
   * of the time offset can vary.
   */
  startOffset:
    | Duration
    | undefined;
  /**
   * Time offset relative to the beginning of the audio that corresponds to the
   * end of the spoken word. This is an experimental feature and the accuracy of
   * the time offset can vary.
   */
  endOffset:
    | Duration
    | undefined;
  /**
   * The Speech confidence between 0.0 and 1.0 for this word. A higher number
   * indicates an estimated greater likelihood that the recognized word is
   * correct. The default of 0.0 is a sentinel value indicating that confidence
   * was not set.
   *
   * This field is not guaranteed to be fully stable over time for the same
   * audio input. Users should also not rely on it to always be provided.
   */
  confidence: number;
}

/** Instructs the speech recognizer how to process the audio content. */
export interface InputAudioConfig {
  /** Required. Audio encoding of the audio content to process. */
  audioEncoding: AudioEncoding;
  /**
   * Required. Sample rate (in Hertz) of the audio content sent in the query.
   * Refer to [Cloud Speech API
   * documentation](https://cloud.google.com/speech-to-text/docs/basics) for
   * more details.
   */
  sampleRateHertz: number;
  /**
   * Required. The language of the supplied audio. Dialogflow does not do
   * translations. See [Language
   * Support](https://cloud.google.com/dialogflow/docs/reference/language)
   * for a list of the currently supported language codes. Note that queries in
   * the same session do not necessarily need to specify the same language.
   */
  languageCode: string;
  /**
   * If `true`, Dialogflow returns
   * [SpeechWordInfo][google.cloud.dialogflow.v2.SpeechWordInfo] in
   * [StreamingRecognitionResult][google.cloud.dialogflow.v2.StreamingRecognitionResult]
   * with information about the recognized speech words, e.g. start and end time
   * offsets. If false or unspecified, Speech doesn't return any word-level
   * information.
   */
  enableWordInfo: boolean;
  /**
   * A list of strings containing words and phrases that the speech
   * recognizer should recognize with higher likelihood.
   *
   * See [the Cloud Speech
   * documentation](https://cloud.google.com/speech-to-text/docs/basics#phrase-hints)
   * for more details.
   *
   * This field is deprecated. Please use [`speech_contexts`]() instead. If you
   * specify both [`phrase_hints`]() and [`speech_contexts`](), Dialogflow will
   * treat the [`phrase_hints`]() as a single additional [`SpeechContext`]().
   *
   * @deprecated
   */
  phraseHints: string[];
  /**
   * Context information to assist speech recognition.
   *
   * See [the Cloud Speech
   * documentation](https://cloud.google.com/speech-to-text/docs/basics#phrase-hints)
   * for more details.
   */
  speechContexts: SpeechContext[];
  /**
   * Optional. Which Speech model to select for the given request.
   * For more information, see
   * [Speech models](https://cloud.google.com/dialogflow/es/docs/speech-models).
   */
  model: string;
  /**
   * Which variant of the [Speech
   * model][google.cloud.dialogflow.v2.InputAudioConfig.model] to use.
   */
  modelVariant: SpeechModelVariant;
  /**
   * If `false` (default), recognition does not cease until the
   * client closes the stream.
   * If `true`, the recognizer will detect a single spoken utterance in input
   * audio. Recognition ceases when it detects the audio's voice has
   * stopped or paused. In this case, once a detected intent is received, the
   * client should close the stream and start a new request with a new stream as
   * needed.
   * Note: This setting is relevant only for streaming methods.
   * Note: When specified, InputAudioConfig.single_utterance takes precedence
   * over StreamingDetectIntentRequest.single_utterance.
   */
  singleUtterance: boolean;
  /**
   * Only used in
   * [Participants.AnalyzeContent][google.cloud.dialogflow.v2.Participants.AnalyzeContent]
   * and
   * [Participants.StreamingAnalyzeContent][google.cloud.dialogflow.v2.Participants.StreamingAnalyzeContent].
   * If `false` and recognition doesn't return any result, trigger
   * `NO_SPEECH_RECOGNIZED` event to Dialogflow agent.
   */
  disableNoSpeechRecognizedEvent: boolean;
  /** Enable automatic punctuation option at the speech backend. */
  enableAutomaticPunctuation: boolean;
  /**
   * If `true`, the request will opt out for STT conformer model migration.
   * This field will be deprecated once force migration takes place in June
   * 2024. Please refer to [Dialogflow ES Speech model
   * migration](https://cloud.google.com/dialogflow/es/docs/speech-model-migration).
   */
  optOutConformerModelMigration: boolean;
}

/** Description of which voice to use for speech synthesis. */
export interface VoiceSelectionParams {
  /**
   * Optional. The name of the voice. If not set, the service will choose a
   * voice based on the other parameters such as language_code and
   * [ssml_gender][google.cloud.dialogflow.v2.VoiceSelectionParams.ssml_gender].
   */
  name: string;
  /**
   * Optional. The preferred gender of the voice. If not set, the service will
   * choose a voice based on the other parameters such as language_code and
   * [name][google.cloud.dialogflow.v2.VoiceSelectionParams.name]. Note that
   * this is only a preference, not requirement. If a voice of the appropriate
   * gender is not available, the synthesizer should substitute a voice with a
   * different gender rather than failing the request.
   */
  ssmlGender: SsmlVoiceGender;
}

/** Configuration of how speech should be synthesized. */
export interface SynthesizeSpeechConfig {
  /**
   * Optional. Speaking rate/speed, in the range [0.25, 4.0]. 1.0 is the normal
   * native speed supported by the specific voice. 2.0 is twice as fast, and 0.5
   * is half as fast. If unset(0.0), defaults to the native 1.0 speed. Any other
   * values < 0.25 or > 4.0 will return an error.
   */
  speakingRate: number;
  /**
   * Optional. Speaking pitch, in the range [-20.0, 20.0]. 20 means increase 20
   * semitones from the original pitch. -20 means decrease 20 semitones from the
   * original pitch.
   */
  pitch: number;
  /**
   * Optional. Volume gain (in dB) of the normal native volume supported by the
   * specific voice, in the range [-96.0, 16.0]. If unset, or set to a value of
   * 0.0 (dB), will play at normal native signal amplitude. A value of -6.0 (dB)
   * will play at approximately half the amplitude of the normal native signal
   * amplitude. A value of +6.0 (dB) will play at approximately twice the
   * amplitude of the normal native signal amplitude. We strongly recommend not
   * to exceed +10 (dB) as there's usually no effective increase in loudness for
   * any value greater than that.
   */
  volumeGainDb: number;
  /**
   * Optional. An identifier which selects 'audio effects' profiles that are
   * applied on (post synthesized) text to speech. Effects are applied on top of
   * each other in the order they are given.
   */
  effectsProfileId: string[];
  /** Optional. The desired voice of the synthesized audio. */
  voice: VoiceSelectionParams | undefined;
}

/**
 * Instructs the speech synthesizer on how to generate the output audio content.
 * If this audio config is supplied in a request, it overrides all existing
 * text-to-speech settings applied to the agent.
 */
export interface OutputAudioConfig {
  /** Required. Audio encoding of the synthesized audio content. */
  audioEncoding: OutputAudioEncoding;
  /**
   * The synthesis sample rate (in hertz) for this audio. If not
   * provided, then the synthesizer will use the default sample rate based on
   * the audio encoding. If this is different from the voice's natural sample
   * rate, then the synthesizer will honor this request by converting to the
   * desired sample rate (which might result in worse audio quality).
   */
  sampleRateHertz: number;
  /** Configuration of how speech should be synthesized. */
  synthesizeSpeechConfig: SynthesizeSpeechConfig | undefined;
}

/** A wrapper of repeated TelephonyDtmf digits. */
export interface TelephonyDtmfEvents {
  /** A sequence of TelephonyDtmf digits. */
  dtmfEvents: TelephonyDtmf[];
}

/**
 * Configures speech transcription for
 * [ConversationProfile][google.cloud.dialogflow.v2.ConversationProfile].
 */
export interface SpeechToTextConfig {
  /**
   * The speech model used in speech to text.
   * `SPEECH_MODEL_VARIANT_UNSPECIFIED`, `USE_BEST_AVAILABLE` will be treated as
   * `USE_ENHANCED`. It can be overridden in
   * [AnalyzeContentRequest][google.cloud.dialogflow.v2.AnalyzeContentRequest]
   * and
   * [StreamingAnalyzeContentRequest][google.cloud.dialogflow.v2.StreamingAnalyzeContentRequest]
   * request. If enhanced model variant is specified and an enhanced version of
   * the specified model for the language does not exist, then it would emit an
   * error.
   */
  speechModelVariant: SpeechModelVariant;
  /**
   * Which Speech model to select. Select the
   * model best suited to your domain to get best results. If a model is not
   * explicitly specified, then Dialogflow auto-selects a model based on other
   * parameters in the SpeechToTextConfig and Agent settings.
   * If enhanced speech model is enabled for the agent and an enhanced
   * version of the specified model for the language does not exist, then the
   * speech is recognized using the standard version of the specified model.
   * Refer to
   * [Cloud Speech API
   * documentation](https://cloud.google.com/speech-to-text/docs/basics#select-model)
   * for more details.
   * If you specify a model, the following models typically have the best
   * performance:
   *
   * - phone_call (best for Agent Assist and telephony)
   * - latest_short (best for Dialogflow non-telephony)
   * - command_and_search
   *
   * Leave this field unspecified to use
   * [Agent Speech
   * settings](https://cloud.google.com/dialogflow/cx/docs/concept/agent#settings-speech)
   * for model selection.
   */
  model: string;
  /** Audio encoding of the audio content to process. */
  audioEncoding: AudioEncoding;
  /**
   * Sample rate (in Hertz) of the audio content sent in the query.
   * Refer to [Cloud Speech API
   * documentation](https://cloud.google.com/speech-to-text/docs/basics) for
   * more details.
   */
  sampleRateHertz: number;
  /**
   * The language of the supplied audio. Dialogflow does not do
   * translations. See [Language
   * Support](https://cloud.google.com/dialogflow/docs/reference/language)
   * for a list of the currently supported language codes. Note that queries in
   * the same session do not necessarily need to specify the same language.
   */
  languageCode: string;
  /**
   * If `true`, Dialogflow returns
   * [SpeechWordInfo][google.cloud.dialogflow.v2.SpeechWordInfo] in
   * [StreamingRecognitionResult][google.cloud.dialogflow.v2.StreamingRecognitionResult]
   * with information about the recognized speech words, e.g. start and end time
   * offsets. If false or unspecified, Speech doesn't return any word-level
   * information.
   */
  enableWordInfo: boolean;
  /**
   * Use timeout based endpointing, interpreting endpointer sensitivy as
   * seconds of timeout value.
   */
  useTimeoutBasedEndpointing: boolean;
}

function createBaseSpeechContext(): SpeechContext {
  return { phrases: [], boost: 0 };
}

export const SpeechContext: MessageFns<SpeechContext> = {
  encode(message: SpeechContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.phrases) {
      writer.uint32(10).string(v!);
    }
    if (message.boost !== 0) {
      writer.uint32(21).float(message.boost);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.phrases.push(reader.string());
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.boost = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechContext {
    return {
      phrases: globalThis.Array.isArray(object?.phrases) ? object.phrases.map((e: any) => globalThis.String(e)) : [],
      boost: isSet(object.boost) ? globalThis.Number(object.boost) : 0,
    };
  },

  toJSON(message: SpeechContext): unknown {
    const obj: any = {};
    if (message.phrases?.length) {
      obj.phrases = message.phrases;
    }
    if (message.boost !== 0) {
      obj.boost = message.boost;
    }
    return obj;
  },

  create(base?: DeepPartial<SpeechContext>): SpeechContext {
    return SpeechContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SpeechContext>): SpeechContext {
    const message = createBaseSpeechContext();
    message.phrases = object.phrases?.map((e) => e) || [];
    message.boost = object.boost ?? 0;
    return message;
  },
};

function createBaseSpeechWordInfo(): SpeechWordInfo {
  return { word: "", startOffset: undefined, endOffset: undefined, confidence: 0 };
}

export const SpeechWordInfo: MessageFns<SpeechWordInfo> = {
  encode(message: SpeechWordInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.word !== "") {
      writer.uint32(26).string(message.word);
    }
    if (message.startOffset !== undefined) {
      Duration.encode(message.startOffset, writer.uint32(10).fork()).join();
    }
    if (message.endOffset !== undefined) {
      Duration.encode(message.endOffset, writer.uint32(18).fork()).join();
    }
    if (message.confidence !== 0) {
      writer.uint32(37).float(message.confidence);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechWordInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechWordInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 26) {
            break;
          }

          message.word = reader.string();
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.endOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.confidence = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechWordInfo {
    return {
      word: isSet(object.word) ? globalThis.String(object.word) : "",
      startOffset: isSet(object.startOffset) ? Duration.fromJSON(object.startOffset) : undefined,
      endOffset: isSet(object.endOffset) ? Duration.fromJSON(object.endOffset) : undefined,
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
    };
  },

  toJSON(message: SpeechWordInfo): unknown {
    const obj: any = {};
    if (message.word !== "") {
      obj.word = message.word;
    }
    if (message.startOffset !== undefined) {
      obj.startOffset = Duration.toJSON(message.startOffset);
    }
    if (message.endOffset !== undefined) {
      obj.endOffset = Duration.toJSON(message.endOffset);
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    return obj;
  },

  create(base?: DeepPartial<SpeechWordInfo>): SpeechWordInfo {
    return SpeechWordInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SpeechWordInfo>): SpeechWordInfo {
    const message = createBaseSpeechWordInfo();
    message.word = object.word ?? "";
    message.startOffset = (object.startOffset !== undefined && object.startOffset !== null)
      ? Duration.fromPartial(object.startOffset)
      : undefined;
    message.endOffset = (object.endOffset !== undefined && object.endOffset !== null)
      ? Duration.fromPartial(object.endOffset)
      : undefined;
    message.confidence = object.confidence ?? 0;
    return message;
  },
};

function createBaseInputAudioConfig(): InputAudioConfig {
  return {
    audioEncoding: 0,
    sampleRateHertz: 0,
    languageCode: "",
    enableWordInfo: false,
    phraseHints: [],
    speechContexts: [],
    model: "",
    modelVariant: 0,
    singleUtterance: false,
    disableNoSpeechRecognizedEvent: false,
    enableAutomaticPunctuation: false,
    optOutConformerModelMigration: false,
  };
}

export const InputAudioConfig: MessageFns<InputAudioConfig> = {
  encode(message: InputAudioConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.audioEncoding !== 0) {
      writer.uint32(8).int32(message.audioEncoding);
    }
    if (message.sampleRateHertz !== 0) {
      writer.uint32(16).int32(message.sampleRateHertz);
    }
    if (message.languageCode !== "") {
      writer.uint32(26).string(message.languageCode);
    }
    if (message.enableWordInfo !== false) {
      writer.uint32(104).bool(message.enableWordInfo);
    }
    for (const v of message.phraseHints) {
      writer.uint32(34).string(v!);
    }
    for (const v of message.speechContexts) {
      SpeechContext.encode(v!, writer.uint32(90).fork()).join();
    }
    if (message.model !== "") {
      writer.uint32(58).string(message.model);
    }
    if (message.modelVariant !== 0) {
      writer.uint32(80).int32(message.modelVariant);
    }
    if (message.singleUtterance !== false) {
      writer.uint32(64).bool(message.singleUtterance);
    }
    if (message.disableNoSpeechRecognizedEvent !== false) {
      writer.uint32(112).bool(message.disableNoSpeechRecognizedEvent);
    }
    if (message.enableAutomaticPunctuation !== false) {
      writer.uint32(136).bool(message.enableAutomaticPunctuation);
    }
    if (message.optOutConformerModelMigration !== false) {
      writer.uint32(208).bool(message.optOutConformerModelMigration);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InputAudioConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInputAudioConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.audioEncoding = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.sampleRateHertz = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.languageCode = reader.string();
          continue;
        case 13:
          if (tag !== 104) {
            break;
          }

          message.enableWordInfo = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.phraseHints.push(reader.string());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.speechContexts.push(SpeechContext.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.model = reader.string();
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.modelVariant = reader.int32() as any;
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.singleUtterance = reader.bool();
          continue;
        case 14:
          if (tag !== 112) {
            break;
          }

          message.disableNoSpeechRecognizedEvent = reader.bool();
          continue;
        case 17:
          if (tag !== 136) {
            break;
          }

          message.enableAutomaticPunctuation = reader.bool();
          continue;
        case 26:
          if (tag !== 208) {
            break;
          }

          message.optOutConformerModelMigration = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InputAudioConfig {
    return {
      audioEncoding: isSet(object.audioEncoding) ? audioEncodingFromJSON(object.audioEncoding) : 0,
      sampleRateHertz: isSet(object.sampleRateHertz) ? globalThis.Number(object.sampleRateHertz) : 0,
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
      enableWordInfo: isSet(object.enableWordInfo) ? globalThis.Boolean(object.enableWordInfo) : false,
      phraseHints: globalThis.Array.isArray(object?.phraseHints)
        ? object.phraseHints.map((e: any) => globalThis.String(e))
        : [],
      speechContexts: globalThis.Array.isArray(object?.speechContexts)
        ? object.speechContexts.map((e: any) => SpeechContext.fromJSON(e))
        : [],
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      modelVariant: isSet(object.modelVariant) ? speechModelVariantFromJSON(object.modelVariant) : 0,
      singleUtterance: isSet(object.singleUtterance) ? globalThis.Boolean(object.singleUtterance) : false,
      disableNoSpeechRecognizedEvent: isSet(object.disableNoSpeechRecognizedEvent)
        ? globalThis.Boolean(object.disableNoSpeechRecognizedEvent)
        : false,
      enableAutomaticPunctuation: isSet(object.enableAutomaticPunctuation)
        ? globalThis.Boolean(object.enableAutomaticPunctuation)
        : false,
      optOutConformerModelMigration: isSet(object.optOutConformerModelMigration)
        ? globalThis.Boolean(object.optOutConformerModelMigration)
        : false,
    };
  },

  toJSON(message: InputAudioConfig): unknown {
    const obj: any = {};
    if (message.audioEncoding !== 0) {
      obj.audioEncoding = audioEncodingToJSON(message.audioEncoding);
    }
    if (message.sampleRateHertz !== 0) {
      obj.sampleRateHertz = Math.round(message.sampleRateHertz);
    }
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    if (message.enableWordInfo !== false) {
      obj.enableWordInfo = message.enableWordInfo;
    }
    if (message.phraseHints?.length) {
      obj.phraseHints = message.phraseHints;
    }
    if (message.speechContexts?.length) {
      obj.speechContexts = message.speechContexts.map((e) => SpeechContext.toJSON(e));
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.modelVariant !== 0) {
      obj.modelVariant = speechModelVariantToJSON(message.modelVariant);
    }
    if (message.singleUtterance !== false) {
      obj.singleUtterance = message.singleUtterance;
    }
    if (message.disableNoSpeechRecognizedEvent !== false) {
      obj.disableNoSpeechRecognizedEvent = message.disableNoSpeechRecognizedEvent;
    }
    if (message.enableAutomaticPunctuation !== false) {
      obj.enableAutomaticPunctuation = message.enableAutomaticPunctuation;
    }
    if (message.optOutConformerModelMigration !== false) {
      obj.optOutConformerModelMigration = message.optOutConformerModelMigration;
    }
    return obj;
  },

  create(base?: DeepPartial<InputAudioConfig>): InputAudioConfig {
    return InputAudioConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InputAudioConfig>): InputAudioConfig {
    const message = createBaseInputAudioConfig();
    message.audioEncoding = object.audioEncoding ?? 0;
    message.sampleRateHertz = object.sampleRateHertz ?? 0;
    message.languageCode = object.languageCode ?? "";
    message.enableWordInfo = object.enableWordInfo ?? false;
    message.phraseHints = object.phraseHints?.map((e) => e) || [];
    message.speechContexts = object.speechContexts?.map((e) => SpeechContext.fromPartial(e)) || [];
    message.model = object.model ?? "";
    message.modelVariant = object.modelVariant ?? 0;
    message.singleUtterance = object.singleUtterance ?? false;
    message.disableNoSpeechRecognizedEvent = object.disableNoSpeechRecognizedEvent ?? false;
    message.enableAutomaticPunctuation = object.enableAutomaticPunctuation ?? false;
    message.optOutConformerModelMigration = object.optOutConformerModelMigration ?? false;
    return message;
  },
};

function createBaseVoiceSelectionParams(): VoiceSelectionParams {
  return { name: "", ssmlGender: 0 };
}

export const VoiceSelectionParams: MessageFns<VoiceSelectionParams> = {
  encode(message: VoiceSelectionParams, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.ssmlGender !== 0) {
      writer.uint32(16).int32(message.ssmlGender);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VoiceSelectionParams {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVoiceSelectionParams();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.ssmlGender = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VoiceSelectionParams {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      ssmlGender: isSet(object.ssmlGender) ? ssmlVoiceGenderFromJSON(object.ssmlGender) : 0,
    };
  },

  toJSON(message: VoiceSelectionParams): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.ssmlGender !== 0) {
      obj.ssmlGender = ssmlVoiceGenderToJSON(message.ssmlGender);
    }
    return obj;
  },

  create(base?: DeepPartial<VoiceSelectionParams>): VoiceSelectionParams {
    return VoiceSelectionParams.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VoiceSelectionParams>): VoiceSelectionParams {
    const message = createBaseVoiceSelectionParams();
    message.name = object.name ?? "";
    message.ssmlGender = object.ssmlGender ?? 0;
    return message;
  },
};

function createBaseSynthesizeSpeechConfig(): SynthesizeSpeechConfig {
  return { speakingRate: 0, pitch: 0, volumeGainDb: 0, effectsProfileId: [], voice: undefined };
}

export const SynthesizeSpeechConfig: MessageFns<SynthesizeSpeechConfig> = {
  encode(message: SynthesizeSpeechConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.speakingRate !== 0) {
      writer.uint32(9).double(message.speakingRate);
    }
    if (message.pitch !== 0) {
      writer.uint32(17).double(message.pitch);
    }
    if (message.volumeGainDb !== 0) {
      writer.uint32(25).double(message.volumeGainDb);
    }
    for (const v of message.effectsProfileId) {
      writer.uint32(42).string(v!);
    }
    if (message.voice !== undefined) {
      VoiceSelectionParams.encode(message.voice, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SynthesizeSpeechConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSynthesizeSpeechConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 9) {
            break;
          }

          message.speakingRate = reader.double();
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.pitch = reader.double();
          continue;
        case 3:
          if (tag !== 25) {
            break;
          }

          message.volumeGainDb = reader.double();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.effectsProfileId.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.voice = VoiceSelectionParams.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SynthesizeSpeechConfig {
    return {
      speakingRate: isSet(object.speakingRate) ? globalThis.Number(object.speakingRate) : 0,
      pitch: isSet(object.pitch) ? globalThis.Number(object.pitch) : 0,
      volumeGainDb: isSet(object.volumeGainDb) ? globalThis.Number(object.volumeGainDb) : 0,
      effectsProfileId: globalThis.Array.isArray(object?.effectsProfileId)
        ? object.effectsProfileId.map((e: any) => globalThis.String(e))
        : [],
      voice: isSet(object.voice) ? VoiceSelectionParams.fromJSON(object.voice) : undefined,
    };
  },

  toJSON(message: SynthesizeSpeechConfig): unknown {
    const obj: any = {};
    if (message.speakingRate !== 0) {
      obj.speakingRate = message.speakingRate;
    }
    if (message.pitch !== 0) {
      obj.pitch = message.pitch;
    }
    if (message.volumeGainDb !== 0) {
      obj.volumeGainDb = message.volumeGainDb;
    }
    if (message.effectsProfileId?.length) {
      obj.effectsProfileId = message.effectsProfileId;
    }
    if (message.voice !== undefined) {
      obj.voice = VoiceSelectionParams.toJSON(message.voice);
    }
    return obj;
  },

  create(base?: DeepPartial<SynthesizeSpeechConfig>): SynthesizeSpeechConfig {
    return SynthesizeSpeechConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SynthesizeSpeechConfig>): SynthesizeSpeechConfig {
    const message = createBaseSynthesizeSpeechConfig();
    message.speakingRate = object.speakingRate ?? 0;
    message.pitch = object.pitch ?? 0;
    message.volumeGainDb = object.volumeGainDb ?? 0;
    message.effectsProfileId = object.effectsProfileId?.map((e) => e) || [];
    message.voice = (object.voice !== undefined && object.voice !== null)
      ? VoiceSelectionParams.fromPartial(object.voice)
      : undefined;
    return message;
  },
};

function createBaseOutputAudioConfig(): OutputAudioConfig {
  return { audioEncoding: 0, sampleRateHertz: 0, synthesizeSpeechConfig: undefined };
}

export const OutputAudioConfig: MessageFns<OutputAudioConfig> = {
  encode(message: OutputAudioConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.audioEncoding !== 0) {
      writer.uint32(8).int32(message.audioEncoding);
    }
    if (message.sampleRateHertz !== 0) {
      writer.uint32(16).int32(message.sampleRateHertz);
    }
    if (message.synthesizeSpeechConfig !== undefined) {
      SynthesizeSpeechConfig.encode(message.synthesizeSpeechConfig, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OutputAudioConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOutputAudioConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.audioEncoding = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.sampleRateHertz = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.synthesizeSpeechConfig = SynthesizeSpeechConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OutputAudioConfig {
    return {
      audioEncoding: isSet(object.audioEncoding) ? outputAudioEncodingFromJSON(object.audioEncoding) : 0,
      sampleRateHertz: isSet(object.sampleRateHertz) ? globalThis.Number(object.sampleRateHertz) : 0,
      synthesizeSpeechConfig: isSet(object.synthesizeSpeechConfig)
        ? SynthesizeSpeechConfig.fromJSON(object.synthesizeSpeechConfig)
        : undefined,
    };
  },

  toJSON(message: OutputAudioConfig): unknown {
    const obj: any = {};
    if (message.audioEncoding !== 0) {
      obj.audioEncoding = outputAudioEncodingToJSON(message.audioEncoding);
    }
    if (message.sampleRateHertz !== 0) {
      obj.sampleRateHertz = Math.round(message.sampleRateHertz);
    }
    if (message.synthesizeSpeechConfig !== undefined) {
      obj.synthesizeSpeechConfig = SynthesizeSpeechConfig.toJSON(message.synthesizeSpeechConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<OutputAudioConfig>): OutputAudioConfig {
    return OutputAudioConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OutputAudioConfig>): OutputAudioConfig {
    const message = createBaseOutputAudioConfig();
    message.audioEncoding = object.audioEncoding ?? 0;
    message.sampleRateHertz = object.sampleRateHertz ?? 0;
    message.synthesizeSpeechConfig =
      (object.synthesizeSpeechConfig !== undefined && object.synthesizeSpeechConfig !== null)
        ? SynthesizeSpeechConfig.fromPartial(object.synthesizeSpeechConfig)
        : undefined;
    return message;
  },
};

function createBaseTelephonyDtmfEvents(): TelephonyDtmfEvents {
  return { dtmfEvents: [] };
}

export const TelephonyDtmfEvents: MessageFns<TelephonyDtmfEvents> = {
  encode(message: TelephonyDtmfEvents, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    writer.uint32(10).fork();
    for (const v of message.dtmfEvents) {
      writer.int32(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TelephonyDtmfEvents {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTelephonyDtmfEvents();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag === 8) {
            message.dtmfEvents.push(reader.int32() as any);

            continue;
          }

          if (tag === 10) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.dtmfEvents.push(reader.int32() as any);
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TelephonyDtmfEvents {
    return {
      dtmfEvents: globalThis.Array.isArray(object?.dtmfEvents)
        ? object.dtmfEvents.map((e: any) => telephonyDtmfFromJSON(e))
        : [],
    };
  },

  toJSON(message: TelephonyDtmfEvents): unknown {
    const obj: any = {};
    if (message.dtmfEvents?.length) {
      obj.dtmfEvents = message.dtmfEvents.map((e) => telephonyDtmfToJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<TelephonyDtmfEvents>): TelephonyDtmfEvents {
    return TelephonyDtmfEvents.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TelephonyDtmfEvents>): TelephonyDtmfEvents {
    const message = createBaseTelephonyDtmfEvents();
    message.dtmfEvents = object.dtmfEvents?.map((e) => e) || [];
    return message;
  },
};

function createBaseSpeechToTextConfig(): SpeechToTextConfig {
  return {
    speechModelVariant: 0,
    model: "",
    audioEncoding: 0,
    sampleRateHertz: 0,
    languageCode: "",
    enableWordInfo: false,
    useTimeoutBasedEndpointing: false,
  };
}

export const SpeechToTextConfig: MessageFns<SpeechToTextConfig> = {
  encode(message: SpeechToTextConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.speechModelVariant !== 0) {
      writer.uint32(8).int32(message.speechModelVariant);
    }
    if (message.model !== "") {
      writer.uint32(18).string(message.model);
    }
    if (message.audioEncoding !== 0) {
      writer.uint32(48).int32(message.audioEncoding);
    }
    if (message.sampleRateHertz !== 0) {
      writer.uint32(56).int32(message.sampleRateHertz);
    }
    if (message.languageCode !== "") {
      writer.uint32(66).string(message.languageCode);
    }
    if (message.enableWordInfo !== false) {
      writer.uint32(72).bool(message.enableWordInfo);
    }
    if (message.useTimeoutBasedEndpointing !== false) {
      writer.uint32(88).bool(message.useTimeoutBasedEndpointing);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechToTextConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechToTextConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.speechModelVariant = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.model = reader.string();
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.audioEncoding = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.sampleRateHertz = reader.int32();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.languageCode = reader.string();
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.enableWordInfo = reader.bool();
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.useTimeoutBasedEndpointing = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechToTextConfig {
    return {
      speechModelVariant: isSet(object.speechModelVariant) ? speechModelVariantFromJSON(object.speechModelVariant) : 0,
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      audioEncoding: isSet(object.audioEncoding) ? audioEncodingFromJSON(object.audioEncoding) : 0,
      sampleRateHertz: isSet(object.sampleRateHertz) ? globalThis.Number(object.sampleRateHertz) : 0,
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
      enableWordInfo: isSet(object.enableWordInfo) ? globalThis.Boolean(object.enableWordInfo) : false,
      useTimeoutBasedEndpointing: isSet(object.useTimeoutBasedEndpointing)
        ? globalThis.Boolean(object.useTimeoutBasedEndpointing)
        : false,
    };
  },

  toJSON(message: SpeechToTextConfig): unknown {
    const obj: any = {};
    if (message.speechModelVariant !== 0) {
      obj.speechModelVariant = speechModelVariantToJSON(message.speechModelVariant);
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.audioEncoding !== 0) {
      obj.audioEncoding = audioEncodingToJSON(message.audioEncoding);
    }
    if (message.sampleRateHertz !== 0) {
      obj.sampleRateHertz = Math.round(message.sampleRateHertz);
    }
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    if (message.enableWordInfo !== false) {
      obj.enableWordInfo = message.enableWordInfo;
    }
    if (message.useTimeoutBasedEndpointing !== false) {
      obj.useTimeoutBasedEndpointing = message.useTimeoutBasedEndpointing;
    }
    return obj;
  },

  create(base?: DeepPartial<SpeechToTextConfig>): SpeechToTextConfig {
    return SpeechToTextConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SpeechToTextConfig>): SpeechToTextConfig {
    const message = createBaseSpeechToTextConfig();
    message.speechModelVariant = object.speechModelVariant ?? 0;
    message.model = object.model ?? "";
    message.audioEncoding = object.audioEncoding ?? 0;
    message.sampleRateHertz = object.sampleRateHertz ?? 0;
    message.languageCode = object.languageCode ?? "";
    message.enableWordInfo = object.enableWordInfo ?? false;
    message.useTimeoutBasedEndpointing = object.useTimeoutBasedEndpointing ?? false;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
