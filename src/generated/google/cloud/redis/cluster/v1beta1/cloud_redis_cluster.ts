// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/redis/cluster/v1beta1/cloud_redis_cluster.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Operation } from "../../../../longrunning/operations.js";
import { FieldMask } from "../../../../protobuf/field_mask.js";
import { Timestamp } from "../../../../protobuf/timestamp.js";

export const protobufPackage = "google.cloud.redis.cluster.v1beta1";

/** Available authorization mode of a Redis cluster. */
export enum AuthorizationMode {
  /** AUTH_MODE_UNSPECIFIED - Not set. */
  AUTH_MODE_UNSPECIFIED = 0,
  /** AUTH_MODE_IAM_AUTH - IAM basic authorization mode */
  AUTH_MODE_IAM_AUTH = 1,
  /** AUTH_MODE_DISABLED - Authorization disabled mode */
  AUTH_MODE_DISABLED = 2,
  UNRECOGNIZED = -1,
}

export function authorizationModeFromJSON(object: any): AuthorizationMode {
  switch (object) {
    case 0:
    case "AUTH_MODE_UNSPECIFIED":
      return AuthorizationMode.AUTH_MODE_UNSPECIFIED;
    case 1:
    case "AUTH_MODE_IAM_AUTH":
      return AuthorizationMode.AUTH_MODE_IAM_AUTH;
    case 2:
    case "AUTH_MODE_DISABLED":
      return AuthorizationMode.AUTH_MODE_DISABLED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AuthorizationMode.UNRECOGNIZED;
  }
}

export function authorizationModeToJSON(object: AuthorizationMode): string {
  switch (object) {
    case AuthorizationMode.AUTH_MODE_UNSPECIFIED:
      return "AUTH_MODE_UNSPECIFIED";
    case AuthorizationMode.AUTH_MODE_IAM_AUTH:
      return "AUTH_MODE_IAM_AUTH";
    case AuthorizationMode.AUTH_MODE_DISABLED:
      return "AUTH_MODE_DISABLED";
    case AuthorizationMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** NodeType of a redis cluster node, */
export enum NodeType {
  NODE_TYPE_UNSPECIFIED = 0,
  /** REDIS_SHARED_CORE_NANO - Redis shared core nano node_type. */
  REDIS_SHARED_CORE_NANO = 1,
  /** REDIS_HIGHMEM_MEDIUM - Redis highmem medium node_type. */
  REDIS_HIGHMEM_MEDIUM = 2,
  /** REDIS_HIGHMEM_XLARGE - Redis highmem xlarge node_type. */
  REDIS_HIGHMEM_XLARGE = 3,
  /** REDIS_STANDARD_SMALL - Redis standard small node_type. */
  REDIS_STANDARD_SMALL = 4,
  UNRECOGNIZED = -1,
}

export function nodeTypeFromJSON(object: any): NodeType {
  switch (object) {
    case 0:
    case "NODE_TYPE_UNSPECIFIED":
      return NodeType.NODE_TYPE_UNSPECIFIED;
    case 1:
    case "REDIS_SHARED_CORE_NANO":
      return NodeType.REDIS_SHARED_CORE_NANO;
    case 2:
    case "REDIS_HIGHMEM_MEDIUM":
      return NodeType.REDIS_HIGHMEM_MEDIUM;
    case 3:
    case "REDIS_HIGHMEM_XLARGE":
      return NodeType.REDIS_HIGHMEM_XLARGE;
    case 4:
    case "REDIS_STANDARD_SMALL":
      return NodeType.REDIS_STANDARD_SMALL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return NodeType.UNRECOGNIZED;
  }
}

export function nodeTypeToJSON(object: NodeType): string {
  switch (object) {
    case NodeType.NODE_TYPE_UNSPECIFIED:
      return "NODE_TYPE_UNSPECIFIED";
    case NodeType.REDIS_SHARED_CORE_NANO:
      return "REDIS_SHARED_CORE_NANO";
    case NodeType.REDIS_HIGHMEM_MEDIUM:
      return "REDIS_HIGHMEM_MEDIUM";
    case NodeType.REDIS_HIGHMEM_XLARGE:
      return "REDIS_HIGHMEM_XLARGE";
    case NodeType.REDIS_STANDARD_SMALL:
      return "REDIS_STANDARD_SMALL";
    case NodeType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Available mode of in-transit encryption. */
export enum TransitEncryptionMode {
  /** TRANSIT_ENCRYPTION_MODE_UNSPECIFIED - In-transit encryption not set. */
  TRANSIT_ENCRYPTION_MODE_UNSPECIFIED = 0,
  /** TRANSIT_ENCRYPTION_MODE_DISABLED - In-transit encryption disabled. */
  TRANSIT_ENCRYPTION_MODE_DISABLED = 1,
  /** TRANSIT_ENCRYPTION_MODE_SERVER_AUTHENTICATION - Use server managed encryption for in-transit encryption. */
  TRANSIT_ENCRYPTION_MODE_SERVER_AUTHENTICATION = 2,
  UNRECOGNIZED = -1,
}

export function transitEncryptionModeFromJSON(object: any): TransitEncryptionMode {
  switch (object) {
    case 0:
    case "TRANSIT_ENCRYPTION_MODE_UNSPECIFIED":
      return TransitEncryptionMode.TRANSIT_ENCRYPTION_MODE_UNSPECIFIED;
    case 1:
    case "TRANSIT_ENCRYPTION_MODE_DISABLED":
      return TransitEncryptionMode.TRANSIT_ENCRYPTION_MODE_DISABLED;
    case 2:
    case "TRANSIT_ENCRYPTION_MODE_SERVER_AUTHENTICATION":
      return TransitEncryptionMode.TRANSIT_ENCRYPTION_MODE_SERVER_AUTHENTICATION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return TransitEncryptionMode.UNRECOGNIZED;
  }
}

export function transitEncryptionModeToJSON(object: TransitEncryptionMode): string {
  switch (object) {
    case TransitEncryptionMode.TRANSIT_ENCRYPTION_MODE_UNSPECIFIED:
      return "TRANSIT_ENCRYPTION_MODE_UNSPECIFIED";
    case TransitEncryptionMode.TRANSIT_ENCRYPTION_MODE_DISABLED:
      return "TRANSIT_ENCRYPTION_MODE_DISABLED";
    case TransitEncryptionMode.TRANSIT_ENCRYPTION_MODE_SERVER_AUTHENTICATION:
      return "TRANSIT_ENCRYPTION_MODE_SERVER_AUTHENTICATION";
    case TransitEncryptionMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Request for [CreateCluster][CloudRedis.CreateCluster]. */
export interface CreateClusterRequest {
  /**
   * Required. The resource name of the cluster location using the form:
   *     `projects/{project_id}/locations/{location_id}`
   * where `location_id` refers to a GCP region.
   */
  parent: string;
  /**
   * Required. The logical name of the Redis cluster in the customer project
   * with the following restrictions:
   *
   * * Must contain only lowercase letters, numbers, and hyphens.
   * * Must start with a letter.
   * * Must be between 1-63 characters.
   * * Must end with a number or a letter.
   * * Must be unique within the customer project / location
   */
  clusterId: string;
  /** Required. The cluster that is to be created. */
  cluster:
    | Cluster
    | undefined;
  /** Idempotent request UUID. */
  requestId: string;
}

/** Request for [ListClusters][CloudRedis.ListClusters]. */
export interface ListClustersRequest {
  /**
   * Required. The resource name of the cluster location using the form:
   *     `projects/{project_id}/locations/{location_id}`
   * where `location_id` refers to a GCP region.
   */
  parent: string;
  /**
   * The maximum number of items to return.
   *
   * If not specified, a default value of 1000 will be used by the service.
   * Regardless of the page_size value, the response may include a partial list
   * and a caller should only rely on response's
   * [`next_page_token`][google.cloud.redis.cluster.v1beta1.ListClustersResponse.next_page_token]
   * to determine if there are more clusters left to be queried.
   */
  pageSize: number;
  /**
   * The `next_page_token` value returned from a previous
   * [ListClusters][CloudRedis.ListClusters] request, if any.
   */
  pageToken: string;
}

/** Response for [ListClusters][CloudRedis.ListClusters]. */
export interface ListClustersResponse {
  /**
   * A list of Redis clusters in the project in the specified location,
   * or across all locations.
   *
   * If the `location_id` in the parent field of the request is "-", all regions
   * available to the project are queried, and the results aggregated.
   * If in such an aggregated query a location is unavailable, a placeholder
   * Redis entry is included in the response with the `name` field set to a
   * value of the form
   * `projects/{project_id}/locations/{location_id}/clusters/`- and the
   * `status` field set to ERROR and `status_message` field set to "location not
   * available for ListClusters".
   */
  clusters: Cluster[];
  /**
   * Token to retrieve the next page of results, or empty if there are no more
   * results in the list.
   */
  nextPageToken: string;
  /** Locations that could not be reached. */
  unreachable: string[];
}

/** Request for [UpdateCluster][CloudRedis.UpdateCluster]. */
export interface UpdateClusterRequest {
  /**
   * Required. Mask of fields to update. At least one path must be supplied in
   * this field. The elements of the repeated paths field may only include these
   * fields from [Cluster][google.cloud.redis.cluster.v1beta1.Cluster]:
   *
   *  *   `size_gb`
   *  *   `replica_count`
   */
  updateMask:
    | string[]
    | undefined;
  /**
   * Required. Update description.
   * Only fields specified in update_mask are updated.
   */
  cluster:
    | Cluster
    | undefined;
  /** Idempotent request UUID. */
  requestId: string;
}

/** Request for [GetCluster][CloudRedis.GetCluster]. */
export interface GetClusterRequest {
  /**
   * Required. Redis cluster resource name using the form:
   *     `projects/{project_id}/locations/{location_id}/clusters/{cluster_id}`
   * where `location_id` refers to a GCP region.
   */
  name: string;
}

/** Request for [DeleteCluster][CloudRedis.DeleteCluster]. */
export interface DeleteClusterRequest {
  /**
   * Required. Redis cluster resource name using the form:
   *     `projects/{project_id}/locations/{location_id}/clusters/{cluster_id}`
   * where `location_id` refers to a GCP region.
   */
  name: string;
  /** Idempotent request UUID. */
  requestId: string;
}

/**
 * Request for
 * [GetClusterCertificateAuthorityRequest][CloudRedis.GetClusterCertificateAuthorityRequest].
 */
export interface GetClusterCertificateAuthorityRequest {
  /**
   * Required. Redis cluster certificate authority resource name using the form:
   *     `projects/{project_id}/locations/{location_id}/clusters/{cluster_id}/certificateAuthority`
   * where `location_id` refers to a GCP region.
   */
  name: string;
}

/** A cluster instance. */
export interface Cluster {
  /**
   * Required. Unique name of the resource in this scope including project and
   * location using the form:
   *     `projects/{project_id}/locations/{location_id}/clusters/{cluster_id}`
   */
  name: string;
  /** Output only. The timestamp associated with the cluster creation request. */
  createTime:
    | Date
    | undefined;
  /**
   * Output only. The current state of this cluster.
   * Can be CREATING, READY, UPDATING, DELETING and SUSPENDED
   */
  state: Cluster_State;
  /** Output only. System assigned, unique identifier for the cluster. */
  uid: string;
  /** Optional. The number of replica nodes per shard. */
  replicaCount?:
    | number
    | undefined;
  /**
   * Optional. The authorization mode of the Redis cluster.
   * If not provided, auth feature is disabled for the cluster.
   */
  authorizationMode: AuthorizationMode;
  /**
   * Optional. The in-transit encryption for the Redis cluster.
   * If not provided, encryption  is disabled for the cluster.
   */
  transitEncryptionMode: TransitEncryptionMode;
  /**
   * Output only. Redis memory size in GB for the entire cluster rounded up to
   * the next integer.
   */
  sizeGb?:
    | number
    | undefined;
  /** Required. Number of shards for the Redis cluster. */
  shardCount?:
    | number
    | undefined;
  /**
   * Required. Each PscConfig configures the consumer network where IPs will
   * be designated to the cluster for client access through Private Service
   * Connect Automation. Currently, only one PscConfig is supported.
   */
  pscConfigs: PscConfig[];
  /**
   * Output only. Endpoints created on each given network, for Redis clients to
   * connect to the cluster. Currently only one discovery endpoint is supported.
   */
  discoveryEndpoints: DiscoveryEndpoint[];
  /**
   * Output only. PSC connections for discovery of the cluster topology and
   * accessing the cluster.
   */
  pscConnections: PscConnection[];
  /** Output only. Additional information about the current state of the cluster. */
  stateInfo:
    | Cluster_StateInfo
    | undefined;
  /**
   * Optional. The type of a redis node in the cluster. NodeType determines the
   * underlying machine-type of a redis node.
   */
  nodeType: NodeType;
  /** Optional. Persistence config (RDB, AOF) for the cluster. */
  persistenceConfig:
    | ClusterPersistenceConfig
    | undefined;
  /** Optional. Key/Value pairs of customer overrides for mutable Redis Configs */
  redisConfigs: { [key: string]: string };
  /**
   * Output only. Precise value of redis memory size in GB for the entire
   * cluster.
   */
  preciseSizeGb?:
    | number
    | undefined;
  /**
   * Optional. This config will be used to determine how the customer wants us
   * to distribute cluster resources within the region.
   */
  zoneDistributionConfig:
    | ZoneDistributionConfig
    | undefined;
  /** Optional. The delete operation will fail when the value is set to true. */
  deletionProtectionEnabled?: boolean | undefined;
}

/** Represents the different states of a Redis cluster. */
export enum Cluster_State {
  /** STATE_UNSPECIFIED - Not set. */
  STATE_UNSPECIFIED = 0,
  /** CREATING - Redis cluster is being created. */
  CREATING = 1,
  /** ACTIVE - Redis cluster has been created and is fully usable. */
  ACTIVE = 2,
  /** UPDATING - Redis cluster configuration is being updated. */
  UPDATING = 3,
  /** DELETING - Redis cluster is being deleted. */
  DELETING = 4,
  UNRECOGNIZED = -1,
}

export function cluster_StateFromJSON(object: any): Cluster_State {
  switch (object) {
    case 0:
    case "STATE_UNSPECIFIED":
      return Cluster_State.STATE_UNSPECIFIED;
    case 1:
    case "CREATING":
      return Cluster_State.CREATING;
    case 2:
    case "ACTIVE":
      return Cluster_State.ACTIVE;
    case 3:
    case "UPDATING":
      return Cluster_State.UPDATING;
    case 4:
    case "DELETING":
      return Cluster_State.DELETING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Cluster_State.UNRECOGNIZED;
  }
}

export function cluster_StateToJSON(object: Cluster_State): string {
  switch (object) {
    case Cluster_State.STATE_UNSPECIFIED:
      return "STATE_UNSPECIFIED";
    case Cluster_State.CREATING:
      return "CREATING";
    case Cluster_State.ACTIVE:
      return "ACTIVE";
    case Cluster_State.UPDATING:
      return "UPDATING";
    case Cluster_State.DELETING:
      return "DELETING";
    case Cluster_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Represents additional information about the state of the cluster. */
export interface Cluster_StateInfo {
  /** Describes ongoing update on the cluster when cluster state is UPDATING. */
  updateInfo?: Cluster_StateInfo_UpdateInfo | undefined;
}

/** Represents information about an updating cluster. */
export interface Cluster_StateInfo_UpdateInfo {
  /** Target number of shards for redis cluster */
  targetShardCount?:
    | number
    | undefined;
  /** Target number of replica nodes per shard. */
  targetReplicaCount?: number | undefined;
}

export interface Cluster_RedisConfigsEntry {
  key: string;
  value: string;
}

export interface PscConfig {
  /**
   * Required. The network where the IP address of the discovery endpoint will
   * be reserved, in the form of
   * projects/{network_project}/global/networks/{network_id}.
   */
  network: string;
}

/** Endpoints on each network, for Redis clients to connect to the cluster. */
export interface DiscoveryEndpoint {
  /**
   * Output only. Address of the exposed Redis endpoint used by clients to
   * connect to the service. The address could be either IP or hostname.
   */
  address: string;
  /** Output only. The port number of the exposed Redis endpoint. */
  port: number;
  /**
   * Output only. Customer configuration for where the endpoint is created and
   * accessed from.
   */
  pscConfig: PscConfig | undefined;
}

/** Details of consumer resources in a PSC connection. */
export interface PscConnection {
  /**
   * Output only. The PSC connection id of the forwarding rule connected to the
   * service attachment.
   */
  pscConnectionId: string;
  /**
   * Output only. The IP allocated on the consumer network for the PSC
   * forwarding rule.
   */
  address: string;
  /**
   * Output only. The URI of the consumer side forwarding rule.
   * Example:
   * projects/{projectNumOrId}/regions/us-east1/forwardingRules/{resourceId}.
   */
  forwardingRule: string;
  /**
   * Output only. The consumer project_id where the forwarding rule is created
   * from.
   */
  projectId: string;
  /**
   * The consumer network where the IP address resides, in the form of
   * projects/{project_id}/global/networks/{network_id}.
   */
  network: string;
}

/** Pre-defined metadata fields. */
export interface OperationMetadata {
  /** Output only. The time the operation was created. */
  createTime:
    | Date
    | undefined;
  /** Output only. The time the operation finished running. */
  endTime:
    | Date
    | undefined;
  /** Output only. Server-defined resource path for the target of the operation. */
  target: string;
  /** Output only. Name of the verb executed by the operation. */
  verb: string;
  /** Output only. Human-readable status of the operation, if any. */
  statusMessage: string;
  /**
   * Output only. Identifies whether the user has requested cancellation
   * of the operation. Operations that have successfully been cancelled
   * have [Operation.error][] value with a
   * [google.rpc.Status.code][google.rpc.Status.code] of 1, corresponding to
   * `Code.CANCELLED`.
   */
  requestedCancellation: boolean;
  /** Output only. API version used to start the operation. */
  apiVersion: string;
}

/** Redis cluster certificate authority */
export interface CertificateAuthority {
  managedServerCa?:
    | CertificateAuthority_ManagedCertificateAuthority
    | undefined;
  /**
   * Identifier. Unique name of the resource in this scope including project,
   * location and cluster using the form:
   *     `projects/{project}/locations/{location}/clusters/{cluster}/certificateAuthority`
   */
  name: string;
}

export interface CertificateAuthority_ManagedCertificateAuthority {
  /**
   * The PEM encoded CA certificate chains for redis managed
   * server authentication
   */
  caCerts: CertificateAuthority_ManagedCertificateAuthority_CertChain[];
}

export interface CertificateAuthority_ManagedCertificateAuthority_CertChain {
  /** The certificates that form the CA chain, from leaf to root order. */
  certificates: string[];
}

/** Configuration of the persistence functionality. */
export interface ClusterPersistenceConfig {
  /** Optional. The mode of persistence. */
  mode: ClusterPersistenceConfig_PersistenceMode;
  /** Optional. RDB configuration. This field will be ignored if mode is not RDB. */
  rdbConfig:
    | ClusterPersistenceConfig_RDBConfig
    | undefined;
  /** Optional. AOF configuration. This field will be ignored if mode is not AOF. */
  aofConfig: ClusterPersistenceConfig_AOFConfig | undefined;
}

/** Available persistence modes. */
export enum ClusterPersistenceConfig_PersistenceMode {
  /** PERSISTENCE_MODE_UNSPECIFIED - Not set. */
  PERSISTENCE_MODE_UNSPECIFIED = 0,
  /** DISABLED - Persistence is disabled, and any snapshot data is deleted. */
  DISABLED = 1,
  /** RDB - RDB based persistence is enabled. */
  RDB = 2,
  /** AOF - AOF based persistence is enabled. */
  AOF = 3,
  UNRECOGNIZED = -1,
}

export function clusterPersistenceConfig_PersistenceModeFromJSON(
  object: any,
): ClusterPersistenceConfig_PersistenceMode {
  switch (object) {
    case 0:
    case "PERSISTENCE_MODE_UNSPECIFIED":
      return ClusterPersistenceConfig_PersistenceMode.PERSISTENCE_MODE_UNSPECIFIED;
    case 1:
    case "DISABLED":
      return ClusterPersistenceConfig_PersistenceMode.DISABLED;
    case 2:
    case "RDB":
      return ClusterPersistenceConfig_PersistenceMode.RDB;
    case 3:
    case "AOF":
      return ClusterPersistenceConfig_PersistenceMode.AOF;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ClusterPersistenceConfig_PersistenceMode.UNRECOGNIZED;
  }
}

export function clusterPersistenceConfig_PersistenceModeToJSON(
  object: ClusterPersistenceConfig_PersistenceMode,
): string {
  switch (object) {
    case ClusterPersistenceConfig_PersistenceMode.PERSISTENCE_MODE_UNSPECIFIED:
      return "PERSISTENCE_MODE_UNSPECIFIED";
    case ClusterPersistenceConfig_PersistenceMode.DISABLED:
      return "DISABLED";
    case ClusterPersistenceConfig_PersistenceMode.RDB:
      return "RDB";
    case ClusterPersistenceConfig_PersistenceMode.AOF:
      return "AOF";
    case ClusterPersistenceConfig_PersistenceMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Configuration of the RDB based persistence. */
export interface ClusterPersistenceConfig_RDBConfig {
  /** Optional. Period between RDB snapshots. */
  rdbSnapshotPeriod: ClusterPersistenceConfig_RDBConfig_SnapshotPeriod;
  /**
   * Optional. The time that the first snapshot was/will be attempted, and to
   * which future snapshots will be aligned. If not provided, the current time
   * will be used.
   */
  rdbSnapshotStartTime: Date | undefined;
}

/** Available snapshot periods. */
export enum ClusterPersistenceConfig_RDBConfig_SnapshotPeriod {
  /** SNAPSHOT_PERIOD_UNSPECIFIED - Not set. */
  SNAPSHOT_PERIOD_UNSPECIFIED = 0,
  /** ONE_HOUR - One hour. */
  ONE_HOUR = 1,
  /** SIX_HOURS - Six hours. */
  SIX_HOURS = 2,
  /** TWELVE_HOURS - Twelve hours. */
  TWELVE_HOURS = 3,
  /** TWENTY_FOUR_HOURS - Twenty four hours. */
  TWENTY_FOUR_HOURS = 4,
  UNRECOGNIZED = -1,
}

export function clusterPersistenceConfig_RDBConfig_SnapshotPeriodFromJSON(
  object: any,
): ClusterPersistenceConfig_RDBConfig_SnapshotPeriod {
  switch (object) {
    case 0:
    case "SNAPSHOT_PERIOD_UNSPECIFIED":
      return ClusterPersistenceConfig_RDBConfig_SnapshotPeriod.SNAPSHOT_PERIOD_UNSPECIFIED;
    case 1:
    case "ONE_HOUR":
      return ClusterPersistenceConfig_RDBConfig_SnapshotPeriod.ONE_HOUR;
    case 2:
    case "SIX_HOURS":
      return ClusterPersistenceConfig_RDBConfig_SnapshotPeriod.SIX_HOURS;
    case 3:
    case "TWELVE_HOURS":
      return ClusterPersistenceConfig_RDBConfig_SnapshotPeriod.TWELVE_HOURS;
    case 4:
    case "TWENTY_FOUR_HOURS":
      return ClusterPersistenceConfig_RDBConfig_SnapshotPeriod.TWENTY_FOUR_HOURS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ClusterPersistenceConfig_RDBConfig_SnapshotPeriod.UNRECOGNIZED;
  }
}

export function clusterPersistenceConfig_RDBConfig_SnapshotPeriodToJSON(
  object: ClusterPersistenceConfig_RDBConfig_SnapshotPeriod,
): string {
  switch (object) {
    case ClusterPersistenceConfig_RDBConfig_SnapshotPeriod.SNAPSHOT_PERIOD_UNSPECIFIED:
      return "SNAPSHOT_PERIOD_UNSPECIFIED";
    case ClusterPersistenceConfig_RDBConfig_SnapshotPeriod.ONE_HOUR:
      return "ONE_HOUR";
    case ClusterPersistenceConfig_RDBConfig_SnapshotPeriod.SIX_HOURS:
      return "SIX_HOURS";
    case ClusterPersistenceConfig_RDBConfig_SnapshotPeriod.TWELVE_HOURS:
      return "TWELVE_HOURS";
    case ClusterPersistenceConfig_RDBConfig_SnapshotPeriod.TWENTY_FOUR_HOURS:
      return "TWENTY_FOUR_HOURS";
    case ClusterPersistenceConfig_RDBConfig_SnapshotPeriod.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Configuration of the AOF based persistence. */
export interface ClusterPersistenceConfig_AOFConfig {
  /** Optional. fsync configuration. */
  appendFsync: ClusterPersistenceConfig_AOFConfig_AppendFsync;
}

/** Available fsync modes. */
export enum ClusterPersistenceConfig_AOFConfig_AppendFsync {
  /** APPEND_FSYNC_UNSPECIFIED - Not set. Default: EVERYSEC */
  APPEND_FSYNC_UNSPECIFIED = 0,
  /**
   * NO - Never fsync. Normally Linux will flush data every 30 seconds with this
   * configuration, but it's up to the kernel's exact tuning.
   */
  NO = 1,
  /**
   * EVERYSEC - fsync every second. Fast enough, and you may lose 1 second of data if
   * there is a disaster
   */
  EVERYSEC = 2,
  /**
   * ALWAYS - fsync every time new commands are appended to the AOF. It has the best
   * data loss protection at the cost of performance
   */
  ALWAYS = 3,
  UNRECOGNIZED = -1,
}

export function clusterPersistenceConfig_AOFConfig_AppendFsyncFromJSON(
  object: any,
): ClusterPersistenceConfig_AOFConfig_AppendFsync {
  switch (object) {
    case 0:
    case "APPEND_FSYNC_UNSPECIFIED":
      return ClusterPersistenceConfig_AOFConfig_AppendFsync.APPEND_FSYNC_UNSPECIFIED;
    case 1:
    case "NO":
      return ClusterPersistenceConfig_AOFConfig_AppendFsync.NO;
    case 2:
    case "EVERYSEC":
      return ClusterPersistenceConfig_AOFConfig_AppendFsync.EVERYSEC;
    case 3:
    case "ALWAYS":
      return ClusterPersistenceConfig_AOFConfig_AppendFsync.ALWAYS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ClusterPersistenceConfig_AOFConfig_AppendFsync.UNRECOGNIZED;
  }
}

export function clusterPersistenceConfig_AOFConfig_AppendFsyncToJSON(
  object: ClusterPersistenceConfig_AOFConfig_AppendFsync,
): string {
  switch (object) {
    case ClusterPersistenceConfig_AOFConfig_AppendFsync.APPEND_FSYNC_UNSPECIFIED:
      return "APPEND_FSYNC_UNSPECIFIED";
    case ClusterPersistenceConfig_AOFConfig_AppendFsync.NO:
      return "NO";
    case ClusterPersistenceConfig_AOFConfig_AppendFsync.EVERYSEC:
      return "EVERYSEC";
    case ClusterPersistenceConfig_AOFConfig_AppendFsync.ALWAYS:
      return "ALWAYS";
    case ClusterPersistenceConfig_AOFConfig_AppendFsync.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Zone distribution config for allocation of cluster resources. */
export interface ZoneDistributionConfig {
  /**
   * Optional. The mode of zone distribution. Defaults to MULTI_ZONE, when not
   * specified.
   */
  mode: ZoneDistributionConfig_ZoneDistributionMode;
  /**
   * Optional. When SINGLE ZONE distribution is selected, zone field would be
   * used to allocate all resources in that zone. This is not applicable to
   * MULTI_ZONE, and would be ignored for MULTI_ZONE clusters.
   */
  zone: string;
}

/**
 * Defines various modes of zone distribution.
 * Currently supports two modes, can be expanded in future to support more
 * types of distribution modes.
 * design doc: go/same-zone-cluster
 */
export enum ZoneDistributionConfig_ZoneDistributionMode {
  /** ZONE_DISTRIBUTION_MODE_UNSPECIFIED - Not Set. Default: MULTI_ZONE */
  ZONE_DISTRIBUTION_MODE_UNSPECIFIED = 0,
  /**
   * MULTI_ZONE - Distribute all resources across 3 zones picked at random, within the
   * region.
   */
  MULTI_ZONE = 1,
  /**
   * SINGLE_ZONE - Distribute all resources in a single zone. The zone field must be
   * specified, when this mode is selected.
   */
  SINGLE_ZONE = 2,
  UNRECOGNIZED = -1,
}

export function zoneDistributionConfig_ZoneDistributionModeFromJSON(
  object: any,
): ZoneDistributionConfig_ZoneDistributionMode {
  switch (object) {
    case 0:
    case "ZONE_DISTRIBUTION_MODE_UNSPECIFIED":
      return ZoneDistributionConfig_ZoneDistributionMode.ZONE_DISTRIBUTION_MODE_UNSPECIFIED;
    case 1:
    case "MULTI_ZONE":
      return ZoneDistributionConfig_ZoneDistributionMode.MULTI_ZONE;
    case 2:
    case "SINGLE_ZONE":
      return ZoneDistributionConfig_ZoneDistributionMode.SINGLE_ZONE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ZoneDistributionConfig_ZoneDistributionMode.UNRECOGNIZED;
  }
}

export function zoneDistributionConfig_ZoneDistributionModeToJSON(
  object: ZoneDistributionConfig_ZoneDistributionMode,
): string {
  switch (object) {
    case ZoneDistributionConfig_ZoneDistributionMode.ZONE_DISTRIBUTION_MODE_UNSPECIFIED:
      return "ZONE_DISTRIBUTION_MODE_UNSPECIFIED";
    case ZoneDistributionConfig_ZoneDistributionMode.MULTI_ZONE:
      return "MULTI_ZONE";
    case ZoneDistributionConfig_ZoneDistributionMode.SINGLE_ZONE:
      return "SINGLE_ZONE";
    case ZoneDistributionConfig_ZoneDistributionMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseCreateClusterRequest(): CreateClusterRequest {
  return { parent: "", clusterId: "", cluster: undefined, requestId: "" };
}

export const CreateClusterRequest: MessageFns<CreateClusterRequest> = {
  encode(message: CreateClusterRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.clusterId !== "") {
      writer.uint32(18).string(message.clusterId);
    }
    if (message.cluster !== undefined) {
      Cluster.encode(message.cluster, writer.uint32(26).fork()).join();
    }
    if (message.requestId !== "") {
      writer.uint32(34).string(message.requestId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateClusterRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateClusterRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.clusterId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.cluster = Cluster.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.requestId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateClusterRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      clusterId: isSet(object.clusterId) ? globalThis.String(object.clusterId) : "",
      cluster: isSet(object.cluster) ? Cluster.fromJSON(object.cluster) : undefined,
      requestId: isSet(object.requestId) ? globalThis.String(object.requestId) : "",
    };
  },

  toJSON(message: CreateClusterRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.clusterId !== "") {
      obj.clusterId = message.clusterId;
    }
    if (message.cluster !== undefined) {
      obj.cluster = Cluster.toJSON(message.cluster);
    }
    if (message.requestId !== "") {
      obj.requestId = message.requestId;
    }
    return obj;
  },

  create(base?: DeepPartial<CreateClusterRequest>): CreateClusterRequest {
    return CreateClusterRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateClusterRequest>): CreateClusterRequest {
    const message = createBaseCreateClusterRequest();
    message.parent = object.parent ?? "";
    message.clusterId = object.clusterId ?? "";
    message.cluster = (object.cluster !== undefined && object.cluster !== null)
      ? Cluster.fromPartial(object.cluster)
      : undefined;
    message.requestId = object.requestId ?? "";
    return message;
  },
};

function createBaseListClustersRequest(): ListClustersRequest {
  return { parent: "", pageSize: 0, pageToken: "" };
}

export const ListClustersRequest: MessageFns<ListClustersRequest> = {
  encode(message: ListClustersRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.pageSize !== 0) {
      writer.uint32(16).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(26).string(message.pageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListClustersRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListClustersRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListClustersRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
    };
  },

  toJSON(message: ListClustersRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListClustersRequest>): ListClustersRequest {
    return ListClustersRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListClustersRequest>): ListClustersRequest {
    const message = createBaseListClustersRequest();
    message.parent = object.parent ?? "";
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    return message;
  },
};

function createBaseListClustersResponse(): ListClustersResponse {
  return { clusters: [], nextPageToken: "", unreachable: [] };
}

export const ListClustersResponse: MessageFns<ListClustersResponse> = {
  encode(message: ListClustersResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.clusters) {
      Cluster.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    for (const v of message.unreachable) {
      writer.uint32(26).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListClustersResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListClustersResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.clusters.push(Cluster.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.unreachable.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListClustersResponse {
    return {
      clusters: globalThis.Array.isArray(object?.clusters) ? object.clusters.map((e: any) => Cluster.fromJSON(e)) : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
      unreachable: globalThis.Array.isArray(object?.unreachable)
        ? object.unreachable.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: ListClustersResponse): unknown {
    const obj: any = {};
    if (message.clusters?.length) {
      obj.clusters = message.clusters.map((e) => Cluster.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    if (message.unreachable?.length) {
      obj.unreachable = message.unreachable;
    }
    return obj;
  },

  create(base?: DeepPartial<ListClustersResponse>): ListClustersResponse {
    return ListClustersResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListClustersResponse>): ListClustersResponse {
    const message = createBaseListClustersResponse();
    message.clusters = object.clusters?.map((e) => Cluster.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    message.unreachable = object.unreachable?.map((e) => e) || [];
    return message;
  },
};

function createBaseUpdateClusterRequest(): UpdateClusterRequest {
  return { updateMask: undefined, cluster: undefined, requestId: "" };
}

export const UpdateClusterRequest: MessageFns<UpdateClusterRequest> = {
  encode(message: UpdateClusterRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.updateMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.updateMask), writer.uint32(10).fork()).join();
    }
    if (message.cluster !== undefined) {
      Cluster.encode(message.cluster, writer.uint32(18).fork()).join();
    }
    if (message.requestId !== "") {
      writer.uint32(26).string(message.requestId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateClusterRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateClusterRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.updateMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.cluster = Cluster.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.requestId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateClusterRequest {
    return {
      updateMask: isSet(object.updateMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.updateMask)) : undefined,
      cluster: isSet(object.cluster) ? Cluster.fromJSON(object.cluster) : undefined,
      requestId: isSet(object.requestId) ? globalThis.String(object.requestId) : "",
    };
  },

  toJSON(message: UpdateClusterRequest): unknown {
    const obj: any = {};
    if (message.updateMask !== undefined) {
      obj.updateMask = FieldMask.toJSON(FieldMask.wrap(message.updateMask));
    }
    if (message.cluster !== undefined) {
      obj.cluster = Cluster.toJSON(message.cluster);
    }
    if (message.requestId !== "") {
      obj.requestId = message.requestId;
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateClusterRequest>): UpdateClusterRequest {
    return UpdateClusterRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateClusterRequest>): UpdateClusterRequest {
    const message = createBaseUpdateClusterRequest();
    message.updateMask = object.updateMask ?? undefined;
    message.cluster = (object.cluster !== undefined && object.cluster !== null)
      ? Cluster.fromPartial(object.cluster)
      : undefined;
    message.requestId = object.requestId ?? "";
    return message;
  },
};

function createBaseGetClusterRequest(): GetClusterRequest {
  return { name: "" };
}

export const GetClusterRequest: MessageFns<GetClusterRequest> = {
  encode(message: GetClusterRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetClusterRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetClusterRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetClusterRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: GetClusterRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<GetClusterRequest>): GetClusterRequest {
    return GetClusterRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetClusterRequest>): GetClusterRequest {
    const message = createBaseGetClusterRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseDeleteClusterRequest(): DeleteClusterRequest {
  return { name: "", requestId: "" };
}

export const DeleteClusterRequest: MessageFns<DeleteClusterRequest> = {
  encode(message: DeleteClusterRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.requestId !== "") {
      writer.uint32(18).string(message.requestId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeleteClusterRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeleteClusterRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.requestId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeleteClusterRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      requestId: isSet(object.requestId) ? globalThis.String(object.requestId) : "",
    };
  },

  toJSON(message: DeleteClusterRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.requestId !== "") {
      obj.requestId = message.requestId;
    }
    return obj;
  },

  create(base?: DeepPartial<DeleteClusterRequest>): DeleteClusterRequest {
    return DeleteClusterRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeleteClusterRequest>): DeleteClusterRequest {
    const message = createBaseDeleteClusterRequest();
    message.name = object.name ?? "";
    message.requestId = object.requestId ?? "";
    return message;
  },
};

function createBaseGetClusterCertificateAuthorityRequest(): GetClusterCertificateAuthorityRequest {
  return { name: "" };
}

export const GetClusterCertificateAuthorityRequest: MessageFns<GetClusterCertificateAuthorityRequest> = {
  encode(message: GetClusterCertificateAuthorityRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetClusterCertificateAuthorityRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetClusterCertificateAuthorityRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetClusterCertificateAuthorityRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: GetClusterCertificateAuthorityRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<GetClusterCertificateAuthorityRequest>): GetClusterCertificateAuthorityRequest {
    return GetClusterCertificateAuthorityRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetClusterCertificateAuthorityRequest>): GetClusterCertificateAuthorityRequest {
    const message = createBaseGetClusterCertificateAuthorityRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseCluster(): Cluster {
  return {
    name: "",
    createTime: undefined,
    state: 0,
    uid: "",
    replicaCount: undefined,
    authorizationMode: 0,
    transitEncryptionMode: 0,
    sizeGb: undefined,
    shardCount: undefined,
    pscConfigs: [],
    discoveryEndpoints: [],
    pscConnections: [],
    stateInfo: undefined,
    nodeType: 0,
    persistenceConfig: undefined,
    redisConfigs: {},
    preciseSizeGb: undefined,
    zoneDistributionConfig: undefined,
    deletionProtectionEnabled: undefined,
  };
}

export const Cluster: MessageFns<Cluster> = {
  encode(message: Cluster, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(26).fork()).join();
    }
    if (message.state !== 0) {
      writer.uint32(32).int32(message.state);
    }
    if (message.uid !== "") {
      writer.uint32(42).string(message.uid);
    }
    if (message.replicaCount !== undefined) {
      writer.uint32(64).int32(message.replicaCount);
    }
    if (message.authorizationMode !== 0) {
      writer.uint32(88).int32(message.authorizationMode);
    }
    if (message.transitEncryptionMode !== 0) {
      writer.uint32(96).int32(message.transitEncryptionMode);
    }
    if (message.sizeGb !== undefined) {
      writer.uint32(104).int32(message.sizeGb);
    }
    if (message.shardCount !== undefined) {
      writer.uint32(112).int32(message.shardCount);
    }
    for (const v of message.pscConfigs) {
      PscConfig.encode(v!, writer.uint32(122).fork()).join();
    }
    for (const v of message.discoveryEndpoints) {
      DiscoveryEndpoint.encode(v!, writer.uint32(130).fork()).join();
    }
    for (const v of message.pscConnections) {
      PscConnection.encode(v!, writer.uint32(138).fork()).join();
    }
    if (message.stateInfo !== undefined) {
      Cluster_StateInfo.encode(message.stateInfo, writer.uint32(146).fork()).join();
    }
    if (message.nodeType !== 0) {
      writer.uint32(152).int32(message.nodeType);
    }
    if (message.persistenceConfig !== undefined) {
      ClusterPersistenceConfig.encode(message.persistenceConfig, writer.uint32(162).fork()).join();
    }
    Object.entries(message.redisConfigs).forEach(([key, value]) => {
      Cluster_RedisConfigsEntry.encode({ key: key as any, value }, writer.uint32(170).fork()).join();
    });
    if (message.preciseSizeGb !== undefined) {
      writer.uint32(177).double(message.preciseSizeGb);
    }
    if (message.zoneDistributionConfig !== undefined) {
      ZoneDistributionConfig.encode(message.zoneDistributionConfig, writer.uint32(186).fork()).join();
    }
    if (message.deletionProtectionEnabled !== undefined) {
      writer.uint32(200).bool(message.deletionProtectionEnabled);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.uid = reader.string();
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.replicaCount = reader.int32();
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.authorizationMode = reader.int32() as any;
          continue;
        case 12:
          if (tag !== 96) {
            break;
          }

          message.transitEncryptionMode = reader.int32() as any;
          continue;
        case 13:
          if (tag !== 104) {
            break;
          }

          message.sizeGb = reader.int32();
          continue;
        case 14:
          if (tag !== 112) {
            break;
          }

          message.shardCount = reader.int32();
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.pscConfigs.push(PscConfig.decode(reader, reader.uint32()));
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.discoveryEndpoints.push(DiscoveryEndpoint.decode(reader, reader.uint32()));
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.pscConnections.push(PscConnection.decode(reader, reader.uint32()));
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.stateInfo = Cluster_StateInfo.decode(reader, reader.uint32());
          continue;
        case 19:
          if (tag !== 152) {
            break;
          }

          message.nodeType = reader.int32() as any;
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.persistenceConfig = ClusterPersistenceConfig.decode(reader, reader.uint32());
          continue;
        case 21:
          if (tag !== 170) {
            break;
          }

          const entry21 = Cluster_RedisConfigsEntry.decode(reader, reader.uint32());
          if (entry21.value !== undefined) {
            message.redisConfigs[entry21.key] = entry21.value;
          }
          continue;
        case 22:
          if (tag !== 177) {
            break;
          }

          message.preciseSizeGb = reader.double();
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.zoneDistributionConfig = ZoneDistributionConfig.decode(reader, reader.uint32());
          continue;
        case 25:
          if (tag !== 200) {
            break;
          }

          message.deletionProtectionEnabled = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      state: isSet(object.state) ? cluster_StateFromJSON(object.state) : 0,
      uid: isSet(object.uid) ? globalThis.String(object.uid) : "",
      replicaCount: isSet(object.replicaCount) ? globalThis.Number(object.replicaCount) : undefined,
      authorizationMode: isSet(object.authorizationMode) ? authorizationModeFromJSON(object.authorizationMode) : 0,
      transitEncryptionMode: isSet(object.transitEncryptionMode)
        ? transitEncryptionModeFromJSON(object.transitEncryptionMode)
        : 0,
      sizeGb: isSet(object.sizeGb) ? globalThis.Number(object.sizeGb) : undefined,
      shardCount: isSet(object.shardCount) ? globalThis.Number(object.shardCount) : undefined,
      pscConfigs: globalThis.Array.isArray(object?.pscConfigs)
        ? object.pscConfigs.map((e: any) => PscConfig.fromJSON(e))
        : [],
      discoveryEndpoints: globalThis.Array.isArray(object?.discoveryEndpoints)
        ? object.discoveryEndpoints.map((e: any) => DiscoveryEndpoint.fromJSON(e))
        : [],
      pscConnections: globalThis.Array.isArray(object?.pscConnections)
        ? object.pscConnections.map((e: any) => PscConnection.fromJSON(e))
        : [],
      stateInfo: isSet(object.stateInfo) ? Cluster_StateInfo.fromJSON(object.stateInfo) : undefined,
      nodeType: isSet(object.nodeType) ? nodeTypeFromJSON(object.nodeType) : 0,
      persistenceConfig: isSet(object.persistenceConfig)
        ? ClusterPersistenceConfig.fromJSON(object.persistenceConfig)
        : undefined,
      redisConfigs: isObject(object.redisConfigs)
        ? Object.entries(object.redisConfigs).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      preciseSizeGb: isSet(object.preciseSizeGb) ? globalThis.Number(object.preciseSizeGb) : undefined,
      zoneDistributionConfig: isSet(object.zoneDistributionConfig)
        ? ZoneDistributionConfig.fromJSON(object.zoneDistributionConfig)
        : undefined,
      deletionProtectionEnabled: isSet(object.deletionProtectionEnabled)
        ? globalThis.Boolean(object.deletionProtectionEnabled)
        : undefined,
    };
  },

  toJSON(message: Cluster): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.state !== 0) {
      obj.state = cluster_StateToJSON(message.state);
    }
    if (message.uid !== "") {
      obj.uid = message.uid;
    }
    if (message.replicaCount !== undefined) {
      obj.replicaCount = Math.round(message.replicaCount);
    }
    if (message.authorizationMode !== 0) {
      obj.authorizationMode = authorizationModeToJSON(message.authorizationMode);
    }
    if (message.transitEncryptionMode !== 0) {
      obj.transitEncryptionMode = transitEncryptionModeToJSON(message.transitEncryptionMode);
    }
    if (message.sizeGb !== undefined) {
      obj.sizeGb = Math.round(message.sizeGb);
    }
    if (message.shardCount !== undefined) {
      obj.shardCount = Math.round(message.shardCount);
    }
    if (message.pscConfigs?.length) {
      obj.pscConfigs = message.pscConfigs.map((e) => PscConfig.toJSON(e));
    }
    if (message.discoveryEndpoints?.length) {
      obj.discoveryEndpoints = message.discoveryEndpoints.map((e) => DiscoveryEndpoint.toJSON(e));
    }
    if (message.pscConnections?.length) {
      obj.pscConnections = message.pscConnections.map((e) => PscConnection.toJSON(e));
    }
    if (message.stateInfo !== undefined) {
      obj.stateInfo = Cluster_StateInfo.toJSON(message.stateInfo);
    }
    if (message.nodeType !== 0) {
      obj.nodeType = nodeTypeToJSON(message.nodeType);
    }
    if (message.persistenceConfig !== undefined) {
      obj.persistenceConfig = ClusterPersistenceConfig.toJSON(message.persistenceConfig);
    }
    if (message.redisConfigs) {
      const entries = Object.entries(message.redisConfigs);
      if (entries.length > 0) {
        obj.redisConfigs = {};
        entries.forEach(([k, v]) => {
          obj.redisConfigs[k] = v;
        });
      }
    }
    if (message.preciseSizeGb !== undefined) {
      obj.preciseSizeGb = message.preciseSizeGb;
    }
    if (message.zoneDistributionConfig !== undefined) {
      obj.zoneDistributionConfig = ZoneDistributionConfig.toJSON(message.zoneDistributionConfig);
    }
    if (message.deletionProtectionEnabled !== undefined) {
      obj.deletionProtectionEnabled = message.deletionProtectionEnabled;
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster>): Cluster {
    return Cluster.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster>): Cluster {
    const message = createBaseCluster();
    message.name = object.name ?? "";
    message.createTime = object.createTime ?? undefined;
    message.state = object.state ?? 0;
    message.uid = object.uid ?? "";
    message.replicaCount = object.replicaCount ?? undefined;
    message.authorizationMode = object.authorizationMode ?? 0;
    message.transitEncryptionMode = object.transitEncryptionMode ?? 0;
    message.sizeGb = object.sizeGb ?? undefined;
    message.shardCount = object.shardCount ?? undefined;
    message.pscConfigs = object.pscConfigs?.map((e) => PscConfig.fromPartial(e)) || [];
    message.discoveryEndpoints = object.discoveryEndpoints?.map((e) => DiscoveryEndpoint.fromPartial(e)) || [];
    message.pscConnections = object.pscConnections?.map((e) => PscConnection.fromPartial(e)) || [];
    message.stateInfo = (object.stateInfo !== undefined && object.stateInfo !== null)
      ? Cluster_StateInfo.fromPartial(object.stateInfo)
      : undefined;
    message.nodeType = object.nodeType ?? 0;
    message.persistenceConfig = (object.persistenceConfig !== undefined && object.persistenceConfig !== null)
      ? ClusterPersistenceConfig.fromPartial(object.persistenceConfig)
      : undefined;
    message.redisConfigs = Object.entries(object.redisConfigs ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.preciseSizeGb = object.preciseSizeGb ?? undefined;
    message.zoneDistributionConfig =
      (object.zoneDistributionConfig !== undefined && object.zoneDistributionConfig !== null)
        ? ZoneDistributionConfig.fromPartial(object.zoneDistributionConfig)
        : undefined;
    message.deletionProtectionEnabled = object.deletionProtectionEnabled ?? undefined;
    return message;
  },
};

function createBaseCluster_StateInfo(): Cluster_StateInfo {
  return { updateInfo: undefined };
}

export const Cluster_StateInfo: MessageFns<Cluster_StateInfo> = {
  encode(message: Cluster_StateInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.updateInfo !== undefined) {
      Cluster_StateInfo_UpdateInfo.encode(message.updateInfo, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster_StateInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster_StateInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.updateInfo = Cluster_StateInfo_UpdateInfo.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster_StateInfo {
    return {
      updateInfo: isSet(object.updateInfo) ? Cluster_StateInfo_UpdateInfo.fromJSON(object.updateInfo) : undefined,
    };
  },

  toJSON(message: Cluster_StateInfo): unknown {
    const obj: any = {};
    if (message.updateInfo !== undefined) {
      obj.updateInfo = Cluster_StateInfo_UpdateInfo.toJSON(message.updateInfo);
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster_StateInfo>): Cluster_StateInfo {
    return Cluster_StateInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster_StateInfo>): Cluster_StateInfo {
    const message = createBaseCluster_StateInfo();
    message.updateInfo = (object.updateInfo !== undefined && object.updateInfo !== null)
      ? Cluster_StateInfo_UpdateInfo.fromPartial(object.updateInfo)
      : undefined;
    return message;
  },
};

function createBaseCluster_StateInfo_UpdateInfo(): Cluster_StateInfo_UpdateInfo {
  return { targetShardCount: undefined, targetReplicaCount: undefined };
}

export const Cluster_StateInfo_UpdateInfo: MessageFns<Cluster_StateInfo_UpdateInfo> = {
  encode(message: Cluster_StateInfo_UpdateInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.targetShardCount !== undefined) {
      writer.uint32(8).int32(message.targetShardCount);
    }
    if (message.targetReplicaCount !== undefined) {
      writer.uint32(16).int32(message.targetReplicaCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster_StateInfo_UpdateInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster_StateInfo_UpdateInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.targetShardCount = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.targetReplicaCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster_StateInfo_UpdateInfo {
    return {
      targetShardCount: isSet(object.targetShardCount) ? globalThis.Number(object.targetShardCount) : undefined,
      targetReplicaCount: isSet(object.targetReplicaCount) ? globalThis.Number(object.targetReplicaCount) : undefined,
    };
  },

  toJSON(message: Cluster_StateInfo_UpdateInfo): unknown {
    const obj: any = {};
    if (message.targetShardCount !== undefined) {
      obj.targetShardCount = Math.round(message.targetShardCount);
    }
    if (message.targetReplicaCount !== undefined) {
      obj.targetReplicaCount = Math.round(message.targetReplicaCount);
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster_StateInfo_UpdateInfo>): Cluster_StateInfo_UpdateInfo {
    return Cluster_StateInfo_UpdateInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster_StateInfo_UpdateInfo>): Cluster_StateInfo_UpdateInfo {
    const message = createBaseCluster_StateInfo_UpdateInfo();
    message.targetShardCount = object.targetShardCount ?? undefined;
    message.targetReplicaCount = object.targetReplicaCount ?? undefined;
    return message;
  },
};

function createBaseCluster_RedisConfigsEntry(): Cluster_RedisConfigsEntry {
  return { key: "", value: "" };
}

export const Cluster_RedisConfigsEntry: MessageFns<Cluster_RedisConfigsEntry> = {
  encode(message: Cluster_RedisConfigsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster_RedisConfigsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster_RedisConfigsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster_RedisConfigsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Cluster_RedisConfigsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster_RedisConfigsEntry>): Cluster_RedisConfigsEntry {
    return Cluster_RedisConfigsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster_RedisConfigsEntry>): Cluster_RedisConfigsEntry {
    const message = createBaseCluster_RedisConfigsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBasePscConfig(): PscConfig {
  return { network: "" };
}

export const PscConfig: MessageFns<PscConfig> = {
  encode(message: PscConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.network !== "") {
      writer.uint32(18).string(message.network);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PscConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePscConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.network = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PscConfig {
    return { network: isSet(object.network) ? globalThis.String(object.network) : "" };
  },

  toJSON(message: PscConfig): unknown {
    const obj: any = {};
    if (message.network !== "") {
      obj.network = message.network;
    }
    return obj;
  },

  create(base?: DeepPartial<PscConfig>): PscConfig {
    return PscConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PscConfig>): PscConfig {
    const message = createBasePscConfig();
    message.network = object.network ?? "";
    return message;
  },
};

function createBaseDiscoveryEndpoint(): DiscoveryEndpoint {
  return { address: "", port: 0, pscConfig: undefined };
}

export const DiscoveryEndpoint: MessageFns<DiscoveryEndpoint> = {
  encode(message: DiscoveryEndpoint, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.address !== "") {
      writer.uint32(10).string(message.address);
    }
    if (message.port !== 0) {
      writer.uint32(16).int32(message.port);
    }
    if (message.pscConfig !== undefined) {
      PscConfig.encode(message.pscConfig, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DiscoveryEndpoint {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDiscoveryEndpoint();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.address = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.port = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pscConfig = PscConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DiscoveryEndpoint {
    return {
      address: isSet(object.address) ? globalThis.String(object.address) : "",
      port: isSet(object.port) ? globalThis.Number(object.port) : 0,
      pscConfig: isSet(object.pscConfig) ? PscConfig.fromJSON(object.pscConfig) : undefined,
    };
  },

  toJSON(message: DiscoveryEndpoint): unknown {
    const obj: any = {};
    if (message.address !== "") {
      obj.address = message.address;
    }
    if (message.port !== 0) {
      obj.port = Math.round(message.port);
    }
    if (message.pscConfig !== undefined) {
      obj.pscConfig = PscConfig.toJSON(message.pscConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<DiscoveryEndpoint>): DiscoveryEndpoint {
    return DiscoveryEndpoint.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DiscoveryEndpoint>): DiscoveryEndpoint {
    const message = createBaseDiscoveryEndpoint();
    message.address = object.address ?? "";
    message.port = object.port ?? 0;
    message.pscConfig = (object.pscConfig !== undefined && object.pscConfig !== null)
      ? PscConfig.fromPartial(object.pscConfig)
      : undefined;
    return message;
  },
};

function createBasePscConnection(): PscConnection {
  return { pscConnectionId: "", address: "", forwardingRule: "", projectId: "", network: "" };
}

export const PscConnection: MessageFns<PscConnection> = {
  encode(message: PscConnection, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.pscConnectionId !== "") {
      writer.uint32(10).string(message.pscConnectionId);
    }
    if (message.address !== "") {
      writer.uint32(18).string(message.address);
    }
    if (message.forwardingRule !== "") {
      writer.uint32(26).string(message.forwardingRule);
    }
    if (message.projectId !== "") {
      writer.uint32(34).string(message.projectId);
    }
    if (message.network !== "") {
      writer.uint32(42).string(message.network);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PscConnection {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePscConnection();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.pscConnectionId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.address = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.forwardingRule = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.network = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PscConnection {
    return {
      pscConnectionId: isSet(object.pscConnectionId) ? globalThis.String(object.pscConnectionId) : "",
      address: isSet(object.address) ? globalThis.String(object.address) : "",
      forwardingRule: isSet(object.forwardingRule) ? globalThis.String(object.forwardingRule) : "",
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      network: isSet(object.network) ? globalThis.String(object.network) : "",
    };
  },

  toJSON(message: PscConnection): unknown {
    const obj: any = {};
    if (message.pscConnectionId !== "") {
      obj.pscConnectionId = message.pscConnectionId;
    }
    if (message.address !== "") {
      obj.address = message.address;
    }
    if (message.forwardingRule !== "") {
      obj.forwardingRule = message.forwardingRule;
    }
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.network !== "") {
      obj.network = message.network;
    }
    return obj;
  },

  create(base?: DeepPartial<PscConnection>): PscConnection {
    return PscConnection.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PscConnection>): PscConnection {
    const message = createBasePscConnection();
    message.pscConnectionId = object.pscConnectionId ?? "";
    message.address = object.address ?? "";
    message.forwardingRule = object.forwardingRule ?? "";
    message.projectId = object.projectId ?? "";
    message.network = object.network ?? "";
    return message;
  },
};

function createBaseOperationMetadata(): OperationMetadata {
  return {
    createTime: undefined,
    endTime: undefined,
    target: "",
    verb: "",
    statusMessage: "",
    requestedCancellation: false,
    apiVersion: "",
  };
}

export const OperationMetadata: MessageFns<OperationMetadata> = {
  encode(message: OperationMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(10).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(18).fork()).join();
    }
    if (message.target !== "") {
      writer.uint32(26).string(message.target);
    }
    if (message.verb !== "") {
      writer.uint32(34).string(message.verb);
    }
    if (message.statusMessage !== "") {
      writer.uint32(42).string(message.statusMessage);
    }
    if (message.requestedCancellation !== false) {
      writer.uint32(48).bool(message.requestedCancellation);
    }
    if (message.apiVersion !== "") {
      writer.uint32(58).string(message.apiVersion);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OperationMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOperationMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.target = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.verb = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.statusMessage = reader.string();
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.requestedCancellation = reader.bool();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.apiVersion = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OperationMetadata {
    return {
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      target: isSet(object.target) ? globalThis.String(object.target) : "",
      verb: isSet(object.verb) ? globalThis.String(object.verb) : "",
      statusMessage: isSet(object.statusMessage) ? globalThis.String(object.statusMessage) : "",
      requestedCancellation: isSet(object.requestedCancellation)
        ? globalThis.Boolean(object.requestedCancellation)
        : false,
      apiVersion: isSet(object.apiVersion) ? globalThis.String(object.apiVersion) : "",
    };
  },

  toJSON(message: OperationMetadata): unknown {
    const obj: any = {};
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (message.target !== "") {
      obj.target = message.target;
    }
    if (message.verb !== "") {
      obj.verb = message.verb;
    }
    if (message.statusMessage !== "") {
      obj.statusMessage = message.statusMessage;
    }
    if (message.requestedCancellation !== false) {
      obj.requestedCancellation = message.requestedCancellation;
    }
    if (message.apiVersion !== "") {
      obj.apiVersion = message.apiVersion;
    }
    return obj;
  },

  create(base?: DeepPartial<OperationMetadata>): OperationMetadata {
    return OperationMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OperationMetadata>): OperationMetadata {
    const message = createBaseOperationMetadata();
    message.createTime = object.createTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    message.target = object.target ?? "";
    message.verb = object.verb ?? "";
    message.statusMessage = object.statusMessage ?? "";
    message.requestedCancellation = object.requestedCancellation ?? false;
    message.apiVersion = object.apiVersion ?? "";
    return message;
  },
};

function createBaseCertificateAuthority(): CertificateAuthority {
  return { managedServerCa: undefined, name: "" };
}

export const CertificateAuthority: MessageFns<CertificateAuthority> = {
  encode(message: CertificateAuthority, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.managedServerCa !== undefined) {
      CertificateAuthority_ManagedCertificateAuthority.encode(message.managedServerCa, writer.uint32(10).fork()).join();
    }
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CertificateAuthority {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCertificateAuthority();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.managedServerCa = CertificateAuthority_ManagedCertificateAuthority.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CertificateAuthority {
    return {
      managedServerCa: isSet(object.managedServerCa)
        ? CertificateAuthority_ManagedCertificateAuthority.fromJSON(object.managedServerCa)
        : undefined,
      name: isSet(object.name) ? globalThis.String(object.name) : "",
    };
  },

  toJSON(message: CertificateAuthority): unknown {
    const obj: any = {};
    if (message.managedServerCa !== undefined) {
      obj.managedServerCa = CertificateAuthority_ManagedCertificateAuthority.toJSON(message.managedServerCa);
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<CertificateAuthority>): CertificateAuthority {
    return CertificateAuthority.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CertificateAuthority>): CertificateAuthority {
    const message = createBaseCertificateAuthority();
    message.managedServerCa = (object.managedServerCa !== undefined && object.managedServerCa !== null)
      ? CertificateAuthority_ManagedCertificateAuthority.fromPartial(object.managedServerCa)
      : undefined;
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseCertificateAuthority_ManagedCertificateAuthority(): CertificateAuthority_ManagedCertificateAuthority {
  return { caCerts: [] };
}

export const CertificateAuthority_ManagedCertificateAuthority: MessageFns<
  CertificateAuthority_ManagedCertificateAuthority
> = {
  encode(
    message: CertificateAuthority_ManagedCertificateAuthority,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    for (const v of message.caCerts) {
      CertificateAuthority_ManagedCertificateAuthority_CertChain.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CertificateAuthority_ManagedCertificateAuthority {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCertificateAuthority_ManagedCertificateAuthority();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.caCerts.push(
            CertificateAuthority_ManagedCertificateAuthority_CertChain.decode(reader, reader.uint32()),
          );
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CertificateAuthority_ManagedCertificateAuthority {
    return {
      caCerts: globalThis.Array.isArray(object?.caCerts)
        ? object.caCerts.map((e: any) => CertificateAuthority_ManagedCertificateAuthority_CertChain.fromJSON(e))
        : [],
    };
  },

  toJSON(message: CertificateAuthority_ManagedCertificateAuthority): unknown {
    const obj: any = {};
    if (message.caCerts?.length) {
      obj.caCerts = message.caCerts.map((e) => CertificateAuthority_ManagedCertificateAuthority_CertChain.toJSON(e));
    }
    return obj;
  },

  create(
    base?: DeepPartial<CertificateAuthority_ManagedCertificateAuthority>,
  ): CertificateAuthority_ManagedCertificateAuthority {
    return CertificateAuthority_ManagedCertificateAuthority.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<CertificateAuthority_ManagedCertificateAuthority>,
  ): CertificateAuthority_ManagedCertificateAuthority {
    const message = createBaseCertificateAuthority_ManagedCertificateAuthority();
    message.caCerts =
      object.caCerts?.map((e) => CertificateAuthority_ManagedCertificateAuthority_CertChain.fromPartial(e)) || [];
    return message;
  },
};

function createBaseCertificateAuthority_ManagedCertificateAuthority_CertChain(): CertificateAuthority_ManagedCertificateAuthority_CertChain {
  return { certificates: [] };
}

export const CertificateAuthority_ManagedCertificateAuthority_CertChain: MessageFns<
  CertificateAuthority_ManagedCertificateAuthority_CertChain
> = {
  encode(
    message: CertificateAuthority_ManagedCertificateAuthority_CertChain,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    for (const v of message.certificates) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): CertificateAuthority_ManagedCertificateAuthority_CertChain {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCertificateAuthority_ManagedCertificateAuthority_CertChain();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.certificates.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CertificateAuthority_ManagedCertificateAuthority_CertChain {
    return {
      certificates: globalThis.Array.isArray(object?.certificates)
        ? object.certificates.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: CertificateAuthority_ManagedCertificateAuthority_CertChain): unknown {
    const obj: any = {};
    if (message.certificates?.length) {
      obj.certificates = message.certificates;
    }
    return obj;
  },

  create(
    base?: DeepPartial<CertificateAuthority_ManagedCertificateAuthority_CertChain>,
  ): CertificateAuthority_ManagedCertificateAuthority_CertChain {
    return CertificateAuthority_ManagedCertificateAuthority_CertChain.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<CertificateAuthority_ManagedCertificateAuthority_CertChain>,
  ): CertificateAuthority_ManagedCertificateAuthority_CertChain {
    const message = createBaseCertificateAuthority_ManagedCertificateAuthority_CertChain();
    message.certificates = object.certificates?.map((e) => e) || [];
    return message;
  },
};

function createBaseClusterPersistenceConfig(): ClusterPersistenceConfig {
  return { mode: 0, rdbConfig: undefined, aofConfig: undefined };
}

export const ClusterPersistenceConfig: MessageFns<ClusterPersistenceConfig> = {
  encode(message: ClusterPersistenceConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.mode !== 0) {
      writer.uint32(8).int32(message.mode);
    }
    if (message.rdbConfig !== undefined) {
      ClusterPersistenceConfig_RDBConfig.encode(message.rdbConfig, writer.uint32(18).fork()).join();
    }
    if (message.aofConfig !== undefined) {
      ClusterPersistenceConfig_AOFConfig.encode(message.aofConfig, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterPersistenceConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterPersistenceConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.mode = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.rdbConfig = ClusterPersistenceConfig_RDBConfig.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.aofConfig = ClusterPersistenceConfig_AOFConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterPersistenceConfig {
    return {
      mode: isSet(object.mode) ? clusterPersistenceConfig_PersistenceModeFromJSON(object.mode) : 0,
      rdbConfig: isSet(object.rdbConfig) ? ClusterPersistenceConfig_RDBConfig.fromJSON(object.rdbConfig) : undefined,
      aofConfig: isSet(object.aofConfig) ? ClusterPersistenceConfig_AOFConfig.fromJSON(object.aofConfig) : undefined,
    };
  },

  toJSON(message: ClusterPersistenceConfig): unknown {
    const obj: any = {};
    if (message.mode !== 0) {
      obj.mode = clusterPersistenceConfig_PersistenceModeToJSON(message.mode);
    }
    if (message.rdbConfig !== undefined) {
      obj.rdbConfig = ClusterPersistenceConfig_RDBConfig.toJSON(message.rdbConfig);
    }
    if (message.aofConfig !== undefined) {
      obj.aofConfig = ClusterPersistenceConfig_AOFConfig.toJSON(message.aofConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterPersistenceConfig>): ClusterPersistenceConfig {
    return ClusterPersistenceConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterPersistenceConfig>): ClusterPersistenceConfig {
    const message = createBaseClusterPersistenceConfig();
    message.mode = object.mode ?? 0;
    message.rdbConfig = (object.rdbConfig !== undefined && object.rdbConfig !== null)
      ? ClusterPersistenceConfig_RDBConfig.fromPartial(object.rdbConfig)
      : undefined;
    message.aofConfig = (object.aofConfig !== undefined && object.aofConfig !== null)
      ? ClusterPersistenceConfig_AOFConfig.fromPartial(object.aofConfig)
      : undefined;
    return message;
  },
};

function createBaseClusterPersistenceConfig_RDBConfig(): ClusterPersistenceConfig_RDBConfig {
  return { rdbSnapshotPeriod: 0, rdbSnapshotStartTime: undefined };
}

export const ClusterPersistenceConfig_RDBConfig: MessageFns<ClusterPersistenceConfig_RDBConfig> = {
  encode(message: ClusterPersistenceConfig_RDBConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.rdbSnapshotPeriod !== 0) {
      writer.uint32(8).int32(message.rdbSnapshotPeriod);
    }
    if (message.rdbSnapshotStartTime !== undefined) {
      Timestamp.encode(toTimestamp(message.rdbSnapshotStartTime), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterPersistenceConfig_RDBConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterPersistenceConfig_RDBConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.rdbSnapshotPeriod = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.rdbSnapshotStartTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterPersistenceConfig_RDBConfig {
    return {
      rdbSnapshotPeriod: isSet(object.rdbSnapshotPeriod)
        ? clusterPersistenceConfig_RDBConfig_SnapshotPeriodFromJSON(object.rdbSnapshotPeriod)
        : 0,
      rdbSnapshotStartTime: isSet(object.rdbSnapshotStartTime)
        ? fromJsonTimestamp(object.rdbSnapshotStartTime)
        : undefined,
    };
  },

  toJSON(message: ClusterPersistenceConfig_RDBConfig): unknown {
    const obj: any = {};
    if (message.rdbSnapshotPeriod !== 0) {
      obj.rdbSnapshotPeriod = clusterPersistenceConfig_RDBConfig_SnapshotPeriodToJSON(message.rdbSnapshotPeriod);
    }
    if (message.rdbSnapshotStartTime !== undefined) {
      obj.rdbSnapshotStartTime = message.rdbSnapshotStartTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterPersistenceConfig_RDBConfig>): ClusterPersistenceConfig_RDBConfig {
    return ClusterPersistenceConfig_RDBConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterPersistenceConfig_RDBConfig>): ClusterPersistenceConfig_RDBConfig {
    const message = createBaseClusterPersistenceConfig_RDBConfig();
    message.rdbSnapshotPeriod = object.rdbSnapshotPeriod ?? 0;
    message.rdbSnapshotStartTime = object.rdbSnapshotStartTime ?? undefined;
    return message;
  },
};

function createBaseClusterPersistenceConfig_AOFConfig(): ClusterPersistenceConfig_AOFConfig {
  return { appendFsync: 0 };
}

export const ClusterPersistenceConfig_AOFConfig: MessageFns<ClusterPersistenceConfig_AOFConfig> = {
  encode(message: ClusterPersistenceConfig_AOFConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.appendFsync !== 0) {
      writer.uint32(8).int32(message.appendFsync);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterPersistenceConfig_AOFConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterPersistenceConfig_AOFConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.appendFsync = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterPersistenceConfig_AOFConfig {
    return {
      appendFsync: isSet(object.appendFsync)
        ? clusterPersistenceConfig_AOFConfig_AppendFsyncFromJSON(object.appendFsync)
        : 0,
    };
  },

  toJSON(message: ClusterPersistenceConfig_AOFConfig): unknown {
    const obj: any = {};
    if (message.appendFsync !== 0) {
      obj.appendFsync = clusterPersistenceConfig_AOFConfig_AppendFsyncToJSON(message.appendFsync);
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterPersistenceConfig_AOFConfig>): ClusterPersistenceConfig_AOFConfig {
    return ClusterPersistenceConfig_AOFConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterPersistenceConfig_AOFConfig>): ClusterPersistenceConfig_AOFConfig {
    const message = createBaseClusterPersistenceConfig_AOFConfig();
    message.appendFsync = object.appendFsync ?? 0;
    return message;
  },
};

function createBaseZoneDistributionConfig(): ZoneDistributionConfig {
  return { mode: 0, zone: "" };
}

export const ZoneDistributionConfig: MessageFns<ZoneDistributionConfig> = {
  encode(message: ZoneDistributionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.mode !== 0) {
      writer.uint32(8).int32(message.mode);
    }
    if (message.zone !== "") {
      writer.uint32(18).string(message.zone);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ZoneDistributionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseZoneDistributionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.mode = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.zone = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ZoneDistributionConfig {
    return {
      mode: isSet(object.mode) ? zoneDistributionConfig_ZoneDistributionModeFromJSON(object.mode) : 0,
      zone: isSet(object.zone) ? globalThis.String(object.zone) : "",
    };
  },

  toJSON(message: ZoneDistributionConfig): unknown {
    const obj: any = {};
    if (message.mode !== 0) {
      obj.mode = zoneDistributionConfig_ZoneDistributionModeToJSON(message.mode);
    }
    if (message.zone !== "") {
      obj.zone = message.zone;
    }
    return obj;
  },

  create(base?: DeepPartial<ZoneDistributionConfig>): ZoneDistributionConfig {
    return ZoneDistributionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ZoneDistributionConfig>): ZoneDistributionConfig {
    const message = createBaseZoneDistributionConfig();
    message.mode = object.mode ?? 0;
    message.zone = object.zone ?? "";
    return message;
  },
};

/**
 * Configures and manages Cloud Memorystore for Redis clusters
 *
 * Google Cloud Memorystore for Redis Cluster
 *
 * The `redis.googleapis.com` service implements the Google Cloud Memorystore
 * for Redis API and defines the following resource model for managing Redis
 * clusters:
 * * The service works with a collection of cloud projects, named: `/projects/*`
 * * Each project has a collection of available locations, named: `/locations/*`
 * * Each location has a collection of Redis clusters, named: `/clusters/*`
 * * As such, Redis clusters are resources of the form:
 *   `/projects/{project_id}/locations/{location_id}/clusters/{instance_id}`
 *
 * Note that location_id must be a GCP `region`; for example:
 * * `projects/redpepper-1290/locations/us-central1/clusters/my-redis`
 *
 * We use API version selector for Flex APIs
 * * The versioning strategy is release-based versioning
 * * Our backend CLH only deals with the superset version (called v1main)
 * * Existing backend for Redis Gen1 and MRR is not touched.
 * * More details in go/redis-flex-api-versioning
 */
export type CloudRedisClusterDefinition = typeof CloudRedisClusterDefinition;
export const CloudRedisClusterDefinition = {
  name: "CloudRedisCluster",
  fullName: "google.cloud.redis.cluster.v1beta1.CloudRedisCluster",
  methods: {
    /**
     * Lists all Redis clusters owned by a project in either the specified
     * location (region) or all locations.
     *
     * The location should have the following format:
     *
     * * `projects/{project_id}/locations/{location_id}`
     *
     * If `location_id` is specified as `-` (wildcard), then all regions
     * available to the project are queried, and the results are aggregated.
     */
    listClusters: {
      name: "ListClusters",
      requestType: ListClustersRequest,
      requestStream: false,
      responseType: ListClustersResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([6, 112, 97, 114, 101, 110, 116])],
          578365826: [
            Buffer.from([
              51,
              18,
              49,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              47,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              115,
            ]),
          ],
        },
      },
    },
    /** Gets the details of a specific Redis cluster. */
    getCluster: {
      name: "GetCluster",
      requestType: GetClusterRequest,
      requestStream: false,
      responseType: Cluster,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              51,
              18,
              49,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Updates the metadata and configuration of a specific Redis cluster.
     *
     * Completed longrunning.Operation will contain the new cluster object
     * in the response field. The returned operation is automatically deleted
     * after a few hours, so there is no need to call DeleteOperation.
     */
    updateCluster: {
      name: "UpdateCluster",
      requestType: UpdateClusterRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              30,
              10,
              7,
              67,
              108,
              117,
              115,
              116,
              101,
              114,
              18,
              19,
              103,
              111,
              111,
              103,
              108,
              101,
              46,
              112,
              114,
              111,
              116,
              111,
              98,
              117,
              102,
              46,
              65,
              110,
              121,
            ]),
          ],
          8410: [
            Buffer.from([19, 99, 108, 117, 115, 116, 101, 114, 44, 117, 112, 100, 97, 116, 101, 95, 109, 97, 115, 107]),
          ],
          578365826: [
            Buffer.from([
              68,
              58,
              7,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              50,
              57,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              46,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Deletes a specific Redis cluster. Cluster stops serving and data is
     * deleted.
     */
    deleteCluster: {
      name: "DeleteCluster",
      requestType: DeleteClusterRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              44,
              10,
              21,
              103,
              111,
              111,
              103,
              108,
              101,
              46,
              112,
              114,
              111,
              116,
              111,
              98,
              117,
              102,
              46,
              69,
              109,
              112,
              116,
              121,
              18,
              19,
              103,
              111,
              111,
              103,
              108,
              101,
              46,
              112,
              114,
              111,
              116,
              111,
              98,
              117,
              102,
              46,
              65,
              110,
              121,
            ]),
          ],
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              51,
              42,
              49,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Creates a Redis cluster based on the specified properties.
     * The creation is executed asynchronously and callers may check the returned
     * operation to track its progress. Once the operation is completed the Redis
     * cluster will be fully functional. The completed longrunning.Operation will
     * contain the new cluster object in the response field.
     *
     * The returned operation is automatically deleted after a few hours, so there
     * is no need to call DeleteOperation.
     */
    createCluster: {
      name: "CreateCluster",
      requestType: CreateClusterRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              30,
              10,
              7,
              67,
              108,
              117,
              115,
              116,
              101,
              114,
              18,
              19,
              103,
              111,
              111,
              103,
              108,
              101,
              46,
              112,
              114,
              111,
              116,
              111,
              98,
              117,
              102,
              46,
              65,
              110,
              121,
            ]),
          ],
          8410: [
            Buffer.from([
              25,
              112,
              97,
              114,
              101,
              110,
              116,
              44,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              44,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              95,
              105,
              100,
            ]),
          ],
          578365826: [
            Buffer.from([
              60,
              58,
              7,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              34,
              49,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              47,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              115,
            ]),
          ],
        },
      },
    },
    /** Gets the details of certificate authority information for Redis cluster. */
    getClusterCertificateAuthority: {
      name: "GetClusterCertificateAuthority",
      requestType: GetClusterCertificateAuthorityRequest,
      requestStream: false,
      responseType: CertificateAuthority,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              72,
              18,
              70,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              115,
              47,
              42,
              47,
              99,
              101,
              114,
              116,
              105,
              102,
              105,
              99,
              97,
              116,
              101,
              65,
              117,
              116,
              104,
              111,
              114,
              105,
              116,
              121,
              125,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface CloudRedisClusterServiceImplementation<CallContextExt = {}> {
  /**
   * Lists all Redis clusters owned by a project in either the specified
   * location (region) or all locations.
   *
   * The location should have the following format:
   *
   * * `projects/{project_id}/locations/{location_id}`
   *
   * If `location_id` is specified as `-` (wildcard), then all regions
   * available to the project are queried, and the results are aggregated.
   */
  listClusters(
    request: ListClustersRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ListClustersResponse>>;
  /** Gets the details of a specific Redis cluster. */
  getCluster(request: GetClusterRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Cluster>>;
  /**
   * Updates the metadata and configuration of a specific Redis cluster.
   *
   * Completed longrunning.Operation will contain the new cluster object
   * in the response field. The returned operation is automatically deleted
   * after a few hours, so there is no need to call DeleteOperation.
   */
  updateCluster(request: UpdateClusterRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
  /**
   * Deletes a specific Redis cluster. Cluster stops serving and data is
   * deleted.
   */
  deleteCluster(request: DeleteClusterRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
  /**
   * Creates a Redis cluster based on the specified properties.
   * The creation is executed asynchronously and callers may check the returned
   * operation to track its progress. Once the operation is completed the Redis
   * cluster will be fully functional. The completed longrunning.Operation will
   * contain the new cluster object in the response field.
   *
   * The returned operation is automatically deleted after a few hours, so there
   * is no need to call DeleteOperation.
   */
  createCluster(request: CreateClusterRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
  /** Gets the details of certificate authority information for Redis cluster. */
  getClusterCertificateAuthority(
    request: GetClusterCertificateAuthorityRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<CertificateAuthority>>;
}

export interface CloudRedisClusterClient<CallOptionsExt = {}> {
  /**
   * Lists all Redis clusters owned by a project in either the specified
   * location (region) or all locations.
   *
   * The location should have the following format:
   *
   * * `projects/{project_id}/locations/{location_id}`
   *
   * If `location_id` is specified as `-` (wildcard), then all regions
   * available to the project are queried, and the results are aggregated.
   */
  listClusters(
    request: DeepPartial<ListClustersRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ListClustersResponse>;
  /** Gets the details of a specific Redis cluster. */
  getCluster(request: DeepPartial<GetClusterRequest>, options?: CallOptions & CallOptionsExt): Promise<Cluster>;
  /**
   * Updates the metadata and configuration of a specific Redis cluster.
   *
   * Completed longrunning.Operation will contain the new cluster object
   * in the response field. The returned operation is automatically deleted
   * after a few hours, so there is no need to call DeleteOperation.
   */
  updateCluster(request: DeepPartial<UpdateClusterRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
  /**
   * Deletes a specific Redis cluster. Cluster stops serving and data is
   * deleted.
   */
  deleteCluster(request: DeepPartial<DeleteClusterRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
  /**
   * Creates a Redis cluster based on the specified properties.
   * The creation is executed asynchronously and callers may check the returned
   * operation to track its progress. Once the operation is completed the Redis
   * cluster will be fully functional. The completed longrunning.Operation will
   * contain the new cluster object in the response field.
   *
   * The returned operation is automatically deleted after a few hours, so there
   * is no need to call DeleteOperation.
   */
  createCluster(request: DeepPartial<CreateClusterRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
  /** Gets the details of certificate authority information for Redis cluster. */
  getClusterCertificateAuthority(
    request: DeepPartial<GetClusterCertificateAuthorityRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<CertificateAuthority>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
