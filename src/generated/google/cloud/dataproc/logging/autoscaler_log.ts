// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/dataproc/logging/autoscaler_log.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Duration } from "../../../protobuf/duration.js";

export const protobufPackage = "google.cloud.dataproc.logging";

/** The Autoscaler state. */
export enum AutoscalerState {
  AUTOSCALER_STATE_UNSPECIFIED = 0,
  /** COOLDOWN - The Autoscaler is sleeping and waiting for the next update. */
  COOLDOWN = 1,
  /**
   * RECOMMENDING - The Autoscaler is in the process of calculating its recommendation on
   * whether to scale the cluster, and if so, how to autoscale.
   */
  RECOMMENDING = 6,
  /** SCALING - The Autoscaler is scaling the cluster. */
  SCALING = 2,
  /** STOPPED - The Autoscaler has stopped. */
  STOPPED = 3,
  /** FAILED - The Autoscaler has failed. */
  FAILED = 4,
  /** INITIALIZING - The Autoscaler is initializing. */
  INITIALIZING = 5,
  UNRECOGNIZED = -1,
}

export function autoscalerStateFromJSON(object: any): AutoscalerState {
  switch (object) {
    case 0:
    case "AUTOSCALER_STATE_UNSPECIFIED":
      return AutoscalerState.AUTOSCALER_STATE_UNSPECIFIED;
    case 1:
    case "COOLDOWN":
      return AutoscalerState.COOLDOWN;
    case 6:
    case "RECOMMENDING":
      return AutoscalerState.RECOMMENDING;
    case 2:
    case "SCALING":
      return AutoscalerState.SCALING;
    case 3:
    case "STOPPED":
      return AutoscalerState.STOPPED;
    case 4:
    case "FAILED":
      return AutoscalerState.FAILED;
    case 5:
    case "INITIALIZING":
      return AutoscalerState.INITIALIZING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AutoscalerState.UNRECOGNIZED;
  }
}

export function autoscalerStateToJSON(object: AutoscalerState): string {
  switch (object) {
    case AutoscalerState.AUTOSCALER_STATE_UNSPECIFIED:
      return "AUTOSCALER_STATE_UNSPECIFIED";
    case AutoscalerState.COOLDOWN:
      return "COOLDOWN";
    case AutoscalerState.RECOMMENDING:
      return "RECOMMENDING";
    case AutoscalerState.SCALING:
      return "SCALING";
    case AutoscalerState.STOPPED:
      return "STOPPED";
    case AutoscalerState.FAILED:
      return "FAILED";
    case AutoscalerState.INITIALIZING:
      return "INITIALIZING";
    case AutoscalerState.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The Autoscaling decision type. */
export enum ScalingDecisionType {
  SCALING_DECISION_TYPE_UNSPECIFIED = 0,
  /** SCALE_UP - Increase the number of primary and/or secondary workers. */
  SCALE_UP = 1,
  /** SCALE_DOWN - Decrease the number of primary and/or secondary workers. */
  SCALE_DOWN = 2,
  /** NO_SCALE - Not changing the number of primary or secondary workers. */
  NO_SCALE = 3,
  /** MIXED - Scale the primary and secondary worker groups in different directions. */
  MIXED = 4,
  /** CANCEL - Cancel the ongoing scale down operation. */
  CANCEL = 5,
  /** DO_NOT_CANCEL - Do not cancel the ongoing scale down operation. */
  DO_NOT_CANCEL = 6,
  UNRECOGNIZED = -1,
}

export function scalingDecisionTypeFromJSON(object: any): ScalingDecisionType {
  switch (object) {
    case 0:
    case "SCALING_DECISION_TYPE_UNSPECIFIED":
      return ScalingDecisionType.SCALING_DECISION_TYPE_UNSPECIFIED;
    case 1:
    case "SCALE_UP":
      return ScalingDecisionType.SCALE_UP;
    case 2:
    case "SCALE_DOWN":
      return ScalingDecisionType.SCALE_DOWN;
    case 3:
    case "NO_SCALE":
      return ScalingDecisionType.NO_SCALE;
    case 4:
    case "MIXED":
      return ScalingDecisionType.MIXED;
    case 5:
    case "CANCEL":
      return ScalingDecisionType.CANCEL;
    case 6:
    case "DO_NOT_CANCEL":
      return ScalingDecisionType.DO_NOT_CANCEL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ScalingDecisionType.UNRECOGNIZED;
  }
}

export function scalingDecisionTypeToJSON(object: ScalingDecisionType): string {
  switch (object) {
    case ScalingDecisionType.SCALING_DECISION_TYPE_UNSPECIFIED:
      return "SCALING_DECISION_TYPE_UNSPECIFIED";
    case ScalingDecisionType.SCALE_UP:
      return "SCALE_UP";
    case ScalingDecisionType.SCALE_DOWN:
      return "SCALE_DOWN";
    case ScalingDecisionType.NO_SCALE:
      return "NO_SCALE";
    case ScalingDecisionType.MIXED:
      return "MIXED";
    case ScalingDecisionType.CANCEL:
      return "CANCEL";
    case ScalingDecisionType.DO_NOT_CANCEL:
      return "DO_NOT_CANCEL";
    case ScalingDecisionType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export enum ConstrainingFactor {
  CONSTRAINING_FACTOR_UNSPECIFIED = 0,
  /**
   * SCALING_CAPPED_DUE_TO_LACK_OF_QUOTA - The project does not have sufficient regional, global, and or preemptible
   * quota to allocate a new VM.
   */
  SCALING_CAPPED_DUE_TO_LACK_OF_QUOTA = 1,
  /**
   * REACHED_MAXIMUM_CLUSTER_SIZE - All worker groups have reached maximum size. This message will not be
   * issued if one group reached maximum size, but workers were able to be
   * allocated to another group.
   */
  REACHED_MAXIMUM_CLUSTER_SIZE = 2,
  /**
   * REACHED_MINIMUM_CLUSTER_SIZE - All worker groups have reached minimum size. This message will not be
   * issued if workers were able to be removed from another group that had not
   * reached minimum size.
   */
  REACHED_MINIMUM_CLUSTER_SIZE = 3,
  /**
   * SECONDARY_SCALEDOWN_SINGLE_REQUEST_LIMIT_REACHED - The secondary worker group cannot be scaled down by more than 1k nodes in a
   * single update request.
   */
  SECONDARY_SCALEDOWN_SINGLE_REQUEST_LIMIT_REACHED = 4,
  UNRECOGNIZED = -1,
}

export function constrainingFactorFromJSON(object: any): ConstrainingFactor {
  switch (object) {
    case 0:
    case "CONSTRAINING_FACTOR_UNSPECIFIED":
      return ConstrainingFactor.CONSTRAINING_FACTOR_UNSPECIFIED;
    case 1:
    case "SCALING_CAPPED_DUE_TO_LACK_OF_QUOTA":
      return ConstrainingFactor.SCALING_CAPPED_DUE_TO_LACK_OF_QUOTA;
    case 2:
    case "REACHED_MAXIMUM_CLUSTER_SIZE":
      return ConstrainingFactor.REACHED_MAXIMUM_CLUSTER_SIZE;
    case 3:
    case "REACHED_MINIMUM_CLUSTER_SIZE":
      return ConstrainingFactor.REACHED_MINIMUM_CLUSTER_SIZE;
    case 4:
    case "SECONDARY_SCALEDOWN_SINGLE_REQUEST_LIMIT_REACHED":
      return ConstrainingFactor.SECONDARY_SCALEDOWN_SINGLE_REQUEST_LIMIT_REACHED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ConstrainingFactor.UNRECOGNIZED;
  }
}

export function constrainingFactorToJSON(object: ConstrainingFactor): string {
  switch (object) {
    case ConstrainingFactor.CONSTRAINING_FACTOR_UNSPECIFIED:
      return "CONSTRAINING_FACTOR_UNSPECIFIED";
    case ConstrainingFactor.SCALING_CAPPED_DUE_TO_LACK_OF_QUOTA:
      return "SCALING_CAPPED_DUE_TO_LACK_OF_QUOTA";
    case ConstrainingFactor.REACHED_MAXIMUM_CLUSTER_SIZE:
      return "REACHED_MAXIMUM_CLUSTER_SIZE";
    case ConstrainingFactor.REACHED_MINIMUM_CLUSTER_SIZE:
      return "REACHED_MINIMUM_CLUSTER_SIZE";
    case ConstrainingFactor.SECONDARY_SCALEDOWN_SINGLE_REQUEST_LIMIT_REACHED:
      return "SECONDARY_SCALEDOWN_SINGLE_REQUEST_LIMIT_REACHED";
    case ConstrainingFactor.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The kind of metric input to the Autoscaling algorithm. */
export enum MetricType {
  /** METRIC_TYPE_UNSPECIFIED - Default. */
  METRIC_TYPE_UNSPECIFIED = 0,
  /** YARN_MEMORY - The yarn memory metric. */
  YARN_MEMORY = 1,
  /** YARN_CORES - The yarn cores or vCPUs metric. */
  YARN_CORES = 2,
  /** SPARK_EXECUTORS - The number of executors in Spark serverless. */
  SPARK_EXECUTORS = 3,
  UNRECOGNIZED = -1,
}

export function metricTypeFromJSON(object: any): MetricType {
  switch (object) {
    case 0:
    case "METRIC_TYPE_UNSPECIFIED":
      return MetricType.METRIC_TYPE_UNSPECIFIED;
    case 1:
    case "YARN_MEMORY":
      return MetricType.YARN_MEMORY;
    case 2:
    case "YARN_CORES":
      return MetricType.YARN_CORES;
    case 3:
    case "SPARK_EXECUTORS":
      return MetricType.SPARK_EXECUTORS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return MetricType.UNRECOGNIZED;
  }
}

export function metricTypeToJSON(object: MetricType): string {
  switch (object) {
    case MetricType.METRIC_TYPE_UNSPECIFIED:
      return "METRIC_TYPE_UNSPECIFIED";
    case MetricType.YARN_MEMORY:
      return "YARN_MEMORY";
    case MetricType.YARN_CORES:
      return "YARN_CORES";
    case MetricType.SPARK_EXECUTORS:
      return "SPARK_EXECUTORS";
    case MetricType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The short version of cluster configuration for Cloud Logging. */
export interface ClusterSize {
  /** The number of primary workers in the cluster. */
  primaryWorkerCount: number;
  /** The number of secondary workers in the cluster. */
  secondaryWorkerCount: number;
}

/**
 * The main proto that will be converted to JSON format and then written to
 * Logging.
 */
export interface AutoscalerLog {
  /** The current Autoscaler status. */
  status:
    | AutoscalerStatus
    | undefined;
  /**
   * Optional. The autoscaling recommendation including its inputs, outputs,
   * scaling decision, and detailed explanation.
   */
  recommendation: AutoscalerRecommendation | undefined;
}

/** The Autoscaler's status, including its state and other details. */
export interface AutoscalerStatus {
  /** The high-level Autoscaler state. */
  state: AutoscalerState;
  /** The detailed description of Autoscaler status. */
  details: string;
  /** The cluster update operation ID. */
  updateClusterOperationId: string;
  /** Error message from an Autoscaler exception, if any. */
  error: string;
}

/**
 * The inputs, outputs, and detailed explanation of the Autoscaling
 * recommendation.
 */
export interface AutoscalerRecommendation {
  /** The autoscaling algorithm inputs. */
  inputs:
    | AutoscalerRecommendation_Inputs
    | undefined;
  /** The algorithm outputs for the recommended cluster size. */
  outputs: AutoscalerRecommendation_Outputs | undefined;
}

/** The input values for the Autoscaling recommendation algorithm. */
export interface AutoscalerRecommendation_Inputs {
  /**
   * The metrics collected by the Dataproc agent running on the cluster.
   * For example, {"avg-yarn-pending-memory": "1040 MB"}
   */
  clusterMetrics: { [key: string]: string };
  /** The cluster configuration before updating the cluster. */
  currentClusterSize:
    | ClusterSize
    | undefined;
  /** The minimum worker counts for each instance group. */
  minWorkerCounts:
    | ClusterSize
    | undefined;
  /** The maximum worker counts for each instance group. */
  maxWorkerCounts: ClusterSize | undefined;
}

export interface AutoscalerRecommendation_Inputs_ClusterMetricsEntry {
  key: string;
  value: string;
}

/** Autoscaler recommendations. */
export interface AutoscalerRecommendation_Outputs {
  /**
   * The high-level autoscaling decision, such as SCALE_UP, SCALE_DOWN,
   * NO_OP.
   */
  decision: ScalingDecisionType;
  /** The recommended cluster size. */
  recommendedClusterSize:
    | ClusterSize
    | undefined;
  /** The graceful decommission timeout for downscaling operations. */
  gracefulDecommissionTimeout:
    | Duration
    | undefined;
  /** Reasons why the Autoscaler didn't add or remove more workers. */
  constraintsReached: ConstrainingFactor[];
  /**
   * Less significant recommendations that are not included in the
   * `AutoscalerStatus.details` message.
   */
  additionalRecommendationDetails: string[];
  /**
   * A unique id for this recommendation that should be included when opening
   * a support ticket.
   */
  recommendationId: string;
  /** The metric source deciding the autoscaling recommendation. */
  decisionMetric: MetricType;
}

function createBaseClusterSize(): ClusterSize {
  return { primaryWorkerCount: 0, secondaryWorkerCount: 0 };
}

export const ClusterSize: MessageFns<ClusterSize> = {
  encode(message: ClusterSize, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.primaryWorkerCount !== 0) {
      writer.uint32(8).int32(message.primaryWorkerCount);
    }
    if (message.secondaryWorkerCount !== 0) {
      writer.uint32(16).int32(message.secondaryWorkerCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterSize {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterSize();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.primaryWorkerCount = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.secondaryWorkerCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterSize {
    return {
      primaryWorkerCount: isSet(object.primaryWorkerCount) ? globalThis.Number(object.primaryWorkerCount) : 0,
      secondaryWorkerCount: isSet(object.secondaryWorkerCount) ? globalThis.Number(object.secondaryWorkerCount) : 0,
    };
  },

  toJSON(message: ClusterSize): unknown {
    const obj: any = {};
    if (message.primaryWorkerCount !== 0) {
      obj.primaryWorkerCount = Math.round(message.primaryWorkerCount);
    }
    if (message.secondaryWorkerCount !== 0) {
      obj.secondaryWorkerCount = Math.round(message.secondaryWorkerCount);
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterSize>): ClusterSize {
    return ClusterSize.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterSize>): ClusterSize {
    const message = createBaseClusterSize();
    message.primaryWorkerCount = object.primaryWorkerCount ?? 0;
    message.secondaryWorkerCount = object.secondaryWorkerCount ?? 0;
    return message;
  },
};

function createBaseAutoscalerLog(): AutoscalerLog {
  return { status: undefined, recommendation: undefined };
}

export const AutoscalerLog: MessageFns<AutoscalerLog> = {
  encode(message: AutoscalerLog, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.status !== undefined) {
      AutoscalerStatus.encode(message.status, writer.uint32(10).fork()).join();
    }
    if (message.recommendation !== undefined) {
      AutoscalerRecommendation.encode(message.recommendation, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutoscalerLog {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutoscalerLog();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.status = AutoscalerStatus.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.recommendation = AutoscalerRecommendation.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutoscalerLog {
    return {
      status: isSet(object.status) ? AutoscalerStatus.fromJSON(object.status) : undefined,
      recommendation: isSet(object.recommendation)
        ? AutoscalerRecommendation.fromJSON(object.recommendation)
        : undefined,
    };
  },

  toJSON(message: AutoscalerLog): unknown {
    const obj: any = {};
    if (message.status !== undefined) {
      obj.status = AutoscalerStatus.toJSON(message.status);
    }
    if (message.recommendation !== undefined) {
      obj.recommendation = AutoscalerRecommendation.toJSON(message.recommendation);
    }
    return obj;
  },

  create(base?: DeepPartial<AutoscalerLog>): AutoscalerLog {
    return AutoscalerLog.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutoscalerLog>): AutoscalerLog {
    const message = createBaseAutoscalerLog();
    message.status = (object.status !== undefined && object.status !== null)
      ? AutoscalerStatus.fromPartial(object.status)
      : undefined;
    message.recommendation = (object.recommendation !== undefined && object.recommendation !== null)
      ? AutoscalerRecommendation.fromPartial(object.recommendation)
      : undefined;
    return message;
  },
};

function createBaseAutoscalerStatus(): AutoscalerStatus {
  return { state: 0, details: "", updateClusterOperationId: "", error: "" };
}

export const AutoscalerStatus: MessageFns<AutoscalerStatus> = {
  encode(message: AutoscalerStatus, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.state !== 0) {
      writer.uint32(8).int32(message.state);
    }
    if (message.details !== "") {
      writer.uint32(18).string(message.details);
    }
    if (message.updateClusterOperationId !== "") {
      writer.uint32(26).string(message.updateClusterOperationId);
    }
    if (message.error !== "") {
      writer.uint32(34).string(message.error);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutoscalerStatus {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutoscalerStatus();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.details = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.updateClusterOperationId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.error = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutoscalerStatus {
    return {
      state: isSet(object.state) ? autoscalerStateFromJSON(object.state) : 0,
      details: isSet(object.details) ? globalThis.String(object.details) : "",
      updateClusterOperationId: isSet(object.updateClusterOperationId)
        ? globalThis.String(object.updateClusterOperationId)
        : "",
      error: isSet(object.error) ? globalThis.String(object.error) : "",
    };
  },

  toJSON(message: AutoscalerStatus): unknown {
    const obj: any = {};
    if (message.state !== 0) {
      obj.state = autoscalerStateToJSON(message.state);
    }
    if (message.details !== "") {
      obj.details = message.details;
    }
    if (message.updateClusterOperationId !== "") {
      obj.updateClusterOperationId = message.updateClusterOperationId;
    }
    if (message.error !== "") {
      obj.error = message.error;
    }
    return obj;
  },

  create(base?: DeepPartial<AutoscalerStatus>): AutoscalerStatus {
    return AutoscalerStatus.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutoscalerStatus>): AutoscalerStatus {
    const message = createBaseAutoscalerStatus();
    message.state = object.state ?? 0;
    message.details = object.details ?? "";
    message.updateClusterOperationId = object.updateClusterOperationId ?? "";
    message.error = object.error ?? "";
    return message;
  },
};

function createBaseAutoscalerRecommendation(): AutoscalerRecommendation {
  return { inputs: undefined, outputs: undefined };
}

export const AutoscalerRecommendation: MessageFns<AutoscalerRecommendation> = {
  encode(message: AutoscalerRecommendation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputs !== undefined) {
      AutoscalerRecommendation_Inputs.encode(message.inputs, writer.uint32(10).fork()).join();
    }
    if (message.outputs !== undefined) {
      AutoscalerRecommendation_Outputs.encode(message.outputs, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutoscalerRecommendation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutoscalerRecommendation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputs = AutoscalerRecommendation_Inputs.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.outputs = AutoscalerRecommendation_Outputs.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutoscalerRecommendation {
    return {
      inputs: isSet(object.inputs) ? AutoscalerRecommendation_Inputs.fromJSON(object.inputs) : undefined,
      outputs: isSet(object.outputs) ? AutoscalerRecommendation_Outputs.fromJSON(object.outputs) : undefined,
    };
  },

  toJSON(message: AutoscalerRecommendation): unknown {
    const obj: any = {};
    if (message.inputs !== undefined) {
      obj.inputs = AutoscalerRecommendation_Inputs.toJSON(message.inputs);
    }
    if (message.outputs !== undefined) {
      obj.outputs = AutoscalerRecommendation_Outputs.toJSON(message.outputs);
    }
    return obj;
  },

  create(base?: DeepPartial<AutoscalerRecommendation>): AutoscalerRecommendation {
    return AutoscalerRecommendation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutoscalerRecommendation>): AutoscalerRecommendation {
    const message = createBaseAutoscalerRecommendation();
    message.inputs = (object.inputs !== undefined && object.inputs !== null)
      ? AutoscalerRecommendation_Inputs.fromPartial(object.inputs)
      : undefined;
    message.outputs = (object.outputs !== undefined && object.outputs !== null)
      ? AutoscalerRecommendation_Outputs.fromPartial(object.outputs)
      : undefined;
    return message;
  },
};

function createBaseAutoscalerRecommendation_Inputs(): AutoscalerRecommendation_Inputs {
  return { clusterMetrics: {}, currentClusterSize: undefined, minWorkerCounts: undefined, maxWorkerCounts: undefined };
}

export const AutoscalerRecommendation_Inputs: MessageFns<AutoscalerRecommendation_Inputs> = {
  encode(message: AutoscalerRecommendation_Inputs, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.clusterMetrics).forEach(([key, value]) => {
      AutoscalerRecommendation_Inputs_ClusterMetricsEntry.encode({ key: key as any, value }, writer.uint32(10).fork())
        .join();
    });
    if (message.currentClusterSize !== undefined) {
      ClusterSize.encode(message.currentClusterSize, writer.uint32(18).fork()).join();
    }
    if (message.minWorkerCounts !== undefined) {
      ClusterSize.encode(message.minWorkerCounts, writer.uint32(26).fork()).join();
    }
    if (message.maxWorkerCounts !== undefined) {
      ClusterSize.encode(message.maxWorkerCounts, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutoscalerRecommendation_Inputs {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutoscalerRecommendation_Inputs();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          const entry1 = AutoscalerRecommendation_Inputs_ClusterMetricsEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.clusterMetrics[entry1.key] = entry1.value;
          }
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.currentClusterSize = ClusterSize.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.minWorkerCounts = ClusterSize.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.maxWorkerCounts = ClusterSize.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutoscalerRecommendation_Inputs {
    return {
      clusterMetrics: isObject(object.clusterMetrics)
        ? Object.entries(object.clusterMetrics).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      currentClusterSize: isSet(object.currentClusterSize)
        ? ClusterSize.fromJSON(object.currentClusterSize)
        : undefined,
      minWorkerCounts: isSet(object.minWorkerCounts) ? ClusterSize.fromJSON(object.minWorkerCounts) : undefined,
      maxWorkerCounts: isSet(object.maxWorkerCounts) ? ClusterSize.fromJSON(object.maxWorkerCounts) : undefined,
    };
  },

  toJSON(message: AutoscalerRecommendation_Inputs): unknown {
    const obj: any = {};
    if (message.clusterMetrics) {
      const entries = Object.entries(message.clusterMetrics);
      if (entries.length > 0) {
        obj.clusterMetrics = {};
        entries.forEach(([k, v]) => {
          obj.clusterMetrics[k] = v;
        });
      }
    }
    if (message.currentClusterSize !== undefined) {
      obj.currentClusterSize = ClusterSize.toJSON(message.currentClusterSize);
    }
    if (message.minWorkerCounts !== undefined) {
      obj.minWorkerCounts = ClusterSize.toJSON(message.minWorkerCounts);
    }
    if (message.maxWorkerCounts !== undefined) {
      obj.maxWorkerCounts = ClusterSize.toJSON(message.maxWorkerCounts);
    }
    return obj;
  },

  create(base?: DeepPartial<AutoscalerRecommendation_Inputs>): AutoscalerRecommendation_Inputs {
    return AutoscalerRecommendation_Inputs.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutoscalerRecommendation_Inputs>): AutoscalerRecommendation_Inputs {
    const message = createBaseAutoscalerRecommendation_Inputs();
    message.clusterMetrics = Object.entries(object.clusterMetrics ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.currentClusterSize = (object.currentClusterSize !== undefined && object.currentClusterSize !== null)
      ? ClusterSize.fromPartial(object.currentClusterSize)
      : undefined;
    message.minWorkerCounts = (object.minWorkerCounts !== undefined && object.minWorkerCounts !== null)
      ? ClusterSize.fromPartial(object.minWorkerCounts)
      : undefined;
    message.maxWorkerCounts = (object.maxWorkerCounts !== undefined && object.maxWorkerCounts !== null)
      ? ClusterSize.fromPartial(object.maxWorkerCounts)
      : undefined;
    return message;
  },
};

function createBaseAutoscalerRecommendation_Inputs_ClusterMetricsEntry(): AutoscalerRecommendation_Inputs_ClusterMetricsEntry {
  return { key: "", value: "" };
}

export const AutoscalerRecommendation_Inputs_ClusterMetricsEntry: MessageFns<
  AutoscalerRecommendation_Inputs_ClusterMetricsEntry
> = {
  encode(
    message: AutoscalerRecommendation_Inputs_ClusterMetricsEntry,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutoscalerRecommendation_Inputs_ClusterMetricsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutoscalerRecommendation_Inputs_ClusterMetricsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutoscalerRecommendation_Inputs_ClusterMetricsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: AutoscalerRecommendation_Inputs_ClusterMetricsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(
    base?: DeepPartial<AutoscalerRecommendation_Inputs_ClusterMetricsEntry>,
  ): AutoscalerRecommendation_Inputs_ClusterMetricsEntry {
    return AutoscalerRecommendation_Inputs_ClusterMetricsEntry.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<AutoscalerRecommendation_Inputs_ClusterMetricsEntry>,
  ): AutoscalerRecommendation_Inputs_ClusterMetricsEntry {
    const message = createBaseAutoscalerRecommendation_Inputs_ClusterMetricsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseAutoscalerRecommendation_Outputs(): AutoscalerRecommendation_Outputs {
  return {
    decision: 0,
    recommendedClusterSize: undefined,
    gracefulDecommissionTimeout: undefined,
    constraintsReached: [],
    additionalRecommendationDetails: [],
    recommendationId: "",
    decisionMetric: 0,
  };
}

export const AutoscalerRecommendation_Outputs: MessageFns<AutoscalerRecommendation_Outputs> = {
  encode(message: AutoscalerRecommendation_Outputs, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.decision !== 0) {
      writer.uint32(8).int32(message.decision);
    }
    if (message.recommendedClusterSize !== undefined) {
      ClusterSize.encode(message.recommendedClusterSize, writer.uint32(18).fork()).join();
    }
    if (message.gracefulDecommissionTimeout !== undefined) {
      Duration.encode(message.gracefulDecommissionTimeout, writer.uint32(26).fork()).join();
    }
    writer.uint32(34).fork();
    for (const v of message.constraintsReached) {
      writer.int32(v);
    }
    writer.join();
    for (const v of message.additionalRecommendationDetails) {
      writer.uint32(42).string(v!);
    }
    if (message.recommendationId !== "") {
      writer.uint32(50).string(message.recommendationId);
    }
    if (message.decisionMetric !== 0) {
      writer.uint32(56).int32(message.decisionMetric);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutoscalerRecommendation_Outputs {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutoscalerRecommendation_Outputs();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.decision = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.recommendedClusterSize = ClusterSize.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.gracefulDecommissionTimeout = Duration.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag === 32) {
            message.constraintsReached.push(reader.int32() as any);

            continue;
          }

          if (tag === 34) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.constraintsReached.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.additionalRecommendationDetails.push(reader.string());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.recommendationId = reader.string();
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.decisionMetric = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutoscalerRecommendation_Outputs {
    return {
      decision: isSet(object.decision) ? scalingDecisionTypeFromJSON(object.decision) : 0,
      recommendedClusterSize: isSet(object.recommendedClusterSize)
        ? ClusterSize.fromJSON(object.recommendedClusterSize)
        : undefined,
      gracefulDecommissionTimeout: isSet(object.gracefulDecommissionTimeout)
        ? Duration.fromJSON(object.gracefulDecommissionTimeout)
        : undefined,
      constraintsReached: globalThis.Array.isArray(object?.constraintsReached)
        ? object.constraintsReached.map((e: any) => constrainingFactorFromJSON(e))
        : [],
      additionalRecommendationDetails: globalThis.Array.isArray(object?.additionalRecommendationDetails)
        ? object.additionalRecommendationDetails.map((e: any) => globalThis.String(e))
        : [],
      recommendationId: isSet(object.recommendationId) ? globalThis.String(object.recommendationId) : "",
      decisionMetric: isSet(object.decisionMetric) ? metricTypeFromJSON(object.decisionMetric) : 0,
    };
  },

  toJSON(message: AutoscalerRecommendation_Outputs): unknown {
    const obj: any = {};
    if (message.decision !== 0) {
      obj.decision = scalingDecisionTypeToJSON(message.decision);
    }
    if (message.recommendedClusterSize !== undefined) {
      obj.recommendedClusterSize = ClusterSize.toJSON(message.recommendedClusterSize);
    }
    if (message.gracefulDecommissionTimeout !== undefined) {
      obj.gracefulDecommissionTimeout = Duration.toJSON(message.gracefulDecommissionTimeout);
    }
    if (message.constraintsReached?.length) {
      obj.constraintsReached = message.constraintsReached.map((e) => constrainingFactorToJSON(e));
    }
    if (message.additionalRecommendationDetails?.length) {
      obj.additionalRecommendationDetails = message.additionalRecommendationDetails;
    }
    if (message.recommendationId !== "") {
      obj.recommendationId = message.recommendationId;
    }
    if (message.decisionMetric !== 0) {
      obj.decisionMetric = metricTypeToJSON(message.decisionMetric);
    }
    return obj;
  },

  create(base?: DeepPartial<AutoscalerRecommendation_Outputs>): AutoscalerRecommendation_Outputs {
    return AutoscalerRecommendation_Outputs.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutoscalerRecommendation_Outputs>): AutoscalerRecommendation_Outputs {
    const message = createBaseAutoscalerRecommendation_Outputs();
    message.decision = object.decision ?? 0;
    message.recommendedClusterSize =
      (object.recommendedClusterSize !== undefined && object.recommendedClusterSize !== null)
        ? ClusterSize.fromPartial(object.recommendedClusterSize)
        : undefined;
    message.gracefulDecommissionTimeout =
      (object.gracefulDecommissionTimeout !== undefined && object.gracefulDecommissionTimeout !== null)
        ? Duration.fromPartial(object.gracefulDecommissionTimeout)
        : undefined;
    message.constraintsReached = object.constraintsReached?.map((e) => e) || [];
    message.additionalRecommendationDetails = object.additionalRecommendationDetails?.map((e) => e) || [];
    message.recommendationId = object.recommendationId ?? "";
    message.decisionMetric = object.decisionMetric ?? 0;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
