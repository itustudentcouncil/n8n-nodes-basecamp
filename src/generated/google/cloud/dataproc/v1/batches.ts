// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/dataproc/v1/batches.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Operation } from "../../../longrunning/operations.js";
import { Empty } from "../../../protobuf/empty.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { EnvironmentConfig, RuntimeConfig, RuntimeInfo } from "./shared.js";

export const protobufPackage = "google.cloud.dataproc.v1";

/** A request to create a batch workload. */
export interface CreateBatchRequest {
  /** Required. The parent resource where this batch will be created. */
  parent: string;
  /** Required. The batch to create. */
  batch:
    | Batch
    | undefined;
  /**
   * Optional. The ID to use for the batch, which will become the final
   * component of the batch's resource name.
   *
   * This value must be 4-63 characters. Valid characters are `/[a-z][0-9]-/`.
   */
  batchId: string;
  /**
   * Optional. A unique ID used to identify the request. If the service
   * receives two
   * [CreateBatchRequest](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.CreateBatchRequest)s
   * with the same request_id, the second request is ignored and the
   * Operation that corresponds to the first Batch created and stored
   * in the backend is returned.
   *
   * Recommendation: Set this value to a
   * [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).
   *
   * The value must contain only letters (a-z, A-Z), numbers (0-9),
   * underscores (_), and hyphens (-). The maximum length is 40 characters.
   */
  requestId: string;
}

/** A request to get the resource representation for a batch workload. */
export interface GetBatchRequest {
  /**
   * Required. The fully qualified name of the batch to retrieve
   * in the format
   * "projects/PROJECT_ID/locations/DATAPROC_REGION/batches/BATCH_ID"
   */
  name: string;
}

/** A request to list batch workloads in a project. */
export interface ListBatchesRequest {
  /** Required. The parent, which owns this collection of batches. */
  parent: string;
  /**
   * Optional. The maximum number of batches to return in each response.
   * The service may return fewer than this value.
   * The default page size is 20; the maximum page size is 1000.
   */
  pageSize: number;
  /**
   * Optional. A page token received from a previous `ListBatches` call.
   * Provide this token to retrieve the subsequent page.
   */
  pageToken: string;
  /**
   * Optional. A filter for the batches to return in the response.
   *
   * A filter is a logical expression constraining the values of various fields
   * in each batch resource. Filters are case sensitive, and may contain
   * multiple clauses combined with logical operators (AND/OR).
   * Supported fields are `batch_id`, `batch_uuid`, `state`, and `create_time`.
   *
   * e.g. `state = RUNNING and create_time < "2023-01-01T00:00:00Z"`
   * filters for batches in state RUNNING that were created before 2023-01-01
   *
   * See https://google.aip.dev/assets/misc/ebnf-filtering.txt for a detailed
   * description of the filter syntax and a list of supported comparisons.
   */
  filter: string;
  /**
   * Optional. Field(s) on which to sort the list of batches.
   *
   * Currently the only supported sort orders are unspecified (empty) and
   * `create_time desc` to sort by most recently created batches first.
   *
   * See https://google.aip.dev/132#ordering for more details.
   */
  orderBy: string;
}

/** A list of batch workloads. */
export interface ListBatchesResponse {
  /** The batches from the specified collection. */
  batches: Batch[];
  /**
   * A token, which can be sent as `page_token` to retrieve the next page.
   * If this field is omitted, there are no subsequent pages.
   */
  nextPageToken: string;
  /**
   * Output only. List of Batches that could not be included in the response.
   * Attempting to get one of these resources may indicate why it was not
   * included in the list response.
   */
  unreachable: string[];
}

/** A request to delete a batch workload. */
export interface DeleteBatchRequest {
  /**
   * Required. The fully qualified name of the batch to retrieve
   * in the format
   * "projects/PROJECT_ID/locations/DATAPROC_REGION/batches/BATCH_ID"
   */
  name: string;
}

/** A representation of a batch workload in the service. */
export interface Batch {
  /** Output only. The resource name of the batch. */
  name: string;
  /**
   * Output only. A batch UUID (Unique Universal Identifier). The service
   * generates this value when it creates the batch.
   */
  uuid: string;
  /** Output only. The time when the batch was created. */
  createTime:
    | Date
    | undefined;
  /** Optional. PySpark batch config. */
  pysparkBatch?:
    | PySparkBatch
    | undefined;
  /** Optional. Spark batch config. */
  sparkBatch?:
    | SparkBatch
    | undefined;
  /** Optional. SparkR batch config. */
  sparkRBatch?:
    | SparkRBatch
    | undefined;
  /** Optional. SparkSql batch config. */
  sparkSqlBatch?:
    | SparkSqlBatch
    | undefined;
  /** Output only. Runtime information about batch execution. */
  runtimeInfo:
    | RuntimeInfo
    | undefined;
  /** Output only. The state of the batch. */
  state: Batch_State;
  /**
   * Output only. Batch state details, such as a failure
   * description if the state is `FAILED`.
   */
  stateMessage: string;
  /** Output only. The time when the batch entered a current state. */
  stateTime:
    | Date
    | undefined;
  /** Output only. The email address of the user who created the batch. */
  creator: string;
  /**
   * Optional. The labels to associate with this batch.
   * Label **keys** must contain 1 to 63 characters, and must conform to
   * [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).
   * Label **values** may be empty, but, if present, must contain 1 to 63
   * characters, and must conform to [RFC
   * 1035](https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be
   * associated with a batch.
   */
  labels: { [key: string]: string };
  /** Optional. Runtime configuration for the batch execution. */
  runtimeConfig:
    | RuntimeConfig
    | undefined;
  /** Optional. Environment configuration for the batch execution. */
  environmentConfig:
    | EnvironmentConfig
    | undefined;
  /** Output only. The resource name of the operation associated with this batch. */
  operation: string;
  /** Output only. Historical state information for the batch. */
  stateHistory: Batch_StateHistory[];
}

/** The batch state. */
export enum Batch_State {
  /** STATE_UNSPECIFIED - The batch state is unknown. */
  STATE_UNSPECIFIED = 0,
  /** PENDING - The batch is created before running. */
  PENDING = 1,
  /** RUNNING - The batch is running. */
  RUNNING = 2,
  /** CANCELLING - The batch is cancelling. */
  CANCELLING = 3,
  /** CANCELLED - The batch cancellation was successful. */
  CANCELLED = 4,
  /** SUCCEEDED - The batch completed successfully. */
  SUCCEEDED = 5,
  /** FAILED - The batch is no longer running due to an error. */
  FAILED = 6,
  UNRECOGNIZED = -1,
}

export function batch_StateFromJSON(object: any): Batch_State {
  switch (object) {
    case 0:
    case "STATE_UNSPECIFIED":
      return Batch_State.STATE_UNSPECIFIED;
    case 1:
    case "PENDING":
      return Batch_State.PENDING;
    case 2:
    case "RUNNING":
      return Batch_State.RUNNING;
    case 3:
    case "CANCELLING":
      return Batch_State.CANCELLING;
    case 4:
    case "CANCELLED":
      return Batch_State.CANCELLED;
    case 5:
    case "SUCCEEDED":
      return Batch_State.SUCCEEDED;
    case 6:
    case "FAILED":
      return Batch_State.FAILED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Batch_State.UNRECOGNIZED;
  }
}

export function batch_StateToJSON(object: Batch_State): string {
  switch (object) {
    case Batch_State.STATE_UNSPECIFIED:
      return "STATE_UNSPECIFIED";
    case Batch_State.PENDING:
      return "PENDING";
    case Batch_State.RUNNING:
      return "RUNNING";
    case Batch_State.CANCELLING:
      return "CANCELLING";
    case Batch_State.CANCELLED:
      return "CANCELLED";
    case Batch_State.SUCCEEDED:
      return "SUCCEEDED";
    case Batch_State.FAILED:
      return "FAILED";
    case Batch_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Historical state information. */
export interface Batch_StateHistory {
  /** Output only. The state of the batch at this point in history. */
  state: Batch_State;
  /** Output only. Details about the state at this point in history. */
  stateMessage: string;
  /** Output only. The time when the batch entered the historical state. */
  stateStartTime: Date | undefined;
}

export interface Batch_LabelsEntry {
  key: string;
  value: string;
}

/**
 * A configuration for running an
 * [Apache
 * PySpark](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html)
 * batch workload.
 */
export interface PySparkBatch {
  /**
   * Required. The HCFS URI of the main Python file to use as the Spark driver.
   * Must be a .py file.
   */
  mainPythonFileUri: string;
  /**
   * Optional. The arguments to pass to the driver. Do not include arguments
   * that can be set as batch properties, such as `--conf`, since a collision
   * can occur that causes an incorrect batch submission.
   */
  args: string[];
  /**
   * Optional. HCFS file URIs of Python files to pass to the PySpark
   * framework. Supported file types: `.py`, `.egg`, and `.zip`.
   */
  pythonFileUris: string[];
  /**
   * Optional. HCFS URIs of jar files to add to the classpath of the
   * Spark driver and tasks.
   */
  jarFileUris: string[];
  /**
   * Optional. HCFS URIs of files to be placed in the working directory of
   * each executor.
   */
  fileUris: string[];
  /**
   * Optional. HCFS URIs of archives to be extracted into the working directory
   * of each executor. Supported file types:
   * `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.
   */
  archiveUris: string[];
}

/**
 * A configuration for running an [Apache Spark](https://spark.apache.org/)
 * batch workload.
 */
export interface SparkBatch {
  /** Optional. The HCFS URI of the jar file that contains the main class. */
  mainJarFileUri?:
    | string
    | undefined;
  /**
   * Optional. The name of the driver main class. The jar file that contains
   * the class must be in the classpath or specified in `jar_file_uris`.
   */
  mainClass?:
    | string
    | undefined;
  /**
   * Optional. The arguments to pass to the driver. Do not include arguments
   * that can be set as batch properties, such as `--conf`, since a collision
   * can occur that causes an incorrect batch submission.
   */
  args: string[];
  /**
   * Optional. HCFS URIs of jar files to add to the classpath of the
   * Spark driver and tasks.
   */
  jarFileUris: string[];
  /**
   * Optional. HCFS URIs of files to be placed in the working directory of
   * each executor.
   */
  fileUris: string[];
  /**
   * Optional. HCFS URIs of archives to be extracted into the working directory
   * of each executor. Supported file types:
   * `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.
   */
  archiveUris: string[];
}

/**
 * A configuration for running an
 * [Apache SparkR](https://spark.apache.org/docs/latest/sparkr.html)
 * batch workload.
 */
export interface SparkRBatch {
  /**
   * Required. The HCFS URI of the main R file to use as the driver.
   * Must be a `.R` or `.r` file.
   */
  mainRFileUri: string;
  /**
   * Optional. The arguments to pass to the Spark driver. Do not include
   * arguments that can be set as batch properties, such as `--conf`, since a
   * collision can occur that causes an incorrect batch submission.
   */
  args: string[];
  /**
   * Optional. HCFS URIs of files to be placed in the working directory of
   * each executor.
   */
  fileUris: string[];
  /**
   * Optional. HCFS URIs of archives to be extracted into the working directory
   * of each executor. Supported file types:
   * `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.
   */
  archiveUris: string[];
}

/**
 * A configuration for running
 * [Apache Spark SQL](https://spark.apache.org/sql/) queries as a batch
 * workload.
 */
export interface SparkSqlBatch {
  /**
   * Required. The HCFS URI of the script that contains Spark SQL queries to
   * execute.
   */
  queryFileUri: string;
  /**
   * Optional. Mapping of query variable names to values (equivalent to the
   * Spark SQL command: `SET name="value";`).
   */
  queryVariables: { [key: string]: string };
  /** Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH. */
  jarFileUris: string[];
}

export interface SparkSqlBatch_QueryVariablesEntry {
  key: string;
  value: string;
}

function createBaseCreateBatchRequest(): CreateBatchRequest {
  return { parent: "", batch: undefined, batchId: "", requestId: "" };
}

export const CreateBatchRequest: MessageFns<CreateBatchRequest> = {
  encode(message: CreateBatchRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.batch !== undefined) {
      Batch.encode(message.batch, writer.uint32(18).fork()).join();
    }
    if (message.batchId !== "") {
      writer.uint32(26).string(message.batchId);
    }
    if (message.requestId !== "") {
      writer.uint32(34).string(message.requestId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateBatchRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateBatchRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.batch = Batch.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.batchId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.requestId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateBatchRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      batch: isSet(object.batch) ? Batch.fromJSON(object.batch) : undefined,
      batchId: isSet(object.batchId) ? globalThis.String(object.batchId) : "",
      requestId: isSet(object.requestId) ? globalThis.String(object.requestId) : "",
    };
  },

  toJSON(message: CreateBatchRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.batch !== undefined) {
      obj.batch = Batch.toJSON(message.batch);
    }
    if (message.batchId !== "") {
      obj.batchId = message.batchId;
    }
    if (message.requestId !== "") {
      obj.requestId = message.requestId;
    }
    return obj;
  },

  create(base?: DeepPartial<CreateBatchRequest>): CreateBatchRequest {
    return CreateBatchRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateBatchRequest>): CreateBatchRequest {
    const message = createBaseCreateBatchRequest();
    message.parent = object.parent ?? "";
    message.batch = (object.batch !== undefined && object.batch !== null) ? Batch.fromPartial(object.batch) : undefined;
    message.batchId = object.batchId ?? "";
    message.requestId = object.requestId ?? "";
    return message;
  },
};

function createBaseGetBatchRequest(): GetBatchRequest {
  return { name: "" };
}

export const GetBatchRequest: MessageFns<GetBatchRequest> = {
  encode(message: GetBatchRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetBatchRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetBatchRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetBatchRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: GetBatchRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<GetBatchRequest>): GetBatchRequest {
    return GetBatchRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetBatchRequest>): GetBatchRequest {
    const message = createBaseGetBatchRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseListBatchesRequest(): ListBatchesRequest {
  return { parent: "", pageSize: 0, pageToken: "", filter: "", orderBy: "" };
}

export const ListBatchesRequest: MessageFns<ListBatchesRequest> = {
  encode(message: ListBatchesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.pageSize !== 0) {
      writer.uint32(16).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(26).string(message.pageToken);
    }
    if (message.filter !== "") {
      writer.uint32(34).string(message.filter);
    }
    if (message.orderBy !== "") {
      writer.uint32(42).string(message.orderBy);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListBatchesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListBatchesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pageToken = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.filter = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.orderBy = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListBatchesRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
      filter: isSet(object.filter) ? globalThis.String(object.filter) : "",
      orderBy: isSet(object.orderBy) ? globalThis.String(object.orderBy) : "",
    };
  },

  toJSON(message: ListBatchesRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    if (message.filter !== "") {
      obj.filter = message.filter;
    }
    if (message.orderBy !== "") {
      obj.orderBy = message.orderBy;
    }
    return obj;
  },

  create(base?: DeepPartial<ListBatchesRequest>): ListBatchesRequest {
    return ListBatchesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListBatchesRequest>): ListBatchesRequest {
    const message = createBaseListBatchesRequest();
    message.parent = object.parent ?? "";
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    message.filter = object.filter ?? "";
    message.orderBy = object.orderBy ?? "";
    return message;
  },
};

function createBaseListBatchesResponse(): ListBatchesResponse {
  return { batches: [], nextPageToken: "", unreachable: [] };
}

export const ListBatchesResponse: MessageFns<ListBatchesResponse> = {
  encode(message: ListBatchesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.batches) {
      Batch.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    for (const v of message.unreachable) {
      writer.uint32(26).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListBatchesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListBatchesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.batches.push(Batch.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.unreachable.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListBatchesResponse {
    return {
      batches: globalThis.Array.isArray(object?.batches) ? object.batches.map((e: any) => Batch.fromJSON(e)) : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
      unreachable: globalThis.Array.isArray(object?.unreachable)
        ? object.unreachable.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: ListBatchesResponse): unknown {
    const obj: any = {};
    if (message.batches?.length) {
      obj.batches = message.batches.map((e) => Batch.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    if (message.unreachable?.length) {
      obj.unreachable = message.unreachable;
    }
    return obj;
  },

  create(base?: DeepPartial<ListBatchesResponse>): ListBatchesResponse {
    return ListBatchesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListBatchesResponse>): ListBatchesResponse {
    const message = createBaseListBatchesResponse();
    message.batches = object.batches?.map((e) => Batch.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    message.unreachable = object.unreachable?.map((e) => e) || [];
    return message;
  },
};

function createBaseDeleteBatchRequest(): DeleteBatchRequest {
  return { name: "" };
}

export const DeleteBatchRequest: MessageFns<DeleteBatchRequest> = {
  encode(message: DeleteBatchRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeleteBatchRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeleteBatchRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeleteBatchRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: DeleteBatchRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<DeleteBatchRequest>): DeleteBatchRequest {
    return DeleteBatchRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeleteBatchRequest>): DeleteBatchRequest {
    const message = createBaseDeleteBatchRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseBatch(): Batch {
  return {
    name: "",
    uuid: "",
    createTime: undefined,
    pysparkBatch: undefined,
    sparkBatch: undefined,
    sparkRBatch: undefined,
    sparkSqlBatch: undefined,
    runtimeInfo: undefined,
    state: 0,
    stateMessage: "",
    stateTime: undefined,
    creator: "",
    labels: {},
    runtimeConfig: undefined,
    environmentConfig: undefined,
    operation: "",
    stateHistory: [],
  };
}

export const Batch: MessageFns<Batch> = {
  encode(message: Batch, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.uuid !== "") {
      writer.uint32(18).string(message.uuid);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(26).fork()).join();
    }
    if (message.pysparkBatch !== undefined) {
      PySparkBatch.encode(message.pysparkBatch, writer.uint32(34).fork()).join();
    }
    if (message.sparkBatch !== undefined) {
      SparkBatch.encode(message.sparkBatch, writer.uint32(42).fork()).join();
    }
    if (message.sparkRBatch !== undefined) {
      SparkRBatch.encode(message.sparkRBatch, writer.uint32(50).fork()).join();
    }
    if (message.sparkSqlBatch !== undefined) {
      SparkSqlBatch.encode(message.sparkSqlBatch, writer.uint32(58).fork()).join();
    }
    if (message.runtimeInfo !== undefined) {
      RuntimeInfo.encode(message.runtimeInfo, writer.uint32(66).fork()).join();
    }
    if (message.state !== 0) {
      writer.uint32(72).int32(message.state);
    }
    if (message.stateMessage !== "") {
      writer.uint32(82).string(message.stateMessage);
    }
    if (message.stateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.stateTime), writer.uint32(90).fork()).join();
    }
    if (message.creator !== "") {
      writer.uint32(98).string(message.creator);
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      Batch_LabelsEntry.encode({ key: key as any, value }, writer.uint32(106).fork()).join();
    });
    if (message.runtimeConfig !== undefined) {
      RuntimeConfig.encode(message.runtimeConfig, writer.uint32(114).fork()).join();
    }
    if (message.environmentConfig !== undefined) {
      EnvironmentConfig.encode(message.environmentConfig, writer.uint32(122).fork()).join();
    }
    if (message.operation !== "") {
      writer.uint32(130).string(message.operation);
    }
    for (const v of message.stateHistory) {
      Batch_StateHistory.encode(v!, writer.uint32(138).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Batch {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatch();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.uuid = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.pysparkBatch = PySparkBatch.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.sparkBatch = SparkBatch.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.sparkRBatch = SparkRBatch.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.sparkSqlBatch = SparkSqlBatch.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.runtimeInfo = RuntimeInfo.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.stateMessage = reader.string();
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.stateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.creator = reader.string();
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          const entry13 = Batch_LabelsEntry.decode(reader, reader.uint32());
          if (entry13.value !== undefined) {
            message.labels[entry13.key] = entry13.value;
          }
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.runtimeConfig = RuntimeConfig.decode(reader, reader.uint32());
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.environmentConfig = EnvironmentConfig.decode(reader, reader.uint32());
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.operation = reader.string();
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.stateHistory.push(Batch_StateHistory.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Batch {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      uuid: isSet(object.uuid) ? globalThis.String(object.uuid) : "",
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      pysparkBatch: isSet(object.pysparkBatch) ? PySparkBatch.fromJSON(object.pysparkBatch) : undefined,
      sparkBatch: isSet(object.sparkBatch) ? SparkBatch.fromJSON(object.sparkBatch) : undefined,
      sparkRBatch: isSet(object.sparkRBatch) ? SparkRBatch.fromJSON(object.sparkRBatch) : undefined,
      sparkSqlBatch: isSet(object.sparkSqlBatch) ? SparkSqlBatch.fromJSON(object.sparkSqlBatch) : undefined,
      runtimeInfo: isSet(object.runtimeInfo) ? RuntimeInfo.fromJSON(object.runtimeInfo) : undefined,
      state: isSet(object.state) ? batch_StateFromJSON(object.state) : 0,
      stateMessage: isSet(object.stateMessage) ? globalThis.String(object.stateMessage) : "",
      stateTime: isSet(object.stateTime) ? fromJsonTimestamp(object.stateTime) : undefined,
      creator: isSet(object.creator) ? globalThis.String(object.creator) : "",
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      runtimeConfig: isSet(object.runtimeConfig) ? RuntimeConfig.fromJSON(object.runtimeConfig) : undefined,
      environmentConfig: isSet(object.environmentConfig)
        ? EnvironmentConfig.fromJSON(object.environmentConfig)
        : undefined,
      operation: isSet(object.operation) ? globalThis.String(object.operation) : "",
      stateHistory: globalThis.Array.isArray(object?.stateHistory)
        ? object.stateHistory.map((e: any) => Batch_StateHistory.fromJSON(e))
        : [],
    };
  },

  toJSON(message: Batch): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.uuid !== "") {
      obj.uuid = message.uuid;
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.pysparkBatch !== undefined) {
      obj.pysparkBatch = PySparkBatch.toJSON(message.pysparkBatch);
    }
    if (message.sparkBatch !== undefined) {
      obj.sparkBatch = SparkBatch.toJSON(message.sparkBatch);
    }
    if (message.sparkRBatch !== undefined) {
      obj.sparkRBatch = SparkRBatch.toJSON(message.sparkRBatch);
    }
    if (message.sparkSqlBatch !== undefined) {
      obj.sparkSqlBatch = SparkSqlBatch.toJSON(message.sparkSqlBatch);
    }
    if (message.runtimeInfo !== undefined) {
      obj.runtimeInfo = RuntimeInfo.toJSON(message.runtimeInfo);
    }
    if (message.state !== 0) {
      obj.state = batch_StateToJSON(message.state);
    }
    if (message.stateMessage !== "") {
      obj.stateMessage = message.stateMessage;
    }
    if (message.stateTime !== undefined) {
      obj.stateTime = message.stateTime.toISOString();
    }
    if (message.creator !== "") {
      obj.creator = message.creator;
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.runtimeConfig !== undefined) {
      obj.runtimeConfig = RuntimeConfig.toJSON(message.runtimeConfig);
    }
    if (message.environmentConfig !== undefined) {
      obj.environmentConfig = EnvironmentConfig.toJSON(message.environmentConfig);
    }
    if (message.operation !== "") {
      obj.operation = message.operation;
    }
    if (message.stateHistory?.length) {
      obj.stateHistory = message.stateHistory.map((e) => Batch_StateHistory.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<Batch>): Batch {
    return Batch.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Batch>): Batch {
    const message = createBaseBatch();
    message.name = object.name ?? "";
    message.uuid = object.uuid ?? "";
    message.createTime = object.createTime ?? undefined;
    message.pysparkBatch = (object.pysparkBatch !== undefined && object.pysparkBatch !== null)
      ? PySparkBatch.fromPartial(object.pysparkBatch)
      : undefined;
    message.sparkBatch = (object.sparkBatch !== undefined && object.sparkBatch !== null)
      ? SparkBatch.fromPartial(object.sparkBatch)
      : undefined;
    message.sparkRBatch = (object.sparkRBatch !== undefined && object.sparkRBatch !== null)
      ? SparkRBatch.fromPartial(object.sparkRBatch)
      : undefined;
    message.sparkSqlBatch = (object.sparkSqlBatch !== undefined && object.sparkSqlBatch !== null)
      ? SparkSqlBatch.fromPartial(object.sparkSqlBatch)
      : undefined;
    message.runtimeInfo = (object.runtimeInfo !== undefined && object.runtimeInfo !== null)
      ? RuntimeInfo.fromPartial(object.runtimeInfo)
      : undefined;
    message.state = object.state ?? 0;
    message.stateMessage = object.stateMessage ?? "";
    message.stateTime = object.stateTime ?? undefined;
    message.creator = object.creator ?? "";
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.runtimeConfig = (object.runtimeConfig !== undefined && object.runtimeConfig !== null)
      ? RuntimeConfig.fromPartial(object.runtimeConfig)
      : undefined;
    message.environmentConfig = (object.environmentConfig !== undefined && object.environmentConfig !== null)
      ? EnvironmentConfig.fromPartial(object.environmentConfig)
      : undefined;
    message.operation = object.operation ?? "";
    message.stateHistory = object.stateHistory?.map((e) => Batch_StateHistory.fromPartial(e)) || [];
    return message;
  },
};

function createBaseBatch_StateHistory(): Batch_StateHistory {
  return { state: 0, stateMessage: "", stateStartTime: undefined };
}

export const Batch_StateHistory: MessageFns<Batch_StateHistory> = {
  encode(message: Batch_StateHistory, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.state !== 0) {
      writer.uint32(8).int32(message.state);
    }
    if (message.stateMessage !== "") {
      writer.uint32(18).string(message.stateMessage);
    }
    if (message.stateStartTime !== undefined) {
      Timestamp.encode(toTimestamp(message.stateStartTime), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Batch_StateHistory {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatch_StateHistory();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.stateMessage = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.stateStartTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Batch_StateHistory {
    return {
      state: isSet(object.state) ? batch_StateFromJSON(object.state) : 0,
      stateMessage: isSet(object.stateMessage) ? globalThis.String(object.stateMessage) : "",
      stateStartTime: isSet(object.stateStartTime) ? fromJsonTimestamp(object.stateStartTime) : undefined,
    };
  },

  toJSON(message: Batch_StateHistory): unknown {
    const obj: any = {};
    if (message.state !== 0) {
      obj.state = batch_StateToJSON(message.state);
    }
    if (message.stateMessage !== "") {
      obj.stateMessage = message.stateMessage;
    }
    if (message.stateStartTime !== undefined) {
      obj.stateStartTime = message.stateStartTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<Batch_StateHistory>): Batch_StateHistory {
    return Batch_StateHistory.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Batch_StateHistory>): Batch_StateHistory {
    const message = createBaseBatch_StateHistory();
    message.state = object.state ?? 0;
    message.stateMessage = object.stateMessage ?? "";
    message.stateStartTime = object.stateStartTime ?? undefined;
    return message;
  },
};

function createBaseBatch_LabelsEntry(): Batch_LabelsEntry {
  return { key: "", value: "" };
}

export const Batch_LabelsEntry: MessageFns<Batch_LabelsEntry> = {
  encode(message: Batch_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Batch_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatch_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Batch_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Batch_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Batch_LabelsEntry>): Batch_LabelsEntry {
    return Batch_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Batch_LabelsEntry>): Batch_LabelsEntry {
    const message = createBaseBatch_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBasePySparkBatch(): PySparkBatch {
  return { mainPythonFileUri: "", args: [], pythonFileUris: [], jarFileUris: [], fileUris: [], archiveUris: [] };
}

export const PySparkBatch: MessageFns<PySparkBatch> = {
  encode(message: PySparkBatch, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.mainPythonFileUri !== "") {
      writer.uint32(10).string(message.mainPythonFileUri);
    }
    for (const v of message.args) {
      writer.uint32(18).string(v!);
    }
    for (const v of message.pythonFileUris) {
      writer.uint32(26).string(v!);
    }
    for (const v of message.jarFileUris) {
      writer.uint32(34).string(v!);
    }
    for (const v of message.fileUris) {
      writer.uint32(42).string(v!);
    }
    for (const v of message.archiveUris) {
      writer.uint32(50).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PySparkBatch {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePySparkBatch();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.mainPythonFileUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.args.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pythonFileUris.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.jarFileUris.push(reader.string());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.fileUris.push(reader.string());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.archiveUris.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PySparkBatch {
    return {
      mainPythonFileUri: isSet(object.mainPythonFileUri) ? globalThis.String(object.mainPythonFileUri) : "",
      args: globalThis.Array.isArray(object?.args) ? object.args.map((e: any) => globalThis.String(e)) : [],
      pythonFileUris: globalThis.Array.isArray(object?.pythonFileUris)
        ? object.pythonFileUris.map((e: any) => globalThis.String(e))
        : [],
      jarFileUris: globalThis.Array.isArray(object?.jarFileUris)
        ? object.jarFileUris.map((e: any) => globalThis.String(e))
        : [],
      fileUris: globalThis.Array.isArray(object?.fileUris) ? object.fileUris.map((e: any) => globalThis.String(e)) : [],
      archiveUris: globalThis.Array.isArray(object?.archiveUris)
        ? object.archiveUris.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: PySparkBatch): unknown {
    const obj: any = {};
    if (message.mainPythonFileUri !== "") {
      obj.mainPythonFileUri = message.mainPythonFileUri;
    }
    if (message.args?.length) {
      obj.args = message.args;
    }
    if (message.pythonFileUris?.length) {
      obj.pythonFileUris = message.pythonFileUris;
    }
    if (message.jarFileUris?.length) {
      obj.jarFileUris = message.jarFileUris;
    }
    if (message.fileUris?.length) {
      obj.fileUris = message.fileUris;
    }
    if (message.archiveUris?.length) {
      obj.archiveUris = message.archiveUris;
    }
    return obj;
  },

  create(base?: DeepPartial<PySparkBatch>): PySparkBatch {
    return PySparkBatch.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PySparkBatch>): PySparkBatch {
    const message = createBasePySparkBatch();
    message.mainPythonFileUri = object.mainPythonFileUri ?? "";
    message.args = object.args?.map((e) => e) || [];
    message.pythonFileUris = object.pythonFileUris?.map((e) => e) || [];
    message.jarFileUris = object.jarFileUris?.map((e) => e) || [];
    message.fileUris = object.fileUris?.map((e) => e) || [];
    message.archiveUris = object.archiveUris?.map((e) => e) || [];
    return message;
  },
};

function createBaseSparkBatch(): SparkBatch {
  return { mainJarFileUri: undefined, mainClass: undefined, args: [], jarFileUris: [], fileUris: [], archiveUris: [] };
}

export const SparkBatch: MessageFns<SparkBatch> = {
  encode(message: SparkBatch, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.mainJarFileUri !== undefined) {
      writer.uint32(10).string(message.mainJarFileUri);
    }
    if (message.mainClass !== undefined) {
      writer.uint32(18).string(message.mainClass);
    }
    for (const v of message.args) {
      writer.uint32(26).string(v!);
    }
    for (const v of message.jarFileUris) {
      writer.uint32(34).string(v!);
    }
    for (const v of message.fileUris) {
      writer.uint32(42).string(v!);
    }
    for (const v of message.archiveUris) {
      writer.uint32(50).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SparkBatch {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSparkBatch();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.mainJarFileUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.mainClass = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.args.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.jarFileUris.push(reader.string());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.fileUris.push(reader.string());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.archiveUris.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SparkBatch {
    return {
      mainJarFileUri: isSet(object.mainJarFileUri) ? globalThis.String(object.mainJarFileUri) : undefined,
      mainClass: isSet(object.mainClass) ? globalThis.String(object.mainClass) : undefined,
      args: globalThis.Array.isArray(object?.args) ? object.args.map((e: any) => globalThis.String(e)) : [],
      jarFileUris: globalThis.Array.isArray(object?.jarFileUris)
        ? object.jarFileUris.map((e: any) => globalThis.String(e))
        : [],
      fileUris: globalThis.Array.isArray(object?.fileUris) ? object.fileUris.map((e: any) => globalThis.String(e)) : [],
      archiveUris: globalThis.Array.isArray(object?.archiveUris)
        ? object.archiveUris.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: SparkBatch): unknown {
    const obj: any = {};
    if (message.mainJarFileUri !== undefined) {
      obj.mainJarFileUri = message.mainJarFileUri;
    }
    if (message.mainClass !== undefined) {
      obj.mainClass = message.mainClass;
    }
    if (message.args?.length) {
      obj.args = message.args;
    }
    if (message.jarFileUris?.length) {
      obj.jarFileUris = message.jarFileUris;
    }
    if (message.fileUris?.length) {
      obj.fileUris = message.fileUris;
    }
    if (message.archiveUris?.length) {
      obj.archiveUris = message.archiveUris;
    }
    return obj;
  },

  create(base?: DeepPartial<SparkBatch>): SparkBatch {
    return SparkBatch.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SparkBatch>): SparkBatch {
    const message = createBaseSparkBatch();
    message.mainJarFileUri = object.mainJarFileUri ?? undefined;
    message.mainClass = object.mainClass ?? undefined;
    message.args = object.args?.map((e) => e) || [];
    message.jarFileUris = object.jarFileUris?.map((e) => e) || [];
    message.fileUris = object.fileUris?.map((e) => e) || [];
    message.archiveUris = object.archiveUris?.map((e) => e) || [];
    return message;
  },
};

function createBaseSparkRBatch(): SparkRBatch {
  return { mainRFileUri: "", args: [], fileUris: [], archiveUris: [] };
}

export const SparkRBatch: MessageFns<SparkRBatch> = {
  encode(message: SparkRBatch, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.mainRFileUri !== "") {
      writer.uint32(10).string(message.mainRFileUri);
    }
    for (const v of message.args) {
      writer.uint32(18).string(v!);
    }
    for (const v of message.fileUris) {
      writer.uint32(26).string(v!);
    }
    for (const v of message.archiveUris) {
      writer.uint32(34).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SparkRBatch {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSparkRBatch();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.mainRFileUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.args.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.fileUris.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.archiveUris.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SparkRBatch {
    return {
      mainRFileUri: isSet(object.mainRFileUri) ? globalThis.String(object.mainRFileUri) : "",
      args: globalThis.Array.isArray(object?.args) ? object.args.map((e: any) => globalThis.String(e)) : [],
      fileUris: globalThis.Array.isArray(object?.fileUris) ? object.fileUris.map((e: any) => globalThis.String(e)) : [],
      archiveUris: globalThis.Array.isArray(object?.archiveUris)
        ? object.archiveUris.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: SparkRBatch): unknown {
    const obj: any = {};
    if (message.mainRFileUri !== "") {
      obj.mainRFileUri = message.mainRFileUri;
    }
    if (message.args?.length) {
      obj.args = message.args;
    }
    if (message.fileUris?.length) {
      obj.fileUris = message.fileUris;
    }
    if (message.archiveUris?.length) {
      obj.archiveUris = message.archiveUris;
    }
    return obj;
  },

  create(base?: DeepPartial<SparkRBatch>): SparkRBatch {
    return SparkRBatch.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SparkRBatch>): SparkRBatch {
    const message = createBaseSparkRBatch();
    message.mainRFileUri = object.mainRFileUri ?? "";
    message.args = object.args?.map((e) => e) || [];
    message.fileUris = object.fileUris?.map((e) => e) || [];
    message.archiveUris = object.archiveUris?.map((e) => e) || [];
    return message;
  },
};

function createBaseSparkSqlBatch(): SparkSqlBatch {
  return { queryFileUri: "", queryVariables: {}, jarFileUris: [] };
}

export const SparkSqlBatch: MessageFns<SparkSqlBatch> = {
  encode(message: SparkSqlBatch, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.queryFileUri !== "") {
      writer.uint32(10).string(message.queryFileUri);
    }
    Object.entries(message.queryVariables).forEach(([key, value]) => {
      SparkSqlBatch_QueryVariablesEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    for (const v of message.jarFileUris) {
      writer.uint32(26).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SparkSqlBatch {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSparkSqlBatch();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.queryFileUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = SparkSqlBatch_QueryVariablesEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.queryVariables[entry2.key] = entry2.value;
          }
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.jarFileUris.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SparkSqlBatch {
    return {
      queryFileUri: isSet(object.queryFileUri) ? globalThis.String(object.queryFileUri) : "",
      queryVariables: isObject(object.queryVariables)
        ? Object.entries(object.queryVariables).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      jarFileUris: globalThis.Array.isArray(object?.jarFileUris)
        ? object.jarFileUris.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: SparkSqlBatch): unknown {
    const obj: any = {};
    if (message.queryFileUri !== "") {
      obj.queryFileUri = message.queryFileUri;
    }
    if (message.queryVariables) {
      const entries = Object.entries(message.queryVariables);
      if (entries.length > 0) {
        obj.queryVariables = {};
        entries.forEach(([k, v]) => {
          obj.queryVariables[k] = v;
        });
      }
    }
    if (message.jarFileUris?.length) {
      obj.jarFileUris = message.jarFileUris;
    }
    return obj;
  },

  create(base?: DeepPartial<SparkSqlBatch>): SparkSqlBatch {
    return SparkSqlBatch.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SparkSqlBatch>): SparkSqlBatch {
    const message = createBaseSparkSqlBatch();
    message.queryFileUri = object.queryFileUri ?? "";
    message.queryVariables = Object.entries(object.queryVariables ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.jarFileUris = object.jarFileUris?.map((e) => e) || [];
    return message;
  },
};

function createBaseSparkSqlBatch_QueryVariablesEntry(): SparkSqlBatch_QueryVariablesEntry {
  return { key: "", value: "" };
}

export const SparkSqlBatch_QueryVariablesEntry: MessageFns<SparkSqlBatch_QueryVariablesEntry> = {
  encode(message: SparkSqlBatch_QueryVariablesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SparkSqlBatch_QueryVariablesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSparkSqlBatch_QueryVariablesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SparkSqlBatch_QueryVariablesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: SparkSqlBatch_QueryVariablesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<SparkSqlBatch_QueryVariablesEntry>): SparkSqlBatch_QueryVariablesEntry {
    return SparkSqlBatch_QueryVariablesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SparkSqlBatch_QueryVariablesEntry>): SparkSqlBatch_QueryVariablesEntry {
    const message = createBaseSparkSqlBatch_QueryVariablesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

/** The BatchController provides methods to manage batch workloads. */
export type BatchControllerDefinition = typeof BatchControllerDefinition;
export const BatchControllerDefinition = {
  name: "BatchController",
  fullName: "google.cloud.dataproc.v1.BatchController",
  methods: {
    /** Creates a batch workload that executes asynchronously. */
    createBatch: {
      name: "CreateBatch",
      requestType: CreateBatchRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              56,
              10,
              5,
              66,
              97,
              116,
              99,
              104,
              18,
              47,
              103,
              111,
              111,
              103,
              108,
              101,
              46,
              99,
              108,
              111,
              117,
              100,
              46,
              100,
              97,
              116,
              97,
              112,
              114,
              111,
              99,
              46,
              118,
              49,
              46,
              66,
              97,
              116,
              99,
              104,
              79,
              112,
              101,
              114,
              97,
              116,
              105,
              111,
              110,
              77,
              101,
              116,
              97,
              100,
              97,
              116,
              97,
            ]),
          ],
          8410: [
            Buffer.from([
              21,
              112,
              97,
              114,
              101,
              110,
              116,
              44,
              98,
              97,
              116,
              99,
              104,
              44,
              98,
              97,
              116,
              99,
              104,
              95,
              105,
              100,
            ]),
          ],
          578365826: [
            Buffer.from([
              52,
              58,
              5,
              98,
              97,
              116,
              99,
              104,
              34,
              43,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              47,
              98,
              97,
              116,
              99,
              104,
              101,
              115,
            ]),
          ],
        },
      },
    },
    /** Gets the batch workload resource representation. */
    getBatch: {
      name: "GetBatch",
      requestType: GetBatchRequest,
      requestStream: false,
      responseType: Batch,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              45,
              18,
              43,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              98,
              97,
              116,
              99,
              104,
              101,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /** Lists batch workloads. */
    listBatches: {
      name: "ListBatches",
      requestType: ListBatchesRequest,
      requestStream: false,
      responseType: ListBatchesResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([6, 112, 97, 114, 101, 110, 116])],
          578365826: [
            Buffer.from([
              45,
              18,
              43,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              47,
              98,
              97,
              116,
              99,
              104,
              101,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Deletes the batch workload resource. If the batch is not in terminal state,
     * the delete fails and the response returns `FAILED_PRECONDITION`.
     */
    deleteBatch: {
      name: "DeleteBatch",
      requestType: DeleteBatchRequest,
      requestStream: false,
      responseType: Empty,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              45,
              42,
              43,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              98,
              97,
              116,
              99,
              104,
              101,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface BatchControllerServiceImplementation<CallContextExt = {}> {
  /** Creates a batch workload that executes asynchronously. */
  createBatch(request: CreateBatchRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
  /** Gets the batch workload resource representation. */
  getBatch(request: GetBatchRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Batch>>;
  /** Lists batch workloads. */
  listBatches(
    request: ListBatchesRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ListBatchesResponse>>;
  /**
   * Deletes the batch workload resource. If the batch is not in terminal state,
   * the delete fails and the response returns `FAILED_PRECONDITION`.
   */
  deleteBatch(request: DeleteBatchRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Empty>>;
}

export interface BatchControllerClient<CallOptionsExt = {}> {
  /** Creates a batch workload that executes asynchronously. */
  createBatch(request: DeepPartial<CreateBatchRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
  /** Gets the batch workload resource representation. */
  getBatch(request: DeepPartial<GetBatchRequest>, options?: CallOptions & CallOptionsExt): Promise<Batch>;
  /** Lists batch workloads. */
  listBatches(
    request: DeepPartial<ListBatchesRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ListBatchesResponse>;
  /**
   * Deletes the batch workload resource. If the batch is not in terminal state,
   * the delete fails and the response returns `FAILED_PRECONDITION`.
   */
  deleteBatch(request: DeepPartial<DeleteBatchRequest>, options?: CallOptions & CallOptionsExt): Promise<Empty>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
