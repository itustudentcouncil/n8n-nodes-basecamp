// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/sql/v1/cloud_sql_resources.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Duration } from "../../../protobuf/duration.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { BoolValue, Int32Value, Int64Value } from "../../../protobuf/wrappers.js";

export const protobufPackage = "google.cloud.sql.v1";

export enum SqlFileType {
  /** SQL_FILE_TYPE_UNSPECIFIED - Unknown file type. */
  SQL_FILE_TYPE_UNSPECIFIED = 0,
  /** SQL - File containing SQL statements. */
  SQL = 1,
  /** CSV - File in CSV format. */
  CSV = 2,
  BAK = 4,
  UNRECOGNIZED = -1,
}

export function sqlFileTypeFromJSON(object: any): SqlFileType {
  switch (object) {
    case 0:
    case "SQL_FILE_TYPE_UNSPECIFIED":
      return SqlFileType.SQL_FILE_TYPE_UNSPECIFIED;
    case 1:
    case "SQL":
      return SqlFileType.SQL;
    case 2:
    case "CSV":
      return SqlFileType.CSV;
    case 4:
    case "BAK":
      return SqlFileType.BAK;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlFileType.UNRECOGNIZED;
  }
}

export function sqlFileTypeToJSON(object: SqlFileType): string {
  switch (object) {
    case SqlFileType.SQL_FILE_TYPE_UNSPECIFIED:
      return "SQL_FILE_TYPE_UNSPECIFIED";
    case SqlFileType.SQL:
      return "SQL";
    case SqlFileType.CSV:
      return "CSV";
    case SqlFileType.BAK:
      return "BAK";
    case SqlFileType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export enum BakType {
  /** BAK_TYPE_UNSPECIFIED - Default type. */
  BAK_TYPE_UNSPECIFIED = 0,
  /** FULL - Full backup. */
  FULL = 1,
  /** DIFF - Differential backup. */
  DIFF = 2,
  /** TLOG - Transaction Log backup */
  TLOG = 3,
  UNRECOGNIZED = -1,
}

export function bakTypeFromJSON(object: any): BakType {
  switch (object) {
    case 0:
    case "BAK_TYPE_UNSPECIFIED":
      return BakType.BAK_TYPE_UNSPECIFIED;
    case 1:
    case "FULL":
      return BakType.FULL;
    case 2:
    case "DIFF":
      return BakType.DIFF;
    case 3:
    case "TLOG":
      return BakType.TLOG;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BakType.UNRECOGNIZED;
  }
}

export function bakTypeToJSON(object: BakType): string {
  switch (object) {
    case BakType.BAK_TYPE_UNSPECIFIED:
      return "BAK_TYPE_UNSPECIFIED";
    case BakType.FULL:
      return "FULL";
    case BakType.DIFF:
      return "DIFF";
    case BakType.TLOG:
      return "TLOG";
    case BakType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export enum SqlBackendType {
  /** SQL_BACKEND_TYPE_UNSPECIFIED - This is an unknown backend type for instance. */
  SQL_BACKEND_TYPE_UNSPECIFIED = 0,
  /**
   * FIRST_GEN - V1 speckle instance.
   *
   * @deprecated
   */
  FIRST_GEN = 1,
  /** SECOND_GEN - V2 speckle instance. */
  SECOND_GEN = 2,
  /** EXTERNAL - On premises instance. */
  EXTERNAL = 3,
  UNRECOGNIZED = -1,
}

export function sqlBackendTypeFromJSON(object: any): SqlBackendType {
  switch (object) {
    case 0:
    case "SQL_BACKEND_TYPE_UNSPECIFIED":
      return SqlBackendType.SQL_BACKEND_TYPE_UNSPECIFIED;
    case 1:
    case "FIRST_GEN":
      return SqlBackendType.FIRST_GEN;
    case 2:
    case "SECOND_GEN":
      return SqlBackendType.SECOND_GEN;
    case 3:
    case "EXTERNAL":
      return SqlBackendType.EXTERNAL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlBackendType.UNRECOGNIZED;
  }
}

export function sqlBackendTypeToJSON(object: SqlBackendType): string {
  switch (object) {
    case SqlBackendType.SQL_BACKEND_TYPE_UNSPECIFIED:
      return "SQL_BACKEND_TYPE_UNSPECIFIED";
    case SqlBackendType.FIRST_GEN:
      return "FIRST_GEN";
    case SqlBackendType.SECOND_GEN:
      return "SECOND_GEN";
    case SqlBackendType.EXTERNAL:
      return "EXTERNAL";
    case SqlBackendType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export enum SqlIpAddressType {
  /** SQL_IP_ADDRESS_TYPE_UNSPECIFIED - This is an unknown IP address type. */
  SQL_IP_ADDRESS_TYPE_UNSPECIFIED = 0,
  /**
   * PRIMARY - IP address the customer is supposed to connect to. Usually this is the
   * load balancer's IP address
   */
  PRIMARY = 1,
  /**
   * OUTGOING - Source IP address of the connection a read replica establishes to its
   * external primary instance. This IP address can be allowlisted by the
   * customer in case it has a firewall that filters incoming connection to its
   * on premises primary instance.
   */
  OUTGOING = 2,
  /** PRIVATE - Private IP used when using private IPs and network peering. */
  PRIVATE = 3,
  /**
   * MIGRATED_1ST_GEN - V1 IP of a migrated instance. We want the user to
   * decommission this IP as soon as the migration is complete.
   * Note: V1 instances with V1 ip addresses will be counted as PRIMARY.
   */
  MIGRATED_1ST_GEN = 4,
  UNRECOGNIZED = -1,
}

export function sqlIpAddressTypeFromJSON(object: any): SqlIpAddressType {
  switch (object) {
    case 0:
    case "SQL_IP_ADDRESS_TYPE_UNSPECIFIED":
      return SqlIpAddressType.SQL_IP_ADDRESS_TYPE_UNSPECIFIED;
    case 1:
    case "PRIMARY":
      return SqlIpAddressType.PRIMARY;
    case 2:
    case "OUTGOING":
      return SqlIpAddressType.OUTGOING;
    case 3:
    case "PRIVATE":
      return SqlIpAddressType.PRIVATE;
    case 4:
    case "MIGRATED_1ST_GEN":
      return SqlIpAddressType.MIGRATED_1ST_GEN;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlIpAddressType.UNRECOGNIZED;
  }
}

export function sqlIpAddressTypeToJSON(object: SqlIpAddressType): string {
  switch (object) {
    case SqlIpAddressType.SQL_IP_ADDRESS_TYPE_UNSPECIFIED:
      return "SQL_IP_ADDRESS_TYPE_UNSPECIFIED";
    case SqlIpAddressType.PRIMARY:
      return "PRIMARY";
    case SqlIpAddressType.OUTGOING:
      return "OUTGOING";
    case SqlIpAddressType.PRIVATE:
      return "PRIVATE";
    case SqlIpAddressType.MIGRATED_1ST_GEN:
      return "MIGRATED_1ST_GEN";
    case SqlIpAddressType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The database engine type and version. */
export enum SqlDatabaseVersion {
  /** SQL_DATABASE_VERSION_UNSPECIFIED - This is an unknown database version. */
  SQL_DATABASE_VERSION_UNSPECIFIED = 0,
  /**
   * MYSQL_5_1 - The database version is MySQL 5.1.
   *
   * @deprecated
   */
  MYSQL_5_1 = 2,
  /**
   * MYSQL_5_5 - The database version is MySQL 5.5.
   *
   * @deprecated
   */
  MYSQL_5_5 = 3,
  /** MYSQL_5_6 - The database version is MySQL 5.6. */
  MYSQL_5_6 = 5,
  /** MYSQL_5_7 - The database version is MySQL 5.7. */
  MYSQL_5_7 = 6,
  /** SQLSERVER_2017_STANDARD - The database version is SQL Server 2017 Standard. */
  SQLSERVER_2017_STANDARD = 11,
  /** SQLSERVER_2017_ENTERPRISE - The database version is SQL Server 2017 Enterprise. */
  SQLSERVER_2017_ENTERPRISE = 14,
  /** SQLSERVER_2017_EXPRESS - The database version is SQL Server 2017 Express. */
  SQLSERVER_2017_EXPRESS = 15,
  /** SQLSERVER_2017_WEB - The database version is SQL Server 2017 Web. */
  SQLSERVER_2017_WEB = 16,
  /** POSTGRES_9_6 - The database version is PostgreSQL 9.6. */
  POSTGRES_9_6 = 9,
  /** POSTGRES_10 - The database version is PostgreSQL 10. */
  POSTGRES_10 = 18,
  /** POSTGRES_11 - The database version is PostgreSQL 11. */
  POSTGRES_11 = 10,
  /** POSTGRES_12 - The database version is PostgreSQL 12. */
  POSTGRES_12 = 19,
  /** POSTGRES_13 - The database version is PostgreSQL 13. */
  POSTGRES_13 = 23,
  /** POSTGRES_14 - The database version is PostgreSQL 14. */
  POSTGRES_14 = 110,
  /** POSTGRES_15 - The database version is PostgreSQL 15. */
  POSTGRES_15 = 172,
  /** POSTGRES_16 - The database version is PostgreSQL 16. */
  POSTGRES_16 = 272,
  /** MYSQL_8_0 - The database version is MySQL 8. */
  MYSQL_8_0 = 20,
  /** MYSQL_8_0_18 - The database major version is MySQL 8.0 and the minor version is 18. */
  MYSQL_8_0_18 = 41,
  /** MYSQL_8_0_26 - The database major version is MySQL 8.0 and the minor version is 26. */
  MYSQL_8_0_26 = 85,
  /** MYSQL_8_0_27 - The database major version is MySQL 8.0 and the minor version is 27. */
  MYSQL_8_0_27 = 111,
  /** MYSQL_8_0_28 - The database major version is MySQL 8.0 and the minor version is 28. */
  MYSQL_8_0_28 = 132,
  /**
   * MYSQL_8_0_29 - The database major version is MySQL 8.0 and the minor version is 29.
   *
   * @deprecated
   */
  MYSQL_8_0_29 = 148,
  /** MYSQL_8_0_30 - The database major version is MySQL 8.0 and the minor version is 30. */
  MYSQL_8_0_30 = 174,
  /** MYSQL_8_0_31 - The database major version is MySQL 8.0 and the minor version is 31. */
  MYSQL_8_0_31 = 197,
  /** MYSQL_8_0_32 - The database major version is MySQL 8.0 and the minor version is 32. */
  MYSQL_8_0_32 = 213,
  /** MYSQL_8_0_33 - The database major version is MySQL 8.0 and the minor version is 33. */
  MYSQL_8_0_33 = 238,
  /** MYSQL_8_0_34 - The database major version is MySQL 8.0 and the minor version is 34. */
  MYSQL_8_0_34 = 239,
  /** MYSQL_8_0_35 - The database major version is MySQL 8.0 and the minor version is 35. */
  MYSQL_8_0_35 = 240,
  /** MYSQL_8_0_36 - The database major version is MySQL 8.0 and the minor version is 36. */
  MYSQL_8_0_36 = 241,
  /** MYSQL_8_0_37 - The database major version is MySQL 8.0 and the minor version is 37. */
  MYSQL_8_0_37 = 355,
  /** MYSQL_8_0_38 - The database major version is MySQL 8.0 and the minor version is 38. */
  MYSQL_8_0_38 = 356,
  /** MYSQL_8_0_39 - The database major version is MySQL 8.0 and the minor version is 39. */
  MYSQL_8_0_39 = 357,
  /** MYSQL_8_0_40 - The database major version is MySQL 8.0 and the minor version is 40. */
  MYSQL_8_0_40 = 358,
  /** MYSQL_8_4 - The database version is MySQL 8.4. */
  MYSQL_8_4 = 398,
  /** MYSQL_8_4_0 - The database version is MySQL 8.4 and the patch version is 0. */
  MYSQL_8_4_0 = 399,
  /** SQLSERVER_2019_STANDARD - The database version is SQL Server 2019 Standard. */
  SQLSERVER_2019_STANDARD = 26,
  /** SQLSERVER_2019_ENTERPRISE - The database version is SQL Server 2019 Enterprise. */
  SQLSERVER_2019_ENTERPRISE = 27,
  /** SQLSERVER_2019_EXPRESS - The database version is SQL Server 2019 Express. */
  SQLSERVER_2019_EXPRESS = 28,
  /** SQLSERVER_2019_WEB - The database version is SQL Server 2019 Web. */
  SQLSERVER_2019_WEB = 29,
  /** SQLSERVER_2022_STANDARD - The database version is SQL Server 2022 Standard. */
  SQLSERVER_2022_STANDARD = 199,
  /** SQLSERVER_2022_ENTERPRISE - The database version is SQL Server 2022 Enterprise. */
  SQLSERVER_2022_ENTERPRISE = 200,
  /** SQLSERVER_2022_EXPRESS - The database version is SQL Server 2022 Express. */
  SQLSERVER_2022_EXPRESS = 201,
  /** SQLSERVER_2022_WEB - The database version is SQL Server 2022 Web. */
  SQLSERVER_2022_WEB = 202,
  UNRECOGNIZED = -1,
}

export function sqlDatabaseVersionFromJSON(object: any): SqlDatabaseVersion {
  switch (object) {
    case 0:
    case "SQL_DATABASE_VERSION_UNSPECIFIED":
      return SqlDatabaseVersion.SQL_DATABASE_VERSION_UNSPECIFIED;
    case 2:
    case "MYSQL_5_1":
      return SqlDatabaseVersion.MYSQL_5_1;
    case 3:
    case "MYSQL_5_5":
      return SqlDatabaseVersion.MYSQL_5_5;
    case 5:
    case "MYSQL_5_6":
      return SqlDatabaseVersion.MYSQL_5_6;
    case 6:
    case "MYSQL_5_7":
      return SqlDatabaseVersion.MYSQL_5_7;
    case 11:
    case "SQLSERVER_2017_STANDARD":
      return SqlDatabaseVersion.SQLSERVER_2017_STANDARD;
    case 14:
    case "SQLSERVER_2017_ENTERPRISE":
      return SqlDatabaseVersion.SQLSERVER_2017_ENTERPRISE;
    case 15:
    case "SQLSERVER_2017_EXPRESS":
      return SqlDatabaseVersion.SQLSERVER_2017_EXPRESS;
    case 16:
    case "SQLSERVER_2017_WEB":
      return SqlDatabaseVersion.SQLSERVER_2017_WEB;
    case 9:
    case "POSTGRES_9_6":
      return SqlDatabaseVersion.POSTGRES_9_6;
    case 18:
    case "POSTGRES_10":
      return SqlDatabaseVersion.POSTGRES_10;
    case 10:
    case "POSTGRES_11":
      return SqlDatabaseVersion.POSTGRES_11;
    case 19:
    case "POSTGRES_12":
      return SqlDatabaseVersion.POSTGRES_12;
    case 23:
    case "POSTGRES_13":
      return SqlDatabaseVersion.POSTGRES_13;
    case 110:
    case "POSTGRES_14":
      return SqlDatabaseVersion.POSTGRES_14;
    case 172:
    case "POSTGRES_15":
      return SqlDatabaseVersion.POSTGRES_15;
    case 272:
    case "POSTGRES_16":
      return SqlDatabaseVersion.POSTGRES_16;
    case 20:
    case "MYSQL_8_0":
      return SqlDatabaseVersion.MYSQL_8_0;
    case 41:
    case "MYSQL_8_0_18":
      return SqlDatabaseVersion.MYSQL_8_0_18;
    case 85:
    case "MYSQL_8_0_26":
      return SqlDatabaseVersion.MYSQL_8_0_26;
    case 111:
    case "MYSQL_8_0_27":
      return SqlDatabaseVersion.MYSQL_8_0_27;
    case 132:
    case "MYSQL_8_0_28":
      return SqlDatabaseVersion.MYSQL_8_0_28;
    case 148:
    case "MYSQL_8_0_29":
      return SqlDatabaseVersion.MYSQL_8_0_29;
    case 174:
    case "MYSQL_8_0_30":
      return SqlDatabaseVersion.MYSQL_8_0_30;
    case 197:
    case "MYSQL_8_0_31":
      return SqlDatabaseVersion.MYSQL_8_0_31;
    case 213:
    case "MYSQL_8_0_32":
      return SqlDatabaseVersion.MYSQL_8_0_32;
    case 238:
    case "MYSQL_8_0_33":
      return SqlDatabaseVersion.MYSQL_8_0_33;
    case 239:
    case "MYSQL_8_0_34":
      return SqlDatabaseVersion.MYSQL_8_0_34;
    case 240:
    case "MYSQL_8_0_35":
      return SqlDatabaseVersion.MYSQL_8_0_35;
    case 241:
    case "MYSQL_8_0_36":
      return SqlDatabaseVersion.MYSQL_8_0_36;
    case 355:
    case "MYSQL_8_0_37":
      return SqlDatabaseVersion.MYSQL_8_0_37;
    case 356:
    case "MYSQL_8_0_38":
      return SqlDatabaseVersion.MYSQL_8_0_38;
    case 357:
    case "MYSQL_8_0_39":
      return SqlDatabaseVersion.MYSQL_8_0_39;
    case 358:
    case "MYSQL_8_0_40":
      return SqlDatabaseVersion.MYSQL_8_0_40;
    case 398:
    case "MYSQL_8_4":
      return SqlDatabaseVersion.MYSQL_8_4;
    case 399:
    case "MYSQL_8_4_0":
      return SqlDatabaseVersion.MYSQL_8_4_0;
    case 26:
    case "SQLSERVER_2019_STANDARD":
      return SqlDatabaseVersion.SQLSERVER_2019_STANDARD;
    case 27:
    case "SQLSERVER_2019_ENTERPRISE":
      return SqlDatabaseVersion.SQLSERVER_2019_ENTERPRISE;
    case 28:
    case "SQLSERVER_2019_EXPRESS":
      return SqlDatabaseVersion.SQLSERVER_2019_EXPRESS;
    case 29:
    case "SQLSERVER_2019_WEB":
      return SqlDatabaseVersion.SQLSERVER_2019_WEB;
    case 199:
    case "SQLSERVER_2022_STANDARD":
      return SqlDatabaseVersion.SQLSERVER_2022_STANDARD;
    case 200:
    case "SQLSERVER_2022_ENTERPRISE":
      return SqlDatabaseVersion.SQLSERVER_2022_ENTERPRISE;
    case 201:
    case "SQLSERVER_2022_EXPRESS":
      return SqlDatabaseVersion.SQLSERVER_2022_EXPRESS;
    case 202:
    case "SQLSERVER_2022_WEB":
      return SqlDatabaseVersion.SQLSERVER_2022_WEB;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlDatabaseVersion.UNRECOGNIZED;
  }
}

export function sqlDatabaseVersionToJSON(object: SqlDatabaseVersion): string {
  switch (object) {
    case SqlDatabaseVersion.SQL_DATABASE_VERSION_UNSPECIFIED:
      return "SQL_DATABASE_VERSION_UNSPECIFIED";
    case SqlDatabaseVersion.MYSQL_5_1:
      return "MYSQL_5_1";
    case SqlDatabaseVersion.MYSQL_5_5:
      return "MYSQL_5_5";
    case SqlDatabaseVersion.MYSQL_5_6:
      return "MYSQL_5_6";
    case SqlDatabaseVersion.MYSQL_5_7:
      return "MYSQL_5_7";
    case SqlDatabaseVersion.SQLSERVER_2017_STANDARD:
      return "SQLSERVER_2017_STANDARD";
    case SqlDatabaseVersion.SQLSERVER_2017_ENTERPRISE:
      return "SQLSERVER_2017_ENTERPRISE";
    case SqlDatabaseVersion.SQLSERVER_2017_EXPRESS:
      return "SQLSERVER_2017_EXPRESS";
    case SqlDatabaseVersion.SQLSERVER_2017_WEB:
      return "SQLSERVER_2017_WEB";
    case SqlDatabaseVersion.POSTGRES_9_6:
      return "POSTGRES_9_6";
    case SqlDatabaseVersion.POSTGRES_10:
      return "POSTGRES_10";
    case SqlDatabaseVersion.POSTGRES_11:
      return "POSTGRES_11";
    case SqlDatabaseVersion.POSTGRES_12:
      return "POSTGRES_12";
    case SqlDatabaseVersion.POSTGRES_13:
      return "POSTGRES_13";
    case SqlDatabaseVersion.POSTGRES_14:
      return "POSTGRES_14";
    case SqlDatabaseVersion.POSTGRES_15:
      return "POSTGRES_15";
    case SqlDatabaseVersion.POSTGRES_16:
      return "POSTGRES_16";
    case SqlDatabaseVersion.MYSQL_8_0:
      return "MYSQL_8_0";
    case SqlDatabaseVersion.MYSQL_8_0_18:
      return "MYSQL_8_0_18";
    case SqlDatabaseVersion.MYSQL_8_0_26:
      return "MYSQL_8_0_26";
    case SqlDatabaseVersion.MYSQL_8_0_27:
      return "MYSQL_8_0_27";
    case SqlDatabaseVersion.MYSQL_8_0_28:
      return "MYSQL_8_0_28";
    case SqlDatabaseVersion.MYSQL_8_0_29:
      return "MYSQL_8_0_29";
    case SqlDatabaseVersion.MYSQL_8_0_30:
      return "MYSQL_8_0_30";
    case SqlDatabaseVersion.MYSQL_8_0_31:
      return "MYSQL_8_0_31";
    case SqlDatabaseVersion.MYSQL_8_0_32:
      return "MYSQL_8_0_32";
    case SqlDatabaseVersion.MYSQL_8_0_33:
      return "MYSQL_8_0_33";
    case SqlDatabaseVersion.MYSQL_8_0_34:
      return "MYSQL_8_0_34";
    case SqlDatabaseVersion.MYSQL_8_0_35:
      return "MYSQL_8_0_35";
    case SqlDatabaseVersion.MYSQL_8_0_36:
      return "MYSQL_8_0_36";
    case SqlDatabaseVersion.MYSQL_8_0_37:
      return "MYSQL_8_0_37";
    case SqlDatabaseVersion.MYSQL_8_0_38:
      return "MYSQL_8_0_38";
    case SqlDatabaseVersion.MYSQL_8_0_39:
      return "MYSQL_8_0_39";
    case SqlDatabaseVersion.MYSQL_8_0_40:
      return "MYSQL_8_0_40";
    case SqlDatabaseVersion.MYSQL_8_4:
      return "MYSQL_8_4";
    case SqlDatabaseVersion.MYSQL_8_4_0:
      return "MYSQL_8_4_0";
    case SqlDatabaseVersion.SQLSERVER_2019_STANDARD:
      return "SQLSERVER_2019_STANDARD";
    case SqlDatabaseVersion.SQLSERVER_2019_ENTERPRISE:
      return "SQLSERVER_2019_ENTERPRISE";
    case SqlDatabaseVersion.SQLSERVER_2019_EXPRESS:
      return "SQLSERVER_2019_EXPRESS";
    case SqlDatabaseVersion.SQLSERVER_2019_WEB:
      return "SQLSERVER_2019_WEB";
    case SqlDatabaseVersion.SQLSERVER_2022_STANDARD:
      return "SQLSERVER_2022_STANDARD";
    case SqlDatabaseVersion.SQLSERVER_2022_ENTERPRISE:
      return "SQLSERVER_2022_ENTERPRISE";
    case SqlDatabaseVersion.SQLSERVER_2022_EXPRESS:
      return "SQLSERVER_2022_EXPRESS";
    case SqlDatabaseVersion.SQLSERVER_2022_WEB:
      return "SQLSERVER_2022_WEB";
    case SqlDatabaseVersion.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The pricing plan for this instance. */
export enum SqlPricingPlan {
  /** SQL_PRICING_PLAN_UNSPECIFIED - This is an unknown pricing plan for this instance. */
  SQL_PRICING_PLAN_UNSPECIFIED = 0,
  /** PACKAGE - The instance is billed at a monthly flat rate. */
  PACKAGE = 1,
  /** PER_USE - The instance is billed per usage. */
  PER_USE = 2,
  UNRECOGNIZED = -1,
}

export function sqlPricingPlanFromJSON(object: any): SqlPricingPlan {
  switch (object) {
    case 0:
    case "SQL_PRICING_PLAN_UNSPECIFIED":
      return SqlPricingPlan.SQL_PRICING_PLAN_UNSPECIFIED;
    case 1:
    case "PACKAGE":
      return SqlPricingPlan.PACKAGE;
    case 2:
    case "PER_USE":
      return SqlPricingPlan.PER_USE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlPricingPlan.UNRECOGNIZED;
  }
}

export function sqlPricingPlanToJSON(object: SqlPricingPlan): string {
  switch (object) {
    case SqlPricingPlan.SQL_PRICING_PLAN_UNSPECIFIED:
      return "SQL_PRICING_PLAN_UNSPECIFIED";
    case SqlPricingPlan.PACKAGE:
      return "PACKAGE";
    case SqlPricingPlan.PER_USE:
      return "PER_USE";
    case SqlPricingPlan.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export enum SqlReplicationType {
  /** SQL_REPLICATION_TYPE_UNSPECIFIED - This is an unknown replication type for a Cloud SQL instance. */
  SQL_REPLICATION_TYPE_UNSPECIFIED = 0,
  /**
   * SYNCHRONOUS - The synchronous replication mode for First Generation instances. It is the
   * default value.
   */
  SYNCHRONOUS = 1,
  /**
   * ASYNCHRONOUS - The asynchronous replication mode for First Generation instances. It
   * provides a slight performance gain, but if an outage occurs while this
   * option is set to asynchronous, you can lose up to a few seconds of updates
   * to your data.
   */
  ASYNCHRONOUS = 2,
  UNRECOGNIZED = -1,
}

export function sqlReplicationTypeFromJSON(object: any): SqlReplicationType {
  switch (object) {
    case 0:
    case "SQL_REPLICATION_TYPE_UNSPECIFIED":
      return SqlReplicationType.SQL_REPLICATION_TYPE_UNSPECIFIED;
    case 1:
    case "SYNCHRONOUS":
      return SqlReplicationType.SYNCHRONOUS;
    case 2:
    case "ASYNCHRONOUS":
      return SqlReplicationType.ASYNCHRONOUS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlReplicationType.UNRECOGNIZED;
  }
}

export function sqlReplicationTypeToJSON(object: SqlReplicationType): string {
  switch (object) {
    case SqlReplicationType.SQL_REPLICATION_TYPE_UNSPECIFIED:
      return "SQL_REPLICATION_TYPE_UNSPECIFIED";
    case SqlReplicationType.SYNCHRONOUS:
      return "SYNCHRONOUS";
    case SqlReplicationType.ASYNCHRONOUS:
      return "ASYNCHRONOUS";
    case SqlReplicationType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The type of disk that is used for a v2 instance to use. */
export enum SqlDataDiskType {
  /** SQL_DATA_DISK_TYPE_UNSPECIFIED - This is an unknown data disk type. */
  SQL_DATA_DISK_TYPE_UNSPECIFIED = 0,
  /** PD_SSD - An SSD data disk. */
  PD_SSD = 1,
  /** PD_HDD - An HDD data disk. */
  PD_HDD = 2,
  /**
   * OBSOLETE_LOCAL_SSD - This field is deprecated and will be removed from a future version of the
   * API.
   *
   * @deprecated
   */
  OBSOLETE_LOCAL_SSD = 3,
  UNRECOGNIZED = -1,
}

export function sqlDataDiskTypeFromJSON(object: any): SqlDataDiskType {
  switch (object) {
    case 0:
    case "SQL_DATA_DISK_TYPE_UNSPECIFIED":
      return SqlDataDiskType.SQL_DATA_DISK_TYPE_UNSPECIFIED;
    case 1:
    case "PD_SSD":
      return SqlDataDiskType.PD_SSD;
    case 2:
    case "PD_HDD":
      return SqlDataDiskType.PD_HDD;
    case 3:
    case "OBSOLETE_LOCAL_SSD":
      return SqlDataDiskType.OBSOLETE_LOCAL_SSD;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlDataDiskType.UNRECOGNIZED;
  }
}

export function sqlDataDiskTypeToJSON(object: SqlDataDiskType): string {
  switch (object) {
    case SqlDataDiskType.SQL_DATA_DISK_TYPE_UNSPECIFIED:
      return "SQL_DATA_DISK_TYPE_UNSPECIFIED";
    case SqlDataDiskType.PD_SSD:
      return "PD_SSD";
    case SqlDataDiskType.PD_HDD:
      return "PD_HDD";
    case SqlDataDiskType.OBSOLETE_LOCAL_SSD:
      return "OBSOLETE_LOCAL_SSD";
    case SqlDataDiskType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The availability type of the given Cloud SQL instance. */
export enum SqlAvailabilityType {
  /** SQL_AVAILABILITY_TYPE_UNSPECIFIED - This is an unknown Availability type. */
  SQL_AVAILABILITY_TYPE_UNSPECIFIED = 0,
  /** ZONAL - Zonal available instance. */
  ZONAL = 1,
  /** REGIONAL - Regional available instance. */
  REGIONAL = 2,
  UNRECOGNIZED = -1,
}

export function sqlAvailabilityTypeFromJSON(object: any): SqlAvailabilityType {
  switch (object) {
    case 0:
    case "SQL_AVAILABILITY_TYPE_UNSPECIFIED":
      return SqlAvailabilityType.SQL_AVAILABILITY_TYPE_UNSPECIFIED;
    case 1:
    case "ZONAL":
      return SqlAvailabilityType.ZONAL;
    case 2:
    case "REGIONAL":
      return SqlAvailabilityType.REGIONAL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlAvailabilityType.UNRECOGNIZED;
  }
}

export function sqlAvailabilityTypeToJSON(object: SqlAvailabilityType): string {
  switch (object) {
    case SqlAvailabilityType.SQL_AVAILABILITY_TYPE_UNSPECIFIED:
      return "SQL_AVAILABILITY_TYPE_UNSPECIFIED";
    case SqlAvailabilityType.ZONAL:
      return "ZONAL";
    case SqlAvailabilityType.REGIONAL:
      return "REGIONAL";
    case SqlAvailabilityType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export enum SqlUpdateTrack {
  /** SQL_UPDATE_TRACK_UNSPECIFIED - This is an unknown maintenance timing preference. */
  SQL_UPDATE_TRACK_UNSPECIFIED = 0,
  /**
   * canary - For an instance with a scheduled maintenance window, this maintenance
   * timing indicates that the maintenance update is scheduled 7 to 14 days
   * after the notification is sent out. Also referred to as `Week 1` (Console)
   * and `preview` (gcloud CLI).
   */
  canary = 1,
  /**
   * stable - For an instance with a scheduled maintenance window, this maintenance
   * timing indicates that the maintenance update is scheduled 15 to 21 days
   * after the notification is sent out. Also referred to as `Week 2` (Console)
   * and `production` (gcloud CLI).
   */
  stable = 2,
  /**
   * week5 - For instance with a scheduled maintenance window, this maintenance
   * timing indicates that the maintenance update is scheduled 35 to 42 days
   * after the notification is sent out.
   */
  week5 = 3,
  UNRECOGNIZED = -1,
}

export function sqlUpdateTrackFromJSON(object: any): SqlUpdateTrack {
  switch (object) {
    case 0:
    case "SQL_UPDATE_TRACK_UNSPECIFIED":
      return SqlUpdateTrack.SQL_UPDATE_TRACK_UNSPECIFIED;
    case 1:
    case "canary":
      return SqlUpdateTrack.canary;
    case 2:
    case "stable":
      return SqlUpdateTrack.stable;
    case 3:
    case "week5":
      return SqlUpdateTrack.week5;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlUpdateTrack.UNRECOGNIZED;
  }
}

export function sqlUpdateTrackToJSON(object: SqlUpdateTrack): string {
  switch (object) {
    case SqlUpdateTrack.SQL_UPDATE_TRACK_UNSPECIFIED:
      return "SQL_UPDATE_TRACK_UNSPECIFIED";
    case SqlUpdateTrack.canary:
      return "canary";
    case SqlUpdateTrack.stable:
      return "stable";
    case SqlUpdateTrack.week5:
      return "week5";
    case SqlUpdateTrack.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** An entry for an Access Control list. */
export interface AclEntry {
  /** The allowlisted value for the access control list. */
  value: string;
  /**
   * The time when this access control entry expires in
   * [RFC 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`.
   */
  expirationTime:
    | Date
    | undefined;
  /** Optional. A label to identify this entry. */
  name: string;
  /** This is always `sql#aclEntry`. */
  kind: string;
}

/** An Admin API warning message. */
export interface ApiWarning {
  /** Code to uniquely identify the warning type. */
  code: ApiWarning_SqlApiWarningCode;
  /** The warning message. */
  message: string;
  /** The region name for REGION_UNREACHABLE warning. */
  region: string;
}

export enum ApiWarning_SqlApiWarningCode {
  /** SQL_API_WARNING_CODE_UNSPECIFIED - An unknown or unset warning type from Cloud SQL API. */
  SQL_API_WARNING_CODE_UNSPECIFIED = 0,
  /**
   * REGION_UNREACHABLE - Warning when one or more regions are not reachable.  The returned result
   * set may be incomplete.
   */
  REGION_UNREACHABLE = 1,
  /**
   * MAX_RESULTS_EXCEEDS_LIMIT - Warning when user provided maxResults parameter exceeds the limit.  The
   * returned result set may be incomplete.
   */
  MAX_RESULTS_EXCEEDS_LIMIT = 2,
  /**
   * COMPROMISED_CREDENTIALS - Warning when user tries to create/update a user with credentials that
   * have previously been compromised by a public data breach.
   */
  COMPROMISED_CREDENTIALS = 3,
  /**
   * INTERNAL_STATE_FAILURE - Warning when the operation succeeds but some non-critical workflow state
   * failed.
   */
  INTERNAL_STATE_FAILURE = 4,
  UNRECOGNIZED = -1,
}

export function apiWarning_SqlApiWarningCodeFromJSON(object: any): ApiWarning_SqlApiWarningCode {
  switch (object) {
    case 0:
    case "SQL_API_WARNING_CODE_UNSPECIFIED":
      return ApiWarning_SqlApiWarningCode.SQL_API_WARNING_CODE_UNSPECIFIED;
    case 1:
    case "REGION_UNREACHABLE":
      return ApiWarning_SqlApiWarningCode.REGION_UNREACHABLE;
    case 2:
    case "MAX_RESULTS_EXCEEDS_LIMIT":
      return ApiWarning_SqlApiWarningCode.MAX_RESULTS_EXCEEDS_LIMIT;
    case 3:
    case "COMPROMISED_CREDENTIALS":
      return ApiWarning_SqlApiWarningCode.COMPROMISED_CREDENTIALS;
    case 4:
    case "INTERNAL_STATE_FAILURE":
      return ApiWarning_SqlApiWarningCode.INTERNAL_STATE_FAILURE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ApiWarning_SqlApiWarningCode.UNRECOGNIZED;
  }
}

export function apiWarning_SqlApiWarningCodeToJSON(object: ApiWarning_SqlApiWarningCode): string {
  switch (object) {
    case ApiWarning_SqlApiWarningCode.SQL_API_WARNING_CODE_UNSPECIFIED:
      return "SQL_API_WARNING_CODE_UNSPECIFIED";
    case ApiWarning_SqlApiWarningCode.REGION_UNREACHABLE:
      return "REGION_UNREACHABLE";
    case ApiWarning_SqlApiWarningCode.MAX_RESULTS_EXCEEDS_LIMIT:
      return "MAX_RESULTS_EXCEEDS_LIMIT";
    case ApiWarning_SqlApiWarningCode.COMPROMISED_CREDENTIALS:
      return "COMPROMISED_CREDENTIALS";
    case ApiWarning_SqlApiWarningCode.INTERNAL_STATE_FAILURE:
      return "INTERNAL_STATE_FAILURE";
    case ApiWarning_SqlApiWarningCode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * We currently only support backup retention by specifying the number
 * of backups we will retain.
 */
export interface BackupRetentionSettings {
  /** The unit that 'retained_backups' represents. */
  retentionUnit: BackupRetentionSettings_RetentionUnit;
  /**
   * Depending on the value of retention_unit, this is used to determine
   * if a backup needs to be deleted.  If retention_unit is 'COUNT', we will
   * retain this many backups.
   */
  retainedBackups: number | undefined;
}

/** The units that retained_backups specifies, we only support COUNT. */
export enum BackupRetentionSettings_RetentionUnit {
  /** RETENTION_UNIT_UNSPECIFIED - Backup retention unit is unspecified, will be treated as COUNT. */
  RETENTION_UNIT_UNSPECIFIED = 0,
  /** COUNT - Retention will be by count, eg. "retain the most recent 7 backups". */
  COUNT = 1,
  UNRECOGNIZED = -1,
}

export function backupRetentionSettings_RetentionUnitFromJSON(object: any): BackupRetentionSettings_RetentionUnit {
  switch (object) {
    case 0:
    case "RETENTION_UNIT_UNSPECIFIED":
      return BackupRetentionSettings_RetentionUnit.RETENTION_UNIT_UNSPECIFIED;
    case 1:
    case "COUNT":
      return BackupRetentionSettings_RetentionUnit.COUNT;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BackupRetentionSettings_RetentionUnit.UNRECOGNIZED;
  }
}

export function backupRetentionSettings_RetentionUnitToJSON(object: BackupRetentionSettings_RetentionUnit): string {
  switch (object) {
    case BackupRetentionSettings_RetentionUnit.RETENTION_UNIT_UNSPECIFIED:
      return "RETENTION_UNIT_UNSPECIFIED";
    case BackupRetentionSettings_RetentionUnit.COUNT:
      return "COUNT";
    case BackupRetentionSettings_RetentionUnit.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Database instance backup configuration. */
export interface BackupConfiguration {
  /**
   * Start time for the daily backup configuration in UTC timezone in the 24
   * hour format - `HH:MM`.
   */
  startTime: string;
  /** Whether this configuration is enabled. */
  enabled:
    | boolean
    | undefined;
  /** This is always `sql#backupConfiguration`. */
  kind: string;
  /**
   * (MySQL only) Whether binary log is enabled. If backup configuration is
   * disabled, binarylog must be disabled as well.
   */
  binaryLogEnabled:
    | boolean
    | undefined;
  /** Reserved for future use. */
  replicationLogArchivingEnabled:
    | boolean
    | undefined;
  /** Location of the backup */
  location: string;
  /** Whether point in time recovery is enabled. */
  pointInTimeRecoveryEnabled:
    | boolean
    | undefined;
  /** Backup retention settings. */
  backupRetentionSettings:
    | BackupRetentionSettings
    | undefined;
  /**
   * The number of days of transaction logs we retain for point in time
   * restore, from 1-7.
   */
  transactionLogRetentionDays:
    | number
    | undefined;
  /**
   * Output only. This value contains the storage location of transactional logs
   * used to perform point-in-time recovery (PITR) for the database.
   */
  transactionalLogStorageState?: BackupConfiguration_TransactionalLogStorageState | undefined;
}

/**
 * This value contains the storage location of the transactional logs
 * used to perform point-in-time recovery (PITR) for the database.
 */
export enum BackupConfiguration_TransactionalLogStorageState {
  /** TRANSACTIONAL_LOG_STORAGE_STATE_UNSPECIFIED - Unspecified. */
  TRANSACTIONAL_LOG_STORAGE_STATE_UNSPECIFIED = 0,
  /**
   * DISK - The transaction logs used for PITR for the instance are stored
   * on a data disk.
   */
  DISK = 1,
  /**
   * SWITCHING_TO_CLOUD_STORAGE - The transaction logs used for PITR for the instance are switching from
   * being stored on a data disk to being stored in Cloud Storage.
   * Only applicable to MySQL.
   */
  SWITCHING_TO_CLOUD_STORAGE = 2,
  /**
   * SWITCHED_TO_CLOUD_STORAGE - The transaction logs used for PITR for the instance are now stored
   * in Cloud Storage. Previously, they were stored on a data disk.
   * Only applicable to MySQL.
   */
  SWITCHED_TO_CLOUD_STORAGE = 3,
  /**
   * CLOUD_STORAGE - The transaction logs used for PITR for the instance are stored in
   * Cloud Storage. Only applicable to MySQL and PostgreSQL.
   */
  CLOUD_STORAGE = 4,
  UNRECOGNIZED = -1,
}

export function backupConfiguration_TransactionalLogStorageStateFromJSON(
  object: any,
): BackupConfiguration_TransactionalLogStorageState {
  switch (object) {
    case 0:
    case "TRANSACTIONAL_LOG_STORAGE_STATE_UNSPECIFIED":
      return BackupConfiguration_TransactionalLogStorageState.TRANSACTIONAL_LOG_STORAGE_STATE_UNSPECIFIED;
    case 1:
    case "DISK":
      return BackupConfiguration_TransactionalLogStorageState.DISK;
    case 2:
    case "SWITCHING_TO_CLOUD_STORAGE":
      return BackupConfiguration_TransactionalLogStorageState.SWITCHING_TO_CLOUD_STORAGE;
    case 3:
    case "SWITCHED_TO_CLOUD_STORAGE":
      return BackupConfiguration_TransactionalLogStorageState.SWITCHED_TO_CLOUD_STORAGE;
    case 4:
    case "CLOUD_STORAGE":
      return BackupConfiguration_TransactionalLogStorageState.CLOUD_STORAGE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BackupConfiguration_TransactionalLogStorageState.UNRECOGNIZED;
  }
}

export function backupConfiguration_TransactionalLogStorageStateToJSON(
  object: BackupConfiguration_TransactionalLogStorageState,
): string {
  switch (object) {
    case BackupConfiguration_TransactionalLogStorageState.TRANSACTIONAL_LOG_STORAGE_STATE_UNSPECIFIED:
      return "TRANSACTIONAL_LOG_STORAGE_STATE_UNSPECIFIED";
    case BackupConfiguration_TransactionalLogStorageState.DISK:
      return "DISK";
    case BackupConfiguration_TransactionalLogStorageState.SWITCHING_TO_CLOUD_STORAGE:
      return "SWITCHING_TO_CLOUD_STORAGE";
    case BackupConfiguration_TransactionalLogStorageState.SWITCHED_TO_CLOUD_STORAGE:
      return "SWITCHED_TO_CLOUD_STORAGE";
    case BackupConfiguration_TransactionalLogStorageState.CLOUD_STORAGE:
      return "CLOUD_STORAGE";
    case BackupConfiguration_TransactionalLogStorageState.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Perform disk shrink context. */
export interface PerformDiskShrinkContext {
  /** The target disk shrink size in GigaBytes. */
  targetSizeGb: Long;
}

/** Backup context. */
export interface BackupContext {
  /** The identifier of the backup. */
  backupId: Long;
  /** This is always `sql#backupContext`. */
  kind: string;
}

/** Represents a SQL database on the Cloud SQL instance. */
export interface Database {
  /** This is always `sql#database`. */
  kind: string;
  /** The Cloud SQL charset value. */
  charset: string;
  /** The Cloud SQL collation value. */
  collation: string;
  /**
   * This field is deprecated and will be removed from a future version of the
   * API.
   */
  etag: string;
  /**
   * The name of the database in the Cloud SQL instance. This does not include
   * the project ID or instance name.
   */
  name: string;
  /** The name of the Cloud SQL instance. This does not include the project ID. */
  instance: string;
  /** The URI of this resource. */
  selfLink: string;
  /**
   * The project ID of the project containing the Cloud SQL database. The Google
   * apps domain is prefixed if applicable.
   */
  project: string;
  sqlserverDatabaseDetails?: SqlServerDatabaseDetails | undefined;
}

/** Represents a Sql Server database on the Cloud SQL instance. */
export interface SqlServerDatabaseDetails {
  /** The version of SQL Server with which the database is to be made compatible */
  compatibilityLevel: number;
  /** The recovery model of a SQL Server database */
  recoveryModel: string;
}

/** Database flags for Cloud SQL instances. */
export interface DatabaseFlags {
  /**
   * The name of the flag. These flags are passed at instance startup, so
   * include both server options and system variables. Flags are
   * specified with underscores, not hyphens. For more information, see
   * [Configuring Database Flags](https://cloud.google.com/sql/docs/mysql/flags)
   * in the Cloud SQL documentation.
   */
  name: string;
  /**
   * The value of the flag. Boolean flags are set to `on` for true
   * and `off` for false. This field must be omitted if the flag
   * doesn't take a value.
   */
  value: string;
}

/** MySQL-specific external server sync settings. */
export interface MySqlSyncConfig {
  /** Flags to use for the initial dump. */
  initialSyncFlags: SyncFlags[];
}

/**
 * Initial sync flags for certain Cloud SQL APIs.
 * Currently used for the MySQL external server initial dump.
 */
export interface SyncFlags {
  /** The name of the flag. */
  name: string;
  /**
   * The value of the flag. This field must be omitted if the flag
   * doesn't take a value.
   */
  value: string;
}

/** Reference to another Cloud SQL instance. */
export interface InstanceReference {
  /**
   * The name of the Cloud SQL instance being referenced.
   * This does not include the project ID.
   */
  name: string;
  /** The region of the Cloud SQL instance being referenced. */
  region: string;
  /**
   * The project ID of the Cloud SQL instance being referenced.
   * The default is the same project ID as the instance references it.
   */
  project: string;
}

/**
 * Read-replica configuration for connecting to the on-premises primary
 * instance.
 */
export interface DemoteMasterConfiguration {
  /** This is always `sql#demoteMasterConfiguration`. */
  kind: string;
  /**
   * MySQL specific configuration when replicating from a MySQL on-premises
   * primary instance. Replication configuration information such as the
   * username, password, certificates, and keys are not stored in the instance
   * metadata. The configuration information is used only to set up the
   * replication connection and is stored by MySQL in a file named
   * `master.info` in the data directory.
   */
  mysqlReplicaConfiguration: DemoteMasterMySqlReplicaConfiguration | undefined;
}

/** Read-replica configuration specific to MySQL databases. */
export interface DemoteMasterMySqlReplicaConfiguration {
  /** This is always `sql#demoteMasterMysqlReplicaConfiguration`. */
  kind: string;
  /** The username for the replication connection. */
  username: string;
  /** The password for the replication connection. */
  password: string;
  /**
   * PEM representation of the replica's private key. The corresponsing public
   * key is encoded in the client's certificate. The format of the replica's
   * private key can be either PKCS #1 or PKCS #8.
   */
  clientKey: string;
  /** PEM representation of the replica's x509 certificate. */
  clientCertificate: string;
  /** PEM representation of the trusted CA's x509 certificate. */
  caCertificate: string;
}

/** Database instance export context. */
export interface ExportContext {
  /**
   * The path to the file in Google Cloud Storage where the export will be
   * stored. The URI is in the form `gs://bucketName/fileName`. If the file
   * already exists, the request succeeds, but the operation fails. If
   * `fileType` is `SQL` and the filename ends with .gz,
   * the contents are compressed.
   */
  uri: string;
  /**
   * Databases to be exported. <br /> `MySQL instances:` If
   * `fileType` is `SQL` and no database is specified, all
   * databases are exported, except for the `mysql` system database.
   * If `fileType` is `CSV`, you can specify one database,
   * either by using this property or by using the
   * `csvExportOptions.selectQuery` property, which takes precedence
   * over this property. <br /> `PostgreSQL instances:` You must specify
   * one database to be exported. If `fileType` is `CSV`,
   * this database must match the one specified in the
   * `csvExportOptions.selectQuery` property. <br /> `SQL Server
   * instances:` You must specify one database to be exported, and the
   * `fileType` must be `BAK`.
   */
  databases: string[];
  /** This is always `sql#exportContext`. */
  kind: string;
  /** Options for exporting data as SQL statements. */
  sqlExportOptions:
    | ExportContext_SqlExportOptions
    | undefined;
  /**
   * Options for exporting data as CSV. `MySQL` and `PostgreSQL`
   * instances only.
   */
  csvExportOptions:
    | ExportContext_SqlCsvExportOptions
    | undefined;
  /** The file type for the specified uri. */
  fileType: SqlFileType;
  /** Option for export offload. */
  offload:
    | boolean
    | undefined;
  /** Options for exporting data as BAK files. */
  bakExportOptions: ExportContext_SqlBakExportOptions | undefined;
}

export interface ExportContext_SqlCsvExportOptions {
  /** The select query used to extract the data. */
  selectQuery: string;
  /**
   * Specifies the character that should appear before a data character that
   * needs to be escaped.
   */
  escapeCharacter: string;
  /** Specifies the quoting character to be used when a data value is quoted. */
  quoteCharacter: string;
  /**
   * Specifies the character that separates columns within each row (line) of
   * the file.
   */
  fieldsTerminatedBy: string;
  /**
   * This is used to separate lines. If a line does not contain all fields,
   * the rest of the columns are set to their default values.
   */
  linesTerminatedBy: string;
}

export interface ExportContext_SqlExportOptions {
  /**
   * Tables to export, or that were exported, from the specified database. If
   * you specify tables, specify one and only one database. For PostgreSQL
   * instances, you can specify only one table.
   */
  tables: string[];
  /** Export only schemas. */
  schemaOnly: boolean | undefined;
  mysqlExportOptions:
    | ExportContext_SqlExportOptions_MysqlExportOptions
    | undefined;
  /** Optional. The number of threads to use for parallel export. */
  threads:
    | number
    | undefined;
  /** Optional. Whether or not the export should be parallel. */
  parallel:
    | boolean
    | undefined;
  /** Optional. Options for exporting from a Cloud SQL for PostgreSQL instance. */
  postgresExportOptions: ExportContext_SqlExportOptions_PostgresExportOptions | undefined;
}

/** Options for exporting from MySQL. */
export interface ExportContext_SqlExportOptions_MysqlExportOptions {
  /**
   * Option to include SQL statement required to set up replication. If set
   * to `1`, the dump file includes a CHANGE MASTER TO statement with the
   * binary log coordinates, and --set-gtid-purged is set to ON. If set to
   * `2`, the CHANGE MASTER TO statement is written as a SQL comment and
   * has no effect. If set to any value other than `1`, --set-gtid-purged
   * is set to OFF.
   */
  masterData: number | undefined;
}

/** Options for exporting from a Cloud SQL for PostgreSQL instance. */
export interface ExportContext_SqlExportOptions_PostgresExportOptions {
  /**
   * Optional. Use this option to include DROP <object> SQL statements.
   * These statements are used to delete database objects before running the
   * import operation.
   */
  clean:
    | boolean
    | undefined;
  /**
   * Optional. Option to include an IF EXISTS SQL statement with each DROP
   * statement produced by clean.
   */
  ifExists: boolean | undefined;
}

/** Options for exporting BAK files (SQL Server-only) */
export interface ExportContext_SqlBakExportOptions {
  /** Whether or not the export should be striped. */
  striped:
    | boolean
    | undefined;
  /**
   * Option for specifying how many stripes to use for the export.
   * If blank, and the value of the striped field is true,
   * the number of stripes is automatically chosen.
   */
  stripeCount:
    | number
    | undefined;
  /** Type of this bak file will be export, FULL or DIFF, SQL Server only */
  bakType: BakType;
  /**
   * Deprecated: copy_only is deprecated. Use differential_base instead
   *
   * @deprecated
   */
  copyOnly:
    | boolean
    | undefined;
  /**
   * Whether or not the backup can be used as a differential base
   * copy_only backup can not be served as differential base
   */
  differentialBase: boolean | undefined;
}

/** Database instance import context. */
export interface ImportContext {
  /**
   * Path to the import file in Cloud Storage, in the form
   * `gs://bucketName/fileName`. Compressed gzip files (.gz) are supported
   * when `fileType` is `SQL`. The instance must have
   * write permissions to the bucket and read access to the file.
   */
  uri: string;
  /**
   * The target database for the import. If `fileType` is `SQL`, this field
   * is required only if the import file does not specify a database, and is
   * overridden by any database specification in the import file. If
   * `fileType` is `CSV`, one database must be specified.
   */
  database: string;
  /** This is always `sql#importContext`. */
  kind: string;
  /**
   * The file type for the specified uri.\`SQL`: The file
   * contains SQL statements. \`CSV`: The file contains CSV data.
   */
  fileType: SqlFileType;
  /** Options for importing data as CSV. */
  csvImportOptions:
    | ImportContext_SqlCsvImportOptions
    | undefined;
  /** The PostgreSQL user for this import operation. PostgreSQL instances only. */
  importUser: string;
  /** Import parameters specific to SQL Server .BAK files */
  bakImportOptions:
    | ImportContext_SqlBakImportOptions
    | undefined;
  /** Optional. Options for importing data from SQL statements. */
  sqlImportOptions: ImportContext_SqlImportOptions | undefined;
}

export interface ImportContext_SqlImportOptions {
  /** Optional. The number of threads to use for parallel import. */
  threads:
    | number
    | undefined;
  /** Optional. Whether or not the import should be parallel. */
  parallel:
    | boolean
    | undefined;
  /** Optional. Options for importing from a Cloud SQL for PostgreSQL instance. */
  postgresImportOptions: ImportContext_SqlImportOptions_PostgresImportOptions | undefined;
}

export interface ImportContext_SqlImportOptions_PostgresImportOptions {
  /**
   * Optional. The --clean flag for the pg_restore utility. This flag
   * applies only if you enabled Cloud SQL to import files in parallel.
   */
  clean:
    | boolean
    | undefined;
  /**
   * Optional. The --if-exists flag for the pg_restore utility. This flag
   * applies only if you enabled Cloud SQL to import files in parallel.
   */
  ifExists: boolean | undefined;
}

export interface ImportContext_SqlCsvImportOptions {
  /** The table to which CSV data is imported. */
  table: string;
  /**
   * The columns to which CSV data is imported. If not specified, all columns
   * of the database table are loaded with CSV data.
   */
  columns: string[];
  /**
   * Specifies the character that should appear before a data character that
   * needs to be escaped.
   */
  escapeCharacter: string;
  /** Specifies the quoting character to be used when a data value is quoted. */
  quoteCharacter: string;
  /**
   * Specifies the character that separates columns within each row (line) of
   * the file.
   */
  fieldsTerminatedBy: string;
  /**
   * This is used to separate lines. If a line does not contain all fields,
   * the rest of the columns are set to their default values.
   */
  linesTerminatedBy: string;
}

export interface ImportContext_SqlBakImportOptions {
  encryptionOptions:
    | ImportContext_SqlBakImportOptions_EncryptionOptions
    | undefined;
  /**
   * Whether or not the backup set being restored is striped.
   * Applies only to Cloud SQL for SQL Server.
   */
  striped:
    | boolean
    | undefined;
  /**
   * Whether or not the backup importing will restore database
   * with NORECOVERY option
   * Applies only to Cloud SQL for SQL Server.
   */
  noRecovery:
    | boolean
    | undefined;
  /**
   * Whether or not the backup importing request will just bring database
   * online without downloading Bak content only one of "no_recovery" and
   * "recovery_only" can be true otherwise error will return. Applies only to
   * Cloud SQL for SQL Server.
   */
  recoveryOnly:
    | boolean
    | undefined;
  /** Type of the bak content, FULL or DIFF */
  bakType: BakType;
  /**
   * Optional. The timestamp when the import should stop. This timestamp is in
   * the [RFC 3339](https://tools.ietf.org/html/rfc3339) format (for example,
   * `2023-10-01T16:19:00.094`). This field is equivalent to the STOPAT
   * keyword and applies to Cloud SQL for SQL Server only.
   */
  stopAt:
    | Date
    | undefined;
  /**
   * Optional. The marked transaction where the import should stop. This field
   * is equivalent to the STOPATMARK keyword and applies to Cloud SQL for SQL
   * Server only.
   */
  stopAtMark: string;
}

export interface ImportContext_SqlBakImportOptions_EncryptionOptions {
  /**
   * Path to the Certificate (.cer) in Cloud Storage, in the form
   * `gs://bucketName/fileName`. The instance must have
   * write permissions to the bucket and read access to the file.
   */
  certPath: string;
  /**
   * Path to the Certificate Private Key (.pvk)  in Cloud Storage, in the
   * form `gs://bucketName/fileName`. The instance must have
   * write permissions to the bucket and read access to the file.
   */
  pvkPath: string;
  /** Password that encrypts the private key */
  pvkPassword: string;
}

/** IP Management configuration. */
export interface IpConfiguration {
  /** Whether the instance is assigned a public IP address or not. */
  ipv4Enabled:
    | boolean
    | undefined;
  /**
   * The resource link for the VPC network from which the Cloud SQL instance is
   * accessible for private IP. For example,
   * `/projects/myProject/global/networks/default`. This setting can
   * be updated, but it cannot be removed after it is set.
   */
  privateNetwork: string;
  /**
   * Use `ssl_mode` instead.
   *
   * Whether SSL/TLS connections over IP are enforced.
   * If set to false, then allow both non-SSL/non-TLS and SSL/TLS connections.
   * For SSL/TLS connections, the client certificate won't be verified. If
   * set to true, then only allow connections encrypted with SSL/TLS and with
   * valid client certificates. If you want to enforce SSL/TLS without enforcing
   * the requirement for valid client certificates, then use the `ssl_mode` flag
   * instead of the `require_ssl` flag.
   */
  requireSsl:
    | boolean
    | undefined;
  /**
   * The list of external networks that are allowed to connect to the instance
   * using the IP. In 'CIDR' notation, also known as 'slash' notation (for
   * example: `157.197.200.0/24`).
   */
  authorizedNetworks: AclEntry[];
  /**
   * The name of the allocated ip range for the private ip Cloud SQL instance.
   * For example: "google-managed-services-default". If set, the instance ip
   * will be created in the allocated range. The range name must comply with
   * [RFC 1035](https://tools.ietf.org/html/rfc1035). Specifically, the name
   * must be 1-63 characters long and match the regular expression
   * `[a-z]([-a-z0-9]*[a-z0-9])?.`
   */
  allocatedIpRange: string;
  /**
   * Controls connectivity to private IP instances from Google services,
   * such as BigQuery.
   */
  enablePrivatePathForGoogleCloudServices:
    | boolean
    | undefined;
  /**
   * Specify how SSL/TLS is enforced in database connections. If you must use
   * the `require_ssl` flag for backward compatibility, then only the following
   * value pairs are valid:
   *
   * For PostgreSQL and MySQL:
   *
   * * `ssl_mode=ALLOW_UNENCRYPTED_AND_ENCRYPTED` and `require_ssl=false`
   * * `ssl_mode=ENCRYPTED_ONLY` and `require_ssl=false`
   * * `ssl_mode=TRUSTED_CLIENT_CERTIFICATE_REQUIRED` and `require_ssl=true`
   *
   * For SQL Server:
   *
   * * `ssl_mode=ALLOW_UNENCRYPTED_AND_ENCRYPTED` and `require_ssl=false`
   * * `ssl_mode=ENCRYPTED_ONLY` and `require_ssl=true`
   *
   * The value of `ssl_mode` has priority over the value of `require_ssl`.
   *
   * For example, for the pair `ssl_mode=ENCRYPTED_ONLY` and
   * `require_ssl=false`, `ssl_mode=ENCRYPTED_ONLY` means accept only SSL
   * connections, while `require_ssl=false` means accept both non-SSL
   * and SSL connections. In this case, MySQL and PostgreSQL databases respect
   * `ssl_mode` and accepts only SSL connections.
   */
  sslMode: IpConfiguration_SslMode;
  /** PSC settings for this instance. */
  pscConfig?:
    | PscConfig
    | undefined;
  /** Specify what type of CA is used for the server certificate. */
  serverCaMode?: IpConfiguration_CaMode | undefined;
}

/** The SSL options for database connections. */
export enum IpConfiguration_SslMode {
  /** SSL_MODE_UNSPECIFIED - The SSL mode is unknown. */
  SSL_MODE_UNSPECIFIED = 0,
  /**
   * ALLOW_UNENCRYPTED_AND_ENCRYPTED - Allow non-SSL/non-TLS and SSL/TLS connections.
   * For SSL connections to MySQL and PostgreSQL, the client certificate
   * isn't verified.
   *
   * When this value is used, the legacy `require_ssl` flag must be false or
   * cleared to avoid a conflict between the values of the two flags.
   */
  ALLOW_UNENCRYPTED_AND_ENCRYPTED = 1,
  /**
   * ENCRYPTED_ONLY - Only allow connections encrypted with SSL/TLS.
   * For SSL connections to MySQL and PostgreSQL, the client certificate
   * isn't verified.
   *
   * When this value is used, the legacy `require_ssl` flag must be false or
   * cleared to avoid a conflict between the values of the two flags.
   */
  ENCRYPTED_ONLY = 2,
  /**
   * TRUSTED_CLIENT_CERTIFICATE_REQUIRED - Only allow connections encrypted with SSL/TLS and with valid
   * client certificates.
   *
   * When this value is used, the legacy `require_ssl` flag must be true or
   * cleared to avoid the conflict between values of two flags.
   * PostgreSQL clients or users that connect using IAM database
   * authentication must use either the
   * [Cloud SQL Auth
   * Proxy](https://cloud.google.com/sql/docs/postgres/connect-auth-proxy) or
   * [Cloud SQL
   * Connectors](https://cloud.google.com/sql/docs/postgres/connect-connectors)
   * to enforce client identity verification.
   *
   * Only applicable to MySQL and PostgreSQL. Not applicable to SQL Server.
   */
  TRUSTED_CLIENT_CERTIFICATE_REQUIRED = 3,
  UNRECOGNIZED = -1,
}

export function ipConfiguration_SslModeFromJSON(object: any): IpConfiguration_SslMode {
  switch (object) {
    case 0:
    case "SSL_MODE_UNSPECIFIED":
      return IpConfiguration_SslMode.SSL_MODE_UNSPECIFIED;
    case 1:
    case "ALLOW_UNENCRYPTED_AND_ENCRYPTED":
      return IpConfiguration_SslMode.ALLOW_UNENCRYPTED_AND_ENCRYPTED;
    case 2:
    case "ENCRYPTED_ONLY":
      return IpConfiguration_SslMode.ENCRYPTED_ONLY;
    case 3:
    case "TRUSTED_CLIENT_CERTIFICATE_REQUIRED":
      return IpConfiguration_SslMode.TRUSTED_CLIENT_CERTIFICATE_REQUIRED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return IpConfiguration_SslMode.UNRECOGNIZED;
  }
}

export function ipConfiguration_SslModeToJSON(object: IpConfiguration_SslMode): string {
  switch (object) {
    case IpConfiguration_SslMode.SSL_MODE_UNSPECIFIED:
      return "SSL_MODE_UNSPECIFIED";
    case IpConfiguration_SslMode.ALLOW_UNENCRYPTED_AND_ENCRYPTED:
      return "ALLOW_UNENCRYPTED_AND_ENCRYPTED";
    case IpConfiguration_SslMode.ENCRYPTED_ONLY:
      return "ENCRYPTED_ONLY";
    case IpConfiguration_SslMode.TRUSTED_CLIENT_CERTIFICATE_REQUIRED:
      return "TRUSTED_CLIENT_CERTIFICATE_REQUIRED";
    case IpConfiguration_SslMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Various Certificate Authority (CA) modes for certificate signing. */
export enum IpConfiguration_CaMode {
  /** CA_MODE_UNSPECIFIED - CA mode is unknown. */
  CA_MODE_UNSPECIFIED = 0,
  /** GOOGLE_MANAGED_INTERNAL_CA - Google-managed self-signed internal CA. */
  GOOGLE_MANAGED_INTERNAL_CA = 1,
  /**
   * GOOGLE_MANAGED_CAS_CA - Google-managed regional CA part of root CA hierarchy hosted on Google
   * Cloud's Certificate Authority Service (CAS).
   */
  GOOGLE_MANAGED_CAS_CA = 2,
  UNRECOGNIZED = -1,
}

export function ipConfiguration_CaModeFromJSON(object: any): IpConfiguration_CaMode {
  switch (object) {
    case 0:
    case "CA_MODE_UNSPECIFIED":
      return IpConfiguration_CaMode.CA_MODE_UNSPECIFIED;
    case 1:
    case "GOOGLE_MANAGED_INTERNAL_CA":
      return IpConfiguration_CaMode.GOOGLE_MANAGED_INTERNAL_CA;
    case 2:
    case "GOOGLE_MANAGED_CAS_CA":
      return IpConfiguration_CaMode.GOOGLE_MANAGED_CAS_CA;
    case -1:
    case "UNRECOGNIZED":
    default:
      return IpConfiguration_CaMode.UNRECOGNIZED;
  }
}

export function ipConfiguration_CaModeToJSON(object: IpConfiguration_CaMode): string {
  switch (object) {
    case IpConfiguration_CaMode.CA_MODE_UNSPECIFIED:
      return "CA_MODE_UNSPECIFIED";
    case IpConfiguration_CaMode.GOOGLE_MANAGED_INTERNAL_CA:
      return "GOOGLE_MANAGED_INTERNAL_CA";
    case IpConfiguration_CaMode.GOOGLE_MANAGED_CAS_CA:
      return "GOOGLE_MANAGED_CAS_CA";
    case IpConfiguration_CaMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** PSC settings for a Cloud SQL instance. */
export interface PscConfig {
  /** Whether PSC connectivity is enabled for this instance. */
  pscEnabled?:
    | boolean
    | undefined;
  /**
   * Optional. The list of consumer projects that are allow-listed for PSC
   * connections to this instance. This instance can be connected to with PSC
   * from any network in these projects.
   *
   * Each consumer project in this list may be represented by a project number
   * (numeric) or by a project id (alphanumeric).
   */
  allowedConsumerProjects: string[];
}

/**
 * Preferred location. This specifies where a Cloud SQL instance is located.
 * Note that if the preferred location is not available, the instance will be
 * located as close as possible within the region. Only one location may be
 * specified.
 */
export interface LocationPreference {
  /**
   * The App Engine application to follow, it must be in the same region as the
   * Cloud SQL instance. WARNING: Changing this might restart the instance.
   *
   * @deprecated
   */
  followGaeApplication: string;
  /**
   * The preferred Compute Engine zone (for example: us-central1-a,
   * us-central1-b, etc.). WARNING: Changing this might restart the instance.
   */
  zone: string;
  /**
   * The preferred Compute Engine zone for the secondary/failover
   * (for example: us-central1-a, us-central1-b, etc.).
   * To disable this field, set it to 'no_secondary_zone'.
   */
  secondaryZone: string;
  /** This is always `sql#locationPreference`. */
  kind: string;
}

/**
 * Maintenance window. This specifies when a Cloud SQL instance is
 * restarted for system maintenance purposes.
 */
export interface MaintenanceWindow {
  /** Hour of day - 0 to 23. Specify in the UTC time zone. */
  hour:
    | number
    | undefined;
  /**
   * Day of week - `MONDAY`, `TUESDAY`, `WEDNESDAY`, `THURSDAY`, `FRIDAY`,
   * `SATURDAY`, or `SUNDAY`. Specify in the UTC time zone.
   * Returned in output as an integer, 1 to 7, where `1` equals Monday.
   */
  day:
    | number
    | undefined;
  /**
   * Maintenance timing settings: `canary`, `stable`, or `week5`.
   * For more information, see [About maintenance on Cloud SQL
   * instances](https://cloud.google.com/sql/docs/mysql/maintenance).
   */
  updateTrack: SqlUpdateTrack;
  /** This is always `sql#maintenanceWindow`. */
  kind: string;
}

/**
 * Deny maintenance Periods. This specifies a date range during when all CSA
 * rollout will be denied.
 */
export interface DenyMaintenancePeriod {
  /**
   * "deny maintenance period" start date. If the year of the start date is
   * empty, the year of the end date also must be empty. In this case, it means
   * the deny maintenance period recurs every year. The date is in format
   * yyyy-mm-dd i.e., 2020-11-01, or mm-dd, i.e., 11-01
   */
  startDate: string;
  /**
   * "deny maintenance period" end date. If the year of the end date is empty,
   * the year of the start date also must be empty. In this case, it means the
   * no maintenance interval recurs every year. The date is in format yyyy-mm-dd
   * i.e., 2020-11-01, or mm-dd, i.e., 11-01
   */
  endDate: string;
  /**
   * Time in UTC when the "deny maintenance period" starts on start_date and
   * ends on end_date. The time is in format: HH:mm:SS, i.e., 00:00:00
   */
  time: string;
}

/**
 * Insights configuration. This specifies when Cloud SQL Insights feature is
 * enabled and optional configuration.
 */
export interface InsightsConfig {
  /** Whether Query Insights feature is enabled. */
  queryInsightsEnabled: boolean;
  /** Whether Query Insights will record client address when enabled. */
  recordClientAddress: boolean;
  /**
   * Whether Query Insights will record application tags from query when
   * enabled.
   */
  recordApplicationTags: boolean;
  /**
   * Maximum query length stored in bytes. Default value: 1024 bytes.
   * Range: 256-4500 bytes. Query length more than this field value will be
   * truncated to this value. When unset, query length will be the default
   * value. Changing query length will restart the database.
   */
  queryStringLength:
    | number
    | undefined;
  /**
   * Number of query execution plans captured by Insights per minute
   * for all queries combined. Default is 5.
   */
  queryPlansPerMinute: number | undefined;
}

/** Read-replica configuration specific to MySQL databases. */
export interface MySqlReplicaConfiguration {
  /**
   * Path to a SQL dump file in Google Cloud Storage from which the replica
   * instance is to be created. The URI is in the form gs://bucketName/fileName.
   * Compressed gzip files (.gz) are also supported.
   * Dumps have the binlog co-ordinates from which replication
   * begins. This can be accomplished by setting --master-data to 1 when using
   * mysqldump.
   */
  dumpFilePath: string;
  /** The username for the replication connection. */
  username: string;
  /** The password for the replication connection. */
  password: string;
  /** Seconds to wait between connect retries. MySQL's default is 60 seconds. */
  connectRetryInterval:
    | number
    | undefined;
  /** Interval in milliseconds between replication heartbeats. */
  masterHeartbeatPeriod:
    | Long
    | undefined;
  /** PEM representation of the trusted CA's x509 certificate. */
  caCertificate: string;
  /** PEM representation of the replica's x509 certificate. */
  clientCertificate: string;
  /**
   * PEM representation of the replica's private key. The corresponsing public
   * key is encoded in the client's certificate.
   */
  clientKey: string;
  /** A list of permissible ciphers to use for SSL encryption. */
  sslCipher: string;
  /**
   * Whether or not to check the primary instance's Common Name value in the
   * certificate that it sends during the SSL handshake.
   */
  verifyServerCertificate:
    | boolean
    | undefined;
  /** This is always `sql#mysqlReplicaConfiguration`. */
  kind: string;
}

/** Disk encryption configuration for an instance. */
export interface DiskEncryptionConfiguration {
  /** Resource name of KMS key for disk encryption */
  kmsKeyName: string;
  /** This is always `sql#diskEncryptionConfiguration`. */
  kind: string;
}

/** Disk encryption status for an instance. */
export interface DiskEncryptionStatus {
  /** KMS key version used to encrypt the Cloud SQL instance resource */
  kmsKeyVersionName: string;
  /** This is always `sql#diskEncryptionStatus`. */
  kind: string;
}

/** Database instance IP mapping */
export interface IpMapping {
  /**
   * The type of this IP address. A `PRIMARY` address is a public address that
   * can accept incoming connections. A `PRIVATE` address is a private address
   * that can accept incoming connections. An `OUTGOING` address is the source
   * address of connections originating from the instance, if supported.
   */
  type: SqlIpAddressType;
  /** The IP address assigned. */
  ipAddress: string;
  /**
   * The due time for this IP to be retired in
   * [RFC 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`. This field is only available when
   * the IP is scheduled to be retired.
   */
  timeToRetire: Date | undefined;
}

/**
 * An Operation resource.&nbsp;For successful operations that return an
 * Operation resource, only the fields relevant to the operation are populated
 * in the resource.
 */
export interface Operation {
  /** This is always `sql#operation`. */
  kind: string;
  targetLink: string;
  /** The status of an operation. */
  status: Operation_SqlOperationStatus;
  /** The email address of the user who initiated this operation. */
  user: string;
  /**
   * The time this operation was enqueued in UTC timezone in [RFC
   * 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`.
   */
  insertTime:
    | Date
    | undefined;
  /**
   * The time this operation actually started in UTC timezone in [RFC
   * 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`.
   */
  startTime:
    | Date
    | undefined;
  /**
   * The time this operation finished in UTC timezone in [RFC
   * 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`.
   */
  endTime:
    | Date
    | undefined;
  /**
   * If errors occurred during processing of this operation, this field will be
   * populated.
   */
  error:
    | OperationErrors
    | undefined;
  /** An Admin API warning message. */
  apiWarning:
    | ApiWarning
    | undefined;
  /**
   * The type of the operation. Valid values are:
   * *  `CREATE`
   * *  `DELETE`
   * *  `UPDATE`
   * *  `RESTART`
   * *  `IMPORT`
   * *  `EXPORT`
   * *  `BACKUP_VOLUME`
   * *  `RESTORE_VOLUME`
   * *  `CREATE_USER`
   * *  `DELETE_USER`
   * *  `CREATE_DATABASE`
   * *  `DELETE_DATABASE`
   */
  operationType: Operation_SqlOperationType;
  /** The context for import operation, if applicable. */
  importContext:
    | ImportContext
    | undefined;
  /** The context for export operation, if applicable. */
  exportContext:
    | ExportContext
    | undefined;
  /** The context for backup operation, if applicable. */
  backupContext:
    | BackupContext
    | undefined;
  /**
   * An identifier that uniquely identifies the operation. You can use this
   * identifier to retrieve the Operations resource that has information about
   * the operation.
   */
  name: string;
  /** Name of the database instance related to this operation. */
  targetId: string;
  /** The URI of this resource. */
  selfLink: string;
  /** The project ID of the target instance related to this operation. */
  targetProject: string;
  /** The context for acquire SSRS lease operation, if applicable. */
  acquireSsrsLeaseContext: AcquireSsrsLeaseContext | undefined;
}

/** The type of Cloud SQL operation. */
export enum Operation_SqlOperationType {
  /** SQL_OPERATION_TYPE_UNSPECIFIED - Unknown operation type. */
  SQL_OPERATION_TYPE_UNSPECIFIED = 0,
  /** IMPORT - Imports data into a Cloud SQL instance. */
  IMPORT = 1,
  /**
   * EXPORT - Exports data from a Cloud SQL instance to a Cloud Storage
   * bucket.
   */
  EXPORT = 2,
  /** CREATE - Creates a new Cloud SQL instance. */
  CREATE = 3,
  /** UPDATE - Updates the settings of a Cloud SQL instance. */
  UPDATE = 4,
  /** DELETE - Deletes a Cloud SQL instance. */
  DELETE = 5,
  /** RESTART - Restarts the Cloud SQL instance. */
  RESTART = 6,
  /** @deprecated */
  BACKUP = 7,
  /** @deprecated */
  SNAPSHOT = 8,
  /** BACKUP_VOLUME - Performs instance backup. */
  BACKUP_VOLUME = 9,
  /** DELETE_VOLUME - Deletes an instance backup. */
  DELETE_VOLUME = 10,
  /** RESTORE_VOLUME - Restores an instance backup. */
  RESTORE_VOLUME = 11,
  /** INJECT_USER - Injects a privileged user in mysql for MOB instances. */
  INJECT_USER = 12,
  /** CLONE - Clones a Cloud SQL instance. */
  CLONE = 14,
  /** STOP_REPLICA - Stops replication on a Cloud SQL read replica instance. */
  STOP_REPLICA = 15,
  /** START_REPLICA - Starts replication on a Cloud SQL read replica instance. */
  START_REPLICA = 16,
  /** PROMOTE_REPLICA - Promotes a Cloud SQL replica instance. */
  PROMOTE_REPLICA = 17,
  /** CREATE_REPLICA - Creates a Cloud SQL replica instance. */
  CREATE_REPLICA = 18,
  /** CREATE_USER - Creates a new user in a Cloud SQL instance. */
  CREATE_USER = 19,
  /** DELETE_USER - Deletes a user from a Cloud SQL instance. */
  DELETE_USER = 20,
  /** UPDATE_USER - Updates an existing user in a Cloud SQL instance. */
  UPDATE_USER = 21,
  /** CREATE_DATABASE - Creates a database in the Cloud SQL instance. */
  CREATE_DATABASE = 22,
  /** DELETE_DATABASE - Deletes a database in the Cloud SQL instance. */
  DELETE_DATABASE = 23,
  /** UPDATE_DATABASE - Updates a database in the Cloud SQL instance. */
  UPDATE_DATABASE = 24,
  /**
   * FAILOVER - Performs failover of an HA-enabled Cloud SQL
   * failover replica.
   */
  FAILOVER = 25,
  /** DELETE_BACKUP - Deletes the backup taken by a backup run. */
  DELETE_BACKUP = 26,
  RECREATE_REPLICA = 27,
  /** TRUNCATE_LOG - Truncates a general or slow log table in MySQL. */
  TRUNCATE_LOG = 28,
  /**
   * DEMOTE_MASTER - Demotes the stand-alone instance to be a Cloud SQL
   * read replica for an external database server.
   */
  DEMOTE_MASTER = 29,
  /**
   * MAINTENANCE - Indicates that the instance is currently in maintenance. Maintenance
   * typically causes the instance to be unavailable for 1-3 minutes.
   */
  MAINTENANCE = 30,
  /**
   * ENABLE_PRIVATE_IP - This field is deprecated, and will be removed in future version of API.
   *
   * @deprecated
   */
  ENABLE_PRIVATE_IP = 31,
  /** @deprecated */
  DEFER_MAINTENANCE = 32,
  /**
   * CREATE_CLONE - Creates clone instance.
   *
   * @deprecated
   */
  CREATE_CLONE = 33,
  /** RESCHEDULE_MAINTENANCE - Reschedule maintenance to another time. */
  RESCHEDULE_MAINTENANCE = 34,
  /**
   * START_EXTERNAL_SYNC - Starts external sync of a Cloud SQL EM replica to an external primary
   * instance.
   */
  START_EXTERNAL_SYNC = 35,
  /** LOG_CLEANUP - Recovers logs from an instance's old data disk. */
  LOG_CLEANUP = 36,
  /**
   * AUTO_RESTART - Performs auto-restart of an HA-enabled Cloud SQL database for auto
   * recovery.
   */
  AUTO_RESTART = 37,
  /** REENCRYPT - Re-encrypts CMEK instances with latest key version. */
  REENCRYPT = 38,
  /**
   * SWITCHOVER - Switches the roles of the primary and replica pair. The target instance
   * should be the replica.
   */
  SWITCHOVER = 39,
  /** ACQUIRE_SSRS_LEASE - Acquire a lease for the setup of SQL Server Reporting Services (SSRS). */
  ACQUIRE_SSRS_LEASE = 42,
  /** RELEASE_SSRS_LEASE - Release a lease for the setup of SQL Server Reporting Services (SSRS). */
  RELEASE_SSRS_LEASE = 43,
  /**
   * RECONFIGURE_OLD_PRIMARY - Reconfigures old primary after a promote replica operation. Effect of a
   * promote operation to the old primary is executed in this operation,
   * asynchronously from the promote replica operation executed to the
   * replica.
   */
  RECONFIGURE_OLD_PRIMARY = 44,
  /**
   * CLUSTER_MAINTENANCE - Indicates that the instance, its read replicas, and its cascading
   * replicas are in maintenance. Maintenance typically gets initiated on
   * groups of replicas first, followed by the primary instance. For each
   * instance, maintenance typically causes the instance to be unavailable for
   * 1-3 minutes.
   */
  CLUSTER_MAINTENANCE = 45,
  /**
   * SELF_SERVICE_MAINTENANCE - Indicates that the instance (and any of its replicas) are currently in
   * maintenance. This is initiated as a self-service request by using SSM.
   * Maintenance typically causes the instance to be unavailable for 1-3
   * minutes.
   */
  SELF_SERVICE_MAINTENANCE = 46,
  /**
   * SWITCHOVER_TO_REPLICA - Switches a primary instance to a replica. This operation runs as part of
   * a switchover operation to the original primary instance.
   */
  SWITCHOVER_TO_REPLICA = 47,
  /** MAJOR_VERSION_UPGRADE - Updates the major version of a Cloud SQL instance. */
  MAJOR_VERSION_UPGRADE = 48,
  UNRECOGNIZED = -1,
}

export function operation_SqlOperationTypeFromJSON(object: any): Operation_SqlOperationType {
  switch (object) {
    case 0:
    case "SQL_OPERATION_TYPE_UNSPECIFIED":
      return Operation_SqlOperationType.SQL_OPERATION_TYPE_UNSPECIFIED;
    case 1:
    case "IMPORT":
      return Operation_SqlOperationType.IMPORT;
    case 2:
    case "EXPORT":
      return Operation_SqlOperationType.EXPORT;
    case 3:
    case "CREATE":
      return Operation_SqlOperationType.CREATE;
    case 4:
    case "UPDATE":
      return Operation_SqlOperationType.UPDATE;
    case 5:
    case "DELETE":
      return Operation_SqlOperationType.DELETE;
    case 6:
    case "RESTART":
      return Operation_SqlOperationType.RESTART;
    case 7:
    case "BACKUP":
      return Operation_SqlOperationType.BACKUP;
    case 8:
    case "SNAPSHOT":
      return Operation_SqlOperationType.SNAPSHOT;
    case 9:
    case "BACKUP_VOLUME":
      return Operation_SqlOperationType.BACKUP_VOLUME;
    case 10:
    case "DELETE_VOLUME":
      return Operation_SqlOperationType.DELETE_VOLUME;
    case 11:
    case "RESTORE_VOLUME":
      return Operation_SqlOperationType.RESTORE_VOLUME;
    case 12:
    case "INJECT_USER":
      return Operation_SqlOperationType.INJECT_USER;
    case 14:
    case "CLONE":
      return Operation_SqlOperationType.CLONE;
    case 15:
    case "STOP_REPLICA":
      return Operation_SqlOperationType.STOP_REPLICA;
    case 16:
    case "START_REPLICA":
      return Operation_SqlOperationType.START_REPLICA;
    case 17:
    case "PROMOTE_REPLICA":
      return Operation_SqlOperationType.PROMOTE_REPLICA;
    case 18:
    case "CREATE_REPLICA":
      return Operation_SqlOperationType.CREATE_REPLICA;
    case 19:
    case "CREATE_USER":
      return Operation_SqlOperationType.CREATE_USER;
    case 20:
    case "DELETE_USER":
      return Operation_SqlOperationType.DELETE_USER;
    case 21:
    case "UPDATE_USER":
      return Operation_SqlOperationType.UPDATE_USER;
    case 22:
    case "CREATE_DATABASE":
      return Operation_SqlOperationType.CREATE_DATABASE;
    case 23:
    case "DELETE_DATABASE":
      return Operation_SqlOperationType.DELETE_DATABASE;
    case 24:
    case "UPDATE_DATABASE":
      return Operation_SqlOperationType.UPDATE_DATABASE;
    case 25:
    case "FAILOVER":
      return Operation_SqlOperationType.FAILOVER;
    case 26:
    case "DELETE_BACKUP":
      return Operation_SqlOperationType.DELETE_BACKUP;
    case 27:
    case "RECREATE_REPLICA":
      return Operation_SqlOperationType.RECREATE_REPLICA;
    case 28:
    case "TRUNCATE_LOG":
      return Operation_SqlOperationType.TRUNCATE_LOG;
    case 29:
    case "DEMOTE_MASTER":
      return Operation_SqlOperationType.DEMOTE_MASTER;
    case 30:
    case "MAINTENANCE":
      return Operation_SqlOperationType.MAINTENANCE;
    case 31:
    case "ENABLE_PRIVATE_IP":
      return Operation_SqlOperationType.ENABLE_PRIVATE_IP;
    case 32:
    case "DEFER_MAINTENANCE":
      return Operation_SqlOperationType.DEFER_MAINTENANCE;
    case 33:
    case "CREATE_CLONE":
      return Operation_SqlOperationType.CREATE_CLONE;
    case 34:
    case "RESCHEDULE_MAINTENANCE":
      return Operation_SqlOperationType.RESCHEDULE_MAINTENANCE;
    case 35:
    case "START_EXTERNAL_SYNC":
      return Operation_SqlOperationType.START_EXTERNAL_SYNC;
    case 36:
    case "LOG_CLEANUP":
      return Operation_SqlOperationType.LOG_CLEANUP;
    case 37:
    case "AUTO_RESTART":
      return Operation_SqlOperationType.AUTO_RESTART;
    case 38:
    case "REENCRYPT":
      return Operation_SqlOperationType.REENCRYPT;
    case 39:
    case "SWITCHOVER":
      return Operation_SqlOperationType.SWITCHOVER;
    case 42:
    case "ACQUIRE_SSRS_LEASE":
      return Operation_SqlOperationType.ACQUIRE_SSRS_LEASE;
    case 43:
    case "RELEASE_SSRS_LEASE":
      return Operation_SqlOperationType.RELEASE_SSRS_LEASE;
    case 44:
    case "RECONFIGURE_OLD_PRIMARY":
      return Operation_SqlOperationType.RECONFIGURE_OLD_PRIMARY;
    case 45:
    case "CLUSTER_MAINTENANCE":
      return Operation_SqlOperationType.CLUSTER_MAINTENANCE;
    case 46:
    case "SELF_SERVICE_MAINTENANCE":
      return Operation_SqlOperationType.SELF_SERVICE_MAINTENANCE;
    case 47:
    case "SWITCHOVER_TO_REPLICA":
      return Operation_SqlOperationType.SWITCHOVER_TO_REPLICA;
    case 48:
    case "MAJOR_VERSION_UPGRADE":
      return Operation_SqlOperationType.MAJOR_VERSION_UPGRADE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Operation_SqlOperationType.UNRECOGNIZED;
  }
}

export function operation_SqlOperationTypeToJSON(object: Operation_SqlOperationType): string {
  switch (object) {
    case Operation_SqlOperationType.SQL_OPERATION_TYPE_UNSPECIFIED:
      return "SQL_OPERATION_TYPE_UNSPECIFIED";
    case Operation_SqlOperationType.IMPORT:
      return "IMPORT";
    case Operation_SqlOperationType.EXPORT:
      return "EXPORT";
    case Operation_SqlOperationType.CREATE:
      return "CREATE";
    case Operation_SqlOperationType.UPDATE:
      return "UPDATE";
    case Operation_SqlOperationType.DELETE:
      return "DELETE";
    case Operation_SqlOperationType.RESTART:
      return "RESTART";
    case Operation_SqlOperationType.BACKUP:
      return "BACKUP";
    case Operation_SqlOperationType.SNAPSHOT:
      return "SNAPSHOT";
    case Operation_SqlOperationType.BACKUP_VOLUME:
      return "BACKUP_VOLUME";
    case Operation_SqlOperationType.DELETE_VOLUME:
      return "DELETE_VOLUME";
    case Operation_SqlOperationType.RESTORE_VOLUME:
      return "RESTORE_VOLUME";
    case Operation_SqlOperationType.INJECT_USER:
      return "INJECT_USER";
    case Operation_SqlOperationType.CLONE:
      return "CLONE";
    case Operation_SqlOperationType.STOP_REPLICA:
      return "STOP_REPLICA";
    case Operation_SqlOperationType.START_REPLICA:
      return "START_REPLICA";
    case Operation_SqlOperationType.PROMOTE_REPLICA:
      return "PROMOTE_REPLICA";
    case Operation_SqlOperationType.CREATE_REPLICA:
      return "CREATE_REPLICA";
    case Operation_SqlOperationType.CREATE_USER:
      return "CREATE_USER";
    case Operation_SqlOperationType.DELETE_USER:
      return "DELETE_USER";
    case Operation_SqlOperationType.UPDATE_USER:
      return "UPDATE_USER";
    case Operation_SqlOperationType.CREATE_DATABASE:
      return "CREATE_DATABASE";
    case Operation_SqlOperationType.DELETE_DATABASE:
      return "DELETE_DATABASE";
    case Operation_SqlOperationType.UPDATE_DATABASE:
      return "UPDATE_DATABASE";
    case Operation_SqlOperationType.FAILOVER:
      return "FAILOVER";
    case Operation_SqlOperationType.DELETE_BACKUP:
      return "DELETE_BACKUP";
    case Operation_SqlOperationType.RECREATE_REPLICA:
      return "RECREATE_REPLICA";
    case Operation_SqlOperationType.TRUNCATE_LOG:
      return "TRUNCATE_LOG";
    case Operation_SqlOperationType.DEMOTE_MASTER:
      return "DEMOTE_MASTER";
    case Operation_SqlOperationType.MAINTENANCE:
      return "MAINTENANCE";
    case Operation_SqlOperationType.ENABLE_PRIVATE_IP:
      return "ENABLE_PRIVATE_IP";
    case Operation_SqlOperationType.DEFER_MAINTENANCE:
      return "DEFER_MAINTENANCE";
    case Operation_SqlOperationType.CREATE_CLONE:
      return "CREATE_CLONE";
    case Operation_SqlOperationType.RESCHEDULE_MAINTENANCE:
      return "RESCHEDULE_MAINTENANCE";
    case Operation_SqlOperationType.START_EXTERNAL_SYNC:
      return "START_EXTERNAL_SYNC";
    case Operation_SqlOperationType.LOG_CLEANUP:
      return "LOG_CLEANUP";
    case Operation_SqlOperationType.AUTO_RESTART:
      return "AUTO_RESTART";
    case Operation_SqlOperationType.REENCRYPT:
      return "REENCRYPT";
    case Operation_SqlOperationType.SWITCHOVER:
      return "SWITCHOVER";
    case Operation_SqlOperationType.ACQUIRE_SSRS_LEASE:
      return "ACQUIRE_SSRS_LEASE";
    case Operation_SqlOperationType.RELEASE_SSRS_LEASE:
      return "RELEASE_SSRS_LEASE";
    case Operation_SqlOperationType.RECONFIGURE_OLD_PRIMARY:
      return "RECONFIGURE_OLD_PRIMARY";
    case Operation_SqlOperationType.CLUSTER_MAINTENANCE:
      return "CLUSTER_MAINTENANCE";
    case Operation_SqlOperationType.SELF_SERVICE_MAINTENANCE:
      return "SELF_SERVICE_MAINTENANCE";
    case Operation_SqlOperationType.SWITCHOVER_TO_REPLICA:
      return "SWITCHOVER_TO_REPLICA";
    case Operation_SqlOperationType.MAJOR_VERSION_UPGRADE:
      return "MAJOR_VERSION_UPGRADE";
    case Operation_SqlOperationType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The status of an operation. */
export enum Operation_SqlOperationStatus {
  /** SQL_OPERATION_STATUS_UNSPECIFIED - The state of the operation is unknown. */
  SQL_OPERATION_STATUS_UNSPECIFIED = 0,
  /** PENDING - The operation has been queued, but has not started yet. */
  PENDING = 1,
  /** RUNNING - The operation is running. */
  RUNNING = 2,
  /** DONE - The operation completed. */
  DONE = 3,
  UNRECOGNIZED = -1,
}

export function operation_SqlOperationStatusFromJSON(object: any): Operation_SqlOperationStatus {
  switch (object) {
    case 0:
    case "SQL_OPERATION_STATUS_UNSPECIFIED":
      return Operation_SqlOperationStatus.SQL_OPERATION_STATUS_UNSPECIFIED;
    case 1:
    case "PENDING":
      return Operation_SqlOperationStatus.PENDING;
    case 2:
    case "RUNNING":
      return Operation_SqlOperationStatus.RUNNING;
    case 3:
    case "DONE":
      return Operation_SqlOperationStatus.DONE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Operation_SqlOperationStatus.UNRECOGNIZED;
  }
}

export function operation_SqlOperationStatusToJSON(object: Operation_SqlOperationStatus): string {
  switch (object) {
    case Operation_SqlOperationStatus.SQL_OPERATION_STATUS_UNSPECIFIED:
      return "SQL_OPERATION_STATUS_UNSPECIFIED";
    case Operation_SqlOperationStatus.PENDING:
      return "PENDING";
    case Operation_SqlOperationStatus.RUNNING:
      return "RUNNING";
    case Operation_SqlOperationStatus.DONE:
      return "DONE";
    case Operation_SqlOperationStatus.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Database instance operation error. */
export interface OperationError {
  /** This is always `sql#operationError`. */
  kind: string;
  /** Identifies the specific error that occurred. */
  code: string;
  /** Additional information about the error encountered. */
  message: string;
}

/** Database instance operation errors list wrapper. */
export interface OperationErrors {
  /** This is always `sql#operationErrors`. */
  kind: string;
  /** The list of errors encountered while processing this operation. */
  errors: OperationError[];
}

/** Database instance local user password validation policy */
export interface PasswordValidationPolicy {
  /** Minimum number of characters allowed. */
  minLength:
    | number
    | undefined;
  /** The complexity of the password. */
  complexity: PasswordValidationPolicy_Complexity;
  /** Number of previous passwords that cannot be reused. */
  reuseInterval:
    | number
    | undefined;
  /** Disallow username as a part of the password. */
  disallowUsernameSubstring:
    | boolean
    | undefined;
  /**
   * Minimum interval after which the password can be changed. This flag is only
   * supported for PostgreSQL.
   */
  passwordChangeInterval:
    | Duration
    | undefined;
  /** Whether the password policy is enabled or not. */
  enablePasswordPolicy:
    | boolean
    | undefined;
  /**
   * This field is deprecated and will be removed in a future version of the
   * API.
   *
   * @deprecated
   */
  disallowCompromisedCredentials: boolean | undefined;
}

/** The complexity choices of the password. */
export enum PasswordValidationPolicy_Complexity {
  /** COMPLEXITY_UNSPECIFIED - Complexity check is not specified. */
  COMPLEXITY_UNSPECIFIED = 0,
  /**
   * COMPLEXITY_DEFAULT - A combination of lowercase, uppercase, numeric, and non-alphanumeric
   * characters.
   */
  COMPLEXITY_DEFAULT = 1,
  UNRECOGNIZED = -1,
}

export function passwordValidationPolicy_ComplexityFromJSON(object: any): PasswordValidationPolicy_Complexity {
  switch (object) {
    case 0:
    case "COMPLEXITY_UNSPECIFIED":
      return PasswordValidationPolicy_Complexity.COMPLEXITY_UNSPECIFIED;
    case 1:
    case "COMPLEXITY_DEFAULT":
      return PasswordValidationPolicy_Complexity.COMPLEXITY_DEFAULT;
    case -1:
    case "UNRECOGNIZED":
    default:
      return PasswordValidationPolicy_Complexity.UNRECOGNIZED;
  }
}

export function passwordValidationPolicy_ComplexityToJSON(object: PasswordValidationPolicy_Complexity): string {
  switch (object) {
    case PasswordValidationPolicy_Complexity.COMPLEXITY_UNSPECIFIED:
      return "COMPLEXITY_UNSPECIFIED";
    case PasswordValidationPolicy_Complexity.COMPLEXITY_DEFAULT:
      return "COMPLEXITY_DEFAULT";
    case PasswordValidationPolicy_Complexity.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Data cache configurations. */
export interface DataCacheConfig {
  /** Whether data cache is enabled for the instance. */
  dataCacheEnabled: boolean;
}

/** Database instance settings. */
export interface Settings {
  /**
   * The version of instance settings. This is a required field for update
   * method to make sure concurrent updates are handled properly. During update,
   * use the most recent settingsVersion value for this instance and do not try
   * to update this value.
   */
  settingsVersion:
    | Long
    | undefined;
  /**
   * The App Engine app IDs that can access this instance.
   * (Deprecated) Applied to First Generation instances only.
   *
   * @deprecated
   */
  authorizedGaeApplications: string[];
  /**
   * The tier (or machine type) for this instance, for example
   * `db-custom-1-3840`. WARNING: Changing this restarts the instance.
   */
  tier: string;
  /** This is always `sql#settings`. */
  kind: string;
  /**
   * User-provided labels, represented as a dictionary where each label is a
   * single key value pair.
   */
  userLabels: { [key: string]: string };
  /**
   * Availability type. Potential values:
   * *  `ZONAL`: The instance serves data from only one zone. Outages in that
   * zone affect data accessibility.
   * *  `REGIONAL`: The instance can serve data from more than one zone in a
   * region (it is highly available)./
   *
   * For more information, see [Overview of the High Availability
   * Configuration](https://cloud.google.com/sql/docs/mysql/high-availability).
   */
  availabilityType: SqlAvailabilityType;
  /**
   * The pricing plan for this instance. This can be either `PER_USE` or
   * `PACKAGE`. Only `PER_USE` is supported for Second Generation instances.
   */
  pricingPlan: SqlPricingPlan;
  /**
   * The type of replication this instance uses. This can be either
   * `ASYNCHRONOUS` or `SYNCHRONOUS`. (Deprecated) This property was only
   * applicable to First Generation instances.
   *
   * @deprecated
   */
  replicationType: SqlReplicationType;
  /**
   * The maximum size to which storage capacity can be automatically increased.
   * The default value is 0, which specifies that there is no limit.
   */
  storageAutoResizeLimit:
    | Long
    | undefined;
  /**
   * The activation policy specifies when the instance is activated; it is
   * applicable only when the instance state is RUNNABLE. Valid values:
   * *  `ALWAYS`: The instance is on, and remains so even in the absence of
   * connection requests.
   * *  `NEVER`: The instance is off; it is not activated, even if a
   * connection request arrives.
   */
  activationPolicy: Settings_SqlActivationPolicy;
  /**
   * The settings for IP Management. This allows to enable or disable the
   * instance IP and manage which external networks can connect to the instance.
   * The IPv4 address cannot be disabled for Second Generation instances.
   */
  ipConfiguration:
    | IpConfiguration
    | undefined;
  /**
   * Configuration to increase storage size automatically. The default value is
   * true.
   */
  storageAutoResize:
    | boolean
    | undefined;
  /**
   * The location preference settings. This allows the instance to be located as
   * near as possible to either an App Engine app or Compute Engine zone for
   * better performance. App Engine co-location was only applicable to First
   * Generation instances.
   */
  locationPreference:
    | LocationPreference
    | undefined;
  /** The database flags passed to the instance at startup. */
  databaseFlags: DatabaseFlags[];
  /**
   * The type of data disk: `PD_SSD` (default) or `PD_HDD`. Not used for
   * First Generation instances.
   */
  dataDiskType: SqlDataDiskType;
  /**
   * The maintenance window for this instance. This specifies when the instance
   * can be restarted for maintenance purposes.
   */
  maintenanceWindow:
    | MaintenanceWindow
    | undefined;
  /** The daily backup configuration for the instance. */
  backupConfiguration:
    | BackupConfiguration
    | undefined;
  /**
   * Configuration specific to read replica instances. Indicates whether
   * replication is enabled or not. WARNING: Changing this restarts the
   * instance.
   */
  databaseReplicationEnabled:
    | boolean
    | undefined;
  /**
   * Configuration specific to read replica instances. Indicates whether
   * database flags for crash-safe replication are enabled. This property was
   * only applicable to First Generation instances.
   *
   * @deprecated
   */
  crashSafeReplicationEnabled:
    | boolean
    | undefined;
  /** The size of data disk, in GB. The data disk size minimum is 10GB. */
  dataDiskSizeGb:
    | Long
    | undefined;
  /** Active Directory configuration, relevant only for Cloud SQL for SQL Server. */
  activeDirectoryConfig:
    | SqlActiveDirectoryConfig
    | undefined;
  /** The name of server Instance collation. */
  collation: string;
  /** Deny maintenance periods */
  denyMaintenancePeriods: DenyMaintenancePeriod[];
  /** Insights configuration, for now relevant only for Postgres. */
  insightsConfig:
    | InsightsConfig
    | undefined;
  /** The local user password validation policy of the instance. */
  passwordValidationPolicy:
    | PasswordValidationPolicy
    | undefined;
  /** SQL Server specific audit configuration. */
  sqlServerAuditConfig:
    | SqlServerAuditConfig
    | undefined;
  /** Optional. The edition of the instance. */
  edition: Settings_Edition;
  /**
   * Specifies if connections must use Cloud SQL connectors.
   * Option values include the following: `NOT_REQUIRED` (Cloud SQL instances
   * can be connected without Cloud SQL
   * Connectors) and `REQUIRED` (Only allow connections that use Cloud SQL
   * Connectors).
   *
   * Note that using REQUIRED disables all existing authorized networks. If
   * this field is not specified when creating a new instance, NOT_REQUIRED is
   * used. If this field is not specified when patching or updating an existing
   * instance, it is left unchanged in the instance.
   */
  connectorEnforcement: Settings_ConnectorEnforcement;
  /** Configuration to protect against accidental instance deletion. */
  deletionProtectionEnabled:
    | boolean
    | undefined;
  /** Server timezone, relevant only for Cloud SQL for SQL Server. */
  timeZone: string;
  /**
   * Specifies advanced machine configuration for the instances relevant only
   * for SQL Server.
   */
  advancedMachineFeatures:
    | AdvancedMachineFeatures
    | undefined;
  /** Configuration for data cache. */
  dataCacheConfig:
    | DataCacheConfig
    | undefined;
  /**
   * Optional. When this parameter is set to true, Cloud SQL instances can
   * connect to Vertex AI to pass requests for real-time predictions and
   * insights to the AI. The default value is false. This applies only to Cloud
   * SQL for PostgreSQL instances.
   */
  enableGoogleMlIntegration:
    | boolean
    | undefined;
  /**
   * Optional. By default, Cloud SQL instances have schema extraction disabled
   * for Dataplex. When this parameter is set to true, schema extraction for
   * Dataplex on Cloud SQL instances is activated.
   */
  enableDataplexIntegration: boolean | undefined;
}

/** Specifies when the instance is activated. */
export enum Settings_SqlActivationPolicy {
  /** SQL_ACTIVATION_POLICY_UNSPECIFIED - Unknown activation plan. */
  SQL_ACTIVATION_POLICY_UNSPECIFIED = 0,
  /** ALWAYS - The instance is always up and running. */
  ALWAYS = 1,
  /** NEVER - The instance never starts. */
  NEVER = 2,
  /**
   * ON_DEMAND - The instance starts upon receiving requests.
   *
   * @deprecated
   */
  ON_DEMAND = 3,
  UNRECOGNIZED = -1,
}

export function settings_SqlActivationPolicyFromJSON(object: any): Settings_SqlActivationPolicy {
  switch (object) {
    case 0:
    case "SQL_ACTIVATION_POLICY_UNSPECIFIED":
      return Settings_SqlActivationPolicy.SQL_ACTIVATION_POLICY_UNSPECIFIED;
    case 1:
    case "ALWAYS":
      return Settings_SqlActivationPolicy.ALWAYS;
    case 2:
    case "NEVER":
      return Settings_SqlActivationPolicy.NEVER;
    case 3:
    case "ON_DEMAND":
      return Settings_SqlActivationPolicy.ON_DEMAND;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Settings_SqlActivationPolicy.UNRECOGNIZED;
  }
}

export function settings_SqlActivationPolicyToJSON(object: Settings_SqlActivationPolicy): string {
  switch (object) {
    case Settings_SqlActivationPolicy.SQL_ACTIVATION_POLICY_UNSPECIFIED:
      return "SQL_ACTIVATION_POLICY_UNSPECIFIED";
    case Settings_SqlActivationPolicy.ALWAYS:
      return "ALWAYS";
    case Settings_SqlActivationPolicy.NEVER:
      return "NEVER";
    case Settings_SqlActivationPolicy.ON_DEMAND:
      return "ON_DEMAND";
    case Settings_SqlActivationPolicy.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The edition of the instance, can be ENTERPRISE or ENTERPRISE_PLUS. */
export enum Settings_Edition {
  /** EDITION_UNSPECIFIED - The instance did not specify the edition. */
  EDITION_UNSPECIFIED = 0,
  /** ENTERPRISE - The instance is an enterprise edition. */
  ENTERPRISE = 2,
  /** ENTERPRISE_PLUS - The instance is an Enterprise Plus edition. */
  ENTERPRISE_PLUS = 3,
  UNRECOGNIZED = -1,
}

export function settings_EditionFromJSON(object: any): Settings_Edition {
  switch (object) {
    case 0:
    case "EDITION_UNSPECIFIED":
      return Settings_Edition.EDITION_UNSPECIFIED;
    case 2:
    case "ENTERPRISE":
      return Settings_Edition.ENTERPRISE;
    case 3:
    case "ENTERPRISE_PLUS":
      return Settings_Edition.ENTERPRISE_PLUS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Settings_Edition.UNRECOGNIZED;
  }
}

export function settings_EditionToJSON(object: Settings_Edition): string {
  switch (object) {
    case Settings_Edition.EDITION_UNSPECIFIED:
      return "EDITION_UNSPECIFIED";
    case Settings_Edition.ENTERPRISE:
      return "ENTERPRISE";
    case Settings_Edition.ENTERPRISE_PLUS:
      return "ENTERPRISE_PLUS";
    case Settings_Edition.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The options for enforcing Cloud SQL connectors in the instance. */
export enum Settings_ConnectorEnforcement {
  /** CONNECTOR_ENFORCEMENT_UNSPECIFIED - The requirement for Cloud SQL connectors is unknown. */
  CONNECTOR_ENFORCEMENT_UNSPECIFIED = 0,
  /** NOT_REQUIRED - Do not require Cloud SQL connectors. */
  NOT_REQUIRED = 1,
  /**
   * REQUIRED - Require all connections to use Cloud SQL connectors, including the
   * Cloud SQL Auth Proxy and Cloud SQL Java, Python, and Go connectors.
   * Note: This disables all existing authorized networks.
   */
  REQUIRED = 2,
  UNRECOGNIZED = -1,
}

export function settings_ConnectorEnforcementFromJSON(object: any): Settings_ConnectorEnforcement {
  switch (object) {
    case 0:
    case "CONNECTOR_ENFORCEMENT_UNSPECIFIED":
      return Settings_ConnectorEnforcement.CONNECTOR_ENFORCEMENT_UNSPECIFIED;
    case 1:
    case "NOT_REQUIRED":
      return Settings_ConnectorEnforcement.NOT_REQUIRED;
    case 2:
    case "REQUIRED":
      return Settings_ConnectorEnforcement.REQUIRED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Settings_ConnectorEnforcement.UNRECOGNIZED;
  }
}

export function settings_ConnectorEnforcementToJSON(object: Settings_ConnectorEnforcement): string {
  switch (object) {
    case Settings_ConnectorEnforcement.CONNECTOR_ENFORCEMENT_UNSPECIFIED:
      return "CONNECTOR_ENFORCEMENT_UNSPECIFIED";
    case Settings_ConnectorEnforcement.NOT_REQUIRED:
      return "NOT_REQUIRED";
    case Settings_ConnectorEnforcement.REQUIRED:
      return "REQUIRED";
    case Settings_ConnectorEnforcement.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export interface Settings_UserLabelsEntry {
  key: string;
  value: string;
}

/** Specifies options for controlling advanced machine features. */
export interface AdvancedMachineFeatures {
  /** The number of threads per physical core. */
  threadsPerCore: number;
}

/** SslCerts Resource */
export interface SslCert {
  /** This is always `sql#sslCert`. */
  kind: string;
  /** Serial number, as extracted from the certificate. */
  certSerialNumber: string;
  /** PEM representation. */
  cert: string;
  /**
   * The time when the certificate was created in [RFC
   * 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`
   */
  createTime:
    | Date
    | undefined;
  /** User supplied name.  Constrained to [a-zA-Z.-_ ]+. */
  commonName: string;
  /**
   * The time when the certificate expires in [RFC
   * 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`.
   */
  expirationTime:
    | Date
    | undefined;
  /** Sha1 Fingerprint. */
  sha1Fingerprint: string;
  /** Name of the database instance. */
  instance: string;
  /** The URI of this resource. */
  selfLink: string;
}

/** SslCertDetail. */
export interface SslCertDetail {
  /** The public information about the cert. */
  certInfo:
    | SslCert
    | undefined;
  /**
   * The private key for the client cert, in pem format.  Keep private in order
   * to protect your security.
   */
  certPrivateKey: string;
}

/** Active Directory configuration, relevant only for Cloud SQL for SQL Server. */
export interface SqlActiveDirectoryConfig {
  /** This is always sql#activeDirectoryConfig. */
  kind: string;
  /** The name of the domain (e.g., mydomain.com). */
  domain: string;
}

/** SQL Server specific audit configuration. */
export interface SqlServerAuditConfig {
  /** This is always sql#sqlServerAuditConfig */
  kind: string;
  /** The name of the destination bucket (e.g., gs://mybucket). */
  bucket: string;
  /** How long to keep generated audit files. */
  retentionInterval:
    | Duration
    | undefined;
  /** How often to upload generated audit files. */
  uploadInterval: Duration | undefined;
}

/** Acquire SSRS lease context. */
export interface AcquireSsrsLeaseContext {
  /**
   * The username to be used as the setup login to connect to the database
   * server for SSRS setup.
   */
  setupLogin?:
    | string
    | undefined;
  /**
   * The username to be used as the service login to connect to the report
   * database for SSRS setup.
   */
  serviceLogin?:
    | string
    | undefined;
  /** The report database to be used for SSRS setup. */
  reportDatabase?:
    | string
    | undefined;
  /** Lease duration needed for SSRS setup. */
  duration?: Duration | undefined;
}

function createBaseAclEntry(): AclEntry {
  return { value: "", expirationTime: undefined, name: "", kind: "" };
}

export const AclEntry: MessageFns<AclEntry> = {
  encode(message: AclEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.value !== "") {
      writer.uint32(10).string(message.value);
    }
    if (message.expirationTime !== undefined) {
      Timestamp.encode(toTimestamp(message.expirationTime), writer.uint32(18).fork()).join();
    }
    if (message.name !== "") {
      writer.uint32(26).string(message.name);
    }
    if (message.kind !== "") {
      writer.uint32(34).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AclEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAclEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.value = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.expirationTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.name = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AclEntry {
    return {
      value: isSet(object.value) ? globalThis.String(object.value) : "",
      expirationTime: isSet(object.expirationTime) ? fromJsonTimestamp(object.expirationTime) : undefined,
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: AclEntry): unknown {
    const obj: any = {};
    if (message.value !== "") {
      obj.value = message.value;
    }
    if (message.expirationTime !== undefined) {
      obj.expirationTime = message.expirationTime.toISOString();
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<AclEntry>): AclEntry {
    return AclEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AclEntry>): AclEntry {
    const message = createBaseAclEntry();
    message.value = object.value ?? "";
    message.expirationTime = object.expirationTime ?? undefined;
    message.name = object.name ?? "";
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseApiWarning(): ApiWarning {
  return { code: 0, message: "", region: "" };
}

export const ApiWarning: MessageFns<ApiWarning> = {
  encode(message: ApiWarning, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.code !== 0) {
      writer.uint32(8).int32(message.code);
    }
    if (message.message !== "") {
      writer.uint32(18).string(message.message);
    }
    if (message.region !== "") {
      writer.uint32(26).string(message.region);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ApiWarning {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseApiWarning();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.code = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.message = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.region = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ApiWarning {
    return {
      code: isSet(object.code) ? apiWarning_SqlApiWarningCodeFromJSON(object.code) : 0,
      message: isSet(object.message) ? globalThis.String(object.message) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
    };
  },

  toJSON(message: ApiWarning): unknown {
    const obj: any = {};
    if (message.code !== 0) {
      obj.code = apiWarning_SqlApiWarningCodeToJSON(message.code);
    }
    if (message.message !== "") {
      obj.message = message.message;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    return obj;
  },

  create(base?: DeepPartial<ApiWarning>): ApiWarning {
    return ApiWarning.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ApiWarning>): ApiWarning {
    const message = createBaseApiWarning();
    message.code = object.code ?? 0;
    message.message = object.message ?? "";
    message.region = object.region ?? "";
    return message;
  },
};

function createBaseBackupRetentionSettings(): BackupRetentionSettings {
  return { retentionUnit: 0, retainedBackups: undefined };
}

export const BackupRetentionSettings: MessageFns<BackupRetentionSettings> = {
  encode(message: BackupRetentionSettings, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.retentionUnit !== 0) {
      writer.uint32(8).int32(message.retentionUnit);
    }
    if (message.retainedBackups !== undefined) {
      Int32Value.encode({ value: message.retainedBackups! }, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BackupRetentionSettings {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackupRetentionSettings();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.retentionUnit = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.retainedBackups = Int32Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BackupRetentionSettings {
    return {
      retentionUnit: isSet(object.retentionUnit)
        ? backupRetentionSettings_RetentionUnitFromJSON(object.retentionUnit)
        : 0,
      retainedBackups: isSet(object.retainedBackups) ? Number(object.retainedBackups) : undefined,
    };
  },

  toJSON(message: BackupRetentionSettings): unknown {
    const obj: any = {};
    if (message.retentionUnit !== 0) {
      obj.retentionUnit = backupRetentionSettings_RetentionUnitToJSON(message.retentionUnit);
    }
    if (message.retainedBackups !== undefined) {
      obj.retainedBackups = message.retainedBackups;
    }
    return obj;
  },

  create(base?: DeepPartial<BackupRetentionSettings>): BackupRetentionSettings {
    return BackupRetentionSettings.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BackupRetentionSettings>): BackupRetentionSettings {
    const message = createBaseBackupRetentionSettings();
    message.retentionUnit = object.retentionUnit ?? 0;
    message.retainedBackups = object.retainedBackups ?? undefined;
    return message;
  },
};

function createBaseBackupConfiguration(): BackupConfiguration {
  return {
    startTime: "",
    enabled: undefined,
    kind: "",
    binaryLogEnabled: undefined,
    replicationLogArchivingEnabled: undefined,
    location: "",
    pointInTimeRecoveryEnabled: undefined,
    backupRetentionSettings: undefined,
    transactionLogRetentionDays: undefined,
    transactionalLogStorageState: undefined,
  };
}

export const BackupConfiguration: MessageFns<BackupConfiguration> = {
  encode(message: BackupConfiguration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startTime !== "") {
      writer.uint32(10).string(message.startTime);
    }
    if (message.enabled !== undefined) {
      BoolValue.encode({ value: message.enabled! }, writer.uint32(18).fork()).join();
    }
    if (message.kind !== "") {
      writer.uint32(26).string(message.kind);
    }
    if (message.binaryLogEnabled !== undefined) {
      BoolValue.encode({ value: message.binaryLogEnabled! }, writer.uint32(34).fork()).join();
    }
    if (message.replicationLogArchivingEnabled !== undefined) {
      BoolValue.encode({ value: message.replicationLogArchivingEnabled! }, writer.uint32(42).fork()).join();
    }
    if (message.location !== "") {
      writer.uint32(50).string(message.location);
    }
    if (message.pointInTimeRecoveryEnabled !== undefined) {
      BoolValue.encode({ value: message.pointInTimeRecoveryEnabled! }, writer.uint32(58).fork()).join();
    }
    if (message.backupRetentionSettings !== undefined) {
      BackupRetentionSettings.encode(message.backupRetentionSettings, writer.uint32(66).fork()).join();
    }
    if (message.transactionLogRetentionDays !== undefined) {
      Int32Value.encode({ value: message.transactionLogRetentionDays! }, writer.uint32(74).fork()).join();
    }
    if (message.transactionalLogStorageState !== undefined) {
      writer.uint32(80).int32(message.transactionalLogStorageState);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BackupConfiguration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackupConfiguration();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startTime = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.enabled = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.binaryLogEnabled = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.replicationLogArchivingEnabled = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.location = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.pointInTimeRecoveryEnabled = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.backupRetentionSettings = BackupRetentionSettings.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.transactionLogRetentionDays = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.transactionalLogStorageState = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BackupConfiguration {
    return {
      startTime: isSet(object.startTime) ? globalThis.String(object.startTime) : "",
      enabled: isSet(object.enabled) ? Boolean(object.enabled) : undefined,
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      binaryLogEnabled: isSet(object.binaryLogEnabled) ? Boolean(object.binaryLogEnabled) : undefined,
      replicationLogArchivingEnabled: isSet(object.replicationLogArchivingEnabled)
        ? Boolean(object.replicationLogArchivingEnabled)
        : undefined,
      location: isSet(object.location) ? globalThis.String(object.location) : "",
      pointInTimeRecoveryEnabled: isSet(object.pointInTimeRecoveryEnabled)
        ? Boolean(object.pointInTimeRecoveryEnabled)
        : undefined,
      backupRetentionSettings: isSet(object.backupRetentionSettings)
        ? BackupRetentionSettings.fromJSON(object.backupRetentionSettings)
        : undefined,
      transactionLogRetentionDays: isSet(object.transactionLogRetentionDays)
        ? Number(object.transactionLogRetentionDays)
        : undefined,
      transactionalLogStorageState: isSet(object.transactionalLogStorageState)
        ? backupConfiguration_TransactionalLogStorageStateFromJSON(object.transactionalLogStorageState)
        : undefined,
    };
  },

  toJSON(message: BackupConfiguration): unknown {
    const obj: any = {};
    if (message.startTime !== "") {
      obj.startTime = message.startTime;
    }
    if (message.enabled !== undefined) {
      obj.enabled = message.enabled;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.binaryLogEnabled !== undefined) {
      obj.binaryLogEnabled = message.binaryLogEnabled;
    }
    if (message.replicationLogArchivingEnabled !== undefined) {
      obj.replicationLogArchivingEnabled = message.replicationLogArchivingEnabled;
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    if (message.pointInTimeRecoveryEnabled !== undefined) {
      obj.pointInTimeRecoveryEnabled = message.pointInTimeRecoveryEnabled;
    }
    if (message.backupRetentionSettings !== undefined) {
      obj.backupRetentionSettings = BackupRetentionSettings.toJSON(message.backupRetentionSettings);
    }
    if (message.transactionLogRetentionDays !== undefined) {
      obj.transactionLogRetentionDays = message.transactionLogRetentionDays;
    }
    if (message.transactionalLogStorageState !== undefined) {
      obj.transactionalLogStorageState = backupConfiguration_TransactionalLogStorageStateToJSON(
        message.transactionalLogStorageState,
      );
    }
    return obj;
  },

  create(base?: DeepPartial<BackupConfiguration>): BackupConfiguration {
    return BackupConfiguration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BackupConfiguration>): BackupConfiguration {
    const message = createBaseBackupConfiguration();
    message.startTime = object.startTime ?? "";
    message.enabled = object.enabled ?? undefined;
    message.kind = object.kind ?? "";
    message.binaryLogEnabled = object.binaryLogEnabled ?? undefined;
    message.replicationLogArchivingEnabled = object.replicationLogArchivingEnabled ?? undefined;
    message.location = object.location ?? "";
    message.pointInTimeRecoveryEnabled = object.pointInTimeRecoveryEnabled ?? undefined;
    message.backupRetentionSettings =
      (object.backupRetentionSettings !== undefined && object.backupRetentionSettings !== null)
        ? BackupRetentionSettings.fromPartial(object.backupRetentionSettings)
        : undefined;
    message.transactionLogRetentionDays = object.transactionLogRetentionDays ?? undefined;
    message.transactionalLogStorageState = object.transactionalLogStorageState ?? undefined;
    return message;
  },
};

function createBasePerformDiskShrinkContext(): PerformDiskShrinkContext {
  return { targetSizeGb: Long.ZERO };
}

export const PerformDiskShrinkContext: MessageFns<PerformDiskShrinkContext> = {
  encode(message: PerformDiskShrinkContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.targetSizeGb.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.targetSizeGb.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PerformDiskShrinkContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePerformDiskShrinkContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.targetSizeGb = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PerformDiskShrinkContext {
    return { targetSizeGb: isSet(object.targetSizeGb) ? Long.fromValue(object.targetSizeGb) : Long.ZERO };
  },

  toJSON(message: PerformDiskShrinkContext): unknown {
    const obj: any = {};
    if (!message.targetSizeGb.equals(Long.ZERO)) {
      obj.targetSizeGb = (message.targetSizeGb || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<PerformDiskShrinkContext>): PerformDiskShrinkContext {
    return PerformDiskShrinkContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PerformDiskShrinkContext>): PerformDiskShrinkContext {
    const message = createBasePerformDiskShrinkContext();
    message.targetSizeGb = (object.targetSizeGb !== undefined && object.targetSizeGb !== null)
      ? Long.fromValue(object.targetSizeGb)
      : Long.ZERO;
    return message;
  },
};

function createBaseBackupContext(): BackupContext {
  return { backupId: Long.ZERO, kind: "" };
}

export const BackupContext: MessageFns<BackupContext> = {
  encode(message: BackupContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.backupId.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.backupId.toString());
    }
    if (message.kind !== "") {
      writer.uint32(18).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BackupContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackupContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.backupId = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BackupContext {
    return {
      backupId: isSet(object.backupId) ? Long.fromValue(object.backupId) : Long.ZERO,
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: BackupContext): unknown {
    const obj: any = {};
    if (!message.backupId.equals(Long.ZERO)) {
      obj.backupId = (message.backupId || Long.ZERO).toString();
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<BackupContext>): BackupContext {
    return BackupContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BackupContext>): BackupContext {
    const message = createBaseBackupContext();
    message.backupId = (object.backupId !== undefined && object.backupId !== null)
      ? Long.fromValue(object.backupId)
      : Long.ZERO;
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseDatabase(): Database {
  return {
    kind: "",
    charset: "",
    collation: "",
    etag: "",
    name: "",
    instance: "",
    selfLink: "",
    project: "",
    sqlserverDatabaseDetails: undefined,
  };
}

export const Database: MessageFns<Database> = {
  encode(message: Database, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.charset !== "") {
      writer.uint32(18).string(message.charset);
    }
    if (message.collation !== "") {
      writer.uint32(26).string(message.collation);
    }
    if (message.etag !== "") {
      writer.uint32(34).string(message.etag);
    }
    if (message.name !== "") {
      writer.uint32(42).string(message.name);
    }
    if (message.instance !== "") {
      writer.uint32(50).string(message.instance);
    }
    if (message.selfLink !== "") {
      writer.uint32(58).string(message.selfLink);
    }
    if (message.project !== "") {
      writer.uint32(66).string(message.project);
    }
    if (message.sqlserverDatabaseDetails !== undefined) {
      SqlServerDatabaseDetails.encode(message.sqlserverDatabaseDetails, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Database {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatabase();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.charset = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.collation = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.etag = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.name = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.instance = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.selfLink = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.project = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.sqlserverDatabaseDetails = SqlServerDatabaseDetails.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Database {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      charset: isSet(object.charset) ? globalThis.String(object.charset) : "",
      collation: isSet(object.collation) ? globalThis.String(object.collation) : "",
      etag: isSet(object.etag) ? globalThis.String(object.etag) : "",
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      instance: isSet(object.instance) ? globalThis.String(object.instance) : "",
      selfLink: isSet(object.selfLink) ? globalThis.String(object.selfLink) : "",
      project: isSet(object.project) ? globalThis.String(object.project) : "",
      sqlserverDatabaseDetails: isSet(object.sqlserverDatabaseDetails)
        ? SqlServerDatabaseDetails.fromJSON(object.sqlserverDatabaseDetails)
        : undefined,
    };
  },

  toJSON(message: Database): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.charset !== "") {
      obj.charset = message.charset;
    }
    if (message.collation !== "") {
      obj.collation = message.collation;
    }
    if (message.etag !== "") {
      obj.etag = message.etag;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.instance !== "") {
      obj.instance = message.instance;
    }
    if (message.selfLink !== "") {
      obj.selfLink = message.selfLink;
    }
    if (message.project !== "") {
      obj.project = message.project;
    }
    if (message.sqlserverDatabaseDetails !== undefined) {
      obj.sqlserverDatabaseDetails = SqlServerDatabaseDetails.toJSON(message.sqlserverDatabaseDetails);
    }
    return obj;
  },

  create(base?: DeepPartial<Database>): Database {
    return Database.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Database>): Database {
    const message = createBaseDatabase();
    message.kind = object.kind ?? "";
    message.charset = object.charset ?? "";
    message.collation = object.collation ?? "";
    message.etag = object.etag ?? "";
    message.name = object.name ?? "";
    message.instance = object.instance ?? "";
    message.selfLink = object.selfLink ?? "";
    message.project = object.project ?? "";
    message.sqlserverDatabaseDetails =
      (object.sqlserverDatabaseDetails !== undefined && object.sqlserverDatabaseDetails !== null)
        ? SqlServerDatabaseDetails.fromPartial(object.sqlserverDatabaseDetails)
        : undefined;
    return message;
  },
};

function createBaseSqlServerDatabaseDetails(): SqlServerDatabaseDetails {
  return { compatibilityLevel: 0, recoveryModel: "" };
}

export const SqlServerDatabaseDetails: MessageFns<SqlServerDatabaseDetails> = {
  encode(message: SqlServerDatabaseDetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.compatibilityLevel !== 0) {
      writer.uint32(8).int32(message.compatibilityLevel);
    }
    if (message.recoveryModel !== "") {
      writer.uint32(18).string(message.recoveryModel);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SqlServerDatabaseDetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSqlServerDatabaseDetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.compatibilityLevel = reader.int32();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.recoveryModel = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SqlServerDatabaseDetails {
    return {
      compatibilityLevel: isSet(object.compatibilityLevel) ? globalThis.Number(object.compatibilityLevel) : 0,
      recoveryModel: isSet(object.recoveryModel) ? globalThis.String(object.recoveryModel) : "",
    };
  },

  toJSON(message: SqlServerDatabaseDetails): unknown {
    const obj: any = {};
    if (message.compatibilityLevel !== 0) {
      obj.compatibilityLevel = Math.round(message.compatibilityLevel);
    }
    if (message.recoveryModel !== "") {
      obj.recoveryModel = message.recoveryModel;
    }
    return obj;
  },

  create(base?: DeepPartial<SqlServerDatabaseDetails>): SqlServerDatabaseDetails {
    return SqlServerDatabaseDetails.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SqlServerDatabaseDetails>): SqlServerDatabaseDetails {
    const message = createBaseSqlServerDatabaseDetails();
    message.compatibilityLevel = object.compatibilityLevel ?? 0;
    message.recoveryModel = object.recoveryModel ?? "";
    return message;
  },
};

function createBaseDatabaseFlags(): DatabaseFlags {
  return { name: "", value: "" };
}

export const DatabaseFlags: MessageFns<DatabaseFlags> = {
  encode(message: DatabaseFlags, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatabaseFlags {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatabaseFlags();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatabaseFlags {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: DatabaseFlags): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<DatabaseFlags>): DatabaseFlags {
    return DatabaseFlags.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatabaseFlags>): DatabaseFlags {
    const message = createBaseDatabaseFlags();
    message.name = object.name ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseMySqlSyncConfig(): MySqlSyncConfig {
  return { initialSyncFlags: [] };
}

export const MySqlSyncConfig: MessageFns<MySqlSyncConfig> = {
  encode(message: MySqlSyncConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.initialSyncFlags) {
      SyncFlags.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MySqlSyncConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMySqlSyncConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.initialSyncFlags.push(SyncFlags.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MySqlSyncConfig {
    return {
      initialSyncFlags: globalThis.Array.isArray(object?.initialSyncFlags)
        ? object.initialSyncFlags.map((e: any) => SyncFlags.fromJSON(e))
        : [],
    };
  },

  toJSON(message: MySqlSyncConfig): unknown {
    const obj: any = {};
    if (message.initialSyncFlags?.length) {
      obj.initialSyncFlags = message.initialSyncFlags.map((e) => SyncFlags.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<MySqlSyncConfig>): MySqlSyncConfig {
    return MySqlSyncConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MySqlSyncConfig>): MySqlSyncConfig {
    const message = createBaseMySqlSyncConfig();
    message.initialSyncFlags = object.initialSyncFlags?.map((e) => SyncFlags.fromPartial(e)) || [];
    return message;
  },
};

function createBaseSyncFlags(): SyncFlags {
  return { name: "", value: "" };
}

export const SyncFlags: MessageFns<SyncFlags> = {
  encode(message: SyncFlags, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SyncFlags {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSyncFlags();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SyncFlags {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: SyncFlags): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<SyncFlags>): SyncFlags {
    return SyncFlags.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SyncFlags>): SyncFlags {
    const message = createBaseSyncFlags();
    message.name = object.name ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseInstanceReference(): InstanceReference {
  return { name: "", region: "", project: "" };
}

export const InstanceReference: MessageFns<InstanceReference> = {
  encode(message: InstanceReference, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.region !== "") {
      writer.uint32(18).string(message.region);
    }
    if (message.project !== "") {
      writer.uint32(26).string(message.project);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstanceReference {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstanceReference();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.region = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.project = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstanceReference {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      project: isSet(object.project) ? globalThis.String(object.project) : "",
    };
  },

  toJSON(message: InstanceReference): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.project !== "") {
      obj.project = message.project;
    }
    return obj;
  },

  create(base?: DeepPartial<InstanceReference>): InstanceReference {
    return InstanceReference.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstanceReference>): InstanceReference {
    const message = createBaseInstanceReference();
    message.name = object.name ?? "";
    message.region = object.region ?? "";
    message.project = object.project ?? "";
    return message;
  },
};

function createBaseDemoteMasterConfiguration(): DemoteMasterConfiguration {
  return { kind: "", mysqlReplicaConfiguration: undefined };
}

export const DemoteMasterConfiguration: MessageFns<DemoteMasterConfiguration> = {
  encode(message: DemoteMasterConfiguration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.mysqlReplicaConfiguration !== undefined) {
      DemoteMasterMySqlReplicaConfiguration.encode(message.mysqlReplicaConfiguration, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DemoteMasterConfiguration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDemoteMasterConfiguration();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.mysqlReplicaConfiguration = DemoteMasterMySqlReplicaConfiguration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DemoteMasterConfiguration {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      mysqlReplicaConfiguration: isSet(object.mysqlReplicaConfiguration)
        ? DemoteMasterMySqlReplicaConfiguration.fromJSON(object.mysqlReplicaConfiguration)
        : undefined,
    };
  },

  toJSON(message: DemoteMasterConfiguration): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.mysqlReplicaConfiguration !== undefined) {
      obj.mysqlReplicaConfiguration = DemoteMasterMySqlReplicaConfiguration.toJSON(message.mysqlReplicaConfiguration);
    }
    return obj;
  },

  create(base?: DeepPartial<DemoteMasterConfiguration>): DemoteMasterConfiguration {
    return DemoteMasterConfiguration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DemoteMasterConfiguration>): DemoteMasterConfiguration {
    const message = createBaseDemoteMasterConfiguration();
    message.kind = object.kind ?? "";
    message.mysqlReplicaConfiguration =
      (object.mysqlReplicaConfiguration !== undefined && object.mysqlReplicaConfiguration !== null)
        ? DemoteMasterMySqlReplicaConfiguration.fromPartial(object.mysqlReplicaConfiguration)
        : undefined;
    return message;
  },
};

function createBaseDemoteMasterMySqlReplicaConfiguration(): DemoteMasterMySqlReplicaConfiguration {
  return { kind: "", username: "", password: "", clientKey: "", clientCertificate: "", caCertificate: "" };
}

export const DemoteMasterMySqlReplicaConfiguration: MessageFns<DemoteMasterMySqlReplicaConfiguration> = {
  encode(message: DemoteMasterMySqlReplicaConfiguration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.username !== "") {
      writer.uint32(18).string(message.username);
    }
    if (message.password !== "") {
      writer.uint32(26).string(message.password);
    }
    if (message.clientKey !== "") {
      writer.uint32(34).string(message.clientKey);
    }
    if (message.clientCertificate !== "") {
      writer.uint32(42).string(message.clientCertificate);
    }
    if (message.caCertificate !== "") {
      writer.uint32(50).string(message.caCertificate);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DemoteMasterMySqlReplicaConfiguration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDemoteMasterMySqlReplicaConfiguration();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.username = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.password = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.clientKey = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.clientCertificate = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.caCertificate = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DemoteMasterMySqlReplicaConfiguration {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      username: isSet(object.username) ? globalThis.String(object.username) : "",
      password: isSet(object.password) ? globalThis.String(object.password) : "",
      clientKey: isSet(object.clientKey) ? globalThis.String(object.clientKey) : "",
      clientCertificate: isSet(object.clientCertificate) ? globalThis.String(object.clientCertificate) : "",
      caCertificate: isSet(object.caCertificate) ? globalThis.String(object.caCertificate) : "",
    };
  },

  toJSON(message: DemoteMasterMySqlReplicaConfiguration): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.username !== "") {
      obj.username = message.username;
    }
    if (message.password !== "") {
      obj.password = message.password;
    }
    if (message.clientKey !== "") {
      obj.clientKey = message.clientKey;
    }
    if (message.clientCertificate !== "") {
      obj.clientCertificate = message.clientCertificate;
    }
    if (message.caCertificate !== "") {
      obj.caCertificate = message.caCertificate;
    }
    return obj;
  },

  create(base?: DeepPartial<DemoteMasterMySqlReplicaConfiguration>): DemoteMasterMySqlReplicaConfiguration {
    return DemoteMasterMySqlReplicaConfiguration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DemoteMasterMySqlReplicaConfiguration>): DemoteMasterMySqlReplicaConfiguration {
    const message = createBaseDemoteMasterMySqlReplicaConfiguration();
    message.kind = object.kind ?? "";
    message.username = object.username ?? "";
    message.password = object.password ?? "";
    message.clientKey = object.clientKey ?? "";
    message.clientCertificate = object.clientCertificate ?? "";
    message.caCertificate = object.caCertificate ?? "";
    return message;
  },
};

function createBaseExportContext(): ExportContext {
  return {
    uri: "",
    databases: [],
    kind: "",
    sqlExportOptions: undefined,
    csvExportOptions: undefined,
    fileType: 0,
    offload: undefined,
    bakExportOptions: undefined,
  };
}

export const ExportContext: MessageFns<ExportContext> = {
  encode(message: ExportContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.uri !== "") {
      writer.uint32(10).string(message.uri);
    }
    for (const v of message.databases) {
      writer.uint32(18).string(v!);
    }
    if (message.kind !== "") {
      writer.uint32(26).string(message.kind);
    }
    if (message.sqlExportOptions !== undefined) {
      ExportContext_SqlExportOptions.encode(message.sqlExportOptions, writer.uint32(34).fork()).join();
    }
    if (message.csvExportOptions !== undefined) {
      ExportContext_SqlCsvExportOptions.encode(message.csvExportOptions, writer.uint32(42).fork()).join();
    }
    if (message.fileType !== 0) {
      writer.uint32(48).int32(message.fileType);
    }
    if (message.offload !== undefined) {
      BoolValue.encode({ value: message.offload! }, writer.uint32(66).fork()).join();
    }
    if (message.bakExportOptions !== undefined) {
      ExportContext_SqlBakExportOptions.encode(message.bakExportOptions, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.uri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.databases.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.sqlExportOptions = ExportContext_SqlExportOptions.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.csvExportOptions = ExportContext_SqlCsvExportOptions.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.fileType = reader.int32() as any;
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.offload = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.bakExportOptions = ExportContext_SqlBakExportOptions.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportContext {
    return {
      uri: isSet(object.uri) ? globalThis.String(object.uri) : "",
      databases: globalThis.Array.isArray(object?.databases)
        ? object.databases.map((e: any) => globalThis.String(e))
        : [],
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      sqlExportOptions: isSet(object.sqlExportOptions)
        ? ExportContext_SqlExportOptions.fromJSON(object.sqlExportOptions)
        : undefined,
      csvExportOptions: isSet(object.csvExportOptions)
        ? ExportContext_SqlCsvExportOptions.fromJSON(object.csvExportOptions)
        : undefined,
      fileType: isSet(object.fileType) ? sqlFileTypeFromJSON(object.fileType) : 0,
      offload: isSet(object.offload) ? Boolean(object.offload) : undefined,
      bakExportOptions: isSet(object.bakExportOptions)
        ? ExportContext_SqlBakExportOptions.fromJSON(object.bakExportOptions)
        : undefined,
    };
  },

  toJSON(message: ExportContext): unknown {
    const obj: any = {};
    if (message.uri !== "") {
      obj.uri = message.uri;
    }
    if (message.databases?.length) {
      obj.databases = message.databases;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.sqlExportOptions !== undefined) {
      obj.sqlExportOptions = ExportContext_SqlExportOptions.toJSON(message.sqlExportOptions);
    }
    if (message.csvExportOptions !== undefined) {
      obj.csvExportOptions = ExportContext_SqlCsvExportOptions.toJSON(message.csvExportOptions);
    }
    if (message.fileType !== 0) {
      obj.fileType = sqlFileTypeToJSON(message.fileType);
    }
    if (message.offload !== undefined) {
      obj.offload = message.offload;
    }
    if (message.bakExportOptions !== undefined) {
      obj.bakExportOptions = ExportContext_SqlBakExportOptions.toJSON(message.bakExportOptions);
    }
    return obj;
  },

  create(base?: DeepPartial<ExportContext>): ExportContext {
    return ExportContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportContext>): ExportContext {
    const message = createBaseExportContext();
    message.uri = object.uri ?? "";
    message.databases = object.databases?.map((e) => e) || [];
    message.kind = object.kind ?? "";
    message.sqlExportOptions = (object.sqlExportOptions !== undefined && object.sqlExportOptions !== null)
      ? ExportContext_SqlExportOptions.fromPartial(object.sqlExportOptions)
      : undefined;
    message.csvExportOptions = (object.csvExportOptions !== undefined && object.csvExportOptions !== null)
      ? ExportContext_SqlCsvExportOptions.fromPartial(object.csvExportOptions)
      : undefined;
    message.fileType = object.fileType ?? 0;
    message.offload = object.offload ?? undefined;
    message.bakExportOptions = (object.bakExportOptions !== undefined && object.bakExportOptions !== null)
      ? ExportContext_SqlBakExportOptions.fromPartial(object.bakExportOptions)
      : undefined;
    return message;
  },
};

function createBaseExportContext_SqlCsvExportOptions(): ExportContext_SqlCsvExportOptions {
  return { selectQuery: "", escapeCharacter: "", quoteCharacter: "", fieldsTerminatedBy: "", linesTerminatedBy: "" };
}

export const ExportContext_SqlCsvExportOptions: MessageFns<ExportContext_SqlCsvExportOptions> = {
  encode(message: ExportContext_SqlCsvExportOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.selectQuery !== "") {
      writer.uint32(10).string(message.selectQuery);
    }
    if (message.escapeCharacter !== "") {
      writer.uint32(18).string(message.escapeCharacter);
    }
    if (message.quoteCharacter !== "") {
      writer.uint32(26).string(message.quoteCharacter);
    }
    if (message.fieldsTerminatedBy !== "") {
      writer.uint32(34).string(message.fieldsTerminatedBy);
    }
    if (message.linesTerminatedBy !== "") {
      writer.uint32(50).string(message.linesTerminatedBy);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportContext_SqlCsvExportOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportContext_SqlCsvExportOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.selectQuery = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.escapeCharacter = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.quoteCharacter = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.fieldsTerminatedBy = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.linesTerminatedBy = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportContext_SqlCsvExportOptions {
    return {
      selectQuery: isSet(object.selectQuery) ? globalThis.String(object.selectQuery) : "",
      escapeCharacter: isSet(object.escapeCharacter) ? globalThis.String(object.escapeCharacter) : "",
      quoteCharacter: isSet(object.quoteCharacter) ? globalThis.String(object.quoteCharacter) : "",
      fieldsTerminatedBy: isSet(object.fieldsTerminatedBy) ? globalThis.String(object.fieldsTerminatedBy) : "",
      linesTerminatedBy: isSet(object.linesTerminatedBy) ? globalThis.String(object.linesTerminatedBy) : "",
    };
  },

  toJSON(message: ExportContext_SqlCsvExportOptions): unknown {
    const obj: any = {};
    if (message.selectQuery !== "") {
      obj.selectQuery = message.selectQuery;
    }
    if (message.escapeCharacter !== "") {
      obj.escapeCharacter = message.escapeCharacter;
    }
    if (message.quoteCharacter !== "") {
      obj.quoteCharacter = message.quoteCharacter;
    }
    if (message.fieldsTerminatedBy !== "") {
      obj.fieldsTerminatedBy = message.fieldsTerminatedBy;
    }
    if (message.linesTerminatedBy !== "") {
      obj.linesTerminatedBy = message.linesTerminatedBy;
    }
    return obj;
  },

  create(base?: DeepPartial<ExportContext_SqlCsvExportOptions>): ExportContext_SqlCsvExportOptions {
    return ExportContext_SqlCsvExportOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportContext_SqlCsvExportOptions>): ExportContext_SqlCsvExportOptions {
    const message = createBaseExportContext_SqlCsvExportOptions();
    message.selectQuery = object.selectQuery ?? "";
    message.escapeCharacter = object.escapeCharacter ?? "";
    message.quoteCharacter = object.quoteCharacter ?? "";
    message.fieldsTerminatedBy = object.fieldsTerminatedBy ?? "";
    message.linesTerminatedBy = object.linesTerminatedBy ?? "";
    return message;
  },
};

function createBaseExportContext_SqlExportOptions(): ExportContext_SqlExportOptions {
  return {
    tables: [],
    schemaOnly: undefined,
    mysqlExportOptions: undefined,
    threads: undefined,
    parallel: undefined,
    postgresExportOptions: undefined,
  };
}

export const ExportContext_SqlExportOptions: MessageFns<ExportContext_SqlExportOptions> = {
  encode(message: ExportContext_SqlExportOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.tables) {
      writer.uint32(10).string(v!);
    }
    if (message.schemaOnly !== undefined) {
      BoolValue.encode({ value: message.schemaOnly! }, writer.uint32(18).fork()).join();
    }
    if (message.mysqlExportOptions !== undefined) {
      ExportContext_SqlExportOptions_MysqlExportOptions.encode(message.mysqlExportOptions, writer.uint32(26).fork())
        .join();
    }
    if (message.threads !== undefined) {
      Int32Value.encode({ value: message.threads! }, writer.uint32(34).fork()).join();
    }
    if (message.parallel !== undefined) {
      BoolValue.encode({ value: message.parallel! }, writer.uint32(42).fork()).join();
    }
    if (message.postgresExportOptions !== undefined) {
      ExportContext_SqlExportOptions_PostgresExportOptions.encode(
        message.postgresExportOptions,
        writer.uint32(50).fork(),
      ).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportContext_SqlExportOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportContext_SqlExportOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.tables.push(reader.string());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.schemaOnly = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.mysqlExportOptions = ExportContext_SqlExportOptions_MysqlExportOptions.decode(
            reader,
            reader.uint32(),
          );
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.threads = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.parallel = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.postgresExportOptions = ExportContext_SqlExportOptions_PostgresExportOptions.decode(
            reader,
            reader.uint32(),
          );
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportContext_SqlExportOptions {
    return {
      tables: globalThis.Array.isArray(object?.tables) ? object.tables.map((e: any) => globalThis.String(e)) : [],
      schemaOnly: isSet(object.schemaOnly) ? Boolean(object.schemaOnly) : undefined,
      mysqlExportOptions: isSet(object.mysqlExportOptions)
        ? ExportContext_SqlExportOptions_MysqlExportOptions.fromJSON(object.mysqlExportOptions)
        : undefined,
      threads: isSet(object.threads) ? Number(object.threads) : undefined,
      parallel: isSet(object.parallel) ? Boolean(object.parallel) : undefined,
      postgresExportOptions: isSet(object.postgresExportOptions)
        ? ExportContext_SqlExportOptions_PostgresExportOptions.fromJSON(object.postgresExportOptions)
        : undefined,
    };
  },

  toJSON(message: ExportContext_SqlExportOptions): unknown {
    const obj: any = {};
    if (message.tables?.length) {
      obj.tables = message.tables;
    }
    if (message.schemaOnly !== undefined) {
      obj.schemaOnly = message.schemaOnly;
    }
    if (message.mysqlExportOptions !== undefined) {
      obj.mysqlExportOptions = ExportContext_SqlExportOptions_MysqlExportOptions.toJSON(message.mysqlExportOptions);
    }
    if (message.threads !== undefined) {
      obj.threads = message.threads;
    }
    if (message.parallel !== undefined) {
      obj.parallel = message.parallel;
    }
    if (message.postgresExportOptions !== undefined) {
      obj.postgresExportOptions = ExportContext_SqlExportOptions_PostgresExportOptions.toJSON(
        message.postgresExportOptions,
      );
    }
    return obj;
  },

  create(base?: DeepPartial<ExportContext_SqlExportOptions>): ExportContext_SqlExportOptions {
    return ExportContext_SqlExportOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportContext_SqlExportOptions>): ExportContext_SqlExportOptions {
    const message = createBaseExportContext_SqlExportOptions();
    message.tables = object.tables?.map((e) => e) || [];
    message.schemaOnly = object.schemaOnly ?? undefined;
    message.mysqlExportOptions = (object.mysqlExportOptions !== undefined && object.mysqlExportOptions !== null)
      ? ExportContext_SqlExportOptions_MysqlExportOptions.fromPartial(object.mysqlExportOptions)
      : undefined;
    message.threads = object.threads ?? undefined;
    message.parallel = object.parallel ?? undefined;
    message.postgresExportOptions =
      (object.postgresExportOptions !== undefined && object.postgresExportOptions !== null)
        ? ExportContext_SqlExportOptions_PostgresExportOptions.fromPartial(object.postgresExportOptions)
        : undefined;
    return message;
  },
};

function createBaseExportContext_SqlExportOptions_MysqlExportOptions(): ExportContext_SqlExportOptions_MysqlExportOptions {
  return { masterData: undefined };
}

export const ExportContext_SqlExportOptions_MysqlExportOptions: MessageFns<
  ExportContext_SqlExportOptions_MysqlExportOptions
> = {
  encode(
    message: ExportContext_SqlExportOptions_MysqlExportOptions,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.masterData !== undefined) {
      Int32Value.encode({ value: message.masterData! }, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportContext_SqlExportOptions_MysqlExportOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportContext_SqlExportOptions_MysqlExportOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.masterData = Int32Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportContext_SqlExportOptions_MysqlExportOptions {
    return { masterData: isSet(object.masterData) ? Number(object.masterData) : undefined };
  },

  toJSON(message: ExportContext_SqlExportOptions_MysqlExportOptions): unknown {
    const obj: any = {};
    if (message.masterData !== undefined) {
      obj.masterData = message.masterData;
    }
    return obj;
  },

  create(
    base?: DeepPartial<ExportContext_SqlExportOptions_MysqlExportOptions>,
  ): ExportContext_SqlExportOptions_MysqlExportOptions {
    return ExportContext_SqlExportOptions_MysqlExportOptions.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ExportContext_SqlExportOptions_MysqlExportOptions>,
  ): ExportContext_SqlExportOptions_MysqlExportOptions {
    const message = createBaseExportContext_SqlExportOptions_MysqlExportOptions();
    message.masterData = object.masterData ?? undefined;
    return message;
  },
};

function createBaseExportContext_SqlExportOptions_PostgresExportOptions(): ExportContext_SqlExportOptions_PostgresExportOptions {
  return { clean: undefined, ifExists: undefined };
}

export const ExportContext_SqlExportOptions_PostgresExportOptions: MessageFns<
  ExportContext_SqlExportOptions_PostgresExportOptions
> = {
  encode(
    message: ExportContext_SqlExportOptions_PostgresExportOptions,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.clean !== undefined) {
      BoolValue.encode({ value: message.clean! }, writer.uint32(10).fork()).join();
    }
    if (message.ifExists !== undefined) {
      BoolValue.encode({ value: message.ifExists! }, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportContext_SqlExportOptions_PostgresExportOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportContext_SqlExportOptions_PostgresExportOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.clean = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.ifExists = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportContext_SqlExportOptions_PostgresExportOptions {
    return {
      clean: isSet(object.clean) ? Boolean(object.clean) : undefined,
      ifExists: isSet(object.ifExists) ? Boolean(object.ifExists) : undefined,
    };
  },

  toJSON(message: ExportContext_SqlExportOptions_PostgresExportOptions): unknown {
    const obj: any = {};
    if (message.clean !== undefined) {
      obj.clean = message.clean;
    }
    if (message.ifExists !== undefined) {
      obj.ifExists = message.ifExists;
    }
    return obj;
  },

  create(
    base?: DeepPartial<ExportContext_SqlExportOptions_PostgresExportOptions>,
  ): ExportContext_SqlExportOptions_PostgresExportOptions {
    return ExportContext_SqlExportOptions_PostgresExportOptions.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ExportContext_SqlExportOptions_PostgresExportOptions>,
  ): ExportContext_SqlExportOptions_PostgresExportOptions {
    const message = createBaseExportContext_SqlExportOptions_PostgresExportOptions();
    message.clean = object.clean ?? undefined;
    message.ifExists = object.ifExists ?? undefined;
    return message;
  },
};

function createBaseExportContext_SqlBakExportOptions(): ExportContext_SqlBakExportOptions {
  return { striped: undefined, stripeCount: undefined, bakType: 0, copyOnly: undefined, differentialBase: undefined };
}

export const ExportContext_SqlBakExportOptions: MessageFns<ExportContext_SqlBakExportOptions> = {
  encode(message: ExportContext_SqlBakExportOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.striped !== undefined) {
      BoolValue.encode({ value: message.striped! }, writer.uint32(10).fork()).join();
    }
    if (message.stripeCount !== undefined) {
      Int32Value.encode({ value: message.stripeCount! }, writer.uint32(18).fork()).join();
    }
    if (message.bakType !== 0) {
      writer.uint32(32).int32(message.bakType);
    }
    if (message.copyOnly !== undefined) {
      BoolValue.encode({ value: message.copyOnly! }, writer.uint32(42).fork()).join();
    }
    if (message.differentialBase !== undefined) {
      BoolValue.encode({ value: message.differentialBase! }, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportContext_SqlBakExportOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportContext_SqlBakExportOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.striped = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.stripeCount = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.bakType = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.copyOnly = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.differentialBase = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportContext_SqlBakExportOptions {
    return {
      striped: isSet(object.striped) ? Boolean(object.striped) : undefined,
      stripeCount: isSet(object.stripeCount) ? Number(object.stripeCount) : undefined,
      bakType: isSet(object.bakType) ? bakTypeFromJSON(object.bakType) : 0,
      copyOnly: isSet(object.copyOnly) ? Boolean(object.copyOnly) : undefined,
      differentialBase: isSet(object.differentialBase) ? Boolean(object.differentialBase) : undefined,
    };
  },

  toJSON(message: ExportContext_SqlBakExportOptions): unknown {
    const obj: any = {};
    if (message.striped !== undefined) {
      obj.striped = message.striped;
    }
    if (message.stripeCount !== undefined) {
      obj.stripeCount = message.stripeCount;
    }
    if (message.bakType !== 0) {
      obj.bakType = bakTypeToJSON(message.bakType);
    }
    if (message.copyOnly !== undefined) {
      obj.copyOnly = message.copyOnly;
    }
    if (message.differentialBase !== undefined) {
      obj.differentialBase = message.differentialBase;
    }
    return obj;
  },

  create(base?: DeepPartial<ExportContext_SqlBakExportOptions>): ExportContext_SqlBakExportOptions {
    return ExportContext_SqlBakExportOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportContext_SqlBakExportOptions>): ExportContext_SqlBakExportOptions {
    const message = createBaseExportContext_SqlBakExportOptions();
    message.striped = object.striped ?? undefined;
    message.stripeCount = object.stripeCount ?? undefined;
    message.bakType = object.bakType ?? 0;
    message.copyOnly = object.copyOnly ?? undefined;
    message.differentialBase = object.differentialBase ?? undefined;
    return message;
  },
};

function createBaseImportContext(): ImportContext {
  return {
    uri: "",
    database: "",
    kind: "",
    fileType: 0,
    csvImportOptions: undefined,
    importUser: "",
    bakImportOptions: undefined,
    sqlImportOptions: undefined,
  };
}

export const ImportContext: MessageFns<ImportContext> = {
  encode(message: ImportContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.uri !== "") {
      writer.uint32(10).string(message.uri);
    }
    if (message.database !== "") {
      writer.uint32(18).string(message.database);
    }
    if (message.kind !== "") {
      writer.uint32(26).string(message.kind);
    }
    if (message.fileType !== 0) {
      writer.uint32(32).int32(message.fileType);
    }
    if (message.csvImportOptions !== undefined) {
      ImportContext_SqlCsvImportOptions.encode(message.csvImportOptions, writer.uint32(42).fork()).join();
    }
    if (message.importUser !== "") {
      writer.uint32(50).string(message.importUser);
    }
    if (message.bakImportOptions !== undefined) {
      ImportContext_SqlBakImportOptions.encode(message.bakImportOptions, writer.uint32(58).fork()).join();
    }
    if (message.sqlImportOptions !== undefined) {
      ImportContext_SqlImportOptions.encode(message.sqlImportOptions, writer.uint32(66).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.uri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.database = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.fileType = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.csvImportOptions = ImportContext_SqlCsvImportOptions.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.importUser = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.bakImportOptions = ImportContext_SqlBakImportOptions.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.sqlImportOptions = ImportContext_SqlImportOptions.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportContext {
    return {
      uri: isSet(object.uri) ? globalThis.String(object.uri) : "",
      database: isSet(object.database) ? globalThis.String(object.database) : "",
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      fileType: isSet(object.fileType) ? sqlFileTypeFromJSON(object.fileType) : 0,
      csvImportOptions: isSet(object.csvImportOptions)
        ? ImportContext_SqlCsvImportOptions.fromJSON(object.csvImportOptions)
        : undefined,
      importUser: isSet(object.importUser) ? globalThis.String(object.importUser) : "",
      bakImportOptions: isSet(object.bakImportOptions)
        ? ImportContext_SqlBakImportOptions.fromJSON(object.bakImportOptions)
        : undefined,
      sqlImportOptions: isSet(object.sqlImportOptions)
        ? ImportContext_SqlImportOptions.fromJSON(object.sqlImportOptions)
        : undefined,
    };
  },

  toJSON(message: ImportContext): unknown {
    const obj: any = {};
    if (message.uri !== "") {
      obj.uri = message.uri;
    }
    if (message.database !== "") {
      obj.database = message.database;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.fileType !== 0) {
      obj.fileType = sqlFileTypeToJSON(message.fileType);
    }
    if (message.csvImportOptions !== undefined) {
      obj.csvImportOptions = ImportContext_SqlCsvImportOptions.toJSON(message.csvImportOptions);
    }
    if (message.importUser !== "") {
      obj.importUser = message.importUser;
    }
    if (message.bakImportOptions !== undefined) {
      obj.bakImportOptions = ImportContext_SqlBakImportOptions.toJSON(message.bakImportOptions);
    }
    if (message.sqlImportOptions !== undefined) {
      obj.sqlImportOptions = ImportContext_SqlImportOptions.toJSON(message.sqlImportOptions);
    }
    return obj;
  },

  create(base?: DeepPartial<ImportContext>): ImportContext {
    return ImportContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportContext>): ImportContext {
    const message = createBaseImportContext();
    message.uri = object.uri ?? "";
    message.database = object.database ?? "";
    message.kind = object.kind ?? "";
    message.fileType = object.fileType ?? 0;
    message.csvImportOptions = (object.csvImportOptions !== undefined && object.csvImportOptions !== null)
      ? ImportContext_SqlCsvImportOptions.fromPartial(object.csvImportOptions)
      : undefined;
    message.importUser = object.importUser ?? "";
    message.bakImportOptions = (object.bakImportOptions !== undefined && object.bakImportOptions !== null)
      ? ImportContext_SqlBakImportOptions.fromPartial(object.bakImportOptions)
      : undefined;
    message.sqlImportOptions = (object.sqlImportOptions !== undefined && object.sqlImportOptions !== null)
      ? ImportContext_SqlImportOptions.fromPartial(object.sqlImportOptions)
      : undefined;
    return message;
  },
};

function createBaseImportContext_SqlImportOptions(): ImportContext_SqlImportOptions {
  return { threads: undefined, parallel: undefined, postgresImportOptions: undefined };
}

export const ImportContext_SqlImportOptions: MessageFns<ImportContext_SqlImportOptions> = {
  encode(message: ImportContext_SqlImportOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.threads !== undefined) {
      Int32Value.encode({ value: message.threads! }, writer.uint32(10).fork()).join();
    }
    if (message.parallel !== undefined) {
      BoolValue.encode({ value: message.parallel! }, writer.uint32(18).fork()).join();
    }
    if (message.postgresImportOptions !== undefined) {
      ImportContext_SqlImportOptions_PostgresImportOptions.encode(
        message.postgresImportOptions,
        writer.uint32(26).fork(),
      ).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportContext_SqlImportOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportContext_SqlImportOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.threads = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.parallel = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.postgresImportOptions = ImportContext_SqlImportOptions_PostgresImportOptions.decode(
            reader,
            reader.uint32(),
          );
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportContext_SqlImportOptions {
    return {
      threads: isSet(object.threads) ? Number(object.threads) : undefined,
      parallel: isSet(object.parallel) ? Boolean(object.parallel) : undefined,
      postgresImportOptions: isSet(object.postgresImportOptions)
        ? ImportContext_SqlImportOptions_PostgresImportOptions.fromJSON(object.postgresImportOptions)
        : undefined,
    };
  },

  toJSON(message: ImportContext_SqlImportOptions): unknown {
    const obj: any = {};
    if (message.threads !== undefined) {
      obj.threads = message.threads;
    }
    if (message.parallel !== undefined) {
      obj.parallel = message.parallel;
    }
    if (message.postgresImportOptions !== undefined) {
      obj.postgresImportOptions = ImportContext_SqlImportOptions_PostgresImportOptions.toJSON(
        message.postgresImportOptions,
      );
    }
    return obj;
  },

  create(base?: DeepPartial<ImportContext_SqlImportOptions>): ImportContext_SqlImportOptions {
    return ImportContext_SqlImportOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportContext_SqlImportOptions>): ImportContext_SqlImportOptions {
    const message = createBaseImportContext_SqlImportOptions();
    message.threads = object.threads ?? undefined;
    message.parallel = object.parallel ?? undefined;
    message.postgresImportOptions =
      (object.postgresImportOptions !== undefined && object.postgresImportOptions !== null)
        ? ImportContext_SqlImportOptions_PostgresImportOptions.fromPartial(object.postgresImportOptions)
        : undefined;
    return message;
  },
};

function createBaseImportContext_SqlImportOptions_PostgresImportOptions(): ImportContext_SqlImportOptions_PostgresImportOptions {
  return { clean: undefined, ifExists: undefined };
}

export const ImportContext_SqlImportOptions_PostgresImportOptions: MessageFns<
  ImportContext_SqlImportOptions_PostgresImportOptions
> = {
  encode(
    message: ImportContext_SqlImportOptions_PostgresImportOptions,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.clean !== undefined) {
      BoolValue.encode({ value: message.clean! }, writer.uint32(10).fork()).join();
    }
    if (message.ifExists !== undefined) {
      BoolValue.encode({ value: message.ifExists! }, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportContext_SqlImportOptions_PostgresImportOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportContext_SqlImportOptions_PostgresImportOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.clean = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.ifExists = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportContext_SqlImportOptions_PostgresImportOptions {
    return {
      clean: isSet(object.clean) ? Boolean(object.clean) : undefined,
      ifExists: isSet(object.ifExists) ? Boolean(object.ifExists) : undefined,
    };
  },

  toJSON(message: ImportContext_SqlImportOptions_PostgresImportOptions): unknown {
    const obj: any = {};
    if (message.clean !== undefined) {
      obj.clean = message.clean;
    }
    if (message.ifExists !== undefined) {
      obj.ifExists = message.ifExists;
    }
    return obj;
  },

  create(
    base?: DeepPartial<ImportContext_SqlImportOptions_PostgresImportOptions>,
  ): ImportContext_SqlImportOptions_PostgresImportOptions {
    return ImportContext_SqlImportOptions_PostgresImportOptions.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ImportContext_SqlImportOptions_PostgresImportOptions>,
  ): ImportContext_SqlImportOptions_PostgresImportOptions {
    const message = createBaseImportContext_SqlImportOptions_PostgresImportOptions();
    message.clean = object.clean ?? undefined;
    message.ifExists = object.ifExists ?? undefined;
    return message;
  },
};

function createBaseImportContext_SqlCsvImportOptions(): ImportContext_SqlCsvImportOptions {
  return {
    table: "",
    columns: [],
    escapeCharacter: "",
    quoteCharacter: "",
    fieldsTerminatedBy: "",
    linesTerminatedBy: "",
  };
}

export const ImportContext_SqlCsvImportOptions: MessageFns<ImportContext_SqlCsvImportOptions> = {
  encode(message: ImportContext_SqlCsvImportOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.table !== "") {
      writer.uint32(10).string(message.table);
    }
    for (const v of message.columns) {
      writer.uint32(18).string(v!);
    }
    if (message.escapeCharacter !== "") {
      writer.uint32(34).string(message.escapeCharacter);
    }
    if (message.quoteCharacter !== "") {
      writer.uint32(42).string(message.quoteCharacter);
    }
    if (message.fieldsTerminatedBy !== "") {
      writer.uint32(50).string(message.fieldsTerminatedBy);
    }
    if (message.linesTerminatedBy !== "") {
      writer.uint32(66).string(message.linesTerminatedBy);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportContext_SqlCsvImportOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportContext_SqlCsvImportOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.table = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.columns.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.escapeCharacter = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.quoteCharacter = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.fieldsTerminatedBy = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.linesTerminatedBy = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportContext_SqlCsvImportOptions {
    return {
      table: isSet(object.table) ? globalThis.String(object.table) : "",
      columns: globalThis.Array.isArray(object?.columns) ? object.columns.map((e: any) => globalThis.String(e)) : [],
      escapeCharacter: isSet(object.escapeCharacter) ? globalThis.String(object.escapeCharacter) : "",
      quoteCharacter: isSet(object.quoteCharacter) ? globalThis.String(object.quoteCharacter) : "",
      fieldsTerminatedBy: isSet(object.fieldsTerminatedBy) ? globalThis.String(object.fieldsTerminatedBy) : "",
      linesTerminatedBy: isSet(object.linesTerminatedBy) ? globalThis.String(object.linesTerminatedBy) : "",
    };
  },

  toJSON(message: ImportContext_SqlCsvImportOptions): unknown {
    const obj: any = {};
    if (message.table !== "") {
      obj.table = message.table;
    }
    if (message.columns?.length) {
      obj.columns = message.columns;
    }
    if (message.escapeCharacter !== "") {
      obj.escapeCharacter = message.escapeCharacter;
    }
    if (message.quoteCharacter !== "") {
      obj.quoteCharacter = message.quoteCharacter;
    }
    if (message.fieldsTerminatedBy !== "") {
      obj.fieldsTerminatedBy = message.fieldsTerminatedBy;
    }
    if (message.linesTerminatedBy !== "") {
      obj.linesTerminatedBy = message.linesTerminatedBy;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportContext_SqlCsvImportOptions>): ImportContext_SqlCsvImportOptions {
    return ImportContext_SqlCsvImportOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportContext_SqlCsvImportOptions>): ImportContext_SqlCsvImportOptions {
    const message = createBaseImportContext_SqlCsvImportOptions();
    message.table = object.table ?? "";
    message.columns = object.columns?.map((e) => e) || [];
    message.escapeCharacter = object.escapeCharacter ?? "";
    message.quoteCharacter = object.quoteCharacter ?? "";
    message.fieldsTerminatedBy = object.fieldsTerminatedBy ?? "";
    message.linesTerminatedBy = object.linesTerminatedBy ?? "";
    return message;
  },
};

function createBaseImportContext_SqlBakImportOptions(): ImportContext_SqlBakImportOptions {
  return {
    encryptionOptions: undefined,
    striped: undefined,
    noRecovery: undefined,
    recoveryOnly: undefined,
    bakType: 0,
    stopAt: undefined,
    stopAtMark: "",
  };
}

export const ImportContext_SqlBakImportOptions: MessageFns<ImportContext_SqlBakImportOptions> = {
  encode(message: ImportContext_SqlBakImportOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.encryptionOptions !== undefined) {
      ImportContext_SqlBakImportOptions_EncryptionOptions.encode(message.encryptionOptions, writer.uint32(10).fork())
        .join();
    }
    if (message.striped !== undefined) {
      BoolValue.encode({ value: message.striped! }, writer.uint32(18).fork()).join();
    }
    if (message.noRecovery !== undefined) {
      BoolValue.encode({ value: message.noRecovery! }, writer.uint32(34).fork()).join();
    }
    if (message.recoveryOnly !== undefined) {
      BoolValue.encode({ value: message.recoveryOnly! }, writer.uint32(42).fork()).join();
    }
    if (message.bakType !== 0) {
      writer.uint32(48).int32(message.bakType);
    }
    if (message.stopAt !== undefined) {
      Timestamp.encode(toTimestamp(message.stopAt), writer.uint32(58).fork()).join();
    }
    if (message.stopAtMark !== "") {
      writer.uint32(66).string(message.stopAtMark);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportContext_SqlBakImportOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportContext_SqlBakImportOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.encryptionOptions = ImportContext_SqlBakImportOptions_EncryptionOptions.decode(
            reader,
            reader.uint32(),
          );
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.striped = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.noRecovery = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.recoveryOnly = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.bakType = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.stopAt = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.stopAtMark = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportContext_SqlBakImportOptions {
    return {
      encryptionOptions: isSet(object.encryptionOptions)
        ? ImportContext_SqlBakImportOptions_EncryptionOptions.fromJSON(object.encryptionOptions)
        : undefined,
      striped: isSet(object.striped) ? Boolean(object.striped) : undefined,
      noRecovery: isSet(object.noRecovery) ? Boolean(object.noRecovery) : undefined,
      recoveryOnly: isSet(object.recoveryOnly) ? Boolean(object.recoveryOnly) : undefined,
      bakType: isSet(object.bakType) ? bakTypeFromJSON(object.bakType) : 0,
      stopAt: isSet(object.stopAt) ? fromJsonTimestamp(object.stopAt) : undefined,
      stopAtMark: isSet(object.stopAtMark) ? globalThis.String(object.stopAtMark) : "",
    };
  },

  toJSON(message: ImportContext_SqlBakImportOptions): unknown {
    const obj: any = {};
    if (message.encryptionOptions !== undefined) {
      obj.encryptionOptions = ImportContext_SqlBakImportOptions_EncryptionOptions.toJSON(message.encryptionOptions);
    }
    if (message.striped !== undefined) {
      obj.striped = message.striped;
    }
    if (message.noRecovery !== undefined) {
      obj.noRecovery = message.noRecovery;
    }
    if (message.recoveryOnly !== undefined) {
      obj.recoveryOnly = message.recoveryOnly;
    }
    if (message.bakType !== 0) {
      obj.bakType = bakTypeToJSON(message.bakType);
    }
    if (message.stopAt !== undefined) {
      obj.stopAt = message.stopAt.toISOString();
    }
    if (message.stopAtMark !== "") {
      obj.stopAtMark = message.stopAtMark;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportContext_SqlBakImportOptions>): ImportContext_SqlBakImportOptions {
    return ImportContext_SqlBakImportOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportContext_SqlBakImportOptions>): ImportContext_SqlBakImportOptions {
    const message = createBaseImportContext_SqlBakImportOptions();
    message.encryptionOptions = (object.encryptionOptions !== undefined && object.encryptionOptions !== null)
      ? ImportContext_SqlBakImportOptions_EncryptionOptions.fromPartial(object.encryptionOptions)
      : undefined;
    message.striped = object.striped ?? undefined;
    message.noRecovery = object.noRecovery ?? undefined;
    message.recoveryOnly = object.recoveryOnly ?? undefined;
    message.bakType = object.bakType ?? 0;
    message.stopAt = object.stopAt ?? undefined;
    message.stopAtMark = object.stopAtMark ?? "";
    return message;
  },
};

function createBaseImportContext_SqlBakImportOptions_EncryptionOptions(): ImportContext_SqlBakImportOptions_EncryptionOptions {
  return { certPath: "", pvkPath: "", pvkPassword: "" };
}

export const ImportContext_SqlBakImportOptions_EncryptionOptions: MessageFns<
  ImportContext_SqlBakImportOptions_EncryptionOptions
> = {
  encode(
    message: ImportContext_SqlBakImportOptions_EncryptionOptions,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.certPath !== "") {
      writer.uint32(10).string(message.certPath);
    }
    if (message.pvkPath !== "") {
      writer.uint32(18).string(message.pvkPath);
    }
    if (message.pvkPassword !== "") {
      writer.uint32(26).string(message.pvkPassword);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportContext_SqlBakImportOptions_EncryptionOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportContext_SqlBakImportOptions_EncryptionOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.certPath = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.pvkPath = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pvkPassword = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportContext_SqlBakImportOptions_EncryptionOptions {
    return {
      certPath: isSet(object.certPath) ? globalThis.String(object.certPath) : "",
      pvkPath: isSet(object.pvkPath) ? globalThis.String(object.pvkPath) : "",
      pvkPassword: isSet(object.pvkPassword) ? globalThis.String(object.pvkPassword) : "",
    };
  },

  toJSON(message: ImportContext_SqlBakImportOptions_EncryptionOptions): unknown {
    const obj: any = {};
    if (message.certPath !== "") {
      obj.certPath = message.certPath;
    }
    if (message.pvkPath !== "") {
      obj.pvkPath = message.pvkPath;
    }
    if (message.pvkPassword !== "") {
      obj.pvkPassword = message.pvkPassword;
    }
    return obj;
  },

  create(
    base?: DeepPartial<ImportContext_SqlBakImportOptions_EncryptionOptions>,
  ): ImportContext_SqlBakImportOptions_EncryptionOptions {
    return ImportContext_SqlBakImportOptions_EncryptionOptions.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ImportContext_SqlBakImportOptions_EncryptionOptions>,
  ): ImportContext_SqlBakImportOptions_EncryptionOptions {
    const message = createBaseImportContext_SqlBakImportOptions_EncryptionOptions();
    message.certPath = object.certPath ?? "";
    message.pvkPath = object.pvkPath ?? "";
    message.pvkPassword = object.pvkPassword ?? "";
    return message;
  },
};

function createBaseIpConfiguration(): IpConfiguration {
  return {
    ipv4Enabled: undefined,
    privateNetwork: "",
    requireSsl: undefined,
    authorizedNetworks: [],
    allocatedIpRange: "",
    enablePrivatePathForGoogleCloudServices: undefined,
    sslMode: 0,
    pscConfig: undefined,
    serverCaMode: undefined,
  };
}

export const IpConfiguration: MessageFns<IpConfiguration> = {
  encode(message: IpConfiguration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.ipv4Enabled !== undefined) {
      BoolValue.encode({ value: message.ipv4Enabled! }, writer.uint32(10).fork()).join();
    }
    if (message.privateNetwork !== "") {
      writer.uint32(18).string(message.privateNetwork);
    }
    if (message.requireSsl !== undefined) {
      BoolValue.encode({ value: message.requireSsl! }, writer.uint32(26).fork()).join();
    }
    for (const v of message.authorizedNetworks) {
      AclEntry.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.allocatedIpRange !== "") {
      writer.uint32(50).string(message.allocatedIpRange);
    }
    if (message.enablePrivatePathForGoogleCloudServices !== undefined) {
      BoolValue.encode({ value: message.enablePrivatePathForGoogleCloudServices! }, writer.uint32(58).fork()).join();
    }
    if (message.sslMode !== 0) {
      writer.uint32(64).int32(message.sslMode);
    }
    if (message.pscConfig !== undefined) {
      PscConfig.encode(message.pscConfig, writer.uint32(74).fork()).join();
    }
    if (message.serverCaMode !== undefined) {
      writer.uint32(80).int32(message.serverCaMode);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): IpConfiguration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseIpConfiguration();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.ipv4Enabled = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.privateNetwork = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.requireSsl = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.authorizedNetworks.push(AclEntry.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.allocatedIpRange = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.enablePrivatePathForGoogleCloudServices = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.sslMode = reader.int32() as any;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.pscConfig = PscConfig.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.serverCaMode = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): IpConfiguration {
    return {
      ipv4Enabled: isSet(object.ipv4Enabled) ? Boolean(object.ipv4Enabled) : undefined,
      privateNetwork: isSet(object.privateNetwork) ? globalThis.String(object.privateNetwork) : "",
      requireSsl: isSet(object.requireSsl) ? Boolean(object.requireSsl) : undefined,
      authorizedNetworks: globalThis.Array.isArray(object?.authorizedNetworks)
        ? object.authorizedNetworks.map((e: any) => AclEntry.fromJSON(e))
        : [],
      allocatedIpRange: isSet(object.allocatedIpRange) ? globalThis.String(object.allocatedIpRange) : "",
      enablePrivatePathForGoogleCloudServices: isSet(object.enablePrivatePathForGoogleCloudServices)
        ? Boolean(object.enablePrivatePathForGoogleCloudServices)
        : undefined,
      sslMode: isSet(object.sslMode) ? ipConfiguration_SslModeFromJSON(object.sslMode) : 0,
      pscConfig: isSet(object.pscConfig) ? PscConfig.fromJSON(object.pscConfig) : undefined,
      serverCaMode: isSet(object.serverCaMode) ? ipConfiguration_CaModeFromJSON(object.serverCaMode) : undefined,
    };
  },

  toJSON(message: IpConfiguration): unknown {
    const obj: any = {};
    if (message.ipv4Enabled !== undefined) {
      obj.ipv4Enabled = message.ipv4Enabled;
    }
    if (message.privateNetwork !== "") {
      obj.privateNetwork = message.privateNetwork;
    }
    if (message.requireSsl !== undefined) {
      obj.requireSsl = message.requireSsl;
    }
    if (message.authorizedNetworks?.length) {
      obj.authorizedNetworks = message.authorizedNetworks.map((e) => AclEntry.toJSON(e));
    }
    if (message.allocatedIpRange !== "") {
      obj.allocatedIpRange = message.allocatedIpRange;
    }
    if (message.enablePrivatePathForGoogleCloudServices !== undefined) {
      obj.enablePrivatePathForGoogleCloudServices = message.enablePrivatePathForGoogleCloudServices;
    }
    if (message.sslMode !== 0) {
      obj.sslMode = ipConfiguration_SslModeToJSON(message.sslMode);
    }
    if (message.pscConfig !== undefined) {
      obj.pscConfig = PscConfig.toJSON(message.pscConfig);
    }
    if (message.serverCaMode !== undefined) {
      obj.serverCaMode = ipConfiguration_CaModeToJSON(message.serverCaMode);
    }
    return obj;
  },

  create(base?: DeepPartial<IpConfiguration>): IpConfiguration {
    return IpConfiguration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<IpConfiguration>): IpConfiguration {
    const message = createBaseIpConfiguration();
    message.ipv4Enabled = object.ipv4Enabled ?? undefined;
    message.privateNetwork = object.privateNetwork ?? "";
    message.requireSsl = object.requireSsl ?? undefined;
    message.authorizedNetworks = object.authorizedNetworks?.map((e) => AclEntry.fromPartial(e)) || [];
    message.allocatedIpRange = object.allocatedIpRange ?? "";
    message.enablePrivatePathForGoogleCloudServices = object.enablePrivatePathForGoogleCloudServices ?? undefined;
    message.sslMode = object.sslMode ?? 0;
    message.pscConfig = (object.pscConfig !== undefined && object.pscConfig !== null)
      ? PscConfig.fromPartial(object.pscConfig)
      : undefined;
    message.serverCaMode = object.serverCaMode ?? undefined;
    return message;
  },
};

function createBasePscConfig(): PscConfig {
  return { pscEnabled: undefined, allowedConsumerProjects: [] };
}

export const PscConfig: MessageFns<PscConfig> = {
  encode(message: PscConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.pscEnabled !== undefined) {
      writer.uint32(8).bool(message.pscEnabled);
    }
    for (const v of message.allowedConsumerProjects) {
      writer.uint32(18).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PscConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePscConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.pscEnabled = reader.bool();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.allowedConsumerProjects.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PscConfig {
    return {
      pscEnabled: isSet(object.pscEnabled) ? globalThis.Boolean(object.pscEnabled) : undefined,
      allowedConsumerProjects: globalThis.Array.isArray(object?.allowedConsumerProjects)
        ? object.allowedConsumerProjects.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: PscConfig): unknown {
    const obj: any = {};
    if (message.pscEnabled !== undefined) {
      obj.pscEnabled = message.pscEnabled;
    }
    if (message.allowedConsumerProjects?.length) {
      obj.allowedConsumerProjects = message.allowedConsumerProjects;
    }
    return obj;
  },

  create(base?: DeepPartial<PscConfig>): PscConfig {
    return PscConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PscConfig>): PscConfig {
    const message = createBasePscConfig();
    message.pscEnabled = object.pscEnabled ?? undefined;
    message.allowedConsumerProjects = object.allowedConsumerProjects?.map((e) => e) || [];
    return message;
  },
};

function createBaseLocationPreference(): LocationPreference {
  return { followGaeApplication: "", zone: "", secondaryZone: "", kind: "" };
}

export const LocationPreference: MessageFns<LocationPreference> = {
  encode(message: LocationPreference, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.followGaeApplication !== "") {
      writer.uint32(10).string(message.followGaeApplication);
    }
    if (message.zone !== "") {
      writer.uint32(18).string(message.zone);
    }
    if (message.secondaryZone !== "") {
      writer.uint32(34).string(message.secondaryZone);
    }
    if (message.kind !== "") {
      writer.uint32(26).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LocationPreference {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLocationPreference();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.followGaeApplication = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.zone = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.secondaryZone = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LocationPreference {
    return {
      followGaeApplication: isSet(object.followGaeApplication) ? globalThis.String(object.followGaeApplication) : "",
      zone: isSet(object.zone) ? globalThis.String(object.zone) : "",
      secondaryZone: isSet(object.secondaryZone) ? globalThis.String(object.secondaryZone) : "",
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: LocationPreference): unknown {
    const obj: any = {};
    if (message.followGaeApplication !== "") {
      obj.followGaeApplication = message.followGaeApplication;
    }
    if (message.zone !== "") {
      obj.zone = message.zone;
    }
    if (message.secondaryZone !== "") {
      obj.secondaryZone = message.secondaryZone;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<LocationPreference>): LocationPreference {
    return LocationPreference.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LocationPreference>): LocationPreference {
    const message = createBaseLocationPreference();
    message.followGaeApplication = object.followGaeApplication ?? "";
    message.zone = object.zone ?? "";
    message.secondaryZone = object.secondaryZone ?? "";
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseMaintenanceWindow(): MaintenanceWindow {
  return { hour: undefined, day: undefined, updateTrack: 0, kind: "" };
}

export const MaintenanceWindow: MessageFns<MaintenanceWindow> = {
  encode(message: MaintenanceWindow, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.hour !== undefined) {
      Int32Value.encode({ value: message.hour! }, writer.uint32(10).fork()).join();
    }
    if (message.day !== undefined) {
      Int32Value.encode({ value: message.day! }, writer.uint32(18).fork()).join();
    }
    if (message.updateTrack !== 0) {
      writer.uint32(24).int32(message.updateTrack);
    }
    if (message.kind !== "") {
      writer.uint32(34).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MaintenanceWindow {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMaintenanceWindow();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.hour = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.day = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.updateTrack = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MaintenanceWindow {
    return {
      hour: isSet(object.hour) ? Number(object.hour) : undefined,
      day: isSet(object.day) ? Number(object.day) : undefined,
      updateTrack: isSet(object.updateTrack) ? sqlUpdateTrackFromJSON(object.updateTrack) : 0,
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: MaintenanceWindow): unknown {
    const obj: any = {};
    if (message.hour !== undefined) {
      obj.hour = message.hour;
    }
    if (message.day !== undefined) {
      obj.day = message.day;
    }
    if (message.updateTrack !== 0) {
      obj.updateTrack = sqlUpdateTrackToJSON(message.updateTrack);
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<MaintenanceWindow>): MaintenanceWindow {
    return MaintenanceWindow.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MaintenanceWindow>): MaintenanceWindow {
    const message = createBaseMaintenanceWindow();
    message.hour = object.hour ?? undefined;
    message.day = object.day ?? undefined;
    message.updateTrack = object.updateTrack ?? 0;
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseDenyMaintenancePeriod(): DenyMaintenancePeriod {
  return { startDate: "", endDate: "", time: "" };
}

export const DenyMaintenancePeriod: MessageFns<DenyMaintenancePeriod> = {
  encode(message: DenyMaintenancePeriod, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startDate !== "") {
      writer.uint32(10).string(message.startDate);
    }
    if (message.endDate !== "") {
      writer.uint32(18).string(message.endDate);
    }
    if (message.time !== "") {
      writer.uint32(26).string(message.time);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DenyMaintenancePeriod {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDenyMaintenancePeriod();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startDate = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.endDate = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.time = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DenyMaintenancePeriod {
    return {
      startDate: isSet(object.startDate) ? globalThis.String(object.startDate) : "",
      endDate: isSet(object.endDate) ? globalThis.String(object.endDate) : "",
      time: isSet(object.time) ? globalThis.String(object.time) : "",
    };
  },

  toJSON(message: DenyMaintenancePeriod): unknown {
    const obj: any = {};
    if (message.startDate !== "") {
      obj.startDate = message.startDate;
    }
    if (message.endDate !== "") {
      obj.endDate = message.endDate;
    }
    if (message.time !== "") {
      obj.time = message.time;
    }
    return obj;
  },

  create(base?: DeepPartial<DenyMaintenancePeriod>): DenyMaintenancePeriod {
    return DenyMaintenancePeriod.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DenyMaintenancePeriod>): DenyMaintenancePeriod {
    const message = createBaseDenyMaintenancePeriod();
    message.startDate = object.startDate ?? "";
    message.endDate = object.endDate ?? "";
    message.time = object.time ?? "";
    return message;
  },
};

function createBaseInsightsConfig(): InsightsConfig {
  return {
    queryInsightsEnabled: false,
    recordClientAddress: false,
    recordApplicationTags: false,
    queryStringLength: undefined,
    queryPlansPerMinute: undefined,
  };
}

export const InsightsConfig: MessageFns<InsightsConfig> = {
  encode(message: InsightsConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.queryInsightsEnabled !== false) {
      writer.uint32(8).bool(message.queryInsightsEnabled);
    }
    if (message.recordClientAddress !== false) {
      writer.uint32(16).bool(message.recordClientAddress);
    }
    if (message.recordApplicationTags !== false) {
      writer.uint32(24).bool(message.recordApplicationTags);
    }
    if (message.queryStringLength !== undefined) {
      Int32Value.encode({ value: message.queryStringLength! }, writer.uint32(34).fork()).join();
    }
    if (message.queryPlansPerMinute !== undefined) {
      Int32Value.encode({ value: message.queryPlansPerMinute! }, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InsightsConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInsightsConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.queryInsightsEnabled = reader.bool();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.recordClientAddress = reader.bool();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.recordApplicationTags = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.queryStringLength = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.queryPlansPerMinute = Int32Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InsightsConfig {
    return {
      queryInsightsEnabled: isSet(object.queryInsightsEnabled)
        ? globalThis.Boolean(object.queryInsightsEnabled)
        : false,
      recordClientAddress: isSet(object.recordClientAddress) ? globalThis.Boolean(object.recordClientAddress) : false,
      recordApplicationTags: isSet(object.recordApplicationTags)
        ? globalThis.Boolean(object.recordApplicationTags)
        : false,
      queryStringLength: isSet(object.queryStringLength) ? Number(object.queryStringLength) : undefined,
      queryPlansPerMinute: isSet(object.queryPlansPerMinute) ? Number(object.queryPlansPerMinute) : undefined,
    };
  },

  toJSON(message: InsightsConfig): unknown {
    const obj: any = {};
    if (message.queryInsightsEnabled !== false) {
      obj.queryInsightsEnabled = message.queryInsightsEnabled;
    }
    if (message.recordClientAddress !== false) {
      obj.recordClientAddress = message.recordClientAddress;
    }
    if (message.recordApplicationTags !== false) {
      obj.recordApplicationTags = message.recordApplicationTags;
    }
    if (message.queryStringLength !== undefined) {
      obj.queryStringLength = message.queryStringLength;
    }
    if (message.queryPlansPerMinute !== undefined) {
      obj.queryPlansPerMinute = message.queryPlansPerMinute;
    }
    return obj;
  },

  create(base?: DeepPartial<InsightsConfig>): InsightsConfig {
    return InsightsConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InsightsConfig>): InsightsConfig {
    const message = createBaseInsightsConfig();
    message.queryInsightsEnabled = object.queryInsightsEnabled ?? false;
    message.recordClientAddress = object.recordClientAddress ?? false;
    message.recordApplicationTags = object.recordApplicationTags ?? false;
    message.queryStringLength = object.queryStringLength ?? undefined;
    message.queryPlansPerMinute = object.queryPlansPerMinute ?? undefined;
    return message;
  },
};

function createBaseMySqlReplicaConfiguration(): MySqlReplicaConfiguration {
  return {
    dumpFilePath: "",
    username: "",
    password: "",
    connectRetryInterval: undefined,
    masterHeartbeatPeriod: undefined,
    caCertificate: "",
    clientCertificate: "",
    clientKey: "",
    sslCipher: "",
    verifyServerCertificate: undefined,
    kind: "",
  };
}

export const MySqlReplicaConfiguration: MessageFns<MySqlReplicaConfiguration> = {
  encode(message: MySqlReplicaConfiguration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dumpFilePath !== "") {
      writer.uint32(10).string(message.dumpFilePath);
    }
    if (message.username !== "") {
      writer.uint32(18).string(message.username);
    }
    if (message.password !== "") {
      writer.uint32(26).string(message.password);
    }
    if (message.connectRetryInterval !== undefined) {
      Int32Value.encode({ value: message.connectRetryInterval! }, writer.uint32(34).fork()).join();
    }
    if (message.masterHeartbeatPeriod !== undefined) {
      Int64Value.encode({ value: message.masterHeartbeatPeriod! }, writer.uint32(42).fork()).join();
    }
    if (message.caCertificate !== "") {
      writer.uint32(50).string(message.caCertificate);
    }
    if (message.clientCertificate !== "") {
      writer.uint32(58).string(message.clientCertificate);
    }
    if (message.clientKey !== "") {
      writer.uint32(66).string(message.clientKey);
    }
    if (message.sslCipher !== "") {
      writer.uint32(74).string(message.sslCipher);
    }
    if (message.verifyServerCertificate !== undefined) {
      BoolValue.encode({ value: message.verifyServerCertificate! }, writer.uint32(82).fork()).join();
    }
    if (message.kind !== "") {
      writer.uint32(90).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MySqlReplicaConfiguration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMySqlReplicaConfiguration();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dumpFilePath = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.username = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.password = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.connectRetryInterval = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.masterHeartbeatPeriod = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.caCertificate = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.clientCertificate = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.clientKey = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.sslCipher = reader.string();
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.verifyServerCertificate = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MySqlReplicaConfiguration {
    return {
      dumpFilePath: isSet(object.dumpFilePath) ? globalThis.String(object.dumpFilePath) : "",
      username: isSet(object.username) ? globalThis.String(object.username) : "",
      password: isSet(object.password) ? globalThis.String(object.password) : "",
      connectRetryInterval: isSet(object.connectRetryInterval) ? Number(object.connectRetryInterval) : undefined,
      masterHeartbeatPeriod: isSet(object.masterHeartbeatPeriod)
        ? Long.fromValue(object.masterHeartbeatPeriod)
        : undefined,
      caCertificate: isSet(object.caCertificate) ? globalThis.String(object.caCertificate) : "",
      clientCertificate: isSet(object.clientCertificate) ? globalThis.String(object.clientCertificate) : "",
      clientKey: isSet(object.clientKey) ? globalThis.String(object.clientKey) : "",
      sslCipher: isSet(object.sslCipher) ? globalThis.String(object.sslCipher) : "",
      verifyServerCertificate: isSet(object.verifyServerCertificate)
        ? Boolean(object.verifyServerCertificate)
        : undefined,
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: MySqlReplicaConfiguration): unknown {
    const obj: any = {};
    if (message.dumpFilePath !== "") {
      obj.dumpFilePath = message.dumpFilePath;
    }
    if (message.username !== "") {
      obj.username = message.username;
    }
    if (message.password !== "") {
      obj.password = message.password;
    }
    if (message.connectRetryInterval !== undefined) {
      obj.connectRetryInterval = message.connectRetryInterval;
    }
    if (message.masterHeartbeatPeriod !== undefined) {
      obj.masterHeartbeatPeriod = message.masterHeartbeatPeriod;
    }
    if (message.caCertificate !== "") {
      obj.caCertificate = message.caCertificate;
    }
    if (message.clientCertificate !== "") {
      obj.clientCertificate = message.clientCertificate;
    }
    if (message.clientKey !== "") {
      obj.clientKey = message.clientKey;
    }
    if (message.sslCipher !== "") {
      obj.sslCipher = message.sslCipher;
    }
    if (message.verifyServerCertificate !== undefined) {
      obj.verifyServerCertificate = message.verifyServerCertificate;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<MySqlReplicaConfiguration>): MySqlReplicaConfiguration {
    return MySqlReplicaConfiguration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MySqlReplicaConfiguration>): MySqlReplicaConfiguration {
    const message = createBaseMySqlReplicaConfiguration();
    message.dumpFilePath = object.dumpFilePath ?? "";
    message.username = object.username ?? "";
    message.password = object.password ?? "";
    message.connectRetryInterval = object.connectRetryInterval ?? undefined;
    message.masterHeartbeatPeriod =
      (object.masterHeartbeatPeriod !== undefined && object.masterHeartbeatPeriod !== null)
        ? Long.fromValue(object.masterHeartbeatPeriod)
        : undefined;
    message.caCertificate = object.caCertificate ?? "";
    message.clientCertificate = object.clientCertificate ?? "";
    message.clientKey = object.clientKey ?? "";
    message.sslCipher = object.sslCipher ?? "";
    message.verifyServerCertificate = object.verifyServerCertificate ?? undefined;
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseDiskEncryptionConfiguration(): DiskEncryptionConfiguration {
  return { kmsKeyName: "", kind: "" };
}

export const DiskEncryptionConfiguration: MessageFns<DiskEncryptionConfiguration> = {
  encode(message: DiskEncryptionConfiguration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kmsKeyName !== "") {
      writer.uint32(10).string(message.kmsKeyName);
    }
    if (message.kind !== "") {
      writer.uint32(18).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DiskEncryptionConfiguration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDiskEncryptionConfiguration();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kmsKeyName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DiskEncryptionConfiguration {
    return {
      kmsKeyName: isSet(object.kmsKeyName) ? globalThis.String(object.kmsKeyName) : "",
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: DiskEncryptionConfiguration): unknown {
    const obj: any = {};
    if (message.kmsKeyName !== "") {
      obj.kmsKeyName = message.kmsKeyName;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<DiskEncryptionConfiguration>): DiskEncryptionConfiguration {
    return DiskEncryptionConfiguration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DiskEncryptionConfiguration>): DiskEncryptionConfiguration {
    const message = createBaseDiskEncryptionConfiguration();
    message.kmsKeyName = object.kmsKeyName ?? "";
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseDiskEncryptionStatus(): DiskEncryptionStatus {
  return { kmsKeyVersionName: "", kind: "" };
}

export const DiskEncryptionStatus: MessageFns<DiskEncryptionStatus> = {
  encode(message: DiskEncryptionStatus, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kmsKeyVersionName !== "") {
      writer.uint32(10).string(message.kmsKeyVersionName);
    }
    if (message.kind !== "") {
      writer.uint32(18).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DiskEncryptionStatus {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDiskEncryptionStatus();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kmsKeyVersionName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DiskEncryptionStatus {
    return {
      kmsKeyVersionName: isSet(object.kmsKeyVersionName) ? globalThis.String(object.kmsKeyVersionName) : "",
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: DiskEncryptionStatus): unknown {
    const obj: any = {};
    if (message.kmsKeyVersionName !== "") {
      obj.kmsKeyVersionName = message.kmsKeyVersionName;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<DiskEncryptionStatus>): DiskEncryptionStatus {
    return DiskEncryptionStatus.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DiskEncryptionStatus>): DiskEncryptionStatus {
    const message = createBaseDiskEncryptionStatus();
    message.kmsKeyVersionName = object.kmsKeyVersionName ?? "";
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseIpMapping(): IpMapping {
  return { type: 0, ipAddress: "", timeToRetire: undefined };
}

export const IpMapping: MessageFns<IpMapping> = {
  encode(message: IpMapping, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.type !== 0) {
      writer.uint32(8).int32(message.type);
    }
    if (message.ipAddress !== "") {
      writer.uint32(18).string(message.ipAddress);
    }
    if (message.timeToRetire !== undefined) {
      Timestamp.encode(toTimestamp(message.timeToRetire), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): IpMapping {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseIpMapping();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.ipAddress = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.timeToRetire = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): IpMapping {
    return {
      type: isSet(object.type) ? sqlIpAddressTypeFromJSON(object.type) : 0,
      ipAddress: isSet(object.ipAddress) ? globalThis.String(object.ipAddress) : "",
      timeToRetire: isSet(object.timeToRetire) ? fromJsonTimestamp(object.timeToRetire) : undefined,
    };
  },

  toJSON(message: IpMapping): unknown {
    const obj: any = {};
    if (message.type !== 0) {
      obj.type = sqlIpAddressTypeToJSON(message.type);
    }
    if (message.ipAddress !== "") {
      obj.ipAddress = message.ipAddress;
    }
    if (message.timeToRetire !== undefined) {
      obj.timeToRetire = message.timeToRetire.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<IpMapping>): IpMapping {
    return IpMapping.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<IpMapping>): IpMapping {
    const message = createBaseIpMapping();
    message.type = object.type ?? 0;
    message.ipAddress = object.ipAddress ?? "";
    message.timeToRetire = object.timeToRetire ?? undefined;
    return message;
  },
};

function createBaseOperation(): Operation {
  return {
    kind: "",
    targetLink: "",
    status: 0,
    user: "",
    insertTime: undefined,
    startTime: undefined,
    endTime: undefined,
    error: undefined,
    apiWarning: undefined,
    operationType: 0,
    importContext: undefined,
    exportContext: undefined,
    backupContext: undefined,
    name: "",
    targetId: "",
    selfLink: "",
    targetProject: "",
    acquireSsrsLeaseContext: undefined,
  };
}

export const Operation: MessageFns<Operation> = {
  encode(message: Operation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.targetLink !== "") {
      writer.uint32(18).string(message.targetLink);
    }
    if (message.status !== 0) {
      writer.uint32(24).int32(message.status);
    }
    if (message.user !== "") {
      writer.uint32(34).string(message.user);
    }
    if (message.insertTime !== undefined) {
      Timestamp.encode(toTimestamp(message.insertTime), writer.uint32(42).fork()).join();
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(50).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(58).fork()).join();
    }
    if (message.error !== undefined) {
      OperationErrors.encode(message.error, writer.uint32(66).fork()).join();
    }
    if (message.apiWarning !== undefined) {
      ApiWarning.encode(message.apiWarning, writer.uint32(154).fork()).join();
    }
    if (message.operationType !== 0) {
      writer.uint32(72).int32(message.operationType);
    }
    if (message.importContext !== undefined) {
      ImportContext.encode(message.importContext, writer.uint32(82).fork()).join();
    }
    if (message.exportContext !== undefined) {
      ExportContext.encode(message.exportContext, writer.uint32(90).fork()).join();
    }
    if (message.backupContext !== undefined) {
      BackupContext.encode(message.backupContext, writer.uint32(138).fork()).join();
    }
    if (message.name !== "") {
      writer.uint32(98).string(message.name);
    }
    if (message.targetId !== "") {
      writer.uint32(106).string(message.targetId);
    }
    if (message.selfLink !== "") {
      writer.uint32(114).string(message.selfLink);
    }
    if (message.targetProject !== "") {
      writer.uint32(122).string(message.targetProject);
    }
    if (message.acquireSsrsLeaseContext !== undefined) {
      AcquireSsrsLeaseContext.encode(message.acquireSsrsLeaseContext, writer.uint32(162).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Operation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOperation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.targetLink = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.status = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.user = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.insertTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.error = OperationErrors.decode(reader, reader.uint32());
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.apiWarning = ApiWarning.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.operationType = reader.int32() as any;
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.importContext = ImportContext.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.exportContext = ExportContext.decode(reader, reader.uint32());
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.backupContext = BackupContext.decode(reader, reader.uint32());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.name = reader.string();
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.targetId = reader.string();
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.selfLink = reader.string();
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.targetProject = reader.string();
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.acquireSsrsLeaseContext = AcquireSsrsLeaseContext.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Operation {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      targetLink: isSet(object.targetLink) ? globalThis.String(object.targetLink) : "",
      status: isSet(object.status) ? operation_SqlOperationStatusFromJSON(object.status) : 0,
      user: isSet(object.user) ? globalThis.String(object.user) : "",
      insertTime: isSet(object.insertTime) ? fromJsonTimestamp(object.insertTime) : undefined,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      error: isSet(object.error) ? OperationErrors.fromJSON(object.error) : undefined,
      apiWarning: isSet(object.apiWarning) ? ApiWarning.fromJSON(object.apiWarning) : undefined,
      operationType: isSet(object.operationType) ? operation_SqlOperationTypeFromJSON(object.operationType) : 0,
      importContext: isSet(object.importContext) ? ImportContext.fromJSON(object.importContext) : undefined,
      exportContext: isSet(object.exportContext) ? ExportContext.fromJSON(object.exportContext) : undefined,
      backupContext: isSet(object.backupContext) ? BackupContext.fromJSON(object.backupContext) : undefined,
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      targetId: isSet(object.targetId) ? globalThis.String(object.targetId) : "",
      selfLink: isSet(object.selfLink) ? globalThis.String(object.selfLink) : "",
      targetProject: isSet(object.targetProject) ? globalThis.String(object.targetProject) : "",
      acquireSsrsLeaseContext: isSet(object.acquireSsrsLeaseContext)
        ? AcquireSsrsLeaseContext.fromJSON(object.acquireSsrsLeaseContext)
        : undefined,
    };
  },

  toJSON(message: Operation): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.targetLink !== "") {
      obj.targetLink = message.targetLink;
    }
    if (message.status !== 0) {
      obj.status = operation_SqlOperationStatusToJSON(message.status);
    }
    if (message.user !== "") {
      obj.user = message.user;
    }
    if (message.insertTime !== undefined) {
      obj.insertTime = message.insertTime.toISOString();
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (message.error !== undefined) {
      obj.error = OperationErrors.toJSON(message.error);
    }
    if (message.apiWarning !== undefined) {
      obj.apiWarning = ApiWarning.toJSON(message.apiWarning);
    }
    if (message.operationType !== 0) {
      obj.operationType = operation_SqlOperationTypeToJSON(message.operationType);
    }
    if (message.importContext !== undefined) {
      obj.importContext = ImportContext.toJSON(message.importContext);
    }
    if (message.exportContext !== undefined) {
      obj.exportContext = ExportContext.toJSON(message.exportContext);
    }
    if (message.backupContext !== undefined) {
      obj.backupContext = BackupContext.toJSON(message.backupContext);
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.targetId !== "") {
      obj.targetId = message.targetId;
    }
    if (message.selfLink !== "") {
      obj.selfLink = message.selfLink;
    }
    if (message.targetProject !== "") {
      obj.targetProject = message.targetProject;
    }
    if (message.acquireSsrsLeaseContext !== undefined) {
      obj.acquireSsrsLeaseContext = AcquireSsrsLeaseContext.toJSON(message.acquireSsrsLeaseContext);
    }
    return obj;
  },

  create(base?: DeepPartial<Operation>): Operation {
    return Operation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Operation>): Operation {
    const message = createBaseOperation();
    message.kind = object.kind ?? "";
    message.targetLink = object.targetLink ?? "";
    message.status = object.status ?? 0;
    message.user = object.user ?? "";
    message.insertTime = object.insertTime ?? undefined;
    message.startTime = object.startTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    message.error = (object.error !== undefined && object.error !== null)
      ? OperationErrors.fromPartial(object.error)
      : undefined;
    message.apiWarning = (object.apiWarning !== undefined && object.apiWarning !== null)
      ? ApiWarning.fromPartial(object.apiWarning)
      : undefined;
    message.operationType = object.operationType ?? 0;
    message.importContext = (object.importContext !== undefined && object.importContext !== null)
      ? ImportContext.fromPartial(object.importContext)
      : undefined;
    message.exportContext = (object.exportContext !== undefined && object.exportContext !== null)
      ? ExportContext.fromPartial(object.exportContext)
      : undefined;
    message.backupContext = (object.backupContext !== undefined && object.backupContext !== null)
      ? BackupContext.fromPartial(object.backupContext)
      : undefined;
    message.name = object.name ?? "";
    message.targetId = object.targetId ?? "";
    message.selfLink = object.selfLink ?? "";
    message.targetProject = object.targetProject ?? "";
    message.acquireSsrsLeaseContext =
      (object.acquireSsrsLeaseContext !== undefined && object.acquireSsrsLeaseContext !== null)
        ? AcquireSsrsLeaseContext.fromPartial(object.acquireSsrsLeaseContext)
        : undefined;
    return message;
  },
};

function createBaseOperationError(): OperationError {
  return { kind: "", code: "", message: "" };
}

export const OperationError: MessageFns<OperationError> = {
  encode(message: OperationError, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.code !== "") {
      writer.uint32(18).string(message.code);
    }
    if (message.message !== "") {
      writer.uint32(26).string(message.message);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OperationError {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOperationError();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.code = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.message = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OperationError {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      code: isSet(object.code) ? globalThis.String(object.code) : "",
      message: isSet(object.message) ? globalThis.String(object.message) : "",
    };
  },

  toJSON(message: OperationError): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.code !== "") {
      obj.code = message.code;
    }
    if (message.message !== "") {
      obj.message = message.message;
    }
    return obj;
  },

  create(base?: DeepPartial<OperationError>): OperationError {
    return OperationError.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OperationError>): OperationError {
    const message = createBaseOperationError();
    message.kind = object.kind ?? "";
    message.code = object.code ?? "";
    message.message = object.message ?? "";
    return message;
  },
};

function createBaseOperationErrors(): OperationErrors {
  return { kind: "", errors: [] };
}

export const OperationErrors: MessageFns<OperationErrors> = {
  encode(message: OperationErrors, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    for (const v of message.errors) {
      OperationError.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OperationErrors {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOperationErrors();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.errors.push(OperationError.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OperationErrors {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      errors: globalThis.Array.isArray(object?.errors) ? object.errors.map((e: any) => OperationError.fromJSON(e)) : [],
    };
  },

  toJSON(message: OperationErrors): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.errors?.length) {
      obj.errors = message.errors.map((e) => OperationError.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<OperationErrors>): OperationErrors {
    return OperationErrors.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OperationErrors>): OperationErrors {
    const message = createBaseOperationErrors();
    message.kind = object.kind ?? "";
    message.errors = object.errors?.map((e) => OperationError.fromPartial(e)) || [];
    return message;
  },
};

function createBasePasswordValidationPolicy(): PasswordValidationPolicy {
  return {
    minLength: undefined,
    complexity: 0,
    reuseInterval: undefined,
    disallowUsernameSubstring: undefined,
    passwordChangeInterval: undefined,
    enablePasswordPolicy: undefined,
    disallowCompromisedCredentials: undefined,
  };
}

export const PasswordValidationPolicy: MessageFns<PasswordValidationPolicy> = {
  encode(message: PasswordValidationPolicy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.minLength !== undefined) {
      Int32Value.encode({ value: message.minLength! }, writer.uint32(10).fork()).join();
    }
    if (message.complexity !== 0) {
      writer.uint32(16).int32(message.complexity);
    }
    if (message.reuseInterval !== undefined) {
      Int32Value.encode({ value: message.reuseInterval! }, writer.uint32(26).fork()).join();
    }
    if (message.disallowUsernameSubstring !== undefined) {
      BoolValue.encode({ value: message.disallowUsernameSubstring! }, writer.uint32(34).fork()).join();
    }
    if (message.passwordChangeInterval !== undefined) {
      Duration.encode(message.passwordChangeInterval, writer.uint32(42).fork()).join();
    }
    if (message.enablePasswordPolicy !== undefined) {
      BoolValue.encode({ value: message.enablePasswordPolicy! }, writer.uint32(50).fork()).join();
    }
    if (message.disallowCompromisedCredentials !== undefined) {
      BoolValue.encode({ value: message.disallowCompromisedCredentials! }, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PasswordValidationPolicy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePasswordValidationPolicy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.minLength = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.complexity = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.reuseInterval = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.disallowUsernameSubstring = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.passwordChangeInterval = Duration.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.enablePasswordPolicy = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.disallowCompromisedCredentials = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PasswordValidationPolicy {
    return {
      minLength: isSet(object.minLength) ? Number(object.minLength) : undefined,
      complexity: isSet(object.complexity) ? passwordValidationPolicy_ComplexityFromJSON(object.complexity) : 0,
      reuseInterval: isSet(object.reuseInterval) ? Number(object.reuseInterval) : undefined,
      disallowUsernameSubstring: isSet(object.disallowUsernameSubstring)
        ? Boolean(object.disallowUsernameSubstring)
        : undefined,
      passwordChangeInterval: isSet(object.passwordChangeInterval)
        ? Duration.fromJSON(object.passwordChangeInterval)
        : undefined,
      enablePasswordPolicy: isSet(object.enablePasswordPolicy) ? Boolean(object.enablePasswordPolicy) : undefined,
      disallowCompromisedCredentials: isSet(object.disallowCompromisedCredentials)
        ? Boolean(object.disallowCompromisedCredentials)
        : undefined,
    };
  },

  toJSON(message: PasswordValidationPolicy): unknown {
    const obj: any = {};
    if (message.minLength !== undefined) {
      obj.minLength = message.minLength;
    }
    if (message.complexity !== 0) {
      obj.complexity = passwordValidationPolicy_ComplexityToJSON(message.complexity);
    }
    if (message.reuseInterval !== undefined) {
      obj.reuseInterval = message.reuseInterval;
    }
    if (message.disallowUsernameSubstring !== undefined) {
      obj.disallowUsernameSubstring = message.disallowUsernameSubstring;
    }
    if (message.passwordChangeInterval !== undefined) {
      obj.passwordChangeInterval = Duration.toJSON(message.passwordChangeInterval);
    }
    if (message.enablePasswordPolicy !== undefined) {
      obj.enablePasswordPolicy = message.enablePasswordPolicy;
    }
    if (message.disallowCompromisedCredentials !== undefined) {
      obj.disallowCompromisedCredentials = message.disallowCompromisedCredentials;
    }
    return obj;
  },

  create(base?: DeepPartial<PasswordValidationPolicy>): PasswordValidationPolicy {
    return PasswordValidationPolicy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PasswordValidationPolicy>): PasswordValidationPolicy {
    const message = createBasePasswordValidationPolicy();
    message.minLength = object.minLength ?? undefined;
    message.complexity = object.complexity ?? 0;
    message.reuseInterval = object.reuseInterval ?? undefined;
    message.disallowUsernameSubstring = object.disallowUsernameSubstring ?? undefined;
    message.passwordChangeInterval =
      (object.passwordChangeInterval !== undefined && object.passwordChangeInterval !== null)
        ? Duration.fromPartial(object.passwordChangeInterval)
        : undefined;
    message.enablePasswordPolicy = object.enablePasswordPolicy ?? undefined;
    message.disallowCompromisedCredentials = object.disallowCompromisedCredentials ?? undefined;
    return message;
  },
};

function createBaseDataCacheConfig(): DataCacheConfig {
  return { dataCacheEnabled: false };
}

export const DataCacheConfig: MessageFns<DataCacheConfig> = {
  encode(message: DataCacheConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dataCacheEnabled !== false) {
      writer.uint32(8).bool(message.dataCacheEnabled);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DataCacheConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDataCacheConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.dataCacheEnabled = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DataCacheConfig {
    return { dataCacheEnabled: isSet(object.dataCacheEnabled) ? globalThis.Boolean(object.dataCacheEnabled) : false };
  },

  toJSON(message: DataCacheConfig): unknown {
    const obj: any = {};
    if (message.dataCacheEnabled !== false) {
      obj.dataCacheEnabled = message.dataCacheEnabled;
    }
    return obj;
  },

  create(base?: DeepPartial<DataCacheConfig>): DataCacheConfig {
    return DataCacheConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DataCacheConfig>): DataCacheConfig {
    const message = createBaseDataCacheConfig();
    message.dataCacheEnabled = object.dataCacheEnabled ?? false;
    return message;
  },
};

function createBaseSettings(): Settings {
  return {
    settingsVersion: undefined,
    authorizedGaeApplications: [],
    tier: "",
    kind: "",
    userLabels: {},
    availabilityType: 0,
    pricingPlan: 0,
    replicationType: 0,
    storageAutoResizeLimit: undefined,
    activationPolicy: 0,
    ipConfiguration: undefined,
    storageAutoResize: undefined,
    locationPreference: undefined,
    databaseFlags: [],
    dataDiskType: 0,
    maintenanceWindow: undefined,
    backupConfiguration: undefined,
    databaseReplicationEnabled: undefined,
    crashSafeReplicationEnabled: undefined,
    dataDiskSizeGb: undefined,
    activeDirectoryConfig: undefined,
    collation: "",
    denyMaintenancePeriods: [],
    insightsConfig: undefined,
    passwordValidationPolicy: undefined,
    sqlServerAuditConfig: undefined,
    edition: 0,
    connectorEnforcement: 0,
    deletionProtectionEnabled: undefined,
    timeZone: "",
    advancedMachineFeatures: undefined,
    dataCacheConfig: undefined,
    enableGoogleMlIntegration: undefined,
    enableDataplexIntegration: undefined,
  };
}

export const Settings: MessageFns<Settings> = {
  encode(message: Settings, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.settingsVersion !== undefined) {
      Int64Value.encode({ value: message.settingsVersion! }, writer.uint32(10).fork()).join();
    }
    for (const v of message.authorizedGaeApplications) {
      writer.uint32(18).string(v!);
    }
    if (message.tier !== "") {
      writer.uint32(26).string(message.tier);
    }
    if (message.kind !== "") {
      writer.uint32(34).string(message.kind);
    }
    Object.entries(message.userLabels).forEach(([key, value]) => {
      Settings_UserLabelsEntry.encode({ key: key as any, value }, writer.uint32(42).fork()).join();
    });
    if (message.availabilityType !== 0) {
      writer.uint32(48).int32(message.availabilityType);
    }
    if (message.pricingPlan !== 0) {
      writer.uint32(56).int32(message.pricingPlan);
    }
    if (message.replicationType !== 0) {
      writer.uint32(64).int32(message.replicationType);
    }
    if (message.storageAutoResizeLimit !== undefined) {
      Int64Value.encode({ value: message.storageAutoResizeLimit! }, writer.uint32(74).fork()).join();
    }
    if (message.activationPolicy !== 0) {
      writer.uint32(80).int32(message.activationPolicy);
    }
    if (message.ipConfiguration !== undefined) {
      IpConfiguration.encode(message.ipConfiguration, writer.uint32(90).fork()).join();
    }
    if (message.storageAutoResize !== undefined) {
      BoolValue.encode({ value: message.storageAutoResize! }, writer.uint32(98).fork()).join();
    }
    if (message.locationPreference !== undefined) {
      LocationPreference.encode(message.locationPreference, writer.uint32(106).fork()).join();
    }
    for (const v of message.databaseFlags) {
      DatabaseFlags.encode(v!, writer.uint32(114).fork()).join();
    }
    if (message.dataDiskType !== 0) {
      writer.uint32(120).int32(message.dataDiskType);
    }
    if (message.maintenanceWindow !== undefined) {
      MaintenanceWindow.encode(message.maintenanceWindow, writer.uint32(130).fork()).join();
    }
    if (message.backupConfiguration !== undefined) {
      BackupConfiguration.encode(message.backupConfiguration, writer.uint32(138).fork()).join();
    }
    if (message.databaseReplicationEnabled !== undefined) {
      BoolValue.encode({ value: message.databaseReplicationEnabled! }, writer.uint32(146).fork()).join();
    }
    if (message.crashSafeReplicationEnabled !== undefined) {
      BoolValue.encode({ value: message.crashSafeReplicationEnabled! }, writer.uint32(154).fork()).join();
    }
    if (message.dataDiskSizeGb !== undefined) {
      Int64Value.encode({ value: message.dataDiskSizeGb! }, writer.uint32(162).fork()).join();
    }
    if (message.activeDirectoryConfig !== undefined) {
      SqlActiveDirectoryConfig.encode(message.activeDirectoryConfig, writer.uint32(178).fork()).join();
    }
    if (message.collation !== "") {
      writer.uint32(186).string(message.collation);
    }
    for (const v of message.denyMaintenancePeriods) {
      DenyMaintenancePeriod.encode(v!, writer.uint32(194).fork()).join();
    }
    if (message.insightsConfig !== undefined) {
      InsightsConfig.encode(message.insightsConfig, writer.uint32(202).fork()).join();
    }
    if (message.passwordValidationPolicy !== undefined) {
      PasswordValidationPolicy.encode(message.passwordValidationPolicy, writer.uint32(218).fork()).join();
    }
    if (message.sqlServerAuditConfig !== undefined) {
      SqlServerAuditConfig.encode(message.sqlServerAuditConfig, writer.uint32(234).fork()).join();
    }
    if (message.edition !== 0) {
      writer.uint32(304).int32(message.edition);
    }
    if (message.connectorEnforcement !== 0) {
      writer.uint32(256).int32(message.connectorEnforcement);
    }
    if (message.deletionProtectionEnabled !== undefined) {
      BoolValue.encode({ value: message.deletionProtectionEnabled! }, writer.uint32(266).fork()).join();
    }
    if (message.timeZone !== "") {
      writer.uint32(274).string(message.timeZone);
    }
    if (message.advancedMachineFeatures !== undefined) {
      AdvancedMachineFeatures.encode(message.advancedMachineFeatures, writer.uint32(282).fork()).join();
    }
    if (message.dataCacheConfig !== undefined) {
      DataCacheConfig.encode(message.dataCacheConfig, writer.uint32(298).fork()).join();
    }
    if (message.enableGoogleMlIntegration !== undefined) {
      BoolValue.encode({ value: message.enableGoogleMlIntegration! }, writer.uint32(322).fork()).join();
    }
    if (message.enableDataplexIntegration !== undefined) {
      BoolValue.encode({ value: message.enableDataplexIntegration! }, writer.uint32(330).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Settings {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSettings();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.settingsVersion = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.authorizedGaeApplications.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.tier = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          const entry5 = Settings_UserLabelsEntry.decode(reader, reader.uint32());
          if (entry5.value !== undefined) {
            message.userLabels[entry5.key] = entry5.value;
          }
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.availabilityType = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.pricingPlan = reader.int32() as any;
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.replicationType = reader.int32() as any;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.storageAutoResizeLimit = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.activationPolicy = reader.int32() as any;
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.ipConfiguration = IpConfiguration.decode(reader, reader.uint32());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.storageAutoResize = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.locationPreference = LocationPreference.decode(reader, reader.uint32());
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.databaseFlags.push(DatabaseFlags.decode(reader, reader.uint32()));
          continue;
        case 15:
          if (tag !== 120) {
            break;
          }

          message.dataDiskType = reader.int32() as any;
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.maintenanceWindow = MaintenanceWindow.decode(reader, reader.uint32());
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.backupConfiguration = BackupConfiguration.decode(reader, reader.uint32());
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.databaseReplicationEnabled = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.crashSafeReplicationEnabled = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.dataDiskSizeGb = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 22:
          if (tag !== 178) {
            break;
          }

          message.activeDirectoryConfig = SqlActiveDirectoryConfig.decode(reader, reader.uint32());
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.collation = reader.string();
          continue;
        case 24:
          if (tag !== 194) {
            break;
          }

          message.denyMaintenancePeriods.push(DenyMaintenancePeriod.decode(reader, reader.uint32()));
          continue;
        case 25:
          if (tag !== 202) {
            break;
          }

          message.insightsConfig = InsightsConfig.decode(reader, reader.uint32());
          continue;
        case 27:
          if (tag !== 218) {
            break;
          }

          message.passwordValidationPolicy = PasswordValidationPolicy.decode(reader, reader.uint32());
          continue;
        case 29:
          if (tag !== 234) {
            break;
          }

          message.sqlServerAuditConfig = SqlServerAuditConfig.decode(reader, reader.uint32());
          continue;
        case 38:
          if (tag !== 304) {
            break;
          }

          message.edition = reader.int32() as any;
          continue;
        case 32:
          if (tag !== 256) {
            break;
          }

          message.connectorEnforcement = reader.int32() as any;
          continue;
        case 33:
          if (tag !== 266) {
            break;
          }

          message.deletionProtectionEnabled = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 34:
          if (tag !== 274) {
            break;
          }

          message.timeZone = reader.string();
          continue;
        case 35:
          if (tag !== 282) {
            break;
          }

          message.advancedMachineFeatures = AdvancedMachineFeatures.decode(reader, reader.uint32());
          continue;
        case 37:
          if (tag !== 298) {
            break;
          }

          message.dataCacheConfig = DataCacheConfig.decode(reader, reader.uint32());
          continue;
        case 40:
          if (tag !== 322) {
            break;
          }

          message.enableGoogleMlIntegration = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 41:
          if (tag !== 330) {
            break;
          }

          message.enableDataplexIntegration = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Settings {
    return {
      settingsVersion: isSet(object.settingsVersion) ? Long.fromValue(object.settingsVersion) : undefined,
      authorizedGaeApplications: globalThis.Array.isArray(object?.authorizedGaeApplications)
        ? object.authorizedGaeApplications.map((e: any) => globalThis.String(e))
        : [],
      tier: isSet(object.tier) ? globalThis.String(object.tier) : "",
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      userLabels: isObject(object.userLabels)
        ? Object.entries(object.userLabels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      availabilityType: isSet(object.availabilityType) ? sqlAvailabilityTypeFromJSON(object.availabilityType) : 0,
      pricingPlan: isSet(object.pricingPlan) ? sqlPricingPlanFromJSON(object.pricingPlan) : 0,
      replicationType: isSet(object.replicationType) ? sqlReplicationTypeFromJSON(object.replicationType) : 0,
      storageAutoResizeLimit: isSet(object.storageAutoResizeLimit)
        ? Long.fromValue(object.storageAutoResizeLimit)
        : undefined,
      activationPolicy: isSet(object.activationPolicy)
        ? settings_SqlActivationPolicyFromJSON(object.activationPolicy)
        : 0,
      ipConfiguration: isSet(object.ipConfiguration) ? IpConfiguration.fromJSON(object.ipConfiguration) : undefined,
      storageAutoResize: isSet(object.storageAutoResize) ? Boolean(object.storageAutoResize) : undefined,
      locationPreference: isSet(object.locationPreference)
        ? LocationPreference.fromJSON(object.locationPreference)
        : undefined,
      databaseFlags: globalThis.Array.isArray(object?.databaseFlags)
        ? object.databaseFlags.map((e: any) => DatabaseFlags.fromJSON(e))
        : [],
      dataDiskType: isSet(object.dataDiskType) ? sqlDataDiskTypeFromJSON(object.dataDiskType) : 0,
      maintenanceWindow: isSet(object.maintenanceWindow)
        ? MaintenanceWindow.fromJSON(object.maintenanceWindow)
        : undefined,
      backupConfiguration: isSet(object.backupConfiguration)
        ? BackupConfiguration.fromJSON(object.backupConfiguration)
        : undefined,
      databaseReplicationEnabled: isSet(object.databaseReplicationEnabled)
        ? Boolean(object.databaseReplicationEnabled)
        : undefined,
      crashSafeReplicationEnabled: isSet(object.crashSafeReplicationEnabled)
        ? Boolean(object.crashSafeReplicationEnabled)
        : undefined,
      dataDiskSizeGb: isSet(object.dataDiskSizeGb) ? Long.fromValue(object.dataDiskSizeGb) : undefined,
      activeDirectoryConfig: isSet(object.activeDirectoryConfig)
        ? SqlActiveDirectoryConfig.fromJSON(object.activeDirectoryConfig)
        : undefined,
      collation: isSet(object.collation) ? globalThis.String(object.collation) : "",
      denyMaintenancePeriods: globalThis.Array.isArray(object?.denyMaintenancePeriods)
        ? object.denyMaintenancePeriods.map((e: any) => DenyMaintenancePeriod.fromJSON(e))
        : [],
      insightsConfig: isSet(object.insightsConfig) ? InsightsConfig.fromJSON(object.insightsConfig) : undefined,
      passwordValidationPolicy: isSet(object.passwordValidationPolicy)
        ? PasswordValidationPolicy.fromJSON(object.passwordValidationPolicy)
        : undefined,
      sqlServerAuditConfig: isSet(object.sqlServerAuditConfig)
        ? SqlServerAuditConfig.fromJSON(object.sqlServerAuditConfig)
        : undefined,
      edition: isSet(object.edition) ? settings_EditionFromJSON(object.edition) : 0,
      connectorEnforcement: isSet(object.connectorEnforcement)
        ? settings_ConnectorEnforcementFromJSON(object.connectorEnforcement)
        : 0,
      deletionProtectionEnabled: isSet(object.deletionProtectionEnabled)
        ? Boolean(object.deletionProtectionEnabled)
        : undefined,
      timeZone: isSet(object.timeZone) ? globalThis.String(object.timeZone) : "",
      advancedMachineFeatures: isSet(object.advancedMachineFeatures)
        ? AdvancedMachineFeatures.fromJSON(object.advancedMachineFeatures)
        : undefined,
      dataCacheConfig: isSet(object.dataCacheConfig) ? DataCacheConfig.fromJSON(object.dataCacheConfig) : undefined,
      enableGoogleMlIntegration: isSet(object.enableGoogleMlIntegration)
        ? Boolean(object.enableGoogleMlIntegration)
        : undefined,
      enableDataplexIntegration: isSet(object.enableDataplexIntegration)
        ? Boolean(object.enableDataplexIntegration)
        : undefined,
    };
  },

  toJSON(message: Settings): unknown {
    const obj: any = {};
    if (message.settingsVersion !== undefined) {
      obj.settingsVersion = message.settingsVersion;
    }
    if (message.authorizedGaeApplications?.length) {
      obj.authorizedGaeApplications = message.authorizedGaeApplications;
    }
    if (message.tier !== "") {
      obj.tier = message.tier;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.userLabels) {
      const entries = Object.entries(message.userLabels);
      if (entries.length > 0) {
        obj.userLabels = {};
        entries.forEach(([k, v]) => {
          obj.userLabels[k] = v;
        });
      }
    }
    if (message.availabilityType !== 0) {
      obj.availabilityType = sqlAvailabilityTypeToJSON(message.availabilityType);
    }
    if (message.pricingPlan !== 0) {
      obj.pricingPlan = sqlPricingPlanToJSON(message.pricingPlan);
    }
    if (message.replicationType !== 0) {
      obj.replicationType = sqlReplicationTypeToJSON(message.replicationType);
    }
    if (message.storageAutoResizeLimit !== undefined) {
      obj.storageAutoResizeLimit = message.storageAutoResizeLimit;
    }
    if (message.activationPolicy !== 0) {
      obj.activationPolicy = settings_SqlActivationPolicyToJSON(message.activationPolicy);
    }
    if (message.ipConfiguration !== undefined) {
      obj.ipConfiguration = IpConfiguration.toJSON(message.ipConfiguration);
    }
    if (message.storageAutoResize !== undefined) {
      obj.storageAutoResize = message.storageAutoResize;
    }
    if (message.locationPreference !== undefined) {
      obj.locationPreference = LocationPreference.toJSON(message.locationPreference);
    }
    if (message.databaseFlags?.length) {
      obj.databaseFlags = message.databaseFlags.map((e) => DatabaseFlags.toJSON(e));
    }
    if (message.dataDiskType !== 0) {
      obj.dataDiskType = sqlDataDiskTypeToJSON(message.dataDiskType);
    }
    if (message.maintenanceWindow !== undefined) {
      obj.maintenanceWindow = MaintenanceWindow.toJSON(message.maintenanceWindow);
    }
    if (message.backupConfiguration !== undefined) {
      obj.backupConfiguration = BackupConfiguration.toJSON(message.backupConfiguration);
    }
    if (message.databaseReplicationEnabled !== undefined) {
      obj.databaseReplicationEnabled = message.databaseReplicationEnabled;
    }
    if (message.crashSafeReplicationEnabled !== undefined) {
      obj.crashSafeReplicationEnabled = message.crashSafeReplicationEnabled;
    }
    if (message.dataDiskSizeGb !== undefined) {
      obj.dataDiskSizeGb = message.dataDiskSizeGb;
    }
    if (message.activeDirectoryConfig !== undefined) {
      obj.activeDirectoryConfig = SqlActiveDirectoryConfig.toJSON(message.activeDirectoryConfig);
    }
    if (message.collation !== "") {
      obj.collation = message.collation;
    }
    if (message.denyMaintenancePeriods?.length) {
      obj.denyMaintenancePeriods = message.denyMaintenancePeriods.map((e) => DenyMaintenancePeriod.toJSON(e));
    }
    if (message.insightsConfig !== undefined) {
      obj.insightsConfig = InsightsConfig.toJSON(message.insightsConfig);
    }
    if (message.passwordValidationPolicy !== undefined) {
      obj.passwordValidationPolicy = PasswordValidationPolicy.toJSON(message.passwordValidationPolicy);
    }
    if (message.sqlServerAuditConfig !== undefined) {
      obj.sqlServerAuditConfig = SqlServerAuditConfig.toJSON(message.sqlServerAuditConfig);
    }
    if (message.edition !== 0) {
      obj.edition = settings_EditionToJSON(message.edition);
    }
    if (message.connectorEnforcement !== 0) {
      obj.connectorEnforcement = settings_ConnectorEnforcementToJSON(message.connectorEnforcement);
    }
    if (message.deletionProtectionEnabled !== undefined) {
      obj.deletionProtectionEnabled = message.deletionProtectionEnabled;
    }
    if (message.timeZone !== "") {
      obj.timeZone = message.timeZone;
    }
    if (message.advancedMachineFeatures !== undefined) {
      obj.advancedMachineFeatures = AdvancedMachineFeatures.toJSON(message.advancedMachineFeatures);
    }
    if (message.dataCacheConfig !== undefined) {
      obj.dataCacheConfig = DataCacheConfig.toJSON(message.dataCacheConfig);
    }
    if (message.enableGoogleMlIntegration !== undefined) {
      obj.enableGoogleMlIntegration = message.enableGoogleMlIntegration;
    }
    if (message.enableDataplexIntegration !== undefined) {
      obj.enableDataplexIntegration = message.enableDataplexIntegration;
    }
    return obj;
  },

  create(base?: DeepPartial<Settings>): Settings {
    return Settings.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Settings>): Settings {
    const message = createBaseSettings();
    message.settingsVersion = (object.settingsVersion !== undefined && object.settingsVersion !== null)
      ? Long.fromValue(object.settingsVersion)
      : undefined;
    message.authorizedGaeApplications = object.authorizedGaeApplications?.map((e) => e) || [];
    message.tier = object.tier ?? "";
    message.kind = object.kind ?? "";
    message.userLabels = Object.entries(object.userLabels ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.availabilityType = object.availabilityType ?? 0;
    message.pricingPlan = object.pricingPlan ?? 0;
    message.replicationType = object.replicationType ?? 0;
    message.storageAutoResizeLimit =
      (object.storageAutoResizeLimit !== undefined && object.storageAutoResizeLimit !== null)
        ? Long.fromValue(object.storageAutoResizeLimit)
        : undefined;
    message.activationPolicy = object.activationPolicy ?? 0;
    message.ipConfiguration = (object.ipConfiguration !== undefined && object.ipConfiguration !== null)
      ? IpConfiguration.fromPartial(object.ipConfiguration)
      : undefined;
    message.storageAutoResize = object.storageAutoResize ?? undefined;
    message.locationPreference = (object.locationPreference !== undefined && object.locationPreference !== null)
      ? LocationPreference.fromPartial(object.locationPreference)
      : undefined;
    message.databaseFlags = object.databaseFlags?.map((e) => DatabaseFlags.fromPartial(e)) || [];
    message.dataDiskType = object.dataDiskType ?? 0;
    message.maintenanceWindow = (object.maintenanceWindow !== undefined && object.maintenanceWindow !== null)
      ? MaintenanceWindow.fromPartial(object.maintenanceWindow)
      : undefined;
    message.backupConfiguration = (object.backupConfiguration !== undefined && object.backupConfiguration !== null)
      ? BackupConfiguration.fromPartial(object.backupConfiguration)
      : undefined;
    message.databaseReplicationEnabled = object.databaseReplicationEnabled ?? undefined;
    message.crashSafeReplicationEnabled = object.crashSafeReplicationEnabled ?? undefined;
    message.dataDiskSizeGb = (object.dataDiskSizeGb !== undefined && object.dataDiskSizeGb !== null)
      ? Long.fromValue(object.dataDiskSizeGb)
      : undefined;
    message.activeDirectoryConfig =
      (object.activeDirectoryConfig !== undefined && object.activeDirectoryConfig !== null)
        ? SqlActiveDirectoryConfig.fromPartial(object.activeDirectoryConfig)
        : undefined;
    message.collation = object.collation ?? "";
    message.denyMaintenancePeriods = object.denyMaintenancePeriods?.map((e) => DenyMaintenancePeriod.fromPartial(e)) ||
      [];
    message.insightsConfig = (object.insightsConfig !== undefined && object.insightsConfig !== null)
      ? InsightsConfig.fromPartial(object.insightsConfig)
      : undefined;
    message.passwordValidationPolicy =
      (object.passwordValidationPolicy !== undefined && object.passwordValidationPolicy !== null)
        ? PasswordValidationPolicy.fromPartial(object.passwordValidationPolicy)
        : undefined;
    message.sqlServerAuditConfig = (object.sqlServerAuditConfig !== undefined && object.sqlServerAuditConfig !== null)
      ? SqlServerAuditConfig.fromPartial(object.sqlServerAuditConfig)
      : undefined;
    message.edition = object.edition ?? 0;
    message.connectorEnforcement = object.connectorEnforcement ?? 0;
    message.deletionProtectionEnabled = object.deletionProtectionEnabled ?? undefined;
    message.timeZone = object.timeZone ?? "";
    message.advancedMachineFeatures =
      (object.advancedMachineFeatures !== undefined && object.advancedMachineFeatures !== null)
        ? AdvancedMachineFeatures.fromPartial(object.advancedMachineFeatures)
        : undefined;
    message.dataCacheConfig = (object.dataCacheConfig !== undefined && object.dataCacheConfig !== null)
      ? DataCacheConfig.fromPartial(object.dataCacheConfig)
      : undefined;
    message.enableGoogleMlIntegration = object.enableGoogleMlIntegration ?? undefined;
    message.enableDataplexIntegration = object.enableDataplexIntegration ?? undefined;
    return message;
  },
};

function createBaseSettings_UserLabelsEntry(): Settings_UserLabelsEntry {
  return { key: "", value: "" };
}

export const Settings_UserLabelsEntry: MessageFns<Settings_UserLabelsEntry> = {
  encode(message: Settings_UserLabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Settings_UserLabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSettings_UserLabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Settings_UserLabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Settings_UserLabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Settings_UserLabelsEntry>): Settings_UserLabelsEntry {
    return Settings_UserLabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Settings_UserLabelsEntry>): Settings_UserLabelsEntry {
    const message = createBaseSettings_UserLabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseAdvancedMachineFeatures(): AdvancedMachineFeatures {
  return { threadsPerCore: 0 };
}

export const AdvancedMachineFeatures: MessageFns<AdvancedMachineFeatures> = {
  encode(message: AdvancedMachineFeatures, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.threadsPerCore !== 0) {
      writer.uint32(8).int32(message.threadsPerCore);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AdvancedMachineFeatures {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAdvancedMachineFeatures();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.threadsPerCore = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AdvancedMachineFeatures {
    return { threadsPerCore: isSet(object.threadsPerCore) ? globalThis.Number(object.threadsPerCore) : 0 };
  },

  toJSON(message: AdvancedMachineFeatures): unknown {
    const obj: any = {};
    if (message.threadsPerCore !== 0) {
      obj.threadsPerCore = Math.round(message.threadsPerCore);
    }
    return obj;
  },

  create(base?: DeepPartial<AdvancedMachineFeatures>): AdvancedMachineFeatures {
    return AdvancedMachineFeatures.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AdvancedMachineFeatures>): AdvancedMachineFeatures {
    const message = createBaseAdvancedMachineFeatures();
    message.threadsPerCore = object.threadsPerCore ?? 0;
    return message;
  },
};

function createBaseSslCert(): SslCert {
  return {
    kind: "",
    certSerialNumber: "",
    cert: "",
    createTime: undefined,
    commonName: "",
    expirationTime: undefined,
    sha1Fingerprint: "",
    instance: "",
    selfLink: "",
  };
}

export const SslCert: MessageFns<SslCert> = {
  encode(message: SslCert, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.certSerialNumber !== "") {
      writer.uint32(18).string(message.certSerialNumber);
    }
    if (message.cert !== "") {
      writer.uint32(26).string(message.cert);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(34).fork()).join();
    }
    if (message.commonName !== "") {
      writer.uint32(42).string(message.commonName);
    }
    if (message.expirationTime !== undefined) {
      Timestamp.encode(toTimestamp(message.expirationTime), writer.uint32(50).fork()).join();
    }
    if (message.sha1Fingerprint !== "") {
      writer.uint32(58).string(message.sha1Fingerprint);
    }
    if (message.instance !== "") {
      writer.uint32(66).string(message.instance);
    }
    if (message.selfLink !== "") {
      writer.uint32(74).string(message.selfLink);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SslCert {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSslCert();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.certSerialNumber = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.cert = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.commonName = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.expirationTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.sha1Fingerprint = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.instance = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.selfLink = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SslCert {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      certSerialNumber: isSet(object.certSerialNumber) ? globalThis.String(object.certSerialNumber) : "",
      cert: isSet(object.cert) ? globalThis.String(object.cert) : "",
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      commonName: isSet(object.commonName) ? globalThis.String(object.commonName) : "",
      expirationTime: isSet(object.expirationTime) ? fromJsonTimestamp(object.expirationTime) : undefined,
      sha1Fingerprint: isSet(object.sha1Fingerprint) ? globalThis.String(object.sha1Fingerprint) : "",
      instance: isSet(object.instance) ? globalThis.String(object.instance) : "",
      selfLink: isSet(object.selfLink) ? globalThis.String(object.selfLink) : "",
    };
  },

  toJSON(message: SslCert): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.certSerialNumber !== "") {
      obj.certSerialNumber = message.certSerialNumber;
    }
    if (message.cert !== "") {
      obj.cert = message.cert;
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.commonName !== "") {
      obj.commonName = message.commonName;
    }
    if (message.expirationTime !== undefined) {
      obj.expirationTime = message.expirationTime.toISOString();
    }
    if (message.sha1Fingerprint !== "") {
      obj.sha1Fingerprint = message.sha1Fingerprint;
    }
    if (message.instance !== "") {
      obj.instance = message.instance;
    }
    if (message.selfLink !== "") {
      obj.selfLink = message.selfLink;
    }
    return obj;
  },

  create(base?: DeepPartial<SslCert>): SslCert {
    return SslCert.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SslCert>): SslCert {
    const message = createBaseSslCert();
    message.kind = object.kind ?? "";
    message.certSerialNumber = object.certSerialNumber ?? "";
    message.cert = object.cert ?? "";
    message.createTime = object.createTime ?? undefined;
    message.commonName = object.commonName ?? "";
    message.expirationTime = object.expirationTime ?? undefined;
    message.sha1Fingerprint = object.sha1Fingerprint ?? "";
    message.instance = object.instance ?? "";
    message.selfLink = object.selfLink ?? "";
    return message;
  },
};

function createBaseSslCertDetail(): SslCertDetail {
  return { certInfo: undefined, certPrivateKey: "" };
}

export const SslCertDetail: MessageFns<SslCertDetail> = {
  encode(message: SslCertDetail, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.certInfo !== undefined) {
      SslCert.encode(message.certInfo, writer.uint32(10).fork()).join();
    }
    if (message.certPrivateKey !== "") {
      writer.uint32(18).string(message.certPrivateKey);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SslCertDetail {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSslCertDetail();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.certInfo = SslCert.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.certPrivateKey = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SslCertDetail {
    return {
      certInfo: isSet(object.certInfo) ? SslCert.fromJSON(object.certInfo) : undefined,
      certPrivateKey: isSet(object.certPrivateKey) ? globalThis.String(object.certPrivateKey) : "",
    };
  },

  toJSON(message: SslCertDetail): unknown {
    const obj: any = {};
    if (message.certInfo !== undefined) {
      obj.certInfo = SslCert.toJSON(message.certInfo);
    }
    if (message.certPrivateKey !== "") {
      obj.certPrivateKey = message.certPrivateKey;
    }
    return obj;
  },

  create(base?: DeepPartial<SslCertDetail>): SslCertDetail {
    return SslCertDetail.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SslCertDetail>): SslCertDetail {
    const message = createBaseSslCertDetail();
    message.certInfo = (object.certInfo !== undefined && object.certInfo !== null)
      ? SslCert.fromPartial(object.certInfo)
      : undefined;
    message.certPrivateKey = object.certPrivateKey ?? "";
    return message;
  },
};

function createBaseSqlActiveDirectoryConfig(): SqlActiveDirectoryConfig {
  return { kind: "", domain: "" };
}

export const SqlActiveDirectoryConfig: MessageFns<SqlActiveDirectoryConfig> = {
  encode(message: SqlActiveDirectoryConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.domain !== "") {
      writer.uint32(18).string(message.domain);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SqlActiveDirectoryConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSqlActiveDirectoryConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.domain = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SqlActiveDirectoryConfig {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      domain: isSet(object.domain) ? globalThis.String(object.domain) : "",
    };
  },

  toJSON(message: SqlActiveDirectoryConfig): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.domain !== "") {
      obj.domain = message.domain;
    }
    return obj;
  },

  create(base?: DeepPartial<SqlActiveDirectoryConfig>): SqlActiveDirectoryConfig {
    return SqlActiveDirectoryConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SqlActiveDirectoryConfig>): SqlActiveDirectoryConfig {
    const message = createBaseSqlActiveDirectoryConfig();
    message.kind = object.kind ?? "";
    message.domain = object.domain ?? "";
    return message;
  },
};

function createBaseSqlServerAuditConfig(): SqlServerAuditConfig {
  return { kind: "", bucket: "", retentionInterval: undefined, uploadInterval: undefined };
}

export const SqlServerAuditConfig: MessageFns<SqlServerAuditConfig> = {
  encode(message: SqlServerAuditConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.bucket !== "") {
      writer.uint32(18).string(message.bucket);
    }
    if (message.retentionInterval !== undefined) {
      Duration.encode(message.retentionInterval, writer.uint32(26).fork()).join();
    }
    if (message.uploadInterval !== undefined) {
      Duration.encode(message.uploadInterval, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SqlServerAuditConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSqlServerAuditConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.bucket = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.retentionInterval = Duration.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.uploadInterval = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SqlServerAuditConfig {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      bucket: isSet(object.bucket) ? globalThis.String(object.bucket) : "",
      retentionInterval: isSet(object.retentionInterval) ? Duration.fromJSON(object.retentionInterval) : undefined,
      uploadInterval: isSet(object.uploadInterval) ? Duration.fromJSON(object.uploadInterval) : undefined,
    };
  },

  toJSON(message: SqlServerAuditConfig): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.bucket !== "") {
      obj.bucket = message.bucket;
    }
    if (message.retentionInterval !== undefined) {
      obj.retentionInterval = Duration.toJSON(message.retentionInterval);
    }
    if (message.uploadInterval !== undefined) {
      obj.uploadInterval = Duration.toJSON(message.uploadInterval);
    }
    return obj;
  },

  create(base?: DeepPartial<SqlServerAuditConfig>): SqlServerAuditConfig {
    return SqlServerAuditConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SqlServerAuditConfig>): SqlServerAuditConfig {
    const message = createBaseSqlServerAuditConfig();
    message.kind = object.kind ?? "";
    message.bucket = object.bucket ?? "";
    message.retentionInterval = (object.retentionInterval !== undefined && object.retentionInterval !== null)
      ? Duration.fromPartial(object.retentionInterval)
      : undefined;
    message.uploadInterval = (object.uploadInterval !== undefined && object.uploadInterval !== null)
      ? Duration.fromPartial(object.uploadInterval)
      : undefined;
    return message;
  },
};

function createBaseAcquireSsrsLeaseContext(): AcquireSsrsLeaseContext {
  return { setupLogin: undefined, serviceLogin: undefined, reportDatabase: undefined, duration: undefined };
}

export const AcquireSsrsLeaseContext: MessageFns<AcquireSsrsLeaseContext> = {
  encode(message: AcquireSsrsLeaseContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.setupLogin !== undefined) {
      writer.uint32(10).string(message.setupLogin);
    }
    if (message.serviceLogin !== undefined) {
      writer.uint32(18).string(message.serviceLogin);
    }
    if (message.reportDatabase !== undefined) {
      writer.uint32(26).string(message.reportDatabase);
    }
    if (message.duration !== undefined) {
      Duration.encode(message.duration, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AcquireSsrsLeaseContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAcquireSsrsLeaseContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.setupLogin = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.serviceLogin = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.reportDatabase = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.duration = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AcquireSsrsLeaseContext {
    return {
      setupLogin: isSet(object.setupLogin) ? globalThis.String(object.setupLogin) : undefined,
      serviceLogin: isSet(object.serviceLogin) ? globalThis.String(object.serviceLogin) : undefined,
      reportDatabase: isSet(object.reportDatabase) ? globalThis.String(object.reportDatabase) : undefined,
      duration: isSet(object.duration) ? Duration.fromJSON(object.duration) : undefined,
    };
  },

  toJSON(message: AcquireSsrsLeaseContext): unknown {
    const obj: any = {};
    if (message.setupLogin !== undefined) {
      obj.setupLogin = message.setupLogin;
    }
    if (message.serviceLogin !== undefined) {
      obj.serviceLogin = message.serviceLogin;
    }
    if (message.reportDatabase !== undefined) {
      obj.reportDatabase = message.reportDatabase;
    }
    if (message.duration !== undefined) {
      obj.duration = Duration.toJSON(message.duration);
    }
    return obj;
  },

  create(base?: DeepPartial<AcquireSsrsLeaseContext>): AcquireSsrsLeaseContext {
    return AcquireSsrsLeaseContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AcquireSsrsLeaseContext>): AcquireSsrsLeaseContext {
    const message = createBaseAcquireSsrsLeaseContext();
    message.setupLogin = object.setupLogin ?? undefined;
    message.serviceLogin = object.serviceLogin ?? undefined;
    message.reportDatabase = object.reportDatabase ?? undefined;
    message.duration = (object.duration !== undefined && object.duration !== null)
      ? Duration.fromPartial(object.duration)
      : undefined;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
