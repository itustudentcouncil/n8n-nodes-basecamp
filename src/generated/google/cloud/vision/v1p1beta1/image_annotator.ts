// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/vision/v1p1beta1/image_annotator.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Status } from "../../../rpc/status.js";
import { Color } from "../../../type/color.js";
import { LatLng } from "../../../type/latlng.js";
import { BoundingPoly, Position } from "./geometry.js";
import { TextAnnotation } from "./text_annotation.js";
import { WebDetection } from "./web_detection.js";

export const protobufPackage = "google.cloud.vision.v1p1beta1";

/**
 * A bucketized representation of likelihood, which is intended to give clients
 * highly stable results across model upgrades.
 */
export enum Likelihood {
  /** UNKNOWN - Unknown likelihood. */
  UNKNOWN = 0,
  /** VERY_UNLIKELY - It is very unlikely that the image belongs to the specified vertical. */
  VERY_UNLIKELY = 1,
  /** UNLIKELY - It is unlikely that the image belongs to the specified vertical. */
  UNLIKELY = 2,
  /** POSSIBLE - It is possible that the image belongs to the specified vertical. */
  POSSIBLE = 3,
  /** LIKELY - It is likely that the image belongs to the specified vertical. */
  LIKELY = 4,
  /** VERY_LIKELY - It is very likely that the image belongs to the specified vertical. */
  VERY_LIKELY = 5,
  UNRECOGNIZED = -1,
}

export function likelihoodFromJSON(object: any): Likelihood {
  switch (object) {
    case 0:
    case "UNKNOWN":
      return Likelihood.UNKNOWN;
    case 1:
    case "VERY_UNLIKELY":
      return Likelihood.VERY_UNLIKELY;
    case 2:
    case "UNLIKELY":
      return Likelihood.UNLIKELY;
    case 3:
    case "POSSIBLE":
      return Likelihood.POSSIBLE;
    case 4:
    case "LIKELY":
      return Likelihood.LIKELY;
    case 5:
    case "VERY_LIKELY":
      return Likelihood.VERY_LIKELY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Likelihood.UNRECOGNIZED;
  }
}

export function likelihoodToJSON(object: Likelihood): string {
  switch (object) {
    case Likelihood.UNKNOWN:
      return "UNKNOWN";
    case Likelihood.VERY_UNLIKELY:
      return "VERY_UNLIKELY";
    case Likelihood.UNLIKELY:
      return "UNLIKELY";
    case Likelihood.POSSIBLE:
      return "POSSIBLE";
    case Likelihood.LIKELY:
      return "LIKELY";
    case Likelihood.VERY_LIKELY:
      return "VERY_LIKELY";
    case Likelihood.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Users describe the type of Google Cloud Vision API tasks to perform over
 * images by using *Feature*s. Each Feature indicates a type of image
 * detection task to perform. Features encode the Cloud Vision API
 * vertical to operate on and the number of top-scoring results to return.
 */
export interface Feature {
  /** The feature type. */
  type: Feature_Type;
  /** Maximum number of results of this type. */
  maxResults: number;
  /**
   * Model to use for the feature.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest". `DOCUMENT_TEXT_DETECTION` and `TEXT_DETECTION` also
   * support "builtin/weekly" for the bleeding edge release updated weekly.
   */
  model: string;
}

/** Type of image feature. */
export enum Feature_Type {
  /** TYPE_UNSPECIFIED - Unspecified feature type. */
  TYPE_UNSPECIFIED = 0,
  /** FACE_DETECTION - Run face detection. */
  FACE_DETECTION = 1,
  /** LANDMARK_DETECTION - Run landmark detection. */
  LANDMARK_DETECTION = 2,
  /** LOGO_DETECTION - Run logo detection. */
  LOGO_DETECTION = 3,
  /** LABEL_DETECTION - Run label detection. */
  LABEL_DETECTION = 4,
  /** TEXT_DETECTION - Run OCR. */
  TEXT_DETECTION = 5,
  /**
   * DOCUMENT_TEXT_DETECTION - Run dense text document OCR. Takes precedence when both
   * DOCUMENT_TEXT_DETECTION and TEXT_DETECTION are present.
   */
  DOCUMENT_TEXT_DETECTION = 11,
  /** SAFE_SEARCH_DETECTION - Run computer vision models to compute image safe-search properties. */
  SAFE_SEARCH_DETECTION = 6,
  /** IMAGE_PROPERTIES - Compute a set of image properties, such as the image's dominant colors. */
  IMAGE_PROPERTIES = 7,
  /** CROP_HINTS - Run crop hints. */
  CROP_HINTS = 9,
  /** WEB_DETECTION - Run web detection. */
  WEB_DETECTION = 10,
  UNRECOGNIZED = -1,
}

export function feature_TypeFromJSON(object: any): Feature_Type {
  switch (object) {
    case 0:
    case "TYPE_UNSPECIFIED":
      return Feature_Type.TYPE_UNSPECIFIED;
    case 1:
    case "FACE_DETECTION":
      return Feature_Type.FACE_DETECTION;
    case 2:
    case "LANDMARK_DETECTION":
      return Feature_Type.LANDMARK_DETECTION;
    case 3:
    case "LOGO_DETECTION":
      return Feature_Type.LOGO_DETECTION;
    case 4:
    case "LABEL_DETECTION":
      return Feature_Type.LABEL_DETECTION;
    case 5:
    case "TEXT_DETECTION":
      return Feature_Type.TEXT_DETECTION;
    case 11:
    case "DOCUMENT_TEXT_DETECTION":
      return Feature_Type.DOCUMENT_TEXT_DETECTION;
    case 6:
    case "SAFE_SEARCH_DETECTION":
      return Feature_Type.SAFE_SEARCH_DETECTION;
    case 7:
    case "IMAGE_PROPERTIES":
      return Feature_Type.IMAGE_PROPERTIES;
    case 9:
    case "CROP_HINTS":
      return Feature_Type.CROP_HINTS;
    case 10:
    case "WEB_DETECTION":
      return Feature_Type.WEB_DETECTION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Feature_Type.UNRECOGNIZED;
  }
}

export function feature_TypeToJSON(object: Feature_Type): string {
  switch (object) {
    case Feature_Type.TYPE_UNSPECIFIED:
      return "TYPE_UNSPECIFIED";
    case Feature_Type.FACE_DETECTION:
      return "FACE_DETECTION";
    case Feature_Type.LANDMARK_DETECTION:
      return "LANDMARK_DETECTION";
    case Feature_Type.LOGO_DETECTION:
      return "LOGO_DETECTION";
    case Feature_Type.LABEL_DETECTION:
      return "LABEL_DETECTION";
    case Feature_Type.TEXT_DETECTION:
      return "TEXT_DETECTION";
    case Feature_Type.DOCUMENT_TEXT_DETECTION:
      return "DOCUMENT_TEXT_DETECTION";
    case Feature_Type.SAFE_SEARCH_DETECTION:
      return "SAFE_SEARCH_DETECTION";
    case Feature_Type.IMAGE_PROPERTIES:
      return "IMAGE_PROPERTIES";
    case Feature_Type.CROP_HINTS:
      return "CROP_HINTS";
    case Feature_Type.WEB_DETECTION:
      return "WEB_DETECTION";
    case Feature_Type.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** External image source (Google Cloud Storage image location). */
export interface ImageSource {
  /**
   * NOTE: For new code `image_uri` below is preferred.
   * Google Cloud Storage image URI, which must be in the following form:
   * `gs://bucket_name/object_name` (for details, see
   * [Google Cloud Storage Request
   * URIs](https://cloud.google.com/storage/docs/reference-uris)).
   * NOTE: Cloud Storage object versioning is not supported.
   */
  gcsImageUri: string;
  /**
   * Image URI which supports:
   * 1) Google Cloud Storage image URI, which must be in the following form:
   * `gs://bucket_name/object_name` (for details, see
   * [Google Cloud Storage Request
   * URIs](https://cloud.google.com/storage/docs/reference-uris)).
   * NOTE: Cloud Storage object versioning is not supported.
   * 2) Publicly accessible image HTTP/HTTPS URL.
   * This is preferred over the legacy `gcs_image_uri` above. When both
   * `gcs_image_uri` and `image_uri` are specified, `image_uri` takes
   * precedence.
   */
  imageUri: string;
}

/** Client image to perform Google Cloud Vision API tasks over. */
export interface Image {
  /**
   * Image content, represented as a stream of bytes.
   * Note: as with all `bytes` fields, protobuffers use a pure binary
   * representation, whereas JSON representations use base64.
   */
  content: Buffer;
  /**
   * Google Cloud Storage image location. If both `content` and `source`
   * are provided for an image, `content` takes precedence and is
   * used to perform the image annotation request.
   */
  source: ImageSource | undefined;
}

/** A face annotation object contains the results of face detection. */
export interface FaceAnnotation {
  /**
   * The bounding polygon around the face. The coordinates of the bounding box
   * are in the original image's scale, as returned in `ImageParams`.
   * The bounding box is computed to "frame" the face in accordance with human
   * expectations. It is based on the landmarker results.
   * Note that one or more x and/or y coordinates may not be generated in the
   * `BoundingPoly` (the polygon will be unbounded) if only a partial face
   * appears in the image to be annotated.
   */
  boundingPoly:
    | BoundingPoly
    | undefined;
  /**
   * The `fd_bounding_poly` bounding polygon is tighter than the
   * `boundingPoly`, and encloses only the skin part of the face. Typically, it
   * is used to eliminate the face from any image analysis that detects the
   * "amount of skin" visible in an image. It is not based on the
   * landmarker results, only on the initial face detection, hence
   * the <code>fd</code> (face detection) prefix.
   */
  fdBoundingPoly:
    | BoundingPoly
    | undefined;
  /** Detected face landmarks. */
  landmarks: FaceAnnotation_Landmark[];
  /**
   * Roll angle, which indicates the amount of clockwise/anti-clockwise rotation
   * of the face relative to the image vertical about the axis perpendicular to
   * the face. Range [-180,180].
   */
  rollAngle: number;
  /**
   * Yaw angle, which indicates the leftward/rightward angle that the face is
   * pointing relative to the vertical plane perpendicular to the image. Range
   * [-180,180].
   */
  panAngle: number;
  /**
   * Pitch angle, which indicates the upwards/downwards angle that the face is
   * pointing relative to the image's horizontal plane. Range [-180,180].
   */
  tiltAngle: number;
  /** Detection confidence. Range [0, 1]. */
  detectionConfidence: number;
  /** Face landmarking confidence. Range [0, 1]. */
  landmarkingConfidence: number;
  /** Joy likelihood. */
  joyLikelihood: Likelihood;
  /** Sorrow likelihood. */
  sorrowLikelihood: Likelihood;
  /** Anger likelihood. */
  angerLikelihood: Likelihood;
  /** Surprise likelihood. */
  surpriseLikelihood: Likelihood;
  /** Under-exposed likelihood. */
  underExposedLikelihood: Likelihood;
  /** Blurred likelihood. */
  blurredLikelihood: Likelihood;
  /** Headwear likelihood. */
  headwearLikelihood: Likelihood;
}

/** A face-specific landmark (for example, a face feature). */
export interface FaceAnnotation_Landmark {
  /** Face landmark type. */
  type: FaceAnnotation_Landmark_Type;
  /** Face landmark position. */
  position: Position | undefined;
}

/**
 * Face landmark (feature) type.
 * Left and right are defined from the vantage of the viewer of the image
 * without considering mirror projections typical of photos. So, `LEFT_EYE`,
 * typically, is the person's right eye.
 */
export enum FaceAnnotation_Landmark_Type {
  /** UNKNOWN_LANDMARK - Unknown face landmark detected. Should not be filled. */
  UNKNOWN_LANDMARK = 0,
  /** LEFT_EYE - Left eye. */
  LEFT_EYE = 1,
  /** RIGHT_EYE - Right eye. */
  RIGHT_EYE = 2,
  /** LEFT_OF_LEFT_EYEBROW - Left of left eyebrow. */
  LEFT_OF_LEFT_EYEBROW = 3,
  /** RIGHT_OF_LEFT_EYEBROW - Right of left eyebrow. */
  RIGHT_OF_LEFT_EYEBROW = 4,
  /** LEFT_OF_RIGHT_EYEBROW - Left of right eyebrow. */
  LEFT_OF_RIGHT_EYEBROW = 5,
  /** RIGHT_OF_RIGHT_EYEBROW - Right of right eyebrow. */
  RIGHT_OF_RIGHT_EYEBROW = 6,
  /** MIDPOINT_BETWEEN_EYES - Midpoint between eyes. */
  MIDPOINT_BETWEEN_EYES = 7,
  /** NOSE_TIP - Nose tip. */
  NOSE_TIP = 8,
  /** UPPER_LIP - Upper lip. */
  UPPER_LIP = 9,
  /** LOWER_LIP - Lower lip. */
  LOWER_LIP = 10,
  /** MOUTH_LEFT - Mouth left. */
  MOUTH_LEFT = 11,
  /** MOUTH_RIGHT - Mouth right. */
  MOUTH_RIGHT = 12,
  /** MOUTH_CENTER - Mouth center. */
  MOUTH_CENTER = 13,
  /** NOSE_BOTTOM_RIGHT - Nose, bottom right. */
  NOSE_BOTTOM_RIGHT = 14,
  /** NOSE_BOTTOM_LEFT - Nose, bottom left. */
  NOSE_BOTTOM_LEFT = 15,
  /** NOSE_BOTTOM_CENTER - Nose, bottom center. */
  NOSE_BOTTOM_CENTER = 16,
  /** LEFT_EYE_TOP_BOUNDARY - Left eye, top boundary. */
  LEFT_EYE_TOP_BOUNDARY = 17,
  /** LEFT_EYE_RIGHT_CORNER - Left eye, right corner. */
  LEFT_EYE_RIGHT_CORNER = 18,
  /** LEFT_EYE_BOTTOM_BOUNDARY - Left eye, bottom boundary. */
  LEFT_EYE_BOTTOM_BOUNDARY = 19,
  /** LEFT_EYE_LEFT_CORNER - Left eye, left corner. */
  LEFT_EYE_LEFT_CORNER = 20,
  /** RIGHT_EYE_TOP_BOUNDARY - Right eye, top boundary. */
  RIGHT_EYE_TOP_BOUNDARY = 21,
  /** RIGHT_EYE_RIGHT_CORNER - Right eye, right corner. */
  RIGHT_EYE_RIGHT_CORNER = 22,
  /** RIGHT_EYE_BOTTOM_BOUNDARY - Right eye, bottom boundary. */
  RIGHT_EYE_BOTTOM_BOUNDARY = 23,
  /** RIGHT_EYE_LEFT_CORNER - Right eye, left corner. */
  RIGHT_EYE_LEFT_CORNER = 24,
  /** LEFT_EYEBROW_UPPER_MIDPOINT - Left eyebrow, upper midpoint. */
  LEFT_EYEBROW_UPPER_MIDPOINT = 25,
  /** RIGHT_EYEBROW_UPPER_MIDPOINT - Right eyebrow, upper midpoint. */
  RIGHT_EYEBROW_UPPER_MIDPOINT = 26,
  /** LEFT_EAR_TRAGION - Left ear tragion. */
  LEFT_EAR_TRAGION = 27,
  /** RIGHT_EAR_TRAGION - Right ear tragion. */
  RIGHT_EAR_TRAGION = 28,
  /** LEFT_EYE_PUPIL - Left eye pupil. */
  LEFT_EYE_PUPIL = 29,
  /** RIGHT_EYE_PUPIL - Right eye pupil. */
  RIGHT_EYE_PUPIL = 30,
  /** FOREHEAD_GLABELLA - Forehead glabella. */
  FOREHEAD_GLABELLA = 31,
  /** CHIN_GNATHION - Chin gnathion. */
  CHIN_GNATHION = 32,
  /** CHIN_LEFT_GONION - Chin left gonion. */
  CHIN_LEFT_GONION = 33,
  /** CHIN_RIGHT_GONION - Chin right gonion. */
  CHIN_RIGHT_GONION = 34,
  UNRECOGNIZED = -1,
}

export function faceAnnotation_Landmark_TypeFromJSON(object: any): FaceAnnotation_Landmark_Type {
  switch (object) {
    case 0:
    case "UNKNOWN_LANDMARK":
      return FaceAnnotation_Landmark_Type.UNKNOWN_LANDMARK;
    case 1:
    case "LEFT_EYE":
      return FaceAnnotation_Landmark_Type.LEFT_EYE;
    case 2:
    case "RIGHT_EYE":
      return FaceAnnotation_Landmark_Type.RIGHT_EYE;
    case 3:
    case "LEFT_OF_LEFT_EYEBROW":
      return FaceAnnotation_Landmark_Type.LEFT_OF_LEFT_EYEBROW;
    case 4:
    case "RIGHT_OF_LEFT_EYEBROW":
      return FaceAnnotation_Landmark_Type.RIGHT_OF_LEFT_EYEBROW;
    case 5:
    case "LEFT_OF_RIGHT_EYEBROW":
      return FaceAnnotation_Landmark_Type.LEFT_OF_RIGHT_EYEBROW;
    case 6:
    case "RIGHT_OF_RIGHT_EYEBROW":
      return FaceAnnotation_Landmark_Type.RIGHT_OF_RIGHT_EYEBROW;
    case 7:
    case "MIDPOINT_BETWEEN_EYES":
      return FaceAnnotation_Landmark_Type.MIDPOINT_BETWEEN_EYES;
    case 8:
    case "NOSE_TIP":
      return FaceAnnotation_Landmark_Type.NOSE_TIP;
    case 9:
    case "UPPER_LIP":
      return FaceAnnotation_Landmark_Type.UPPER_LIP;
    case 10:
    case "LOWER_LIP":
      return FaceAnnotation_Landmark_Type.LOWER_LIP;
    case 11:
    case "MOUTH_LEFT":
      return FaceAnnotation_Landmark_Type.MOUTH_LEFT;
    case 12:
    case "MOUTH_RIGHT":
      return FaceAnnotation_Landmark_Type.MOUTH_RIGHT;
    case 13:
    case "MOUTH_CENTER":
      return FaceAnnotation_Landmark_Type.MOUTH_CENTER;
    case 14:
    case "NOSE_BOTTOM_RIGHT":
      return FaceAnnotation_Landmark_Type.NOSE_BOTTOM_RIGHT;
    case 15:
    case "NOSE_BOTTOM_LEFT":
      return FaceAnnotation_Landmark_Type.NOSE_BOTTOM_LEFT;
    case 16:
    case "NOSE_BOTTOM_CENTER":
      return FaceAnnotation_Landmark_Type.NOSE_BOTTOM_CENTER;
    case 17:
    case "LEFT_EYE_TOP_BOUNDARY":
      return FaceAnnotation_Landmark_Type.LEFT_EYE_TOP_BOUNDARY;
    case 18:
    case "LEFT_EYE_RIGHT_CORNER":
      return FaceAnnotation_Landmark_Type.LEFT_EYE_RIGHT_CORNER;
    case 19:
    case "LEFT_EYE_BOTTOM_BOUNDARY":
      return FaceAnnotation_Landmark_Type.LEFT_EYE_BOTTOM_BOUNDARY;
    case 20:
    case "LEFT_EYE_LEFT_CORNER":
      return FaceAnnotation_Landmark_Type.LEFT_EYE_LEFT_CORNER;
    case 21:
    case "RIGHT_EYE_TOP_BOUNDARY":
      return FaceAnnotation_Landmark_Type.RIGHT_EYE_TOP_BOUNDARY;
    case 22:
    case "RIGHT_EYE_RIGHT_CORNER":
      return FaceAnnotation_Landmark_Type.RIGHT_EYE_RIGHT_CORNER;
    case 23:
    case "RIGHT_EYE_BOTTOM_BOUNDARY":
      return FaceAnnotation_Landmark_Type.RIGHT_EYE_BOTTOM_BOUNDARY;
    case 24:
    case "RIGHT_EYE_LEFT_CORNER":
      return FaceAnnotation_Landmark_Type.RIGHT_EYE_LEFT_CORNER;
    case 25:
    case "LEFT_EYEBROW_UPPER_MIDPOINT":
      return FaceAnnotation_Landmark_Type.LEFT_EYEBROW_UPPER_MIDPOINT;
    case 26:
    case "RIGHT_EYEBROW_UPPER_MIDPOINT":
      return FaceAnnotation_Landmark_Type.RIGHT_EYEBROW_UPPER_MIDPOINT;
    case 27:
    case "LEFT_EAR_TRAGION":
      return FaceAnnotation_Landmark_Type.LEFT_EAR_TRAGION;
    case 28:
    case "RIGHT_EAR_TRAGION":
      return FaceAnnotation_Landmark_Type.RIGHT_EAR_TRAGION;
    case 29:
    case "LEFT_EYE_PUPIL":
      return FaceAnnotation_Landmark_Type.LEFT_EYE_PUPIL;
    case 30:
    case "RIGHT_EYE_PUPIL":
      return FaceAnnotation_Landmark_Type.RIGHT_EYE_PUPIL;
    case 31:
    case "FOREHEAD_GLABELLA":
      return FaceAnnotation_Landmark_Type.FOREHEAD_GLABELLA;
    case 32:
    case "CHIN_GNATHION":
      return FaceAnnotation_Landmark_Type.CHIN_GNATHION;
    case 33:
    case "CHIN_LEFT_GONION":
      return FaceAnnotation_Landmark_Type.CHIN_LEFT_GONION;
    case 34:
    case "CHIN_RIGHT_GONION":
      return FaceAnnotation_Landmark_Type.CHIN_RIGHT_GONION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return FaceAnnotation_Landmark_Type.UNRECOGNIZED;
  }
}

export function faceAnnotation_Landmark_TypeToJSON(object: FaceAnnotation_Landmark_Type): string {
  switch (object) {
    case FaceAnnotation_Landmark_Type.UNKNOWN_LANDMARK:
      return "UNKNOWN_LANDMARK";
    case FaceAnnotation_Landmark_Type.LEFT_EYE:
      return "LEFT_EYE";
    case FaceAnnotation_Landmark_Type.RIGHT_EYE:
      return "RIGHT_EYE";
    case FaceAnnotation_Landmark_Type.LEFT_OF_LEFT_EYEBROW:
      return "LEFT_OF_LEFT_EYEBROW";
    case FaceAnnotation_Landmark_Type.RIGHT_OF_LEFT_EYEBROW:
      return "RIGHT_OF_LEFT_EYEBROW";
    case FaceAnnotation_Landmark_Type.LEFT_OF_RIGHT_EYEBROW:
      return "LEFT_OF_RIGHT_EYEBROW";
    case FaceAnnotation_Landmark_Type.RIGHT_OF_RIGHT_EYEBROW:
      return "RIGHT_OF_RIGHT_EYEBROW";
    case FaceAnnotation_Landmark_Type.MIDPOINT_BETWEEN_EYES:
      return "MIDPOINT_BETWEEN_EYES";
    case FaceAnnotation_Landmark_Type.NOSE_TIP:
      return "NOSE_TIP";
    case FaceAnnotation_Landmark_Type.UPPER_LIP:
      return "UPPER_LIP";
    case FaceAnnotation_Landmark_Type.LOWER_LIP:
      return "LOWER_LIP";
    case FaceAnnotation_Landmark_Type.MOUTH_LEFT:
      return "MOUTH_LEFT";
    case FaceAnnotation_Landmark_Type.MOUTH_RIGHT:
      return "MOUTH_RIGHT";
    case FaceAnnotation_Landmark_Type.MOUTH_CENTER:
      return "MOUTH_CENTER";
    case FaceAnnotation_Landmark_Type.NOSE_BOTTOM_RIGHT:
      return "NOSE_BOTTOM_RIGHT";
    case FaceAnnotation_Landmark_Type.NOSE_BOTTOM_LEFT:
      return "NOSE_BOTTOM_LEFT";
    case FaceAnnotation_Landmark_Type.NOSE_BOTTOM_CENTER:
      return "NOSE_BOTTOM_CENTER";
    case FaceAnnotation_Landmark_Type.LEFT_EYE_TOP_BOUNDARY:
      return "LEFT_EYE_TOP_BOUNDARY";
    case FaceAnnotation_Landmark_Type.LEFT_EYE_RIGHT_CORNER:
      return "LEFT_EYE_RIGHT_CORNER";
    case FaceAnnotation_Landmark_Type.LEFT_EYE_BOTTOM_BOUNDARY:
      return "LEFT_EYE_BOTTOM_BOUNDARY";
    case FaceAnnotation_Landmark_Type.LEFT_EYE_LEFT_CORNER:
      return "LEFT_EYE_LEFT_CORNER";
    case FaceAnnotation_Landmark_Type.RIGHT_EYE_TOP_BOUNDARY:
      return "RIGHT_EYE_TOP_BOUNDARY";
    case FaceAnnotation_Landmark_Type.RIGHT_EYE_RIGHT_CORNER:
      return "RIGHT_EYE_RIGHT_CORNER";
    case FaceAnnotation_Landmark_Type.RIGHT_EYE_BOTTOM_BOUNDARY:
      return "RIGHT_EYE_BOTTOM_BOUNDARY";
    case FaceAnnotation_Landmark_Type.RIGHT_EYE_LEFT_CORNER:
      return "RIGHT_EYE_LEFT_CORNER";
    case FaceAnnotation_Landmark_Type.LEFT_EYEBROW_UPPER_MIDPOINT:
      return "LEFT_EYEBROW_UPPER_MIDPOINT";
    case FaceAnnotation_Landmark_Type.RIGHT_EYEBROW_UPPER_MIDPOINT:
      return "RIGHT_EYEBROW_UPPER_MIDPOINT";
    case FaceAnnotation_Landmark_Type.LEFT_EAR_TRAGION:
      return "LEFT_EAR_TRAGION";
    case FaceAnnotation_Landmark_Type.RIGHT_EAR_TRAGION:
      return "RIGHT_EAR_TRAGION";
    case FaceAnnotation_Landmark_Type.LEFT_EYE_PUPIL:
      return "LEFT_EYE_PUPIL";
    case FaceAnnotation_Landmark_Type.RIGHT_EYE_PUPIL:
      return "RIGHT_EYE_PUPIL";
    case FaceAnnotation_Landmark_Type.FOREHEAD_GLABELLA:
      return "FOREHEAD_GLABELLA";
    case FaceAnnotation_Landmark_Type.CHIN_GNATHION:
      return "CHIN_GNATHION";
    case FaceAnnotation_Landmark_Type.CHIN_LEFT_GONION:
      return "CHIN_LEFT_GONION";
    case FaceAnnotation_Landmark_Type.CHIN_RIGHT_GONION:
      return "CHIN_RIGHT_GONION";
    case FaceAnnotation_Landmark_Type.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Detected entity location information. */
export interface LocationInfo {
  /** lat/long location coordinates. */
  latLng: LatLng | undefined;
}

/** A `Property` consists of a user-supplied name/value pair. */
export interface Property {
  /** Name of the property. */
  name: string;
  /** Value of the property. */
  value: string;
  /** Value of numeric properties. */
  uint64Value: Long;
}

/** Set of detected entity features. */
export interface EntityAnnotation {
  /**
   * Opaque entity ID. Some IDs may be available in
   * [Google Knowledge Graph Search
   * API](https://developers.google.com/knowledge-graph/).
   */
  mid: string;
  /**
   * The language code for the locale in which the entity textual
   * `description` is expressed.
   */
  locale: string;
  /** Entity textual description, expressed in its `locale` language. */
  description: string;
  /** Overall score of the result. Range [0, 1]. */
  score: number;
  /**
   * The accuracy of the entity detection in an image.
   * For example, for an image in which the "Eiffel Tower" entity is detected,
   * this field represents the confidence that there is a tower in the query
   * image. Range [0, 1].
   */
  confidence: number;
  /**
   * The relevancy of the ICA (Image Content Annotation) label to the
   * image. For example, the relevancy of "tower" is likely higher to an image
   * containing the detected "Eiffel Tower" than to an image containing a
   * detected distant towering building, even though the confidence that
   * there is a tower in each image may be the same. Range [0, 1].
   */
  topicality: number;
  /**
   * Image region to which this entity belongs. Not produced
   * for `LABEL_DETECTION` features.
   */
  boundingPoly:
    | BoundingPoly
    | undefined;
  /**
   * The location information for the detected entity. Multiple
   * `LocationInfo` elements can be present because one location may
   * indicate the location of the scene in the image, and another location
   * may indicate the location of the place where the image was taken.
   * Location information is usually present for landmarks.
   */
  locations: LocationInfo[];
  /**
   * Some entities may have optional user-supplied `Property` (name/value)
   * fields, such a score or string that qualifies the entity.
   */
  properties: Property[];
}

/**
 * Set of features pertaining to the image, computed by computer vision
 * methods over safe-search verticals (for example, adult, spoof, medical,
 * violence).
 */
export interface SafeSearchAnnotation {
  /**
   * Represents the adult content likelihood for the image. Adult content may
   * contain elements such as nudity, pornographic images or cartoons, or
   * sexual activities.
   */
  adult: Likelihood;
  /**
   * Spoof likelihood. The likelihood that an modification
   * was made to the image's canonical version to make it appear
   * funny or offensive.
   */
  spoof: Likelihood;
  /** Likelihood that this is a medical image. */
  medical: Likelihood;
  /** Likelihood that this image contains violent content. */
  violence: Likelihood;
  /**
   * Likelihood that the request image contains racy content. Racy content may
   * include (but is not limited to) skimpy or sheer clothing, strategically
   * covered nudity, lewd or provocative poses, or close-ups of sensitive
   * body areas.
   */
  racy: Likelihood;
}

/** Rectangle determined by min and max `LatLng` pairs. */
export interface LatLongRect {
  /** Min lat/long pair. */
  minLatLng:
    | LatLng
    | undefined;
  /** Max lat/long pair. */
  maxLatLng: LatLng | undefined;
}

/**
 * Color information consists of RGB channels, score, and the fraction of
 * the image that the color occupies in the image.
 */
export interface ColorInfo {
  /** RGB components of the color. */
  color:
    | Color
    | undefined;
  /** Image-specific score for this color. Value in range [0, 1]. */
  score: number;
  /**
   * The fraction of pixels the color occupies in the image.
   * Value in range [0, 1].
   */
  pixelFraction: number;
}

/** Set of dominant colors and their corresponding scores. */
export interface DominantColorsAnnotation {
  /** RGB color values with their score and pixel fraction. */
  colors: ColorInfo[];
}

/** Stores image properties, such as dominant colors. */
export interface ImageProperties {
  /** If present, dominant colors completed successfully. */
  dominantColors: DominantColorsAnnotation | undefined;
}

/** Single crop hint that is used to generate a new crop when serving an image. */
export interface CropHint {
  /**
   * The bounding polygon for the crop region. The coordinates of the bounding
   * box are in the original image's scale, as returned in `ImageParams`.
   */
  boundingPoly:
    | BoundingPoly
    | undefined;
  /** Confidence of this being a salient region.  Range [0, 1]. */
  confidence: number;
  /**
   * Fraction of importance of this salient region with respect to the original
   * image.
   */
  importanceFraction: number;
}

/** Set of crop hints that are used to generate new crops when serving images. */
export interface CropHintsAnnotation {
  /** Crop hint results. */
  cropHints: CropHint[];
}

/** Parameters for crop hints annotation request. */
export interface CropHintsParams {
  /**
   * Aspect ratios in floats, representing the ratio of the width to the height
   * of the image. For example, if the desired aspect ratio is 4/3, the
   * corresponding float value should be 1.33333.  If not specified, the
   * best possible crop is returned. The number of provided aspect ratios is
   * limited to a maximum of 16; any aspect ratios provided after the 16th are
   * ignored.
   */
  aspectRatios: number[];
}

/** Parameters for web detection request. */
export interface WebDetectionParams {
  /** Whether to include results derived from the geo information in the image. */
  includeGeoResults: boolean;
}

/**
 * Parameters for text detections. This is used to control TEXT_DETECTION and
 * DOCUMENT_TEXT_DETECTION features.
 */
export interface TextDetectionParams {
  /**
   * By default, Cloud Vision API only includes confidence score for
   * DOCUMENT_TEXT_DETECTION result. Set the flag to true to include confidence
   * score for TEXT_DETECTION as well.
   */
  enableTextDetectionConfidenceScore: boolean;
  /** A list of advanced OCR options to fine-tune OCR behavior. */
  advancedOcrOptions: string[];
}

/** Image context and/or feature-specific parameters. */
export interface ImageContext {
  /** lat/long rectangle that specifies the location of the image. */
  latLongRect:
    | LatLongRect
    | undefined;
  /**
   * List of languages to use for TEXT_DETECTION. In most cases, an empty value
   * yields the best results since it enables automatic language detection. For
   * languages based on the Latin alphabet, setting `language_hints` is not
   * needed. In rare cases, when the language of the text in the image is known,
   * setting a hint will help get better results (although it will be a
   * significant hindrance if the hint is wrong). Text detection returns an
   * error if one or more of the specified languages is not one of the
   * [supported languages](https://cloud.google.com/vision/docs/languages).
   */
  languageHints: string[];
  /** Parameters for crop hints annotation request. */
  cropHintsParams:
    | CropHintsParams
    | undefined;
  /** Parameters for web detection. */
  webDetectionParams:
    | WebDetectionParams
    | undefined;
  /** Parameters for text detection and document text detection. */
  textDetectionParams: TextDetectionParams | undefined;
}

/**
 * Request for performing Google Cloud Vision API tasks over a user-provided
 * image, with user-requested features.
 */
export interface AnnotateImageRequest {
  /** The image to be processed. */
  image:
    | Image
    | undefined;
  /** Requested features. */
  features: Feature[];
  /** Additional context that may accompany the image. */
  imageContext: ImageContext | undefined;
}

/** Response to an image annotation request. */
export interface AnnotateImageResponse {
  /** If present, face detection has completed successfully. */
  faceAnnotations: FaceAnnotation[];
  /** If present, landmark detection has completed successfully. */
  landmarkAnnotations: EntityAnnotation[];
  /** If present, logo detection has completed successfully. */
  logoAnnotations: EntityAnnotation[];
  /** If present, label detection has completed successfully. */
  labelAnnotations: EntityAnnotation[];
  /** If present, text (OCR) detection has completed successfully. */
  textAnnotations: EntityAnnotation[];
  /**
   * If present, text (OCR) detection or document (OCR) text detection has
   * completed successfully.
   * This annotation provides the structural hierarchy for the OCR detected
   * text.
   */
  fullTextAnnotation:
    | TextAnnotation
    | undefined;
  /** If present, safe-search annotation has completed successfully. */
  safeSearchAnnotation:
    | SafeSearchAnnotation
    | undefined;
  /** If present, image properties were extracted successfully. */
  imagePropertiesAnnotation:
    | ImageProperties
    | undefined;
  /** If present, crop hints have completed successfully. */
  cropHintsAnnotation:
    | CropHintsAnnotation
    | undefined;
  /** If present, web detection has completed successfully. */
  webDetection:
    | WebDetection
    | undefined;
  /**
   * If set, represents the error message for the operation.
   * Note that filled-in image annotations are guaranteed to be
   * correct, even when `error` is set.
   */
  error: Status | undefined;
}

/** Multiple image annotation requests are batched into a single service call. */
export interface BatchAnnotateImagesRequest {
  /** Required. Individual image annotation requests for this batch. */
  requests: AnnotateImageRequest[];
}

/** Response to a batch image annotation request. */
export interface BatchAnnotateImagesResponse {
  /** Individual responses to image annotation requests within the batch. */
  responses: AnnotateImageResponse[];
}

function createBaseFeature(): Feature {
  return { type: 0, maxResults: 0, model: "" };
}

export const Feature: MessageFns<Feature> = {
  encode(message: Feature, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.type !== 0) {
      writer.uint32(8).int32(message.type);
    }
    if (message.maxResults !== 0) {
      writer.uint32(16).int32(message.maxResults);
    }
    if (message.model !== "") {
      writer.uint32(26).string(message.model);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Feature {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFeature();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.maxResults = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.model = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Feature {
    return {
      type: isSet(object.type) ? feature_TypeFromJSON(object.type) : 0,
      maxResults: isSet(object.maxResults) ? globalThis.Number(object.maxResults) : 0,
      model: isSet(object.model) ? globalThis.String(object.model) : "",
    };
  },

  toJSON(message: Feature): unknown {
    const obj: any = {};
    if (message.type !== 0) {
      obj.type = feature_TypeToJSON(message.type);
    }
    if (message.maxResults !== 0) {
      obj.maxResults = Math.round(message.maxResults);
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    return obj;
  },

  create(base?: DeepPartial<Feature>): Feature {
    return Feature.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Feature>): Feature {
    const message = createBaseFeature();
    message.type = object.type ?? 0;
    message.maxResults = object.maxResults ?? 0;
    message.model = object.model ?? "";
    return message;
  },
};

function createBaseImageSource(): ImageSource {
  return { gcsImageUri: "", imageUri: "" };
}

export const ImageSource: MessageFns<ImageSource> = {
  encode(message: ImageSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcsImageUri !== "") {
      writer.uint32(10).string(message.gcsImageUri);
    }
    if (message.imageUri !== "") {
      writer.uint32(18).string(message.imageUri);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImageSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImageSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.gcsImageUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.imageUri = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImageSource {
    return {
      gcsImageUri: isSet(object.gcsImageUri) ? globalThis.String(object.gcsImageUri) : "",
      imageUri: isSet(object.imageUri) ? globalThis.String(object.imageUri) : "",
    };
  },

  toJSON(message: ImageSource): unknown {
    const obj: any = {};
    if (message.gcsImageUri !== "") {
      obj.gcsImageUri = message.gcsImageUri;
    }
    if (message.imageUri !== "") {
      obj.imageUri = message.imageUri;
    }
    return obj;
  },

  create(base?: DeepPartial<ImageSource>): ImageSource {
    return ImageSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImageSource>): ImageSource {
    const message = createBaseImageSource();
    message.gcsImageUri = object.gcsImageUri ?? "";
    message.imageUri = object.imageUri ?? "";
    return message;
  },
};

function createBaseImage(): Image {
  return { content: Buffer.alloc(0), source: undefined };
}

export const Image: MessageFns<Image> = {
  encode(message: Image, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.content.length !== 0) {
      writer.uint32(10).bytes(message.content);
    }
    if (message.source !== undefined) {
      ImageSource.encode(message.source, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Image {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImage();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.content = Buffer.from(reader.bytes());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.source = ImageSource.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Image {
    return {
      content: isSet(object.content) ? Buffer.from(bytesFromBase64(object.content)) : Buffer.alloc(0),
      source: isSet(object.source) ? ImageSource.fromJSON(object.source) : undefined,
    };
  },

  toJSON(message: Image): unknown {
    const obj: any = {};
    if (message.content.length !== 0) {
      obj.content = base64FromBytes(message.content);
    }
    if (message.source !== undefined) {
      obj.source = ImageSource.toJSON(message.source);
    }
    return obj;
  },

  create(base?: DeepPartial<Image>): Image {
    return Image.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Image>): Image {
    const message = createBaseImage();
    message.content = object.content ?? Buffer.alloc(0);
    message.source = (object.source !== undefined && object.source !== null)
      ? ImageSource.fromPartial(object.source)
      : undefined;
    return message;
  },
};

function createBaseFaceAnnotation(): FaceAnnotation {
  return {
    boundingPoly: undefined,
    fdBoundingPoly: undefined,
    landmarks: [],
    rollAngle: 0,
    panAngle: 0,
    tiltAngle: 0,
    detectionConfidence: 0,
    landmarkingConfidence: 0,
    joyLikelihood: 0,
    sorrowLikelihood: 0,
    angerLikelihood: 0,
    surpriseLikelihood: 0,
    underExposedLikelihood: 0,
    blurredLikelihood: 0,
    headwearLikelihood: 0,
  };
}

export const FaceAnnotation: MessageFns<FaceAnnotation> = {
  encode(message: FaceAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.boundingPoly !== undefined) {
      BoundingPoly.encode(message.boundingPoly, writer.uint32(10).fork()).join();
    }
    if (message.fdBoundingPoly !== undefined) {
      BoundingPoly.encode(message.fdBoundingPoly, writer.uint32(18).fork()).join();
    }
    for (const v of message.landmarks) {
      FaceAnnotation_Landmark.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.rollAngle !== 0) {
      writer.uint32(37).float(message.rollAngle);
    }
    if (message.panAngle !== 0) {
      writer.uint32(45).float(message.panAngle);
    }
    if (message.tiltAngle !== 0) {
      writer.uint32(53).float(message.tiltAngle);
    }
    if (message.detectionConfidence !== 0) {
      writer.uint32(61).float(message.detectionConfidence);
    }
    if (message.landmarkingConfidence !== 0) {
      writer.uint32(69).float(message.landmarkingConfidence);
    }
    if (message.joyLikelihood !== 0) {
      writer.uint32(72).int32(message.joyLikelihood);
    }
    if (message.sorrowLikelihood !== 0) {
      writer.uint32(80).int32(message.sorrowLikelihood);
    }
    if (message.angerLikelihood !== 0) {
      writer.uint32(88).int32(message.angerLikelihood);
    }
    if (message.surpriseLikelihood !== 0) {
      writer.uint32(96).int32(message.surpriseLikelihood);
    }
    if (message.underExposedLikelihood !== 0) {
      writer.uint32(104).int32(message.underExposedLikelihood);
    }
    if (message.blurredLikelihood !== 0) {
      writer.uint32(112).int32(message.blurredLikelihood);
    }
    if (message.headwearLikelihood !== 0) {
      writer.uint32(120).int32(message.headwearLikelihood);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FaceAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFaceAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.boundingPoly = BoundingPoly.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.fdBoundingPoly = BoundingPoly.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.landmarks.push(FaceAnnotation_Landmark.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.rollAngle = reader.float();
          continue;
        case 5:
          if (tag !== 45) {
            break;
          }

          message.panAngle = reader.float();
          continue;
        case 6:
          if (tag !== 53) {
            break;
          }

          message.tiltAngle = reader.float();
          continue;
        case 7:
          if (tag !== 61) {
            break;
          }

          message.detectionConfidence = reader.float();
          continue;
        case 8:
          if (tag !== 69) {
            break;
          }

          message.landmarkingConfidence = reader.float();
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.joyLikelihood = reader.int32() as any;
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.sorrowLikelihood = reader.int32() as any;
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.angerLikelihood = reader.int32() as any;
          continue;
        case 12:
          if (tag !== 96) {
            break;
          }

          message.surpriseLikelihood = reader.int32() as any;
          continue;
        case 13:
          if (tag !== 104) {
            break;
          }

          message.underExposedLikelihood = reader.int32() as any;
          continue;
        case 14:
          if (tag !== 112) {
            break;
          }

          message.blurredLikelihood = reader.int32() as any;
          continue;
        case 15:
          if (tag !== 120) {
            break;
          }

          message.headwearLikelihood = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FaceAnnotation {
    return {
      boundingPoly: isSet(object.boundingPoly) ? BoundingPoly.fromJSON(object.boundingPoly) : undefined,
      fdBoundingPoly: isSet(object.fdBoundingPoly) ? BoundingPoly.fromJSON(object.fdBoundingPoly) : undefined,
      landmarks: globalThis.Array.isArray(object?.landmarks)
        ? object.landmarks.map((e: any) => FaceAnnotation_Landmark.fromJSON(e))
        : [],
      rollAngle: isSet(object.rollAngle) ? globalThis.Number(object.rollAngle) : 0,
      panAngle: isSet(object.panAngle) ? globalThis.Number(object.panAngle) : 0,
      tiltAngle: isSet(object.tiltAngle) ? globalThis.Number(object.tiltAngle) : 0,
      detectionConfidence: isSet(object.detectionConfidence) ? globalThis.Number(object.detectionConfidence) : 0,
      landmarkingConfidence: isSet(object.landmarkingConfidence) ? globalThis.Number(object.landmarkingConfidence) : 0,
      joyLikelihood: isSet(object.joyLikelihood) ? likelihoodFromJSON(object.joyLikelihood) : 0,
      sorrowLikelihood: isSet(object.sorrowLikelihood) ? likelihoodFromJSON(object.sorrowLikelihood) : 0,
      angerLikelihood: isSet(object.angerLikelihood) ? likelihoodFromJSON(object.angerLikelihood) : 0,
      surpriseLikelihood: isSet(object.surpriseLikelihood) ? likelihoodFromJSON(object.surpriseLikelihood) : 0,
      underExposedLikelihood: isSet(object.underExposedLikelihood)
        ? likelihoodFromJSON(object.underExposedLikelihood)
        : 0,
      blurredLikelihood: isSet(object.blurredLikelihood) ? likelihoodFromJSON(object.blurredLikelihood) : 0,
      headwearLikelihood: isSet(object.headwearLikelihood) ? likelihoodFromJSON(object.headwearLikelihood) : 0,
    };
  },

  toJSON(message: FaceAnnotation): unknown {
    const obj: any = {};
    if (message.boundingPoly !== undefined) {
      obj.boundingPoly = BoundingPoly.toJSON(message.boundingPoly);
    }
    if (message.fdBoundingPoly !== undefined) {
      obj.fdBoundingPoly = BoundingPoly.toJSON(message.fdBoundingPoly);
    }
    if (message.landmarks?.length) {
      obj.landmarks = message.landmarks.map((e) => FaceAnnotation_Landmark.toJSON(e));
    }
    if (message.rollAngle !== 0) {
      obj.rollAngle = message.rollAngle;
    }
    if (message.panAngle !== 0) {
      obj.panAngle = message.panAngle;
    }
    if (message.tiltAngle !== 0) {
      obj.tiltAngle = message.tiltAngle;
    }
    if (message.detectionConfidence !== 0) {
      obj.detectionConfidence = message.detectionConfidence;
    }
    if (message.landmarkingConfidence !== 0) {
      obj.landmarkingConfidence = message.landmarkingConfidence;
    }
    if (message.joyLikelihood !== 0) {
      obj.joyLikelihood = likelihoodToJSON(message.joyLikelihood);
    }
    if (message.sorrowLikelihood !== 0) {
      obj.sorrowLikelihood = likelihoodToJSON(message.sorrowLikelihood);
    }
    if (message.angerLikelihood !== 0) {
      obj.angerLikelihood = likelihoodToJSON(message.angerLikelihood);
    }
    if (message.surpriseLikelihood !== 0) {
      obj.surpriseLikelihood = likelihoodToJSON(message.surpriseLikelihood);
    }
    if (message.underExposedLikelihood !== 0) {
      obj.underExposedLikelihood = likelihoodToJSON(message.underExposedLikelihood);
    }
    if (message.blurredLikelihood !== 0) {
      obj.blurredLikelihood = likelihoodToJSON(message.blurredLikelihood);
    }
    if (message.headwearLikelihood !== 0) {
      obj.headwearLikelihood = likelihoodToJSON(message.headwearLikelihood);
    }
    return obj;
  },

  create(base?: DeepPartial<FaceAnnotation>): FaceAnnotation {
    return FaceAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FaceAnnotation>): FaceAnnotation {
    const message = createBaseFaceAnnotation();
    message.boundingPoly = (object.boundingPoly !== undefined && object.boundingPoly !== null)
      ? BoundingPoly.fromPartial(object.boundingPoly)
      : undefined;
    message.fdBoundingPoly = (object.fdBoundingPoly !== undefined && object.fdBoundingPoly !== null)
      ? BoundingPoly.fromPartial(object.fdBoundingPoly)
      : undefined;
    message.landmarks = object.landmarks?.map((e) => FaceAnnotation_Landmark.fromPartial(e)) || [];
    message.rollAngle = object.rollAngle ?? 0;
    message.panAngle = object.panAngle ?? 0;
    message.tiltAngle = object.tiltAngle ?? 0;
    message.detectionConfidence = object.detectionConfidence ?? 0;
    message.landmarkingConfidence = object.landmarkingConfidence ?? 0;
    message.joyLikelihood = object.joyLikelihood ?? 0;
    message.sorrowLikelihood = object.sorrowLikelihood ?? 0;
    message.angerLikelihood = object.angerLikelihood ?? 0;
    message.surpriseLikelihood = object.surpriseLikelihood ?? 0;
    message.underExposedLikelihood = object.underExposedLikelihood ?? 0;
    message.blurredLikelihood = object.blurredLikelihood ?? 0;
    message.headwearLikelihood = object.headwearLikelihood ?? 0;
    return message;
  },
};

function createBaseFaceAnnotation_Landmark(): FaceAnnotation_Landmark {
  return { type: 0, position: undefined };
}

export const FaceAnnotation_Landmark: MessageFns<FaceAnnotation_Landmark> = {
  encode(message: FaceAnnotation_Landmark, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.type !== 0) {
      writer.uint32(24).int32(message.type);
    }
    if (message.position !== undefined) {
      Position.encode(message.position, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FaceAnnotation_Landmark {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFaceAnnotation_Landmark();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 24) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.position = Position.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FaceAnnotation_Landmark {
    return {
      type: isSet(object.type) ? faceAnnotation_Landmark_TypeFromJSON(object.type) : 0,
      position: isSet(object.position) ? Position.fromJSON(object.position) : undefined,
    };
  },

  toJSON(message: FaceAnnotation_Landmark): unknown {
    const obj: any = {};
    if (message.type !== 0) {
      obj.type = faceAnnotation_Landmark_TypeToJSON(message.type);
    }
    if (message.position !== undefined) {
      obj.position = Position.toJSON(message.position);
    }
    return obj;
  },

  create(base?: DeepPartial<FaceAnnotation_Landmark>): FaceAnnotation_Landmark {
    return FaceAnnotation_Landmark.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FaceAnnotation_Landmark>): FaceAnnotation_Landmark {
    const message = createBaseFaceAnnotation_Landmark();
    message.type = object.type ?? 0;
    message.position = (object.position !== undefined && object.position !== null)
      ? Position.fromPartial(object.position)
      : undefined;
    return message;
  },
};

function createBaseLocationInfo(): LocationInfo {
  return { latLng: undefined };
}

export const LocationInfo: MessageFns<LocationInfo> = {
  encode(message: LocationInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.latLng !== undefined) {
      LatLng.encode(message.latLng, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LocationInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLocationInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.latLng = LatLng.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LocationInfo {
    return { latLng: isSet(object.latLng) ? LatLng.fromJSON(object.latLng) : undefined };
  },

  toJSON(message: LocationInfo): unknown {
    const obj: any = {};
    if (message.latLng !== undefined) {
      obj.latLng = LatLng.toJSON(message.latLng);
    }
    return obj;
  },

  create(base?: DeepPartial<LocationInfo>): LocationInfo {
    return LocationInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LocationInfo>): LocationInfo {
    const message = createBaseLocationInfo();
    message.latLng = (object.latLng !== undefined && object.latLng !== null)
      ? LatLng.fromPartial(object.latLng)
      : undefined;
    return message;
  },
};

function createBaseProperty(): Property {
  return { name: "", value: "", uint64Value: Long.UZERO };
}

export const Property: MessageFns<Property> = {
  encode(message: Property, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    if (!message.uint64Value.equals(Long.UZERO)) {
      writer.uint32(24).uint64(message.uint64Value.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Property {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProperty();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.uint64Value = Long.fromString(reader.uint64().toString(), true);
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Property {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
      uint64Value: isSet(object.uint64Value) ? Long.fromValue(object.uint64Value) : Long.UZERO,
    };
  },

  toJSON(message: Property): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    if (!message.uint64Value.equals(Long.UZERO)) {
      obj.uint64Value = (message.uint64Value || Long.UZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<Property>): Property {
    return Property.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Property>): Property {
    const message = createBaseProperty();
    message.name = object.name ?? "";
    message.value = object.value ?? "";
    message.uint64Value = (object.uint64Value !== undefined && object.uint64Value !== null)
      ? Long.fromValue(object.uint64Value)
      : Long.UZERO;
    return message;
  },
};

function createBaseEntityAnnotation(): EntityAnnotation {
  return {
    mid: "",
    locale: "",
    description: "",
    score: 0,
    confidence: 0,
    topicality: 0,
    boundingPoly: undefined,
    locations: [],
    properties: [],
  };
}

export const EntityAnnotation: MessageFns<EntityAnnotation> = {
  encode(message: EntityAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.mid !== "") {
      writer.uint32(10).string(message.mid);
    }
    if (message.locale !== "") {
      writer.uint32(18).string(message.locale);
    }
    if (message.description !== "") {
      writer.uint32(26).string(message.description);
    }
    if (message.score !== 0) {
      writer.uint32(37).float(message.score);
    }
    if (message.confidence !== 0) {
      writer.uint32(45).float(message.confidence);
    }
    if (message.topicality !== 0) {
      writer.uint32(53).float(message.topicality);
    }
    if (message.boundingPoly !== undefined) {
      BoundingPoly.encode(message.boundingPoly, writer.uint32(58).fork()).join();
    }
    for (const v of message.locations) {
      LocationInfo.encode(v!, writer.uint32(66).fork()).join();
    }
    for (const v of message.properties) {
      Property.encode(v!, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EntityAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEntityAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.mid = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.locale = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.description = reader.string();
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.score = reader.float();
          continue;
        case 5:
          if (tag !== 45) {
            break;
          }

          message.confidence = reader.float();
          continue;
        case 6:
          if (tag !== 53) {
            break;
          }

          message.topicality = reader.float();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.boundingPoly = BoundingPoly.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.locations.push(LocationInfo.decode(reader, reader.uint32()));
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.properties.push(Property.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EntityAnnotation {
    return {
      mid: isSet(object.mid) ? globalThis.String(object.mid) : "",
      locale: isSet(object.locale) ? globalThis.String(object.locale) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      score: isSet(object.score) ? globalThis.Number(object.score) : 0,
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
      topicality: isSet(object.topicality) ? globalThis.Number(object.topicality) : 0,
      boundingPoly: isSet(object.boundingPoly) ? BoundingPoly.fromJSON(object.boundingPoly) : undefined,
      locations: globalThis.Array.isArray(object?.locations)
        ? object.locations.map((e: any) => LocationInfo.fromJSON(e))
        : [],
      properties: globalThis.Array.isArray(object?.properties)
        ? object.properties.map((e: any) => Property.fromJSON(e))
        : [],
    };
  },

  toJSON(message: EntityAnnotation): unknown {
    const obj: any = {};
    if (message.mid !== "") {
      obj.mid = message.mid;
    }
    if (message.locale !== "") {
      obj.locale = message.locale;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.score !== 0) {
      obj.score = message.score;
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    if (message.topicality !== 0) {
      obj.topicality = message.topicality;
    }
    if (message.boundingPoly !== undefined) {
      obj.boundingPoly = BoundingPoly.toJSON(message.boundingPoly);
    }
    if (message.locations?.length) {
      obj.locations = message.locations.map((e) => LocationInfo.toJSON(e));
    }
    if (message.properties?.length) {
      obj.properties = message.properties.map((e) => Property.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<EntityAnnotation>): EntityAnnotation {
    return EntityAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EntityAnnotation>): EntityAnnotation {
    const message = createBaseEntityAnnotation();
    message.mid = object.mid ?? "";
    message.locale = object.locale ?? "";
    message.description = object.description ?? "";
    message.score = object.score ?? 0;
    message.confidence = object.confidence ?? 0;
    message.topicality = object.topicality ?? 0;
    message.boundingPoly = (object.boundingPoly !== undefined && object.boundingPoly !== null)
      ? BoundingPoly.fromPartial(object.boundingPoly)
      : undefined;
    message.locations = object.locations?.map((e) => LocationInfo.fromPartial(e)) || [];
    message.properties = object.properties?.map((e) => Property.fromPartial(e)) || [];
    return message;
  },
};

function createBaseSafeSearchAnnotation(): SafeSearchAnnotation {
  return { adult: 0, spoof: 0, medical: 0, violence: 0, racy: 0 };
}

export const SafeSearchAnnotation: MessageFns<SafeSearchAnnotation> = {
  encode(message: SafeSearchAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.adult !== 0) {
      writer.uint32(8).int32(message.adult);
    }
    if (message.spoof !== 0) {
      writer.uint32(16).int32(message.spoof);
    }
    if (message.medical !== 0) {
      writer.uint32(24).int32(message.medical);
    }
    if (message.violence !== 0) {
      writer.uint32(32).int32(message.violence);
    }
    if (message.racy !== 0) {
      writer.uint32(72).int32(message.racy);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SafeSearchAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSafeSearchAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.adult = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.spoof = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.medical = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.violence = reader.int32() as any;
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.racy = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SafeSearchAnnotation {
    return {
      adult: isSet(object.adult) ? likelihoodFromJSON(object.adult) : 0,
      spoof: isSet(object.spoof) ? likelihoodFromJSON(object.spoof) : 0,
      medical: isSet(object.medical) ? likelihoodFromJSON(object.medical) : 0,
      violence: isSet(object.violence) ? likelihoodFromJSON(object.violence) : 0,
      racy: isSet(object.racy) ? likelihoodFromJSON(object.racy) : 0,
    };
  },

  toJSON(message: SafeSearchAnnotation): unknown {
    const obj: any = {};
    if (message.adult !== 0) {
      obj.adult = likelihoodToJSON(message.adult);
    }
    if (message.spoof !== 0) {
      obj.spoof = likelihoodToJSON(message.spoof);
    }
    if (message.medical !== 0) {
      obj.medical = likelihoodToJSON(message.medical);
    }
    if (message.violence !== 0) {
      obj.violence = likelihoodToJSON(message.violence);
    }
    if (message.racy !== 0) {
      obj.racy = likelihoodToJSON(message.racy);
    }
    return obj;
  },

  create(base?: DeepPartial<SafeSearchAnnotation>): SafeSearchAnnotation {
    return SafeSearchAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SafeSearchAnnotation>): SafeSearchAnnotation {
    const message = createBaseSafeSearchAnnotation();
    message.adult = object.adult ?? 0;
    message.spoof = object.spoof ?? 0;
    message.medical = object.medical ?? 0;
    message.violence = object.violence ?? 0;
    message.racy = object.racy ?? 0;
    return message;
  },
};

function createBaseLatLongRect(): LatLongRect {
  return { minLatLng: undefined, maxLatLng: undefined };
}

export const LatLongRect: MessageFns<LatLongRect> = {
  encode(message: LatLongRect, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.minLatLng !== undefined) {
      LatLng.encode(message.minLatLng, writer.uint32(10).fork()).join();
    }
    if (message.maxLatLng !== undefined) {
      LatLng.encode(message.maxLatLng, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LatLongRect {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLatLongRect();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.minLatLng = LatLng.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.maxLatLng = LatLng.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LatLongRect {
    return {
      minLatLng: isSet(object.minLatLng) ? LatLng.fromJSON(object.minLatLng) : undefined,
      maxLatLng: isSet(object.maxLatLng) ? LatLng.fromJSON(object.maxLatLng) : undefined,
    };
  },

  toJSON(message: LatLongRect): unknown {
    const obj: any = {};
    if (message.minLatLng !== undefined) {
      obj.minLatLng = LatLng.toJSON(message.minLatLng);
    }
    if (message.maxLatLng !== undefined) {
      obj.maxLatLng = LatLng.toJSON(message.maxLatLng);
    }
    return obj;
  },

  create(base?: DeepPartial<LatLongRect>): LatLongRect {
    return LatLongRect.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LatLongRect>): LatLongRect {
    const message = createBaseLatLongRect();
    message.minLatLng = (object.minLatLng !== undefined && object.minLatLng !== null)
      ? LatLng.fromPartial(object.minLatLng)
      : undefined;
    message.maxLatLng = (object.maxLatLng !== undefined && object.maxLatLng !== null)
      ? LatLng.fromPartial(object.maxLatLng)
      : undefined;
    return message;
  },
};

function createBaseColorInfo(): ColorInfo {
  return { color: undefined, score: 0, pixelFraction: 0 };
}

export const ColorInfo: MessageFns<ColorInfo> = {
  encode(message: ColorInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.color !== undefined) {
      Color.encode(message.color, writer.uint32(10).fork()).join();
    }
    if (message.score !== 0) {
      writer.uint32(21).float(message.score);
    }
    if (message.pixelFraction !== 0) {
      writer.uint32(29).float(message.pixelFraction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ColorInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseColorInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.color = Color.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.score = reader.float();
          continue;
        case 3:
          if (tag !== 29) {
            break;
          }

          message.pixelFraction = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ColorInfo {
    return {
      color: isSet(object.color) ? Color.fromJSON(object.color) : undefined,
      score: isSet(object.score) ? globalThis.Number(object.score) : 0,
      pixelFraction: isSet(object.pixelFraction) ? globalThis.Number(object.pixelFraction) : 0,
    };
  },

  toJSON(message: ColorInfo): unknown {
    const obj: any = {};
    if (message.color !== undefined) {
      obj.color = Color.toJSON(message.color);
    }
    if (message.score !== 0) {
      obj.score = message.score;
    }
    if (message.pixelFraction !== 0) {
      obj.pixelFraction = message.pixelFraction;
    }
    return obj;
  },

  create(base?: DeepPartial<ColorInfo>): ColorInfo {
    return ColorInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ColorInfo>): ColorInfo {
    const message = createBaseColorInfo();
    message.color = (object.color !== undefined && object.color !== null) ? Color.fromPartial(object.color) : undefined;
    message.score = object.score ?? 0;
    message.pixelFraction = object.pixelFraction ?? 0;
    return message;
  },
};

function createBaseDominantColorsAnnotation(): DominantColorsAnnotation {
  return { colors: [] };
}

export const DominantColorsAnnotation: MessageFns<DominantColorsAnnotation> = {
  encode(message: DominantColorsAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.colors) {
      ColorInfo.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DominantColorsAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDominantColorsAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.colors.push(ColorInfo.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DominantColorsAnnotation {
    return {
      colors: globalThis.Array.isArray(object?.colors) ? object.colors.map((e: any) => ColorInfo.fromJSON(e)) : [],
    };
  },

  toJSON(message: DominantColorsAnnotation): unknown {
    const obj: any = {};
    if (message.colors?.length) {
      obj.colors = message.colors.map((e) => ColorInfo.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<DominantColorsAnnotation>): DominantColorsAnnotation {
    return DominantColorsAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DominantColorsAnnotation>): DominantColorsAnnotation {
    const message = createBaseDominantColorsAnnotation();
    message.colors = object.colors?.map((e) => ColorInfo.fromPartial(e)) || [];
    return message;
  },
};

function createBaseImageProperties(): ImageProperties {
  return { dominantColors: undefined };
}

export const ImageProperties: MessageFns<ImageProperties> = {
  encode(message: ImageProperties, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dominantColors !== undefined) {
      DominantColorsAnnotation.encode(message.dominantColors, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImageProperties {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImageProperties();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dominantColors = DominantColorsAnnotation.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImageProperties {
    return {
      dominantColors: isSet(object.dominantColors)
        ? DominantColorsAnnotation.fromJSON(object.dominantColors)
        : undefined,
    };
  },

  toJSON(message: ImageProperties): unknown {
    const obj: any = {};
    if (message.dominantColors !== undefined) {
      obj.dominantColors = DominantColorsAnnotation.toJSON(message.dominantColors);
    }
    return obj;
  },

  create(base?: DeepPartial<ImageProperties>): ImageProperties {
    return ImageProperties.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImageProperties>): ImageProperties {
    const message = createBaseImageProperties();
    message.dominantColors = (object.dominantColors !== undefined && object.dominantColors !== null)
      ? DominantColorsAnnotation.fromPartial(object.dominantColors)
      : undefined;
    return message;
  },
};

function createBaseCropHint(): CropHint {
  return { boundingPoly: undefined, confidence: 0, importanceFraction: 0 };
}

export const CropHint: MessageFns<CropHint> = {
  encode(message: CropHint, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.boundingPoly !== undefined) {
      BoundingPoly.encode(message.boundingPoly, writer.uint32(10).fork()).join();
    }
    if (message.confidence !== 0) {
      writer.uint32(21).float(message.confidence);
    }
    if (message.importanceFraction !== 0) {
      writer.uint32(29).float(message.importanceFraction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CropHint {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCropHint();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.boundingPoly = BoundingPoly.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.confidence = reader.float();
          continue;
        case 3:
          if (tag !== 29) {
            break;
          }

          message.importanceFraction = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CropHint {
    return {
      boundingPoly: isSet(object.boundingPoly) ? BoundingPoly.fromJSON(object.boundingPoly) : undefined,
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
      importanceFraction: isSet(object.importanceFraction) ? globalThis.Number(object.importanceFraction) : 0,
    };
  },

  toJSON(message: CropHint): unknown {
    const obj: any = {};
    if (message.boundingPoly !== undefined) {
      obj.boundingPoly = BoundingPoly.toJSON(message.boundingPoly);
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    if (message.importanceFraction !== 0) {
      obj.importanceFraction = message.importanceFraction;
    }
    return obj;
  },

  create(base?: DeepPartial<CropHint>): CropHint {
    return CropHint.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CropHint>): CropHint {
    const message = createBaseCropHint();
    message.boundingPoly = (object.boundingPoly !== undefined && object.boundingPoly !== null)
      ? BoundingPoly.fromPartial(object.boundingPoly)
      : undefined;
    message.confidence = object.confidence ?? 0;
    message.importanceFraction = object.importanceFraction ?? 0;
    return message;
  },
};

function createBaseCropHintsAnnotation(): CropHintsAnnotation {
  return { cropHints: [] };
}

export const CropHintsAnnotation: MessageFns<CropHintsAnnotation> = {
  encode(message: CropHintsAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.cropHints) {
      CropHint.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CropHintsAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCropHintsAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.cropHints.push(CropHint.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CropHintsAnnotation {
    return {
      cropHints: globalThis.Array.isArray(object?.cropHints)
        ? object.cropHints.map((e: any) => CropHint.fromJSON(e))
        : [],
    };
  },

  toJSON(message: CropHintsAnnotation): unknown {
    const obj: any = {};
    if (message.cropHints?.length) {
      obj.cropHints = message.cropHints.map((e) => CropHint.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<CropHintsAnnotation>): CropHintsAnnotation {
    return CropHintsAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CropHintsAnnotation>): CropHintsAnnotation {
    const message = createBaseCropHintsAnnotation();
    message.cropHints = object.cropHints?.map((e) => CropHint.fromPartial(e)) || [];
    return message;
  },
};

function createBaseCropHintsParams(): CropHintsParams {
  return { aspectRatios: [] };
}

export const CropHintsParams: MessageFns<CropHintsParams> = {
  encode(message: CropHintsParams, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    writer.uint32(10).fork();
    for (const v of message.aspectRatios) {
      writer.float(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CropHintsParams {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCropHintsParams();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag === 13) {
            message.aspectRatios.push(reader.float());

            continue;
          }

          if (tag === 10) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.aspectRatios.push(reader.float());
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CropHintsParams {
    return {
      aspectRatios: globalThis.Array.isArray(object?.aspectRatios)
        ? object.aspectRatios.map((e: any) => globalThis.Number(e))
        : [],
    };
  },

  toJSON(message: CropHintsParams): unknown {
    const obj: any = {};
    if (message.aspectRatios?.length) {
      obj.aspectRatios = message.aspectRatios;
    }
    return obj;
  },

  create(base?: DeepPartial<CropHintsParams>): CropHintsParams {
    return CropHintsParams.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CropHintsParams>): CropHintsParams {
    const message = createBaseCropHintsParams();
    message.aspectRatios = object.aspectRatios?.map((e) => e) || [];
    return message;
  },
};

function createBaseWebDetectionParams(): WebDetectionParams {
  return { includeGeoResults: false };
}

export const WebDetectionParams: MessageFns<WebDetectionParams> = {
  encode(message: WebDetectionParams, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.includeGeoResults !== false) {
      writer.uint32(16).bool(message.includeGeoResults);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): WebDetectionParams {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseWebDetectionParams();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 16) {
            break;
          }

          message.includeGeoResults = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): WebDetectionParams {
    return {
      includeGeoResults: isSet(object.includeGeoResults) ? globalThis.Boolean(object.includeGeoResults) : false,
    };
  },

  toJSON(message: WebDetectionParams): unknown {
    const obj: any = {};
    if (message.includeGeoResults !== false) {
      obj.includeGeoResults = message.includeGeoResults;
    }
    return obj;
  },

  create(base?: DeepPartial<WebDetectionParams>): WebDetectionParams {
    return WebDetectionParams.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<WebDetectionParams>): WebDetectionParams {
    const message = createBaseWebDetectionParams();
    message.includeGeoResults = object.includeGeoResults ?? false;
    return message;
  },
};

function createBaseTextDetectionParams(): TextDetectionParams {
  return { enableTextDetectionConfidenceScore: false, advancedOcrOptions: [] };
}

export const TextDetectionParams: MessageFns<TextDetectionParams> = {
  encode(message: TextDetectionParams, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.enableTextDetectionConfidenceScore !== false) {
      writer.uint32(72).bool(message.enableTextDetectionConfidenceScore);
    }
    for (const v of message.advancedOcrOptions) {
      writer.uint32(90).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TextDetectionParams {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextDetectionParams();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 9:
          if (tag !== 72) {
            break;
          }

          message.enableTextDetectionConfidenceScore = reader.bool();
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.advancedOcrOptions.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextDetectionParams {
    return {
      enableTextDetectionConfidenceScore: isSet(object.enableTextDetectionConfidenceScore)
        ? globalThis.Boolean(object.enableTextDetectionConfidenceScore)
        : false,
      advancedOcrOptions: globalThis.Array.isArray(object?.advancedOcrOptions)
        ? object.advancedOcrOptions.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: TextDetectionParams): unknown {
    const obj: any = {};
    if (message.enableTextDetectionConfidenceScore !== false) {
      obj.enableTextDetectionConfidenceScore = message.enableTextDetectionConfidenceScore;
    }
    if (message.advancedOcrOptions?.length) {
      obj.advancedOcrOptions = message.advancedOcrOptions;
    }
    return obj;
  },

  create(base?: DeepPartial<TextDetectionParams>): TextDetectionParams {
    return TextDetectionParams.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TextDetectionParams>): TextDetectionParams {
    const message = createBaseTextDetectionParams();
    message.enableTextDetectionConfidenceScore = object.enableTextDetectionConfidenceScore ?? false;
    message.advancedOcrOptions = object.advancedOcrOptions?.map((e) => e) || [];
    return message;
  },
};

function createBaseImageContext(): ImageContext {
  return {
    latLongRect: undefined,
    languageHints: [],
    cropHintsParams: undefined,
    webDetectionParams: undefined,
    textDetectionParams: undefined,
  };
}

export const ImageContext: MessageFns<ImageContext> = {
  encode(message: ImageContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.latLongRect !== undefined) {
      LatLongRect.encode(message.latLongRect, writer.uint32(10).fork()).join();
    }
    for (const v of message.languageHints) {
      writer.uint32(18).string(v!);
    }
    if (message.cropHintsParams !== undefined) {
      CropHintsParams.encode(message.cropHintsParams, writer.uint32(34).fork()).join();
    }
    if (message.webDetectionParams !== undefined) {
      WebDetectionParams.encode(message.webDetectionParams, writer.uint32(50).fork()).join();
    }
    if (message.textDetectionParams !== undefined) {
      TextDetectionParams.encode(message.textDetectionParams, writer.uint32(98).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImageContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImageContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.latLongRect = LatLongRect.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.languageHints.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.cropHintsParams = CropHintsParams.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.webDetectionParams = WebDetectionParams.decode(reader, reader.uint32());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.textDetectionParams = TextDetectionParams.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImageContext {
    return {
      latLongRect: isSet(object.latLongRect) ? LatLongRect.fromJSON(object.latLongRect) : undefined,
      languageHints: globalThis.Array.isArray(object?.languageHints)
        ? object.languageHints.map((e: any) => globalThis.String(e))
        : [],
      cropHintsParams: isSet(object.cropHintsParams) ? CropHintsParams.fromJSON(object.cropHintsParams) : undefined,
      webDetectionParams: isSet(object.webDetectionParams)
        ? WebDetectionParams.fromJSON(object.webDetectionParams)
        : undefined,
      textDetectionParams: isSet(object.textDetectionParams)
        ? TextDetectionParams.fromJSON(object.textDetectionParams)
        : undefined,
    };
  },

  toJSON(message: ImageContext): unknown {
    const obj: any = {};
    if (message.latLongRect !== undefined) {
      obj.latLongRect = LatLongRect.toJSON(message.latLongRect);
    }
    if (message.languageHints?.length) {
      obj.languageHints = message.languageHints;
    }
    if (message.cropHintsParams !== undefined) {
      obj.cropHintsParams = CropHintsParams.toJSON(message.cropHintsParams);
    }
    if (message.webDetectionParams !== undefined) {
      obj.webDetectionParams = WebDetectionParams.toJSON(message.webDetectionParams);
    }
    if (message.textDetectionParams !== undefined) {
      obj.textDetectionParams = TextDetectionParams.toJSON(message.textDetectionParams);
    }
    return obj;
  },

  create(base?: DeepPartial<ImageContext>): ImageContext {
    return ImageContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImageContext>): ImageContext {
    const message = createBaseImageContext();
    message.latLongRect = (object.latLongRect !== undefined && object.latLongRect !== null)
      ? LatLongRect.fromPartial(object.latLongRect)
      : undefined;
    message.languageHints = object.languageHints?.map((e) => e) || [];
    message.cropHintsParams = (object.cropHintsParams !== undefined && object.cropHintsParams !== null)
      ? CropHintsParams.fromPartial(object.cropHintsParams)
      : undefined;
    message.webDetectionParams = (object.webDetectionParams !== undefined && object.webDetectionParams !== null)
      ? WebDetectionParams.fromPartial(object.webDetectionParams)
      : undefined;
    message.textDetectionParams = (object.textDetectionParams !== undefined && object.textDetectionParams !== null)
      ? TextDetectionParams.fromPartial(object.textDetectionParams)
      : undefined;
    return message;
  },
};

function createBaseAnnotateImageRequest(): AnnotateImageRequest {
  return { image: undefined, features: [], imageContext: undefined };
}

export const AnnotateImageRequest: MessageFns<AnnotateImageRequest> = {
  encode(message: AnnotateImageRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.image !== undefined) {
      Image.encode(message.image, writer.uint32(10).fork()).join();
    }
    for (const v of message.features) {
      Feature.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.imageContext !== undefined) {
      ImageContext.encode(message.imageContext, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnnotateImageRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnnotateImageRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.image = Image.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.features.push(Feature.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.imageContext = ImageContext.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnnotateImageRequest {
    return {
      image: isSet(object.image) ? Image.fromJSON(object.image) : undefined,
      features: globalThis.Array.isArray(object?.features) ? object.features.map((e: any) => Feature.fromJSON(e)) : [],
      imageContext: isSet(object.imageContext) ? ImageContext.fromJSON(object.imageContext) : undefined,
    };
  },

  toJSON(message: AnnotateImageRequest): unknown {
    const obj: any = {};
    if (message.image !== undefined) {
      obj.image = Image.toJSON(message.image);
    }
    if (message.features?.length) {
      obj.features = message.features.map((e) => Feature.toJSON(e));
    }
    if (message.imageContext !== undefined) {
      obj.imageContext = ImageContext.toJSON(message.imageContext);
    }
    return obj;
  },

  create(base?: DeepPartial<AnnotateImageRequest>): AnnotateImageRequest {
    return AnnotateImageRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AnnotateImageRequest>): AnnotateImageRequest {
    const message = createBaseAnnotateImageRequest();
    message.image = (object.image !== undefined && object.image !== null) ? Image.fromPartial(object.image) : undefined;
    message.features = object.features?.map((e) => Feature.fromPartial(e)) || [];
    message.imageContext = (object.imageContext !== undefined && object.imageContext !== null)
      ? ImageContext.fromPartial(object.imageContext)
      : undefined;
    return message;
  },
};

function createBaseAnnotateImageResponse(): AnnotateImageResponse {
  return {
    faceAnnotations: [],
    landmarkAnnotations: [],
    logoAnnotations: [],
    labelAnnotations: [],
    textAnnotations: [],
    fullTextAnnotation: undefined,
    safeSearchAnnotation: undefined,
    imagePropertiesAnnotation: undefined,
    cropHintsAnnotation: undefined,
    webDetection: undefined,
    error: undefined,
  };
}

export const AnnotateImageResponse: MessageFns<AnnotateImageResponse> = {
  encode(message: AnnotateImageResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.faceAnnotations) {
      FaceAnnotation.encode(v!, writer.uint32(10).fork()).join();
    }
    for (const v of message.landmarkAnnotations) {
      EntityAnnotation.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.logoAnnotations) {
      EntityAnnotation.encode(v!, writer.uint32(26).fork()).join();
    }
    for (const v of message.labelAnnotations) {
      EntityAnnotation.encode(v!, writer.uint32(34).fork()).join();
    }
    for (const v of message.textAnnotations) {
      EntityAnnotation.encode(v!, writer.uint32(42).fork()).join();
    }
    if (message.fullTextAnnotation !== undefined) {
      TextAnnotation.encode(message.fullTextAnnotation, writer.uint32(98).fork()).join();
    }
    if (message.safeSearchAnnotation !== undefined) {
      SafeSearchAnnotation.encode(message.safeSearchAnnotation, writer.uint32(50).fork()).join();
    }
    if (message.imagePropertiesAnnotation !== undefined) {
      ImageProperties.encode(message.imagePropertiesAnnotation, writer.uint32(66).fork()).join();
    }
    if (message.cropHintsAnnotation !== undefined) {
      CropHintsAnnotation.encode(message.cropHintsAnnotation, writer.uint32(90).fork()).join();
    }
    if (message.webDetection !== undefined) {
      WebDetection.encode(message.webDetection, writer.uint32(106).fork()).join();
    }
    if (message.error !== undefined) {
      Status.encode(message.error, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnnotateImageResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnnotateImageResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.faceAnnotations.push(FaceAnnotation.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.landmarkAnnotations.push(EntityAnnotation.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.logoAnnotations.push(EntityAnnotation.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.labelAnnotations.push(EntityAnnotation.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.textAnnotations.push(EntityAnnotation.decode(reader, reader.uint32()));
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.fullTextAnnotation = TextAnnotation.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.safeSearchAnnotation = SafeSearchAnnotation.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.imagePropertiesAnnotation = ImageProperties.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.cropHintsAnnotation = CropHintsAnnotation.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.webDetection = WebDetection.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.error = Status.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnnotateImageResponse {
    return {
      faceAnnotations: globalThis.Array.isArray(object?.faceAnnotations)
        ? object.faceAnnotations.map((e: any) => FaceAnnotation.fromJSON(e))
        : [],
      landmarkAnnotations: globalThis.Array.isArray(object?.landmarkAnnotations)
        ? object.landmarkAnnotations.map((e: any) => EntityAnnotation.fromJSON(e))
        : [],
      logoAnnotations: globalThis.Array.isArray(object?.logoAnnotations)
        ? object.logoAnnotations.map((e: any) => EntityAnnotation.fromJSON(e))
        : [],
      labelAnnotations: globalThis.Array.isArray(object?.labelAnnotations)
        ? object.labelAnnotations.map((e: any) => EntityAnnotation.fromJSON(e))
        : [],
      textAnnotations: globalThis.Array.isArray(object?.textAnnotations)
        ? object.textAnnotations.map((e: any) => EntityAnnotation.fromJSON(e))
        : [],
      fullTextAnnotation: isSet(object.fullTextAnnotation)
        ? TextAnnotation.fromJSON(object.fullTextAnnotation)
        : undefined,
      safeSearchAnnotation: isSet(object.safeSearchAnnotation)
        ? SafeSearchAnnotation.fromJSON(object.safeSearchAnnotation)
        : undefined,
      imagePropertiesAnnotation: isSet(object.imagePropertiesAnnotation)
        ? ImageProperties.fromJSON(object.imagePropertiesAnnotation)
        : undefined,
      cropHintsAnnotation: isSet(object.cropHintsAnnotation)
        ? CropHintsAnnotation.fromJSON(object.cropHintsAnnotation)
        : undefined,
      webDetection: isSet(object.webDetection) ? WebDetection.fromJSON(object.webDetection) : undefined,
      error: isSet(object.error) ? Status.fromJSON(object.error) : undefined,
    };
  },

  toJSON(message: AnnotateImageResponse): unknown {
    const obj: any = {};
    if (message.faceAnnotations?.length) {
      obj.faceAnnotations = message.faceAnnotations.map((e) => FaceAnnotation.toJSON(e));
    }
    if (message.landmarkAnnotations?.length) {
      obj.landmarkAnnotations = message.landmarkAnnotations.map((e) => EntityAnnotation.toJSON(e));
    }
    if (message.logoAnnotations?.length) {
      obj.logoAnnotations = message.logoAnnotations.map((e) => EntityAnnotation.toJSON(e));
    }
    if (message.labelAnnotations?.length) {
      obj.labelAnnotations = message.labelAnnotations.map((e) => EntityAnnotation.toJSON(e));
    }
    if (message.textAnnotations?.length) {
      obj.textAnnotations = message.textAnnotations.map((e) => EntityAnnotation.toJSON(e));
    }
    if (message.fullTextAnnotation !== undefined) {
      obj.fullTextAnnotation = TextAnnotation.toJSON(message.fullTextAnnotation);
    }
    if (message.safeSearchAnnotation !== undefined) {
      obj.safeSearchAnnotation = SafeSearchAnnotation.toJSON(message.safeSearchAnnotation);
    }
    if (message.imagePropertiesAnnotation !== undefined) {
      obj.imagePropertiesAnnotation = ImageProperties.toJSON(message.imagePropertiesAnnotation);
    }
    if (message.cropHintsAnnotation !== undefined) {
      obj.cropHintsAnnotation = CropHintsAnnotation.toJSON(message.cropHintsAnnotation);
    }
    if (message.webDetection !== undefined) {
      obj.webDetection = WebDetection.toJSON(message.webDetection);
    }
    if (message.error !== undefined) {
      obj.error = Status.toJSON(message.error);
    }
    return obj;
  },

  create(base?: DeepPartial<AnnotateImageResponse>): AnnotateImageResponse {
    return AnnotateImageResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AnnotateImageResponse>): AnnotateImageResponse {
    const message = createBaseAnnotateImageResponse();
    message.faceAnnotations = object.faceAnnotations?.map((e) => FaceAnnotation.fromPartial(e)) || [];
    message.landmarkAnnotations = object.landmarkAnnotations?.map((e) => EntityAnnotation.fromPartial(e)) || [];
    message.logoAnnotations = object.logoAnnotations?.map((e) => EntityAnnotation.fromPartial(e)) || [];
    message.labelAnnotations = object.labelAnnotations?.map((e) => EntityAnnotation.fromPartial(e)) || [];
    message.textAnnotations = object.textAnnotations?.map((e) => EntityAnnotation.fromPartial(e)) || [];
    message.fullTextAnnotation = (object.fullTextAnnotation !== undefined && object.fullTextAnnotation !== null)
      ? TextAnnotation.fromPartial(object.fullTextAnnotation)
      : undefined;
    message.safeSearchAnnotation = (object.safeSearchAnnotation !== undefined && object.safeSearchAnnotation !== null)
      ? SafeSearchAnnotation.fromPartial(object.safeSearchAnnotation)
      : undefined;
    message.imagePropertiesAnnotation =
      (object.imagePropertiesAnnotation !== undefined && object.imagePropertiesAnnotation !== null)
        ? ImageProperties.fromPartial(object.imagePropertiesAnnotation)
        : undefined;
    message.cropHintsAnnotation = (object.cropHintsAnnotation !== undefined && object.cropHintsAnnotation !== null)
      ? CropHintsAnnotation.fromPartial(object.cropHintsAnnotation)
      : undefined;
    message.webDetection = (object.webDetection !== undefined && object.webDetection !== null)
      ? WebDetection.fromPartial(object.webDetection)
      : undefined;
    message.error = (object.error !== undefined && object.error !== null)
      ? Status.fromPartial(object.error)
      : undefined;
    return message;
  },
};

function createBaseBatchAnnotateImagesRequest(): BatchAnnotateImagesRequest {
  return { requests: [] };
}

export const BatchAnnotateImagesRequest: MessageFns<BatchAnnotateImagesRequest> = {
  encode(message: BatchAnnotateImagesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.requests) {
      AnnotateImageRequest.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchAnnotateImagesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchAnnotateImagesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.requests.push(AnnotateImageRequest.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchAnnotateImagesRequest {
    return {
      requests: globalThis.Array.isArray(object?.requests)
        ? object.requests.map((e: any) => AnnotateImageRequest.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchAnnotateImagesRequest): unknown {
    const obj: any = {};
    if (message.requests?.length) {
      obj.requests = message.requests.map((e) => AnnotateImageRequest.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchAnnotateImagesRequest>): BatchAnnotateImagesRequest {
    return BatchAnnotateImagesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchAnnotateImagesRequest>): BatchAnnotateImagesRequest {
    const message = createBaseBatchAnnotateImagesRequest();
    message.requests = object.requests?.map((e) => AnnotateImageRequest.fromPartial(e)) || [];
    return message;
  },
};

function createBaseBatchAnnotateImagesResponse(): BatchAnnotateImagesResponse {
  return { responses: [] };
}

export const BatchAnnotateImagesResponse: MessageFns<BatchAnnotateImagesResponse> = {
  encode(message: BatchAnnotateImagesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.responses) {
      AnnotateImageResponse.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchAnnotateImagesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchAnnotateImagesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.responses.push(AnnotateImageResponse.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchAnnotateImagesResponse {
    return {
      responses: globalThis.Array.isArray(object?.responses)
        ? object.responses.map((e: any) => AnnotateImageResponse.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchAnnotateImagesResponse): unknown {
    const obj: any = {};
    if (message.responses?.length) {
      obj.responses = message.responses.map((e) => AnnotateImageResponse.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchAnnotateImagesResponse>): BatchAnnotateImagesResponse {
    return BatchAnnotateImagesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchAnnotateImagesResponse>): BatchAnnotateImagesResponse {
    const message = createBaseBatchAnnotateImagesResponse();
    message.responses = object.responses?.map((e) => AnnotateImageResponse.fromPartial(e)) || [];
    return message;
  },
};

/**
 * Service that performs Google Cloud Vision API detection tasks over client
 * images, such as face, landmark, logo, label, and text detection. The
 * ImageAnnotator service returns detected entities from the images.
 */
export type ImageAnnotatorDefinition = typeof ImageAnnotatorDefinition;
export const ImageAnnotatorDefinition = {
  name: "ImageAnnotator",
  fullName: "google.cloud.vision.v1p1beta1.ImageAnnotator",
  methods: {
    /** Run image detection and annotation for a batch of images. */
    batchAnnotateImages: {
      name: "BatchAnnotateImages",
      requestType: BatchAnnotateImagesRequest,
      requestStream: false,
      responseType: BatchAnnotateImagesResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([8, 114, 101, 113, 117, 101, 115, 116, 115])],
          578365826: [
            Buffer.from([
              31,
              58,
              1,
              42,
              34,
              26,
              47,
              118,
              49,
              112,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              105,
              109,
              97,
              103,
              101,
              115,
              58,
              97,
              110,
              110,
              111,
              116,
              97,
              116,
              101,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface ImageAnnotatorServiceImplementation<CallContextExt = {}> {
  /** Run image detection and annotation for a batch of images. */
  batchAnnotateImages(
    request: BatchAnnotateImagesRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<BatchAnnotateImagesResponse>>;
}

export interface ImageAnnotatorClient<CallOptionsExt = {}> {
  /** Run image detection and annotation for a batch of images. */
  batchAnnotateImages(
    request: DeepPartial<BatchAnnotateImagesRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<BatchAnnotateImagesResponse>;
}

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
