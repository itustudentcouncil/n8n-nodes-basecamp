// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/automl/v1beta1/prediction_service.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Operation } from "../../../longrunning/operations.js";
import { AnnotationPayload } from "./annotation_payload.js";
import { ExamplePayload } from "./data_items.js";
import { BatchPredictInputConfig, BatchPredictOutputConfig } from "./io.js";

export const protobufPackage = "google.cloud.automl.v1beta1";

/** Request message for [PredictionService.Predict][google.cloud.automl.v1beta1.PredictionService.Predict]. */
export interface PredictRequest {
  /** Required. Name of the model requested to serve the prediction. */
  name: string;
  /**
   * Required. Payload to perform a prediction on. The payload must match the
   * problem type that the model was trained to solve.
   */
  payload:
    | ExamplePayload
    | undefined;
  /**
   * Additional domain-specific parameters, any string must be up to 25000
   * characters long.
   *
   * *  For Image Classification:
   *
   *    `score_threshold` - (float) A value from 0.0 to 1.0. When the model
   *     makes predictions for an image, it will only produce results that have
   *     at least this confidence score. The default is 0.5.
   *
   *  *  For Image Object Detection:
   *    `score_threshold` - (float) When Model detects objects on the image,
   *        it will only produce bounding boxes which have at least this
   *        confidence score. Value in 0 to 1 range, default is 0.5.
   *    `max_bounding_box_count` - (int64) No more than this number of bounding
   *        boxes will be returned in the response. Default is 100, the
   *        requested value may be limited by server.
   * *  For Tables:
   *    feature_imp<span>ortan</span>ce - (boolean) Whether feature importance
   *        should be populated in the returned TablesAnnotation.
   *        The default is false.
   */
  params: { [key: string]: string };
}

export interface PredictRequest_ParamsEntry {
  key: string;
  value: string;
}

/** Response message for [PredictionService.Predict][google.cloud.automl.v1beta1.PredictionService.Predict]. */
export interface PredictResponse {
  /**
   * Prediction result.
   * Translation and Text Sentiment will return precisely one payload.
   */
  payload: AnnotationPayload[];
  /**
   * The preprocessed example that AutoML actually makes prediction on.
   * Empty if AutoML does not preprocess the input example.
   * * For Text Extraction:
   *   If the input is a .pdf file, the OCR'ed text will be provided in
   *   [document_text][google.cloud.automl.v1beta1.Document.document_text].
   */
  preprocessedInput:
    | ExamplePayload
    | undefined;
  /**
   * Additional domain-specific prediction response metadata.
   *
   * * For Image Object Detection:
   *  `max_bounding_box_count` - (int64) At most that many bounding boxes per
   *      image could have been returned.
   *
   * * For Text Sentiment:
   *  `sentiment_score` - (float, deprecated) A value between -1 and 1,
   *      -1 maps to least positive sentiment, while 1 maps to the most positive
   *      one and the higher the score, the more positive the sentiment in the
   *      document is. Yet these values are relative to the training data, so
   *      e.g. if all data was positive then -1 will be also positive (though
   *      the least).
   *      The sentiment_score shouldn't be confused with "score" or "magnitude"
   *      from the previous Natural Language Sentiment Analysis API.
   */
  metadata: { [key: string]: string };
}

export interface PredictResponse_MetadataEntry {
  key: string;
  value: string;
}

/** Request message for [PredictionService.BatchPredict][google.cloud.automl.v1beta1.PredictionService.BatchPredict]. */
export interface BatchPredictRequest {
  /** Required. Name of the model requested to serve the batch prediction. */
  name: string;
  /** Required. The input configuration for batch prediction. */
  inputConfig:
    | BatchPredictInputConfig
    | undefined;
  /**
   * Required. The Configuration specifying where output predictions should
   * be written.
   */
  outputConfig:
    | BatchPredictOutputConfig
    | undefined;
  /**
   * Required. Additional domain-specific parameters for the predictions, any string must
   * be up to 25000 characters long.
   *
   * *  For Text Classification:
   *
   *    `score_threshold` - (float) A value from 0.0 to 1.0. When the model
   *         makes predictions for a text snippet, it will only produce results
   *         that have at least this confidence score. The default is 0.5.
   *
   * *  For Image Classification:
   *
   *    `score_threshold` - (float) A value from 0.0 to 1.0. When the model
   *         makes predictions for an image, it will only produce results that
   *         have at least this confidence score. The default is 0.5.
   *
   * *  For Image Object Detection:
   *
   *    `score_threshold` - (float) When Model detects objects on the image,
   *        it will only produce bounding boxes which have at least this
   *        confidence score. Value in 0 to 1 range, default is 0.5.
   *    `max_bounding_box_count` - (int64) No more than this number of bounding
   *        boxes will be produced per image. Default is 100, the
   *        requested value may be limited by server.
   *
   * *  For Video Classification :
   *
   *    `score_threshold` - (float) A value from 0.0 to 1.0. When the model
   *        makes predictions for a video, it will only produce results that
   *        have at least this confidence score. The default is 0.5.
   *    `segment_classification` - (boolean) Set to true to request
   *        segment-level classification. AutoML Video Intelligence returns
   *        labels and their confidence scores for the entire segment of the
   *        video that user specified in the request configuration.
   *        The default is "true".
   *    `shot_classification` - (boolean) Set to true to request shot-level
   *        classification. AutoML Video Intelligence determines the boundaries
   *        for each camera shot in the entire segment of the video that user
   *        specified in the request configuration. AutoML Video Intelligence
   *        then returns labels and their confidence scores for each detected
   *        shot, along with the start and end time of the shot.
   *        WARNING: Model evaluation is not done for this classification type,
   *        the quality of it depends on training data, but there are no metrics
   *        provided to describe that quality. The default is "false".
   *    `1s_interval_classification` - (boolean) Set to true to request
   *        classification for a video at one-second intervals. AutoML Video
   *        Intelligence returns labels and their confidence scores for each
   *        second of the entire segment of the video that user specified in the
   *        request configuration.
   *        WARNING: Model evaluation is not done for this classification
   *        type, the quality of it depends on training data, but there are no
   *        metrics provided to describe that quality. The default is
   *        "false".
   *
   * *  For Tables:
   *
   *    feature_imp<span>ortan</span>ce - (boolean) Whether feature importance
   *        should be populated in the returned TablesAnnotations. The
   *        default is false.
   *
   * *  For Video Object Tracking:
   *
   *    `score_threshold` - (float) When Model detects objects on video frames,
   *        it will only produce bounding boxes which have at least this
   *        confidence score. Value in 0 to 1 range, default is 0.5.
   *    `max_bounding_box_count` - (int64) No more than this number of bounding
   *        boxes will be returned per frame. Default is 100, the requested
   *        value may be limited by server.
   *    `min_bounding_box_size` - (float) Only bounding boxes with shortest edge
   *      at least that long as a relative value of video frame size will be
   *      returned. Value in 0 to 1 range. Default is 0.
   */
  params: { [key: string]: string };
}

export interface BatchPredictRequest_ParamsEntry {
  key: string;
  value: string;
}

/**
 * Result of the Batch Predict. This message is returned in
 * [response][google.longrunning.Operation.response] of the operation returned
 * by the [PredictionService.BatchPredict][google.cloud.automl.v1beta1.PredictionService.BatchPredict].
 */
export interface BatchPredictResult {
  /**
   * Additional domain-specific prediction response metadata.
   *
   * *  For Image Object Detection:
   *  `max_bounding_box_count` - (int64) At most that many bounding boxes per
   *      image could have been returned.
   *
   * *  For Video Object Tracking:
   *  `max_bounding_box_count` - (int64) At most that many bounding boxes per
   *      frame could have been returned.
   */
  metadata: { [key: string]: string };
}

export interface BatchPredictResult_MetadataEntry {
  key: string;
  value: string;
}

function createBasePredictRequest(): PredictRequest {
  return { name: "", payload: undefined, params: {} };
}

export const PredictRequest: MessageFns<PredictRequest> = {
  encode(message: PredictRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.payload !== undefined) {
      ExamplePayload.encode(message.payload, writer.uint32(18).fork()).join();
    }
    Object.entries(message.params).forEach(([key, value]) => {
      PredictRequest_ParamsEntry.encode({ key: key as any, value }, writer.uint32(26).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PredictRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePredictRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.payload = ExamplePayload.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          const entry3 = PredictRequest_ParamsEntry.decode(reader, reader.uint32());
          if (entry3.value !== undefined) {
            message.params[entry3.key] = entry3.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PredictRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      payload: isSet(object.payload) ? ExamplePayload.fromJSON(object.payload) : undefined,
      params: isObject(object.params)
        ? Object.entries(object.params).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: PredictRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.payload !== undefined) {
      obj.payload = ExamplePayload.toJSON(message.payload);
    }
    if (message.params) {
      const entries = Object.entries(message.params);
      if (entries.length > 0) {
        obj.params = {};
        entries.forEach(([k, v]) => {
          obj.params[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<PredictRequest>): PredictRequest {
    return PredictRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PredictRequest>): PredictRequest {
    const message = createBasePredictRequest();
    message.name = object.name ?? "";
    message.payload = (object.payload !== undefined && object.payload !== null)
      ? ExamplePayload.fromPartial(object.payload)
      : undefined;
    message.params = Object.entries(object.params ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBasePredictRequest_ParamsEntry(): PredictRequest_ParamsEntry {
  return { key: "", value: "" };
}

export const PredictRequest_ParamsEntry: MessageFns<PredictRequest_ParamsEntry> = {
  encode(message: PredictRequest_ParamsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PredictRequest_ParamsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePredictRequest_ParamsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PredictRequest_ParamsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: PredictRequest_ParamsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<PredictRequest_ParamsEntry>): PredictRequest_ParamsEntry {
    return PredictRequest_ParamsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PredictRequest_ParamsEntry>): PredictRequest_ParamsEntry {
    const message = createBasePredictRequest_ParamsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBasePredictResponse(): PredictResponse {
  return { payload: [], preprocessedInput: undefined, metadata: {} };
}

export const PredictResponse: MessageFns<PredictResponse> = {
  encode(message: PredictResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.payload) {
      AnnotationPayload.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.preprocessedInput !== undefined) {
      ExamplePayload.encode(message.preprocessedInput, writer.uint32(26).fork()).join();
    }
    Object.entries(message.metadata).forEach(([key, value]) => {
      PredictResponse_MetadataEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PredictResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePredictResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.payload.push(AnnotationPayload.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.preprocessedInput = ExamplePayload.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = PredictResponse_MetadataEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.metadata[entry2.key] = entry2.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PredictResponse {
    return {
      payload: globalThis.Array.isArray(object?.payload)
        ? object.payload.map((e: any) => AnnotationPayload.fromJSON(e))
        : [],
      preprocessedInput: isSet(object.preprocessedInput)
        ? ExamplePayload.fromJSON(object.preprocessedInput)
        : undefined,
      metadata: isObject(object.metadata)
        ? Object.entries(object.metadata).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: PredictResponse): unknown {
    const obj: any = {};
    if (message.payload?.length) {
      obj.payload = message.payload.map((e) => AnnotationPayload.toJSON(e));
    }
    if (message.preprocessedInput !== undefined) {
      obj.preprocessedInput = ExamplePayload.toJSON(message.preprocessedInput);
    }
    if (message.metadata) {
      const entries = Object.entries(message.metadata);
      if (entries.length > 0) {
        obj.metadata = {};
        entries.forEach(([k, v]) => {
          obj.metadata[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<PredictResponse>): PredictResponse {
    return PredictResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PredictResponse>): PredictResponse {
    const message = createBasePredictResponse();
    message.payload = object.payload?.map((e) => AnnotationPayload.fromPartial(e)) || [];
    message.preprocessedInput = (object.preprocessedInput !== undefined && object.preprocessedInput !== null)
      ? ExamplePayload.fromPartial(object.preprocessedInput)
      : undefined;
    message.metadata = Object.entries(object.metadata ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBasePredictResponse_MetadataEntry(): PredictResponse_MetadataEntry {
  return { key: "", value: "" };
}

export const PredictResponse_MetadataEntry: MessageFns<PredictResponse_MetadataEntry> = {
  encode(message: PredictResponse_MetadataEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PredictResponse_MetadataEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePredictResponse_MetadataEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PredictResponse_MetadataEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: PredictResponse_MetadataEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<PredictResponse_MetadataEntry>): PredictResponse_MetadataEntry {
    return PredictResponse_MetadataEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PredictResponse_MetadataEntry>): PredictResponse_MetadataEntry {
    const message = createBasePredictResponse_MetadataEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseBatchPredictRequest(): BatchPredictRequest {
  return { name: "", inputConfig: undefined, outputConfig: undefined, params: {} };
}

export const BatchPredictRequest: MessageFns<BatchPredictRequest> = {
  encode(message: BatchPredictRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.inputConfig !== undefined) {
      BatchPredictInputConfig.encode(message.inputConfig, writer.uint32(26).fork()).join();
    }
    if (message.outputConfig !== undefined) {
      BatchPredictOutputConfig.encode(message.outputConfig, writer.uint32(34).fork()).join();
    }
    Object.entries(message.params).forEach(([key, value]) => {
      BatchPredictRequest_ParamsEntry.encode({ key: key as any, value }, writer.uint32(42).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchPredictRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchPredictRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.inputConfig = BatchPredictInputConfig.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.outputConfig = BatchPredictOutputConfig.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          const entry5 = BatchPredictRequest_ParamsEntry.decode(reader, reader.uint32());
          if (entry5.value !== undefined) {
            message.params[entry5.key] = entry5.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchPredictRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      inputConfig: isSet(object.inputConfig) ? BatchPredictInputConfig.fromJSON(object.inputConfig) : undefined,
      outputConfig: isSet(object.outputConfig) ? BatchPredictOutputConfig.fromJSON(object.outputConfig) : undefined,
      params: isObject(object.params)
        ? Object.entries(object.params).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: BatchPredictRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.inputConfig !== undefined) {
      obj.inputConfig = BatchPredictInputConfig.toJSON(message.inputConfig);
    }
    if (message.outputConfig !== undefined) {
      obj.outputConfig = BatchPredictOutputConfig.toJSON(message.outputConfig);
    }
    if (message.params) {
      const entries = Object.entries(message.params);
      if (entries.length > 0) {
        obj.params = {};
        entries.forEach(([k, v]) => {
          obj.params[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<BatchPredictRequest>): BatchPredictRequest {
    return BatchPredictRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchPredictRequest>): BatchPredictRequest {
    const message = createBaseBatchPredictRequest();
    message.name = object.name ?? "";
    message.inputConfig = (object.inputConfig !== undefined && object.inputConfig !== null)
      ? BatchPredictInputConfig.fromPartial(object.inputConfig)
      : undefined;
    message.outputConfig = (object.outputConfig !== undefined && object.outputConfig !== null)
      ? BatchPredictOutputConfig.fromPartial(object.outputConfig)
      : undefined;
    message.params = Object.entries(object.params ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseBatchPredictRequest_ParamsEntry(): BatchPredictRequest_ParamsEntry {
  return { key: "", value: "" };
}

export const BatchPredictRequest_ParamsEntry: MessageFns<BatchPredictRequest_ParamsEntry> = {
  encode(message: BatchPredictRequest_ParamsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchPredictRequest_ParamsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchPredictRequest_ParamsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchPredictRequest_ParamsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: BatchPredictRequest_ParamsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<BatchPredictRequest_ParamsEntry>): BatchPredictRequest_ParamsEntry {
    return BatchPredictRequest_ParamsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchPredictRequest_ParamsEntry>): BatchPredictRequest_ParamsEntry {
    const message = createBaseBatchPredictRequest_ParamsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseBatchPredictResult(): BatchPredictResult {
  return { metadata: {} };
}

export const BatchPredictResult: MessageFns<BatchPredictResult> = {
  encode(message: BatchPredictResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.metadata).forEach(([key, value]) => {
      BatchPredictResult_MetadataEntry.encode({ key: key as any, value }, writer.uint32(10).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchPredictResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchPredictResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          const entry1 = BatchPredictResult_MetadataEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.metadata[entry1.key] = entry1.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchPredictResult {
    return {
      metadata: isObject(object.metadata)
        ? Object.entries(object.metadata).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: BatchPredictResult): unknown {
    const obj: any = {};
    if (message.metadata) {
      const entries = Object.entries(message.metadata);
      if (entries.length > 0) {
        obj.metadata = {};
        entries.forEach(([k, v]) => {
          obj.metadata[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<BatchPredictResult>): BatchPredictResult {
    return BatchPredictResult.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchPredictResult>): BatchPredictResult {
    const message = createBaseBatchPredictResult();
    message.metadata = Object.entries(object.metadata ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseBatchPredictResult_MetadataEntry(): BatchPredictResult_MetadataEntry {
  return { key: "", value: "" };
}

export const BatchPredictResult_MetadataEntry: MessageFns<BatchPredictResult_MetadataEntry> = {
  encode(message: BatchPredictResult_MetadataEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchPredictResult_MetadataEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchPredictResult_MetadataEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchPredictResult_MetadataEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: BatchPredictResult_MetadataEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<BatchPredictResult_MetadataEntry>): BatchPredictResult_MetadataEntry {
    return BatchPredictResult_MetadataEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchPredictResult_MetadataEntry>): BatchPredictResult_MetadataEntry {
    const message = createBaseBatchPredictResult_MetadataEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

/**
 * AutoML Prediction API.
 *
 * On any input that is documented to expect a string parameter in
 * snake_case or kebab-case, either of those cases is accepted.
 */
export type PredictionServiceDefinition = typeof PredictionServiceDefinition;
export const PredictionServiceDefinition = {
  name: "PredictionService",
  fullName: "google.cloud.automl.v1beta1.PredictionService",
  methods: {
    /**
     * Perform an online prediction. The prediction result will be directly
     * returned in the response.
     * Available for following ML problems, and their expected request payloads:
     * * Image Classification - Image in .JPEG, .GIF or .PNG format, image_bytes
     *                          up to 30MB.
     * * Image Object Detection - Image in .JPEG, .GIF or .PNG format, image_bytes
     *                            up to 30MB.
     * * Text Classification - TextSnippet, content up to 60,000 characters,
     *                         UTF-8 encoded.
     * * Text Extraction - TextSnippet, content up to 30,000 characters,
     *                     UTF-8 NFC encoded.
     * * Translation - TextSnippet, content up to 25,000 characters, UTF-8
     *                 encoded.
     * * Tables - Row, with column values matching the columns of the model,
     *            up to 5MB. Not available for FORECASTING
     *
     * [prediction_type][google.cloud.automl.v1beta1.TablesModelMetadata.prediction_type].
     * * Text Sentiment - TextSnippet, content up 500 characters, UTF-8
     *                     encoded.
     */
    predict: {
      name: "Predict",
      requestType: PredictRequest,
      requestStream: false,
      responseType: PredictResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([19, 110, 97, 109, 101, 44, 112, 97, 121, 108, 111, 97, 100, 44, 112, 97, 114, 97, 109, 115]),
          ],
          578365826: [
            Buffer.from([
              60,
              58,
              1,
              42,
              34,
              55,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              112,
              114,
              101,
              100,
              105,
              99,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * Perform a batch prediction. Unlike the online [Predict][google.cloud.automl.v1beta1.PredictionService.Predict], batch
     * prediction result won't be immediately available in the response. Instead,
     * a long running operation object is returned. User can poll the operation
     * result via [GetOperation][google.longrunning.Operations.GetOperation]
     * method. Once the operation is done, [BatchPredictResult][google.cloud.automl.v1beta1.BatchPredictResult] is returned in
     * the [response][google.longrunning.Operation.response] field.
     * Available for following ML problems:
     * * Image Classification
     * * Image Object Detection
     * * Video Classification
     * * Video Object Tracking * Text Extraction
     * * Tables
     */
    batchPredict: {
      name: "BatchPredict",
      requestType: BatchPredictRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              39,
              10,
              18,
              66,
              97,
              116,
              99,
              104,
              80,
              114,
              101,
              100,
              105,
              99,
              116,
              82,
              101,
              115,
              117,
              108,
              116,
              18,
              17,
              79,
              112,
              101,
              114,
              97,
              116,
              105,
              111,
              110,
              77,
              101,
              116,
              97,
              100,
              97,
              116,
              97,
            ]),
          ],
          8410: [
            Buffer.from([
              38,
              110,
              97,
              109,
              101,
              44,
              105,
              110,
              112,
              117,
              116,
              95,
              99,
              111,
              110,
              102,
              105,
              103,
              44,
              111,
              117,
              116,
              112,
              117,
              116,
              95,
              99,
              111,
              110,
              102,
              105,
              103,
              44,
              112,
              97,
              114,
              97,
              109,
              115,
            ]),
          ],
          578365826: [
            Buffer.from([
              65,
              58,
              1,
              42,
              34,
              60,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              98,
              97,
              116,
              99,
              104,
              80,
              114,
              101,
              100,
              105,
              99,
              116,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface PredictionServiceImplementation<CallContextExt = {}> {
  /**
   * Perform an online prediction. The prediction result will be directly
   * returned in the response.
   * Available for following ML problems, and their expected request payloads:
   * * Image Classification - Image in .JPEG, .GIF or .PNG format, image_bytes
   *                          up to 30MB.
   * * Image Object Detection - Image in .JPEG, .GIF or .PNG format, image_bytes
   *                            up to 30MB.
   * * Text Classification - TextSnippet, content up to 60,000 characters,
   *                         UTF-8 encoded.
   * * Text Extraction - TextSnippet, content up to 30,000 characters,
   *                     UTF-8 NFC encoded.
   * * Translation - TextSnippet, content up to 25,000 characters, UTF-8
   *                 encoded.
   * * Tables - Row, with column values matching the columns of the model,
   *            up to 5MB. Not available for FORECASTING
   *
   * [prediction_type][google.cloud.automl.v1beta1.TablesModelMetadata.prediction_type].
   * * Text Sentiment - TextSnippet, content up 500 characters, UTF-8
   *                     encoded.
   */
  predict(request: PredictRequest, context: CallContext & CallContextExt): Promise<DeepPartial<PredictResponse>>;
  /**
   * Perform a batch prediction. Unlike the online [Predict][google.cloud.automl.v1beta1.PredictionService.Predict], batch
   * prediction result won't be immediately available in the response. Instead,
   * a long running operation object is returned. User can poll the operation
   * result via [GetOperation][google.longrunning.Operations.GetOperation]
   * method. Once the operation is done, [BatchPredictResult][google.cloud.automl.v1beta1.BatchPredictResult] is returned in
   * the [response][google.longrunning.Operation.response] field.
   * Available for following ML problems:
   * * Image Classification
   * * Image Object Detection
   * * Video Classification
   * * Video Object Tracking * Text Extraction
   * * Tables
   */
  batchPredict(request: BatchPredictRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
}

export interface PredictionServiceClient<CallOptionsExt = {}> {
  /**
   * Perform an online prediction. The prediction result will be directly
   * returned in the response.
   * Available for following ML problems, and their expected request payloads:
   * * Image Classification - Image in .JPEG, .GIF or .PNG format, image_bytes
   *                          up to 30MB.
   * * Image Object Detection - Image in .JPEG, .GIF or .PNG format, image_bytes
   *                            up to 30MB.
   * * Text Classification - TextSnippet, content up to 60,000 characters,
   *                         UTF-8 encoded.
   * * Text Extraction - TextSnippet, content up to 30,000 characters,
   *                     UTF-8 NFC encoded.
   * * Translation - TextSnippet, content up to 25,000 characters, UTF-8
   *                 encoded.
   * * Tables - Row, with column values matching the columns of the model,
   *            up to 5MB. Not available for FORECASTING
   *
   * [prediction_type][google.cloud.automl.v1beta1.TablesModelMetadata.prediction_type].
   * * Text Sentiment - TextSnippet, content up 500 characters, UTF-8
   *                     encoded.
   */
  predict(request: DeepPartial<PredictRequest>, options?: CallOptions & CallOptionsExt): Promise<PredictResponse>;
  /**
   * Perform a batch prediction. Unlike the online [Predict][google.cloud.automl.v1beta1.PredictionService.Predict], batch
   * prediction result won't be immediately available in the response. Instead,
   * a long running operation object is returned. User can poll the operation
   * result via [GetOperation][google.longrunning.Operations.GetOperation]
   * method. Once the operation is done, [BatchPredictResult][google.cloud.automl.v1beta1.BatchPredictResult] is returned in
   * the [response][google.longrunning.Operation.response] field.
   * Available for following ML problems:
   * * Image Classification
   * * Image Object Detection
   * * Video Classification
   * * Video Object Tracking * Text Extraction
   * * Tables
   */
  batchPredict(request: DeepPartial<BatchPredictRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
