// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/automl/v1beta1/image.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { ClassificationType, classificationTypeFromJSON, classificationTypeToJSON } from "./classification.js";

export const protobufPackage = "google.cloud.automl.v1beta1";

/** Dataset metadata that is specific to image classification. */
export interface ImageClassificationDatasetMetadata {
  /** Required. Type of the classification problem. */
  classificationType: ClassificationType;
}

/** Dataset metadata specific to image object detection. */
export interface ImageObjectDetectionDatasetMetadata {
}

/** Model metadata for image classification. */
export interface ImageClassificationModelMetadata {
  /**
   * Optional. The ID of the `base` model. If it is specified, the new model
   * will be created based on the `base` model. Otherwise, the new model will be
   * created from scratch. The `base` model must be in the same
   * `project` and `location` as the new model to create, and have the same
   * `model_type`.
   */
  baseModelId: string;
  /**
   * Required. The train budget of creating this model, expressed in hours. The
   * actual `train_cost` will be equal or less than this value.
   */
  trainBudget: Long;
  /**
   * Output only. The actual train cost of creating this model, expressed in
   * hours. If this model is created from a `base` model, the train cost used
   * to create the `base` model are not included.
   */
  trainCost: Long;
  /**
   * Output only. The reason that this create model operation stopped,
   * e.g. `BUDGET_REACHED`, `MODEL_CONVERGED`.
   */
  stopReason: string;
  /**
   * Optional. Type of the model. The available values are:
   * *   `cloud` - Model to be used via prediction calls to AutoML API.
   *               This is the default value.
   * *   `mobile-low-latency-1` - A model that, in addition to providing
   *               prediction via AutoML API, can also be exported (see
   *               [AutoMl.ExportModel][google.cloud.automl.v1beta1.AutoMl.ExportModel]) and used on a mobile or edge device
   *               with TensorFlow afterwards. Expected to have low latency, but
   *               may have lower prediction quality than other models.
   * *   `mobile-versatile-1` - A model that, in addition to providing
   *               prediction via AutoML API, can also be exported (see
   *               [AutoMl.ExportModel][google.cloud.automl.v1beta1.AutoMl.ExportModel]) and used on a mobile or edge device
   *               with TensorFlow afterwards.
   * *   `mobile-high-accuracy-1` - A model that, in addition to providing
   *               prediction via AutoML API, can also be exported (see
   *               [AutoMl.ExportModel][google.cloud.automl.v1beta1.AutoMl.ExportModel]) and used on a mobile or edge device
   *               with TensorFlow afterwards.  Expected to have a higher
   *               latency, but should also have a higher prediction quality
   *               than other models.
   * *   `mobile-core-ml-low-latency-1` - A model that, in addition to providing
   *               prediction via AutoML API, can also be exported (see
   *               [AutoMl.ExportModel][google.cloud.automl.v1beta1.AutoMl.ExportModel]) and used on a mobile device with Core
   *               ML afterwards. Expected to have low latency, but may have
   *               lower prediction quality than other models.
   * *   `mobile-core-ml-versatile-1` - A model that, in addition to providing
   *               prediction via AutoML API, can also be exported (see
   *               [AutoMl.ExportModel][google.cloud.automl.v1beta1.AutoMl.ExportModel]) and used on a mobile device with Core
   *               ML afterwards.
   * *   `mobile-core-ml-high-accuracy-1` - A model that, in addition to
   *               providing prediction via AutoML API, can also be exported
   *               (see [AutoMl.ExportModel][google.cloud.automl.v1beta1.AutoMl.ExportModel]) and used on a mobile device with
   *               Core ML afterwards.  Expected to have a higher latency, but
   *               should also have a higher prediction quality than other
   *               models.
   */
  modelType: string;
  /**
   * Output only. An approximate number of online prediction QPS that can
   * be supported by this model per each node on which it is deployed.
   */
  nodeQps: number;
  /**
   * Output only. The number of nodes this model is deployed on. A node is an
   * abstraction of a machine resource, which can handle online prediction QPS
   * as given in the node_qps field.
   */
  nodeCount: Long;
}

/** Model metadata specific to image object detection. */
export interface ImageObjectDetectionModelMetadata {
  /**
   * Optional. Type of the model. The available values are:
   * *   `cloud-high-accuracy-1` - (default) A model to be used via prediction
   *               calls to AutoML API. Expected to have a higher latency, but
   *               should also have a higher prediction quality than other
   *               models.
   * *   `cloud-low-latency-1` -  A model to be used via prediction
   *               calls to AutoML API. Expected to have low latency, but may
   *               have lower prediction quality than other models.
   * *   `mobile-low-latency-1` - A model that, in addition to providing
   *               prediction via AutoML API, can also be exported (see
   *               [AutoMl.ExportModel][google.cloud.automl.v1beta1.AutoMl.ExportModel]) and used on a mobile or edge device
   *               with TensorFlow afterwards. Expected to have low latency, but
   *               may have lower prediction quality than other models.
   * *   `mobile-versatile-1` - A model that, in addition to providing
   *               prediction via AutoML API, can also be exported (see
   *               [AutoMl.ExportModel][google.cloud.automl.v1beta1.AutoMl.ExportModel]) and used on a mobile or edge device
   *               with TensorFlow afterwards.
   * *   `mobile-high-accuracy-1` - A model that, in addition to providing
   *               prediction via AutoML API, can also be exported (see
   *               [AutoMl.ExportModel][google.cloud.automl.v1beta1.AutoMl.ExportModel]) and used on a mobile or edge device
   *               with TensorFlow afterwards.  Expected to have a higher
   *               latency, but should also have a higher prediction quality
   *               than other models.
   */
  modelType: string;
  /**
   * Output only. The number of nodes this model is deployed on. A node is an
   * abstraction of a machine resource, which can handle online prediction QPS
   * as given in the qps_per_node field.
   */
  nodeCount: Long;
  /**
   * Output only. An approximate number of online prediction QPS that can
   * be supported by this model per each node on which it is deployed.
   */
  nodeQps: number;
  /**
   * Output only. The reason that this create model operation stopped,
   * e.g. `BUDGET_REACHED`, `MODEL_CONVERGED`.
   */
  stopReason: string;
  /**
   * The train budget of creating this model, expressed in milli node
   * hours i.e. 1,000 value in this field means 1 node hour. The actual
   * `train_cost` will be equal or less than this value. If further model
   * training ceases to provide any improvements, it will stop without using
   * full budget and the stop_reason will be `MODEL_CONVERGED`.
   * Note, node_hour  = actual_hour * number_of_nodes_invovled.
   * For model type `cloud-high-accuracy-1`(default) and `cloud-low-latency-1`,
   * the train budget must be between 20,000 and 900,000 milli node hours,
   * inclusive. The default value is 216, 000 which represents one day in
   * wall time.
   * For model type `mobile-low-latency-1`, `mobile-versatile-1`,
   * `mobile-high-accuracy-1`, `mobile-core-ml-low-latency-1`,
   * `mobile-core-ml-versatile-1`, `mobile-core-ml-high-accuracy-1`, the train
   * budget must be between 1,000 and 100,000 milli node hours, inclusive.
   * The default value is 24, 000 which represents one day in wall time.
   */
  trainBudgetMilliNodeHours: Long;
  /**
   * Output only. The actual train cost of creating this model, expressed in
   * milli node hours, i.e. 1,000 value in this field means 1 node hour.
   * Guaranteed to not exceed the train budget.
   */
  trainCostMilliNodeHours: Long;
}

/** Model deployment metadata specific to Image Classification. */
export interface ImageClassificationModelDeploymentMetadata {
  /**
   * Input only. The number of nodes to deploy the model on. A node is an
   * abstraction of a machine resource, which can handle online prediction QPS
   * as given in the model's
   *
   * [node_qps][google.cloud.automl.v1beta1.ImageClassificationModelMetadata.node_qps].
   * Must be between 1 and 100, inclusive on both ends.
   */
  nodeCount: Long;
}

/** Model deployment metadata specific to Image Object Detection. */
export interface ImageObjectDetectionModelDeploymentMetadata {
  /**
   * Input only. The number of nodes to deploy the model on. A node is an
   * abstraction of a machine resource, which can handle online prediction QPS
   * as given in the model's
   *
   * [qps_per_node][google.cloud.automl.v1beta1.ImageObjectDetectionModelMetadata.qps_per_node].
   * Must be between 1 and 100, inclusive on both ends.
   */
  nodeCount: Long;
}

function createBaseImageClassificationDatasetMetadata(): ImageClassificationDatasetMetadata {
  return { classificationType: 0 };
}

export const ImageClassificationDatasetMetadata: MessageFns<ImageClassificationDatasetMetadata> = {
  encode(message: ImageClassificationDatasetMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.classificationType !== 0) {
      writer.uint32(8).int32(message.classificationType);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImageClassificationDatasetMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImageClassificationDatasetMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.classificationType = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImageClassificationDatasetMetadata {
    return {
      classificationType: isSet(object.classificationType) ? classificationTypeFromJSON(object.classificationType) : 0,
    };
  },

  toJSON(message: ImageClassificationDatasetMetadata): unknown {
    const obj: any = {};
    if (message.classificationType !== 0) {
      obj.classificationType = classificationTypeToJSON(message.classificationType);
    }
    return obj;
  },

  create(base?: DeepPartial<ImageClassificationDatasetMetadata>): ImageClassificationDatasetMetadata {
    return ImageClassificationDatasetMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImageClassificationDatasetMetadata>): ImageClassificationDatasetMetadata {
    const message = createBaseImageClassificationDatasetMetadata();
    message.classificationType = object.classificationType ?? 0;
    return message;
  },
};

function createBaseImageObjectDetectionDatasetMetadata(): ImageObjectDetectionDatasetMetadata {
  return {};
}

export const ImageObjectDetectionDatasetMetadata: MessageFns<ImageObjectDetectionDatasetMetadata> = {
  encode(_: ImageObjectDetectionDatasetMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImageObjectDetectionDatasetMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImageObjectDetectionDatasetMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): ImageObjectDetectionDatasetMetadata {
    return {};
  },

  toJSON(_: ImageObjectDetectionDatasetMetadata): unknown {
    const obj: any = {};
    return obj;
  },

  create(base?: DeepPartial<ImageObjectDetectionDatasetMetadata>): ImageObjectDetectionDatasetMetadata {
    return ImageObjectDetectionDatasetMetadata.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<ImageObjectDetectionDatasetMetadata>): ImageObjectDetectionDatasetMetadata {
    const message = createBaseImageObjectDetectionDatasetMetadata();
    return message;
  },
};

function createBaseImageClassificationModelMetadata(): ImageClassificationModelMetadata {
  return {
    baseModelId: "",
    trainBudget: Long.ZERO,
    trainCost: Long.ZERO,
    stopReason: "",
    modelType: "",
    nodeQps: 0,
    nodeCount: Long.ZERO,
  };
}

export const ImageClassificationModelMetadata: MessageFns<ImageClassificationModelMetadata> = {
  encode(message: ImageClassificationModelMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.baseModelId !== "") {
      writer.uint32(10).string(message.baseModelId);
    }
    if (!message.trainBudget.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.trainBudget.toString());
    }
    if (!message.trainCost.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.trainCost.toString());
    }
    if (message.stopReason !== "") {
      writer.uint32(42).string(message.stopReason);
    }
    if (message.modelType !== "") {
      writer.uint32(58).string(message.modelType);
    }
    if (message.nodeQps !== 0) {
      writer.uint32(105).double(message.nodeQps);
    }
    if (!message.nodeCount.equals(Long.ZERO)) {
      writer.uint32(112).int64(message.nodeCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImageClassificationModelMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImageClassificationModelMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.baseModelId = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.trainBudget = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.trainCost = Long.fromString(reader.int64().toString());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.stopReason = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.modelType = reader.string();
          continue;
        case 13:
          if (tag !== 105) {
            break;
          }

          message.nodeQps = reader.double();
          continue;
        case 14:
          if (tag !== 112) {
            break;
          }

          message.nodeCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImageClassificationModelMetadata {
    return {
      baseModelId: isSet(object.baseModelId) ? globalThis.String(object.baseModelId) : "",
      trainBudget: isSet(object.trainBudget) ? Long.fromValue(object.trainBudget) : Long.ZERO,
      trainCost: isSet(object.trainCost) ? Long.fromValue(object.trainCost) : Long.ZERO,
      stopReason: isSet(object.stopReason) ? globalThis.String(object.stopReason) : "",
      modelType: isSet(object.modelType) ? globalThis.String(object.modelType) : "",
      nodeQps: isSet(object.nodeQps) ? globalThis.Number(object.nodeQps) : 0,
      nodeCount: isSet(object.nodeCount) ? Long.fromValue(object.nodeCount) : Long.ZERO,
    };
  },

  toJSON(message: ImageClassificationModelMetadata): unknown {
    const obj: any = {};
    if (message.baseModelId !== "") {
      obj.baseModelId = message.baseModelId;
    }
    if (!message.trainBudget.equals(Long.ZERO)) {
      obj.trainBudget = (message.trainBudget || Long.ZERO).toString();
    }
    if (!message.trainCost.equals(Long.ZERO)) {
      obj.trainCost = (message.trainCost || Long.ZERO).toString();
    }
    if (message.stopReason !== "") {
      obj.stopReason = message.stopReason;
    }
    if (message.modelType !== "") {
      obj.modelType = message.modelType;
    }
    if (message.nodeQps !== 0) {
      obj.nodeQps = message.nodeQps;
    }
    if (!message.nodeCount.equals(Long.ZERO)) {
      obj.nodeCount = (message.nodeCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ImageClassificationModelMetadata>): ImageClassificationModelMetadata {
    return ImageClassificationModelMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImageClassificationModelMetadata>): ImageClassificationModelMetadata {
    const message = createBaseImageClassificationModelMetadata();
    message.baseModelId = object.baseModelId ?? "";
    message.trainBudget = (object.trainBudget !== undefined && object.trainBudget !== null)
      ? Long.fromValue(object.trainBudget)
      : Long.ZERO;
    message.trainCost = (object.trainCost !== undefined && object.trainCost !== null)
      ? Long.fromValue(object.trainCost)
      : Long.ZERO;
    message.stopReason = object.stopReason ?? "";
    message.modelType = object.modelType ?? "";
    message.nodeQps = object.nodeQps ?? 0;
    message.nodeCount = (object.nodeCount !== undefined && object.nodeCount !== null)
      ? Long.fromValue(object.nodeCount)
      : Long.ZERO;
    return message;
  },
};

function createBaseImageObjectDetectionModelMetadata(): ImageObjectDetectionModelMetadata {
  return {
    modelType: "",
    nodeCount: Long.ZERO,
    nodeQps: 0,
    stopReason: "",
    trainBudgetMilliNodeHours: Long.ZERO,
    trainCostMilliNodeHours: Long.ZERO,
  };
}

export const ImageObjectDetectionModelMetadata: MessageFns<ImageObjectDetectionModelMetadata> = {
  encode(message: ImageObjectDetectionModelMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.modelType !== "") {
      writer.uint32(10).string(message.modelType);
    }
    if (!message.nodeCount.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.nodeCount.toString());
    }
    if (message.nodeQps !== 0) {
      writer.uint32(33).double(message.nodeQps);
    }
    if (message.stopReason !== "") {
      writer.uint32(42).string(message.stopReason);
    }
    if (!message.trainBudgetMilliNodeHours.equals(Long.ZERO)) {
      writer.uint32(48).int64(message.trainBudgetMilliNodeHours.toString());
    }
    if (!message.trainCostMilliNodeHours.equals(Long.ZERO)) {
      writer.uint32(56).int64(message.trainCostMilliNodeHours.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImageObjectDetectionModelMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImageObjectDetectionModelMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.modelType = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.nodeCount = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 33) {
            break;
          }

          message.nodeQps = reader.double();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.stopReason = reader.string();
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.trainBudgetMilliNodeHours = Long.fromString(reader.int64().toString());
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.trainCostMilliNodeHours = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImageObjectDetectionModelMetadata {
    return {
      modelType: isSet(object.modelType) ? globalThis.String(object.modelType) : "",
      nodeCount: isSet(object.nodeCount) ? Long.fromValue(object.nodeCount) : Long.ZERO,
      nodeQps: isSet(object.nodeQps) ? globalThis.Number(object.nodeQps) : 0,
      stopReason: isSet(object.stopReason) ? globalThis.String(object.stopReason) : "",
      trainBudgetMilliNodeHours: isSet(object.trainBudgetMilliNodeHours)
        ? Long.fromValue(object.trainBudgetMilliNodeHours)
        : Long.ZERO,
      trainCostMilliNodeHours: isSet(object.trainCostMilliNodeHours)
        ? Long.fromValue(object.trainCostMilliNodeHours)
        : Long.ZERO,
    };
  },

  toJSON(message: ImageObjectDetectionModelMetadata): unknown {
    const obj: any = {};
    if (message.modelType !== "") {
      obj.modelType = message.modelType;
    }
    if (!message.nodeCount.equals(Long.ZERO)) {
      obj.nodeCount = (message.nodeCount || Long.ZERO).toString();
    }
    if (message.nodeQps !== 0) {
      obj.nodeQps = message.nodeQps;
    }
    if (message.stopReason !== "") {
      obj.stopReason = message.stopReason;
    }
    if (!message.trainBudgetMilliNodeHours.equals(Long.ZERO)) {
      obj.trainBudgetMilliNodeHours = (message.trainBudgetMilliNodeHours || Long.ZERO).toString();
    }
    if (!message.trainCostMilliNodeHours.equals(Long.ZERO)) {
      obj.trainCostMilliNodeHours = (message.trainCostMilliNodeHours || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ImageObjectDetectionModelMetadata>): ImageObjectDetectionModelMetadata {
    return ImageObjectDetectionModelMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImageObjectDetectionModelMetadata>): ImageObjectDetectionModelMetadata {
    const message = createBaseImageObjectDetectionModelMetadata();
    message.modelType = object.modelType ?? "";
    message.nodeCount = (object.nodeCount !== undefined && object.nodeCount !== null)
      ? Long.fromValue(object.nodeCount)
      : Long.ZERO;
    message.nodeQps = object.nodeQps ?? 0;
    message.stopReason = object.stopReason ?? "";
    message.trainBudgetMilliNodeHours =
      (object.trainBudgetMilliNodeHours !== undefined && object.trainBudgetMilliNodeHours !== null)
        ? Long.fromValue(object.trainBudgetMilliNodeHours)
        : Long.ZERO;
    message.trainCostMilliNodeHours =
      (object.trainCostMilliNodeHours !== undefined && object.trainCostMilliNodeHours !== null)
        ? Long.fromValue(object.trainCostMilliNodeHours)
        : Long.ZERO;
    return message;
  },
};

function createBaseImageClassificationModelDeploymentMetadata(): ImageClassificationModelDeploymentMetadata {
  return { nodeCount: Long.ZERO };
}

export const ImageClassificationModelDeploymentMetadata: MessageFns<ImageClassificationModelDeploymentMetadata> = {
  encode(message: ImageClassificationModelDeploymentMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.nodeCount.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.nodeCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImageClassificationModelDeploymentMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImageClassificationModelDeploymentMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.nodeCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImageClassificationModelDeploymentMetadata {
    return { nodeCount: isSet(object.nodeCount) ? Long.fromValue(object.nodeCount) : Long.ZERO };
  },

  toJSON(message: ImageClassificationModelDeploymentMetadata): unknown {
    const obj: any = {};
    if (!message.nodeCount.equals(Long.ZERO)) {
      obj.nodeCount = (message.nodeCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ImageClassificationModelDeploymentMetadata>): ImageClassificationModelDeploymentMetadata {
    return ImageClassificationModelDeploymentMetadata.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ImageClassificationModelDeploymentMetadata>,
  ): ImageClassificationModelDeploymentMetadata {
    const message = createBaseImageClassificationModelDeploymentMetadata();
    message.nodeCount = (object.nodeCount !== undefined && object.nodeCount !== null)
      ? Long.fromValue(object.nodeCount)
      : Long.ZERO;
    return message;
  },
};

function createBaseImageObjectDetectionModelDeploymentMetadata(): ImageObjectDetectionModelDeploymentMetadata {
  return { nodeCount: Long.ZERO };
}

export const ImageObjectDetectionModelDeploymentMetadata: MessageFns<ImageObjectDetectionModelDeploymentMetadata> = {
  encode(
    message: ImageObjectDetectionModelDeploymentMetadata,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (!message.nodeCount.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.nodeCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImageObjectDetectionModelDeploymentMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImageObjectDetectionModelDeploymentMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.nodeCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImageObjectDetectionModelDeploymentMetadata {
    return { nodeCount: isSet(object.nodeCount) ? Long.fromValue(object.nodeCount) : Long.ZERO };
  },

  toJSON(message: ImageObjectDetectionModelDeploymentMetadata): unknown {
    const obj: any = {};
    if (!message.nodeCount.equals(Long.ZERO)) {
      obj.nodeCount = (message.nodeCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ImageObjectDetectionModelDeploymentMetadata>): ImageObjectDetectionModelDeploymentMetadata {
    return ImageObjectDetectionModelDeploymentMetadata.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ImageObjectDetectionModelDeploymentMetadata>,
  ): ImageObjectDetectionModelDeploymentMetadata {
    const message = createBaseImageObjectDetectionModelDeploymentMetadata();
    message.nodeCount = (object.nodeCount !== undefined && object.nodeCount !== null)
      ? Long.fromValue(object.nodeCount)
      : Long.ZERO;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
