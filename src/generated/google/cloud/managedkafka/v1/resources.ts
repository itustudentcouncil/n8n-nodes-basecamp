// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/managedkafka/v1/resources.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Timestamp } from "../../../protobuf/timestamp.js";

export const protobufPackage = "google.cloud.managedkafka.v1";

/** An Apache Kafka cluster deployed in a location. */
export interface Cluster {
  /**
   * Required. Configuration properties for a Kafka cluster deployed to Google
   * Cloud Platform.
   */
  gcpConfig?:
    | GcpConfig
    | undefined;
  /**
   * Identifier. The name of the cluster. Structured like:
   * projects/{project_number}/locations/{location}/clusters/{cluster_id}
   */
  name: string;
  /** Output only. The time when the cluster was created. */
  createTime:
    | Date
    | undefined;
  /** Output only. The time when the cluster was last updated. */
  updateTime:
    | Date
    | undefined;
  /** Optional. Labels as key value pairs. */
  labels: { [key: string]: string };
  /** Required. Capacity configuration for the Kafka cluster. */
  capacityConfig:
    | CapacityConfig
    | undefined;
  /** Optional. Rebalance configuration for the Kafka cluster. */
  rebalanceConfig:
    | RebalanceConfig
    | undefined;
  /** Output only. The current state of the cluster. */
  state: Cluster_State;
}

/** The state of the cluster. */
export enum Cluster_State {
  /** STATE_UNSPECIFIED - A state was not specified. */
  STATE_UNSPECIFIED = 0,
  /** CREATING - The cluster is being created. */
  CREATING = 1,
  /** ACTIVE - The cluster is active. */
  ACTIVE = 2,
  /** DELETING - The cluster is being deleted. */
  DELETING = 3,
  UNRECOGNIZED = -1,
}

export function cluster_StateFromJSON(object: any): Cluster_State {
  switch (object) {
    case 0:
    case "STATE_UNSPECIFIED":
      return Cluster_State.STATE_UNSPECIFIED;
    case 1:
    case "CREATING":
      return Cluster_State.CREATING;
    case 2:
    case "ACTIVE":
      return Cluster_State.ACTIVE;
    case 3:
    case "DELETING":
      return Cluster_State.DELETING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Cluster_State.UNRECOGNIZED;
  }
}

export function cluster_StateToJSON(object: Cluster_State): string {
  switch (object) {
    case Cluster_State.STATE_UNSPECIFIED:
      return "STATE_UNSPECIFIED";
    case Cluster_State.CREATING:
      return "CREATING";
    case Cluster_State.ACTIVE:
      return "ACTIVE";
    case Cluster_State.DELETING:
      return "DELETING";
    case Cluster_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export interface Cluster_LabelsEntry {
  key: string;
  value: string;
}

/** A capacity configuration of a Kafka cluster. */
export interface CapacityConfig {
  /** Required. The number of vCPUs to provision for the cluster. Minimum: 3. */
  vcpuCount: Long;
  /**
   * Required. The memory to provision for the cluster in bytes.
   * The CPU:memory ratio (vCPU:GiB) must be between 1:1 and 1:8.
   * Minimum: 3221225472 (3 GiB).
   */
  memoryBytes: Long;
}

/** Defines rebalancing behavior of a Kafka cluster. */
export interface RebalanceConfig {
  /**
   * Optional. The rebalance behavior for the cluster.
   * When not specified, defaults to `NO_REBALANCE`.
   */
  mode: RebalanceConfig_Mode;
}

/** The partition rebalance mode for the cluster. */
export enum RebalanceConfig_Mode {
  /** MODE_UNSPECIFIED - A mode was not specified. Do not use. */
  MODE_UNSPECIFIED = 0,
  /** NO_REBALANCE - Do not rebalance automatically. */
  NO_REBALANCE = 1,
  /**
   * AUTO_REBALANCE_ON_SCALE_UP - Automatically rebalance topic partitions among brokers when the
   * cluster is scaled up.
   */
  AUTO_REBALANCE_ON_SCALE_UP = 2,
  UNRECOGNIZED = -1,
}

export function rebalanceConfig_ModeFromJSON(object: any): RebalanceConfig_Mode {
  switch (object) {
    case 0:
    case "MODE_UNSPECIFIED":
      return RebalanceConfig_Mode.MODE_UNSPECIFIED;
    case 1:
    case "NO_REBALANCE":
      return RebalanceConfig_Mode.NO_REBALANCE;
    case 2:
    case "AUTO_REBALANCE_ON_SCALE_UP":
      return RebalanceConfig_Mode.AUTO_REBALANCE_ON_SCALE_UP;
    case -1:
    case "UNRECOGNIZED":
    default:
      return RebalanceConfig_Mode.UNRECOGNIZED;
  }
}

export function rebalanceConfig_ModeToJSON(object: RebalanceConfig_Mode): string {
  switch (object) {
    case RebalanceConfig_Mode.MODE_UNSPECIFIED:
      return "MODE_UNSPECIFIED";
    case RebalanceConfig_Mode.NO_REBALANCE:
      return "NO_REBALANCE";
    case RebalanceConfig_Mode.AUTO_REBALANCE_ON_SCALE_UP:
      return "AUTO_REBALANCE_ON_SCALE_UP";
    case RebalanceConfig_Mode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * The configuration of a Virtual Private Cloud (VPC) network that can access
 * the Kafka cluster.
 */
export interface NetworkConfig {
  /**
   * Required. Name of the VPC subnet in which to create Private Service Connect
   * (PSC) endpoints for the Kafka brokers and bootstrap address. Structured
   * like: projects/{project}/regions/{region}/subnetworks/{subnet_id}
   *
   * The subnet must be located in the same region as the Kafka cluster. The
   * project may differ. Multiple subnets from the same parent network must not
   * be specified.
   *
   * The CIDR range of the subnet must be within the IPv4 address ranges for
   * private networks, as specified in RFC 1918.
   */
  subnet: string;
}

/** The configuration of access to the Kafka cluster. */
export interface AccessConfig {
  /**
   * Required. Virtual Private Cloud (VPC) networks that must be granted direct
   * access to the Kafka cluster. Minimum of 1 network is required. Maximum 10
   * networks can be specified.
   */
  networkConfigs: NetworkConfig[];
}

/**
 * Configuration properties for a Kafka cluster deployed to Google Cloud
 * Platform.
 */
export interface GcpConfig {
  /** Required. Access configuration for the Kafka cluster. */
  accessConfig:
    | AccessConfig
    | undefined;
  /**
   * Optional. Immutable. The Cloud KMS Key name to use for encryption. The key
   * must be located in the same region as the cluster and cannot be changed.
   * Structured like:
   * projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}.
   */
  kmsKey: string;
}

/** A Kafka topic in a given cluster. */
export interface Topic {
  /**
   * Identifier. The name of the topic. The `topic` segment is used when
   * connecting directly to the cluster. Structured like:
   * projects/{project}/locations/{location}/clusters/{cluster}/topics/{topic}
   */
  name: string;
  /**
   * Required. The number of partitions this topic has. The partition count can
   * only be increased, not decreased. Please note that if partitions are
   * increased for a topic that has a key, the partitioning logic or the
   * ordering of the messages will be affected.
   */
  partitionCount: number;
  /**
   * Required. Immutable. The number of replicas of each partition. A
   * replication factor of 3 is recommended for high availability.
   */
  replicationFactor: number;
  /**
   * Optional. Configurations for the topic that are overridden from the cluster
   * defaults. The key of the map is a Kafka topic property name, for example:
   * `cleanup.policy`, `compression.type`.
   */
  configs: { [key: string]: string };
}

export interface Topic_ConfigsEntry {
  key: string;
  value: string;
}

/** Metadata for a consumer group corresponding to a specific topic. */
export interface ConsumerTopicMetadata {
  /**
   * Optional. Metadata for this consumer group and topic for all partition
   * indexes it has metadata for.
   */
  partitions: { [key: number]: ConsumerPartitionMetadata };
}

export interface ConsumerTopicMetadata_PartitionsEntry {
  key: number;
  value: ConsumerPartitionMetadata | undefined;
}

/** Metadata for a consumer group corresponding to a specific partition. */
export interface ConsumerPartitionMetadata {
  /**
   * Required. The current offset for this partition, or 0 if no offset has been
   * committed.
   */
  offset: Long;
  /**
   * Optional. The associated metadata for this partition, or empty if it does
   * not exist.
   */
  metadata: string;
}

/** A Kafka consumer group in a given cluster. */
export interface ConsumerGroup {
  /**
   * Identifier. The name of the consumer group. The `consumer_group` segment is
   * used when connecting directly to the cluster. Structured like:
   * projects/{project}/locations/{location}/clusters/{cluster}/consumerGroups/{consumer_group}
   */
  name: string;
  /**
   * Optional. Metadata for this consumer group for all topics it has metadata
   * for. The key of the map is a topic name, structured like:
   * projects/{project}/locations/{location}/clusters/{cluster}/topics/{topic}
   */
  topics: { [key: string]: ConsumerTopicMetadata };
}

export interface ConsumerGroup_TopicsEntry {
  key: string;
  value: ConsumerTopicMetadata | undefined;
}

/** Represents the metadata of the long-running operation. */
export interface OperationMetadata {
  /** Output only. The time the operation was created. */
  createTime:
    | Date
    | undefined;
  /** Output only. The time the operation finished running. */
  endTime:
    | Date
    | undefined;
  /** Output only. Server-defined resource path for the target of the operation. */
  target: string;
  /** Output only. Name of the verb executed by the operation. */
  verb: string;
  /** Output only. Human-readable status of the operation, if any. */
  statusMessage: string;
  /**
   * Output only. Identifies whether the user has requested cancellation
   * of the operation. Operations that have been cancelled successfully
   * have [Operation.error][] value with a
   * [google.rpc.Status.code][google.rpc.Status.code] of 1, corresponding to
   * `Code.CANCELLED`.
   */
  requestedCancellation: boolean;
  /** Output only. API version used to start the operation. */
  apiVersion: string;
}

function createBaseCluster(): Cluster {
  return {
    gcpConfig: undefined,
    name: "",
    createTime: undefined,
    updateTime: undefined,
    labels: {},
    capacityConfig: undefined,
    rebalanceConfig: undefined,
    state: 0,
  };
}

export const Cluster: MessageFns<Cluster> = {
  encode(message: Cluster, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcpConfig !== undefined) {
      GcpConfig.encode(message.gcpConfig, writer.uint32(74).fork()).join();
    }
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(18).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(26).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      Cluster_LabelsEntry.encode({ key: key as any, value }, writer.uint32(34).fork()).join();
    });
    if (message.capacityConfig !== undefined) {
      CapacityConfig.encode(message.capacityConfig, writer.uint32(42).fork()).join();
    }
    if (message.rebalanceConfig !== undefined) {
      RebalanceConfig.encode(message.rebalanceConfig, writer.uint32(66).fork()).join();
    }
    if (message.state !== 0) {
      writer.uint32(80).int32(message.state);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 9:
          if (tag !== 74) {
            break;
          }

          message.gcpConfig = GcpConfig.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          const entry4 = Cluster_LabelsEntry.decode(reader, reader.uint32());
          if (entry4.value !== undefined) {
            message.labels[entry4.key] = entry4.value;
          }
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.capacityConfig = CapacityConfig.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.rebalanceConfig = RebalanceConfig.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster {
    return {
      gcpConfig: isSet(object.gcpConfig) ? GcpConfig.fromJSON(object.gcpConfig) : undefined,
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      capacityConfig: isSet(object.capacityConfig) ? CapacityConfig.fromJSON(object.capacityConfig) : undefined,
      rebalanceConfig: isSet(object.rebalanceConfig) ? RebalanceConfig.fromJSON(object.rebalanceConfig) : undefined,
      state: isSet(object.state) ? cluster_StateFromJSON(object.state) : 0,
    };
  },

  toJSON(message: Cluster): unknown {
    const obj: any = {};
    if (message.gcpConfig !== undefined) {
      obj.gcpConfig = GcpConfig.toJSON(message.gcpConfig);
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.capacityConfig !== undefined) {
      obj.capacityConfig = CapacityConfig.toJSON(message.capacityConfig);
    }
    if (message.rebalanceConfig !== undefined) {
      obj.rebalanceConfig = RebalanceConfig.toJSON(message.rebalanceConfig);
    }
    if (message.state !== 0) {
      obj.state = cluster_StateToJSON(message.state);
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster>): Cluster {
    return Cluster.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster>): Cluster {
    const message = createBaseCluster();
    message.gcpConfig = (object.gcpConfig !== undefined && object.gcpConfig !== null)
      ? GcpConfig.fromPartial(object.gcpConfig)
      : undefined;
    message.name = object.name ?? "";
    message.createTime = object.createTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.capacityConfig = (object.capacityConfig !== undefined && object.capacityConfig !== null)
      ? CapacityConfig.fromPartial(object.capacityConfig)
      : undefined;
    message.rebalanceConfig = (object.rebalanceConfig !== undefined && object.rebalanceConfig !== null)
      ? RebalanceConfig.fromPartial(object.rebalanceConfig)
      : undefined;
    message.state = object.state ?? 0;
    return message;
  },
};

function createBaseCluster_LabelsEntry(): Cluster_LabelsEntry {
  return { key: "", value: "" };
}

export const Cluster_LabelsEntry: MessageFns<Cluster_LabelsEntry> = {
  encode(message: Cluster_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Cluster_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster_LabelsEntry>): Cluster_LabelsEntry {
    return Cluster_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster_LabelsEntry>): Cluster_LabelsEntry {
    const message = createBaseCluster_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseCapacityConfig(): CapacityConfig {
  return { vcpuCount: Long.ZERO, memoryBytes: Long.ZERO };
}

export const CapacityConfig: MessageFns<CapacityConfig> = {
  encode(message: CapacityConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.vcpuCount.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.vcpuCount.toString());
    }
    if (!message.memoryBytes.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.memoryBytes.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CapacityConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCapacityConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.vcpuCount = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.memoryBytes = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CapacityConfig {
    return {
      vcpuCount: isSet(object.vcpuCount) ? Long.fromValue(object.vcpuCount) : Long.ZERO,
      memoryBytes: isSet(object.memoryBytes) ? Long.fromValue(object.memoryBytes) : Long.ZERO,
    };
  },

  toJSON(message: CapacityConfig): unknown {
    const obj: any = {};
    if (!message.vcpuCount.equals(Long.ZERO)) {
      obj.vcpuCount = (message.vcpuCount || Long.ZERO).toString();
    }
    if (!message.memoryBytes.equals(Long.ZERO)) {
      obj.memoryBytes = (message.memoryBytes || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<CapacityConfig>): CapacityConfig {
    return CapacityConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CapacityConfig>): CapacityConfig {
    const message = createBaseCapacityConfig();
    message.vcpuCount = (object.vcpuCount !== undefined && object.vcpuCount !== null)
      ? Long.fromValue(object.vcpuCount)
      : Long.ZERO;
    message.memoryBytes = (object.memoryBytes !== undefined && object.memoryBytes !== null)
      ? Long.fromValue(object.memoryBytes)
      : Long.ZERO;
    return message;
  },
};

function createBaseRebalanceConfig(): RebalanceConfig {
  return { mode: 0 };
}

export const RebalanceConfig: MessageFns<RebalanceConfig> = {
  encode(message: RebalanceConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.mode !== 0) {
      writer.uint32(8).int32(message.mode);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RebalanceConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRebalanceConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.mode = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RebalanceConfig {
    return { mode: isSet(object.mode) ? rebalanceConfig_ModeFromJSON(object.mode) : 0 };
  },

  toJSON(message: RebalanceConfig): unknown {
    const obj: any = {};
    if (message.mode !== 0) {
      obj.mode = rebalanceConfig_ModeToJSON(message.mode);
    }
    return obj;
  },

  create(base?: DeepPartial<RebalanceConfig>): RebalanceConfig {
    return RebalanceConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RebalanceConfig>): RebalanceConfig {
    const message = createBaseRebalanceConfig();
    message.mode = object.mode ?? 0;
    return message;
  },
};

function createBaseNetworkConfig(): NetworkConfig {
  return { subnet: "" };
}

export const NetworkConfig: MessageFns<NetworkConfig> = {
  encode(message: NetworkConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.subnet !== "") {
      writer.uint32(18).string(message.subnet);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NetworkConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNetworkConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.subnet = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NetworkConfig {
    return { subnet: isSet(object.subnet) ? globalThis.String(object.subnet) : "" };
  },

  toJSON(message: NetworkConfig): unknown {
    const obj: any = {};
    if (message.subnet !== "") {
      obj.subnet = message.subnet;
    }
    return obj;
  },

  create(base?: DeepPartial<NetworkConfig>): NetworkConfig {
    return NetworkConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<NetworkConfig>): NetworkConfig {
    const message = createBaseNetworkConfig();
    message.subnet = object.subnet ?? "";
    return message;
  },
};

function createBaseAccessConfig(): AccessConfig {
  return { networkConfigs: [] };
}

export const AccessConfig: MessageFns<AccessConfig> = {
  encode(message: AccessConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.networkConfigs) {
      NetworkConfig.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AccessConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAccessConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.networkConfigs.push(NetworkConfig.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AccessConfig {
    return {
      networkConfigs: globalThis.Array.isArray(object?.networkConfigs)
        ? object.networkConfigs.map((e: any) => NetworkConfig.fromJSON(e))
        : [],
    };
  },

  toJSON(message: AccessConfig): unknown {
    const obj: any = {};
    if (message.networkConfigs?.length) {
      obj.networkConfigs = message.networkConfigs.map((e) => NetworkConfig.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<AccessConfig>): AccessConfig {
    return AccessConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AccessConfig>): AccessConfig {
    const message = createBaseAccessConfig();
    message.networkConfigs = object.networkConfigs?.map((e) => NetworkConfig.fromPartial(e)) || [];
    return message;
  },
};

function createBaseGcpConfig(): GcpConfig {
  return { accessConfig: undefined, kmsKey: "" };
}

export const GcpConfig: MessageFns<GcpConfig> = {
  encode(message: GcpConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.accessConfig !== undefined) {
      AccessConfig.encode(message.accessConfig, writer.uint32(26).fork()).join();
    }
    if (message.kmsKey !== "") {
      writer.uint32(18).string(message.kmsKey);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GcpConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGcpConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 26) {
            break;
          }

          message.accessConfig = AccessConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.kmsKey = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GcpConfig {
    return {
      accessConfig: isSet(object.accessConfig) ? AccessConfig.fromJSON(object.accessConfig) : undefined,
      kmsKey: isSet(object.kmsKey) ? globalThis.String(object.kmsKey) : "",
    };
  },

  toJSON(message: GcpConfig): unknown {
    const obj: any = {};
    if (message.accessConfig !== undefined) {
      obj.accessConfig = AccessConfig.toJSON(message.accessConfig);
    }
    if (message.kmsKey !== "") {
      obj.kmsKey = message.kmsKey;
    }
    return obj;
  },

  create(base?: DeepPartial<GcpConfig>): GcpConfig {
    return GcpConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GcpConfig>): GcpConfig {
    const message = createBaseGcpConfig();
    message.accessConfig = (object.accessConfig !== undefined && object.accessConfig !== null)
      ? AccessConfig.fromPartial(object.accessConfig)
      : undefined;
    message.kmsKey = object.kmsKey ?? "";
    return message;
  },
};

function createBaseTopic(): Topic {
  return { name: "", partitionCount: 0, replicationFactor: 0, configs: {} };
}

export const Topic: MessageFns<Topic> = {
  encode(message: Topic, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.partitionCount !== 0) {
      writer.uint32(16).int32(message.partitionCount);
    }
    if (message.replicationFactor !== 0) {
      writer.uint32(24).int32(message.replicationFactor);
    }
    Object.entries(message.configs).forEach(([key, value]) => {
      Topic_ConfigsEntry.encode({ key: key as any, value }, writer.uint32(34).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Topic {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTopic();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.partitionCount = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.replicationFactor = reader.int32();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          const entry4 = Topic_ConfigsEntry.decode(reader, reader.uint32());
          if (entry4.value !== undefined) {
            message.configs[entry4.key] = entry4.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Topic {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      partitionCount: isSet(object.partitionCount) ? globalThis.Number(object.partitionCount) : 0,
      replicationFactor: isSet(object.replicationFactor) ? globalThis.Number(object.replicationFactor) : 0,
      configs: isObject(object.configs)
        ? Object.entries(object.configs).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: Topic): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.partitionCount !== 0) {
      obj.partitionCount = Math.round(message.partitionCount);
    }
    if (message.replicationFactor !== 0) {
      obj.replicationFactor = Math.round(message.replicationFactor);
    }
    if (message.configs) {
      const entries = Object.entries(message.configs);
      if (entries.length > 0) {
        obj.configs = {};
        entries.forEach(([k, v]) => {
          obj.configs[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<Topic>): Topic {
    return Topic.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Topic>): Topic {
    const message = createBaseTopic();
    message.name = object.name ?? "";
    message.partitionCount = object.partitionCount ?? 0;
    message.replicationFactor = object.replicationFactor ?? 0;
    message.configs = Object.entries(object.configs ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseTopic_ConfigsEntry(): Topic_ConfigsEntry {
  return { key: "", value: "" };
}

export const Topic_ConfigsEntry: MessageFns<Topic_ConfigsEntry> = {
  encode(message: Topic_ConfigsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Topic_ConfigsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTopic_ConfigsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Topic_ConfigsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Topic_ConfigsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Topic_ConfigsEntry>): Topic_ConfigsEntry {
    return Topic_ConfigsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Topic_ConfigsEntry>): Topic_ConfigsEntry {
    const message = createBaseTopic_ConfigsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseConsumerTopicMetadata(): ConsumerTopicMetadata {
  return { partitions: {} };
}

export const ConsumerTopicMetadata: MessageFns<ConsumerTopicMetadata> = {
  encode(message: ConsumerTopicMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.partitions).forEach(([key, value]) => {
      ConsumerTopicMetadata_PartitionsEntry.encode({ key: key as any, value }, writer.uint32(10).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ConsumerTopicMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseConsumerTopicMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          const entry1 = ConsumerTopicMetadata_PartitionsEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.partitions[entry1.key] = entry1.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ConsumerTopicMetadata {
    return {
      partitions: isObject(object.partitions)
        ? Object.entries(object.partitions).reduce<{ [key: number]: ConsumerPartitionMetadata }>(
          (acc, [key, value]) => {
            acc[globalThis.Number(key)] = ConsumerPartitionMetadata.fromJSON(value);
            return acc;
          },
          {},
        )
        : {},
    };
  },

  toJSON(message: ConsumerTopicMetadata): unknown {
    const obj: any = {};
    if (message.partitions) {
      const entries = Object.entries(message.partitions);
      if (entries.length > 0) {
        obj.partitions = {};
        entries.forEach(([k, v]) => {
          obj.partitions[k] = ConsumerPartitionMetadata.toJSON(v);
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<ConsumerTopicMetadata>): ConsumerTopicMetadata {
    return ConsumerTopicMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ConsumerTopicMetadata>): ConsumerTopicMetadata {
    const message = createBaseConsumerTopicMetadata();
    message.partitions = Object.entries(object.partitions ?? {}).reduce<{ [key: number]: ConsumerPartitionMetadata }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[globalThis.Number(key)] = ConsumerPartitionMetadata.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseConsumerTopicMetadata_PartitionsEntry(): ConsumerTopicMetadata_PartitionsEntry {
  return { key: 0, value: undefined };
}

export const ConsumerTopicMetadata_PartitionsEntry: MessageFns<ConsumerTopicMetadata_PartitionsEntry> = {
  encode(message: ConsumerTopicMetadata_PartitionsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== 0) {
      writer.uint32(8).int32(message.key);
    }
    if (message.value !== undefined) {
      ConsumerPartitionMetadata.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ConsumerTopicMetadata_PartitionsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseConsumerTopicMetadata_PartitionsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.key = reader.int32();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = ConsumerPartitionMetadata.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ConsumerTopicMetadata_PartitionsEntry {
    return {
      key: isSet(object.key) ? globalThis.Number(object.key) : 0,
      value: isSet(object.value) ? ConsumerPartitionMetadata.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: ConsumerTopicMetadata_PartitionsEntry): unknown {
    const obj: any = {};
    if (message.key !== 0) {
      obj.key = Math.round(message.key);
    }
    if (message.value !== undefined) {
      obj.value = ConsumerPartitionMetadata.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<ConsumerTopicMetadata_PartitionsEntry>): ConsumerTopicMetadata_PartitionsEntry {
    return ConsumerTopicMetadata_PartitionsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ConsumerTopicMetadata_PartitionsEntry>): ConsumerTopicMetadata_PartitionsEntry {
    const message = createBaseConsumerTopicMetadata_PartitionsEntry();
    message.key = object.key ?? 0;
    message.value = (object.value !== undefined && object.value !== null)
      ? ConsumerPartitionMetadata.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseConsumerPartitionMetadata(): ConsumerPartitionMetadata {
  return { offset: Long.ZERO, metadata: "" };
}

export const ConsumerPartitionMetadata: MessageFns<ConsumerPartitionMetadata> = {
  encode(message: ConsumerPartitionMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.offset.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.offset.toString());
    }
    if (message.metadata !== "") {
      writer.uint32(18).string(message.metadata);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ConsumerPartitionMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseConsumerPartitionMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.offset = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.metadata = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ConsumerPartitionMetadata {
    return {
      offset: isSet(object.offset) ? Long.fromValue(object.offset) : Long.ZERO,
      metadata: isSet(object.metadata) ? globalThis.String(object.metadata) : "",
    };
  },

  toJSON(message: ConsumerPartitionMetadata): unknown {
    const obj: any = {};
    if (!message.offset.equals(Long.ZERO)) {
      obj.offset = (message.offset || Long.ZERO).toString();
    }
    if (message.metadata !== "") {
      obj.metadata = message.metadata;
    }
    return obj;
  },

  create(base?: DeepPartial<ConsumerPartitionMetadata>): ConsumerPartitionMetadata {
    return ConsumerPartitionMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ConsumerPartitionMetadata>): ConsumerPartitionMetadata {
    const message = createBaseConsumerPartitionMetadata();
    message.offset = (object.offset !== undefined && object.offset !== null)
      ? Long.fromValue(object.offset)
      : Long.ZERO;
    message.metadata = object.metadata ?? "";
    return message;
  },
};

function createBaseConsumerGroup(): ConsumerGroup {
  return { name: "", topics: {} };
}

export const ConsumerGroup: MessageFns<ConsumerGroup> = {
  encode(message: ConsumerGroup, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    Object.entries(message.topics).forEach(([key, value]) => {
      ConsumerGroup_TopicsEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ConsumerGroup {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseConsumerGroup();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = ConsumerGroup_TopicsEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.topics[entry2.key] = entry2.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ConsumerGroup {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      topics: isObject(object.topics)
        ? Object.entries(object.topics).reduce<{ [key: string]: ConsumerTopicMetadata }>((acc, [key, value]) => {
          acc[key] = ConsumerTopicMetadata.fromJSON(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: ConsumerGroup): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.topics) {
      const entries = Object.entries(message.topics);
      if (entries.length > 0) {
        obj.topics = {};
        entries.forEach(([k, v]) => {
          obj.topics[k] = ConsumerTopicMetadata.toJSON(v);
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<ConsumerGroup>): ConsumerGroup {
    return ConsumerGroup.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ConsumerGroup>): ConsumerGroup {
    const message = createBaseConsumerGroup();
    message.name = object.name ?? "";
    message.topics = Object.entries(object.topics ?? {}).reduce<{ [key: string]: ConsumerTopicMetadata }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = ConsumerTopicMetadata.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseConsumerGroup_TopicsEntry(): ConsumerGroup_TopicsEntry {
  return { key: "", value: undefined };
}

export const ConsumerGroup_TopicsEntry: MessageFns<ConsumerGroup_TopicsEntry> = {
  encode(message: ConsumerGroup_TopicsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      ConsumerTopicMetadata.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ConsumerGroup_TopicsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseConsumerGroup_TopicsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = ConsumerTopicMetadata.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ConsumerGroup_TopicsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? ConsumerTopicMetadata.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: ConsumerGroup_TopicsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = ConsumerTopicMetadata.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<ConsumerGroup_TopicsEntry>): ConsumerGroup_TopicsEntry {
    return ConsumerGroup_TopicsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ConsumerGroup_TopicsEntry>): ConsumerGroup_TopicsEntry {
    const message = createBaseConsumerGroup_TopicsEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? ConsumerTopicMetadata.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseOperationMetadata(): OperationMetadata {
  return {
    createTime: undefined,
    endTime: undefined,
    target: "",
    verb: "",
    statusMessage: "",
    requestedCancellation: false,
    apiVersion: "",
  };
}

export const OperationMetadata: MessageFns<OperationMetadata> = {
  encode(message: OperationMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(10).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(18).fork()).join();
    }
    if (message.target !== "") {
      writer.uint32(26).string(message.target);
    }
    if (message.verb !== "") {
      writer.uint32(34).string(message.verb);
    }
    if (message.statusMessage !== "") {
      writer.uint32(42).string(message.statusMessage);
    }
    if (message.requestedCancellation !== false) {
      writer.uint32(48).bool(message.requestedCancellation);
    }
    if (message.apiVersion !== "") {
      writer.uint32(58).string(message.apiVersion);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OperationMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOperationMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.target = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.verb = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.statusMessage = reader.string();
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.requestedCancellation = reader.bool();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.apiVersion = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OperationMetadata {
    return {
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      target: isSet(object.target) ? globalThis.String(object.target) : "",
      verb: isSet(object.verb) ? globalThis.String(object.verb) : "",
      statusMessage: isSet(object.statusMessage) ? globalThis.String(object.statusMessage) : "",
      requestedCancellation: isSet(object.requestedCancellation)
        ? globalThis.Boolean(object.requestedCancellation)
        : false,
      apiVersion: isSet(object.apiVersion) ? globalThis.String(object.apiVersion) : "",
    };
  },

  toJSON(message: OperationMetadata): unknown {
    const obj: any = {};
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (message.target !== "") {
      obj.target = message.target;
    }
    if (message.verb !== "") {
      obj.verb = message.verb;
    }
    if (message.statusMessage !== "") {
      obj.statusMessage = message.statusMessage;
    }
    if (message.requestedCancellation !== false) {
      obj.requestedCancellation = message.requestedCancellation;
    }
    if (message.apiVersion !== "") {
      obj.apiVersion = message.apiVersion;
    }
    return obj;
  },

  create(base?: DeepPartial<OperationMetadata>): OperationMetadata {
    return OperationMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OperationMetadata>): OperationMetadata {
    const message = createBaseOperationMetadata();
    message.createTime = object.createTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    message.target = object.target ?? "";
    message.verb = object.verb ?? "";
    message.statusMessage = object.statusMessage ?? "";
    message.requestedCancellation = object.requestedCancellation ?? false;
    message.apiVersion = object.apiVersion ?? "";
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
