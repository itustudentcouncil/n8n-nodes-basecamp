// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/asset/v1p7beta1/asset_service.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Operation } from "../../../longrunning/operations.js";
import { Timestamp } from "../../../protobuf/timestamp.js";

export const protobufPackage = "google.cloud.asset.v1p7beta1";

/** Asset content type. */
export enum ContentType {
  /** CONTENT_TYPE_UNSPECIFIED - Unspecified content type. */
  CONTENT_TYPE_UNSPECIFIED = 0,
  /** RESOURCE - Resource metadata. */
  RESOURCE = 1,
  /** IAM_POLICY - The actual IAM policy set on a resource. */
  IAM_POLICY = 2,
  /** ORG_POLICY - The organization policy set on an asset. */
  ORG_POLICY = 4,
  /** ACCESS_POLICY - The Access Context Manager policy set on an asset. */
  ACCESS_POLICY = 5,
  /** RELATIONSHIP - The related resources. */
  RELATIONSHIP = 7,
  UNRECOGNIZED = -1,
}

export function contentTypeFromJSON(object: any): ContentType {
  switch (object) {
    case 0:
    case "CONTENT_TYPE_UNSPECIFIED":
      return ContentType.CONTENT_TYPE_UNSPECIFIED;
    case 1:
    case "RESOURCE":
      return ContentType.RESOURCE;
    case 2:
    case "IAM_POLICY":
      return ContentType.IAM_POLICY;
    case 4:
    case "ORG_POLICY":
      return ContentType.ORG_POLICY;
    case 5:
    case "ACCESS_POLICY":
      return ContentType.ACCESS_POLICY;
    case 7:
    case "RELATIONSHIP":
      return ContentType.RELATIONSHIP;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ContentType.UNRECOGNIZED;
  }
}

export function contentTypeToJSON(object: ContentType): string {
  switch (object) {
    case ContentType.CONTENT_TYPE_UNSPECIFIED:
      return "CONTENT_TYPE_UNSPECIFIED";
    case ContentType.RESOURCE:
      return "RESOURCE";
    case ContentType.IAM_POLICY:
      return "IAM_POLICY";
    case ContentType.ORG_POLICY:
      return "ORG_POLICY";
    case ContentType.ACCESS_POLICY:
      return "ACCESS_POLICY";
    case ContentType.RELATIONSHIP:
      return "RELATIONSHIP";
    case ContentType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Export asset request. */
export interface ExportAssetsRequest {
  /**
   * Required. The relative name of the root asset. This can only be an
   * organization number (such as "organizations/123"), a project ID (such as
   * "projects/my-project-id"), or a project number (such as "projects/12345"),
   * or a folder number (such as "folders/123").
   */
  parent: string;
  /**
   * Timestamp to take an asset snapshot. This can only be set to a timestamp
   * between the current time and the current time minus 35 days (inclusive).
   * If not specified, the current time will be used. Due to delays in resource
   * data collection and indexing, there is a volatile window during which
   * running the same query may get different results.
   */
  readTime:
    | Date
    | undefined;
  /**
   * A list of asset types to take a snapshot for. For example:
   * "compute.googleapis.com/Disk".
   *
   * Regular expressions are also supported. For example:
   *
   * * "compute.googleapis.com.*" snapshots resources whose asset type starts
   * with "compute.googleapis.com".
   * * ".*Instance" snapshots resources whose asset type ends with "Instance".
   * * ".*Instance.*" snapshots resources whose asset type contains "Instance".
   *
   * See [RE2](https://github.com/google/re2/wiki/Syntax) for all supported
   * regular expression syntax. If the regular expression does not match any
   * supported asset type, an INVALID_ARGUMENT error will be returned.
   *
   * If specified, only matching assets will be returned, otherwise, it will
   * snapshot all asset types. See [Introduction to Cloud Asset
   * Inventory](https://cloud.google.com/asset-inventory/docs/overview)
   * for all supported asset types.
   */
  assetTypes: string[];
  /**
   * Asset content type. If not specified, no content but the asset name will be
   * returned.
   */
  contentType: ContentType;
  /**
   * Required. Output configuration indicating where the results will be output
   * to.
   */
  outputConfig:
    | OutputConfig
    | undefined;
  /**
   * A list of relationship types to export, for example:
   * `INSTANCE_TO_INSTANCEGROUP`. This field should only be specified if
   * content_type=RELATIONSHIP. If specified, it will snapshot [asset_types]'
   * specified relationships, or give errors if any relationship_types'
   * supported types are not in [asset_types]. If not specified, it will
   * snapshot all [asset_types]' supported relationships. An unspecified
   * [asset_types] field means all supported asset_types. See [Introduction to
   * Cloud Asset
   * Inventory](https://cloud.google.com/asset-inventory/docs/overview) for all
   * supported asset types and relationship types.
   */
  relationshipTypes: string[];
}

/**
 * The export asset response. This message is returned by the
 * [google.longrunning.Operations.GetOperation][google.longrunning.Operations.GetOperation]
 * method in the returned
 * [google.longrunning.Operation.response][google.longrunning.Operation.response]
 * field.
 */
export interface ExportAssetsResponse {
  /** Time the snapshot was taken. */
  readTime:
    | Date
    | undefined;
  /** Output configuration indicating where the results were output to. */
  outputConfig:
    | OutputConfig
    | undefined;
  /**
   * Output result indicating where the assets were exported to. For example, a
   * set of actual Cloud Storage object URIs where the assets are
   * exported to. The URIs can be different from what [output_config] has
   * specified, as the service will split the output object into multiple ones
   * once it exceeds a single Cloud Storage object limit.
   */
  outputResult: OutputResult | undefined;
}

/** Output configuration for export assets destination. */
export interface OutputConfig {
  /** Destination on Cloud Storage. */
  gcsDestination?:
    | GcsDestination
    | undefined;
  /**
   * Destination on BigQuery. The output table stores the fields in asset
   * proto as columns in BigQuery.
   */
  bigqueryDestination?: BigQueryDestination | undefined;
}

/** Output result of export assets. */
export interface OutputResult {
  /** Export result on Cloud Storage. */
  gcsResult?: GcsOutputResult | undefined;
}

/** A Cloud Storage output result. */
export interface GcsOutputResult {
  /**
   * List of URIs of the Cloud Storage objects. Example:
   * "gs://bucket_name/object_name".
   */
  uris: string[];
}

/** A Cloud Storage location. */
export interface GcsDestination {
  /**
   * The URI of the Cloud Storage object. It's the same URI that is used by
   * gsutil. Example: "gs://bucket_name/object_name". See [Viewing and
   * Editing Object
   * Metadata](https://cloud.google.com/storage/docs/viewing-editing-metadata)
   * for more information.
   */
  uri?:
    | string
    | undefined;
  /**
   * The URI prefix of all generated Cloud Storage objects. Example:
   * "gs://bucket_name/object_name_prefix". Each object URI is in format:
   * "gs://bucket_name/object_name_prefix/{ASSET_TYPE}/{SHARD_NUMBER} and only
   * contains assets for that type. <shard number> starts from 0. Example:
   * "gs://bucket_name/object_name_prefix/compute.googleapis.com/Disk/0" is
   * the first shard of output objects containing all
   * compute.googleapis.com/Disk assets. An INVALID_ARGUMENT error will be
   * returned if file with the same name "gs://bucket_name/object_name_prefix"
   * already exists.
   */
  uriPrefix?: string | undefined;
}

/** A BigQuery destination for exporting assets to. */
export interface BigQueryDestination {
  /**
   * Required. The BigQuery dataset in format
   * "projects/projectId/datasets/datasetId", to which the snapshot result
   * should be exported. If this dataset does not exist, the export call returns
   * an INVALID_ARGUMENT error.
   */
  dataset: string;
  /**
   * Required. The BigQuery table to which the snapshot result should be
   * written. If this table does not exist, a new table with the given name
   * will be created.
   */
  table: string;
  /**
   * If the destination table already exists and this flag is `TRUE`, the
   * table will be overwritten by the contents of assets snapshot. If the flag
   * is `FALSE` or unset and the destination table already exists, the export
   * call returns an INVALID_ARGUMEMT error.
   */
  force: boolean;
  /**
   * [partition_spec] determines whether to export to partitioned table(s) and
   * how to partition the data.
   *
   * If [partition_spec] is unset or [partition_spec.partition_key] is unset or
   * `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to
   * non-partitioned table(s). [force] will decide whether to overwrite existing
   * table(s).
   *
   * If [partition_spec] is specified. First, the snapshot results will be
   * written to partitioned table(s) with two additional timestamp columns,
   * readTime and requestTime, one of which will be the partition key. Secondly,
   * in the case when any destination table already exists, it will first try to
   * update existing table's schema as necessary by appending additional
   * columns. Then, if [force] is `TRUE`, the corresponding partition will be
   * overwritten by the snapshot results (data in different partitions will
   * remain intact); if [force] is unset or `FALSE`, it will append the data. An
   * error will be returned if the schema update or data appension fails.
   */
  partitionSpec:
    | PartitionSpec
    | undefined;
  /**
   * If this flag is `TRUE`, the snapshot results will be written to one or
   * multiple tables, each of which contains results of one asset type. The
   * [force] and [partition_spec] fields will apply to each of them.
   *
   * Field [table] will be concatenated with "_" and the asset type names (see
   * https://cloud.google.com/asset-inventory/docs/supported-asset-types for
   * supported asset types) to construct per-asset-type table names, in which
   * all non-alphanumeric characters like "." and "/" will be substituted by
   * "_". Example: if field [table] is "mytable" and snapshot results
   * contain "storage.googleapis.com/Bucket" assets, the corresponding table
   * name will be "mytable_storage_googleapis_com_Bucket". If any of these
   * tables does not exist, a new table with the concatenated name will be
   * created.
   *
   * When [content_type] in the ExportAssetsRequest is `RESOURCE`, the schema of
   * each table will include RECORD-type columns mapped to the nested fields in
   * the Asset.resource.data field of that asset type (up to the 15 nested level
   * BigQuery supports
   * (https://cloud.google.com/bigquery/docs/nested-repeated#limitations)). The
   * fields in >15 nested levels will be stored in JSON format string as a child
   * column of its parent RECORD column.
   *
   * If error occurs when exporting to any table, the whole export call will
   * return an error but the export results that already succeed will persist.
   * Example: if exporting to table_type_A succeeds when exporting to
   * table_type_B fails during one export call, the results in table_type_A will
   * persist and there will not be partial results persisting in a table.
   */
  separateTablesPerAssetType: boolean;
}

/** Specifications of BigQuery partitioned table as export destination. */
export interface PartitionSpec {
  /** The partition key for BigQuery partitioned table. */
  partitionKey: PartitionSpec_PartitionKey;
}

/**
 * This enum is used to determine the partition key column when exporting
 * assets to BigQuery partitioned table(s). Note that, if the partition key is
 * a timestamp column, the actual partition is based on its date value
 * (expressed in UTC. see details in
 * https://cloud.google.com/bigquery/docs/partitioned-tables#date_timestamp_partitioned_tables).
 */
export enum PartitionSpec_PartitionKey {
  /** PARTITION_KEY_UNSPECIFIED - Unspecified partition key. If used, it means using non-partitioned table. */
  PARTITION_KEY_UNSPECIFIED = 0,
  /**
   * READ_TIME - The time when the snapshot is taken. If specified as partition key, the
   * result table(s) is partitoned by the additional timestamp column,
   * readTime. If [read_time] in ExportAssetsRequest is specified, the
   * readTime column's value will be the same as it. Otherwise, its value will
   * be the current time that is used to take the snapshot.
   */
  READ_TIME = 1,
  /**
   * REQUEST_TIME - The time when the request is received and started to be processed. If
   * specified as partition key, the result table(s) is partitoned by the
   * requestTime column, an additional timestamp column representing when the
   * request was received.
   */
  REQUEST_TIME = 2,
  UNRECOGNIZED = -1,
}

export function partitionSpec_PartitionKeyFromJSON(object: any): PartitionSpec_PartitionKey {
  switch (object) {
    case 0:
    case "PARTITION_KEY_UNSPECIFIED":
      return PartitionSpec_PartitionKey.PARTITION_KEY_UNSPECIFIED;
    case 1:
    case "READ_TIME":
      return PartitionSpec_PartitionKey.READ_TIME;
    case 2:
    case "REQUEST_TIME":
      return PartitionSpec_PartitionKey.REQUEST_TIME;
    case -1:
    case "UNRECOGNIZED":
    default:
      return PartitionSpec_PartitionKey.UNRECOGNIZED;
  }
}

export function partitionSpec_PartitionKeyToJSON(object: PartitionSpec_PartitionKey): string {
  switch (object) {
    case PartitionSpec_PartitionKey.PARTITION_KEY_UNSPECIFIED:
      return "PARTITION_KEY_UNSPECIFIED";
    case PartitionSpec_PartitionKey.READ_TIME:
      return "READ_TIME";
    case PartitionSpec_PartitionKey.REQUEST_TIME:
      return "REQUEST_TIME";
    case PartitionSpec_PartitionKey.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseExportAssetsRequest(): ExportAssetsRequest {
  return {
    parent: "",
    readTime: undefined,
    assetTypes: [],
    contentType: 0,
    outputConfig: undefined,
    relationshipTypes: [],
  };
}

export const ExportAssetsRequest: MessageFns<ExportAssetsRequest> = {
  encode(message: ExportAssetsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.readTime !== undefined) {
      Timestamp.encode(toTimestamp(message.readTime), writer.uint32(18).fork()).join();
    }
    for (const v of message.assetTypes) {
      writer.uint32(26).string(v!);
    }
    if (message.contentType !== 0) {
      writer.uint32(32).int32(message.contentType);
    }
    if (message.outputConfig !== undefined) {
      OutputConfig.encode(message.outputConfig, writer.uint32(42).fork()).join();
    }
    for (const v of message.relationshipTypes) {
      writer.uint32(50).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportAssetsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportAssetsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.readTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.assetTypes.push(reader.string());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.contentType = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.outputConfig = OutputConfig.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.relationshipTypes.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportAssetsRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      readTime: isSet(object.readTime) ? fromJsonTimestamp(object.readTime) : undefined,
      assetTypes: globalThis.Array.isArray(object?.assetTypes)
        ? object.assetTypes.map((e: any) => globalThis.String(e))
        : [],
      contentType: isSet(object.contentType) ? contentTypeFromJSON(object.contentType) : 0,
      outputConfig: isSet(object.outputConfig) ? OutputConfig.fromJSON(object.outputConfig) : undefined,
      relationshipTypes: globalThis.Array.isArray(object?.relationshipTypes)
        ? object.relationshipTypes.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: ExportAssetsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.readTime !== undefined) {
      obj.readTime = message.readTime.toISOString();
    }
    if (message.assetTypes?.length) {
      obj.assetTypes = message.assetTypes;
    }
    if (message.contentType !== 0) {
      obj.contentType = contentTypeToJSON(message.contentType);
    }
    if (message.outputConfig !== undefined) {
      obj.outputConfig = OutputConfig.toJSON(message.outputConfig);
    }
    if (message.relationshipTypes?.length) {
      obj.relationshipTypes = message.relationshipTypes;
    }
    return obj;
  },

  create(base?: DeepPartial<ExportAssetsRequest>): ExportAssetsRequest {
    return ExportAssetsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportAssetsRequest>): ExportAssetsRequest {
    const message = createBaseExportAssetsRequest();
    message.parent = object.parent ?? "";
    message.readTime = object.readTime ?? undefined;
    message.assetTypes = object.assetTypes?.map((e) => e) || [];
    message.contentType = object.contentType ?? 0;
    message.outputConfig = (object.outputConfig !== undefined && object.outputConfig !== null)
      ? OutputConfig.fromPartial(object.outputConfig)
      : undefined;
    message.relationshipTypes = object.relationshipTypes?.map((e) => e) || [];
    return message;
  },
};

function createBaseExportAssetsResponse(): ExportAssetsResponse {
  return { readTime: undefined, outputConfig: undefined, outputResult: undefined };
}

export const ExportAssetsResponse: MessageFns<ExportAssetsResponse> = {
  encode(message: ExportAssetsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.readTime !== undefined) {
      Timestamp.encode(toTimestamp(message.readTime), writer.uint32(10).fork()).join();
    }
    if (message.outputConfig !== undefined) {
      OutputConfig.encode(message.outputConfig, writer.uint32(18).fork()).join();
    }
    if (message.outputResult !== undefined) {
      OutputResult.encode(message.outputResult, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportAssetsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportAssetsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.readTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.outputConfig = OutputConfig.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.outputResult = OutputResult.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportAssetsResponse {
    return {
      readTime: isSet(object.readTime) ? fromJsonTimestamp(object.readTime) : undefined,
      outputConfig: isSet(object.outputConfig) ? OutputConfig.fromJSON(object.outputConfig) : undefined,
      outputResult: isSet(object.outputResult) ? OutputResult.fromJSON(object.outputResult) : undefined,
    };
  },

  toJSON(message: ExportAssetsResponse): unknown {
    const obj: any = {};
    if (message.readTime !== undefined) {
      obj.readTime = message.readTime.toISOString();
    }
    if (message.outputConfig !== undefined) {
      obj.outputConfig = OutputConfig.toJSON(message.outputConfig);
    }
    if (message.outputResult !== undefined) {
      obj.outputResult = OutputResult.toJSON(message.outputResult);
    }
    return obj;
  },

  create(base?: DeepPartial<ExportAssetsResponse>): ExportAssetsResponse {
    return ExportAssetsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportAssetsResponse>): ExportAssetsResponse {
    const message = createBaseExportAssetsResponse();
    message.readTime = object.readTime ?? undefined;
    message.outputConfig = (object.outputConfig !== undefined && object.outputConfig !== null)
      ? OutputConfig.fromPartial(object.outputConfig)
      : undefined;
    message.outputResult = (object.outputResult !== undefined && object.outputResult !== null)
      ? OutputResult.fromPartial(object.outputResult)
      : undefined;
    return message;
  },
};

function createBaseOutputConfig(): OutputConfig {
  return { gcsDestination: undefined, bigqueryDestination: undefined };
}

export const OutputConfig: MessageFns<OutputConfig> = {
  encode(message: OutputConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcsDestination !== undefined) {
      GcsDestination.encode(message.gcsDestination, writer.uint32(10).fork()).join();
    }
    if (message.bigqueryDestination !== undefined) {
      BigQueryDestination.encode(message.bigqueryDestination, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OutputConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOutputConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.gcsDestination = GcsDestination.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.bigqueryDestination = BigQueryDestination.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OutputConfig {
    return {
      gcsDestination: isSet(object.gcsDestination) ? GcsDestination.fromJSON(object.gcsDestination) : undefined,
      bigqueryDestination: isSet(object.bigqueryDestination)
        ? BigQueryDestination.fromJSON(object.bigqueryDestination)
        : undefined,
    };
  },

  toJSON(message: OutputConfig): unknown {
    const obj: any = {};
    if (message.gcsDestination !== undefined) {
      obj.gcsDestination = GcsDestination.toJSON(message.gcsDestination);
    }
    if (message.bigqueryDestination !== undefined) {
      obj.bigqueryDestination = BigQueryDestination.toJSON(message.bigqueryDestination);
    }
    return obj;
  },

  create(base?: DeepPartial<OutputConfig>): OutputConfig {
    return OutputConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OutputConfig>): OutputConfig {
    const message = createBaseOutputConfig();
    message.gcsDestination = (object.gcsDestination !== undefined && object.gcsDestination !== null)
      ? GcsDestination.fromPartial(object.gcsDestination)
      : undefined;
    message.bigqueryDestination = (object.bigqueryDestination !== undefined && object.bigqueryDestination !== null)
      ? BigQueryDestination.fromPartial(object.bigqueryDestination)
      : undefined;
    return message;
  },
};

function createBaseOutputResult(): OutputResult {
  return { gcsResult: undefined };
}

export const OutputResult: MessageFns<OutputResult> = {
  encode(message: OutputResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcsResult !== undefined) {
      GcsOutputResult.encode(message.gcsResult, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OutputResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOutputResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.gcsResult = GcsOutputResult.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OutputResult {
    return { gcsResult: isSet(object.gcsResult) ? GcsOutputResult.fromJSON(object.gcsResult) : undefined };
  },

  toJSON(message: OutputResult): unknown {
    const obj: any = {};
    if (message.gcsResult !== undefined) {
      obj.gcsResult = GcsOutputResult.toJSON(message.gcsResult);
    }
    return obj;
  },

  create(base?: DeepPartial<OutputResult>): OutputResult {
    return OutputResult.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OutputResult>): OutputResult {
    const message = createBaseOutputResult();
    message.gcsResult = (object.gcsResult !== undefined && object.gcsResult !== null)
      ? GcsOutputResult.fromPartial(object.gcsResult)
      : undefined;
    return message;
  },
};

function createBaseGcsOutputResult(): GcsOutputResult {
  return { uris: [] };
}

export const GcsOutputResult: MessageFns<GcsOutputResult> = {
  encode(message: GcsOutputResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.uris) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GcsOutputResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGcsOutputResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.uris.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GcsOutputResult {
    return { uris: globalThis.Array.isArray(object?.uris) ? object.uris.map((e: any) => globalThis.String(e)) : [] };
  },

  toJSON(message: GcsOutputResult): unknown {
    const obj: any = {};
    if (message.uris?.length) {
      obj.uris = message.uris;
    }
    return obj;
  },

  create(base?: DeepPartial<GcsOutputResult>): GcsOutputResult {
    return GcsOutputResult.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GcsOutputResult>): GcsOutputResult {
    const message = createBaseGcsOutputResult();
    message.uris = object.uris?.map((e) => e) || [];
    return message;
  },
};

function createBaseGcsDestination(): GcsDestination {
  return { uri: undefined, uriPrefix: undefined };
}

export const GcsDestination: MessageFns<GcsDestination> = {
  encode(message: GcsDestination, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.uri !== undefined) {
      writer.uint32(10).string(message.uri);
    }
    if (message.uriPrefix !== undefined) {
      writer.uint32(18).string(message.uriPrefix);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GcsDestination {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGcsDestination();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.uri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.uriPrefix = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GcsDestination {
    return {
      uri: isSet(object.uri) ? globalThis.String(object.uri) : undefined,
      uriPrefix: isSet(object.uriPrefix) ? globalThis.String(object.uriPrefix) : undefined,
    };
  },

  toJSON(message: GcsDestination): unknown {
    const obj: any = {};
    if (message.uri !== undefined) {
      obj.uri = message.uri;
    }
    if (message.uriPrefix !== undefined) {
      obj.uriPrefix = message.uriPrefix;
    }
    return obj;
  },

  create(base?: DeepPartial<GcsDestination>): GcsDestination {
    return GcsDestination.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GcsDestination>): GcsDestination {
    const message = createBaseGcsDestination();
    message.uri = object.uri ?? undefined;
    message.uriPrefix = object.uriPrefix ?? undefined;
    return message;
  },
};

function createBaseBigQueryDestination(): BigQueryDestination {
  return { dataset: "", table: "", force: false, partitionSpec: undefined, separateTablesPerAssetType: false };
}

export const BigQueryDestination: MessageFns<BigQueryDestination> = {
  encode(message: BigQueryDestination, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dataset !== "") {
      writer.uint32(10).string(message.dataset);
    }
    if (message.table !== "") {
      writer.uint32(18).string(message.table);
    }
    if (message.force !== false) {
      writer.uint32(24).bool(message.force);
    }
    if (message.partitionSpec !== undefined) {
      PartitionSpec.encode(message.partitionSpec, writer.uint32(34).fork()).join();
    }
    if (message.separateTablesPerAssetType !== false) {
      writer.uint32(40).bool(message.separateTablesPerAssetType);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigQueryDestination {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigQueryDestination();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dataset = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.table = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.force = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.partitionSpec = PartitionSpec.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.separateTablesPerAssetType = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigQueryDestination {
    return {
      dataset: isSet(object.dataset) ? globalThis.String(object.dataset) : "",
      table: isSet(object.table) ? globalThis.String(object.table) : "",
      force: isSet(object.force) ? globalThis.Boolean(object.force) : false,
      partitionSpec: isSet(object.partitionSpec) ? PartitionSpec.fromJSON(object.partitionSpec) : undefined,
      separateTablesPerAssetType: isSet(object.separateTablesPerAssetType)
        ? globalThis.Boolean(object.separateTablesPerAssetType)
        : false,
    };
  },

  toJSON(message: BigQueryDestination): unknown {
    const obj: any = {};
    if (message.dataset !== "") {
      obj.dataset = message.dataset;
    }
    if (message.table !== "") {
      obj.table = message.table;
    }
    if (message.force !== false) {
      obj.force = message.force;
    }
    if (message.partitionSpec !== undefined) {
      obj.partitionSpec = PartitionSpec.toJSON(message.partitionSpec);
    }
    if (message.separateTablesPerAssetType !== false) {
      obj.separateTablesPerAssetType = message.separateTablesPerAssetType;
    }
    return obj;
  },

  create(base?: DeepPartial<BigQueryDestination>): BigQueryDestination {
    return BigQueryDestination.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigQueryDestination>): BigQueryDestination {
    const message = createBaseBigQueryDestination();
    message.dataset = object.dataset ?? "";
    message.table = object.table ?? "";
    message.force = object.force ?? false;
    message.partitionSpec = (object.partitionSpec !== undefined && object.partitionSpec !== null)
      ? PartitionSpec.fromPartial(object.partitionSpec)
      : undefined;
    message.separateTablesPerAssetType = object.separateTablesPerAssetType ?? false;
    return message;
  },
};

function createBasePartitionSpec(): PartitionSpec {
  return { partitionKey: 0 };
}

export const PartitionSpec: MessageFns<PartitionSpec> = {
  encode(message: PartitionSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.partitionKey !== 0) {
      writer.uint32(8).int32(message.partitionKey);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PartitionSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePartitionSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.partitionKey = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PartitionSpec {
    return { partitionKey: isSet(object.partitionKey) ? partitionSpec_PartitionKeyFromJSON(object.partitionKey) : 0 };
  },

  toJSON(message: PartitionSpec): unknown {
    const obj: any = {};
    if (message.partitionKey !== 0) {
      obj.partitionKey = partitionSpec_PartitionKeyToJSON(message.partitionKey);
    }
    return obj;
  },

  create(base?: DeepPartial<PartitionSpec>): PartitionSpec {
    return PartitionSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PartitionSpec>): PartitionSpec {
    const message = createBasePartitionSpec();
    message.partitionKey = object.partitionKey ?? 0;
    return message;
  },
};

/** Asset service definition. */
export type AssetServiceDefinition = typeof AssetServiceDefinition;
export const AssetServiceDefinition = {
  name: "AssetService",
  fullName: "google.cloud.asset.v1p7beta1.AssetService",
  methods: {
    /**
     * Exports assets with time and resource types to a given Cloud Storage
     * location/BigQuery table. For Cloud Storage location destinations, the
     * output format is newline-delimited JSON. Each line represents a
     * [google.cloud.asset.v1p7beta1.Asset][google.cloud.asset.v1p7beta1.Asset] in
     * the JSON format; for BigQuery table destinations, the output table stores
     * the fields in asset proto as columns. This API implements the
     * [google.longrunning.Operation][google.longrunning.Operation] API , which
     * allows you to keep track of the export. We recommend intervals of at least
     * 2 seconds with exponential retry to poll the export operation result. For
     * regular-size resource parent, the export operation usually finishes within
     * 5 minutes.
     */
    exportAssets: {
      name: "ExportAssets",
      requestType: ExportAssetsRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              101,
              10,
              49,
              103,
              111,
              111,
              103,
              108,
              101,
              46,
              99,
              108,
              111,
              117,
              100,
              46,
              97,
              115,
              115,
              101,
              116,
              46,
              118,
              49,
              112,
              55,
              98,
              101,
              116,
              97,
              49,
              46,
              69,
              120,
              112,
              111,
              114,
              116,
              65,
              115,
              115,
              101,
              116,
              115,
              82,
              101,
              115,
              112,
              111,
              110,
              115,
              101,
              18,
              48,
              103,
              111,
              111,
              103,
              108,
              101,
              46,
              99,
              108,
              111,
              117,
              100,
              46,
              97,
              115,
              115,
              101,
              116,
              46,
              118,
              49,
              112,
              55,
              98,
              101,
              116,
              97,
              49,
              46,
              69,
              120,
              112,
              111,
              114,
              116,
              65,
              115,
              115,
              101,
              116,
              115,
              82,
              101,
              113,
              117,
              101,
              115,
              116,
            ]),
          ],
          578365826: [
            Buffer.from([
              41,
              58,
              1,
              42,
              34,
              36,
              47,
              118,
              49,
              112,
              55,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              42,
              47,
              42,
              125,
              58,
              101,
              120,
              112,
              111,
              114,
              116,
              65,
              115,
              115,
              101,
              116,
              115,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface AssetServiceImplementation<CallContextExt = {}> {
  /**
   * Exports assets with time and resource types to a given Cloud Storage
   * location/BigQuery table. For Cloud Storage location destinations, the
   * output format is newline-delimited JSON. Each line represents a
   * [google.cloud.asset.v1p7beta1.Asset][google.cloud.asset.v1p7beta1.Asset] in
   * the JSON format; for BigQuery table destinations, the output table stores
   * the fields in asset proto as columns. This API implements the
   * [google.longrunning.Operation][google.longrunning.Operation] API , which
   * allows you to keep track of the export. We recommend intervals of at least
   * 2 seconds with exponential retry to poll the export operation result. For
   * regular-size resource parent, the export operation usually finishes within
   * 5 minutes.
   */
  exportAssets(request: ExportAssetsRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
}

export interface AssetServiceClient<CallOptionsExt = {}> {
  /**
   * Exports assets with time and resource types to a given Cloud Storage
   * location/BigQuery table. For Cloud Storage location destinations, the
   * output format is newline-delimited JSON. Each line represents a
   * [google.cloud.asset.v1p7beta1.Asset][google.cloud.asset.v1p7beta1.Asset] in
   * the JSON format; for BigQuery table destinations, the output table stores
   * the fields in asset proto as columns. This API implements the
   * [google.longrunning.Operation][google.longrunning.Operation] API , which
   * allows you to keep track of the export. We recommend intervals of at least
   * 2 seconds with exponential retry to poll the export operation result. For
   * regular-size resource parent, the export operation usually finishes within
   * 5 minutes.
   */
  exportAssets(request: DeepPartial<ExportAssetsRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
