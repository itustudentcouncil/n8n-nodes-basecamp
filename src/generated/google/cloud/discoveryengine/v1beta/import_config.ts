// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/discoveryengine/v1beta/import_config.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { FieldMask } from "../../../protobuf/field_mask.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { Status } from "../../../rpc/status.js";
import { DateMessage } from "../../../type/date.js";
import { CompletionSuggestion, SuggestionDenyListEntry } from "./completion.js";
import { Document } from "./document.js";
import { SampleQuery } from "./sample_query.js";
import { UserEvent } from "./user_event.js";

export const protobufPackage = "google.cloud.discoveryengine.v1beta";

/** Cloud Storage location for input content. */
export interface GcsSource {
  /**
   * Required. Cloud Storage URIs to input files. Each URI can be up to
   * 2000 characters long. URIs can match the full object path (for example,
   * `gs://bucket/directory/object.json`) or a pattern matching one or more
   * files, such as `gs://bucket/directory/*.json`.
   *
   * A request can contain at most 100 files (or 100,000 files if `data_schema`
   * is `content`). Each file can be up to 2 GB (or 100 MB if `data_schema` is
   * `content`).
   */
  inputUris: string[];
  /**
   * The schema to use when parsing the data from the source.
   *
   * Supported values for document imports:
   *
   * * `document` (default): One JSON
   * [Document][google.cloud.discoveryengine.v1beta.Document] per line. Each
   * document must
   *   have a valid
   *   [Document.id][google.cloud.discoveryengine.v1beta.Document.id].
   * * `content`: Unstructured data (e.g. PDF, HTML). Each file matched by
   *   `input_uris` becomes a document, with the ID set to the first 128
   *   bits of SHA256(URI) encoded as a hex string.
   * * `custom`: One custom data JSON per row in arbitrary format that conforms
   *   to the defined [Schema][google.cloud.discoveryengine.v1beta.Schema] of
   *   the data store. This can only be used by the GENERIC Data Store vertical.
   * * `csv`: A CSV file with header conforming to the defined
   * [Schema][google.cloud.discoveryengine.v1beta.Schema] of the
   *   data store. Each entry after the header is imported as a Document.
   *   This can only be used by the GENERIC Data Store vertical.
   *
   * Supported values for user event imports:
   *
   * * `user_event` (default): One JSON
   * [UserEvent][google.cloud.discoveryengine.v1beta.UserEvent] per line.
   */
  dataSchema: string;
}

/** BigQuery source import data from. */
export interface BigQuerySource {
  /** BigQuery time partitioned table's _PARTITIONDATE in YYYY-MM-DD format. */
  partitionDate?:
    | DateMessage
    | undefined;
  /**
   * The project ID or the project number that contains the BigQuery source. Has
   * a length limit of 128 characters. If not specified, inherits the project
   * ID from the parent request.
   */
  projectId: string;
  /**
   * Required. The BigQuery data set to copy the data from with a length limit
   * of 1,024 characters.
   */
  datasetId: string;
  /**
   * Required. The BigQuery table to copy the data from with a length limit of
   * 1,024 characters.
   */
  tableId: string;
  /**
   * Intermediate Cloud Storage directory used for the import with a length
   * limit of 2,000 characters. Can be specified if one wants to have the
   * BigQuery export to a specific Cloud Storage directory.
   */
  gcsStagingDir: string;
  /**
   * The schema to use when parsing the data from the source.
   *
   * Supported values for user event imports:
   *
   * * `user_event` (default): One
   * [UserEvent][google.cloud.discoveryengine.v1beta.UserEvent] per row.
   *
   * Supported values for document imports:
   *
   * * `document` (default): One
   * [Document][google.cloud.discoveryengine.v1beta.Document] format per
   *   row. Each document must have a valid
   *   [Document.id][google.cloud.discoveryengine.v1beta.Document.id] and one of
   *   [Document.json_data][google.cloud.discoveryengine.v1beta.Document.json_data]
   *   or
   *   [Document.struct_data][google.cloud.discoveryengine.v1beta.Document.struct_data].
   * * `custom`: One custom data per row in arbitrary format that conforms to
   *   the defined [Schema][google.cloud.discoveryengine.v1beta.Schema] of the
   *   data store. This can only be used by the GENERIC Data Store vertical.
   */
  dataSchema: string;
}

/** The Spanner source for importing data */
export interface SpannerSource {
  /**
   * The project ID that contains the Spanner source. Has a length limit of 128
   * characters. If not specified, inherits the project ID from the parent
   * request.
   */
  projectId: string;
  /** Required. The instance ID of the source Spanner table. */
  instanceId: string;
  /** Required. The database ID of the source Spanner table. */
  databaseId: string;
  /** Required. The table name of the Spanner database that needs to be imported. */
  tableId: string;
  /**
   * Whether to apply data boost on Spanner export. Enabling this option will
   * incur additional cost. More info can be found
   * [here](https://cloud.google.com/spanner/docs/databoost/databoost-overview#billing_and_quotas).
   */
  enableDataBoost: boolean;
}

/**
 * The Bigtable Options object that contains information to support
 * the import.
 */
export interface BigtableOptions {
  /**
   * The field name used for saving row key value in the document. The name has
   * to match the pattern `[a-zA-Z0-9][a-zA-Z0-9-_]*`.
   */
  keyFieldName: string;
  /**
   * The mapping from family names to an object that contains column families
   * level information for the given column family. If a family is not present
   * in this map it will be ignored.
   */
  families: { [key: string]: BigtableOptions_BigtableColumnFamily };
}

/**
 * The type of values in a Bigtable column or column family.
 * The values are expected to be encoded using
 * [HBase
 * Bytes.toBytes](https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/util/Bytes.html)
 * function when the encoding value is set to `BINARY`.
 */
export enum BigtableOptions_Type {
  /** TYPE_UNSPECIFIED - The type is unspecified. */
  TYPE_UNSPECIFIED = 0,
  /** STRING - String type. */
  STRING = 1,
  /** NUMBER - Numerical type. */
  NUMBER = 2,
  /** INTEGER - Integer type. */
  INTEGER = 3,
  /** VAR_INTEGER - Variable length integer type. */
  VAR_INTEGER = 4,
  /** BIG_NUMERIC - BigDecimal type. */
  BIG_NUMERIC = 5,
  /** BOOLEAN - Boolean type. */
  BOOLEAN = 6,
  /** JSON - JSON type. */
  JSON = 7,
  UNRECOGNIZED = -1,
}

export function bigtableOptions_TypeFromJSON(object: any): BigtableOptions_Type {
  switch (object) {
    case 0:
    case "TYPE_UNSPECIFIED":
      return BigtableOptions_Type.TYPE_UNSPECIFIED;
    case 1:
    case "STRING":
      return BigtableOptions_Type.STRING;
    case 2:
    case "NUMBER":
      return BigtableOptions_Type.NUMBER;
    case 3:
    case "INTEGER":
      return BigtableOptions_Type.INTEGER;
    case 4:
    case "VAR_INTEGER":
      return BigtableOptions_Type.VAR_INTEGER;
    case 5:
    case "BIG_NUMERIC":
      return BigtableOptions_Type.BIG_NUMERIC;
    case 6:
    case "BOOLEAN":
      return BigtableOptions_Type.BOOLEAN;
    case 7:
    case "JSON":
      return BigtableOptions_Type.JSON;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BigtableOptions_Type.UNRECOGNIZED;
  }
}

export function bigtableOptions_TypeToJSON(object: BigtableOptions_Type): string {
  switch (object) {
    case BigtableOptions_Type.TYPE_UNSPECIFIED:
      return "TYPE_UNSPECIFIED";
    case BigtableOptions_Type.STRING:
      return "STRING";
    case BigtableOptions_Type.NUMBER:
      return "NUMBER";
    case BigtableOptions_Type.INTEGER:
      return "INTEGER";
    case BigtableOptions_Type.VAR_INTEGER:
      return "VAR_INTEGER";
    case BigtableOptions_Type.BIG_NUMERIC:
      return "BIG_NUMERIC";
    case BigtableOptions_Type.BOOLEAN:
      return "BOOLEAN";
    case BigtableOptions_Type.JSON:
      return "JSON";
    case BigtableOptions_Type.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The encoding mode of a Bigtable column or column family. */
export enum BigtableOptions_Encoding {
  /** ENCODING_UNSPECIFIED - The encoding is unspecified. */
  ENCODING_UNSPECIFIED = 0,
  /** TEXT - Text encoding. */
  TEXT = 1,
  /** BINARY - Binary encoding. */
  BINARY = 2,
  UNRECOGNIZED = -1,
}

export function bigtableOptions_EncodingFromJSON(object: any): BigtableOptions_Encoding {
  switch (object) {
    case 0:
    case "ENCODING_UNSPECIFIED":
      return BigtableOptions_Encoding.ENCODING_UNSPECIFIED;
    case 1:
    case "TEXT":
      return BigtableOptions_Encoding.TEXT;
    case 2:
    case "BINARY":
      return BigtableOptions_Encoding.BINARY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BigtableOptions_Encoding.UNRECOGNIZED;
  }
}

export function bigtableOptions_EncodingToJSON(object: BigtableOptions_Encoding): string {
  switch (object) {
    case BigtableOptions_Encoding.ENCODING_UNSPECIFIED:
      return "ENCODING_UNSPECIFIED";
    case BigtableOptions_Encoding.TEXT:
      return "TEXT";
    case BigtableOptions_Encoding.BINARY:
      return "BINARY";
    case BigtableOptions_Encoding.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The column family of the Bigtable. */
export interface BigtableOptions_BigtableColumnFamily {
  /**
   * The field name to use for this column family in the document. The
   * name has to match the pattern `[a-zA-Z0-9][a-zA-Z0-9-_]*`. If not set,
   * it is parsed from the family name with best effort. However, due to
   * different naming patterns, field name collisions could happen, where
   * parsing behavior is undefined.
   */
  fieldName: string;
  /**
   * The encoding mode of the values when the type is not STRING.
   * Acceptable encoding values are:
   *
   * * `TEXT`: indicates values are alphanumeric text strings.
   * * `BINARY`: indicates values are encoded using `HBase Bytes.toBytes`
   * family of functions. This can be overridden for a specific column
   * by listing that column in `columns` and specifying an encoding for it.
   */
  encoding: BigtableOptions_Encoding;
  /**
   * The type of values in this column family.
   * The values are expected to be encoded using `HBase Bytes.toBytes`
   * function when the encoding value is set to `BINARY`.
   */
  type: BigtableOptions_Type;
  /**
   * The list of objects that contains column level information for each
   * column. If a column is not present in this list it will be ignored.
   */
  columns: BigtableOptions_BigtableColumn[];
}

/** The column of the Bigtable. */
export interface BigtableOptions_BigtableColumn {
  /**
   * Required. Qualifier of the column. If it cannot be decoded with utf-8,
   * use a base-64 encoded string instead.
   */
  qualifier: Buffer;
  /**
   * The field name to use for this column in the document. The name has to
   * match the pattern `[a-zA-Z0-9][a-zA-Z0-9-_]*`.
   * If not set, it is parsed from the qualifier bytes with best effort.
   * However, due to different naming patterns, field name collisions could
   * happen, where parsing behavior is undefined.
   */
  fieldName: string;
  /**
   * The encoding mode of the values when the type is not `STRING`.
   * Acceptable encoding values are:
   *
   * * `TEXT`: indicates values are alphanumeric text strings.
   * * `BINARY`: indicates values are encoded using `HBase Bytes.toBytes`
   * family of functions. This can be overridden for a specific column
   * by listing that column in `columns` and specifying an encoding for it.
   */
  encoding: BigtableOptions_Encoding;
  /**
   * The type of values in this column family.
   * The values are expected to be encoded using `HBase Bytes.toBytes`
   * function when the encoding value is set to `BINARY`.
   */
  type: BigtableOptions_Type;
}

export interface BigtableOptions_FamiliesEntry {
  key: string;
  value: BigtableOptions_BigtableColumnFamily | undefined;
}

/** The Cloud Bigtable source for importing data. */
export interface BigtableSource {
  /**
   * The project ID that contains the Bigtable source. Has a length limit of 128
   * characters. If not specified, inherits the project ID from the parent
   * request.
   */
  projectId: string;
  /** Required. The instance ID of the Cloud Bigtable that needs to be imported. */
  instanceId: string;
  /** Required. The table ID of the Cloud Bigtable that needs to be imported. */
  tableId: string;
  /**
   * Required. Bigtable options that contains information needed when parsing
   * data into typed structures. For example, column type annotations.
   */
  bigtableOptions: BigtableOptions | undefined;
}

/** Cloud FhirStore source import data from. */
export interface FhirStoreSource {
  /**
   * Required. The full resource name of the FHIR store to import data from, in
   * the format of
   * `projects/{project}/locations/{location}/datasets/{dataset}/fhirStores/{fhir_store}`.
   */
  fhirStore: string;
  /**
   * Intermediate Cloud Storage directory used for the import with a length
   * limit of 2,000 characters. Can be specified if one wants to have the
   * FhirStore export to a specific Cloud Storage directory.
   */
  gcsStagingDir: string;
  /**
   * The FHIR resource types to import. The resource types should be a subset of
   * all [supported FHIR resource
   * types](https://cloud.google.com/generative-ai-app-builder/docs/fhir-schema-reference#resource-level-specification).
   * Default to all supported FHIR resource types if empty.
   */
  resourceTypes: string[];
}

/** Cloud SQL source import data from. */
export interface CloudSqlSource {
  /**
   * The project ID that contains the Cloud SQL source. Has a length limit of
   * 128 characters. If not specified, inherits the project ID from the parent
   * request.
   */
  projectId: string;
  /**
   * Required. The Cloud SQL instance to copy the data from with a length limit
   * of 256 characters.
   */
  instanceId: string;
  /**
   * Required. The Cloud SQL database to copy the data from with a length limit
   * of 256 characters.
   */
  databaseId: string;
  /**
   * Required. The Cloud SQL table to copy the data from with a length limit of
   * 256 characters.
   */
  tableId: string;
  /**
   * Intermediate Cloud Storage directory used for the import with a length
   * limit of 2,000 characters. Can be specified if one wants to have the
   * Cloud SQL export to a specific Cloud Storage directory.
   *
   * Ensure that the Cloud SQL service account has the necessary Cloud
   * Storage Admin permissions to access the specified Cloud Storage directory.
   */
  gcsStagingDir: string;
  /**
   * Option for serverless export. Enabling this option will incur additional
   * cost. More info can be found
   * [here](https://cloud.google.com/sql/pricing#serverless).
   */
  offload: boolean;
}

/** AlloyDB source import data from. */
export interface AlloyDbSource {
  /**
   * The project ID that contains the AlloyDB source.
   * Has a length limit of 128 characters. If not specified, inherits the
   * project ID from the parent request.
   */
  projectId: string;
  /**
   * Required. The AlloyDB location to copy the data from with a length limit of
   * 256 characters.
   */
  locationId: string;
  /**
   * Required. The AlloyDB cluster to copy the data from with a length limit of
   * 256 characters.
   */
  clusterId: string;
  /**
   * Required. The AlloyDB database to copy the data from with a length limit of
   * 256 characters.
   */
  databaseId: string;
  /**
   * Required. The AlloyDB table to copy the data from with a length limit of
   * 256 characters.
   */
  tableId: string;
  /**
   * Intermediate Cloud Storage directory used for the import with a length
   * limit of 2,000 characters. Can be specified if one wants to have the
   * AlloyDB export to a specific Cloud Storage directory.
   *
   * Ensure that the AlloyDB service account has the necessary Cloud
   * Storage Admin permissions to access the specified Cloud Storage directory.
   */
  gcsStagingDir: string;
}

/** Firestore source import data from. */
export interface FirestoreSource {
  /**
   * The project ID that the Cloud SQL source is in with a length limit of 128
   * characters. If not specified, inherits the project ID from the parent
   * request.
   */
  projectId: string;
  /**
   * Required. The Firestore database to copy the data from with a length limit
   * of 256 characters.
   */
  databaseId: string;
  /**
   * Required. The Firestore collection (or entity) to copy the data from with a
   * length limit of 1,500 characters.
   */
  collectionId: string;
  /**
   * Intermediate Cloud Storage directory used for the import with a length
   * limit of 2,000 characters. Can be specified if one wants to have the
   * Firestore export to a specific Cloud Storage directory.
   *
   * Ensure that the Firestore service account has the necessary Cloud
   * Storage Admin permissions to access the specified Cloud Storage directory.
   */
  gcsStagingDir: string;
}

/** Configuration of destination for Import related errors. */
export interface ImportErrorConfig {
  /**
   * Cloud Storage prefix for import errors. This must be an empty,
   * existing Cloud Storage directory. Import errors are written to
   * sharded files in this directory, one per line, as a JSON-encoded
   * `google.rpc.Status` message.
   */
  gcsPrefix?: string | undefined;
}

/** Request message for the ImportUserEvents request. */
export interface ImportUserEventsRequest {
  /** The Inline source for the input content for UserEvents. */
  inlineSource?:
    | ImportUserEventsRequest_InlineSource
    | undefined;
  /** Cloud Storage location for the input content. */
  gcsSource?:
    | GcsSource
    | undefined;
  /** BigQuery input source. */
  bigquerySource?:
    | BigQuerySource
    | undefined;
  /**
   * Required. Parent DataStore resource name, of the form
   * `projects/{project}/locations/{location}/collections/{collection}/dataStores/{data_store}`
   */
  parent: string;
  /**
   * The desired location of errors incurred during the Import. Cannot be set
   * for inline user event imports.
   */
  errorConfig: ImportErrorConfig | undefined;
}

/** The inline source for the input config for ImportUserEvents method. */
export interface ImportUserEventsRequest_InlineSource {
  /** Required. A list of user events to import. Recommended max of 10k items. */
  userEvents: UserEvent[];
}

/**
 * Response of the ImportUserEventsRequest. If the long running
 * operation was successful, then this message is returned by the
 * google.longrunning.Operations.response field if the operation was successful.
 */
export interface ImportUserEventsResponse {
  /** A sample of errors encountered while processing the request. */
  errorSamples: Status[];
  /**
   * Echoes the destination for the complete errors if this field was set in
   * the request.
   */
  errorConfig:
    | ImportErrorConfig
    | undefined;
  /** Count of user events imported with complete existing Documents. */
  joinedEventsCount: Long;
  /**
   * Count of user events imported, but with Document information not found
   * in the existing Branch.
   */
  unjoinedEventsCount: Long;
}

/**
 * Metadata related to the progress of the Import operation. This is
 * returned by the google.longrunning.Operation.metadata field.
 */
export interface ImportUserEventsMetadata {
  /** Operation create time. */
  createTime:
    | Date
    | undefined;
  /**
   * Operation last update time. If the operation is done, this is also the
   * finish time.
   */
  updateTime:
    | Date
    | undefined;
  /** Count of entries that were processed successfully. */
  successCount: Long;
  /** Count of entries that encountered errors while processing. */
  failureCount: Long;
}

/**
 * Metadata related to the progress of the ImportDocuments operation. This is
 * returned by the google.longrunning.Operation.metadata field.
 */
export interface ImportDocumentsMetadata {
  /** Operation create time. */
  createTime:
    | Date
    | undefined;
  /**
   * Operation last update time. If the operation is done, this is also the
   * finish time.
   */
  updateTime:
    | Date
    | undefined;
  /** Count of entries that were processed successfully. */
  successCount: Long;
  /** Count of entries that encountered errors while processing. */
  failureCount: Long;
  /** Total count of entries that were processed. */
  totalCount: Long;
}

/** Request message for Import methods. */
export interface ImportDocumentsRequest {
  /** The Inline source for the input content for documents. */
  inlineSource?:
    | ImportDocumentsRequest_InlineSource
    | undefined;
  /** Cloud Storage location for the input content. */
  gcsSource?:
    | GcsSource
    | undefined;
  /** BigQuery input source. */
  bigquerySource?:
    | BigQuerySource
    | undefined;
  /** FhirStore input source. */
  fhirStoreSource?:
    | FhirStoreSource
    | undefined;
  /** Spanner input source. */
  spannerSource?:
    | SpannerSource
    | undefined;
  /** Cloud SQL input source. */
  cloudSqlSource?:
    | CloudSqlSource
    | undefined;
  /** Firestore input source. */
  firestoreSource?:
    | FirestoreSource
    | undefined;
  /** AlloyDB input source. */
  alloyDbSource?:
    | AlloyDbSource
    | undefined;
  /** Cloud Bigtable input source. */
  bigtableSource?:
    | BigtableSource
    | undefined;
  /**
   * Required. The parent branch resource name, such as
   * `projects/{project}/locations/{location}/collections/{collection}/dataStores/{data_store}/branches/{branch}`.
   * Requires create/update permission.
   */
  parent: string;
  /** The desired location of errors incurred during the Import. */
  errorConfig:
    | ImportErrorConfig
    | undefined;
  /**
   * The mode of reconciliation between existing documents and the documents to
   * be imported. Defaults to
   * [ReconciliationMode.INCREMENTAL][google.cloud.discoveryengine.v1beta.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL].
   */
  reconciliationMode: ImportDocumentsRequest_ReconciliationMode;
  /**
   * Indicates which fields in the provided imported documents to update. If
   * not set, the default is to update all fields.
   */
  updateMask:
    | string[]
    | undefined;
  /**
   * Whether to automatically generate IDs for the documents if absent.
   *
   * If set to `true`,
   * [Document.id][google.cloud.discoveryengine.v1beta.Document.id]s are
   * automatically generated based on the hash of the payload, where IDs may not
   * be consistent during multiple imports. In which case
   * [ReconciliationMode.FULL][google.cloud.discoveryengine.v1beta.ImportDocumentsRequest.ReconciliationMode.FULL]
   * is highly recommended to avoid duplicate contents. If unset or set to
   * `false`, [Document.id][google.cloud.discoveryengine.v1beta.Document.id]s
   * have to be specified using
   * [id_field][google.cloud.discoveryengine.v1beta.ImportDocumentsRequest.id_field],
   * otherwise, documents without IDs fail to be imported.
   *
   * Supported data sources:
   *
   * * [GcsSource][google.cloud.discoveryengine.v1beta.GcsSource].
   * [GcsSource.data_schema][google.cloud.discoveryengine.v1beta.GcsSource.data_schema]
   * must be `custom` or `csv`. Otherwise, an INVALID_ARGUMENT error is thrown.
   * * [BigQuerySource][google.cloud.discoveryengine.v1beta.BigQuerySource].
   * [BigQuerySource.data_schema][google.cloud.discoveryengine.v1beta.BigQuerySource.data_schema]
   * must be `custom` or `csv`. Otherwise, an INVALID_ARGUMENT error is thrown.
   * * [SpannerSource][google.cloud.discoveryengine.v1beta.SpannerSource].
   * * [CloudSqlSource][google.cloud.discoveryengine.v1beta.CloudSqlSource].
   * * [FirestoreSource][google.cloud.discoveryengine.v1beta.FirestoreSource].
   * * [BigtableSource][google.cloud.discoveryengine.v1beta.BigtableSource].
   */
  autoGenerateIds: boolean;
  /**
   * The field indicates the ID field or column to be used as unique IDs of
   * the documents.
   *
   * For [GcsSource][google.cloud.discoveryengine.v1beta.GcsSource] it is the
   * key of the JSON field. For instance, `my_id` for JSON `{"my_id":
   * "some_uuid"}`. For others, it may be the column name of the table where the
   * unique ids are stored.
   *
   * The values of the JSON field or the table column are used as the
   * [Document.id][google.cloud.discoveryengine.v1beta.Document.id]s. The JSON
   * field or the table column must be of string type, and the values must be
   * set as valid strings conform to
   * [RFC-1034](https://tools.ietf.org/html/rfc1034) with 1-63 characters.
   * Otherwise, documents without valid IDs fail to be imported.
   *
   * Only set this field when
   * [auto_generate_ids][google.cloud.discoveryengine.v1beta.ImportDocumentsRequest.auto_generate_ids]
   * is unset or set as `false`. Otherwise, an INVALID_ARGUMENT error is thrown.
   *
   * If it is unset, a default value `_id` is used when importing from the
   * allowed data sources.
   *
   * Supported data sources:
   *
   * * [GcsSource][google.cloud.discoveryengine.v1beta.GcsSource].
   * [GcsSource.data_schema][google.cloud.discoveryengine.v1beta.GcsSource.data_schema]
   * must be `custom` or `csv`. Otherwise, an INVALID_ARGUMENT error is thrown.
   * * [BigQuerySource][google.cloud.discoveryengine.v1beta.BigQuerySource].
   * [BigQuerySource.data_schema][google.cloud.discoveryengine.v1beta.BigQuerySource.data_schema]
   * must be `custom` or `csv`. Otherwise, an INVALID_ARGUMENT error is thrown.
   * * [SpannerSource][google.cloud.discoveryengine.v1beta.SpannerSource].
   * * [CloudSqlSource][google.cloud.discoveryengine.v1beta.CloudSqlSource].
   * * [FirestoreSource][google.cloud.discoveryengine.v1beta.FirestoreSource].
   * * [BigtableSource][google.cloud.discoveryengine.v1beta.BigtableSource].
   */
  idField: string;
}

/**
 * Indicates how imported documents are reconciled with the existing documents
 * created or imported before.
 */
export enum ImportDocumentsRequest_ReconciliationMode {
  /** RECONCILIATION_MODE_UNSPECIFIED - Defaults to `INCREMENTAL`. */
  RECONCILIATION_MODE_UNSPECIFIED = 0,
  /** INCREMENTAL - Inserts new documents or updates existing documents. */
  INCREMENTAL = 1,
  /**
   * FULL - Calculates diff and replaces the entire document dataset. Existing
   * documents may be deleted if they are not present in the source location.
   */
  FULL = 2,
  UNRECOGNIZED = -1,
}

export function importDocumentsRequest_ReconciliationModeFromJSON(
  object: any,
): ImportDocumentsRequest_ReconciliationMode {
  switch (object) {
    case 0:
    case "RECONCILIATION_MODE_UNSPECIFIED":
      return ImportDocumentsRequest_ReconciliationMode.RECONCILIATION_MODE_UNSPECIFIED;
    case 1:
    case "INCREMENTAL":
      return ImportDocumentsRequest_ReconciliationMode.INCREMENTAL;
    case 2:
    case "FULL":
      return ImportDocumentsRequest_ReconciliationMode.FULL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ImportDocumentsRequest_ReconciliationMode.UNRECOGNIZED;
  }
}

export function importDocumentsRequest_ReconciliationModeToJSON(
  object: ImportDocumentsRequest_ReconciliationMode,
): string {
  switch (object) {
    case ImportDocumentsRequest_ReconciliationMode.RECONCILIATION_MODE_UNSPECIFIED:
      return "RECONCILIATION_MODE_UNSPECIFIED";
    case ImportDocumentsRequest_ReconciliationMode.INCREMENTAL:
      return "INCREMENTAL";
    case ImportDocumentsRequest_ReconciliationMode.FULL:
      return "FULL";
    case ImportDocumentsRequest_ReconciliationMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The inline source for the input config for ImportDocuments method. */
export interface ImportDocumentsRequest_InlineSource {
  /**
   * Required. A list of documents to update/create. Each document must have a
   * valid [Document.id][google.cloud.discoveryengine.v1beta.Document.id].
   * Recommended max of 100 items.
   */
  documents: Document[];
}

/**
 * Response of the
 * [ImportDocumentsRequest][google.cloud.discoveryengine.v1beta.ImportDocumentsRequest].
 * If the long running operation is done, then this message is returned by the
 * google.longrunning.Operations.response field if the operation was successful.
 */
export interface ImportDocumentsResponse {
  /** A sample of errors encountered while processing the request. */
  errorSamples: Status[];
  /** Echoes the destination for the complete errors in the request if set. */
  errorConfig: ImportErrorConfig | undefined;
}

/**
 * Request message for
 * [CompletionService.ImportSuggestionDenyListEntries][google.cloud.discoveryengine.v1beta.CompletionService.ImportSuggestionDenyListEntries]
 * method.
 */
export interface ImportSuggestionDenyListEntriesRequest {
  /** The Inline source for the input content for suggestion deny list entries. */
  inlineSource?:
    | ImportSuggestionDenyListEntriesRequest_InlineSource
    | undefined;
  /**
   * Cloud Storage location for the input content.
   *
   * Only 1 file can be specified that contains all entries to import.
   * Supported values `gcs_source.schema` for autocomplete suggestion deny
   * list entry imports:
   *
   * * `suggestion_deny_list` (default): One JSON [SuggestionDenyListEntry]
   * per line.
   */
  gcsSource?:
    | GcsSource
    | undefined;
  /**
   * Required. The parent data store resource name for which to import denylist
   * entries. Follows pattern projects/* /locations/* /collections/* /dataStores/*.
   */
  parent: string;
}

/** The inline source for SuggestionDenyListEntry. */
export interface ImportSuggestionDenyListEntriesRequest_InlineSource {
  /** Required. A list of all denylist entries to import. Max of 1000 items. */
  entries: SuggestionDenyListEntry[];
}

/**
 * Response message for
 * [CompletionService.ImportSuggestionDenyListEntries][google.cloud.discoveryengine.v1beta.CompletionService.ImportSuggestionDenyListEntries]
 * method.
 */
export interface ImportSuggestionDenyListEntriesResponse {
  /** A sample of errors encountered while processing the request. */
  errorSamples: Status[];
  /** Count of deny list entries successfully imported. */
  importedEntriesCount: Long;
  /** Count of deny list entries that failed to be imported. */
  failedEntriesCount: Long;
}

/**
 * Metadata related to the progress of the ImportSuggestionDenyListEntries
 * operation. This is returned by the google.longrunning.Operation.metadata
 * field.
 */
export interface ImportSuggestionDenyListEntriesMetadata {
  /** Operation create time. */
  createTime:
    | Date
    | undefined;
  /**
   * Operation last update time. If the operation is done, this is also the
   * finish time.
   */
  updateTime: Date | undefined;
}

/**
 * Request message for
 * [CompletionService.ImportCompletionSuggestions][google.cloud.discoveryengine.v1beta.CompletionService.ImportCompletionSuggestions]
 * method.
 */
export interface ImportCompletionSuggestionsRequest {
  /** The Inline source for suggestion entries. */
  inlineSource?:
    | ImportCompletionSuggestionsRequest_InlineSource
    | undefined;
  /** Cloud Storage location for the input content. */
  gcsSource?:
    | GcsSource
    | undefined;
  /** BigQuery input source. */
  bigquerySource?:
    | BigQuerySource
    | undefined;
  /**
   * Required. The parent data store resource name for which to import customer
   * autocomplete suggestions.
   *
   * Follows pattern `projects/* /locations/* /collections/* /dataStores/*`
   */
  parent: string;
  /** The desired location of errors incurred during the Import. */
  errorConfig: ImportErrorConfig | undefined;
}

/** The inline source for CompletionSuggestions. */
export interface ImportCompletionSuggestionsRequest_InlineSource {
  /** Required. A list of all denylist entries to import. Max of 1000 items. */
  suggestions: CompletionSuggestion[];
}

/**
 * Response of the
 * [CompletionService.ImportCompletionSuggestions][google.cloud.discoveryengine.v1beta.CompletionService.ImportCompletionSuggestions]
 * method. If the long running operation is done, this message is returned by
 * the google.longrunning.Operations.response field if the operation is
 * successful.
 */
export interface ImportCompletionSuggestionsResponse {
  /** A sample of errors encountered while processing the request. */
  errorSamples: Status[];
  /** The desired location of errors incurred during the Import. */
  errorConfig: ImportErrorConfig | undefined;
}

/**
 * Metadata related to the progress of the ImportCompletionSuggestions
 * operation. This will be returned by the google.longrunning.Operation.metadata
 * field.
 */
export interface ImportCompletionSuggestionsMetadata {
  /** Operation create time. */
  createTime:
    | Date
    | undefined;
  /**
   * Operation last update time. If the operation is done, this is also the
   * finish time.
   */
  updateTime:
    | Date
    | undefined;
  /**
   * Count of
   * [CompletionSuggestion][google.cloud.discoveryengine.v1beta.CompletionSuggestion]s
   * successfully imported.
   */
  successCount: Long;
  /**
   * Count of
   * [CompletionSuggestion][google.cloud.discoveryengine.v1beta.CompletionSuggestion]s
   * that failed to be imported.
   */
  failureCount: Long;
}

/**
 * Request message for
 * [SampleQueryService.ImportSampleQueries][google.cloud.discoveryengine.v1beta.SampleQueryService.ImportSampleQueries]
 * method.
 */
export interface ImportSampleQueriesRequest {
  /** The Inline source for sample query entries. */
  inlineSource?:
    | ImportSampleQueriesRequest_InlineSource
    | undefined;
  /** Cloud Storage location for the input content. */
  gcsSource?:
    | GcsSource
    | undefined;
  /** BigQuery input source. */
  bigquerySource?:
    | BigQuerySource
    | undefined;
  /**
   * Required. The parent sample query set resource name, such as
   * `projects/{project}/locations/{location}/sampleQuerySets/{sampleQuerySet}`.
   *
   * If the caller does not have permission to list
   * [SampleQuery][google.cloud.discoveryengine.v1beta.SampleQuery]s under this
   * sample query set, regardless of whether or not this sample query set
   * exists, a `PERMISSION_DENIED` error is returned.
   */
  parent: string;
  /** The desired location of errors incurred during the Import. */
  errorConfig: ImportErrorConfig | undefined;
}

/**
 * The inline source for
 * [SampleQuery][google.cloud.discoveryengine.v1beta.SampleQuery]s.
 */
export interface ImportSampleQueriesRequest_InlineSource {
  /**
   * Required. A list of
   * [SampleQuery][google.cloud.discoveryengine.v1beta.SampleQuery]s to
   * import. Max of 1000 items.
   */
  sampleQueries: SampleQuery[];
}

/**
 * Response of the
 * [SampleQueryService.ImportSampleQueries][google.cloud.discoveryengine.v1beta.SampleQueryService.ImportSampleQueries]
 * method. If the long running operation is done, this message is returned by
 * the google.longrunning.Operations.response field if the operation is
 * successful.
 */
export interface ImportSampleQueriesResponse {
  /** A sample of errors encountered while processing the request. */
  errorSamples: Status[];
  /** The desired location of errors incurred during the Import. */
  errorConfig: ImportErrorConfig | undefined;
}

/**
 * Metadata related to the progress of the ImportSampleQueries
 * operation. This will be returned by the google.longrunning.Operation.metadata
 * field.
 */
export interface ImportSampleQueriesMetadata {
  /** ImportSampleQueries operation create time. */
  createTime:
    | Date
    | undefined;
  /**
   * ImportSampleQueries operation last update time. If the operation is done,
   * this is also the finish time.
   */
  updateTime:
    | Date
    | undefined;
  /**
   * Count of [SampleQuery][google.cloud.discoveryengine.v1beta.SampleQuery]s
   * successfully imported.
   */
  successCount: Long;
  /**
   * Count of [SampleQuery][google.cloud.discoveryengine.v1beta.SampleQuery]s
   * that failed to be imported.
   */
  failureCount: Long;
  /**
   * Total count of
   * [SampleQuery][google.cloud.discoveryengine.v1beta.SampleQuery]s that were
   * processed.
   */
  totalCount: Long;
}

function createBaseGcsSource(): GcsSource {
  return { inputUris: [], dataSchema: "" };
}

export const GcsSource: MessageFns<GcsSource> = {
  encode(message: GcsSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.inputUris) {
      writer.uint32(10).string(v!);
    }
    if (message.dataSchema !== "") {
      writer.uint32(18).string(message.dataSchema);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GcsSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGcsSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputUris.push(reader.string());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.dataSchema = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GcsSource {
    return {
      inputUris: globalThis.Array.isArray(object?.inputUris)
        ? object.inputUris.map((e: any) => globalThis.String(e))
        : [],
      dataSchema: isSet(object.dataSchema) ? globalThis.String(object.dataSchema) : "",
    };
  },

  toJSON(message: GcsSource): unknown {
    const obj: any = {};
    if (message.inputUris?.length) {
      obj.inputUris = message.inputUris;
    }
    if (message.dataSchema !== "") {
      obj.dataSchema = message.dataSchema;
    }
    return obj;
  },

  create(base?: DeepPartial<GcsSource>): GcsSource {
    return GcsSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GcsSource>): GcsSource {
    const message = createBaseGcsSource();
    message.inputUris = object.inputUris?.map((e) => e) || [];
    message.dataSchema = object.dataSchema ?? "";
    return message;
  },
};

function createBaseBigQuerySource(): BigQuerySource {
  return { partitionDate: undefined, projectId: "", datasetId: "", tableId: "", gcsStagingDir: "", dataSchema: "" };
}

export const BigQuerySource: MessageFns<BigQuerySource> = {
  encode(message: BigQuerySource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.partitionDate !== undefined) {
      DateMessage.encode(message.partitionDate, writer.uint32(42).fork()).join();
    }
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.datasetId !== "") {
      writer.uint32(18).string(message.datasetId);
    }
    if (message.tableId !== "") {
      writer.uint32(26).string(message.tableId);
    }
    if (message.gcsStagingDir !== "") {
      writer.uint32(34).string(message.gcsStagingDir);
    }
    if (message.dataSchema !== "") {
      writer.uint32(50).string(message.dataSchema);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigQuerySource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigQuerySource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 5:
          if (tag !== 42) {
            break;
          }

          message.partitionDate = DateMessage.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.tableId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.gcsStagingDir = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.dataSchema = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigQuerySource {
    return {
      partitionDate: isSet(object.partitionDate) ? DateMessage.fromJSON(object.partitionDate) : undefined,
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      datasetId: isSet(object.datasetId) ? globalThis.String(object.datasetId) : "",
      tableId: isSet(object.tableId) ? globalThis.String(object.tableId) : "",
      gcsStagingDir: isSet(object.gcsStagingDir) ? globalThis.String(object.gcsStagingDir) : "",
      dataSchema: isSet(object.dataSchema) ? globalThis.String(object.dataSchema) : "",
    };
  },

  toJSON(message: BigQuerySource): unknown {
    const obj: any = {};
    if (message.partitionDate !== undefined) {
      obj.partitionDate = DateMessage.toJSON(message.partitionDate);
    }
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    if (message.tableId !== "") {
      obj.tableId = message.tableId;
    }
    if (message.gcsStagingDir !== "") {
      obj.gcsStagingDir = message.gcsStagingDir;
    }
    if (message.dataSchema !== "") {
      obj.dataSchema = message.dataSchema;
    }
    return obj;
  },

  create(base?: DeepPartial<BigQuerySource>): BigQuerySource {
    return BigQuerySource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigQuerySource>): BigQuerySource {
    const message = createBaseBigQuerySource();
    message.partitionDate = (object.partitionDate !== undefined && object.partitionDate !== null)
      ? DateMessage.fromPartial(object.partitionDate)
      : undefined;
    message.projectId = object.projectId ?? "";
    message.datasetId = object.datasetId ?? "";
    message.tableId = object.tableId ?? "";
    message.gcsStagingDir = object.gcsStagingDir ?? "";
    message.dataSchema = object.dataSchema ?? "";
    return message;
  },
};

function createBaseSpannerSource(): SpannerSource {
  return { projectId: "", instanceId: "", databaseId: "", tableId: "", enableDataBoost: false };
}

export const SpannerSource: MessageFns<SpannerSource> = {
  encode(message: SpannerSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.instanceId !== "") {
      writer.uint32(18).string(message.instanceId);
    }
    if (message.databaseId !== "") {
      writer.uint32(26).string(message.databaseId);
    }
    if (message.tableId !== "") {
      writer.uint32(34).string(message.tableId);
    }
    if (message.enableDataBoost !== false) {
      writer.uint32(40).bool(message.enableDataBoost);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpannerSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpannerSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.instanceId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.databaseId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.tableId = reader.string();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.enableDataBoost = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpannerSource {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      instanceId: isSet(object.instanceId) ? globalThis.String(object.instanceId) : "",
      databaseId: isSet(object.databaseId) ? globalThis.String(object.databaseId) : "",
      tableId: isSet(object.tableId) ? globalThis.String(object.tableId) : "",
      enableDataBoost: isSet(object.enableDataBoost) ? globalThis.Boolean(object.enableDataBoost) : false,
    };
  },

  toJSON(message: SpannerSource): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.instanceId !== "") {
      obj.instanceId = message.instanceId;
    }
    if (message.databaseId !== "") {
      obj.databaseId = message.databaseId;
    }
    if (message.tableId !== "") {
      obj.tableId = message.tableId;
    }
    if (message.enableDataBoost !== false) {
      obj.enableDataBoost = message.enableDataBoost;
    }
    return obj;
  },

  create(base?: DeepPartial<SpannerSource>): SpannerSource {
    return SpannerSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SpannerSource>): SpannerSource {
    const message = createBaseSpannerSource();
    message.projectId = object.projectId ?? "";
    message.instanceId = object.instanceId ?? "";
    message.databaseId = object.databaseId ?? "";
    message.tableId = object.tableId ?? "";
    message.enableDataBoost = object.enableDataBoost ?? false;
    return message;
  },
};

function createBaseBigtableOptions(): BigtableOptions {
  return { keyFieldName: "", families: {} };
}

export const BigtableOptions: MessageFns<BigtableOptions> = {
  encode(message: BigtableOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.keyFieldName !== "") {
      writer.uint32(10).string(message.keyFieldName);
    }
    Object.entries(message.families).forEach(([key, value]) => {
      BigtableOptions_FamiliesEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigtableOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigtableOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.keyFieldName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = BigtableOptions_FamiliesEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.families[entry2.key] = entry2.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigtableOptions {
    return {
      keyFieldName: isSet(object.keyFieldName) ? globalThis.String(object.keyFieldName) : "",
      families: isObject(object.families)
        ? Object.entries(object.families).reduce<{ [key: string]: BigtableOptions_BigtableColumnFamily }>(
          (acc, [key, value]) => {
            acc[key] = BigtableOptions_BigtableColumnFamily.fromJSON(value);
            return acc;
          },
          {},
        )
        : {},
    };
  },

  toJSON(message: BigtableOptions): unknown {
    const obj: any = {};
    if (message.keyFieldName !== "") {
      obj.keyFieldName = message.keyFieldName;
    }
    if (message.families) {
      const entries = Object.entries(message.families);
      if (entries.length > 0) {
        obj.families = {};
        entries.forEach(([k, v]) => {
          obj.families[k] = BigtableOptions_BigtableColumnFamily.toJSON(v);
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<BigtableOptions>): BigtableOptions {
    return BigtableOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigtableOptions>): BigtableOptions {
    const message = createBaseBigtableOptions();
    message.keyFieldName = object.keyFieldName ?? "";
    message.families = Object.entries(object.families ?? {}).reduce<
      { [key: string]: BigtableOptions_BigtableColumnFamily }
    >((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = BigtableOptions_BigtableColumnFamily.fromPartial(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseBigtableOptions_BigtableColumnFamily(): BigtableOptions_BigtableColumnFamily {
  return { fieldName: "", encoding: 0, type: 0, columns: [] };
}

export const BigtableOptions_BigtableColumnFamily: MessageFns<BigtableOptions_BigtableColumnFamily> = {
  encode(message: BigtableOptions_BigtableColumnFamily, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.fieldName !== "") {
      writer.uint32(10).string(message.fieldName);
    }
    if (message.encoding !== 0) {
      writer.uint32(16).int32(message.encoding);
    }
    if (message.type !== 0) {
      writer.uint32(24).int32(message.type);
    }
    for (const v of message.columns) {
      BigtableOptions_BigtableColumn.encode(v!, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigtableOptions_BigtableColumnFamily {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigtableOptions_BigtableColumnFamily();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.fieldName = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.encoding = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.columns.push(BigtableOptions_BigtableColumn.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigtableOptions_BigtableColumnFamily {
    return {
      fieldName: isSet(object.fieldName) ? globalThis.String(object.fieldName) : "",
      encoding: isSet(object.encoding) ? bigtableOptions_EncodingFromJSON(object.encoding) : 0,
      type: isSet(object.type) ? bigtableOptions_TypeFromJSON(object.type) : 0,
      columns: globalThis.Array.isArray(object?.columns)
        ? object.columns.map((e: any) => BigtableOptions_BigtableColumn.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BigtableOptions_BigtableColumnFamily): unknown {
    const obj: any = {};
    if (message.fieldName !== "") {
      obj.fieldName = message.fieldName;
    }
    if (message.encoding !== 0) {
      obj.encoding = bigtableOptions_EncodingToJSON(message.encoding);
    }
    if (message.type !== 0) {
      obj.type = bigtableOptions_TypeToJSON(message.type);
    }
    if (message.columns?.length) {
      obj.columns = message.columns.map((e) => BigtableOptions_BigtableColumn.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BigtableOptions_BigtableColumnFamily>): BigtableOptions_BigtableColumnFamily {
    return BigtableOptions_BigtableColumnFamily.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigtableOptions_BigtableColumnFamily>): BigtableOptions_BigtableColumnFamily {
    const message = createBaseBigtableOptions_BigtableColumnFamily();
    message.fieldName = object.fieldName ?? "";
    message.encoding = object.encoding ?? 0;
    message.type = object.type ?? 0;
    message.columns = object.columns?.map((e) => BigtableOptions_BigtableColumn.fromPartial(e)) || [];
    return message;
  },
};

function createBaseBigtableOptions_BigtableColumn(): BigtableOptions_BigtableColumn {
  return { qualifier: Buffer.alloc(0), fieldName: "", encoding: 0, type: 0 };
}

export const BigtableOptions_BigtableColumn: MessageFns<BigtableOptions_BigtableColumn> = {
  encode(message: BigtableOptions_BigtableColumn, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.qualifier.length !== 0) {
      writer.uint32(10).bytes(message.qualifier);
    }
    if (message.fieldName !== "") {
      writer.uint32(18).string(message.fieldName);
    }
    if (message.encoding !== 0) {
      writer.uint32(24).int32(message.encoding);
    }
    if (message.type !== 0) {
      writer.uint32(32).int32(message.type);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigtableOptions_BigtableColumn {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigtableOptions_BigtableColumn();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.qualifier = Buffer.from(reader.bytes());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.fieldName = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.encoding = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigtableOptions_BigtableColumn {
    return {
      qualifier: isSet(object.qualifier) ? Buffer.from(bytesFromBase64(object.qualifier)) : Buffer.alloc(0),
      fieldName: isSet(object.fieldName) ? globalThis.String(object.fieldName) : "",
      encoding: isSet(object.encoding) ? bigtableOptions_EncodingFromJSON(object.encoding) : 0,
      type: isSet(object.type) ? bigtableOptions_TypeFromJSON(object.type) : 0,
    };
  },

  toJSON(message: BigtableOptions_BigtableColumn): unknown {
    const obj: any = {};
    if (message.qualifier.length !== 0) {
      obj.qualifier = base64FromBytes(message.qualifier);
    }
    if (message.fieldName !== "") {
      obj.fieldName = message.fieldName;
    }
    if (message.encoding !== 0) {
      obj.encoding = bigtableOptions_EncodingToJSON(message.encoding);
    }
    if (message.type !== 0) {
      obj.type = bigtableOptions_TypeToJSON(message.type);
    }
    return obj;
  },

  create(base?: DeepPartial<BigtableOptions_BigtableColumn>): BigtableOptions_BigtableColumn {
    return BigtableOptions_BigtableColumn.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigtableOptions_BigtableColumn>): BigtableOptions_BigtableColumn {
    const message = createBaseBigtableOptions_BigtableColumn();
    message.qualifier = object.qualifier ?? Buffer.alloc(0);
    message.fieldName = object.fieldName ?? "";
    message.encoding = object.encoding ?? 0;
    message.type = object.type ?? 0;
    return message;
  },
};

function createBaseBigtableOptions_FamiliesEntry(): BigtableOptions_FamiliesEntry {
  return { key: "", value: undefined };
}

export const BigtableOptions_FamiliesEntry: MessageFns<BigtableOptions_FamiliesEntry> = {
  encode(message: BigtableOptions_FamiliesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      BigtableOptions_BigtableColumnFamily.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigtableOptions_FamiliesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigtableOptions_FamiliesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = BigtableOptions_BigtableColumnFamily.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigtableOptions_FamiliesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? BigtableOptions_BigtableColumnFamily.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: BigtableOptions_FamiliesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = BigtableOptions_BigtableColumnFamily.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<BigtableOptions_FamiliesEntry>): BigtableOptions_FamiliesEntry {
    return BigtableOptions_FamiliesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigtableOptions_FamiliesEntry>): BigtableOptions_FamiliesEntry {
    const message = createBaseBigtableOptions_FamiliesEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? BigtableOptions_BigtableColumnFamily.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseBigtableSource(): BigtableSource {
  return { projectId: "", instanceId: "", tableId: "", bigtableOptions: undefined };
}

export const BigtableSource: MessageFns<BigtableSource> = {
  encode(message: BigtableSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.instanceId !== "") {
      writer.uint32(18).string(message.instanceId);
    }
    if (message.tableId !== "") {
      writer.uint32(26).string(message.tableId);
    }
    if (message.bigtableOptions !== undefined) {
      BigtableOptions.encode(message.bigtableOptions, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigtableSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigtableSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.instanceId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.tableId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.bigtableOptions = BigtableOptions.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigtableSource {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      instanceId: isSet(object.instanceId) ? globalThis.String(object.instanceId) : "",
      tableId: isSet(object.tableId) ? globalThis.String(object.tableId) : "",
      bigtableOptions: isSet(object.bigtableOptions) ? BigtableOptions.fromJSON(object.bigtableOptions) : undefined,
    };
  },

  toJSON(message: BigtableSource): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.instanceId !== "") {
      obj.instanceId = message.instanceId;
    }
    if (message.tableId !== "") {
      obj.tableId = message.tableId;
    }
    if (message.bigtableOptions !== undefined) {
      obj.bigtableOptions = BigtableOptions.toJSON(message.bigtableOptions);
    }
    return obj;
  },

  create(base?: DeepPartial<BigtableSource>): BigtableSource {
    return BigtableSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigtableSource>): BigtableSource {
    const message = createBaseBigtableSource();
    message.projectId = object.projectId ?? "";
    message.instanceId = object.instanceId ?? "";
    message.tableId = object.tableId ?? "";
    message.bigtableOptions = (object.bigtableOptions !== undefined && object.bigtableOptions !== null)
      ? BigtableOptions.fromPartial(object.bigtableOptions)
      : undefined;
    return message;
  },
};

function createBaseFhirStoreSource(): FhirStoreSource {
  return { fhirStore: "", gcsStagingDir: "", resourceTypes: [] };
}

export const FhirStoreSource: MessageFns<FhirStoreSource> = {
  encode(message: FhirStoreSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.fhirStore !== "") {
      writer.uint32(10).string(message.fhirStore);
    }
    if (message.gcsStagingDir !== "") {
      writer.uint32(18).string(message.gcsStagingDir);
    }
    for (const v of message.resourceTypes) {
      writer.uint32(26).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FhirStoreSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFhirStoreSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.fhirStore = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.gcsStagingDir = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.resourceTypes.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FhirStoreSource {
    return {
      fhirStore: isSet(object.fhirStore) ? globalThis.String(object.fhirStore) : "",
      gcsStagingDir: isSet(object.gcsStagingDir) ? globalThis.String(object.gcsStagingDir) : "",
      resourceTypes: globalThis.Array.isArray(object?.resourceTypes)
        ? object.resourceTypes.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: FhirStoreSource): unknown {
    const obj: any = {};
    if (message.fhirStore !== "") {
      obj.fhirStore = message.fhirStore;
    }
    if (message.gcsStagingDir !== "") {
      obj.gcsStagingDir = message.gcsStagingDir;
    }
    if (message.resourceTypes?.length) {
      obj.resourceTypes = message.resourceTypes;
    }
    return obj;
  },

  create(base?: DeepPartial<FhirStoreSource>): FhirStoreSource {
    return FhirStoreSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FhirStoreSource>): FhirStoreSource {
    const message = createBaseFhirStoreSource();
    message.fhirStore = object.fhirStore ?? "";
    message.gcsStagingDir = object.gcsStagingDir ?? "";
    message.resourceTypes = object.resourceTypes?.map((e) => e) || [];
    return message;
  },
};

function createBaseCloudSqlSource(): CloudSqlSource {
  return { projectId: "", instanceId: "", databaseId: "", tableId: "", gcsStagingDir: "", offload: false };
}

export const CloudSqlSource: MessageFns<CloudSqlSource> = {
  encode(message: CloudSqlSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.instanceId !== "") {
      writer.uint32(18).string(message.instanceId);
    }
    if (message.databaseId !== "") {
      writer.uint32(26).string(message.databaseId);
    }
    if (message.tableId !== "") {
      writer.uint32(34).string(message.tableId);
    }
    if (message.gcsStagingDir !== "") {
      writer.uint32(42).string(message.gcsStagingDir);
    }
    if (message.offload !== false) {
      writer.uint32(48).bool(message.offload);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CloudSqlSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCloudSqlSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.instanceId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.databaseId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.tableId = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.gcsStagingDir = reader.string();
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.offload = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CloudSqlSource {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      instanceId: isSet(object.instanceId) ? globalThis.String(object.instanceId) : "",
      databaseId: isSet(object.databaseId) ? globalThis.String(object.databaseId) : "",
      tableId: isSet(object.tableId) ? globalThis.String(object.tableId) : "",
      gcsStagingDir: isSet(object.gcsStagingDir) ? globalThis.String(object.gcsStagingDir) : "",
      offload: isSet(object.offload) ? globalThis.Boolean(object.offload) : false,
    };
  },

  toJSON(message: CloudSqlSource): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.instanceId !== "") {
      obj.instanceId = message.instanceId;
    }
    if (message.databaseId !== "") {
      obj.databaseId = message.databaseId;
    }
    if (message.tableId !== "") {
      obj.tableId = message.tableId;
    }
    if (message.gcsStagingDir !== "") {
      obj.gcsStagingDir = message.gcsStagingDir;
    }
    if (message.offload !== false) {
      obj.offload = message.offload;
    }
    return obj;
  },

  create(base?: DeepPartial<CloudSqlSource>): CloudSqlSource {
    return CloudSqlSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CloudSqlSource>): CloudSqlSource {
    const message = createBaseCloudSqlSource();
    message.projectId = object.projectId ?? "";
    message.instanceId = object.instanceId ?? "";
    message.databaseId = object.databaseId ?? "";
    message.tableId = object.tableId ?? "";
    message.gcsStagingDir = object.gcsStagingDir ?? "";
    message.offload = object.offload ?? false;
    return message;
  },
};

function createBaseAlloyDbSource(): AlloyDbSource {
  return { projectId: "", locationId: "", clusterId: "", databaseId: "", tableId: "", gcsStagingDir: "" };
}

export const AlloyDbSource: MessageFns<AlloyDbSource> = {
  encode(message: AlloyDbSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.locationId !== "") {
      writer.uint32(18).string(message.locationId);
    }
    if (message.clusterId !== "") {
      writer.uint32(26).string(message.clusterId);
    }
    if (message.databaseId !== "") {
      writer.uint32(34).string(message.databaseId);
    }
    if (message.tableId !== "") {
      writer.uint32(42).string(message.tableId);
    }
    if (message.gcsStagingDir !== "") {
      writer.uint32(50).string(message.gcsStagingDir);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AlloyDbSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAlloyDbSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.locationId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.clusterId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.databaseId = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.tableId = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.gcsStagingDir = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AlloyDbSource {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      locationId: isSet(object.locationId) ? globalThis.String(object.locationId) : "",
      clusterId: isSet(object.clusterId) ? globalThis.String(object.clusterId) : "",
      databaseId: isSet(object.databaseId) ? globalThis.String(object.databaseId) : "",
      tableId: isSet(object.tableId) ? globalThis.String(object.tableId) : "",
      gcsStagingDir: isSet(object.gcsStagingDir) ? globalThis.String(object.gcsStagingDir) : "",
    };
  },

  toJSON(message: AlloyDbSource): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.locationId !== "") {
      obj.locationId = message.locationId;
    }
    if (message.clusterId !== "") {
      obj.clusterId = message.clusterId;
    }
    if (message.databaseId !== "") {
      obj.databaseId = message.databaseId;
    }
    if (message.tableId !== "") {
      obj.tableId = message.tableId;
    }
    if (message.gcsStagingDir !== "") {
      obj.gcsStagingDir = message.gcsStagingDir;
    }
    return obj;
  },

  create(base?: DeepPartial<AlloyDbSource>): AlloyDbSource {
    return AlloyDbSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AlloyDbSource>): AlloyDbSource {
    const message = createBaseAlloyDbSource();
    message.projectId = object.projectId ?? "";
    message.locationId = object.locationId ?? "";
    message.clusterId = object.clusterId ?? "";
    message.databaseId = object.databaseId ?? "";
    message.tableId = object.tableId ?? "";
    message.gcsStagingDir = object.gcsStagingDir ?? "";
    return message;
  },
};

function createBaseFirestoreSource(): FirestoreSource {
  return { projectId: "", databaseId: "", collectionId: "", gcsStagingDir: "" };
}

export const FirestoreSource: MessageFns<FirestoreSource> = {
  encode(message: FirestoreSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.databaseId !== "") {
      writer.uint32(18).string(message.databaseId);
    }
    if (message.collectionId !== "") {
      writer.uint32(26).string(message.collectionId);
    }
    if (message.gcsStagingDir !== "") {
      writer.uint32(34).string(message.gcsStagingDir);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FirestoreSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFirestoreSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.databaseId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.collectionId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.gcsStagingDir = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FirestoreSource {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      databaseId: isSet(object.databaseId) ? globalThis.String(object.databaseId) : "",
      collectionId: isSet(object.collectionId) ? globalThis.String(object.collectionId) : "",
      gcsStagingDir: isSet(object.gcsStagingDir) ? globalThis.String(object.gcsStagingDir) : "",
    };
  },

  toJSON(message: FirestoreSource): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.databaseId !== "") {
      obj.databaseId = message.databaseId;
    }
    if (message.collectionId !== "") {
      obj.collectionId = message.collectionId;
    }
    if (message.gcsStagingDir !== "") {
      obj.gcsStagingDir = message.gcsStagingDir;
    }
    return obj;
  },

  create(base?: DeepPartial<FirestoreSource>): FirestoreSource {
    return FirestoreSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FirestoreSource>): FirestoreSource {
    const message = createBaseFirestoreSource();
    message.projectId = object.projectId ?? "";
    message.databaseId = object.databaseId ?? "";
    message.collectionId = object.collectionId ?? "";
    message.gcsStagingDir = object.gcsStagingDir ?? "";
    return message;
  },
};

function createBaseImportErrorConfig(): ImportErrorConfig {
  return { gcsPrefix: undefined };
}

export const ImportErrorConfig: MessageFns<ImportErrorConfig> = {
  encode(message: ImportErrorConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcsPrefix !== undefined) {
      writer.uint32(10).string(message.gcsPrefix);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportErrorConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportErrorConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.gcsPrefix = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportErrorConfig {
    return { gcsPrefix: isSet(object.gcsPrefix) ? globalThis.String(object.gcsPrefix) : undefined };
  },

  toJSON(message: ImportErrorConfig): unknown {
    const obj: any = {};
    if (message.gcsPrefix !== undefined) {
      obj.gcsPrefix = message.gcsPrefix;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportErrorConfig>): ImportErrorConfig {
    return ImportErrorConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportErrorConfig>): ImportErrorConfig {
    const message = createBaseImportErrorConfig();
    message.gcsPrefix = object.gcsPrefix ?? undefined;
    return message;
  },
};

function createBaseImportUserEventsRequest(): ImportUserEventsRequest {
  return {
    inlineSource: undefined,
    gcsSource: undefined,
    bigquerySource: undefined,
    parent: "",
    errorConfig: undefined,
  };
}

export const ImportUserEventsRequest: MessageFns<ImportUserEventsRequest> = {
  encode(message: ImportUserEventsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inlineSource !== undefined) {
      ImportUserEventsRequest_InlineSource.encode(message.inlineSource, writer.uint32(18).fork()).join();
    }
    if (message.gcsSource !== undefined) {
      GcsSource.encode(message.gcsSource, writer.uint32(26).fork()).join();
    }
    if (message.bigquerySource !== undefined) {
      BigQuerySource.encode(message.bigquerySource, writer.uint32(34).fork()).join();
    }
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.errorConfig !== undefined) {
      ImportErrorConfig.encode(message.errorConfig, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportUserEventsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportUserEventsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inlineSource = ImportUserEventsRequest_InlineSource.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.gcsSource = GcsSource.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.bigquerySource = BigQuerySource.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.errorConfig = ImportErrorConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportUserEventsRequest {
    return {
      inlineSource: isSet(object.inlineSource)
        ? ImportUserEventsRequest_InlineSource.fromJSON(object.inlineSource)
        : undefined,
      gcsSource: isSet(object.gcsSource) ? GcsSource.fromJSON(object.gcsSource) : undefined,
      bigquerySource: isSet(object.bigquerySource) ? BigQuerySource.fromJSON(object.bigquerySource) : undefined,
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      errorConfig: isSet(object.errorConfig) ? ImportErrorConfig.fromJSON(object.errorConfig) : undefined,
    };
  },

  toJSON(message: ImportUserEventsRequest): unknown {
    const obj: any = {};
    if (message.inlineSource !== undefined) {
      obj.inlineSource = ImportUserEventsRequest_InlineSource.toJSON(message.inlineSource);
    }
    if (message.gcsSource !== undefined) {
      obj.gcsSource = GcsSource.toJSON(message.gcsSource);
    }
    if (message.bigquerySource !== undefined) {
      obj.bigquerySource = BigQuerySource.toJSON(message.bigquerySource);
    }
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.errorConfig !== undefined) {
      obj.errorConfig = ImportErrorConfig.toJSON(message.errorConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<ImportUserEventsRequest>): ImportUserEventsRequest {
    return ImportUserEventsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportUserEventsRequest>): ImportUserEventsRequest {
    const message = createBaseImportUserEventsRequest();
    message.inlineSource = (object.inlineSource !== undefined && object.inlineSource !== null)
      ? ImportUserEventsRequest_InlineSource.fromPartial(object.inlineSource)
      : undefined;
    message.gcsSource = (object.gcsSource !== undefined && object.gcsSource !== null)
      ? GcsSource.fromPartial(object.gcsSource)
      : undefined;
    message.bigquerySource = (object.bigquerySource !== undefined && object.bigquerySource !== null)
      ? BigQuerySource.fromPartial(object.bigquerySource)
      : undefined;
    message.parent = object.parent ?? "";
    message.errorConfig = (object.errorConfig !== undefined && object.errorConfig !== null)
      ? ImportErrorConfig.fromPartial(object.errorConfig)
      : undefined;
    return message;
  },
};

function createBaseImportUserEventsRequest_InlineSource(): ImportUserEventsRequest_InlineSource {
  return { userEvents: [] };
}

export const ImportUserEventsRequest_InlineSource: MessageFns<ImportUserEventsRequest_InlineSource> = {
  encode(message: ImportUserEventsRequest_InlineSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.userEvents) {
      UserEvent.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportUserEventsRequest_InlineSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportUserEventsRequest_InlineSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.userEvents.push(UserEvent.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportUserEventsRequest_InlineSource {
    return {
      userEvents: globalThis.Array.isArray(object?.userEvents)
        ? object.userEvents.map((e: any) => UserEvent.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ImportUserEventsRequest_InlineSource): unknown {
    const obj: any = {};
    if (message.userEvents?.length) {
      obj.userEvents = message.userEvents.map((e) => UserEvent.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ImportUserEventsRequest_InlineSource>): ImportUserEventsRequest_InlineSource {
    return ImportUserEventsRequest_InlineSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportUserEventsRequest_InlineSource>): ImportUserEventsRequest_InlineSource {
    const message = createBaseImportUserEventsRequest_InlineSource();
    message.userEvents = object.userEvents?.map((e) => UserEvent.fromPartial(e)) || [];
    return message;
  },
};

function createBaseImportUserEventsResponse(): ImportUserEventsResponse {
  return { errorSamples: [], errorConfig: undefined, joinedEventsCount: Long.ZERO, unjoinedEventsCount: Long.ZERO };
}

export const ImportUserEventsResponse: MessageFns<ImportUserEventsResponse> = {
  encode(message: ImportUserEventsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.errorSamples) {
      Status.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.errorConfig !== undefined) {
      ImportErrorConfig.encode(message.errorConfig, writer.uint32(18).fork()).join();
    }
    if (!message.joinedEventsCount.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.joinedEventsCount.toString());
    }
    if (!message.unjoinedEventsCount.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.unjoinedEventsCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportUserEventsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportUserEventsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.errorSamples.push(Status.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.errorConfig = ImportErrorConfig.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.joinedEventsCount = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.unjoinedEventsCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportUserEventsResponse {
    return {
      errorSamples: globalThis.Array.isArray(object?.errorSamples)
        ? object.errorSamples.map((e: any) => Status.fromJSON(e))
        : [],
      errorConfig: isSet(object.errorConfig) ? ImportErrorConfig.fromJSON(object.errorConfig) : undefined,
      joinedEventsCount: isSet(object.joinedEventsCount) ? Long.fromValue(object.joinedEventsCount) : Long.ZERO,
      unjoinedEventsCount: isSet(object.unjoinedEventsCount) ? Long.fromValue(object.unjoinedEventsCount) : Long.ZERO,
    };
  },

  toJSON(message: ImportUserEventsResponse): unknown {
    const obj: any = {};
    if (message.errorSamples?.length) {
      obj.errorSamples = message.errorSamples.map((e) => Status.toJSON(e));
    }
    if (message.errorConfig !== undefined) {
      obj.errorConfig = ImportErrorConfig.toJSON(message.errorConfig);
    }
    if (!message.joinedEventsCount.equals(Long.ZERO)) {
      obj.joinedEventsCount = (message.joinedEventsCount || Long.ZERO).toString();
    }
    if (!message.unjoinedEventsCount.equals(Long.ZERO)) {
      obj.unjoinedEventsCount = (message.unjoinedEventsCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ImportUserEventsResponse>): ImportUserEventsResponse {
    return ImportUserEventsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportUserEventsResponse>): ImportUserEventsResponse {
    const message = createBaseImportUserEventsResponse();
    message.errorSamples = object.errorSamples?.map((e) => Status.fromPartial(e)) || [];
    message.errorConfig = (object.errorConfig !== undefined && object.errorConfig !== null)
      ? ImportErrorConfig.fromPartial(object.errorConfig)
      : undefined;
    message.joinedEventsCount = (object.joinedEventsCount !== undefined && object.joinedEventsCount !== null)
      ? Long.fromValue(object.joinedEventsCount)
      : Long.ZERO;
    message.unjoinedEventsCount = (object.unjoinedEventsCount !== undefined && object.unjoinedEventsCount !== null)
      ? Long.fromValue(object.unjoinedEventsCount)
      : Long.ZERO;
    return message;
  },
};

function createBaseImportUserEventsMetadata(): ImportUserEventsMetadata {
  return { createTime: undefined, updateTime: undefined, successCount: Long.ZERO, failureCount: Long.ZERO };
}

export const ImportUserEventsMetadata: MessageFns<ImportUserEventsMetadata> = {
  encode(message: ImportUserEventsMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(10).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(18).fork()).join();
    }
    if (!message.successCount.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.successCount.toString());
    }
    if (!message.failureCount.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.failureCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportUserEventsMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportUserEventsMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.successCount = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.failureCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportUserEventsMetadata {
    return {
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      successCount: isSet(object.successCount) ? Long.fromValue(object.successCount) : Long.ZERO,
      failureCount: isSet(object.failureCount) ? Long.fromValue(object.failureCount) : Long.ZERO,
    };
  },

  toJSON(message: ImportUserEventsMetadata): unknown {
    const obj: any = {};
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (!message.successCount.equals(Long.ZERO)) {
      obj.successCount = (message.successCount || Long.ZERO).toString();
    }
    if (!message.failureCount.equals(Long.ZERO)) {
      obj.failureCount = (message.failureCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ImportUserEventsMetadata>): ImportUserEventsMetadata {
    return ImportUserEventsMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportUserEventsMetadata>): ImportUserEventsMetadata {
    const message = createBaseImportUserEventsMetadata();
    message.createTime = object.createTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.successCount = (object.successCount !== undefined && object.successCount !== null)
      ? Long.fromValue(object.successCount)
      : Long.ZERO;
    message.failureCount = (object.failureCount !== undefined && object.failureCount !== null)
      ? Long.fromValue(object.failureCount)
      : Long.ZERO;
    return message;
  },
};

function createBaseImportDocumentsMetadata(): ImportDocumentsMetadata {
  return {
    createTime: undefined,
    updateTime: undefined,
    successCount: Long.ZERO,
    failureCount: Long.ZERO,
    totalCount: Long.ZERO,
  };
}

export const ImportDocumentsMetadata: MessageFns<ImportDocumentsMetadata> = {
  encode(message: ImportDocumentsMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(10).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(18).fork()).join();
    }
    if (!message.successCount.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.successCount.toString());
    }
    if (!message.failureCount.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.failureCount.toString());
    }
    if (!message.totalCount.equals(Long.ZERO)) {
      writer.uint32(40).int64(message.totalCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportDocumentsMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportDocumentsMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.successCount = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.failureCount = Long.fromString(reader.int64().toString());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.totalCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportDocumentsMetadata {
    return {
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      successCount: isSet(object.successCount) ? Long.fromValue(object.successCount) : Long.ZERO,
      failureCount: isSet(object.failureCount) ? Long.fromValue(object.failureCount) : Long.ZERO,
      totalCount: isSet(object.totalCount) ? Long.fromValue(object.totalCount) : Long.ZERO,
    };
  },

  toJSON(message: ImportDocumentsMetadata): unknown {
    const obj: any = {};
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (!message.successCount.equals(Long.ZERO)) {
      obj.successCount = (message.successCount || Long.ZERO).toString();
    }
    if (!message.failureCount.equals(Long.ZERO)) {
      obj.failureCount = (message.failureCount || Long.ZERO).toString();
    }
    if (!message.totalCount.equals(Long.ZERO)) {
      obj.totalCount = (message.totalCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ImportDocumentsMetadata>): ImportDocumentsMetadata {
    return ImportDocumentsMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportDocumentsMetadata>): ImportDocumentsMetadata {
    const message = createBaseImportDocumentsMetadata();
    message.createTime = object.createTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.successCount = (object.successCount !== undefined && object.successCount !== null)
      ? Long.fromValue(object.successCount)
      : Long.ZERO;
    message.failureCount = (object.failureCount !== undefined && object.failureCount !== null)
      ? Long.fromValue(object.failureCount)
      : Long.ZERO;
    message.totalCount = (object.totalCount !== undefined && object.totalCount !== null)
      ? Long.fromValue(object.totalCount)
      : Long.ZERO;
    return message;
  },
};

function createBaseImportDocumentsRequest(): ImportDocumentsRequest {
  return {
    inlineSource: undefined,
    gcsSource: undefined,
    bigquerySource: undefined,
    fhirStoreSource: undefined,
    spannerSource: undefined,
    cloudSqlSource: undefined,
    firestoreSource: undefined,
    alloyDbSource: undefined,
    bigtableSource: undefined,
    parent: "",
    errorConfig: undefined,
    reconciliationMode: 0,
    updateMask: undefined,
    autoGenerateIds: false,
    idField: "",
  };
}

export const ImportDocumentsRequest: MessageFns<ImportDocumentsRequest> = {
  encode(message: ImportDocumentsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inlineSource !== undefined) {
      ImportDocumentsRequest_InlineSource.encode(message.inlineSource, writer.uint32(18).fork()).join();
    }
    if (message.gcsSource !== undefined) {
      GcsSource.encode(message.gcsSource, writer.uint32(26).fork()).join();
    }
    if (message.bigquerySource !== undefined) {
      BigQuerySource.encode(message.bigquerySource, writer.uint32(34).fork()).join();
    }
    if (message.fhirStoreSource !== undefined) {
      FhirStoreSource.encode(message.fhirStoreSource, writer.uint32(82).fork()).join();
    }
    if (message.spannerSource !== undefined) {
      SpannerSource.encode(message.spannerSource, writer.uint32(90).fork()).join();
    }
    if (message.cloudSqlSource !== undefined) {
      CloudSqlSource.encode(message.cloudSqlSource, writer.uint32(98).fork()).join();
    }
    if (message.firestoreSource !== undefined) {
      FirestoreSource.encode(message.firestoreSource, writer.uint32(106).fork()).join();
    }
    if (message.alloyDbSource !== undefined) {
      AlloyDbSource.encode(message.alloyDbSource, writer.uint32(114).fork()).join();
    }
    if (message.bigtableSource !== undefined) {
      BigtableSource.encode(message.bigtableSource, writer.uint32(122).fork()).join();
    }
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.errorConfig !== undefined) {
      ImportErrorConfig.encode(message.errorConfig, writer.uint32(42).fork()).join();
    }
    if (message.reconciliationMode !== 0) {
      writer.uint32(48).int32(message.reconciliationMode);
    }
    if (message.updateMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.updateMask), writer.uint32(58).fork()).join();
    }
    if (message.autoGenerateIds !== false) {
      writer.uint32(64).bool(message.autoGenerateIds);
    }
    if (message.idField !== "") {
      writer.uint32(74).string(message.idField);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportDocumentsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportDocumentsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inlineSource = ImportDocumentsRequest_InlineSource.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.gcsSource = GcsSource.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.bigquerySource = BigQuerySource.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.fhirStoreSource = FhirStoreSource.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.spannerSource = SpannerSource.decode(reader, reader.uint32());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.cloudSqlSource = CloudSqlSource.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.firestoreSource = FirestoreSource.decode(reader, reader.uint32());
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.alloyDbSource = AlloyDbSource.decode(reader, reader.uint32());
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.bigtableSource = BigtableSource.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.errorConfig = ImportErrorConfig.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.reconciliationMode = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.updateMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.autoGenerateIds = reader.bool();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.idField = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportDocumentsRequest {
    return {
      inlineSource: isSet(object.inlineSource)
        ? ImportDocumentsRequest_InlineSource.fromJSON(object.inlineSource)
        : undefined,
      gcsSource: isSet(object.gcsSource) ? GcsSource.fromJSON(object.gcsSource) : undefined,
      bigquerySource: isSet(object.bigquerySource) ? BigQuerySource.fromJSON(object.bigquerySource) : undefined,
      fhirStoreSource: isSet(object.fhirStoreSource) ? FhirStoreSource.fromJSON(object.fhirStoreSource) : undefined,
      spannerSource: isSet(object.spannerSource) ? SpannerSource.fromJSON(object.spannerSource) : undefined,
      cloudSqlSource: isSet(object.cloudSqlSource) ? CloudSqlSource.fromJSON(object.cloudSqlSource) : undefined,
      firestoreSource: isSet(object.firestoreSource) ? FirestoreSource.fromJSON(object.firestoreSource) : undefined,
      alloyDbSource: isSet(object.alloyDbSource) ? AlloyDbSource.fromJSON(object.alloyDbSource) : undefined,
      bigtableSource: isSet(object.bigtableSource) ? BigtableSource.fromJSON(object.bigtableSource) : undefined,
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      errorConfig: isSet(object.errorConfig) ? ImportErrorConfig.fromJSON(object.errorConfig) : undefined,
      reconciliationMode: isSet(object.reconciliationMode)
        ? importDocumentsRequest_ReconciliationModeFromJSON(object.reconciliationMode)
        : 0,
      updateMask: isSet(object.updateMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.updateMask)) : undefined,
      autoGenerateIds: isSet(object.autoGenerateIds) ? globalThis.Boolean(object.autoGenerateIds) : false,
      idField: isSet(object.idField) ? globalThis.String(object.idField) : "",
    };
  },

  toJSON(message: ImportDocumentsRequest): unknown {
    const obj: any = {};
    if (message.inlineSource !== undefined) {
      obj.inlineSource = ImportDocumentsRequest_InlineSource.toJSON(message.inlineSource);
    }
    if (message.gcsSource !== undefined) {
      obj.gcsSource = GcsSource.toJSON(message.gcsSource);
    }
    if (message.bigquerySource !== undefined) {
      obj.bigquerySource = BigQuerySource.toJSON(message.bigquerySource);
    }
    if (message.fhirStoreSource !== undefined) {
      obj.fhirStoreSource = FhirStoreSource.toJSON(message.fhirStoreSource);
    }
    if (message.spannerSource !== undefined) {
      obj.spannerSource = SpannerSource.toJSON(message.spannerSource);
    }
    if (message.cloudSqlSource !== undefined) {
      obj.cloudSqlSource = CloudSqlSource.toJSON(message.cloudSqlSource);
    }
    if (message.firestoreSource !== undefined) {
      obj.firestoreSource = FirestoreSource.toJSON(message.firestoreSource);
    }
    if (message.alloyDbSource !== undefined) {
      obj.alloyDbSource = AlloyDbSource.toJSON(message.alloyDbSource);
    }
    if (message.bigtableSource !== undefined) {
      obj.bigtableSource = BigtableSource.toJSON(message.bigtableSource);
    }
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.errorConfig !== undefined) {
      obj.errorConfig = ImportErrorConfig.toJSON(message.errorConfig);
    }
    if (message.reconciliationMode !== 0) {
      obj.reconciliationMode = importDocumentsRequest_ReconciliationModeToJSON(message.reconciliationMode);
    }
    if (message.updateMask !== undefined) {
      obj.updateMask = FieldMask.toJSON(FieldMask.wrap(message.updateMask));
    }
    if (message.autoGenerateIds !== false) {
      obj.autoGenerateIds = message.autoGenerateIds;
    }
    if (message.idField !== "") {
      obj.idField = message.idField;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportDocumentsRequest>): ImportDocumentsRequest {
    return ImportDocumentsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportDocumentsRequest>): ImportDocumentsRequest {
    const message = createBaseImportDocumentsRequest();
    message.inlineSource = (object.inlineSource !== undefined && object.inlineSource !== null)
      ? ImportDocumentsRequest_InlineSource.fromPartial(object.inlineSource)
      : undefined;
    message.gcsSource = (object.gcsSource !== undefined && object.gcsSource !== null)
      ? GcsSource.fromPartial(object.gcsSource)
      : undefined;
    message.bigquerySource = (object.bigquerySource !== undefined && object.bigquerySource !== null)
      ? BigQuerySource.fromPartial(object.bigquerySource)
      : undefined;
    message.fhirStoreSource = (object.fhirStoreSource !== undefined && object.fhirStoreSource !== null)
      ? FhirStoreSource.fromPartial(object.fhirStoreSource)
      : undefined;
    message.spannerSource = (object.spannerSource !== undefined && object.spannerSource !== null)
      ? SpannerSource.fromPartial(object.spannerSource)
      : undefined;
    message.cloudSqlSource = (object.cloudSqlSource !== undefined && object.cloudSqlSource !== null)
      ? CloudSqlSource.fromPartial(object.cloudSqlSource)
      : undefined;
    message.firestoreSource = (object.firestoreSource !== undefined && object.firestoreSource !== null)
      ? FirestoreSource.fromPartial(object.firestoreSource)
      : undefined;
    message.alloyDbSource = (object.alloyDbSource !== undefined && object.alloyDbSource !== null)
      ? AlloyDbSource.fromPartial(object.alloyDbSource)
      : undefined;
    message.bigtableSource = (object.bigtableSource !== undefined && object.bigtableSource !== null)
      ? BigtableSource.fromPartial(object.bigtableSource)
      : undefined;
    message.parent = object.parent ?? "";
    message.errorConfig = (object.errorConfig !== undefined && object.errorConfig !== null)
      ? ImportErrorConfig.fromPartial(object.errorConfig)
      : undefined;
    message.reconciliationMode = object.reconciliationMode ?? 0;
    message.updateMask = object.updateMask ?? undefined;
    message.autoGenerateIds = object.autoGenerateIds ?? false;
    message.idField = object.idField ?? "";
    return message;
  },
};

function createBaseImportDocumentsRequest_InlineSource(): ImportDocumentsRequest_InlineSource {
  return { documents: [] };
}

export const ImportDocumentsRequest_InlineSource: MessageFns<ImportDocumentsRequest_InlineSource> = {
  encode(message: ImportDocumentsRequest_InlineSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.documents) {
      Document.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportDocumentsRequest_InlineSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportDocumentsRequest_InlineSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.documents.push(Document.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportDocumentsRequest_InlineSource {
    return {
      documents: globalThis.Array.isArray(object?.documents)
        ? object.documents.map((e: any) => Document.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ImportDocumentsRequest_InlineSource): unknown {
    const obj: any = {};
    if (message.documents?.length) {
      obj.documents = message.documents.map((e) => Document.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ImportDocumentsRequest_InlineSource>): ImportDocumentsRequest_InlineSource {
    return ImportDocumentsRequest_InlineSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportDocumentsRequest_InlineSource>): ImportDocumentsRequest_InlineSource {
    const message = createBaseImportDocumentsRequest_InlineSource();
    message.documents = object.documents?.map((e) => Document.fromPartial(e)) || [];
    return message;
  },
};

function createBaseImportDocumentsResponse(): ImportDocumentsResponse {
  return { errorSamples: [], errorConfig: undefined };
}

export const ImportDocumentsResponse: MessageFns<ImportDocumentsResponse> = {
  encode(message: ImportDocumentsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.errorSamples) {
      Status.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.errorConfig !== undefined) {
      ImportErrorConfig.encode(message.errorConfig, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportDocumentsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportDocumentsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.errorSamples.push(Status.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.errorConfig = ImportErrorConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportDocumentsResponse {
    return {
      errorSamples: globalThis.Array.isArray(object?.errorSamples)
        ? object.errorSamples.map((e: any) => Status.fromJSON(e))
        : [],
      errorConfig: isSet(object.errorConfig) ? ImportErrorConfig.fromJSON(object.errorConfig) : undefined,
    };
  },

  toJSON(message: ImportDocumentsResponse): unknown {
    const obj: any = {};
    if (message.errorSamples?.length) {
      obj.errorSamples = message.errorSamples.map((e) => Status.toJSON(e));
    }
    if (message.errorConfig !== undefined) {
      obj.errorConfig = ImportErrorConfig.toJSON(message.errorConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<ImportDocumentsResponse>): ImportDocumentsResponse {
    return ImportDocumentsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportDocumentsResponse>): ImportDocumentsResponse {
    const message = createBaseImportDocumentsResponse();
    message.errorSamples = object.errorSamples?.map((e) => Status.fromPartial(e)) || [];
    message.errorConfig = (object.errorConfig !== undefined && object.errorConfig !== null)
      ? ImportErrorConfig.fromPartial(object.errorConfig)
      : undefined;
    return message;
  },
};

function createBaseImportSuggestionDenyListEntriesRequest(): ImportSuggestionDenyListEntriesRequest {
  return { inlineSource: undefined, gcsSource: undefined, parent: "" };
}

export const ImportSuggestionDenyListEntriesRequest: MessageFns<ImportSuggestionDenyListEntriesRequest> = {
  encode(message: ImportSuggestionDenyListEntriesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inlineSource !== undefined) {
      ImportSuggestionDenyListEntriesRequest_InlineSource.encode(message.inlineSource, writer.uint32(18).fork()).join();
    }
    if (message.gcsSource !== undefined) {
      GcsSource.encode(message.gcsSource, writer.uint32(26).fork()).join();
    }
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportSuggestionDenyListEntriesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportSuggestionDenyListEntriesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inlineSource = ImportSuggestionDenyListEntriesRequest_InlineSource.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.gcsSource = GcsSource.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportSuggestionDenyListEntriesRequest {
    return {
      inlineSource: isSet(object.inlineSource)
        ? ImportSuggestionDenyListEntriesRequest_InlineSource.fromJSON(object.inlineSource)
        : undefined,
      gcsSource: isSet(object.gcsSource) ? GcsSource.fromJSON(object.gcsSource) : undefined,
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
    };
  },

  toJSON(message: ImportSuggestionDenyListEntriesRequest): unknown {
    const obj: any = {};
    if (message.inlineSource !== undefined) {
      obj.inlineSource = ImportSuggestionDenyListEntriesRequest_InlineSource.toJSON(message.inlineSource);
    }
    if (message.gcsSource !== undefined) {
      obj.gcsSource = GcsSource.toJSON(message.gcsSource);
    }
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportSuggestionDenyListEntriesRequest>): ImportSuggestionDenyListEntriesRequest {
    return ImportSuggestionDenyListEntriesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportSuggestionDenyListEntriesRequest>): ImportSuggestionDenyListEntriesRequest {
    const message = createBaseImportSuggestionDenyListEntriesRequest();
    message.inlineSource = (object.inlineSource !== undefined && object.inlineSource !== null)
      ? ImportSuggestionDenyListEntriesRequest_InlineSource.fromPartial(object.inlineSource)
      : undefined;
    message.gcsSource = (object.gcsSource !== undefined && object.gcsSource !== null)
      ? GcsSource.fromPartial(object.gcsSource)
      : undefined;
    message.parent = object.parent ?? "";
    return message;
  },
};

function createBaseImportSuggestionDenyListEntriesRequest_InlineSource(): ImportSuggestionDenyListEntriesRequest_InlineSource {
  return { entries: [] };
}

export const ImportSuggestionDenyListEntriesRequest_InlineSource: MessageFns<
  ImportSuggestionDenyListEntriesRequest_InlineSource
> = {
  encode(
    message: ImportSuggestionDenyListEntriesRequest_InlineSource,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    for (const v of message.entries) {
      SuggestionDenyListEntry.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportSuggestionDenyListEntriesRequest_InlineSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportSuggestionDenyListEntriesRequest_InlineSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.entries.push(SuggestionDenyListEntry.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportSuggestionDenyListEntriesRequest_InlineSource {
    return {
      entries: globalThis.Array.isArray(object?.entries)
        ? object.entries.map((e: any) => SuggestionDenyListEntry.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ImportSuggestionDenyListEntriesRequest_InlineSource): unknown {
    const obj: any = {};
    if (message.entries?.length) {
      obj.entries = message.entries.map((e) => SuggestionDenyListEntry.toJSON(e));
    }
    return obj;
  },

  create(
    base?: DeepPartial<ImportSuggestionDenyListEntriesRequest_InlineSource>,
  ): ImportSuggestionDenyListEntriesRequest_InlineSource {
    return ImportSuggestionDenyListEntriesRequest_InlineSource.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ImportSuggestionDenyListEntriesRequest_InlineSource>,
  ): ImportSuggestionDenyListEntriesRequest_InlineSource {
    const message = createBaseImportSuggestionDenyListEntriesRequest_InlineSource();
    message.entries = object.entries?.map((e) => SuggestionDenyListEntry.fromPartial(e)) || [];
    return message;
  },
};

function createBaseImportSuggestionDenyListEntriesResponse(): ImportSuggestionDenyListEntriesResponse {
  return { errorSamples: [], importedEntriesCount: Long.ZERO, failedEntriesCount: Long.ZERO };
}

export const ImportSuggestionDenyListEntriesResponse: MessageFns<ImportSuggestionDenyListEntriesResponse> = {
  encode(message: ImportSuggestionDenyListEntriesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.errorSamples) {
      Status.encode(v!, writer.uint32(10).fork()).join();
    }
    if (!message.importedEntriesCount.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.importedEntriesCount.toString());
    }
    if (!message.failedEntriesCount.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.failedEntriesCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportSuggestionDenyListEntriesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportSuggestionDenyListEntriesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.errorSamples.push(Status.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.importedEntriesCount = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.failedEntriesCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportSuggestionDenyListEntriesResponse {
    return {
      errorSamples: globalThis.Array.isArray(object?.errorSamples)
        ? object.errorSamples.map((e: any) => Status.fromJSON(e))
        : [],
      importedEntriesCount: isSet(object.importedEntriesCount)
        ? Long.fromValue(object.importedEntriesCount)
        : Long.ZERO,
      failedEntriesCount: isSet(object.failedEntriesCount) ? Long.fromValue(object.failedEntriesCount) : Long.ZERO,
    };
  },

  toJSON(message: ImportSuggestionDenyListEntriesResponse): unknown {
    const obj: any = {};
    if (message.errorSamples?.length) {
      obj.errorSamples = message.errorSamples.map((e) => Status.toJSON(e));
    }
    if (!message.importedEntriesCount.equals(Long.ZERO)) {
      obj.importedEntriesCount = (message.importedEntriesCount || Long.ZERO).toString();
    }
    if (!message.failedEntriesCount.equals(Long.ZERO)) {
      obj.failedEntriesCount = (message.failedEntriesCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ImportSuggestionDenyListEntriesResponse>): ImportSuggestionDenyListEntriesResponse {
    return ImportSuggestionDenyListEntriesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportSuggestionDenyListEntriesResponse>): ImportSuggestionDenyListEntriesResponse {
    const message = createBaseImportSuggestionDenyListEntriesResponse();
    message.errorSamples = object.errorSamples?.map((e) => Status.fromPartial(e)) || [];
    message.importedEntriesCount = (object.importedEntriesCount !== undefined && object.importedEntriesCount !== null)
      ? Long.fromValue(object.importedEntriesCount)
      : Long.ZERO;
    message.failedEntriesCount = (object.failedEntriesCount !== undefined && object.failedEntriesCount !== null)
      ? Long.fromValue(object.failedEntriesCount)
      : Long.ZERO;
    return message;
  },
};

function createBaseImportSuggestionDenyListEntriesMetadata(): ImportSuggestionDenyListEntriesMetadata {
  return { createTime: undefined, updateTime: undefined };
}

export const ImportSuggestionDenyListEntriesMetadata: MessageFns<ImportSuggestionDenyListEntriesMetadata> = {
  encode(message: ImportSuggestionDenyListEntriesMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(10).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportSuggestionDenyListEntriesMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportSuggestionDenyListEntriesMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportSuggestionDenyListEntriesMetadata {
    return {
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
    };
  },

  toJSON(message: ImportSuggestionDenyListEntriesMetadata): unknown {
    const obj: any = {};
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<ImportSuggestionDenyListEntriesMetadata>): ImportSuggestionDenyListEntriesMetadata {
    return ImportSuggestionDenyListEntriesMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportSuggestionDenyListEntriesMetadata>): ImportSuggestionDenyListEntriesMetadata {
    const message = createBaseImportSuggestionDenyListEntriesMetadata();
    message.createTime = object.createTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    return message;
  },
};

function createBaseImportCompletionSuggestionsRequest(): ImportCompletionSuggestionsRequest {
  return {
    inlineSource: undefined,
    gcsSource: undefined,
    bigquerySource: undefined,
    parent: "",
    errorConfig: undefined,
  };
}

export const ImportCompletionSuggestionsRequest: MessageFns<ImportCompletionSuggestionsRequest> = {
  encode(message: ImportCompletionSuggestionsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inlineSource !== undefined) {
      ImportCompletionSuggestionsRequest_InlineSource.encode(message.inlineSource, writer.uint32(18).fork()).join();
    }
    if (message.gcsSource !== undefined) {
      GcsSource.encode(message.gcsSource, writer.uint32(26).fork()).join();
    }
    if (message.bigquerySource !== undefined) {
      BigQuerySource.encode(message.bigquerySource, writer.uint32(34).fork()).join();
    }
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.errorConfig !== undefined) {
      ImportErrorConfig.encode(message.errorConfig, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportCompletionSuggestionsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportCompletionSuggestionsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inlineSource = ImportCompletionSuggestionsRequest_InlineSource.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.gcsSource = GcsSource.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.bigquerySource = BigQuerySource.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.errorConfig = ImportErrorConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportCompletionSuggestionsRequest {
    return {
      inlineSource: isSet(object.inlineSource)
        ? ImportCompletionSuggestionsRequest_InlineSource.fromJSON(object.inlineSource)
        : undefined,
      gcsSource: isSet(object.gcsSource) ? GcsSource.fromJSON(object.gcsSource) : undefined,
      bigquerySource: isSet(object.bigquerySource) ? BigQuerySource.fromJSON(object.bigquerySource) : undefined,
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      errorConfig: isSet(object.errorConfig) ? ImportErrorConfig.fromJSON(object.errorConfig) : undefined,
    };
  },

  toJSON(message: ImportCompletionSuggestionsRequest): unknown {
    const obj: any = {};
    if (message.inlineSource !== undefined) {
      obj.inlineSource = ImportCompletionSuggestionsRequest_InlineSource.toJSON(message.inlineSource);
    }
    if (message.gcsSource !== undefined) {
      obj.gcsSource = GcsSource.toJSON(message.gcsSource);
    }
    if (message.bigquerySource !== undefined) {
      obj.bigquerySource = BigQuerySource.toJSON(message.bigquerySource);
    }
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.errorConfig !== undefined) {
      obj.errorConfig = ImportErrorConfig.toJSON(message.errorConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<ImportCompletionSuggestionsRequest>): ImportCompletionSuggestionsRequest {
    return ImportCompletionSuggestionsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportCompletionSuggestionsRequest>): ImportCompletionSuggestionsRequest {
    const message = createBaseImportCompletionSuggestionsRequest();
    message.inlineSource = (object.inlineSource !== undefined && object.inlineSource !== null)
      ? ImportCompletionSuggestionsRequest_InlineSource.fromPartial(object.inlineSource)
      : undefined;
    message.gcsSource = (object.gcsSource !== undefined && object.gcsSource !== null)
      ? GcsSource.fromPartial(object.gcsSource)
      : undefined;
    message.bigquerySource = (object.bigquerySource !== undefined && object.bigquerySource !== null)
      ? BigQuerySource.fromPartial(object.bigquerySource)
      : undefined;
    message.parent = object.parent ?? "";
    message.errorConfig = (object.errorConfig !== undefined && object.errorConfig !== null)
      ? ImportErrorConfig.fromPartial(object.errorConfig)
      : undefined;
    return message;
  },
};

function createBaseImportCompletionSuggestionsRequest_InlineSource(): ImportCompletionSuggestionsRequest_InlineSource {
  return { suggestions: [] };
}

export const ImportCompletionSuggestionsRequest_InlineSource: MessageFns<
  ImportCompletionSuggestionsRequest_InlineSource
> = {
  encode(
    message: ImportCompletionSuggestionsRequest_InlineSource,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    for (const v of message.suggestions) {
      CompletionSuggestion.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportCompletionSuggestionsRequest_InlineSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportCompletionSuggestionsRequest_InlineSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.suggestions.push(CompletionSuggestion.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportCompletionSuggestionsRequest_InlineSource {
    return {
      suggestions: globalThis.Array.isArray(object?.suggestions)
        ? object.suggestions.map((e: any) => CompletionSuggestion.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ImportCompletionSuggestionsRequest_InlineSource): unknown {
    const obj: any = {};
    if (message.suggestions?.length) {
      obj.suggestions = message.suggestions.map((e) => CompletionSuggestion.toJSON(e));
    }
    return obj;
  },

  create(
    base?: DeepPartial<ImportCompletionSuggestionsRequest_InlineSource>,
  ): ImportCompletionSuggestionsRequest_InlineSource {
    return ImportCompletionSuggestionsRequest_InlineSource.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ImportCompletionSuggestionsRequest_InlineSource>,
  ): ImportCompletionSuggestionsRequest_InlineSource {
    const message = createBaseImportCompletionSuggestionsRequest_InlineSource();
    message.suggestions = object.suggestions?.map((e) => CompletionSuggestion.fromPartial(e)) || [];
    return message;
  },
};

function createBaseImportCompletionSuggestionsResponse(): ImportCompletionSuggestionsResponse {
  return { errorSamples: [], errorConfig: undefined };
}

export const ImportCompletionSuggestionsResponse: MessageFns<ImportCompletionSuggestionsResponse> = {
  encode(message: ImportCompletionSuggestionsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.errorSamples) {
      Status.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.errorConfig !== undefined) {
      ImportErrorConfig.encode(message.errorConfig, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportCompletionSuggestionsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportCompletionSuggestionsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.errorSamples.push(Status.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.errorConfig = ImportErrorConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportCompletionSuggestionsResponse {
    return {
      errorSamples: globalThis.Array.isArray(object?.errorSamples)
        ? object.errorSamples.map((e: any) => Status.fromJSON(e))
        : [],
      errorConfig: isSet(object.errorConfig) ? ImportErrorConfig.fromJSON(object.errorConfig) : undefined,
    };
  },

  toJSON(message: ImportCompletionSuggestionsResponse): unknown {
    const obj: any = {};
    if (message.errorSamples?.length) {
      obj.errorSamples = message.errorSamples.map((e) => Status.toJSON(e));
    }
    if (message.errorConfig !== undefined) {
      obj.errorConfig = ImportErrorConfig.toJSON(message.errorConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<ImportCompletionSuggestionsResponse>): ImportCompletionSuggestionsResponse {
    return ImportCompletionSuggestionsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportCompletionSuggestionsResponse>): ImportCompletionSuggestionsResponse {
    const message = createBaseImportCompletionSuggestionsResponse();
    message.errorSamples = object.errorSamples?.map((e) => Status.fromPartial(e)) || [];
    message.errorConfig = (object.errorConfig !== undefined && object.errorConfig !== null)
      ? ImportErrorConfig.fromPartial(object.errorConfig)
      : undefined;
    return message;
  },
};

function createBaseImportCompletionSuggestionsMetadata(): ImportCompletionSuggestionsMetadata {
  return { createTime: undefined, updateTime: undefined, successCount: Long.ZERO, failureCount: Long.ZERO };
}

export const ImportCompletionSuggestionsMetadata: MessageFns<ImportCompletionSuggestionsMetadata> = {
  encode(message: ImportCompletionSuggestionsMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(10).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(18).fork()).join();
    }
    if (!message.successCount.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.successCount.toString());
    }
    if (!message.failureCount.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.failureCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportCompletionSuggestionsMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportCompletionSuggestionsMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.successCount = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.failureCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportCompletionSuggestionsMetadata {
    return {
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      successCount: isSet(object.successCount) ? Long.fromValue(object.successCount) : Long.ZERO,
      failureCount: isSet(object.failureCount) ? Long.fromValue(object.failureCount) : Long.ZERO,
    };
  },

  toJSON(message: ImportCompletionSuggestionsMetadata): unknown {
    const obj: any = {};
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (!message.successCount.equals(Long.ZERO)) {
      obj.successCount = (message.successCount || Long.ZERO).toString();
    }
    if (!message.failureCount.equals(Long.ZERO)) {
      obj.failureCount = (message.failureCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ImportCompletionSuggestionsMetadata>): ImportCompletionSuggestionsMetadata {
    return ImportCompletionSuggestionsMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportCompletionSuggestionsMetadata>): ImportCompletionSuggestionsMetadata {
    const message = createBaseImportCompletionSuggestionsMetadata();
    message.createTime = object.createTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.successCount = (object.successCount !== undefined && object.successCount !== null)
      ? Long.fromValue(object.successCount)
      : Long.ZERO;
    message.failureCount = (object.failureCount !== undefined && object.failureCount !== null)
      ? Long.fromValue(object.failureCount)
      : Long.ZERO;
    return message;
  },
};

function createBaseImportSampleQueriesRequest(): ImportSampleQueriesRequest {
  return {
    inlineSource: undefined,
    gcsSource: undefined,
    bigquerySource: undefined,
    parent: "",
    errorConfig: undefined,
  };
}

export const ImportSampleQueriesRequest: MessageFns<ImportSampleQueriesRequest> = {
  encode(message: ImportSampleQueriesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inlineSource !== undefined) {
      ImportSampleQueriesRequest_InlineSource.encode(message.inlineSource, writer.uint32(18).fork()).join();
    }
    if (message.gcsSource !== undefined) {
      GcsSource.encode(message.gcsSource, writer.uint32(26).fork()).join();
    }
    if (message.bigquerySource !== undefined) {
      BigQuerySource.encode(message.bigquerySource, writer.uint32(34).fork()).join();
    }
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.errorConfig !== undefined) {
      ImportErrorConfig.encode(message.errorConfig, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportSampleQueriesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportSampleQueriesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inlineSource = ImportSampleQueriesRequest_InlineSource.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.gcsSource = GcsSource.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.bigquerySource = BigQuerySource.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.errorConfig = ImportErrorConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportSampleQueriesRequest {
    return {
      inlineSource: isSet(object.inlineSource)
        ? ImportSampleQueriesRequest_InlineSource.fromJSON(object.inlineSource)
        : undefined,
      gcsSource: isSet(object.gcsSource) ? GcsSource.fromJSON(object.gcsSource) : undefined,
      bigquerySource: isSet(object.bigquerySource) ? BigQuerySource.fromJSON(object.bigquerySource) : undefined,
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      errorConfig: isSet(object.errorConfig) ? ImportErrorConfig.fromJSON(object.errorConfig) : undefined,
    };
  },

  toJSON(message: ImportSampleQueriesRequest): unknown {
    const obj: any = {};
    if (message.inlineSource !== undefined) {
      obj.inlineSource = ImportSampleQueriesRequest_InlineSource.toJSON(message.inlineSource);
    }
    if (message.gcsSource !== undefined) {
      obj.gcsSource = GcsSource.toJSON(message.gcsSource);
    }
    if (message.bigquerySource !== undefined) {
      obj.bigquerySource = BigQuerySource.toJSON(message.bigquerySource);
    }
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.errorConfig !== undefined) {
      obj.errorConfig = ImportErrorConfig.toJSON(message.errorConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<ImportSampleQueriesRequest>): ImportSampleQueriesRequest {
    return ImportSampleQueriesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportSampleQueriesRequest>): ImportSampleQueriesRequest {
    const message = createBaseImportSampleQueriesRequest();
    message.inlineSource = (object.inlineSource !== undefined && object.inlineSource !== null)
      ? ImportSampleQueriesRequest_InlineSource.fromPartial(object.inlineSource)
      : undefined;
    message.gcsSource = (object.gcsSource !== undefined && object.gcsSource !== null)
      ? GcsSource.fromPartial(object.gcsSource)
      : undefined;
    message.bigquerySource = (object.bigquerySource !== undefined && object.bigquerySource !== null)
      ? BigQuerySource.fromPartial(object.bigquerySource)
      : undefined;
    message.parent = object.parent ?? "";
    message.errorConfig = (object.errorConfig !== undefined && object.errorConfig !== null)
      ? ImportErrorConfig.fromPartial(object.errorConfig)
      : undefined;
    return message;
  },
};

function createBaseImportSampleQueriesRequest_InlineSource(): ImportSampleQueriesRequest_InlineSource {
  return { sampleQueries: [] };
}

export const ImportSampleQueriesRequest_InlineSource: MessageFns<ImportSampleQueriesRequest_InlineSource> = {
  encode(message: ImportSampleQueriesRequest_InlineSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.sampleQueries) {
      SampleQuery.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportSampleQueriesRequest_InlineSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportSampleQueriesRequest_InlineSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.sampleQueries.push(SampleQuery.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportSampleQueriesRequest_InlineSource {
    return {
      sampleQueries: globalThis.Array.isArray(object?.sampleQueries)
        ? object.sampleQueries.map((e: any) => SampleQuery.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ImportSampleQueriesRequest_InlineSource): unknown {
    const obj: any = {};
    if (message.sampleQueries?.length) {
      obj.sampleQueries = message.sampleQueries.map((e) => SampleQuery.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ImportSampleQueriesRequest_InlineSource>): ImportSampleQueriesRequest_InlineSource {
    return ImportSampleQueriesRequest_InlineSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportSampleQueriesRequest_InlineSource>): ImportSampleQueriesRequest_InlineSource {
    const message = createBaseImportSampleQueriesRequest_InlineSource();
    message.sampleQueries = object.sampleQueries?.map((e) => SampleQuery.fromPartial(e)) || [];
    return message;
  },
};

function createBaseImportSampleQueriesResponse(): ImportSampleQueriesResponse {
  return { errorSamples: [], errorConfig: undefined };
}

export const ImportSampleQueriesResponse: MessageFns<ImportSampleQueriesResponse> = {
  encode(message: ImportSampleQueriesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.errorSamples) {
      Status.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.errorConfig !== undefined) {
      ImportErrorConfig.encode(message.errorConfig, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportSampleQueriesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportSampleQueriesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.errorSamples.push(Status.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.errorConfig = ImportErrorConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportSampleQueriesResponse {
    return {
      errorSamples: globalThis.Array.isArray(object?.errorSamples)
        ? object.errorSamples.map((e: any) => Status.fromJSON(e))
        : [],
      errorConfig: isSet(object.errorConfig) ? ImportErrorConfig.fromJSON(object.errorConfig) : undefined,
    };
  },

  toJSON(message: ImportSampleQueriesResponse): unknown {
    const obj: any = {};
    if (message.errorSamples?.length) {
      obj.errorSamples = message.errorSamples.map((e) => Status.toJSON(e));
    }
    if (message.errorConfig !== undefined) {
      obj.errorConfig = ImportErrorConfig.toJSON(message.errorConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<ImportSampleQueriesResponse>): ImportSampleQueriesResponse {
    return ImportSampleQueriesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportSampleQueriesResponse>): ImportSampleQueriesResponse {
    const message = createBaseImportSampleQueriesResponse();
    message.errorSamples = object.errorSamples?.map((e) => Status.fromPartial(e)) || [];
    message.errorConfig = (object.errorConfig !== undefined && object.errorConfig !== null)
      ? ImportErrorConfig.fromPartial(object.errorConfig)
      : undefined;
    return message;
  },
};

function createBaseImportSampleQueriesMetadata(): ImportSampleQueriesMetadata {
  return {
    createTime: undefined,
    updateTime: undefined,
    successCount: Long.ZERO,
    failureCount: Long.ZERO,
    totalCount: Long.ZERO,
  };
}

export const ImportSampleQueriesMetadata: MessageFns<ImportSampleQueriesMetadata> = {
  encode(message: ImportSampleQueriesMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(10).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(18).fork()).join();
    }
    if (!message.successCount.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.successCount.toString());
    }
    if (!message.failureCount.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.failureCount.toString());
    }
    if (!message.totalCount.equals(Long.ZERO)) {
      writer.uint32(40).int64(message.totalCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportSampleQueriesMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportSampleQueriesMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.successCount = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.failureCount = Long.fromString(reader.int64().toString());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.totalCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportSampleQueriesMetadata {
    return {
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      successCount: isSet(object.successCount) ? Long.fromValue(object.successCount) : Long.ZERO,
      failureCount: isSet(object.failureCount) ? Long.fromValue(object.failureCount) : Long.ZERO,
      totalCount: isSet(object.totalCount) ? Long.fromValue(object.totalCount) : Long.ZERO,
    };
  },

  toJSON(message: ImportSampleQueriesMetadata): unknown {
    const obj: any = {};
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (!message.successCount.equals(Long.ZERO)) {
      obj.successCount = (message.successCount || Long.ZERO).toString();
    }
    if (!message.failureCount.equals(Long.ZERO)) {
      obj.failureCount = (message.failureCount || Long.ZERO).toString();
    }
    if (!message.totalCount.equals(Long.ZERO)) {
      obj.totalCount = (message.totalCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ImportSampleQueriesMetadata>): ImportSampleQueriesMetadata {
    return ImportSampleQueriesMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportSampleQueriesMetadata>): ImportSampleQueriesMetadata {
    const message = createBaseImportSampleQueriesMetadata();
    message.createTime = object.createTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.successCount = (object.successCount !== undefined && object.successCount !== null)
      ? Long.fromValue(object.successCount)
      : Long.ZERO;
    message.failureCount = (object.failureCount !== undefined && object.failureCount !== null)
      ? Long.fromValue(object.failureCount)
      : Long.ZERO;
    message.totalCount = (object.totalCount !== undefined && object.totalCount !== null)
      ? Long.fromValue(object.totalCount)
      : Long.ZERO;
    return message;
  },
};

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
