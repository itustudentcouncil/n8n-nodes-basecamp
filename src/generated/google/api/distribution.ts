// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/api/distribution.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Any } from "../protobuf/any.js";
import { Timestamp } from "../protobuf/timestamp.js";

export const protobufPackage = "google.api";

/**
 * `Distribution` contains summary statistics for a population of values. It
 * optionally contains a histogram representing the distribution of those values
 * across a set of buckets.
 *
 * The summary statistics are the count, mean, sum of the squared deviation from
 * the mean, the minimum, and the maximum of the set of population of values.
 * The histogram is based on a sequence of buckets and gives a count of values
 * that fall into each bucket. The boundaries of the buckets are given either
 * explicitly or by formulas for buckets of fixed or exponentially increasing
 * widths.
 *
 * Although it is not forbidden, it is generally a bad idea to include
 * non-finite values (infinities or NaNs) in the population of values, as this
 * will render the `mean` and `sum_of_squared_deviation` fields meaningless.
 */
export interface Distribution {
  /**
   * The number of values in the population. Must be non-negative. This value
   * must equal the sum of the values in `bucket_counts` if a histogram is
   * provided.
   */
  count: Long;
  /**
   * The arithmetic mean of the values in the population. If `count` is zero
   * then this field must be zero.
   */
  mean: number;
  /**
   * The sum of squared deviations from the mean of the values in the
   * population. For values x_i this is:
   *
   *     Sum[i=1..n]((x_i - mean)^2)
   *
   * Knuth, "The Art of Computer Programming", Vol. 2, page 232, 3rd edition
   * describes Welford's method for accumulating this sum in one pass.
   *
   * If `count` is zero then this field must be zero.
   */
  sumOfSquaredDeviation: number;
  /**
   * If specified, contains the range of the population values. The field
   * must not be present if the `count` is zero.
   */
  range:
    | Distribution_Range
    | undefined;
  /**
   * Defines the histogram bucket boundaries. If the distribution does not
   * contain a histogram, then omit this field.
   */
  bucketOptions:
    | Distribution_BucketOptions
    | undefined;
  /**
   * The number of values in each bucket of the histogram, as described in
   * `bucket_options`. If the distribution does not have a histogram, then omit
   * this field. If there is a histogram, then the sum of the values in
   * `bucket_counts` must equal the value in the `count` field of the
   * distribution.
   *
   * If present, `bucket_counts` should contain N values, where N is the number
   * of buckets specified in `bucket_options`. If you supply fewer than N
   * values, the remaining values are assumed to be 0.
   *
   * The order of the values in `bucket_counts` follows the bucket numbering
   * schemes described for the three bucket types. The first value must be the
   * count for the underflow bucket (number 0). The next N-2 values are the
   * counts for the finite buckets (number 1 through N-2). The N'th value in
   * `bucket_counts` is the count for the overflow bucket (number N-1).
   */
  bucketCounts: Long[];
  /** Must be in increasing order of `value` field. */
  exemplars: Distribution_Exemplar[];
}

/** The range of the population values. */
export interface Distribution_Range {
  /** The minimum of the population values. */
  min: number;
  /** The maximum of the population values. */
  max: number;
}

/**
 * `BucketOptions` describes the bucket boundaries used to create a histogram
 * for the distribution. The buckets can be in a linear sequence, an
 * exponential sequence, or each bucket can be specified explicitly.
 * `BucketOptions` does not include the number of values in each bucket.
 *
 * A bucket has an inclusive lower bound and exclusive upper bound for the
 * values that are counted for that bucket. The upper bound of a bucket must
 * be strictly greater than the lower bound. The sequence of N buckets for a
 * distribution consists of an underflow bucket (number 0), zero or more
 * finite buckets (number 1 through N - 2) and an overflow bucket (number N -
 * 1). The buckets are contiguous: the lower bound of bucket i (i > 0) is the
 * same as the upper bound of bucket i - 1. The buckets span the whole range
 * of finite values: lower bound of the underflow bucket is -infinity and the
 * upper bound of the overflow bucket is +infinity. The finite buckets are
 * so-called because both bounds are finite.
 */
export interface Distribution_BucketOptions {
  /** The linear bucket. */
  linearBuckets?:
    | Distribution_BucketOptions_Linear
    | undefined;
  /** The exponential buckets. */
  exponentialBuckets?:
    | Distribution_BucketOptions_Exponential
    | undefined;
  /** The explicit buckets. */
  explicitBuckets?: Distribution_BucketOptions_Explicit | undefined;
}

/**
 * Specifies a linear sequence of buckets that all have the same width
 * (except overflow and underflow). Each bucket represents a constant
 * absolute uncertainty on the specific value in the bucket.
 *
 * There are `num_finite_buckets + 2` (= N) buckets. Bucket `i` has the
 * following boundaries:
 *
 *    Upper bound (0 <= i < N-1):     offset + (width * i).
 *
 *    Lower bound (1 <= i < N):       offset + (width * (i - 1)).
 */
export interface Distribution_BucketOptions_Linear {
  /** Must be greater than 0. */
  numFiniteBuckets: number;
  /** Must be greater than 0. */
  width: number;
  /** Lower bound of the first bucket. */
  offset: number;
}

/**
 * Specifies an exponential sequence of buckets that have a width that is
 * proportional to the value of the lower bound. Each bucket represents a
 * constant relative uncertainty on a specific value in the bucket.
 *
 * There are `num_finite_buckets + 2` (= N) buckets. Bucket `i` has the
 * following boundaries:
 *
 *    Upper bound (0 <= i < N-1):     scale * (growth_factor ^ i).
 *
 *    Lower bound (1 <= i < N):       scale * (growth_factor ^ (i - 1)).
 */
export interface Distribution_BucketOptions_Exponential {
  /** Must be greater than 0. */
  numFiniteBuckets: number;
  /** Must be greater than 1. */
  growthFactor: number;
  /** Must be greater than 0. */
  scale: number;
}

/**
 * Specifies a set of buckets with arbitrary widths.
 *
 * There are `size(bounds) + 1` (= N) buckets. Bucket `i` has the following
 * boundaries:
 *
 *    Upper bound (0 <= i < N-1):     bounds[i]
 *    Lower bound (1 <= i < N);       bounds[i - 1]
 *
 * The `bounds` field must contain at least one element. If `bounds` has
 * only one element, then there are no finite buckets, and that single
 * element is the common boundary of the overflow and underflow buckets.
 */
export interface Distribution_BucketOptions_Explicit {
  /** The values must be monotonically increasing. */
  bounds: number[];
}

/**
 * Exemplars are example points that may be used to annotate aggregated
 * distribution values. They are metadata that gives information about a
 * particular value added to a Distribution bucket, such as a trace ID that
 * was active when a value was added. They may contain further information,
 * such as a example values and timestamps, origin, etc.
 */
export interface Distribution_Exemplar {
  /**
   * Value of the exemplar point. This value determines to which bucket the
   * exemplar belongs.
   */
  value: number;
  /** The observation (sampling) time of the above value. */
  timestamp:
    | Date
    | undefined;
  /**
   * Contextual information about the example value. Examples are:
   *
   *   Trace: type.googleapis.com/google.monitoring.v3.SpanContext
   *
   *   Literal string: type.googleapis.com/google.protobuf.StringValue
   *
   *   Labels dropped during aggregation:
   *     type.googleapis.com/google.monitoring.v3.DroppedLabels
   *
   * There may be only a single attachment of any given message type in a
   * single exemplar, and this is enforced by the system.
   */
  attachments: Any[];
}

function createBaseDistribution(): Distribution {
  return {
    count: Long.ZERO,
    mean: 0,
    sumOfSquaredDeviation: 0,
    range: undefined,
    bucketOptions: undefined,
    bucketCounts: [],
    exemplars: [],
  };
}

export const Distribution: MessageFns<Distribution> = {
  encode(message: Distribution, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.count.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.count.toString());
    }
    if (message.mean !== 0) {
      writer.uint32(17).double(message.mean);
    }
    if (message.sumOfSquaredDeviation !== 0) {
      writer.uint32(25).double(message.sumOfSquaredDeviation);
    }
    if (message.range !== undefined) {
      Distribution_Range.encode(message.range, writer.uint32(34).fork()).join();
    }
    if (message.bucketOptions !== undefined) {
      Distribution_BucketOptions.encode(message.bucketOptions, writer.uint32(50).fork()).join();
    }
    writer.uint32(58).fork();
    for (const v of message.bucketCounts) {
      writer.int64(v.toString());
    }
    writer.join();
    for (const v of message.exemplars) {
      Distribution_Exemplar.encode(v!, writer.uint32(82).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Distribution {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDistribution();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.count = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.mean = reader.double();
          continue;
        case 3:
          if (tag !== 25) {
            break;
          }

          message.sumOfSquaredDeviation = reader.double();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.range = Distribution_Range.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.bucketOptions = Distribution_BucketOptions.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag === 56) {
            message.bucketCounts.push(Long.fromString(reader.int64().toString()));

            continue;
          }

          if (tag === 58) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.bucketCounts.push(Long.fromString(reader.int64().toString()));
            }

            continue;
          }

          break;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.exemplars.push(Distribution_Exemplar.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Distribution {
    return {
      count: isSet(object.count) ? Long.fromValue(object.count) : Long.ZERO,
      mean: isSet(object.mean) ? globalThis.Number(object.mean) : 0,
      sumOfSquaredDeviation: isSet(object.sumOfSquaredDeviation) ? globalThis.Number(object.sumOfSquaredDeviation) : 0,
      range: isSet(object.range) ? Distribution_Range.fromJSON(object.range) : undefined,
      bucketOptions: isSet(object.bucketOptions)
        ? Distribution_BucketOptions.fromJSON(object.bucketOptions)
        : undefined,
      bucketCounts: globalThis.Array.isArray(object?.bucketCounts)
        ? object.bucketCounts.map((e: any) => Long.fromValue(e))
        : [],
      exemplars: globalThis.Array.isArray(object?.exemplars)
        ? object.exemplars.map((e: any) => Distribution_Exemplar.fromJSON(e))
        : [],
    };
  },

  toJSON(message: Distribution): unknown {
    const obj: any = {};
    if (!message.count.equals(Long.ZERO)) {
      obj.count = (message.count || Long.ZERO).toString();
    }
    if (message.mean !== 0) {
      obj.mean = message.mean;
    }
    if (message.sumOfSquaredDeviation !== 0) {
      obj.sumOfSquaredDeviation = message.sumOfSquaredDeviation;
    }
    if (message.range !== undefined) {
      obj.range = Distribution_Range.toJSON(message.range);
    }
    if (message.bucketOptions !== undefined) {
      obj.bucketOptions = Distribution_BucketOptions.toJSON(message.bucketOptions);
    }
    if (message.bucketCounts?.length) {
      obj.bucketCounts = message.bucketCounts.map((e) => (e || Long.ZERO).toString());
    }
    if (message.exemplars?.length) {
      obj.exemplars = message.exemplars.map((e) => Distribution_Exemplar.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<Distribution>): Distribution {
    return Distribution.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Distribution>): Distribution {
    const message = createBaseDistribution();
    message.count = (object.count !== undefined && object.count !== null) ? Long.fromValue(object.count) : Long.ZERO;
    message.mean = object.mean ?? 0;
    message.sumOfSquaredDeviation = object.sumOfSquaredDeviation ?? 0;
    message.range = (object.range !== undefined && object.range !== null)
      ? Distribution_Range.fromPartial(object.range)
      : undefined;
    message.bucketOptions = (object.bucketOptions !== undefined && object.bucketOptions !== null)
      ? Distribution_BucketOptions.fromPartial(object.bucketOptions)
      : undefined;
    message.bucketCounts = object.bucketCounts?.map((e) => Long.fromValue(e)) || [];
    message.exemplars = object.exemplars?.map((e) => Distribution_Exemplar.fromPartial(e)) || [];
    return message;
  },
};

function createBaseDistribution_Range(): Distribution_Range {
  return { min: 0, max: 0 };
}

export const Distribution_Range: MessageFns<Distribution_Range> = {
  encode(message: Distribution_Range, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.min !== 0) {
      writer.uint32(9).double(message.min);
    }
    if (message.max !== 0) {
      writer.uint32(17).double(message.max);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Distribution_Range {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDistribution_Range();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 9) {
            break;
          }

          message.min = reader.double();
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.max = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Distribution_Range {
    return {
      min: isSet(object.min) ? globalThis.Number(object.min) : 0,
      max: isSet(object.max) ? globalThis.Number(object.max) : 0,
    };
  },

  toJSON(message: Distribution_Range): unknown {
    const obj: any = {};
    if (message.min !== 0) {
      obj.min = message.min;
    }
    if (message.max !== 0) {
      obj.max = message.max;
    }
    return obj;
  },

  create(base?: DeepPartial<Distribution_Range>): Distribution_Range {
    return Distribution_Range.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Distribution_Range>): Distribution_Range {
    const message = createBaseDistribution_Range();
    message.min = object.min ?? 0;
    message.max = object.max ?? 0;
    return message;
  },
};

function createBaseDistribution_BucketOptions(): Distribution_BucketOptions {
  return { linearBuckets: undefined, exponentialBuckets: undefined, explicitBuckets: undefined };
}

export const Distribution_BucketOptions: MessageFns<Distribution_BucketOptions> = {
  encode(message: Distribution_BucketOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.linearBuckets !== undefined) {
      Distribution_BucketOptions_Linear.encode(message.linearBuckets, writer.uint32(10).fork()).join();
    }
    if (message.exponentialBuckets !== undefined) {
      Distribution_BucketOptions_Exponential.encode(message.exponentialBuckets, writer.uint32(18).fork()).join();
    }
    if (message.explicitBuckets !== undefined) {
      Distribution_BucketOptions_Explicit.encode(message.explicitBuckets, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Distribution_BucketOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDistribution_BucketOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.linearBuckets = Distribution_BucketOptions_Linear.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.exponentialBuckets = Distribution_BucketOptions_Exponential.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.explicitBuckets = Distribution_BucketOptions_Explicit.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Distribution_BucketOptions {
    return {
      linearBuckets: isSet(object.linearBuckets)
        ? Distribution_BucketOptions_Linear.fromJSON(object.linearBuckets)
        : undefined,
      exponentialBuckets: isSet(object.exponentialBuckets)
        ? Distribution_BucketOptions_Exponential.fromJSON(object.exponentialBuckets)
        : undefined,
      explicitBuckets: isSet(object.explicitBuckets)
        ? Distribution_BucketOptions_Explicit.fromJSON(object.explicitBuckets)
        : undefined,
    };
  },

  toJSON(message: Distribution_BucketOptions): unknown {
    const obj: any = {};
    if (message.linearBuckets !== undefined) {
      obj.linearBuckets = Distribution_BucketOptions_Linear.toJSON(message.linearBuckets);
    }
    if (message.exponentialBuckets !== undefined) {
      obj.exponentialBuckets = Distribution_BucketOptions_Exponential.toJSON(message.exponentialBuckets);
    }
    if (message.explicitBuckets !== undefined) {
      obj.explicitBuckets = Distribution_BucketOptions_Explicit.toJSON(message.explicitBuckets);
    }
    return obj;
  },

  create(base?: DeepPartial<Distribution_BucketOptions>): Distribution_BucketOptions {
    return Distribution_BucketOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Distribution_BucketOptions>): Distribution_BucketOptions {
    const message = createBaseDistribution_BucketOptions();
    message.linearBuckets = (object.linearBuckets !== undefined && object.linearBuckets !== null)
      ? Distribution_BucketOptions_Linear.fromPartial(object.linearBuckets)
      : undefined;
    message.exponentialBuckets = (object.exponentialBuckets !== undefined && object.exponentialBuckets !== null)
      ? Distribution_BucketOptions_Exponential.fromPartial(object.exponentialBuckets)
      : undefined;
    message.explicitBuckets = (object.explicitBuckets !== undefined && object.explicitBuckets !== null)
      ? Distribution_BucketOptions_Explicit.fromPartial(object.explicitBuckets)
      : undefined;
    return message;
  },
};

function createBaseDistribution_BucketOptions_Linear(): Distribution_BucketOptions_Linear {
  return { numFiniteBuckets: 0, width: 0, offset: 0 };
}

export const Distribution_BucketOptions_Linear: MessageFns<Distribution_BucketOptions_Linear> = {
  encode(message: Distribution_BucketOptions_Linear, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.numFiniteBuckets !== 0) {
      writer.uint32(8).int32(message.numFiniteBuckets);
    }
    if (message.width !== 0) {
      writer.uint32(17).double(message.width);
    }
    if (message.offset !== 0) {
      writer.uint32(25).double(message.offset);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Distribution_BucketOptions_Linear {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDistribution_BucketOptions_Linear();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.numFiniteBuckets = reader.int32();
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.width = reader.double();
          continue;
        case 3:
          if (tag !== 25) {
            break;
          }

          message.offset = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Distribution_BucketOptions_Linear {
    return {
      numFiniteBuckets: isSet(object.numFiniteBuckets) ? globalThis.Number(object.numFiniteBuckets) : 0,
      width: isSet(object.width) ? globalThis.Number(object.width) : 0,
      offset: isSet(object.offset) ? globalThis.Number(object.offset) : 0,
    };
  },

  toJSON(message: Distribution_BucketOptions_Linear): unknown {
    const obj: any = {};
    if (message.numFiniteBuckets !== 0) {
      obj.numFiniteBuckets = Math.round(message.numFiniteBuckets);
    }
    if (message.width !== 0) {
      obj.width = message.width;
    }
    if (message.offset !== 0) {
      obj.offset = message.offset;
    }
    return obj;
  },

  create(base?: DeepPartial<Distribution_BucketOptions_Linear>): Distribution_BucketOptions_Linear {
    return Distribution_BucketOptions_Linear.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Distribution_BucketOptions_Linear>): Distribution_BucketOptions_Linear {
    const message = createBaseDistribution_BucketOptions_Linear();
    message.numFiniteBuckets = object.numFiniteBuckets ?? 0;
    message.width = object.width ?? 0;
    message.offset = object.offset ?? 0;
    return message;
  },
};

function createBaseDistribution_BucketOptions_Exponential(): Distribution_BucketOptions_Exponential {
  return { numFiniteBuckets: 0, growthFactor: 0, scale: 0 };
}

export const Distribution_BucketOptions_Exponential: MessageFns<Distribution_BucketOptions_Exponential> = {
  encode(message: Distribution_BucketOptions_Exponential, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.numFiniteBuckets !== 0) {
      writer.uint32(8).int32(message.numFiniteBuckets);
    }
    if (message.growthFactor !== 0) {
      writer.uint32(17).double(message.growthFactor);
    }
    if (message.scale !== 0) {
      writer.uint32(25).double(message.scale);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Distribution_BucketOptions_Exponential {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDistribution_BucketOptions_Exponential();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.numFiniteBuckets = reader.int32();
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.growthFactor = reader.double();
          continue;
        case 3:
          if (tag !== 25) {
            break;
          }

          message.scale = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Distribution_BucketOptions_Exponential {
    return {
      numFiniteBuckets: isSet(object.numFiniteBuckets) ? globalThis.Number(object.numFiniteBuckets) : 0,
      growthFactor: isSet(object.growthFactor) ? globalThis.Number(object.growthFactor) : 0,
      scale: isSet(object.scale) ? globalThis.Number(object.scale) : 0,
    };
  },

  toJSON(message: Distribution_BucketOptions_Exponential): unknown {
    const obj: any = {};
    if (message.numFiniteBuckets !== 0) {
      obj.numFiniteBuckets = Math.round(message.numFiniteBuckets);
    }
    if (message.growthFactor !== 0) {
      obj.growthFactor = message.growthFactor;
    }
    if (message.scale !== 0) {
      obj.scale = message.scale;
    }
    return obj;
  },

  create(base?: DeepPartial<Distribution_BucketOptions_Exponential>): Distribution_BucketOptions_Exponential {
    return Distribution_BucketOptions_Exponential.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Distribution_BucketOptions_Exponential>): Distribution_BucketOptions_Exponential {
    const message = createBaseDistribution_BucketOptions_Exponential();
    message.numFiniteBuckets = object.numFiniteBuckets ?? 0;
    message.growthFactor = object.growthFactor ?? 0;
    message.scale = object.scale ?? 0;
    return message;
  },
};

function createBaseDistribution_BucketOptions_Explicit(): Distribution_BucketOptions_Explicit {
  return { bounds: [] };
}

export const Distribution_BucketOptions_Explicit: MessageFns<Distribution_BucketOptions_Explicit> = {
  encode(message: Distribution_BucketOptions_Explicit, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    writer.uint32(10).fork();
    for (const v of message.bounds) {
      writer.double(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Distribution_BucketOptions_Explicit {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDistribution_BucketOptions_Explicit();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag === 9) {
            message.bounds.push(reader.double());

            continue;
          }

          if (tag === 10) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.bounds.push(reader.double());
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Distribution_BucketOptions_Explicit {
    return {
      bounds: globalThis.Array.isArray(object?.bounds) ? object.bounds.map((e: any) => globalThis.Number(e)) : [],
    };
  },

  toJSON(message: Distribution_BucketOptions_Explicit): unknown {
    const obj: any = {};
    if (message.bounds?.length) {
      obj.bounds = message.bounds;
    }
    return obj;
  },

  create(base?: DeepPartial<Distribution_BucketOptions_Explicit>): Distribution_BucketOptions_Explicit {
    return Distribution_BucketOptions_Explicit.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Distribution_BucketOptions_Explicit>): Distribution_BucketOptions_Explicit {
    const message = createBaseDistribution_BucketOptions_Explicit();
    message.bounds = object.bounds?.map((e) => e) || [];
    return message;
  },
};

function createBaseDistribution_Exemplar(): Distribution_Exemplar {
  return { value: 0, timestamp: undefined, attachments: [] };
}

export const Distribution_Exemplar: MessageFns<Distribution_Exemplar> = {
  encode(message: Distribution_Exemplar, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.value !== 0) {
      writer.uint32(9).double(message.value);
    }
    if (message.timestamp !== undefined) {
      Timestamp.encode(toTimestamp(message.timestamp), writer.uint32(18).fork()).join();
    }
    for (const v of message.attachments) {
      Any.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Distribution_Exemplar {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDistribution_Exemplar();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 9) {
            break;
          }

          message.value = reader.double();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.timestamp = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.attachments.push(Any.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Distribution_Exemplar {
    return {
      value: isSet(object.value) ? globalThis.Number(object.value) : 0,
      timestamp: isSet(object.timestamp) ? fromJsonTimestamp(object.timestamp) : undefined,
      attachments: globalThis.Array.isArray(object?.attachments)
        ? object.attachments.map((e: any) => Any.fromJSON(e))
        : [],
    };
  },

  toJSON(message: Distribution_Exemplar): unknown {
    const obj: any = {};
    if (message.value !== 0) {
      obj.value = message.value;
    }
    if (message.timestamp !== undefined) {
      obj.timestamp = message.timestamp.toISOString();
    }
    if (message.attachments?.length) {
      obj.attachments = message.attachments.map((e) => Any.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<Distribution_Exemplar>): Distribution_Exemplar {
    return Distribution_Exemplar.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Distribution_Exemplar>): Distribution_Exemplar {
    const message = createBaseDistribution_Exemplar();
    message.value = object.value ?? 0;
    message.timestamp = object.timestamp ?? undefined;
    message.attachments = object.attachments?.map((e) => Any.fromPartial(e)) || [];
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
