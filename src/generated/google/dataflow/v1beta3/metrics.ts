// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/dataflow/v1beta3/metrics.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Value } from "../../protobuf/struct.js";
import { Timestamp } from "../../protobuf/timestamp.js";

export const protobufPackage = "google.dataflow.v1beta3";

/** The state of some component of job execution. */
export enum ExecutionState {
  /** EXECUTION_STATE_UNKNOWN - The component state is unknown or unspecified. */
  EXECUTION_STATE_UNKNOWN = 0,
  /** EXECUTION_STATE_NOT_STARTED - The component is not yet running. */
  EXECUTION_STATE_NOT_STARTED = 1,
  /** EXECUTION_STATE_RUNNING - The component is currently running. */
  EXECUTION_STATE_RUNNING = 2,
  /** EXECUTION_STATE_SUCCEEDED - The component succeeded. */
  EXECUTION_STATE_SUCCEEDED = 3,
  /** EXECUTION_STATE_FAILED - The component failed. */
  EXECUTION_STATE_FAILED = 4,
  /** EXECUTION_STATE_CANCELLED - Execution of the component was cancelled. */
  EXECUTION_STATE_CANCELLED = 5,
  UNRECOGNIZED = -1,
}

export function executionStateFromJSON(object: any): ExecutionState {
  switch (object) {
    case 0:
    case "EXECUTION_STATE_UNKNOWN":
      return ExecutionState.EXECUTION_STATE_UNKNOWN;
    case 1:
    case "EXECUTION_STATE_NOT_STARTED":
      return ExecutionState.EXECUTION_STATE_NOT_STARTED;
    case 2:
    case "EXECUTION_STATE_RUNNING":
      return ExecutionState.EXECUTION_STATE_RUNNING;
    case 3:
    case "EXECUTION_STATE_SUCCEEDED":
      return ExecutionState.EXECUTION_STATE_SUCCEEDED;
    case 4:
    case "EXECUTION_STATE_FAILED":
      return ExecutionState.EXECUTION_STATE_FAILED;
    case 5:
    case "EXECUTION_STATE_CANCELLED":
      return ExecutionState.EXECUTION_STATE_CANCELLED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ExecutionState.UNRECOGNIZED;
  }
}

export function executionStateToJSON(object: ExecutionState): string {
  switch (object) {
    case ExecutionState.EXECUTION_STATE_UNKNOWN:
      return "EXECUTION_STATE_UNKNOWN";
    case ExecutionState.EXECUTION_STATE_NOT_STARTED:
      return "EXECUTION_STATE_NOT_STARTED";
    case ExecutionState.EXECUTION_STATE_RUNNING:
      return "EXECUTION_STATE_RUNNING";
    case ExecutionState.EXECUTION_STATE_SUCCEEDED:
      return "EXECUTION_STATE_SUCCEEDED";
    case ExecutionState.EXECUTION_STATE_FAILED:
      return "EXECUTION_STATE_FAILED";
    case ExecutionState.EXECUTION_STATE_CANCELLED:
      return "EXECUTION_STATE_CANCELLED";
    case ExecutionState.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Identifies a metric, by describing the source which generated the
 * metric.
 */
export interface MetricStructuredName {
  /**
   * Origin (namespace) of metric name. May be blank for user-define metrics;
   * will be "dataflow" for metrics defined by the Dataflow service or SDK.
   */
  origin: string;
  /** Worker-defined metric name. */
  name: string;
  /**
   * Zero or more labeled fields which identify the part of the job this
   * metric is associated with, such as the name of a step or collection.
   *
   * For example, built-in counters associated with steps will have
   * context['step'] = <step-name>. Counters associated with PCollections
   * in the SDK will have context['pcollection'] = <pcollection-name>.
   */
  context: { [key: string]: string };
}

export interface MetricStructuredName_ContextEntry {
  key: string;
  value: string;
}

/** Describes the state of a metric. */
export interface MetricUpdate {
  /** Name of the metric. */
  name:
    | MetricStructuredName
    | undefined;
  /**
   * Metric aggregation kind.  The possible metric aggregation kinds are
   * "Sum", "Max", "Min", "Mean", "Set", "And", "Or", and "Distribution".
   * The specified aggregation kind is case-insensitive.
   *
   * If omitted, this is not an aggregated value but instead
   * a single metric sample value.
   */
  kind: string;
  /**
   * True if this metric is reported as the total cumulative aggregate
   * value accumulated since the worker started working on this WorkItem.
   * By default this is false, indicating that this metric is reported
   * as a delta that is not associated with any WorkItem.
   */
  cumulative: boolean;
  /**
   * Worker-computed aggregate value for aggregation kinds "Sum", "Max", "Min",
   * "And", and "Or".  The possible value types are Long, Double, and Boolean.
   */
  scalar:
    | any
    | undefined;
  /**
   * Worker-computed aggregate value for the "Mean" aggregation kind.
   * This holds the sum of the aggregated values and is used in combination
   * with mean_count below to obtain the actual mean aggregate value.
   * The only possible value types are Long and Double.
   */
  meanSum:
    | any
    | undefined;
  /**
   * Worker-computed aggregate value for the "Mean" aggregation kind.
   * This holds the count of the aggregated values and is used in combination
   * with mean_sum above to obtain the actual mean aggregate value.
   * The only possible value type is Long.
   */
  meanCount:
    | any
    | undefined;
  /**
   * Worker-computed aggregate value for the "Set" aggregation kind.  The only
   * possible value type is a list of Values whose type can be Long, Double,
   * or String, according to the metric's type.  All Values in the list must
   * be of the same type.
   */
  set:
    | any
    | undefined;
  /** A struct value describing properties of a distribution of numeric values. */
  distribution:
    | any
    | undefined;
  /**
   * A struct value describing properties of a Gauge.
   * Metrics of gauge type show the value of a metric across time, and is
   * aggregated based on the newest value.
   */
  gauge:
    | any
    | undefined;
  /**
   * Worker-computed aggregate value for internal use by the Dataflow
   * service.
   */
  internal:
    | any
    | undefined;
  /**
   * Timestamp associated with the metric value. Optional when workers are
   * reporting work progress; it will be filled in responses from the
   * metrics API.
   */
  updateTime: Date | undefined;
}

/** Request to get job metrics. */
export interface GetJobMetricsRequest {
  /** A project id. */
  projectId: string;
  /** The job to get metrics for. */
  jobId: string;
  /**
   * Return only metric data that has changed since this time.
   * Default is to return all information about all metrics for the job.
   */
  startTime:
    | Date
    | undefined;
  /**
   * The [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) that
   * contains the job specified by job_id.
   */
  location: string;
}

/**
 * JobMetrics contains a collection of metrics describing the detailed progress
 * of a Dataflow job. Metrics correspond to user-defined and system-defined
 * metrics in the job.
 *
 * This resource captures only the most recent values of each metric;
 * time-series data can be queried for them (under the same metric names)
 * from Cloud Monitoring.
 */
export interface JobMetrics {
  /** Timestamp as of which metric values are current. */
  metricTime:
    | Date
    | undefined;
  /** All metrics for this job. */
  metrics: MetricUpdate[];
}

/** Request to get job execution details. */
export interface GetJobExecutionDetailsRequest {
  /** A project id. */
  projectId: string;
  /** The job to get execution details for. */
  jobId: string;
  /**
   * The [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) that
   * contains the job specified by job_id.
   */
  location: string;
  /**
   * If specified, determines the maximum number of stages to
   * return.  If unspecified, the service may choose an appropriate
   * default, or may return an arbitrarily large number of results.
   */
  pageSize: number;
  /**
   * If supplied, this should be the value of next_page_token returned
   * by an earlier call. This will cause the next page of results to
   * be returned.
   */
  pageToken: string;
}

/** Information about the progress of some component of job execution. */
export interface ProgressTimeseries {
  /** The current progress of the component, in the range [0,1]. */
  currentProgress: number;
  /**
   * History of progress for the component.
   *
   * Points are sorted by time.
   */
  dataPoints: ProgressTimeseries_Point[];
}

/** A point in the timeseries. */
export interface ProgressTimeseries_Point {
  /** The timestamp of the point. */
  time:
    | Date
    | undefined;
  /** The value of the point. */
  value: number;
}

/** Information about a particular execution stage of a job. */
export interface StageSummary {
  /** ID of this stage */
  stageId: string;
  /** State of this stage. */
  state: ExecutionState;
  /** Start time of this stage. */
  startTime:
    | Date
    | undefined;
  /**
   * End time of this stage.
   *
   * If the work item is completed, this is the actual end time of the stage.
   * Otherwise, it is the predicted end time.
   */
  endTime:
    | Date
    | undefined;
  /**
   * Progress for this stage.
   * Only applicable to Batch jobs.
   */
  progress:
    | ProgressTimeseries
    | undefined;
  /** Metrics for this stage. */
  metrics: MetricUpdate[];
}

/** Information about the execution of a job. */
export interface JobExecutionDetails {
  /** The stages of the job execution. */
  stages: StageSummary[];
  /**
   * If present, this response does not contain all requested tasks.  To obtain
   * the next page of results, repeat the request with page_token set to this
   * value.
   */
  nextPageToken: string;
}

/**
 * Request to get information about a particular execution stage of a job.
 * Currently only tracked for Batch jobs.
 */
export interface GetStageExecutionDetailsRequest {
  /** A project id. */
  projectId: string;
  /** The job to get execution details for. */
  jobId: string;
  /**
   * The [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) that
   * contains the job specified by job_id.
   */
  location: string;
  /** The stage for which to fetch information. */
  stageId: string;
  /**
   * If specified, determines the maximum number of work items to
   * return.  If unspecified, the service may choose an appropriate
   * default, or may return an arbitrarily large number of results.
   */
  pageSize: number;
  /**
   * If supplied, this should be the value of next_page_token returned
   * by an earlier call. This will cause the next page of results to
   * be returned.
   */
  pageToken: string;
  /** Lower time bound of work items to include, by start time. */
  startTime:
    | Date
    | undefined;
  /** Upper time bound of work items to include, by start time. */
  endTime: Date | undefined;
}

/** Information about an individual work item execution. */
export interface WorkItemDetails {
  /** Name of this work item. */
  taskId: string;
  /** Attempt ID of this work item */
  attemptId: string;
  /** Start time of this work item attempt. */
  startTime:
    | Date
    | undefined;
  /**
   * End time of this work item attempt.
   *
   * If the work item is completed, this is the actual end time of the work
   * item.  Otherwise, it is the predicted end time.
   */
  endTime:
    | Date
    | undefined;
  /** State of this work item. */
  state: ExecutionState;
  /** Progress of this work item. */
  progress:
    | ProgressTimeseries
    | undefined;
  /** Metrics for this work item. */
  metrics: MetricUpdate[];
}

/** Information about a worker */
export interface WorkerDetails {
  /** Name of this worker */
  workerName: string;
  /** Work items processed by this worker, sorted by time. */
  workItems: WorkItemDetails[];
}

/** Information about the workers and work items within a stage. */
export interface StageExecutionDetails {
  /** Workers that have done work on the stage. */
  workers: WorkerDetails[];
  /**
   * If present, this response does not contain all requested tasks.  To obtain
   * the next page of results, repeat the request with page_token set to this
   * value.
   */
  nextPageToken: string;
}

function createBaseMetricStructuredName(): MetricStructuredName {
  return { origin: "", name: "", context: {} };
}

export const MetricStructuredName: MessageFns<MetricStructuredName> = {
  encode(message: MetricStructuredName, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.origin !== "") {
      writer.uint32(10).string(message.origin);
    }
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    Object.entries(message.context).forEach(([key, value]) => {
      MetricStructuredName_ContextEntry.encode({ key: key as any, value }, writer.uint32(26).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MetricStructuredName {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMetricStructuredName();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.origin = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          const entry3 = MetricStructuredName_ContextEntry.decode(reader, reader.uint32());
          if (entry3.value !== undefined) {
            message.context[entry3.key] = entry3.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MetricStructuredName {
    return {
      origin: isSet(object.origin) ? globalThis.String(object.origin) : "",
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      context: isObject(object.context)
        ? Object.entries(object.context).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: MetricStructuredName): unknown {
    const obj: any = {};
    if (message.origin !== "") {
      obj.origin = message.origin;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.context) {
      const entries = Object.entries(message.context);
      if (entries.length > 0) {
        obj.context = {};
        entries.forEach(([k, v]) => {
          obj.context[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<MetricStructuredName>): MetricStructuredName {
    return MetricStructuredName.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MetricStructuredName>): MetricStructuredName {
    const message = createBaseMetricStructuredName();
    message.origin = object.origin ?? "";
    message.name = object.name ?? "";
    message.context = Object.entries(object.context ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseMetricStructuredName_ContextEntry(): MetricStructuredName_ContextEntry {
  return { key: "", value: "" };
}

export const MetricStructuredName_ContextEntry: MessageFns<MetricStructuredName_ContextEntry> = {
  encode(message: MetricStructuredName_ContextEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MetricStructuredName_ContextEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMetricStructuredName_ContextEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MetricStructuredName_ContextEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: MetricStructuredName_ContextEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<MetricStructuredName_ContextEntry>): MetricStructuredName_ContextEntry {
    return MetricStructuredName_ContextEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MetricStructuredName_ContextEntry>): MetricStructuredName_ContextEntry {
    const message = createBaseMetricStructuredName_ContextEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseMetricUpdate(): MetricUpdate {
  return {
    name: undefined,
    kind: "",
    cumulative: false,
    scalar: undefined,
    meanSum: undefined,
    meanCount: undefined,
    set: undefined,
    distribution: undefined,
    gauge: undefined,
    internal: undefined,
    updateTime: undefined,
  };
}

export const MetricUpdate: MessageFns<MetricUpdate> = {
  encode(message: MetricUpdate, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== undefined) {
      MetricStructuredName.encode(message.name, writer.uint32(10).fork()).join();
    }
    if (message.kind !== "") {
      writer.uint32(18).string(message.kind);
    }
    if (message.cumulative !== false) {
      writer.uint32(24).bool(message.cumulative);
    }
    if (message.scalar !== undefined) {
      Value.encode(Value.wrap(message.scalar), writer.uint32(34).fork()).join();
    }
    if (message.meanSum !== undefined) {
      Value.encode(Value.wrap(message.meanSum), writer.uint32(42).fork()).join();
    }
    if (message.meanCount !== undefined) {
      Value.encode(Value.wrap(message.meanCount), writer.uint32(50).fork()).join();
    }
    if (message.set !== undefined) {
      Value.encode(Value.wrap(message.set), writer.uint32(58).fork()).join();
    }
    if (message.distribution !== undefined) {
      Value.encode(Value.wrap(message.distribution), writer.uint32(90).fork()).join();
    }
    if (message.gauge !== undefined) {
      Value.encode(Value.wrap(message.gauge), writer.uint32(98).fork()).join();
    }
    if (message.internal !== undefined) {
      Value.encode(Value.wrap(message.internal), writer.uint32(66).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MetricUpdate {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMetricUpdate();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = MetricStructuredName.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.cumulative = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.scalar = Value.unwrap(Value.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.meanSum = Value.unwrap(Value.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.meanCount = Value.unwrap(Value.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.set = Value.unwrap(Value.decode(reader, reader.uint32()));
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.distribution = Value.unwrap(Value.decode(reader, reader.uint32()));
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.gauge = Value.unwrap(Value.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.internal = Value.unwrap(Value.decode(reader, reader.uint32()));
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MetricUpdate {
    return {
      name: isSet(object.name) ? MetricStructuredName.fromJSON(object.name) : undefined,
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      cumulative: isSet(object.cumulative) ? globalThis.Boolean(object.cumulative) : false,
      scalar: isSet(object?.scalar) ? object.scalar : undefined,
      meanSum: isSet(object?.meanSum) ? object.meanSum : undefined,
      meanCount: isSet(object?.meanCount) ? object.meanCount : undefined,
      set: isSet(object?.set) ? object.set : undefined,
      distribution: isSet(object?.distribution) ? object.distribution : undefined,
      gauge: isSet(object?.gauge) ? object.gauge : undefined,
      internal: isSet(object?.internal) ? object.internal : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
    };
  },

  toJSON(message: MetricUpdate): unknown {
    const obj: any = {};
    if (message.name !== undefined) {
      obj.name = MetricStructuredName.toJSON(message.name);
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.cumulative !== false) {
      obj.cumulative = message.cumulative;
    }
    if (message.scalar !== undefined) {
      obj.scalar = message.scalar;
    }
    if (message.meanSum !== undefined) {
      obj.meanSum = message.meanSum;
    }
    if (message.meanCount !== undefined) {
      obj.meanCount = message.meanCount;
    }
    if (message.set !== undefined) {
      obj.set = message.set;
    }
    if (message.distribution !== undefined) {
      obj.distribution = message.distribution;
    }
    if (message.gauge !== undefined) {
      obj.gauge = message.gauge;
    }
    if (message.internal !== undefined) {
      obj.internal = message.internal;
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<MetricUpdate>): MetricUpdate {
    return MetricUpdate.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MetricUpdate>): MetricUpdate {
    const message = createBaseMetricUpdate();
    message.name = (object.name !== undefined && object.name !== null)
      ? MetricStructuredName.fromPartial(object.name)
      : undefined;
    message.kind = object.kind ?? "";
    message.cumulative = object.cumulative ?? false;
    message.scalar = object.scalar ?? undefined;
    message.meanSum = object.meanSum ?? undefined;
    message.meanCount = object.meanCount ?? undefined;
    message.set = object.set ?? undefined;
    message.distribution = object.distribution ?? undefined;
    message.gauge = object.gauge ?? undefined;
    message.internal = object.internal ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    return message;
  },
};

function createBaseGetJobMetricsRequest(): GetJobMetricsRequest {
  return { projectId: "", jobId: "", startTime: undefined, location: "" };
}

export const GetJobMetricsRequest: MessageFns<GetJobMetricsRequest> = {
  encode(message: GetJobMetricsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.jobId !== "") {
      writer.uint32(18).string(message.jobId);
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(26).fork()).join();
    }
    if (message.location !== "") {
      writer.uint32(34).string(message.location);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetJobMetricsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetJobMetricsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.jobId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.location = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetJobMetricsRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      jobId: isSet(object.jobId) ? globalThis.String(object.jobId) : "",
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      location: isSet(object.location) ? globalThis.String(object.location) : "",
    };
  },

  toJSON(message: GetJobMetricsRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.jobId !== "") {
      obj.jobId = message.jobId;
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    return obj;
  },

  create(base?: DeepPartial<GetJobMetricsRequest>): GetJobMetricsRequest {
    return GetJobMetricsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetJobMetricsRequest>): GetJobMetricsRequest {
    const message = createBaseGetJobMetricsRequest();
    message.projectId = object.projectId ?? "";
    message.jobId = object.jobId ?? "";
    message.startTime = object.startTime ?? undefined;
    message.location = object.location ?? "";
    return message;
  },
};

function createBaseJobMetrics(): JobMetrics {
  return { metricTime: undefined, metrics: [] };
}

export const JobMetrics: MessageFns<JobMetrics> = {
  encode(message: JobMetrics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.metricTime !== undefined) {
      Timestamp.encode(toTimestamp(message.metricTime), writer.uint32(10).fork()).join();
    }
    for (const v of message.metrics) {
      MetricUpdate.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobMetrics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobMetrics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.metricTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.metrics.push(MetricUpdate.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobMetrics {
    return {
      metricTime: isSet(object.metricTime) ? fromJsonTimestamp(object.metricTime) : undefined,
      metrics: globalThis.Array.isArray(object?.metrics)
        ? object.metrics.map((e: any) => MetricUpdate.fromJSON(e))
        : [],
    };
  },

  toJSON(message: JobMetrics): unknown {
    const obj: any = {};
    if (message.metricTime !== undefined) {
      obj.metricTime = message.metricTime.toISOString();
    }
    if (message.metrics?.length) {
      obj.metrics = message.metrics.map((e) => MetricUpdate.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<JobMetrics>): JobMetrics {
    return JobMetrics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobMetrics>): JobMetrics {
    const message = createBaseJobMetrics();
    message.metricTime = object.metricTime ?? undefined;
    message.metrics = object.metrics?.map((e) => MetricUpdate.fromPartial(e)) || [];
    return message;
  },
};

function createBaseGetJobExecutionDetailsRequest(): GetJobExecutionDetailsRequest {
  return { projectId: "", jobId: "", location: "", pageSize: 0, pageToken: "" };
}

export const GetJobExecutionDetailsRequest: MessageFns<GetJobExecutionDetailsRequest> = {
  encode(message: GetJobExecutionDetailsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.jobId !== "") {
      writer.uint32(18).string(message.jobId);
    }
    if (message.location !== "") {
      writer.uint32(26).string(message.location);
    }
    if (message.pageSize !== 0) {
      writer.uint32(32).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(42).string(message.pageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetJobExecutionDetailsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetJobExecutionDetailsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.jobId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.location = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.pageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetJobExecutionDetailsRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      jobId: isSet(object.jobId) ? globalThis.String(object.jobId) : "",
      location: isSet(object.location) ? globalThis.String(object.location) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
    };
  },

  toJSON(message: GetJobExecutionDetailsRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.jobId !== "") {
      obj.jobId = message.jobId;
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<GetJobExecutionDetailsRequest>): GetJobExecutionDetailsRequest {
    return GetJobExecutionDetailsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetJobExecutionDetailsRequest>): GetJobExecutionDetailsRequest {
    const message = createBaseGetJobExecutionDetailsRequest();
    message.projectId = object.projectId ?? "";
    message.jobId = object.jobId ?? "";
    message.location = object.location ?? "";
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    return message;
  },
};

function createBaseProgressTimeseries(): ProgressTimeseries {
  return { currentProgress: 0, dataPoints: [] };
}

export const ProgressTimeseries: MessageFns<ProgressTimeseries> = {
  encode(message: ProgressTimeseries, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.currentProgress !== 0) {
      writer.uint32(9).double(message.currentProgress);
    }
    for (const v of message.dataPoints) {
      ProgressTimeseries_Point.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ProgressTimeseries {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProgressTimeseries();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 9) {
            break;
          }

          message.currentProgress = reader.double();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.dataPoints.push(ProgressTimeseries_Point.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ProgressTimeseries {
    return {
      currentProgress: isSet(object.currentProgress) ? globalThis.Number(object.currentProgress) : 0,
      dataPoints: globalThis.Array.isArray(object?.dataPoints)
        ? object.dataPoints.map((e: any) => ProgressTimeseries_Point.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ProgressTimeseries): unknown {
    const obj: any = {};
    if (message.currentProgress !== 0) {
      obj.currentProgress = message.currentProgress;
    }
    if (message.dataPoints?.length) {
      obj.dataPoints = message.dataPoints.map((e) => ProgressTimeseries_Point.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ProgressTimeseries>): ProgressTimeseries {
    return ProgressTimeseries.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ProgressTimeseries>): ProgressTimeseries {
    const message = createBaseProgressTimeseries();
    message.currentProgress = object.currentProgress ?? 0;
    message.dataPoints = object.dataPoints?.map((e) => ProgressTimeseries_Point.fromPartial(e)) || [];
    return message;
  },
};

function createBaseProgressTimeseries_Point(): ProgressTimeseries_Point {
  return { time: undefined, value: 0 };
}

export const ProgressTimeseries_Point: MessageFns<ProgressTimeseries_Point> = {
  encode(message: ProgressTimeseries_Point, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.time !== undefined) {
      Timestamp.encode(toTimestamp(message.time), writer.uint32(10).fork()).join();
    }
    if (message.value !== 0) {
      writer.uint32(17).double(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ProgressTimeseries_Point {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProgressTimeseries_Point();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.time = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.value = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ProgressTimeseries_Point {
    return {
      time: isSet(object.time) ? fromJsonTimestamp(object.time) : undefined,
      value: isSet(object.value) ? globalThis.Number(object.value) : 0,
    };
  },

  toJSON(message: ProgressTimeseries_Point): unknown {
    const obj: any = {};
    if (message.time !== undefined) {
      obj.time = message.time.toISOString();
    }
    if (message.value !== 0) {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<ProgressTimeseries_Point>): ProgressTimeseries_Point {
    return ProgressTimeseries_Point.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ProgressTimeseries_Point>): ProgressTimeseries_Point {
    const message = createBaseProgressTimeseries_Point();
    message.time = object.time ?? undefined;
    message.value = object.value ?? 0;
    return message;
  },
};

function createBaseStageSummary(): StageSummary {
  return { stageId: "", state: 0, startTime: undefined, endTime: undefined, progress: undefined, metrics: [] };
}

export const StageSummary: MessageFns<StageSummary> = {
  encode(message: StageSummary, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.stageId !== "") {
      writer.uint32(10).string(message.stageId);
    }
    if (message.state !== 0) {
      writer.uint32(16).int32(message.state);
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(26).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(34).fork()).join();
    }
    if (message.progress !== undefined) {
      ProgressTimeseries.encode(message.progress, writer.uint32(42).fork()).join();
    }
    for (const v of message.metrics) {
      MetricUpdate.encode(v!, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StageSummary {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStageSummary();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.stageId = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.progress = ProgressTimeseries.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.metrics.push(MetricUpdate.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StageSummary {
    return {
      stageId: isSet(object.stageId) ? globalThis.String(object.stageId) : "",
      state: isSet(object.state) ? executionStateFromJSON(object.state) : 0,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      progress: isSet(object.progress) ? ProgressTimeseries.fromJSON(object.progress) : undefined,
      metrics: globalThis.Array.isArray(object?.metrics)
        ? object.metrics.map((e: any) => MetricUpdate.fromJSON(e))
        : [],
    };
  },

  toJSON(message: StageSummary): unknown {
    const obj: any = {};
    if (message.stageId !== "") {
      obj.stageId = message.stageId;
    }
    if (message.state !== 0) {
      obj.state = executionStateToJSON(message.state);
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (message.progress !== undefined) {
      obj.progress = ProgressTimeseries.toJSON(message.progress);
    }
    if (message.metrics?.length) {
      obj.metrics = message.metrics.map((e) => MetricUpdate.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<StageSummary>): StageSummary {
    return StageSummary.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StageSummary>): StageSummary {
    const message = createBaseStageSummary();
    message.stageId = object.stageId ?? "";
    message.state = object.state ?? 0;
    message.startTime = object.startTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    message.progress = (object.progress !== undefined && object.progress !== null)
      ? ProgressTimeseries.fromPartial(object.progress)
      : undefined;
    message.metrics = object.metrics?.map((e) => MetricUpdate.fromPartial(e)) || [];
    return message;
  },
};

function createBaseJobExecutionDetails(): JobExecutionDetails {
  return { stages: [], nextPageToken: "" };
}

export const JobExecutionDetails: MessageFns<JobExecutionDetails> = {
  encode(message: JobExecutionDetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.stages) {
      StageSummary.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobExecutionDetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobExecutionDetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.stages.push(StageSummary.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobExecutionDetails {
    return {
      stages: globalThis.Array.isArray(object?.stages) ? object.stages.map((e: any) => StageSummary.fromJSON(e)) : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: JobExecutionDetails): unknown {
    const obj: any = {};
    if (message.stages?.length) {
      obj.stages = message.stages.map((e) => StageSummary.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<JobExecutionDetails>): JobExecutionDetails {
    return JobExecutionDetails.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobExecutionDetails>): JobExecutionDetails {
    const message = createBaseJobExecutionDetails();
    message.stages = object.stages?.map((e) => StageSummary.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBaseGetStageExecutionDetailsRequest(): GetStageExecutionDetailsRequest {
  return {
    projectId: "",
    jobId: "",
    location: "",
    stageId: "",
    pageSize: 0,
    pageToken: "",
    startTime: undefined,
    endTime: undefined,
  };
}

export const GetStageExecutionDetailsRequest: MessageFns<GetStageExecutionDetailsRequest> = {
  encode(message: GetStageExecutionDetailsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.jobId !== "") {
      writer.uint32(18).string(message.jobId);
    }
    if (message.location !== "") {
      writer.uint32(26).string(message.location);
    }
    if (message.stageId !== "") {
      writer.uint32(34).string(message.stageId);
    }
    if (message.pageSize !== 0) {
      writer.uint32(40).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(50).string(message.pageToken);
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(58).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(66).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetStageExecutionDetailsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetStageExecutionDetailsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.jobId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.location = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.stageId = reader.string();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.pageToken = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetStageExecutionDetailsRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      jobId: isSet(object.jobId) ? globalThis.String(object.jobId) : "",
      location: isSet(object.location) ? globalThis.String(object.location) : "",
      stageId: isSet(object.stageId) ? globalThis.String(object.stageId) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
    };
  },

  toJSON(message: GetStageExecutionDetailsRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.jobId !== "") {
      obj.jobId = message.jobId;
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    if (message.stageId !== "") {
      obj.stageId = message.stageId;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<GetStageExecutionDetailsRequest>): GetStageExecutionDetailsRequest {
    return GetStageExecutionDetailsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetStageExecutionDetailsRequest>): GetStageExecutionDetailsRequest {
    const message = createBaseGetStageExecutionDetailsRequest();
    message.projectId = object.projectId ?? "";
    message.jobId = object.jobId ?? "";
    message.location = object.location ?? "";
    message.stageId = object.stageId ?? "";
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    message.startTime = object.startTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    return message;
  },
};

function createBaseWorkItemDetails(): WorkItemDetails {
  return {
    taskId: "",
    attemptId: "",
    startTime: undefined,
    endTime: undefined,
    state: 0,
    progress: undefined,
    metrics: [],
  };
}

export const WorkItemDetails: MessageFns<WorkItemDetails> = {
  encode(message: WorkItemDetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.taskId !== "") {
      writer.uint32(10).string(message.taskId);
    }
    if (message.attemptId !== "") {
      writer.uint32(18).string(message.attemptId);
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(26).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(34).fork()).join();
    }
    if (message.state !== 0) {
      writer.uint32(40).int32(message.state);
    }
    if (message.progress !== undefined) {
      ProgressTimeseries.encode(message.progress, writer.uint32(50).fork()).join();
    }
    for (const v of message.metrics) {
      MetricUpdate.encode(v!, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): WorkItemDetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseWorkItemDetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.taskId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.attemptId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.progress = ProgressTimeseries.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.metrics.push(MetricUpdate.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): WorkItemDetails {
    return {
      taskId: isSet(object.taskId) ? globalThis.String(object.taskId) : "",
      attemptId: isSet(object.attemptId) ? globalThis.String(object.attemptId) : "",
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      state: isSet(object.state) ? executionStateFromJSON(object.state) : 0,
      progress: isSet(object.progress) ? ProgressTimeseries.fromJSON(object.progress) : undefined,
      metrics: globalThis.Array.isArray(object?.metrics)
        ? object.metrics.map((e: any) => MetricUpdate.fromJSON(e))
        : [],
    };
  },

  toJSON(message: WorkItemDetails): unknown {
    const obj: any = {};
    if (message.taskId !== "") {
      obj.taskId = message.taskId;
    }
    if (message.attemptId !== "") {
      obj.attemptId = message.attemptId;
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (message.state !== 0) {
      obj.state = executionStateToJSON(message.state);
    }
    if (message.progress !== undefined) {
      obj.progress = ProgressTimeseries.toJSON(message.progress);
    }
    if (message.metrics?.length) {
      obj.metrics = message.metrics.map((e) => MetricUpdate.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<WorkItemDetails>): WorkItemDetails {
    return WorkItemDetails.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<WorkItemDetails>): WorkItemDetails {
    const message = createBaseWorkItemDetails();
    message.taskId = object.taskId ?? "";
    message.attemptId = object.attemptId ?? "";
    message.startTime = object.startTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    message.state = object.state ?? 0;
    message.progress = (object.progress !== undefined && object.progress !== null)
      ? ProgressTimeseries.fromPartial(object.progress)
      : undefined;
    message.metrics = object.metrics?.map((e) => MetricUpdate.fromPartial(e)) || [];
    return message;
  },
};

function createBaseWorkerDetails(): WorkerDetails {
  return { workerName: "", workItems: [] };
}

export const WorkerDetails: MessageFns<WorkerDetails> = {
  encode(message: WorkerDetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.workerName !== "") {
      writer.uint32(10).string(message.workerName);
    }
    for (const v of message.workItems) {
      WorkItemDetails.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): WorkerDetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseWorkerDetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.workerName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.workItems.push(WorkItemDetails.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): WorkerDetails {
    return {
      workerName: isSet(object.workerName) ? globalThis.String(object.workerName) : "",
      workItems: globalThis.Array.isArray(object?.workItems)
        ? object.workItems.map((e: any) => WorkItemDetails.fromJSON(e))
        : [],
    };
  },

  toJSON(message: WorkerDetails): unknown {
    const obj: any = {};
    if (message.workerName !== "") {
      obj.workerName = message.workerName;
    }
    if (message.workItems?.length) {
      obj.workItems = message.workItems.map((e) => WorkItemDetails.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<WorkerDetails>): WorkerDetails {
    return WorkerDetails.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<WorkerDetails>): WorkerDetails {
    const message = createBaseWorkerDetails();
    message.workerName = object.workerName ?? "";
    message.workItems = object.workItems?.map((e) => WorkItemDetails.fromPartial(e)) || [];
    return message;
  },
};

function createBaseStageExecutionDetails(): StageExecutionDetails {
  return { workers: [], nextPageToken: "" };
}

export const StageExecutionDetails: MessageFns<StageExecutionDetails> = {
  encode(message: StageExecutionDetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.workers) {
      WorkerDetails.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StageExecutionDetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStageExecutionDetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.workers.push(WorkerDetails.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StageExecutionDetails {
    return {
      workers: globalThis.Array.isArray(object?.workers)
        ? object.workers.map((e: any) => WorkerDetails.fromJSON(e))
        : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: StageExecutionDetails): unknown {
    const obj: any = {};
    if (message.workers?.length) {
      obj.workers = message.workers.map((e) => WorkerDetails.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<StageExecutionDetails>): StageExecutionDetails {
    return StageExecutionDetails.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StageExecutionDetails>): StageExecutionDetails {
    const message = createBaseStageExecutionDetails();
    message.workers = object.workers?.map((e) => WorkerDetails.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

/**
 * The Dataflow Metrics API lets you monitor the progress of Dataflow
 * jobs.
 */
export type MetricsV1Beta3Definition = typeof MetricsV1Beta3Definition;
export const MetricsV1Beta3Definition = {
  name: "MetricsV1Beta3",
  fullName: "google.dataflow.v1beta3.MetricsV1Beta3",
  methods: {
    /**
     * Request the job status.
     *
     * To request the status of a job, we recommend using
     * `projects.locations.jobs.getMetrics` with a [regional endpoint]
     * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints). Using
     * `projects.jobs.getMetrics` is not recommended, as you can only request the
     * status of jobs that are running in `us-central1`.
     */
    getJobMetrics: {
      name: "GetJobMetrics",
      requestType: GetJobMetricsRequest,
      requestStream: false,
      responseType: JobMetrics,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              125,
              90,
              51,
              18,
              49,
              47,
              118,
              49,
              98,
              51,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              106,
              111,
              98,
              115,
              47,
              123,
              106,
              111,
              98,
              95,
              105,
              100,
              125,
              47,
              109,
              101,
              116,
              114,
              105,
              99,
              115,
              18,
              70,
              47,
              118,
              49,
              98,
              51,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              123,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              125,
              47,
              106,
              111,
              98,
              115,
              47,
              123,
              106,
              111,
              98,
              95,
              105,
              100,
              125,
              47,
              109,
              101,
              116,
              114,
              105,
              99,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Request detailed information about the execution status of the job.
     *
     * EXPERIMENTAL.  This API is subject to change or removal without notice.
     */
    getJobExecutionDetails: {
      name: "GetJobExecutionDetails",
      requestType: GetJobExecutionDetailsRequest,
      requestStream: false,
      responseType: JobExecutionDetails,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              81,
              18,
              79,
              47,
              118,
              49,
              98,
              51,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              123,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              125,
              47,
              106,
              111,
              98,
              115,
              47,
              123,
              106,
              111,
              98,
              95,
              105,
              100,
              125,
              47,
              101,
              120,
              101,
              99,
              117,
              116,
              105,
              111,
              110,
              68,
              101,
              116,
              97,
              105,
              108,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Request detailed information about the execution status of a stage of the
     * job.
     *
     * EXPERIMENTAL.  This API is subject to change or removal without notice.
     */
    getStageExecutionDetails: {
      name: "GetStageExecutionDetails",
      requestType: GetStageExecutionDetailsRequest,
      requestStream: false,
      responseType: StageExecutionDetails,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              99,
              18,
              97,
              47,
              118,
              49,
              98,
              51,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              123,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              125,
              47,
              106,
              111,
              98,
              115,
              47,
              123,
              106,
              111,
              98,
              95,
              105,
              100,
              125,
              47,
              115,
              116,
              97,
              103,
              101,
              115,
              47,
              123,
              115,
              116,
              97,
              103,
              101,
              95,
              105,
              100,
              125,
              47,
              101,
              120,
              101,
              99,
              117,
              116,
              105,
              111,
              110,
              68,
              101,
              116,
              97,
              105,
              108,
              115,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface MetricsV1Beta3ServiceImplementation<CallContextExt = {}> {
  /**
   * Request the job status.
   *
   * To request the status of a job, we recommend using
   * `projects.locations.jobs.getMetrics` with a [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints). Using
   * `projects.jobs.getMetrics` is not recommended, as you can only request the
   * status of jobs that are running in `us-central1`.
   */
  getJobMetrics(request: GetJobMetricsRequest, context: CallContext & CallContextExt): Promise<DeepPartial<JobMetrics>>;
  /**
   * Request detailed information about the execution status of the job.
   *
   * EXPERIMENTAL.  This API is subject to change or removal without notice.
   */
  getJobExecutionDetails(
    request: GetJobExecutionDetailsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<JobExecutionDetails>>;
  /**
   * Request detailed information about the execution status of a stage of the
   * job.
   *
   * EXPERIMENTAL.  This API is subject to change or removal without notice.
   */
  getStageExecutionDetails(
    request: GetStageExecutionDetailsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<StageExecutionDetails>>;
}

export interface MetricsV1Beta3Client<CallOptionsExt = {}> {
  /**
   * Request the job status.
   *
   * To request the status of a job, we recommend using
   * `projects.locations.jobs.getMetrics` with a [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints). Using
   * `projects.jobs.getMetrics` is not recommended, as you can only request the
   * status of jobs that are running in `us-central1`.
   */
  getJobMetrics(
    request: DeepPartial<GetJobMetricsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<JobMetrics>;
  /**
   * Request detailed information about the execution status of the job.
   *
   * EXPERIMENTAL.  This API is subject to change or removal without notice.
   */
  getJobExecutionDetails(
    request: DeepPartial<GetJobExecutionDetailsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<JobExecutionDetails>;
  /**
   * Request detailed information about the execution status of a stage of the
   * job.
   *
   * EXPERIMENTAL.  This API is subject to change or removal without notice.
   */
  getStageExecutionDetails(
    request: DeepPartial<GetStageExecutionDetailsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<StageExecutionDetails>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
