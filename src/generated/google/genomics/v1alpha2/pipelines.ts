// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/genomics/v1alpha2/pipelines.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Operation } from "../../longrunning/operations.js";
import { Duration } from "../../protobuf/duration.js";
import { Empty } from "../../protobuf/empty.js";
import { Timestamp } from "../../protobuf/timestamp.js";
import { Code, codeFromJSON, codeToJSON } from "../../rpc/code.js";

export const protobufPackage = "google.genomics.v1alpha2";

/**
 * Describes a Compute Engine resource that is being managed by a running
 * [pipeline][google.genomics.v1alpha2.Pipeline].
 */
export interface ComputeEngine {
  /** The instance on which the operation is running. */
  instanceName: string;
  /** The availability zone in which the instance resides. */
  zone: string;
  /** The machine type of the instance. */
  machineType: string;
  /** The names of the disks that were created for this pipeline. */
  diskNames: string[];
}

/**
 * Runtime metadata that will be populated in the
 * [runtimeMetadata][google.genomics.v1.OperationMetadata.runtime_metadata]
 * field of the Operation associated with a RunPipeline execution.
 */
export interface RuntimeMetadata {
  /** Execution information specific to Google Compute Engine. */
  computeEngine: ComputeEngine | undefined;
}

/**
 * The pipeline object. Represents a transformation from a set of input
 * parameters to a set of output parameters. The transformation is defined
 * as a docker image and command to run within that image. Each pipeline
 * is run on a Google Compute Engine VM. A pipeline can be created with the
 * `create` method and then later run with the `run` method, or a pipeline can
 * be defined and run all at once with the `run` method.
 */
export interface Pipeline {
  /**
   * Required. The project in which to create the pipeline. The caller must have
   * WRITE access.
   */
  projectId: string;
  /**
   * Required. A user specified pipeline name that does not have to be unique.
   * This name can be used for filtering Pipelines in ListPipelines.
   */
  name: string;
  /** User-specified description. */
  description: string;
  /** Input parameters of the pipeline. */
  inputParameters: PipelineParameter[];
  /** Output parameters of the pipeline. */
  outputParameters: PipelineParameter[];
  /** Specifies the docker run information. */
  docker?:
    | DockerExecutor
    | undefined;
  /**
   * Required. Specifies resource requirements for the pipeline run.
   * Required fields:
   *
   * *
   * [minimumCpuCores][google.genomics.v1alpha2.PipelineResources.minimum_cpu_cores]
   *
   * *
   * [minimumRamGb][google.genomics.v1alpha2.PipelineResources.minimum_ram_gb]
   */
  resources:
    | PipelineResources
    | undefined;
  /**
   * Unique pipeline id that is generated by the service when CreatePipeline
   * is called. Cannot be specified in the Pipeline used in the
   * CreatePipelineRequest, and will be populated in the response to
   * CreatePipeline and all subsequent Get and List calls. Indicates that the
   * service has registered this pipeline.
   */
  pipelineId: string;
}

/**
 * The request to create a pipeline. The pipeline field here should not have
 * `pipelineId` populated, as that will be populated by the server.
 */
export interface CreatePipelineRequest {
  /** The pipeline to create. Should not have `pipelineId` populated. */
  pipeline: Pipeline | undefined;
}

/** The pipeline run arguments. */
export interface RunPipelineArgs {
  /**
   * Required. The project in which to run the pipeline. The caller must have
   * WRITER access to all Google Cloud services and resources (e.g. Google
   * Compute Engine) will be used.
   */
  projectId: string;
  /**
   * Pipeline input arguments; keys are defined in the pipeline documentation.
   * All input parameters that do not have default values  must be specified.
   * If parameters with defaults are specified here, the defaults will be
   * overridden.
   */
  inputs: { [key: string]: string };
  /**
   * Pipeline output arguments; keys are defined in the pipeline
   * documentation.  All output parameters of without default values
   * must be specified.  If parameters with defaults are specified
   * here, the defaults will be overridden.
   */
  outputs: { [key: string]: string };
  /**
   * The Google Cloud Service Account that will be used to access data and
   * services. By default, the compute service account associated with
   * `projectId` is used.
   */
  serviceAccount:
    | ServiceAccount
    | undefined;
  /**
   * This field is deprecated. Use `labels` instead. Client-specified pipeline
   * operation identifier.
   */
  clientId: string;
  /** Specifies resource requirements/overrides for the pipeline run. */
  resources:
    | PipelineResources
    | undefined;
  /**
   * Required. Logging options. Used by the service to communicate results
   * to the user.
   */
  logging:
    | LoggingOptions
    | undefined;
  /**
   * How long to keep the VM up after a failure (for example docker command
   * failed, copying input or output files failed, etc). While the VM is up, one
   * can ssh into the VM to debug. Default is 0; maximum allowed value is 1 day.
   */
  keepVmAliveOnFailureDuration:
    | Duration
    | undefined;
  /**
   * Labels to apply to this pipeline run. Labels will also be applied to
   * compute resources (VM, disks) created by this pipeline run. When listing
   * operations, operations can [filtered by labels]
   * [google.longrunning.ListOperationsRequest.filter].
   * Label keys may not be empty; label values may be empty. Non-empty labels
   * must be 1-63 characters long, and comply with [RFC1035]
   * (https://www.ietf.org/rfc/rfc1035.txt).
   * Specifically, the name must be 1-63 characters long and match the regular
   * expression `[a-z]([-a-z0-9]*[a-z0-9])?` which means the first
   * character must be a lowercase letter, and all following characters must be
   * a dash, lowercase letter, or digit, except the last character, which cannot
   * be a dash.
   */
  labels: { [key: string]: string };
}

export interface RunPipelineArgs_InputsEntry {
  key: string;
  value: string;
}

export interface RunPipelineArgs_OutputsEntry {
  key: string;
  value: string;
}

export interface RunPipelineArgs_LabelsEntry {
  key: string;
  value: string;
}

/**
 * The request to run a pipeline. If `pipelineId` is specified, it
 * refers to a saved pipeline created with CreatePipeline and set as
 * the `pipelineId` of the returned Pipeline object. If
 * `ephemeralPipeline` is specified, that pipeline is run once
 * with the given args and not saved. It is an error to specify both
 * `pipelineId` and `ephemeralPipeline`. `pipelineArgs`
 * must be specified.
 */
export interface RunPipelineRequest {
  /** The already created pipeline to run. */
  pipelineId?:
    | string
    | undefined;
  /** A new pipeline object to run once and then delete. */
  ephemeralPipeline?:
    | Pipeline
    | undefined;
  /** The arguments to use when running this pipeline. */
  pipelineArgs: RunPipelineArgs | undefined;
}

/** A request to get a saved pipeline by id. */
export interface GetPipelineRequest {
  /**
   * Caller must have READ access to the project in which this pipeline
   * is defined.
   */
  pipelineId: string;
}

/**
 * A request to list pipelines in a given project. Pipelines can be
 * filtered by name using `namePrefix`: all pipelines with names that
 * begin with `namePrefix` will be returned. Uses standard pagination:
 * `pageSize` indicates how many pipelines to return, and
 * `pageToken` comes from a previous ListPipelinesResponse to
 * indicate offset.
 */
export interface ListPipelinesRequest {
  /**
   * Required. The name of the project to search for pipelines. Caller
   * must have READ access to this project.
   */
  projectId: string;
  /**
   * Pipelines with names that match this prefix should be
   * returned.  If unspecified, all pipelines in the project, up to
   * `pageSize`, will be returned.
   */
  namePrefix: string;
  /**
   * Number of pipelines to return at once. Defaults to 256, and max
   * is 2048.
   */
  pageSize: number;
  /**
   * Token to use to indicate where to start getting results.
   * If unspecified, returns the first page of results.
   */
  pageToken: string;
}

/**
 * The response of ListPipelines. Contains at most `pageSize`
 * pipelines. If it contains `pageSize` pipelines, and more pipelines
 * exist, then `nextPageToken` will be populated and should be
 * used as the `pageToken` argument to a subsequent ListPipelines
 * request.
 */
export interface ListPipelinesResponse {
  /** The matched pipelines. */
  pipelines: Pipeline[];
  /** The token to use to get the next page of results. */
  nextPageToken: string;
}

/** The request to delete a saved pipeline by ID. */
export interface DeletePipelineRequest {
  /**
   * Caller must have WRITE access to the project in which this pipeline
   * is defined.
   */
  pipelineId: string;
}

/**
 * Request to get controller configuation.  Should only be used
 * by VMs created by the Pipelines Service and not by end users.
 */
export interface GetControllerConfigRequest {
  /** The operation to retrieve controller configuration for. */
  operationId: string;
  validationToken: Long;
}

/**
 * Stores the information that the controller will fetch from the
 * server in order to run. Should only be used by VMs created by the
 * Pipelines Service and not by end users.
 */
export interface ControllerConfig {
  image: string;
  cmd: string;
  gcsLogPath: string;
  machineType: string;
  vars: { [key: string]: string };
  disks: { [key: string]: string };
  gcsSources: { [key: string]: ControllerConfig_RepeatedString };
  gcsSinks: { [key: string]: ControllerConfig_RepeatedString };
}

export interface ControllerConfig_RepeatedString {
  values: string[];
}

export interface ControllerConfig_VarsEntry {
  key: string;
  value: string;
}

export interface ControllerConfig_DisksEntry {
  key: string;
  value: string;
}

export interface ControllerConfig_GcsSourcesEntry {
  key: string;
  value: ControllerConfig_RepeatedString | undefined;
}

export interface ControllerConfig_GcsSinksEntry {
  key: string;
  value: ControllerConfig_RepeatedString | undefined;
}

/**
 * Stores the list of events and times they occured for major events in job
 * execution.
 */
export interface TimestampEvent {
  /** String indicating the type of event */
  description: string;
  /** The time this event occured. */
  timestamp: Date | undefined;
}

/**
 * Request to set operation status. Should only be used by VMs
 * created by the Pipelines Service and not by end users.
 */
export interface SetOperationStatusRequest {
  operationId: string;
  timestampEvents: TimestampEvent[];
  errorCode: Code;
  errorMessage: string;
  validationToken: Long;
}

/** A Google Cloud Service Account. */
export interface ServiceAccount {
  /**
   * Email address of the service account. Defaults to `default`,
   * which uses the compute service account associated with the project.
   */
  email: string;
  /**
   * List of scopes to be enabled for this service account on the VM.
   * The following scopes are automatically included:
   *
   * * https://www.googleapis.com/auth/compute
   * * https://www.googleapis.com/auth/devstorage.full_control
   * * https://www.googleapis.com/auth/genomics
   * * https://www.googleapis.com/auth/logging.write
   * * https://www.googleapis.com/auth/monitoring.write
   */
  scopes: string[];
}

/** The logging options for the pipeline run. */
export interface LoggingOptions {
  /**
   * The location in Google Cloud Storage to which the pipeline logs
   * will be copied. Can be specified as a fully qualified directory
   * path, in which case logs will be output with a unique identifier
   * as the filename in that directory, or as a fully specified path,
   * which must end in `.log`, in which case that path will be
   * used, and the user must ensure that logs are not
   * overwritten. Stdout and stderr logs from the run are also
   * generated and output as `-stdout.log` and `-stderr.log`.
   */
  gcsPath: string;
}

/** The system resources for the pipeline run. */
export interface PipelineResources {
  /** The minimum number of cores to use. Defaults to 1. */
  minimumCpuCores: number;
  /**
   * Whether to use preemptible VMs. Defaults to `false`. In order to use this,
   * must be true for both create time and run time. Cannot be true at run time
   * if false at create time.
   */
  preemptible: boolean;
  /** The minimum amount of RAM to use. Defaults to 3.75 (GB) */
  minimumRamGb: number;
  /** Disks to attach. */
  disks: PipelineResources_Disk[];
  /**
   * List of Google Compute Engine availability zones to which resource
   * creation will restricted. If empty, any zone may be chosen.
   */
  zones: string[];
  /** The size of the boot disk. Defaults to 10 (GB). */
  bootDiskSizeGb: number;
  /**
   * Whether to assign an external IP to the instance. This is an experimental
   * feature that may go away. Defaults to false.
   * Corresponds to `--no_address` flag for [gcloud compute instances create]
   * (https://cloud.google.com/sdk/gcloud/reference/compute/instances/create).
   * In order to use this, must be true for both create time and run time.
   * Cannot be true at run time if false at create time. If you need to ssh into
   * a private IP VM for debugging, you can ssh to a public VM and then ssh into
   * the private VM's Internal IP.  If noAddress is set, this pipeline run may
   * only load docker images from Google Container Registry and not Docker Hub.
   * ** Note: To use this option, your project must be in Google Access for
   * Private IPs Early Access Program.**
   */
  noAddress: boolean;
}

/** A Google Compute Engine disk resource specification. */
export interface PipelineResources_Disk {
  /**
   * Required. The name of the disk that can be used in the pipeline
   * parameters. Must be 1 - 63 characters.
   * The name "boot" is reserved for system use.
   */
  name: string;
  /** Required. The type of the disk to create. */
  type: PipelineResources_Disk_Type;
  /**
   * The size of the disk. Defaults to 500 (GB).
   * This field is not applicable for local SSD.
   */
  sizeGb: number;
  /**
   * The full or partial URL of the persistent disk to attach. See
   * https://cloud.google.com/compute/docs/reference/latest/instances#resource
   * and
   * https://cloud.google.com/compute/docs/disks/persistent-disks#snapshots
   * for more details.
   */
  source: string;
  /**
   * Deprecated. Disks created by the Pipelines API will be deleted at the end
   * of the pipeline run, regardless of what this field is set to.
   */
  autoDelete: boolean;
  /**
   * Required at create time and cannot be overridden at run time.
   * Specifies the path in the docker container where files on
   * this disk should be located. For example, if `mountPoint`
   * is `/mnt/disk`, and the parameter has `localPath`
   * `inputs/file.txt`, the docker container can access the data at
   * `/mnt/disk/inputs/file.txt`.
   */
  mountPoint: string;
}

/** The types of disks that may be attached to VMs. */
export enum PipelineResources_Disk_Type {
  /** TYPE_UNSPECIFIED - Default disk type. Use one of the other options below. */
  TYPE_UNSPECIFIED = 0,
  /**
   * PERSISTENT_HDD - Specifies a Google Compute Engine persistent hard disk. See
   * https://cloud.google.com/compute/docs/disks/#pdspecs for details.
   */
  PERSISTENT_HDD = 1,
  /**
   * PERSISTENT_SSD - Specifies a Google Compute Engine persistent solid-state disk. See
   * https://cloud.google.com/compute/docs/disks/#pdspecs for details.
   */
  PERSISTENT_SSD = 2,
  /**
   * LOCAL_SSD - Specifies a Google Compute Engine local SSD.
   * See https://cloud.google.com/compute/docs/disks/local-ssd for details.
   */
  LOCAL_SSD = 3,
  UNRECOGNIZED = -1,
}

export function pipelineResources_Disk_TypeFromJSON(object: any): PipelineResources_Disk_Type {
  switch (object) {
    case 0:
    case "TYPE_UNSPECIFIED":
      return PipelineResources_Disk_Type.TYPE_UNSPECIFIED;
    case 1:
    case "PERSISTENT_HDD":
      return PipelineResources_Disk_Type.PERSISTENT_HDD;
    case 2:
    case "PERSISTENT_SSD":
      return PipelineResources_Disk_Type.PERSISTENT_SSD;
    case 3:
    case "LOCAL_SSD":
      return PipelineResources_Disk_Type.LOCAL_SSD;
    case -1:
    case "UNRECOGNIZED":
    default:
      return PipelineResources_Disk_Type.UNRECOGNIZED;
  }
}

export function pipelineResources_Disk_TypeToJSON(object: PipelineResources_Disk_Type): string {
  switch (object) {
    case PipelineResources_Disk_Type.TYPE_UNSPECIFIED:
      return "TYPE_UNSPECIFIED";
    case PipelineResources_Disk_Type.PERSISTENT_HDD:
      return "PERSISTENT_HDD";
    case PipelineResources_Disk_Type.PERSISTENT_SSD:
      return "PERSISTENT_SSD";
    case PipelineResources_Disk_Type.LOCAL_SSD:
      return "LOCAL_SSD";
    case PipelineResources_Disk_Type.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Parameters facilitate setting and delivering data into the
 * pipeline's execution environment. They are defined at create time,
 * with optional defaults, and can be overridden at run time.
 *
 * If `localCopy` is unset, then the parameter specifies a string that
 * is passed as-is into the pipeline, as the value of the environment
 * variable with the given name.  A default value can be optionally
 * specified at create time. The default can be overridden at run time
 * using the inputs map. If no default is given, a value must be
 * supplied at runtime.
 *
 * If `localCopy` is defined, then the parameter specifies a data
 * source or sink, both in Google Cloud Storage and on the Docker container
 * where the pipeline computation is run. The [service account associated with
 * the Pipeline][google.genomics.v1alpha2.RunPipelineArgs.service_account] (by
 * default the project's Compute Engine service account) must have access to the
 * Google Cloud Storage paths.
 *
 * At run time, the Google Cloud Storage paths can be overridden if a default
 * was provided at create time, or must be set otherwise. The pipeline runner
 * should add a key/value pair to either the inputs or outputs map. The
 * indicated data copies will be carried out before/after pipeline execution,
 * just as if the corresponding arguments were provided to `gsutil cp`.
 *
 * For example: Given the following `PipelineParameter`, specified
 * in the `inputParameters` list:
 *
 * ```
 * {name: "input_file", localCopy: {path: "file.txt", disk: "pd1"}}
 * ```
 *
 * where `disk` is defined in the `PipelineResources` object as:
 *
 * ```
 * {name: "pd1", mountPoint: "/mnt/disk/"}
 * ```
 *
 * We create a disk named `pd1`, mount it on the host VM, and map
 * `/mnt/pd1` to `/mnt/disk` in the docker container.  At
 * runtime, an entry for `input_file` would be required in the inputs
 * map, such as:
 *
 * ```
 *   inputs["input_file"] = "gs://my-bucket/bar.txt"
 * ```
 *
 * This would generate the following gsutil call:
 *
 * ```
 *   gsutil cp gs://my-bucket/bar.txt /mnt/pd1/file.txt
 * ```
 *
 * The file `/mnt/pd1/file.txt` maps to `/mnt/disk/file.txt` in the
 * Docker container. Acceptable paths are:
 *
 * <table>
 *   <thead>
 *     <tr><th>Google Cloud storage path</th><th>Local path</th></tr>
 *   </thead>
 *   <tbody>
 *     <tr><td>file</td><td>file</td></tr>
 *     <tr><td>glob</td><td>directory</td></tr>
 *   </tbody>
 * </table>
 *
 * For outputs, the direction of the copy is reversed:
 *
 * ```
 *   gsutil cp /mnt/disk/file.txt gs://my-bucket/bar.txt
 * ```
 *
 * Acceptable paths are:
 *
 * <table>
 *   <thead>
 *     <tr><th>Local path</th><th>Google Cloud Storage path</th></tr>
 *   </thead>
 *   <tbody>
 *     <tr><td>file</td><td>file</td></tr>
 *     <tr>
 *       <td>file</td>
 *       <td>directory - directory must already exist</td>
 *     </tr>
 *     <tr>
 *       <td>glob</td>
 *       <td>directory - directory will be created if it doesn't exist</td></tr>
 *   </tbody>
 * </table>
 *
 * One restriction due to docker limitations, is that for outputs that are found
 * on the boot disk, the local path cannot be a glob and must be a file.
 */
export interface PipelineParameter {
  /**
   * Required. Name of the parameter - the pipeline runner uses this string
   * as the key to the input and output maps in RunPipeline.
   */
  name: string;
  /** Human-readable description. */
  description: string;
  /**
   * The default value for this parameter. Can be overridden at runtime.
   * If `localCopy` is present, then this must be a Google Cloud Storage path
   * beginning with `gs://`.
   */
  defaultValue: string;
  /**
   * If present, this parameter is marked for copying to and from the VM.
   * `LocalCopy` indicates where on the VM the file should be. The value
   * given to this parameter (either at runtime or using `defaultValue`)
   * must be the remote path where the file should be.
   */
  localCopy: PipelineParameter_LocalCopy | undefined;
}

/** LocalCopy defines how a remote file should be copied to and from the VM. */
export interface PipelineParameter_LocalCopy {
  /**
   * Required. The path within the user's docker container where
   * this input should be localized to and from, relative to the specified
   * disk's mount point. For example: file.txt,
   */
  path: string;
  /**
   * Required. The name of the disk where this parameter is
   * located. Can be the name of one of the disks specified in the
   * Resources field, or "boot", which represents the Docker
   * instance's boot disk and has a mount point of `/`.
   */
  disk: string;
}

/** The Docker execuctor specification. */
export interface DockerExecutor {
  /**
   * Required. Image name from either Docker Hub or Google Container Registry.
   * Users that run pipelines must have READ access to the image.
   */
  imageName: string;
  /**
   * Required. The command or newline delimited script to run. The command
   * string will be executed within a bash shell.
   *
   * If the command exits with a non-zero exit code, output parameter
   * de-localization will be skipped and the pipeline operation's
   * [`error`][google.longrunning.Operation.error] field will be populated.
   *
   * Maximum command string length is 16384.
   */
  cmd: string;
}

function createBaseComputeEngine(): ComputeEngine {
  return { instanceName: "", zone: "", machineType: "", diskNames: [] };
}

export const ComputeEngine: MessageFns<ComputeEngine> = {
  encode(message: ComputeEngine, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.instanceName !== "") {
      writer.uint32(10).string(message.instanceName);
    }
    if (message.zone !== "") {
      writer.uint32(18).string(message.zone);
    }
    if (message.machineType !== "") {
      writer.uint32(26).string(message.machineType);
    }
    for (const v of message.diskNames) {
      writer.uint32(34).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ComputeEngine {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseComputeEngine();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.instanceName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.zone = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.machineType = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.diskNames.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ComputeEngine {
    return {
      instanceName: isSet(object.instanceName) ? globalThis.String(object.instanceName) : "",
      zone: isSet(object.zone) ? globalThis.String(object.zone) : "",
      machineType: isSet(object.machineType) ? globalThis.String(object.machineType) : "",
      diskNames: globalThis.Array.isArray(object?.diskNames)
        ? object.diskNames.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: ComputeEngine): unknown {
    const obj: any = {};
    if (message.instanceName !== "") {
      obj.instanceName = message.instanceName;
    }
    if (message.zone !== "") {
      obj.zone = message.zone;
    }
    if (message.machineType !== "") {
      obj.machineType = message.machineType;
    }
    if (message.diskNames?.length) {
      obj.diskNames = message.diskNames;
    }
    return obj;
  },

  create(base?: DeepPartial<ComputeEngine>): ComputeEngine {
    return ComputeEngine.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ComputeEngine>): ComputeEngine {
    const message = createBaseComputeEngine();
    message.instanceName = object.instanceName ?? "";
    message.zone = object.zone ?? "";
    message.machineType = object.machineType ?? "";
    message.diskNames = object.diskNames?.map((e) => e) || [];
    return message;
  },
};

function createBaseRuntimeMetadata(): RuntimeMetadata {
  return { computeEngine: undefined };
}

export const RuntimeMetadata: MessageFns<RuntimeMetadata> = {
  encode(message: RuntimeMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.computeEngine !== undefined) {
      ComputeEngine.encode(message.computeEngine, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RuntimeMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRuntimeMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.computeEngine = ComputeEngine.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RuntimeMetadata {
    return { computeEngine: isSet(object.computeEngine) ? ComputeEngine.fromJSON(object.computeEngine) : undefined };
  },

  toJSON(message: RuntimeMetadata): unknown {
    const obj: any = {};
    if (message.computeEngine !== undefined) {
      obj.computeEngine = ComputeEngine.toJSON(message.computeEngine);
    }
    return obj;
  },

  create(base?: DeepPartial<RuntimeMetadata>): RuntimeMetadata {
    return RuntimeMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RuntimeMetadata>): RuntimeMetadata {
    const message = createBaseRuntimeMetadata();
    message.computeEngine = (object.computeEngine !== undefined && object.computeEngine !== null)
      ? ComputeEngine.fromPartial(object.computeEngine)
      : undefined;
    return message;
  },
};

function createBasePipeline(): Pipeline {
  return {
    projectId: "",
    name: "",
    description: "",
    inputParameters: [],
    outputParameters: [],
    docker: undefined,
    resources: undefined,
    pipelineId: "",
  };
}

export const Pipeline: MessageFns<Pipeline> = {
  encode(message: Pipeline, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    if (message.description !== "") {
      writer.uint32(26).string(message.description);
    }
    for (const v of message.inputParameters) {
      PipelineParameter.encode(v!, writer.uint32(66).fork()).join();
    }
    for (const v of message.outputParameters) {
      PipelineParameter.encode(v!, writer.uint32(74).fork()).join();
    }
    if (message.docker !== undefined) {
      DockerExecutor.encode(message.docker, writer.uint32(42).fork()).join();
    }
    if (message.resources !== undefined) {
      PipelineResources.encode(message.resources, writer.uint32(50).fork()).join();
    }
    if (message.pipelineId !== "") {
      writer.uint32(58).string(message.pipelineId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Pipeline {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipeline();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.description = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.inputParameters.push(PipelineParameter.decode(reader, reader.uint32()));
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.outputParameters.push(PipelineParameter.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.docker = DockerExecutor.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.resources = PipelineResources.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.pipelineId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Pipeline {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      inputParameters: globalThis.Array.isArray(object?.inputParameters)
        ? object.inputParameters.map((e: any) => PipelineParameter.fromJSON(e))
        : [],
      outputParameters: globalThis.Array.isArray(object?.outputParameters)
        ? object.outputParameters.map((e: any) => PipelineParameter.fromJSON(e))
        : [],
      docker: isSet(object.docker) ? DockerExecutor.fromJSON(object.docker) : undefined,
      resources: isSet(object.resources) ? PipelineResources.fromJSON(object.resources) : undefined,
      pipelineId: isSet(object.pipelineId) ? globalThis.String(object.pipelineId) : "",
    };
  },

  toJSON(message: Pipeline): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.inputParameters?.length) {
      obj.inputParameters = message.inputParameters.map((e) => PipelineParameter.toJSON(e));
    }
    if (message.outputParameters?.length) {
      obj.outputParameters = message.outputParameters.map((e) => PipelineParameter.toJSON(e));
    }
    if (message.docker !== undefined) {
      obj.docker = DockerExecutor.toJSON(message.docker);
    }
    if (message.resources !== undefined) {
      obj.resources = PipelineResources.toJSON(message.resources);
    }
    if (message.pipelineId !== "") {
      obj.pipelineId = message.pipelineId;
    }
    return obj;
  },

  create(base?: DeepPartial<Pipeline>): Pipeline {
    return Pipeline.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Pipeline>): Pipeline {
    const message = createBasePipeline();
    message.projectId = object.projectId ?? "";
    message.name = object.name ?? "";
    message.description = object.description ?? "";
    message.inputParameters = object.inputParameters?.map((e) => PipelineParameter.fromPartial(e)) || [];
    message.outputParameters = object.outputParameters?.map((e) => PipelineParameter.fromPartial(e)) || [];
    message.docker = (object.docker !== undefined && object.docker !== null)
      ? DockerExecutor.fromPartial(object.docker)
      : undefined;
    message.resources = (object.resources !== undefined && object.resources !== null)
      ? PipelineResources.fromPartial(object.resources)
      : undefined;
    message.pipelineId = object.pipelineId ?? "";
    return message;
  },
};

function createBaseCreatePipelineRequest(): CreatePipelineRequest {
  return { pipeline: undefined };
}

export const CreatePipelineRequest: MessageFns<CreatePipelineRequest> = {
  encode(message: CreatePipelineRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.pipeline !== undefined) {
      Pipeline.encode(message.pipeline, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreatePipelineRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreatePipelineRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.pipeline = Pipeline.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreatePipelineRequest {
    return { pipeline: isSet(object.pipeline) ? Pipeline.fromJSON(object.pipeline) : undefined };
  },

  toJSON(message: CreatePipelineRequest): unknown {
    const obj: any = {};
    if (message.pipeline !== undefined) {
      obj.pipeline = Pipeline.toJSON(message.pipeline);
    }
    return obj;
  },

  create(base?: DeepPartial<CreatePipelineRequest>): CreatePipelineRequest {
    return CreatePipelineRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreatePipelineRequest>): CreatePipelineRequest {
    const message = createBaseCreatePipelineRequest();
    message.pipeline = (object.pipeline !== undefined && object.pipeline !== null)
      ? Pipeline.fromPartial(object.pipeline)
      : undefined;
    return message;
  },
};

function createBaseRunPipelineArgs(): RunPipelineArgs {
  return {
    projectId: "",
    inputs: {},
    outputs: {},
    serviceAccount: undefined,
    clientId: "",
    resources: undefined,
    logging: undefined,
    keepVmAliveOnFailureDuration: undefined,
    labels: {},
  };
}

export const RunPipelineArgs: MessageFns<RunPipelineArgs> = {
  encode(message: RunPipelineArgs, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    Object.entries(message.inputs).forEach(([key, value]) => {
      RunPipelineArgs_InputsEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    Object.entries(message.outputs).forEach(([key, value]) => {
      RunPipelineArgs_OutputsEntry.encode({ key: key as any, value }, writer.uint32(26).fork()).join();
    });
    if (message.serviceAccount !== undefined) {
      ServiceAccount.encode(message.serviceAccount, writer.uint32(34).fork()).join();
    }
    if (message.clientId !== "") {
      writer.uint32(42).string(message.clientId);
    }
    if (message.resources !== undefined) {
      PipelineResources.encode(message.resources, writer.uint32(50).fork()).join();
    }
    if (message.logging !== undefined) {
      LoggingOptions.encode(message.logging, writer.uint32(58).fork()).join();
    }
    if (message.keepVmAliveOnFailureDuration !== undefined) {
      Duration.encode(message.keepVmAliveOnFailureDuration, writer.uint32(66).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      RunPipelineArgs_LabelsEntry.encode({ key: key as any, value }, writer.uint32(74).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RunPipelineArgs {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRunPipelineArgs();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = RunPipelineArgs_InputsEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.inputs[entry2.key] = entry2.value;
          }
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          const entry3 = RunPipelineArgs_OutputsEntry.decode(reader, reader.uint32());
          if (entry3.value !== undefined) {
            message.outputs[entry3.key] = entry3.value;
          }
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.serviceAccount = ServiceAccount.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.clientId = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.resources = PipelineResources.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.logging = LoggingOptions.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.keepVmAliveOnFailureDuration = Duration.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          const entry9 = RunPipelineArgs_LabelsEntry.decode(reader, reader.uint32());
          if (entry9.value !== undefined) {
            message.labels[entry9.key] = entry9.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RunPipelineArgs {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      inputs: isObject(object.inputs)
        ? Object.entries(object.inputs).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      outputs: isObject(object.outputs)
        ? Object.entries(object.outputs).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      serviceAccount: isSet(object.serviceAccount) ? ServiceAccount.fromJSON(object.serviceAccount) : undefined,
      clientId: isSet(object.clientId) ? globalThis.String(object.clientId) : "",
      resources: isSet(object.resources) ? PipelineResources.fromJSON(object.resources) : undefined,
      logging: isSet(object.logging) ? LoggingOptions.fromJSON(object.logging) : undefined,
      keepVmAliveOnFailureDuration: isSet(object.keepVmAliveOnFailureDuration)
        ? Duration.fromJSON(object.keepVmAliveOnFailureDuration)
        : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: RunPipelineArgs): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.inputs) {
      const entries = Object.entries(message.inputs);
      if (entries.length > 0) {
        obj.inputs = {};
        entries.forEach(([k, v]) => {
          obj.inputs[k] = v;
        });
      }
    }
    if (message.outputs) {
      const entries = Object.entries(message.outputs);
      if (entries.length > 0) {
        obj.outputs = {};
        entries.forEach(([k, v]) => {
          obj.outputs[k] = v;
        });
      }
    }
    if (message.serviceAccount !== undefined) {
      obj.serviceAccount = ServiceAccount.toJSON(message.serviceAccount);
    }
    if (message.clientId !== "") {
      obj.clientId = message.clientId;
    }
    if (message.resources !== undefined) {
      obj.resources = PipelineResources.toJSON(message.resources);
    }
    if (message.logging !== undefined) {
      obj.logging = LoggingOptions.toJSON(message.logging);
    }
    if (message.keepVmAliveOnFailureDuration !== undefined) {
      obj.keepVmAliveOnFailureDuration = Duration.toJSON(message.keepVmAliveOnFailureDuration);
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<RunPipelineArgs>): RunPipelineArgs {
    return RunPipelineArgs.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RunPipelineArgs>): RunPipelineArgs {
    const message = createBaseRunPipelineArgs();
    message.projectId = object.projectId ?? "";
    message.inputs = Object.entries(object.inputs ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.outputs = Object.entries(object.outputs ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.serviceAccount = (object.serviceAccount !== undefined && object.serviceAccount !== null)
      ? ServiceAccount.fromPartial(object.serviceAccount)
      : undefined;
    message.clientId = object.clientId ?? "";
    message.resources = (object.resources !== undefined && object.resources !== null)
      ? PipelineResources.fromPartial(object.resources)
      : undefined;
    message.logging = (object.logging !== undefined && object.logging !== null)
      ? LoggingOptions.fromPartial(object.logging)
      : undefined;
    message.keepVmAliveOnFailureDuration =
      (object.keepVmAliveOnFailureDuration !== undefined && object.keepVmAliveOnFailureDuration !== null)
        ? Duration.fromPartial(object.keepVmAliveOnFailureDuration)
        : undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseRunPipelineArgs_InputsEntry(): RunPipelineArgs_InputsEntry {
  return { key: "", value: "" };
}

export const RunPipelineArgs_InputsEntry: MessageFns<RunPipelineArgs_InputsEntry> = {
  encode(message: RunPipelineArgs_InputsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RunPipelineArgs_InputsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRunPipelineArgs_InputsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RunPipelineArgs_InputsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: RunPipelineArgs_InputsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<RunPipelineArgs_InputsEntry>): RunPipelineArgs_InputsEntry {
    return RunPipelineArgs_InputsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RunPipelineArgs_InputsEntry>): RunPipelineArgs_InputsEntry {
    const message = createBaseRunPipelineArgs_InputsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseRunPipelineArgs_OutputsEntry(): RunPipelineArgs_OutputsEntry {
  return { key: "", value: "" };
}

export const RunPipelineArgs_OutputsEntry: MessageFns<RunPipelineArgs_OutputsEntry> = {
  encode(message: RunPipelineArgs_OutputsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RunPipelineArgs_OutputsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRunPipelineArgs_OutputsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RunPipelineArgs_OutputsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: RunPipelineArgs_OutputsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<RunPipelineArgs_OutputsEntry>): RunPipelineArgs_OutputsEntry {
    return RunPipelineArgs_OutputsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RunPipelineArgs_OutputsEntry>): RunPipelineArgs_OutputsEntry {
    const message = createBaseRunPipelineArgs_OutputsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseRunPipelineArgs_LabelsEntry(): RunPipelineArgs_LabelsEntry {
  return { key: "", value: "" };
}

export const RunPipelineArgs_LabelsEntry: MessageFns<RunPipelineArgs_LabelsEntry> = {
  encode(message: RunPipelineArgs_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RunPipelineArgs_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRunPipelineArgs_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RunPipelineArgs_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: RunPipelineArgs_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<RunPipelineArgs_LabelsEntry>): RunPipelineArgs_LabelsEntry {
    return RunPipelineArgs_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RunPipelineArgs_LabelsEntry>): RunPipelineArgs_LabelsEntry {
    const message = createBaseRunPipelineArgs_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseRunPipelineRequest(): RunPipelineRequest {
  return { pipelineId: undefined, ephemeralPipeline: undefined, pipelineArgs: undefined };
}

export const RunPipelineRequest: MessageFns<RunPipelineRequest> = {
  encode(message: RunPipelineRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.pipelineId !== undefined) {
      writer.uint32(10).string(message.pipelineId);
    }
    if (message.ephemeralPipeline !== undefined) {
      Pipeline.encode(message.ephemeralPipeline, writer.uint32(18).fork()).join();
    }
    if (message.pipelineArgs !== undefined) {
      RunPipelineArgs.encode(message.pipelineArgs, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RunPipelineRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRunPipelineRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.pipelineId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.ephemeralPipeline = Pipeline.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pipelineArgs = RunPipelineArgs.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RunPipelineRequest {
    return {
      pipelineId: isSet(object.pipelineId) ? globalThis.String(object.pipelineId) : undefined,
      ephemeralPipeline: isSet(object.ephemeralPipeline) ? Pipeline.fromJSON(object.ephemeralPipeline) : undefined,
      pipelineArgs: isSet(object.pipelineArgs) ? RunPipelineArgs.fromJSON(object.pipelineArgs) : undefined,
    };
  },

  toJSON(message: RunPipelineRequest): unknown {
    const obj: any = {};
    if (message.pipelineId !== undefined) {
      obj.pipelineId = message.pipelineId;
    }
    if (message.ephemeralPipeline !== undefined) {
      obj.ephemeralPipeline = Pipeline.toJSON(message.ephemeralPipeline);
    }
    if (message.pipelineArgs !== undefined) {
      obj.pipelineArgs = RunPipelineArgs.toJSON(message.pipelineArgs);
    }
    return obj;
  },

  create(base?: DeepPartial<RunPipelineRequest>): RunPipelineRequest {
    return RunPipelineRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RunPipelineRequest>): RunPipelineRequest {
    const message = createBaseRunPipelineRequest();
    message.pipelineId = object.pipelineId ?? undefined;
    message.ephemeralPipeline = (object.ephemeralPipeline !== undefined && object.ephemeralPipeline !== null)
      ? Pipeline.fromPartial(object.ephemeralPipeline)
      : undefined;
    message.pipelineArgs = (object.pipelineArgs !== undefined && object.pipelineArgs !== null)
      ? RunPipelineArgs.fromPartial(object.pipelineArgs)
      : undefined;
    return message;
  },
};

function createBaseGetPipelineRequest(): GetPipelineRequest {
  return { pipelineId: "" };
}

export const GetPipelineRequest: MessageFns<GetPipelineRequest> = {
  encode(message: GetPipelineRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.pipelineId !== "") {
      writer.uint32(10).string(message.pipelineId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetPipelineRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetPipelineRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.pipelineId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetPipelineRequest {
    return { pipelineId: isSet(object.pipelineId) ? globalThis.String(object.pipelineId) : "" };
  },

  toJSON(message: GetPipelineRequest): unknown {
    const obj: any = {};
    if (message.pipelineId !== "") {
      obj.pipelineId = message.pipelineId;
    }
    return obj;
  },

  create(base?: DeepPartial<GetPipelineRequest>): GetPipelineRequest {
    return GetPipelineRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetPipelineRequest>): GetPipelineRequest {
    const message = createBaseGetPipelineRequest();
    message.pipelineId = object.pipelineId ?? "";
    return message;
  },
};

function createBaseListPipelinesRequest(): ListPipelinesRequest {
  return { projectId: "", namePrefix: "", pageSize: 0, pageToken: "" };
}

export const ListPipelinesRequest: MessageFns<ListPipelinesRequest> = {
  encode(message: ListPipelinesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.namePrefix !== "") {
      writer.uint32(18).string(message.namePrefix);
    }
    if (message.pageSize !== 0) {
      writer.uint32(24).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(34).string(message.pageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListPipelinesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListPipelinesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.namePrefix = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.pageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListPipelinesRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      namePrefix: isSet(object.namePrefix) ? globalThis.String(object.namePrefix) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
    };
  },

  toJSON(message: ListPipelinesRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.namePrefix !== "") {
      obj.namePrefix = message.namePrefix;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListPipelinesRequest>): ListPipelinesRequest {
    return ListPipelinesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListPipelinesRequest>): ListPipelinesRequest {
    const message = createBaseListPipelinesRequest();
    message.projectId = object.projectId ?? "";
    message.namePrefix = object.namePrefix ?? "";
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    return message;
  },
};

function createBaseListPipelinesResponse(): ListPipelinesResponse {
  return { pipelines: [], nextPageToken: "" };
}

export const ListPipelinesResponse: MessageFns<ListPipelinesResponse> = {
  encode(message: ListPipelinesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.pipelines) {
      Pipeline.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListPipelinesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListPipelinesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.pipelines.push(Pipeline.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListPipelinesResponse {
    return {
      pipelines: globalThis.Array.isArray(object?.pipelines)
        ? object.pipelines.map((e: any) => Pipeline.fromJSON(e))
        : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: ListPipelinesResponse): unknown {
    const obj: any = {};
    if (message.pipelines?.length) {
      obj.pipelines = message.pipelines.map((e) => Pipeline.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListPipelinesResponse>): ListPipelinesResponse {
    return ListPipelinesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListPipelinesResponse>): ListPipelinesResponse {
    const message = createBaseListPipelinesResponse();
    message.pipelines = object.pipelines?.map((e) => Pipeline.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBaseDeletePipelineRequest(): DeletePipelineRequest {
  return { pipelineId: "" };
}

export const DeletePipelineRequest: MessageFns<DeletePipelineRequest> = {
  encode(message: DeletePipelineRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.pipelineId !== "") {
      writer.uint32(10).string(message.pipelineId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeletePipelineRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeletePipelineRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.pipelineId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeletePipelineRequest {
    return { pipelineId: isSet(object.pipelineId) ? globalThis.String(object.pipelineId) : "" };
  },

  toJSON(message: DeletePipelineRequest): unknown {
    const obj: any = {};
    if (message.pipelineId !== "") {
      obj.pipelineId = message.pipelineId;
    }
    return obj;
  },

  create(base?: DeepPartial<DeletePipelineRequest>): DeletePipelineRequest {
    return DeletePipelineRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeletePipelineRequest>): DeletePipelineRequest {
    const message = createBaseDeletePipelineRequest();
    message.pipelineId = object.pipelineId ?? "";
    return message;
  },
};

function createBaseGetControllerConfigRequest(): GetControllerConfigRequest {
  return { operationId: "", validationToken: Long.UZERO };
}

export const GetControllerConfigRequest: MessageFns<GetControllerConfigRequest> = {
  encode(message: GetControllerConfigRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.operationId !== "") {
      writer.uint32(10).string(message.operationId);
    }
    if (!message.validationToken.equals(Long.UZERO)) {
      writer.uint32(16).uint64(message.validationToken.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetControllerConfigRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetControllerConfigRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.operationId = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.validationToken = Long.fromString(reader.uint64().toString(), true);
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetControllerConfigRequest {
    return {
      operationId: isSet(object.operationId) ? globalThis.String(object.operationId) : "",
      validationToken: isSet(object.validationToken) ? Long.fromValue(object.validationToken) : Long.UZERO,
    };
  },

  toJSON(message: GetControllerConfigRequest): unknown {
    const obj: any = {};
    if (message.operationId !== "") {
      obj.operationId = message.operationId;
    }
    if (!message.validationToken.equals(Long.UZERO)) {
      obj.validationToken = (message.validationToken || Long.UZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<GetControllerConfigRequest>): GetControllerConfigRequest {
    return GetControllerConfigRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetControllerConfigRequest>): GetControllerConfigRequest {
    const message = createBaseGetControllerConfigRequest();
    message.operationId = object.operationId ?? "";
    message.validationToken = (object.validationToken !== undefined && object.validationToken !== null)
      ? Long.fromValue(object.validationToken)
      : Long.UZERO;
    return message;
  },
};

function createBaseControllerConfig(): ControllerConfig {
  return { image: "", cmd: "", gcsLogPath: "", machineType: "", vars: {}, disks: {}, gcsSources: {}, gcsSinks: {} };
}

export const ControllerConfig: MessageFns<ControllerConfig> = {
  encode(message: ControllerConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.image !== "") {
      writer.uint32(10).string(message.image);
    }
    if (message.cmd !== "") {
      writer.uint32(18).string(message.cmd);
    }
    if (message.gcsLogPath !== "") {
      writer.uint32(26).string(message.gcsLogPath);
    }
    if (message.machineType !== "") {
      writer.uint32(34).string(message.machineType);
    }
    Object.entries(message.vars).forEach(([key, value]) => {
      ControllerConfig_VarsEntry.encode({ key: key as any, value }, writer.uint32(42).fork()).join();
    });
    Object.entries(message.disks).forEach(([key, value]) => {
      ControllerConfig_DisksEntry.encode({ key: key as any, value }, writer.uint32(50).fork()).join();
    });
    Object.entries(message.gcsSources).forEach(([key, value]) => {
      ControllerConfig_GcsSourcesEntry.encode({ key: key as any, value }, writer.uint32(58).fork()).join();
    });
    Object.entries(message.gcsSinks).forEach(([key, value]) => {
      ControllerConfig_GcsSinksEntry.encode({ key: key as any, value }, writer.uint32(66).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ControllerConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseControllerConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.image = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.cmd = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.gcsLogPath = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.machineType = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          const entry5 = ControllerConfig_VarsEntry.decode(reader, reader.uint32());
          if (entry5.value !== undefined) {
            message.vars[entry5.key] = entry5.value;
          }
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          const entry6 = ControllerConfig_DisksEntry.decode(reader, reader.uint32());
          if (entry6.value !== undefined) {
            message.disks[entry6.key] = entry6.value;
          }
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          const entry7 = ControllerConfig_GcsSourcesEntry.decode(reader, reader.uint32());
          if (entry7.value !== undefined) {
            message.gcsSources[entry7.key] = entry7.value;
          }
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          const entry8 = ControllerConfig_GcsSinksEntry.decode(reader, reader.uint32());
          if (entry8.value !== undefined) {
            message.gcsSinks[entry8.key] = entry8.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ControllerConfig {
    return {
      image: isSet(object.image) ? globalThis.String(object.image) : "",
      cmd: isSet(object.cmd) ? globalThis.String(object.cmd) : "",
      gcsLogPath: isSet(object.gcsLogPath) ? globalThis.String(object.gcsLogPath) : "",
      machineType: isSet(object.machineType) ? globalThis.String(object.machineType) : "",
      vars: isObject(object.vars)
        ? Object.entries(object.vars).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      disks: isObject(object.disks)
        ? Object.entries(object.disks).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      gcsSources: isObject(object.gcsSources)
        ? Object.entries(object.gcsSources).reduce<{ [key: string]: ControllerConfig_RepeatedString }>(
          (acc, [key, value]) => {
            acc[key] = ControllerConfig_RepeatedString.fromJSON(value);
            return acc;
          },
          {},
        )
        : {},
      gcsSinks: isObject(object.gcsSinks)
        ? Object.entries(object.gcsSinks).reduce<{ [key: string]: ControllerConfig_RepeatedString }>(
          (acc, [key, value]) => {
            acc[key] = ControllerConfig_RepeatedString.fromJSON(value);
            return acc;
          },
          {},
        )
        : {},
    };
  },

  toJSON(message: ControllerConfig): unknown {
    const obj: any = {};
    if (message.image !== "") {
      obj.image = message.image;
    }
    if (message.cmd !== "") {
      obj.cmd = message.cmd;
    }
    if (message.gcsLogPath !== "") {
      obj.gcsLogPath = message.gcsLogPath;
    }
    if (message.machineType !== "") {
      obj.machineType = message.machineType;
    }
    if (message.vars) {
      const entries = Object.entries(message.vars);
      if (entries.length > 0) {
        obj.vars = {};
        entries.forEach(([k, v]) => {
          obj.vars[k] = v;
        });
      }
    }
    if (message.disks) {
      const entries = Object.entries(message.disks);
      if (entries.length > 0) {
        obj.disks = {};
        entries.forEach(([k, v]) => {
          obj.disks[k] = v;
        });
      }
    }
    if (message.gcsSources) {
      const entries = Object.entries(message.gcsSources);
      if (entries.length > 0) {
        obj.gcsSources = {};
        entries.forEach(([k, v]) => {
          obj.gcsSources[k] = ControllerConfig_RepeatedString.toJSON(v);
        });
      }
    }
    if (message.gcsSinks) {
      const entries = Object.entries(message.gcsSinks);
      if (entries.length > 0) {
        obj.gcsSinks = {};
        entries.forEach(([k, v]) => {
          obj.gcsSinks[k] = ControllerConfig_RepeatedString.toJSON(v);
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<ControllerConfig>): ControllerConfig {
    return ControllerConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ControllerConfig>): ControllerConfig {
    const message = createBaseControllerConfig();
    message.image = object.image ?? "";
    message.cmd = object.cmd ?? "";
    message.gcsLogPath = object.gcsLogPath ?? "";
    message.machineType = object.machineType ?? "";
    message.vars = Object.entries(object.vars ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.disks = Object.entries(object.disks ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.gcsSources = Object.entries(object.gcsSources ?? {}).reduce<
      { [key: string]: ControllerConfig_RepeatedString }
    >((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = ControllerConfig_RepeatedString.fromPartial(value);
      }
      return acc;
    }, {});
    message.gcsSinks = Object.entries(object.gcsSinks ?? {}).reduce<{ [key: string]: ControllerConfig_RepeatedString }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = ControllerConfig_RepeatedString.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseControllerConfig_RepeatedString(): ControllerConfig_RepeatedString {
  return { values: [] };
}

export const ControllerConfig_RepeatedString: MessageFns<ControllerConfig_RepeatedString> = {
  encode(message: ControllerConfig_RepeatedString, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.values) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ControllerConfig_RepeatedString {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseControllerConfig_RepeatedString();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.values.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ControllerConfig_RepeatedString {
    return {
      values: globalThis.Array.isArray(object?.values) ? object.values.map((e: any) => globalThis.String(e)) : [],
    };
  },

  toJSON(message: ControllerConfig_RepeatedString): unknown {
    const obj: any = {};
    if (message.values?.length) {
      obj.values = message.values;
    }
    return obj;
  },

  create(base?: DeepPartial<ControllerConfig_RepeatedString>): ControllerConfig_RepeatedString {
    return ControllerConfig_RepeatedString.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ControllerConfig_RepeatedString>): ControllerConfig_RepeatedString {
    const message = createBaseControllerConfig_RepeatedString();
    message.values = object.values?.map((e) => e) || [];
    return message;
  },
};

function createBaseControllerConfig_VarsEntry(): ControllerConfig_VarsEntry {
  return { key: "", value: "" };
}

export const ControllerConfig_VarsEntry: MessageFns<ControllerConfig_VarsEntry> = {
  encode(message: ControllerConfig_VarsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ControllerConfig_VarsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseControllerConfig_VarsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ControllerConfig_VarsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: ControllerConfig_VarsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<ControllerConfig_VarsEntry>): ControllerConfig_VarsEntry {
    return ControllerConfig_VarsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ControllerConfig_VarsEntry>): ControllerConfig_VarsEntry {
    const message = createBaseControllerConfig_VarsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseControllerConfig_DisksEntry(): ControllerConfig_DisksEntry {
  return { key: "", value: "" };
}

export const ControllerConfig_DisksEntry: MessageFns<ControllerConfig_DisksEntry> = {
  encode(message: ControllerConfig_DisksEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ControllerConfig_DisksEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseControllerConfig_DisksEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ControllerConfig_DisksEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: ControllerConfig_DisksEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<ControllerConfig_DisksEntry>): ControllerConfig_DisksEntry {
    return ControllerConfig_DisksEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ControllerConfig_DisksEntry>): ControllerConfig_DisksEntry {
    const message = createBaseControllerConfig_DisksEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseControllerConfig_GcsSourcesEntry(): ControllerConfig_GcsSourcesEntry {
  return { key: "", value: undefined };
}

export const ControllerConfig_GcsSourcesEntry: MessageFns<ControllerConfig_GcsSourcesEntry> = {
  encode(message: ControllerConfig_GcsSourcesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      ControllerConfig_RepeatedString.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ControllerConfig_GcsSourcesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseControllerConfig_GcsSourcesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = ControllerConfig_RepeatedString.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ControllerConfig_GcsSourcesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? ControllerConfig_RepeatedString.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: ControllerConfig_GcsSourcesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = ControllerConfig_RepeatedString.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<ControllerConfig_GcsSourcesEntry>): ControllerConfig_GcsSourcesEntry {
    return ControllerConfig_GcsSourcesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ControllerConfig_GcsSourcesEntry>): ControllerConfig_GcsSourcesEntry {
    const message = createBaseControllerConfig_GcsSourcesEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? ControllerConfig_RepeatedString.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseControllerConfig_GcsSinksEntry(): ControllerConfig_GcsSinksEntry {
  return { key: "", value: undefined };
}

export const ControllerConfig_GcsSinksEntry: MessageFns<ControllerConfig_GcsSinksEntry> = {
  encode(message: ControllerConfig_GcsSinksEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      ControllerConfig_RepeatedString.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ControllerConfig_GcsSinksEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseControllerConfig_GcsSinksEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = ControllerConfig_RepeatedString.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ControllerConfig_GcsSinksEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? ControllerConfig_RepeatedString.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: ControllerConfig_GcsSinksEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = ControllerConfig_RepeatedString.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<ControllerConfig_GcsSinksEntry>): ControllerConfig_GcsSinksEntry {
    return ControllerConfig_GcsSinksEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ControllerConfig_GcsSinksEntry>): ControllerConfig_GcsSinksEntry {
    const message = createBaseControllerConfig_GcsSinksEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? ControllerConfig_RepeatedString.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseTimestampEvent(): TimestampEvent {
  return { description: "", timestamp: undefined };
}

export const TimestampEvent: MessageFns<TimestampEvent> = {
  encode(message: TimestampEvent, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.description !== "") {
      writer.uint32(10).string(message.description);
    }
    if (message.timestamp !== undefined) {
      Timestamp.encode(toTimestamp(message.timestamp), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TimestampEvent {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTimestampEvent();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.description = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.timestamp = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TimestampEvent {
    return {
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      timestamp: isSet(object.timestamp) ? fromJsonTimestamp(object.timestamp) : undefined,
    };
  },

  toJSON(message: TimestampEvent): unknown {
    const obj: any = {};
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.timestamp !== undefined) {
      obj.timestamp = message.timestamp.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<TimestampEvent>): TimestampEvent {
    return TimestampEvent.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TimestampEvent>): TimestampEvent {
    const message = createBaseTimestampEvent();
    message.description = object.description ?? "";
    message.timestamp = object.timestamp ?? undefined;
    return message;
  },
};

function createBaseSetOperationStatusRequest(): SetOperationStatusRequest {
  return { operationId: "", timestampEvents: [], errorCode: 0, errorMessage: "", validationToken: Long.UZERO };
}

export const SetOperationStatusRequest: MessageFns<SetOperationStatusRequest> = {
  encode(message: SetOperationStatusRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.operationId !== "") {
      writer.uint32(10).string(message.operationId);
    }
    for (const v of message.timestampEvents) {
      TimestampEvent.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.errorCode !== 0) {
      writer.uint32(24).int32(message.errorCode);
    }
    if (message.errorMessage !== "") {
      writer.uint32(34).string(message.errorMessage);
    }
    if (!message.validationToken.equals(Long.UZERO)) {
      writer.uint32(40).uint64(message.validationToken.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SetOperationStatusRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSetOperationStatusRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.operationId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.timestampEvents.push(TimestampEvent.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.errorCode = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.errorMessage = reader.string();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.validationToken = Long.fromString(reader.uint64().toString(), true);
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SetOperationStatusRequest {
    return {
      operationId: isSet(object.operationId) ? globalThis.String(object.operationId) : "",
      timestampEvents: globalThis.Array.isArray(object?.timestampEvents)
        ? object.timestampEvents.map((e: any) => TimestampEvent.fromJSON(e))
        : [],
      errorCode: isSet(object.errorCode) ? codeFromJSON(object.errorCode) : 0,
      errorMessage: isSet(object.errorMessage) ? globalThis.String(object.errorMessage) : "",
      validationToken: isSet(object.validationToken) ? Long.fromValue(object.validationToken) : Long.UZERO,
    };
  },

  toJSON(message: SetOperationStatusRequest): unknown {
    const obj: any = {};
    if (message.operationId !== "") {
      obj.operationId = message.operationId;
    }
    if (message.timestampEvents?.length) {
      obj.timestampEvents = message.timestampEvents.map((e) => TimestampEvent.toJSON(e));
    }
    if (message.errorCode !== 0) {
      obj.errorCode = codeToJSON(message.errorCode);
    }
    if (message.errorMessage !== "") {
      obj.errorMessage = message.errorMessage;
    }
    if (!message.validationToken.equals(Long.UZERO)) {
      obj.validationToken = (message.validationToken || Long.UZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<SetOperationStatusRequest>): SetOperationStatusRequest {
    return SetOperationStatusRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SetOperationStatusRequest>): SetOperationStatusRequest {
    const message = createBaseSetOperationStatusRequest();
    message.operationId = object.operationId ?? "";
    message.timestampEvents = object.timestampEvents?.map((e) => TimestampEvent.fromPartial(e)) || [];
    message.errorCode = object.errorCode ?? 0;
    message.errorMessage = object.errorMessage ?? "";
    message.validationToken = (object.validationToken !== undefined && object.validationToken !== null)
      ? Long.fromValue(object.validationToken)
      : Long.UZERO;
    return message;
  },
};

function createBaseServiceAccount(): ServiceAccount {
  return { email: "", scopes: [] };
}

export const ServiceAccount: MessageFns<ServiceAccount> = {
  encode(message: ServiceAccount, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.email !== "") {
      writer.uint32(10).string(message.email);
    }
    for (const v of message.scopes) {
      writer.uint32(18).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ServiceAccount {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseServiceAccount();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.email = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.scopes.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ServiceAccount {
    return {
      email: isSet(object.email) ? globalThis.String(object.email) : "",
      scopes: globalThis.Array.isArray(object?.scopes) ? object.scopes.map((e: any) => globalThis.String(e)) : [],
    };
  },

  toJSON(message: ServiceAccount): unknown {
    const obj: any = {};
    if (message.email !== "") {
      obj.email = message.email;
    }
    if (message.scopes?.length) {
      obj.scopes = message.scopes;
    }
    return obj;
  },

  create(base?: DeepPartial<ServiceAccount>): ServiceAccount {
    return ServiceAccount.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ServiceAccount>): ServiceAccount {
    const message = createBaseServiceAccount();
    message.email = object.email ?? "";
    message.scopes = object.scopes?.map((e) => e) || [];
    return message;
  },
};

function createBaseLoggingOptions(): LoggingOptions {
  return { gcsPath: "" };
}

export const LoggingOptions: MessageFns<LoggingOptions> = {
  encode(message: LoggingOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcsPath !== "") {
      writer.uint32(10).string(message.gcsPath);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LoggingOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLoggingOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.gcsPath = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LoggingOptions {
    return { gcsPath: isSet(object.gcsPath) ? globalThis.String(object.gcsPath) : "" };
  },

  toJSON(message: LoggingOptions): unknown {
    const obj: any = {};
    if (message.gcsPath !== "") {
      obj.gcsPath = message.gcsPath;
    }
    return obj;
  },

  create(base?: DeepPartial<LoggingOptions>): LoggingOptions {
    return LoggingOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LoggingOptions>): LoggingOptions {
    const message = createBaseLoggingOptions();
    message.gcsPath = object.gcsPath ?? "";
    return message;
  },
};

function createBasePipelineResources(): PipelineResources {
  return {
    minimumCpuCores: 0,
    preemptible: false,
    minimumRamGb: 0,
    disks: [],
    zones: [],
    bootDiskSizeGb: 0,
    noAddress: false,
  };
}

export const PipelineResources: MessageFns<PipelineResources> = {
  encode(message: PipelineResources, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.minimumCpuCores !== 0) {
      writer.uint32(8).int32(message.minimumCpuCores);
    }
    if (message.preemptible !== false) {
      writer.uint32(16).bool(message.preemptible);
    }
    if (message.minimumRamGb !== 0) {
      writer.uint32(25).double(message.minimumRamGb);
    }
    for (const v of message.disks) {
      PipelineResources_Disk.encode(v!, writer.uint32(34).fork()).join();
    }
    for (const v of message.zones) {
      writer.uint32(42).string(v!);
    }
    if (message.bootDiskSizeGb !== 0) {
      writer.uint32(48).int32(message.bootDiskSizeGb);
    }
    if (message.noAddress !== false) {
      writer.uint32(56).bool(message.noAddress);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineResources {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineResources();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.minimumCpuCores = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.preemptible = reader.bool();
          continue;
        case 3:
          if (tag !== 25) {
            break;
          }

          message.minimumRamGb = reader.double();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.disks.push(PipelineResources_Disk.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.zones.push(reader.string());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.bootDiskSizeGb = reader.int32();
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.noAddress = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineResources {
    return {
      minimumCpuCores: isSet(object.minimumCpuCores) ? globalThis.Number(object.minimumCpuCores) : 0,
      preemptible: isSet(object.preemptible) ? globalThis.Boolean(object.preemptible) : false,
      minimumRamGb: isSet(object.minimumRamGb) ? globalThis.Number(object.minimumRamGb) : 0,
      disks: globalThis.Array.isArray(object?.disks)
        ? object.disks.map((e: any) => PipelineResources_Disk.fromJSON(e))
        : [],
      zones: globalThis.Array.isArray(object?.zones) ? object.zones.map((e: any) => globalThis.String(e)) : [],
      bootDiskSizeGb: isSet(object.bootDiskSizeGb) ? globalThis.Number(object.bootDiskSizeGb) : 0,
      noAddress: isSet(object.noAddress) ? globalThis.Boolean(object.noAddress) : false,
    };
  },

  toJSON(message: PipelineResources): unknown {
    const obj: any = {};
    if (message.minimumCpuCores !== 0) {
      obj.minimumCpuCores = Math.round(message.minimumCpuCores);
    }
    if (message.preemptible !== false) {
      obj.preemptible = message.preemptible;
    }
    if (message.minimumRamGb !== 0) {
      obj.minimumRamGb = message.minimumRamGb;
    }
    if (message.disks?.length) {
      obj.disks = message.disks.map((e) => PipelineResources_Disk.toJSON(e));
    }
    if (message.zones?.length) {
      obj.zones = message.zones;
    }
    if (message.bootDiskSizeGb !== 0) {
      obj.bootDiskSizeGb = Math.round(message.bootDiskSizeGb);
    }
    if (message.noAddress !== false) {
      obj.noAddress = message.noAddress;
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineResources>): PipelineResources {
    return PipelineResources.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineResources>): PipelineResources {
    const message = createBasePipelineResources();
    message.minimumCpuCores = object.minimumCpuCores ?? 0;
    message.preemptible = object.preemptible ?? false;
    message.minimumRamGb = object.minimumRamGb ?? 0;
    message.disks = object.disks?.map((e) => PipelineResources_Disk.fromPartial(e)) || [];
    message.zones = object.zones?.map((e) => e) || [];
    message.bootDiskSizeGb = object.bootDiskSizeGb ?? 0;
    message.noAddress = object.noAddress ?? false;
    return message;
  },
};

function createBasePipelineResources_Disk(): PipelineResources_Disk {
  return { name: "", type: 0, sizeGb: 0, source: "", autoDelete: false, mountPoint: "" };
}

export const PipelineResources_Disk: MessageFns<PipelineResources_Disk> = {
  encode(message: PipelineResources_Disk, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.type !== 0) {
      writer.uint32(16).int32(message.type);
    }
    if (message.sizeGb !== 0) {
      writer.uint32(24).int32(message.sizeGb);
    }
    if (message.source !== "") {
      writer.uint32(34).string(message.source);
    }
    if (message.autoDelete !== false) {
      writer.uint32(48).bool(message.autoDelete);
    }
    if (message.mountPoint !== "") {
      writer.uint32(66).string(message.mountPoint);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineResources_Disk {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineResources_Disk();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.sizeGb = reader.int32();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.source = reader.string();
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.autoDelete = reader.bool();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.mountPoint = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineResources_Disk {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      type: isSet(object.type) ? pipelineResources_Disk_TypeFromJSON(object.type) : 0,
      sizeGb: isSet(object.sizeGb) ? globalThis.Number(object.sizeGb) : 0,
      source: isSet(object.source) ? globalThis.String(object.source) : "",
      autoDelete: isSet(object.autoDelete) ? globalThis.Boolean(object.autoDelete) : false,
      mountPoint: isSet(object.mountPoint) ? globalThis.String(object.mountPoint) : "",
    };
  },

  toJSON(message: PipelineResources_Disk): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.type !== 0) {
      obj.type = pipelineResources_Disk_TypeToJSON(message.type);
    }
    if (message.sizeGb !== 0) {
      obj.sizeGb = Math.round(message.sizeGb);
    }
    if (message.source !== "") {
      obj.source = message.source;
    }
    if (message.autoDelete !== false) {
      obj.autoDelete = message.autoDelete;
    }
    if (message.mountPoint !== "") {
      obj.mountPoint = message.mountPoint;
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineResources_Disk>): PipelineResources_Disk {
    return PipelineResources_Disk.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineResources_Disk>): PipelineResources_Disk {
    const message = createBasePipelineResources_Disk();
    message.name = object.name ?? "";
    message.type = object.type ?? 0;
    message.sizeGb = object.sizeGb ?? 0;
    message.source = object.source ?? "";
    message.autoDelete = object.autoDelete ?? false;
    message.mountPoint = object.mountPoint ?? "";
    return message;
  },
};

function createBasePipelineParameter(): PipelineParameter {
  return { name: "", description: "", defaultValue: "", localCopy: undefined };
}

export const PipelineParameter: MessageFns<PipelineParameter> = {
  encode(message: PipelineParameter, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.description !== "") {
      writer.uint32(18).string(message.description);
    }
    if (message.defaultValue !== "") {
      writer.uint32(42).string(message.defaultValue);
    }
    if (message.localCopy !== undefined) {
      PipelineParameter_LocalCopy.encode(message.localCopy, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineParameter {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineParameter();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.description = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.defaultValue = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.localCopy = PipelineParameter_LocalCopy.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineParameter {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      defaultValue: isSet(object.defaultValue) ? globalThis.String(object.defaultValue) : "",
      localCopy: isSet(object.localCopy) ? PipelineParameter_LocalCopy.fromJSON(object.localCopy) : undefined,
    };
  },

  toJSON(message: PipelineParameter): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.defaultValue !== "") {
      obj.defaultValue = message.defaultValue;
    }
    if (message.localCopy !== undefined) {
      obj.localCopy = PipelineParameter_LocalCopy.toJSON(message.localCopy);
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineParameter>): PipelineParameter {
    return PipelineParameter.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineParameter>): PipelineParameter {
    const message = createBasePipelineParameter();
    message.name = object.name ?? "";
    message.description = object.description ?? "";
    message.defaultValue = object.defaultValue ?? "";
    message.localCopy = (object.localCopy !== undefined && object.localCopy !== null)
      ? PipelineParameter_LocalCopy.fromPartial(object.localCopy)
      : undefined;
    return message;
  },
};

function createBasePipelineParameter_LocalCopy(): PipelineParameter_LocalCopy {
  return { path: "", disk: "" };
}

export const PipelineParameter_LocalCopy: MessageFns<PipelineParameter_LocalCopy> = {
  encode(message: PipelineParameter_LocalCopy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.path !== "") {
      writer.uint32(10).string(message.path);
    }
    if (message.disk !== "") {
      writer.uint32(18).string(message.disk);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineParameter_LocalCopy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineParameter_LocalCopy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.path = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.disk = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineParameter_LocalCopy {
    return {
      path: isSet(object.path) ? globalThis.String(object.path) : "",
      disk: isSet(object.disk) ? globalThis.String(object.disk) : "",
    };
  },

  toJSON(message: PipelineParameter_LocalCopy): unknown {
    const obj: any = {};
    if (message.path !== "") {
      obj.path = message.path;
    }
    if (message.disk !== "") {
      obj.disk = message.disk;
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineParameter_LocalCopy>): PipelineParameter_LocalCopy {
    return PipelineParameter_LocalCopy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineParameter_LocalCopy>): PipelineParameter_LocalCopy {
    const message = createBasePipelineParameter_LocalCopy();
    message.path = object.path ?? "";
    message.disk = object.disk ?? "";
    return message;
  },
};

function createBaseDockerExecutor(): DockerExecutor {
  return { imageName: "", cmd: "" };
}

export const DockerExecutor: MessageFns<DockerExecutor> = {
  encode(message: DockerExecutor, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.imageName !== "") {
      writer.uint32(10).string(message.imageName);
    }
    if (message.cmd !== "") {
      writer.uint32(18).string(message.cmd);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DockerExecutor {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDockerExecutor();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.imageName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.cmd = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DockerExecutor {
    return {
      imageName: isSet(object.imageName) ? globalThis.String(object.imageName) : "",
      cmd: isSet(object.cmd) ? globalThis.String(object.cmd) : "",
    };
  },

  toJSON(message: DockerExecutor): unknown {
    const obj: any = {};
    if (message.imageName !== "") {
      obj.imageName = message.imageName;
    }
    if (message.cmd !== "") {
      obj.cmd = message.cmd;
    }
    return obj;
  },

  create(base?: DeepPartial<DockerExecutor>): DockerExecutor {
    return DockerExecutor.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DockerExecutor>): DockerExecutor {
    const message = createBaseDockerExecutor();
    message.imageName = object.imageName ?? "";
    message.cmd = object.cmd ?? "";
    return message;
  },
};

/** A service for running genomics pipelines. */
export type PipelinesV1Alpha2Definition = typeof PipelinesV1Alpha2Definition;
export const PipelinesV1Alpha2Definition = {
  name: "PipelinesV1Alpha2",
  fullName: "google.genomics.v1alpha2.PipelinesV1Alpha2",
  methods: {
    /**
     * Creates a pipeline that can be run later. Create takes a Pipeline that
     * has all fields other than `pipelineId` populated, and then returns
     * the same pipeline with `pipelineId` populated. This id can be used
     * to run the pipeline.
     *
     * Caller must have WRITE permission to the project.
     */
    createPipeline: {
      name: "CreatePipeline",
      requestType: CreatePipelineRequest,
      requestStream: false,
      responseType: Pipeline,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              31,
              58,
              8,
              112,
              105,
              112,
              101,
              108,
              105,
              110,
              101,
              34,
              19,
              47,
              118,
              49,
              97,
              108,
              112,
              104,
              97,
              50,
              47,
              112,
              105,
              112,
              101,
              108,
              105,
              110,
              101,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Runs a pipeline. If `pipelineId` is specified in the request, then
     * run a saved pipeline. If `ephemeralPipeline` is specified, then run
     * that pipeline once without saving a copy.
     *
     * The caller must have READ permission to the project where the pipeline
     * is stored and WRITE permission to the project where the pipeline will be
     * run, as VMs will be created and storage will be used.
     */
    runPipeline: {
      name: "RunPipeline",
      requestType: RunPipelineRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              28,
              58,
              1,
              42,
              34,
              23,
              47,
              118,
              49,
              97,
              108,
              112,
              104,
              97,
              50,
              47,
              112,
              105,
              112,
              101,
              108,
              105,
              110,
              101,
              115,
              58,
              114,
              117,
              110,
            ]),
          ],
        },
      },
    },
    /**
     * Retrieves a pipeline based on ID.
     *
     * Caller must have READ permission to the project.
     */
    getPipeline: {
      name: "GetPipeline",
      requestType: GetPipelineRequest,
      requestStream: false,
      responseType: Pipeline,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              35,
              18,
              33,
              47,
              118,
              49,
              97,
              108,
              112,
              104,
              97,
              50,
              47,
              112,
              105,
              112,
              101,
              108,
              105,
              110,
              101,
              115,
              47,
              123,
              112,
              105,
              112,
              101,
              108,
              105,
              110,
              101,
              95,
              105,
              100,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Lists pipelines.
     *
     * Caller must have READ permission to the project.
     */
    listPipelines: {
      name: "ListPipelines",
      requestType: ListPipelinesRequest,
      requestStream: false,
      responseType: ListPipelinesResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              21,
              18,
              19,
              47,
              118,
              49,
              97,
              108,
              112,
              104,
              97,
              50,
              47,
              112,
              105,
              112,
              101,
              108,
              105,
              110,
              101,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Deletes a pipeline based on ID.
     *
     * Caller must have WRITE permission to the project.
     */
    deletePipeline: {
      name: "DeletePipeline",
      requestType: DeletePipelineRequest,
      requestStream: false,
      responseType: Empty,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              35,
              42,
              33,
              47,
              118,
              49,
              97,
              108,
              112,
              104,
              97,
              50,
              47,
              112,
              105,
              112,
              101,
              108,
              105,
              110,
              101,
              115,
              47,
              123,
              112,
              105,
              112,
              101,
              108,
              105,
              110,
              101,
              95,
              105,
              100,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Gets controller configuration information. Should only be called
     * by VMs created by the Pipelines Service and not by end users.
     */
    getControllerConfig: {
      name: "GetControllerConfig",
      requestType: GetControllerConfigRequest,
      requestStream: false,
      responseType: ControllerConfig,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              41,
              18,
              39,
              47,
              118,
              49,
              97,
              108,
              112,
              104,
              97,
              50,
              47,
              112,
              105,
              112,
              101,
              108,
              105,
              110,
              101,
              115,
              58,
              103,
              101,
              116,
              67,
              111,
              110,
              116,
              114,
              111,
              108,
              108,
              101,
              114,
              67,
              111,
              110,
              102,
              105,
              103,
            ]),
          ],
        },
      },
    },
    /**
     * Sets status of a given operation. Any new timestamps (as determined by
     * description) are appended to TimestampEvents. Should only be called by VMs
     * created by the Pipelines Service and not by end users.
     */
    setOperationStatus: {
      name: "SetOperationStatus",
      requestType: SetOperationStatusRequest,
      requestStream: false,
      responseType: Empty,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              43,
              58,
              1,
              42,
              26,
              38,
              47,
              118,
              49,
              97,
              108,
              112,
              104,
              97,
              50,
              47,
              112,
              105,
              112,
              101,
              108,
              105,
              110,
              101,
              115,
              58,
              115,
              101,
              116,
              79,
              112,
              101,
              114,
              97,
              116,
              105,
              111,
              110,
              83,
              116,
              97,
              116,
              117,
              115,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface PipelinesV1Alpha2ServiceImplementation<CallContextExt = {}> {
  /**
   * Creates a pipeline that can be run later. Create takes a Pipeline that
   * has all fields other than `pipelineId` populated, and then returns
   * the same pipeline with `pipelineId` populated. This id can be used
   * to run the pipeline.
   *
   * Caller must have WRITE permission to the project.
   */
  createPipeline(request: CreatePipelineRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Pipeline>>;
  /**
   * Runs a pipeline. If `pipelineId` is specified in the request, then
   * run a saved pipeline. If `ephemeralPipeline` is specified, then run
   * that pipeline once without saving a copy.
   *
   * The caller must have READ permission to the project where the pipeline
   * is stored and WRITE permission to the project where the pipeline will be
   * run, as VMs will be created and storage will be used.
   */
  runPipeline(request: RunPipelineRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
  /**
   * Retrieves a pipeline based on ID.
   *
   * Caller must have READ permission to the project.
   */
  getPipeline(request: GetPipelineRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Pipeline>>;
  /**
   * Lists pipelines.
   *
   * Caller must have READ permission to the project.
   */
  listPipelines(
    request: ListPipelinesRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ListPipelinesResponse>>;
  /**
   * Deletes a pipeline based on ID.
   *
   * Caller must have WRITE permission to the project.
   */
  deletePipeline(request: DeletePipelineRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Empty>>;
  /**
   * Gets controller configuration information. Should only be called
   * by VMs created by the Pipelines Service and not by end users.
   */
  getControllerConfig(
    request: GetControllerConfigRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ControllerConfig>>;
  /**
   * Sets status of a given operation. Any new timestamps (as determined by
   * description) are appended to TimestampEvents. Should only be called by VMs
   * created by the Pipelines Service and not by end users.
   */
  setOperationStatus(
    request: SetOperationStatusRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Empty>>;
}

export interface PipelinesV1Alpha2Client<CallOptionsExt = {}> {
  /**
   * Creates a pipeline that can be run later. Create takes a Pipeline that
   * has all fields other than `pipelineId` populated, and then returns
   * the same pipeline with `pipelineId` populated. This id can be used
   * to run the pipeline.
   *
   * Caller must have WRITE permission to the project.
   */
  createPipeline(
    request: DeepPartial<CreatePipelineRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Pipeline>;
  /**
   * Runs a pipeline. If `pipelineId` is specified in the request, then
   * run a saved pipeline. If `ephemeralPipeline` is specified, then run
   * that pipeline once without saving a copy.
   *
   * The caller must have READ permission to the project where the pipeline
   * is stored and WRITE permission to the project where the pipeline will be
   * run, as VMs will be created and storage will be used.
   */
  runPipeline(request: DeepPartial<RunPipelineRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
  /**
   * Retrieves a pipeline based on ID.
   *
   * Caller must have READ permission to the project.
   */
  getPipeline(request: DeepPartial<GetPipelineRequest>, options?: CallOptions & CallOptionsExt): Promise<Pipeline>;
  /**
   * Lists pipelines.
   *
   * Caller must have READ permission to the project.
   */
  listPipelines(
    request: DeepPartial<ListPipelinesRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ListPipelinesResponse>;
  /**
   * Deletes a pipeline based on ID.
   *
   * Caller must have WRITE permission to the project.
   */
  deletePipeline(request: DeepPartial<DeletePipelineRequest>, options?: CallOptions & CallOptionsExt): Promise<Empty>;
  /**
   * Gets controller configuration information. Should only be called
   * by VMs created by the Pipelines Service and not by end users.
   */
  getControllerConfig(
    request: DeepPartial<GetControllerConfigRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ControllerConfig>;
  /**
   * Sets status of a given operation. Any new timestamps (as determined by
   * description) are appended to TimestampEvents. Should only be called by VMs
   * created by the Pipelines Service and not by end users.
   */
  setOperationStatus(
    request: DeepPartial<SetOperationStatusRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Empty>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
