// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/datastore/v1/aggregation_result.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Timestamp } from "../../protobuf/timestamp.js";
import { Value } from "./entity.js";
import {
  QueryResultBatch_MoreResultsType,
  queryResultBatch_MoreResultsTypeFromJSON,
  queryResultBatch_MoreResultsTypeToJSON,
} from "./query.js";

export const protobufPackage = "google.datastore.v1";

/**
 * The result of a single bucket from a Datastore aggregation query.
 *
 * The keys of `aggregate_properties` are the same for all results in an
 * aggregation query, unlike entity queries which can have different fields
 * present for each result.
 */
export interface AggregationResult {
  /**
   * The result of the aggregation functions, ex: `COUNT(*) AS total_entities`.
   *
   * The key is the
   * [alias][google.datastore.v1.AggregationQuery.Aggregation.alias] assigned to
   * the aggregation function on input and the size of this map equals the
   * number of aggregation functions in the query.
   */
  aggregateProperties: { [key: string]: Value };
}

export interface AggregationResult_AggregatePropertiesEntry {
  key: string;
  value: Value | undefined;
}

/** A batch of aggregation results produced by an aggregation query. */
export interface AggregationResultBatch {
  /** The aggregation results for this batch. */
  aggregationResults: AggregationResult[];
  /**
   * The state of the query after the current batch.
   * Only COUNT(*) aggregations are supported in the initial launch. Therefore,
   * expected result type is limited to `NO_MORE_RESULTS`.
   */
  moreResults: QueryResultBatch_MoreResultsType;
  /**
   * Read timestamp this batch was returned from.
   *
   * In a single transaction, subsequent query result batches for the same query
   * can have a greater timestamp. Each batch's read timestamp
   * is valid for all preceding batches.
   */
  readTime: Date | undefined;
}

function createBaseAggregationResult(): AggregationResult {
  return { aggregateProperties: {} };
}

export const AggregationResult: MessageFns<AggregationResult> = {
  encode(message: AggregationResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.aggregateProperties).forEach(([key, value]) => {
      AggregationResult_AggregatePropertiesEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AggregationResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAggregationResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = AggregationResult_AggregatePropertiesEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.aggregateProperties[entry2.key] = entry2.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AggregationResult {
    return {
      aggregateProperties: isObject(object.aggregateProperties)
        ? Object.entries(object.aggregateProperties).reduce<{ [key: string]: Value }>((acc, [key, value]) => {
          acc[key] = Value.fromJSON(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: AggregationResult): unknown {
    const obj: any = {};
    if (message.aggregateProperties) {
      const entries = Object.entries(message.aggregateProperties);
      if (entries.length > 0) {
        obj.aggregateProperties = {};
        entries.forEach(([k, v]) => {
          obj.aggregateProperties[k] = Value.toJSON(v);
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<AggregationResult>): AggregationResult {
    return AggregationResult.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AggregationResult>): AggregationResult {
    const message = createBaseAggregationResult();
    message.aggregateProperties = Object.entries(object.aggregateProperties ?? {}).reduce<{ [key: string]: Value }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = Value.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseAggregationResult_AggregatePropertiesEntry(): AggregationResult_AggregatePropertiesEntry {
  return { key: "", value: undefined };
}

export const AggregationResult_AggregatePropertiesEntry: MessageFns<AggregationResult_AggregatePropertiesEntry> = {
  encode(message: AggregationResult_AggregatePropertiesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      Value.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AggregationResult_AggregatePropertiesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAggregationResult_AggregatePropertiesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = Value.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AggregationResult_AggregatePropertiesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? Value.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: AggregationResult_AggregatePropertiesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = Value.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<AggregationResult_AggregatePropertiesEntry>): AggregationResult_AggregatePropertiesEntry {
    return AggregationResult_AggregatePropertiesEntry.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<AggregationResult_AggregatePropertiesEntry>,
  ): AggregationResult_AggregatePropertiesEntry {
    const message = createBaseAggregationResult_AggregatePropertiesEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null) ? Value.fromPartial(object.value) : undefined;
    return message;
  },
};

function createBaseAggregationResultBatch(): AggregationResultBatch {
  return { aggregationResults: [], moreResults: 0, readTime: undefined };
}

export const AggregationResultBatch: MessageFns<AggregationResultBatch> = {
  encode(message: AggregationResultBatch, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.aggregationResults) {
      AggregationResult.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.moreResults !== 0) {
      writer.uint32(16).int32(message.moreResults);
    }
    if (message.readTime !== undefined) {
      Timestamp.encode(toTimestamp(message.readTime), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AggregationResultBatch {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAggregationResultBatch();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.aggregationResults.push(AggregationResult.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.moreResults = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.readTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AggregationResultBatch {
    return {
      aggregationResults: globalThis.Array.isArray(object?.aggregationResults)
        ? object.aggregationResults.map((e: any) => AggregationResult.fromJSON(e))
        : [],
      moreResults: isSet(object.moreResults) ? queryResultBatch_MoreResultsTypeFromJSON(object.moreResults) : 0,
      readTime: isSet(object.readTime) ? fromJsonTimestamp(object.readTime) : undefined,
    };
  },

  toJSON(message: AggregationResultBatch): unknown {
    const obj: any = {};
    if (message.aggregationResults?.length) {
      obj.aggregationResults = message.aggregationResults.map((e) => AggregationResult.toJSON(e));
    }
    if (message.moreResults !== 0) {
      obj.moreResults = queryResultBatch_MoreResultsTypeToJSON(message.moreResults);
    }
    if (message.readTime !== undefined) {
      obj.readTime = message.readTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<AggregationResultBatch>): AggregationResultBatch {
    return AggregationResultBatch.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AggregationResultBatch>): AggregationResultBatch {
    const message = createBaseAggregationResultBatch();
    message.aggregationResults = object.aggregationResults?.map((e) => AggregationResult.fromPartial(e)) || [];
    message.moreResults = object.moreResults ?? 0;
    message.readTime = object.readTime ?? undefined;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
