// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/spanner/admin/database/v1/backup.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Operation } from "../../../../longrunning/operations.js";
import { FieldMask } from "../../../../protobuf/field_mask.js";
import { Timestamp } from "../../../../protobuf/timestamp.js";
import {
  DatabaseDialect,
  databaseDialectFromJSON,
  databaseDialectToJSON,
  EncryptionInfo,
  OperationProgress,
} from "./common.js";

export const protobufPackage = "google.spanner.admin.database.v1";

/** A backup of a Cloud Spanner database. */
export interface Backup {
  /**
   * Required for the
   * [CreateBackup][google.spanner.admin.database.v1.DatabaseAdmin.CreateBackup]
   * operation. Name of the database from which this backup was created. This
   * needs to be in the same instance as the backup. Values are of the form
   * `projects/<project>/instances/<instance>/databases/<database>`.
   */
  database: string;
  /**
   * The backup will contain an externally consistent copy of the database at
   * the timestamp specified by `version_time`. If `version_time` is not
   * specified, the system will set `version_time` to the `create_time` of the
   * backup.
   */
  versionTime:
    | Date
    | undefined;
  /**
   * Required for the
   * [CreateBackup][google.spanner.admin.database.v1.DatabaseAdmin.CreateBackup]
   * operation. The expiration time of the backup, with microseconds
   * granularity that must be at least 6 hours and at most 366 days
   * from the time the CreateBackup request is processed. Once the `expire_time`
   * has passed, the backup is eligible to be automatically deleted by Cloud
   * Spanner to free the resources used by the backup.
   */
  expireTime:
    | Date
    | undefined;
  /**
   * Output only for the
   * [CreateBackup][google.spanner.admin.database.v1.DatabaseAdmin.CreateBackup]
   * operation. Required for the
   * [UpdateBackup][google.spanner.admin.database.v1.DatabaseAdmin.UpdateBackup]
   * operation.
   *
   * A globally unique identifier for the backup which cannot be
   * changed. Values are of the form
   * `projects/<project>/instances/<instance>/backups/[a-z][a-z0-9_\-]*[a-z0-9]`
   * The final segment of the name must be between 2 and 60 characters
   * in length.
   *
   * The backup is stored in the location(s) specified in the instance
   * configuration of the instance containing the backup, identified
   * by the prefix of the backup name of the form
   * `projects/<project>/instances/<instance>`.
   */
  name: string;
  /**
   * Output only. The time the
   * [CreateBackup][google.spanner.admin.database.v1.DatabaseAdmin.CreateBackup]
   * request is received. If the request does not specify `version_time`, the
   * `version_time` of the backup will be equivalent to the `create_time`.
   */
  createTime:
    | Date
    | undefined;
  /** Output only. Size of the backup in bytes. */
  sizeBytes: Long;
  /**
   * Output only. The number of bytes that will be freed by deleting this
   * backup. This value will be zero if, for example, this backup is part of an
   * incremental backup chain and younger backups in the chain require that we
   * keep its data. For backups not in an incremental backup chain, this is
   * always the size of the backup. This value may change if backups on the same
   * chain get created, deleted or expired.
   */
  freeableSizeBytes: Long;
  /**
   * Output only. For a backup in an incremental backup chain, this is the
   * storage space needed to keep the data that has changed since the previous
   * backup. For all other backups, this is always the size of the backup. This
   * value may change if backups on the same chain get deleted or expired.
   *
   * This field can be used to calculate the total storage space used by a set
   * of backups. For example, the total space used by all backups of a database
   * can be computed by summing up this field.
   */
  exclusiveSizeBytes: Long;
  /** Output only. The current state of the backup. */
  state: Backup_State;
  /**
   * Output only. The names of the restored databases that reference the backup.
   * The database names are of
   * the form `projects/<project>/instances/<instance>/databases/<database>`.
   * Referencing databases may exist in different instances. The existence of
   * any referencing database prevents the backup from being deleted. When a
   * restored database from the backup enters the `READY` state, the reference
   * to the backup is removed.
   */
  referencingDatabases: string[];
  /** Output only. The encryption information for the backup. */
  encryptionInfo:
    | EncryptionInfo
    | undefined;
  /**
   * Output only. The encryption information for the backup, whether it is
   * protected by one or more KMS keys. The information includes all Cloud
   * KMS key versions used to encrypt the backup. The `encryption_status' field
   * inside of each `EncryptionInfo` is not populated. At least one of the key
   * versions must be available for the backup to be restored. If a key version
   * is revoked in the middle of a restore, the restore behavior is undefined.
   */
  encryptionInformation: EncryptionInfo[];
  /** Output only. The database dialect information for the backup. */
  databaseDialect: DatabaseDialect;
  /**
   * Output only. The names of the destination backups being created by copying
   * this source backup. The backup names are of the form
   * `projects/<project>/instances/<instance>/backups/<backup>`.
   * Referencing backups may exist in different instances. The existence of
   * any referencing backup prevents the backup from being deleted. When the
   * copy operation is done (either successfully completed or cancelled or the
   * destination backup is deleted), the reference to the backup is removed.
   */
  referencingBackups: string[];
  /**
   * Output only. The max allowed expiration time of the backup, with
   * microseconds granularity. A backup's expiration time can be configured in
   * multiple APIs: CreateBackup, UpdateBackup, CopyBackup. When updating or
   * copying an existing backup, the expiration time specified must be
   * less than `Backup.max_expire_time`.
   */
  maxExpireTime:
    | Date
    | undefined;
  /**
   * Output only. List of backup schedule URIs that are associated with
   * creating this backup. This is only applicable for scheduled backups, and
   * is empty for on-demand backups.
   *
   * To optimize for storage, whenever possible, multiple schedules are
   * collapsed together to create one backup. In such cases, this field captures
   * the list of all backup schedule URIs that are associated with creating
   * this backup. If collapsing is not done, then this field captures the
   * single backup schedule URI associated with creating this backup.
   */
  backupSchedules: string[];
  /**
   * Output only. Populated only for backups in an incremental backup chain.
   * Backups share the same chain id if and only if they belong to the same
   * incremental backup chain. Use this field to determine which backups are
   * part of the same incremental backup chain. The ordering of backups in the
   * chain can be determined by ordering the backup `version_time`.
   */
  incrementalBackupChainId: string;
  /**
   * Output only. Data deleted at a time older than this is guaranteed not to be
   * retained in order to support this backup. For a backup in an incremental
   * backup chain, this is the version time of the oldest backup that exists or
   * ever existed in the chain. For all other backups, this is the version time
   * of the backup. This field can be used to understand what data is being
   * retained by the backup system.
   */
  oldestVersionTime: Date | undefined;
}

/** Indicates the current state of the backup. */
export enum Backup_State {
  /** STATE_UNSPECIFIED - Not specified. */
  STATE_UNSPECIFIED = 0,
  /**
   * CREATING - The pending backup is still being created. Operations on the
   * backup may fail with `FAILED_PRECONDITION` in this state.
   */
  CREATING = 1,
  /** READY - The backup is complete and ready for use. */
  READY = 2,
  UNRECOGNIZED = -1,
}

export function backup_StateFromJSON(object: any): Backup_State {
  switch (object) {
    case 0:
    case "STATE_UNSPECIFIED":
      return Backup_State.STATE_UNSPECIFIED;
    case 1:
    case "CREATING":
      return Backup_State.CREATING;
    case 2:
    case "READY":
      return Backup_State.READY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Backup_State.UNRECOGNIZED;
  }
}

export function backup_StateToJSON(object: Backup_State): string {
  switch (object) {
    case Backup_State.STATE_UNSPECIFIED:
      return "STATE_UNSPECIFIED";
    case Backup_State.CREATING:
      return "CREATING";
    case Backup_State.READY:
      return "READY";
    case Backup_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * The request for
 * [CreateBackup][google.spanner.admin.database.v1.DatabaseAdmin.CreateBackup].
 */
export interface CreateBackupRequest {
  /**
   * Required. The name of the instance in which the backup will be
   * created. This must be the same instance that contains the database the
   * backup will be created from. The backup will be stored in the
   * location(s) specified in the instance configuration of this
   * instance. Values are of the form
   * `projects/<project>/instances/<instance>`.
   */
  parent: string;
  /**
   * Required. The id of the backup to be created. The `backup_id` appended to
   * `parent` forms the full backup name of the form
   * `projects/<project>/instances/<instance>/backups/<backup_id>`.
   */
  backupId: string;
  /** Required. The backup to create. */
  backup:
    | Backup
    | undefined;
  /**
   * Optional. The encryption configuration used to encrypt the backup. If this
   * field is not specified, the backup will use the same encryption
   * configuration as the database by default, namely
   * [encryption_type][google.spanner.admin.database.v1.CreateBackupEncryptionConfig.encryption_type]
   * = `USE_DATABASE_ENCRYPTION`.
   */
  encryptionConfig: CreateBackupEncryptionConfig | undefined;
}

/**
 * Metadata type for the operation returned by
 * [CreateBackup][google.spanner.admin.database.v1.DatabaseAdmin.CreateBackup].
 */
export interface CreateBackupMetadata {
  /** The name of the backup being created. */
  name: string;
  /** The name of the database the backup is created from. */
  database: string;
  /**
   * The progress of the
   * [CreateBackup][google.spanner.admin.database.v1.DatabaseAdmin.CreateBackup]
   * operation.
   */
  progress:
    | OperationProgress
    | undefined;
  /**
   * The time at which cancellation of this operation was received.
   * [Operations.CancelOperation][google.longrunning.Operations.CancelOperation]
   * starts asynchronous cancellation on a long-running operation. The server
   * makes a best effort to cancel the operation, but success is not guaranteed.
   * Clients can use
   * [Operations.GetOperation][google.longrunning.Operations.GetOperation] or
   * other methods to check whether the cancellation succeeded or whether the
   * operation completed despite cancellation. On successful cancellation,
   * the operation is not deleted; instead, it becomes an operation with
   * an [Operation.error][google.longrunning.Operation.error] value with a
   * [google.rpc.Status.code][google.rpc.Status.code] of 1,
   * corresponding to `Code.CANCELLED`.
   */
  cancelTime: Date | undefined;
}

/**
 * The request for
 * [CopyBackup][google.spanner.admin.database.v1.DatabaseAdmin.CopyBackup].
 */
export interface CopyBackupRequest {
  /**
   * Required. The name of the destination instance that will contain the backup
   * copy. Values are of the form: `projects/<project>/instances/<instance>`.
   */
  parent: string;
  /**
   * Required. The id of the backup copy.
   * The `backup_id` appended to `parent` forms the full backup_uri of the form
   * `projects/<project>/instances/<instance>/backups/<backup>`.
   */
  backupId: string;
  /**
   * Required. The source backup to be copied.
   * The source backup needs to be in READY state for it to be copied.
   * Once CopyBackup is in progress, the source backup cannot be deleted or
   * cleaned up on expiration until CopyBackup is finished.
   * Values are of the form:
   * `projects/<project>/instances/<instance>/backups/<backup>`.
   */
  sourceBackup: string;
  /**
   * Required. The expiration time of the backup in microsecond granularity.
   * The expiration time must be at least 6 hours and at most 366 days
   * from the `create_time` of the source backup. Once the `expire_time` has
   * passed, the backup is eligible to be automatically deleted by Cloud Spanner
   * to free the resources used by the backup.
   */
  expireTime:
    | Date
    | undefined;
  /**
   * Optional. The encryption configuration used to encrypt the backup. If this
   * field is not specified, the backup will use the same encryption
   * configuration as the source backup by default, namely
   * [encryption_type][google.spanner.admin.database.v1.CopyBackupEncryptionConfig.encryption_type]
   * = `USE_CONFIG_DEFAULT_OR_BACKUP_ENCRYPTION`.
   */
  encryptionConfig: CopyBackupEncryptionConfig | undefined;
}

/**
 * Metadata type for the operation returned by
 * [CopyBackup][google.spanner.admin.database.v1.DatabaseAdmin.CopyBackup].
 */
export interface CopyBackupMetadata {
  /**
   * The name of the backup being created through the copy operation.
   * Values are of the form
   * `projects/<project>/instances/<instance>/backups/<backup>`.
   */
  name: string;
  /**
   * The name of the source backup that is being copied.
   * Values are of the form
   * `projects/<project>/instances/<instance>/backups/<backup>`.
   */
  sourceBackup: string;
  /**
   * The progress of the
   * [CopyBackup][google.spanner.admin.database.v1.DatabaseAdmin.CopyBackup]
   * operation.
   */
  progress:
    | OperationProgress
    | undefined;
  /**
   * The time at which cancellation of CopyBackup operation was received.
   * [Operations.CancelOperation][google.longrunning.Operations.CancelOperation]
   * starts asynchronous cancellation on a long-running operation. The server
   * makes a best effort to cancel the operation, but success is not guaranteed.
   * Clients can use
   * [Operations.GetOperation][google.longrunning.Operations.GetOperation] or
   * other methods to check whether the cancellation succeeded or whether the
   * operation completed despite cancellation. On successful cancellation,
   * the operation is not deleted; instead, it becomes an operation with
   * an [Operation.error][google.longrunning.Operation.error] value with a
   * [google.rpc.Status.code][google.rpc.Status.code] of 1,
   * corresponding to `Code.CANCELLED`.
   */
  cancelTime: Date | undefined;
}

/**
 * The request for
 * [UpdateBackup][google.spanner.admin.database.v1.DatabaseAdmin.UpdateBackup].
 */
export interface UpdateBackupRequest {
  /**
   * Required. The backup to update. `backup.name`, and the fields to be updated
   * as specified by `update_mask` are required. Other fields are ignored.
   * Update is only supported for the following fields:
   *  * `backup.expire_time`.
   */
  backup:
    | Backup
    | undefined;
  /**
   * Required. A mask specifying which fields (e.g. `expire_time`) in the
   * Backup resource should be updated. This mask is relative to the Backup
   * resource, not to the request message. The field mask must always be
   * specified; this prevents any future fields from being erased accidentally
   * by clients that do not know about them.
   */
  updateMask: string[] | undefined;
}

/**
 * The request for
 * [GetBackup][google.spanner.admin.database.v1.DatabaseAdmin.GetBackup].
 */
export interface GetBackupRequest {
  /**
   * Required. Name of the backup.
   * Values are of the form
   * `projects/<project>/instances/<instance>/backups/<backup>`.
   */
  name: string;
}

/**
 * The request for
 * [DeleteBackup][google.spanner.admin.database.v1.DatabaseAdmin.DeleteBackup].
 */
export interface DeleteBackupRequest {
  /**
   * Required. Name of the backup to delete.
   * Values are of the form
   * `projects/<project>/instances/<instance>/backups/<backup>`.
   */
  name: string;
}

/**
 * The request for
 * [ListBackups][google.spanner.admin.database.v1.DatabaseAdmin.ListBackups].
 */
export interface ListBackupsRequest {
  /**
   * Required. The instance to list backups from.  Values are of the
   * form `projects/<project>/instances/<instance>`.
   */
  parent: string;
  /**
   * An expression that filters the list of returned backups.
   *
   * A filter expression consists of a field name, a comparison operator, and a
   * value for filtering.
   * The value must be a string, a number, or a boolean. The comparison operator
   * must be one of: `<`, `>`, `<=`, `>=`, `!=`, `=`, or `:`.
   * Colon `:` is the contains operator. Filter rules are not case sensitive.
   *
   * The following fields in the
   * [Backup][google.spanner.admin.database.v1.Backup] are eligible for
   * filtering:
   *
   *   * `name`
   *   * `database`
   *   * `state`
   *   * `create_time`  (and values are of the format YYYY-MM-DDTHH:MM:SSZ)
   *   * `expire_time`  (and values are of the format YYYY-MM-DDTHH:MM:SSZ)
   *   * `version_time` (and values are of the format YYYY-MM-DDTHH:MM:SSZ)
   *   * `size_bytes`
   *   * `backup_schedules`
   *
   * You can combine multiple expressions by enclosing each expression in
   * parentheses. By default, expressions are combined with AND logic, but
   * you can specify AND, OR, and NOT logic explicitly.
   *
   * Here are a few examples:
   *
   *   * `name:Howl` - The backup's name contains the string "howl".
   *   * `database:prod`
   *          - The database's name contains the string "prod".
   *   * `state:CREATING` - The backup is pending creation.
   *   * `state:READY` - The backup is fully created and ready for use.
   *   * `(name:howl) AND (create_time < \"2018-03-28T14:50:00Z\")`
   *          - The backup name contains the string "howl" and `create_time`
   *              of the backup is before 2018-03-28T14:50:00Z.
   *   * `expire_time < \"2018-03-28T14:50:00Z\"`
   *          - The backup `expire_time` is before 2018-03-28T14:50:00Z.
   *   * `size_bytes > 10000000000` - The backup's size is greater than 10GB
   *   * `backup_schedules:daily`
   *          - The backup is created from a schedule with "daily" in its name.
   */
  filter: string;
  /**
   * Number of backups to be returned in the response. If 0 or
   * less, defaults to the server's maximum allowed page size.
   */
  pageSize: number;
  /**
   * If non-empty, `page_token` should contain a
   * [next_page_token][google.spanner.admin.database.v1.ListBackupsResponse.next_page_token]
   * from a previous
   * [ListBackupsResponse][google.spanner.admin.database.v1.ListBackupsResponse]
   * to the same `parent` and with the same `filter`.
   */
  pageToken: string;
}

/**
 * The response for
 * [ListBackups][google.spanner.admin.database.v1.DatabaseAdmin.ListBackups].
 */
export interface ListBackupsResponse {
  /**
   * The list of matching backups. Backups returned are ordered by `create_time`
   * in descending order, starting from the most recent `create_time`.
   */
  backups: Backup[];
  /**
   * `next_page_token` can be sent in a subsequent
   * [ListBackups][google.spanner.admin.database.v1.DatabaseAdmin.ListBackups]
   * call to fetch more of the matching backups.
   */
  nextPageToken: string;
}

/**
 * The request for
 * [ListBackupOperations][google.spanner.admin.database.v1.DatabaseAdmin.ListBackupOperations].
 */
export interface ListBackupOperationsRequest {
  /**
   * Required. The instance of the backup operations. Values are of
   * the form `projects/<project>/instances/<instance>`.
   */
  parent: string;
  /**
   * An expression that filters the list of returned backup operations.
   *
   * A filter expression consists of a field name, a
   * comparison operator, and a value for filtering.
   * The value must be a string, a number, or a boolean. The comparison operator
   * must be one of: `<`, `>`, `<=`, `>=`, `!=`, `=`, or `:`.
   * Colon `:` is the contains operator. Filter rules are not case sensitive.
   *
   * The following fields in the [operation][google.longrunning.Operation]
   * are eligible for filtering:
   *
   *   * `name` - The name of the long-running operation
   *   * `done` - False if the operation is in progress, else true.
   *   * `metadata.@type` - the type of metadata. For example, the type string
   *      for
   *      [CreateBackupMetadata][google.spanner.admin.database.v1.CreateBackupMetadata]
   *      is
   *      `type.googleapis.com/google.spanner.admin.database.v1.CreateBackupMetadata`.
   *   * `metadata.<field_name>` - any field in metadata.value.
   *      `metadata.@type` must be specified first if filtering on metadata
   *      fields.
   *   * `error` - Error associated with the long-running operation.
   *   * `response.@type` - the type of response.
   *   * `response.<field_name>` - any field in response.value.
   *
   * You can combine multiple expressions by enclosing each expression in
   * parentheses. By default, expressions are combined with AND logic, but
   * you can specify AND, OR, and NOT logic explicitly.
   *
   * Here are a few examples:
   *
   *   * `done:true` - The operation is complete.
   *   * `(metadata.@type=type.googleapis.com/google.spanner.admin.database.v1.CreateBackupMetadata) AND` \
   *      `metadata.database:prod` - Returns operations where:
   *      * The operation's metadata type is
   *      [CreateBackupMetadata][google.spanner.admin.database.v1.CreateBackupMetadata].
   *      * The source database name of backup contains the string "prod".
   *   * `(metadata.@type=type.googleapis.com/google.spanner.admin.database.v1.CreateBackupMetadata) AND` \
   *     `(metadata.name:howl) AND` \
   *     `(metadata.progress.start_time < \"2018-03-28T14:50:00Z\") AND` \
   *     `(error:*)` - Returns operations where:
   *     * The operation's metadata type is
   *     [CreateBackupMetadata][google.spanner.admin.database.v1.CreateBackupMetadata].
   *     * The backup name contains the string "howl".
   *     * The operation started before 2018-03-28T14:50:00Z.
   *     * The operation resulted in an error.
   *   * `(metadata.@type=type.googleapis.com/google.spanner.admin.database.v1.CopyBackupMetadata) AND` \
   *     `(metadata.source_backup:test) AND` \
   *     `(metadata.progress.start_time < \"2022-01-18T14:50:00Z\") AND` \
   *     `(error:*)` - Returns operations where:
   *     * The operation's metadata type is
   *     [CopyBackupMetadata][google.spanner.admin.database.v1.CopyBackupMetadata].
   *     * The source backup name contains the string "test".
   *     * The operation started before 2022-01-18T14:50:00Z.
   *     * The operation resulted in an error.
   *   * `((metadata.@type=type.googleapis.com/google.spanner.admin.database.v1.CreateBackupMetadata) AND` \
   *     `(metadata.database:test_db)) OR` \
   *     `((metadata.@type=type.googleapis.com/google.spanner.admin.database.v1.CopyBackupMetadata)
   *     AND` \
   *     `(metadata.source_backup:test_bkp)) AND` \
   *     `(error:*)` - Returns operations where:
   *     * The operation's metadata matches either of criteria:
   *       * The operation's metadata type is
   *       [CreateBackupMetadata][google.spanner.admin.database.v1.CreateBackupMetadata]
   *       AND the source database name of the backup contains the string
   *       "test_db"
   *       * The operation's metadata type is
   *       [CopyBackupMetadata][google.spanner.admin.database.v1.CopyBackupMetadata]
   *       AND the source backup name contains the string "test_bkp"
   *     * The operation resulted in an error.
   */
  filter: string;
  /**
   * Number of operations to be returned in the response. If 0 or
   * less, defaults to the server's maximum allowed page size.
   */
  pageSize: number;
  /**
   * If non-empty, `page_token` should contain a
   * [next_page_token][google.spanner.admin.database.v1.ListBackupOperationsResponse.next_page_token]
   * from a previous
   * [ListBackupOperationsResponse][google.spanner.admin.database.v1.ListBackupOperationsResponse]
   * to the same `parent` and with the same `filter`.
   */
  pageToken: string;
}

/**
 * The response for
 * [ListBackupOperations][google.spanner.admin.database.v1.DatabaseAdmin.ListBackupOperations].
 */
export interface ListBackupOperationsResponse {
  /**
   * The list of matching backup [long-running
   * operations][google.longrunning.Operation]. Each operation's name will be
   * prefixed by the backup's name. The operation's
   * [metadata][google.longrunning.Operation.metadata] field type
   * `metadata.type_url` describes the type of the metadata. Operations returned
   * include those that are pending or have completed/failed/canceled within the
   * last 7 days. Operations returned are ordered by
   * `operation.metadata.value.progress.start_time` in descending order starting
   * from the most recently started operation.
   */
  operations: Operation[];
  /**
   * `next_page_token` can be sent in a subsequent
   * [ListBackupOperations][google.spanner.admin.database.v1.DatabaseAdmin.ListBackupOperations]
   * call to fetch more of the matching metadata.
   */
  nextPageToken: string;
}

/** Information about a backup. */
export interface BackupInfo {
  /** Name of the backup. */
  backup: string;
  /**
   * The backup contains an externally consistent copy of `source_database` at
   * the timestamp specified by `version_time`. If the
   * [CreateBackup][google.spanner.admin.database.v1.DatabaseAdmin.CreateBackup]
   * request did not specify `version_time`, the `version_time` of the backup is
   * equivalent to the `create_time`.
   */
  versionTime:
    | Date
    | undefined;
  /**
   * The time the
   * [CreateBackup][google.spanner.admin.database.v1.DatabaseAdmin.CreateBackup]
   * request was received.
   */
  createTime:
    | Date
    | undefined;
  /** Name of the database the backup was created from. */
  sourceDatabase: string;
}

/** Encryption configuration for the backup to create. */
export interface CreateBackupEncryptionConfig {
  /** Required. The encryption type of the backup. */
  encryptionType: CreateBackupEncryptionConfig_EncryptionType;
  /**
   * Optional. The Cloud KMS key that will be used to protect the backup.
   * This field should be set only when
   * [encryption_type][google.spanner.admin.database.v1.CreateBackupEncryptionConfig.encryption_type]
   * is `CUSTOMER_MANAGED_ENCRYPTION`. Values are of the form
   * `projects/<project>/locations/<location>/keyRings/<key_ring>/cryptoKeys/<kms_key_name>`.
   */
  kmsKeyName: string;
  /**
   * Optional. Specifies the KMS configuration for the one or more keys used to
   * protect the backup. Values are of the form
   * `projects/<project>/locations/<location>/keyRings/<key_ring>/cryptoKeys/<kms_key_name>`.
   *
   * The keys referenced by kms_key_names must fully cover all
   * regions of the backup's instance configuration. Some examples:
   * * For single region instance configs, specify a single regional
   * location KMS key.
   * * For multi-regional instance configs of type GOOGLE_MANAGED,
   * either specify a multi-regional location KMS key or multiple regional
   * location KMS keys that cover all regions in the instance config.
   * * For an instance config of type USER_MANAGED, please specify only
   * regional location KMS keys to cover each region in the instance config.
   * Multi-regional location KMS keys are not supported for USER_MANAGED
   * instance configs.
   */
  kmsKeyNames: string[];
}

/** Encryption types for the backup. */
export enum CreateBackupEncryptionConfig_EncryptionType {
  /** ENCRYPTION_TYPE_UNSPECIFIED - Unspecified. Do not use. */
  ENCRYPTION_TYPE_UNSPECIFIED = 0,
  /**
   * USE_DATABASE_ENCRYPTION - Use the same encryption configuration as the database. This is the
   * default option when
   * [encryption_config][google.spanner.admin.database.v1.CreateBackupEncryptionConfig]
   * is empty. For example, if the database is using
   * `Customer_Managed_Encryption`, the backup will be using the same Cloud
   * KMS key as the database.
   */
  USE_DATABASE_ENCRYPTION = 1,
  /** GOOGLE_DEFAULT_ENCRYPTION - Use Google default encryption. */
  GOOGLE_DEFAULT_ENCRYPTION = 2,
  /**
   * CUSTOMER_MANAGED_ENCRYPTION - Use customer managed encryption. If specified, `kms_key_name`
   * must contain a valid Cloud KMS key.
   */
  CUSTOMER_MANAGED_ENCRYPTION = 3,
  UNRECOGNIZED = -1,
}

export function createBackupEncryptionConfig_EncryptionTypeFromJSON(
  object: any,
): CreateBackupEncryptionConfig_EncryptionType {
  switch (object) {
    case 0:
    case "ENCRYPTION_TYPE_UNSPECIFIED":
      return CreateBackupEncryptionConfig_EncryptionType.ENCRYPTION_TYPE_UNSPECIFIED;
    case 1:
    case "USE_DATABASE_ENCRYPTION":
      return CreateBackupEncryptionConfig_EncryptionType.USE_DATABASE_ENCRYPTION;
    case 2:
    case "GOOGLE_DEFAULT_ENCRYPTION":
      return CreateBackupEncryptionConfig_EncryptionType.GOOGLE_DEFAULT_ENCRYPTION;
    case 3:
    case "CUSTOMER_MANAGED_ENCRYPTION":
      return CreateBackupEncryptionConfig_EncryptionType.CUSTOMER_MANAGED_ENCRYPTION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return CreateBackupEncryptionConfig_EncryptionType.UNRECOGNIZED;
  }
}

export function createBackupEncryptionConfig_EncryptionTypeToJSON(
  object: CreateBackupEncryptionConfig_EncryptionType,
): string {
  switch (object) {
    case CreateBackupEncryptionConfig_EncryptionType.ENCRYPTION_TYPE_UNSPECIFIED:
      return "ENCRYPTION_TYPE_UNSPECIFIED";
    case CreateBackupEncryptionConfig_EncryptionType.USE_DATABASE_ENCRYPTION:
      return "USE_DATABASE_ENCRYPTION";
    case CreateBackupEncryptionConfig_EncryptionType.GOOGLE_DEFAULT_ENCRYPTION:
      return "GOOGLE_DEFAULT_ENCRYPTION";
    case CreateBackupEncryptionConfig_EncryptionType.CUSTOMER_MANAGED_ENCRYPTION:
      return "CUSTOMER_MANAGED_ENCRYPTION";
    case CreateBackupEncryptionConfig_EncryptionType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Encryption configuration for the copied backup. */
export interface CopyBackupEncryptionConfig {
  /** Required. The encryption type of the backup. */
  encryptionType: CopyBackupEncryptionConfig_EncryptionType;
  /**
   * Optional. The Cloud KMS key that will be used to protect the backup.
   * This field should be set only when
   * [encryption_type][google.spanner.admin.database.v1.CopyBackupEncryptionConfig.encryption_type]
   * is `CUSTOMER_MANAGED_ENCRYPTION`. Values are of the form
   * `projects/<project>/locations/<location>/keyRings/<key_ring>/cryptoKeys/<kms_key_name>`.
   */
  kmsKeyName: string;
  /**
   * Optional. Specifies the KMS configuration for the one or more keys used to
   * protect the backup. Values are of the form
   * `projects/<project>/locations/<location>/keyRings/<key_ring>/cryptoKeys/<kms_key_name>`.
   * Kms keys specified can be in any order.
   *
   * The keys referenced by kms_key_names must fully cover all
   * regions of the backup's instance configuration. Some examples:
   * * For single region instance configs, specify a single regional
   * location KMS key.
   * * For multi-regional instance configs of type GOOGLE_MANAGED,
   * either specify a multi-regional location KMS key or multiple regional
   * location KMS keys that cover all regions in the instance config.
   * * For an instance config of type USER_MANAGED, please specify only
   * regional location KMS keys to cover each region in the instance config.
   * Multi-regional location KMS keys are not supported for USER_MANAGED
   * instance configs.
   */
  kmsKeyNames: string[];
}

/** Encryption types for the backup. */
export enum CopyBackupEncryptionConfig_EncryptionType {
  /** ENCRYPTION_TYPE_UNSPECIFIED - Unspecified. Do not use. */
  ENCRYPTION_TYPE_UNSPECIFIED = 0,
  /**
   * USE_CONFIG_DEFAULT_OR_BACKUP_ENCRYPTION - This is the default option for
   * [CopyBackup][google.spanner.admin.database.v1.DatabaseAdmin.CopyBackup]
   * when
   * [encryption_config][google.spanner.admin.database.v1.CopyBackupEncryptionConfig]
   * is not specified. For example, if the source backup is using
   * `Customer_Managed_Encryption`, the backup will be using the same Cloud
   * KMS key as the source backup.
   */
  USE_CONFIG_DEFAULT_OR_BACKUP_ENCRYPTION = 1,
  /** GOOGLE_DEFAULT_ENCRYPTION - Use Google default encryption. */
  GOOGLE_DEFAULT_ENCRYPTION = 2,
  /**
   * CUSTOMER_MANAGED_ENCRYPTION - Use customer managed encryption. If specified, either `kms_key_name` or
   * `kms_key_names` must contain valid Cloud KMS key(s).
   */
  CUSTOMER_MANAGED_ENCRYPTION = 3,
  UNRECOGNIZED = -1,
}

export function copyBackupEncryptionConfig_EncryptionTypeFromJSON(
  object: any,
): CopyBackupEncryptionConfig_EncryptionType {
  switch (object) {
    case 0:
    case "ENCRYPTION_TYPE_UNSPECIFIED":
      return CopyBackupEncryptionConfig_EncryptionType.ENCRYPTION_TYPE_UNSPECIFIED;
    case 1:
    case "USE_CONFIG_DEFAULT_OR_BACKUP_ENCRYPTION":
      return CopyBackupEncryptionConfig_EncryptionType.USE_CONFIG_DEFAULT_OR_BACKUP_ENCRYPTION;
    case 2:
    case "GOOGLE_DEFAULT_ENCRYPTION":
      return CopyBackupEncryptionConfig_EncryptionType.GOOGLE_DEFAULT_ENCRYPTION;
    case 3:
    case "CUSTOMER_MANAGED_ENCRYPTION":
      return CopyBackupEncryptionConfig_EncryptionType.CUSTOMER_MANAGED_ENCRYPTION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return CopyBackupEncryptionConfig_EncryptionType.UNRECOGNIZED;
  }
}

export function copyBackupEncryptionConfig_EncryptionTypeToJSON(
  object: CopyBackupEncryptionConfig_EncryptionType,
): string {
  switch (object) {
    case CopyBackupEncryptionConfig_EncryptionType.ENCRYPTION_TYPE_UNSPECIFIED:
      return "ENCRYPTION_TYPE_UNSPECIFIED";
    case CopyBackupEncryptionConfig_EncryptionType.USE_CONFIG_DEFAULT_OR_BACKUP_ENCRYPTION:
      return "USE_CONFIG_DEFAULT_OR_BACKUP_ENCRYPTION";
    case CopyBackupEncryptionConfig_EncryptionType.GOOGLE_DEFAULT_ENCRYPTION:
      return "GOOGLE_DEFAULT_ENCRYPTION";
    case CopyBackupEncryptionConfig_EncryptionType.CUSTOMER_MANAGED_ENCRYPTION:
      return "CUSTOMER_MANAGED_ENCRYPTION";
    case CopyBackupEncryptionConfig_EncryptionType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * The specification for full backups.
 * A full backup stores the entire contents of the database at a given
 * version time.
 */
export interface FullBackupSpec {
}

/**
 * The specification for incremental backup chains.
 * An incremental backup stores the delta of changes between a previous
 * backup and the database contents at a given version time. An
 * incremental backup chain consists of a full backup and zero or more
 * successive incremental backups. The first backup created for an
 * incremental backup chain is always a full backup.
 */
export interface IncrementalBackupSpec {
}

function createBaseBackup(): Backup {
  return {
    database: "",
    versionTime: undefined,
    expireTime: undefined,
    name: "",
    createTime: undefined,
    sizeBytes: Long.ZERO,
    freeableSizeBytes: Long.ZERO,
    exclusiveSizeBytes: Long.ZERO,
    state: 0,
    referencingDatabases: [],
    encryptionInfo: undefined,
    encryptionInformation: [],
    databaseDialect: 0,
    referencingBackups: [],
    maxExpireTime: undefined,
    backupSchedules: [],
    incrementalBackupChainId: "",
    oldestVersionTime: undefined,
  };
}

export const Backup: MessageFns<Backup> = {
  encode(message: Backup, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.database !== "") {
      writer.uint32(18).string(message.database);
    }
    if (message.versionTime !== undefined) {
      Timestamp.encode(toTimestamp(message.versionTime), writer.uint32(74).fork()).join();
    }
    if (message.expireTime !== undefined) {
      Timestamp.encode(toTimestamp(message.expireTime), writer.uint32(26).fork()).join();
    }
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(34).fork()).join();
    }
    if (!message.sizeBytes.equals(Long.ZERO)) {
      writer.uint32(40).int64(message.sizeBytes.toString());
    }
    if (!message.freeableSizeBytes.equals(Long.ZERO)) {
      writer.uint32(120).int64(message.freeableSizeBytes.toString());
    }
    if (!message.exclusiveSizeBytes.equals(Long.ZERO)) {
      writer.uint32(128).int64(message.exclusiveSizeBytes.toString());
    }
    if (message.state !== 0) {
      writer.uint32(48).int32(message.state);
    }
    for (const v of message.referencingDatabases) {
      writer.uint32(58).string(v!);
    }
    if (message.encryptionInfo !== undefined) {
      EncryptionInfo.encode(message.encryptionInfo, writer.uint32(66).fork()).join();
    }
    for (const v of message.encryptionInformation) {
      EncryptionInfo.encode(v!, writer.uint32(106).fork()).join();
    }
    if (message.databaseDialect !== 0) {
      writer.uint32(80).int32(message.databaseDialect);
    }
    for (const v of message.referencingBackups) {
      writer.uint32(90).string(v!);
    }
    if (message.maxExpireTime !== undefined) {
      Timestamp.encode(toTimestamp(message.maxExpireTime), writer.uint32(98).fork()).join();
    }
    for (const v of message.backupSchedules) {
      writer.uint32(114).string(v!);
    }
    if (message.incrementalBackupChainId !== "") {
      writer.uint32(138).string(message.incrementalBackupChainId);
    }
    if (message.oldestVersionTime !== undefined) {
      Timestamp.encode(toTimestamp(message.oldestVersionTime), writer.uint32(146).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Backup {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackup();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.database = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.versionTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.expireTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.sizeBytes = Long.fromString(reader.int64().toString());
          continue;
        case 15:
          if (tag !== 120) {
            break;
          }

          message.freeableSizeBytes = Long.fromString(reader.int64().toString());
          continue;
        case 16:
          if (tag !== 128) {
            break;
          }

          message.exclusiveSizeBytes = Long.fromString(reader.int64().toString());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.referencingDatabases.push(reader.string());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.encryptionInfo = EncryptionInfo.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.encryptionInformation.push(EncryptionInfo.decode(reader, reader.uint32()));
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.databaseDialect = reader.int32() as any;
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.referencingBackups.push(reader.string());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.maxExpireTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.backupSchedules.push(reader.string());
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.incrementalBackupChainId = reader.string();
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.oldestVersionTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Backup {
    return {
      database: isSet(object.database) ? globalThis.String(object.database) : "",
      versionTime: isSet(object.versionTime) ? fromJsonTimestamp(object.versionTime) : undefined,
      expireTime: isSet(object.expireTime) ? fromJsonTimestamp(object.expireTime) : undefined,
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      sizeBytes: isSet(object.sizeBytes) ? Long.fromValue(object.sizeBytes) : Long.ZERO,
      freeableSizeBytes: isSet(object.freeableSizeBytes) ? Long.fromValue(object.freeableSizeBytes) : Long.ZERO,
      exclusiveSizeBytes: isSet(object.exclusiveSizeBytes) ? Long.fromValue(object.exclusiveSizeBytes) : Long.ZERO,
      state: isSet(object.state) ? backup_StateFromJSON(object.state) : 0,
      referencingDatabases: globalThis.Array.isArray(object?.referencingDatabases)
        ? object.referencingDatabases.map((e: any) => globalThis.String(e))
        : [],
      encryptionInfo: isSet(object.encryptionInfo) ? EncryptionInfo.fromJSON(object.encryptionInfo) : undefined,
      encryptionInformation: globalThis.Array.isArray(object?.encryptionInformation)
        ? object.encryptionInformation.map((e: any) => EncryptionInfo.fromJSON(e))
        : [],
      databaseDialect: isSet(object.databaseDialect) ? databaseDialectFromJSON(object.databaseDialect) : 0,
      referencingBackups: globalThis.Array.isArray(object?.referencingBackups)
        ? object.referencingBackups.map((e: any) => globalThis.String(e))
        : [],
      maxExpireTime: isSet(object.maxExpireTime) ? fromJsonTimestamp(object.maxExpireTime) : undefined,
      backupSchedules: globalThis.Array.isArray(object?.backupSchedules)
        ? object.backupSchedules.map((e: any) => globalThis.String(e))
        : [],
      incrementalBackupChainId: isSet(object.incrementalBackupChainId)
        ? globalThis.String(object.incrementalBackupChainId)
        : "",
      oldestVersionTime: isSet(object.oldestVersionTime) ? fromJsonTimestamp(object.oldestVersionTime) : undefined,
    };
  },

  toJSON(message: Backup): unknown {
    const obj: any = {};
    if (message.database !== "") {
      obj.database = message.database;
    }
    if (message.versionTime !== undefined) {
      obj.versionTime = message.versionTime.toISOString();
    }
    if (message.expireTime !== undefined) {
      obj.expireTime = message.expireTime.toISOString();
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (!message.sizeBytes.equals(Long.ZERO)) {
      obj.sizeBytes = (message.sizeBytes || Long.ZERO).toString();
    }
    if (!message.freeableSizeBytes.equals(Long.ZERO)) {
      obj.freeableSizeBytes = (message.freeableSizeBytes || Long.ZERO).toString();
    }
    if (!message.exclusiveSizeBytes.equals(Long.ZERO)) {
      obj.exclusiveSizeBytes = (message.exclusiveSizeBytes || Long.ZERO).toString();
    }
    if (message.state !== 0) {
      obj.state = backup_StateToJSON(message.state);
    }
    if (message.referencingDatabases?.length) {
      obj.referencingDatabases = message.referencingDatabases;
    }
    if (message.encryptionInfo !== undefined) {
      obj.encryptionInfo = EncryptionInfo.toJSON(message.encryptionInfo);
    }
    if (message.encryptionInformation?.length) {
      obj.encryptionInformation = message.encryptionInformation.map((e) => EncryptionInfo.toJSON(e));
    }
    if (message.databaseDialect !== 0) {
      obj.databaseDialect = databaseDialectToJSON(message.databaseDialect);
    }
    if (message.referencingBackups?.length) {
      obj.referencingBackups = message.referencingBackups;
    }
    if (message.maxExpireTime !== undefined) {
      obj.maxExpireTime = message.maxExpireTime.toISOString();
    }
    if (message.backupSchedules?.length) {
      obj.backupSchedules = message.backupSchedules;
    }
    if (message.incrementalBackupChainId !== "") {
      obj.incrementalBackupChainId = message.incrementalBackupChainId;
    }
    if (message.oldestVersionTime !== undefined) {
      obj.oldestVersionTime = message.oldestVersionTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<Backup>): Backup {
    return Backup.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Backup>): Backup {
    const message = createBaseBackup();
    message.database = object.database ?? "";
    message.versionTime = object.versionTime ?? undefined;
    message.expireTime = object.expireTime ?? undefined;
    message.name = object.name ?? "";
    message.createTime = object.createTime ?? undefined;
    message.sizeBytes = (object.sizeBytes !== undefined && object.sizeBytes !== null)
      ? Long.fromValue(object.sizeBytes)
      : Long.ZERO;
    message.freeableSizeBytes = (object.freeableSizeBytes !== undefined && object.freeableSizeBytes !== null)
      ? Long.fromValue(object.freeableSizeBytes)
      : Long.ZERO;
    message.exclusiveSizeBytes = (object.exclusiveSizeBytes !== undefined && object.exclusiveSizeBytes !== null)
      ? Long.fromValue(object.exclusiveSizeBytes)
      : Long.ZERO;
    message.state = object.state ?? 0;
    message.referencingDatabases = object.referencingDatabases?.map((e) => e) || [];
    message.encryptionInfo = (object.encryptionInfo !== undefined && object.encryptionInfo !== null)
      ? EncryptionInfo.fromPartial(object.encryptionInfo)
      : undefined;
    message.encryptionInformation = object.encryptionInformation?.map((e) => EncryptionInfo.fromPartial(e)) || [];
    message.databaseDialect = object.databaseDialect ?? 0;
    message.referencingBackups = object.referencingBackups?.map((e) => e) || [];
    message.maxExpireTime = object.maxExpireTime ?? undefined;
    message.backupSchedules = object.backupSchedules?.map((e) => e) || [];
    message.incrementalBackupChainId = object.incrementalBackupChainId ?? "";
    message.oldestVersionTime = object.oldestVersionTime ?? undefined;
    return message;
  },
};

function createBaseCreateBackupRequest(): CreateBackupRequest {
  return { parent: "", backupId: "", backup: undefined, encryptionConfig: undefined };
}

export const CreateBackupRequest: MessageFns<CreateBackupRequest> = {
  encode(message: CreateBackupRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.backupId !== "") {
      writer.uint32(18).string(message.backupId);
    }
    if (message.backup !== undefined) {
      Backup.encode(message.backup, writer.uint32(26).fork()).join();
    }
    if (message.encryptionConfig !== undefined) {
      CreateBackupEncryptionConfig.encode(message.encryptionConfig, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateBackupRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateBackupRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.backupId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.backup = Backup.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.encryptionConfig = CreateBackupEncryptionConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateBackupRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      backupId: isSet(object.backupId) ? globalThis.String(object.backupId) : "",
      backup: isSet(object.backup) ? Backup.fromJSON(object.backup) : undefined,
      encryptionConfig: isSet(object.encryptionConfig)
        ? CreateBackupEncryptionConfig.fromJSON(object.encryptionConfig)
        : undefined,
    };
  },

  toJSON(message: CreateBackupRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.backupId !== "") {
      obj.backupId = message.backupId;
    }
    if (message.backup !== undefined) {
      obj.backup = Backup.toJSON(message.backup);
    }
    if (message.encryptionConfig !== undefined) {
      obj.encryptionConfig = CreateBackupEncryptionConfig.toJSON(message.encryptionConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<CreateBackupRequest>): CreateBackupRequest {
    return CreateBackupRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateBackupRequest>): CreateBackupRequest {
    const message = createBaseCreateBackupRequest();
    message.parent = object.parent ?? "";
    message.backupId = object.backupId ?? "";
    message.backup = (object.backup !== undefined && object.backup !== null)
      ? Backup.fromPartial(object.backup)
      : undefined;
    message.encryptionConfig = (object.encryptionConfig !== undefined && object.encryptionConfig !== null)
      ? CreateBackupEncryptionConfig.fromPartial(object.encryptionConfig)
      : undefined;
    return message;
  },
};

function createBaseCreateBackupMetadata(): CreateBackupMetadata {
  return { name: "", database: "", progress: undefined, cancelTime: undefined };
}

export const CreateBackupMetadata: MessageFns<CreateBackupMetadata> = {
  encode(message: CreateBackupMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.database !== "") {
      writer.uint32(18).string(message.database);
    }
    if (message.progress !== undefined) {
      OperationProgress.encode(message.progress, writer.uint32(26).fork()).join();
    }
    if (message.cancelTime !== undefined) {
      Timestamp.encode(toTimestamp(message.cancelTime), writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateBackupMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateBackupMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.database = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.progress = OperationProgress.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.cancelTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateBackupMetadata {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      database: isSet(object.database) ? globalThis.String(object.database) : "",
      progress: isSet(object.progress) ? OperationProgress.fromJSON(object.progress) : undefined,
      cancelTime: isSet(object.cancelTime) ? fromJsonTimestamp(object.cancelTime) : undefined,
    };
  },

  toJSON(message: CreateBackupMetadata): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.database !== "") {
      obj.database = message.database;
    }
    if (message.progress !== undefined) {
      obj.progress = OperationProgress.toJSON(message.progress);
    }
    if (message.cancelTime !== undefined) {
      obj.cancelTime = message.cancelTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<CreateBackupMetadata>): CreateBackupMetadata {
    return CreateBackupMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateBackupMetadata>): CreateBackupMetadata {
    const message = createBaseCreateBackupMetadata();
    message.name = object.name ?? "";
    message.database = object.database ?? "";
    message.progress = (object.progress !== undefined && object.progress !== null)
      ? OperationProgress.fromPartial(object.progress)
      : undefined;
    message.cancelTime = object.cancelTime ?? undefined;
    return message;
  },
};

function createBaseCopyBackupRequest(): CopyBackupRequest {
  return { parent: "", backupId: "", sourceBackup: "", expireTime: undefined, encryptionConfig: undefined };
}

export const CopyBackupRequest: MessageFns<CopyBackupRequest> = {
  encode(message: CopyBackupRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.backupId !== "") {
      writer.uint32(18).string(message.backupId);
    }
    if (message.sourceBackup !== "") {
      writer.uint32(26).string(message.sourceBackup);
    }
    if (message.expireTime !== undefined) {
      Timestamp.encode(toTimestamp(message.expireTime), writer.uint32(34).fork()).join();
    }
    if (message.encryptionConfig !== undefined) {
      CopyBackupEncryptionConfig.encode(message.encryptionConfig, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CopyBackupRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCopyBackupRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.backupId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.sourceBackup = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.expireTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.encryptionConfig = CopyBackupEncryptionConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CopyBackupRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      backupId: isSet(object.backupId) ? globalThis.String(object.backupId) : "",
      sourceBackup: isSet(object.sourceBackup) ? globalThis.String(object.sourceBackup) : "",
      expireTime: isSet(object.expireTime) ? fromJsonTimestamp(object.expireTime) : undefined,
      encryptionConfig: isSet(object.encryptionConfig)
        ? CopyBackupEncryptionConfig.fromJSON(object.encryptionConfig)
        : undefined,
    };
  },

  toJSON(message: CopyBackupRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.backupId !== "") {
      obj.backupId = message.backupId;
    }
    if (message.sourceBackup !== "") {
      obj.sourceBackup = message.sourceBackup;
    }
    if (message.expireTime !== undefined) {
      obj.expireTime = message.expireTime.toISOString();
    }
    if (message.encryptionConfig !== undefined) {
      obj.encryptionConfig = CopyBackupEncryptionConfig.toJSON(message.encryptionConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<CopyBackupRequest>): CopyBackupRequest {
    return CopyBackupRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CopyBackupRequest>): CopyBackupRequest {
    const message = createBaseCopyBackupRequest();
    message.parent = object.parent ?? "";
    message.backupId = object.backupId ?? "";
    message.sourceBackup = object.sourceBackup ?? "";
    message.expireTime = object.expireTime ?? undefined;
    message.encryptionConfig = (object.encryptionConfig !== undefined && object.encryptionConfig !== null)
      ? CopyBackupEncryptionConfig.fromPartial(object.encryptionConfig)
      : undefined;
    return message;
  },
};

function createBaseCopyBackupMetadata(): CopyBackupMetadata {
  return { name: "", sourceBackup: "", progress: undefined, cancelTime: undefined };
}

export const CopyBackupMetadata: MessageFns<CopyBackupMetadata> = {
  encode(message: CopyBackupMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.sourceBackup !== "") {
      writer.uint32(18).string(message.sourceBackup);
    }
    if (message.progress !== undefined) {
      OperationProgress.encode(message.progress, writer.uint32(26).fork()).join();
    }
    if (message.cancelTime !== undefined) {
      Timestamp.encode(toTimestamp(message.cancelTime), writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CopyBackupMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCopyBackupMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.sourceBackup = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.progress = OperationProgress.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.cancelTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CopyBackupMetadata {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      sourceBackup: isSet(object.sourceBackup) ? globalThis.String(object.sourceBackup) : "",
      progress: isSet(object.progress) ? OperationProgress.fromJSON(object.progress) : undefined,
      cancelTime: isSet(object.cancelTime) ? fromJsonTimestamp(object.cancelTime) : undefined,
    };
  },

  toJSON(message: CopyBackupMetadata): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.sourceBackup !== "") {
      obj.sourceBackup = message.sourceBackup;
    }
    if (message.progress !== undefined) {
      obj.progress = OperationProgress.toJSON(message.progress);
    }
    if (message.cancelTime !== undefined) {
      obj.cancelTime = message.cancelTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<CopyBackupMetadata>): CopyBackupMetadata {
    return CopyBackupMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CopyBackupMetadata>): CopyBackupMetadata {
    const message = createBaseCopyBackupMetadata();
    message.name = object.name ?? "";
    message.sourceBackup = object.sourceBackup ?? "";
    message.progress = (object.progress !== undefined && object.progress !== null)
      ? OperationProgress.fromPartial(object.progress)
      : undefined;
    message.cancelTime = object.cancelTime ?? undefined;
    return message;
  },
};

function createBaseUpdateBackupRequest(): UpdateBackupRequest {
  return { backup: undefined, updateMask: undefined };
}

export const UpdateBackupRequest: MessageFns<UpdateBackupRequest> = {
  encode(message: UpdateBackupRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.backup !== undefined) {
      Backup.encode(message.backup, writer.uint32(10).fork()).join();
    }
    if (message.updateMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.updateMask), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateBackupRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateBackupRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.backup = Backup.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.updateMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateBackupRequest {
    return {
      backup: isSet(object.backup) ? Backup.fromJSON(object.backup) : undefined,
      updateMask: isSet(object.updateMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.updateMask)) : undefined,
    };
  },

  toJSON(message: UpdateBackupRequest): unknown {
    const obj: any = {};
    if (message.backup !== undefined) {
      obj.backup = Backup.toJSON(message.backup);
    }
    if (message.updateMask !== undefined) {
      obj.updateMask = FieldMask.toJSON(FieldMask.wrap(message.updateMask));
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateBackupRequest>): UpdateBackupRequest {
    return UpdateBackupRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateBackupRequest>): UpdateBackupRequest {
    const message = createBaseUpdateBackupRequest();
    message.backup = (object.backup !== undefined && object.backup !== null)
      ? Backup.fromPartial(object.backup)
      : undefined;
    message.updateMask = object.updateMask ?? undefined;
    return message;
  },
};

function createBaseGetBackupRequest(): GetBackupRequest {
  return { name: "" };
}

export const GetBackupRequest: MessageFns<GetBackupRequest> = {
  encode(message: GetBackupRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetBackupRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetBackupRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetBackupRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: GetBackupRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<GetBackupRequest>): GetBackupRequest {
    return GetBackupRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetBackupRequest>): GetBackupRequest {
    const message = createBaseGetBackupRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseDeleteBackupRequest(): DeleteBackupRequest {
  return { name: "" };
}

export const DeleteBackupRequest: MessageFns<DeleteBackupRequest> = {
  encode(message: DeleteBackupRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeleteBackupRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeleteBackupRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeleteBackupRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: DeleteBackupRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<DeleteBackupRequest>): DeleteBackupRequest {
    return DeleteBackupRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeleteBackupRequest>): DeleteBackupRequest {
    const message = createBaseDeleteBackupRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseListBackupsRequest(): ListBackupsRequest {
  return { parent: "", filter: "", pageSize: 0, pageToken: "" };
}

export const ListBackupsRequest: MessageFns<ListBackupsRequest> = {
  encode(message: ListBackupsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.filter !== "") {
      writer.uint32(18).string(message.filter);
    }
    if (message.pageSize !== 0) {
      writer.uint32(24).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(34).string(message.pageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListBackupsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListBackupsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.filter = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.pageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListBackupsRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      filter: isSet(object.filter) ? globalThis.String(object.filter) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
    };
  },

  toJSON(message: ListBackupsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.filter !== "") {
      obj.filter = message.filter;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListBackupsRequest>): ListBackupsRequest {
    return ListBackupsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListBackupsRequest>): ListBackupsRequest {
    const message = createBaseListBackupsRequest();
    message.parent = object.parent ?? "";
    message.filter = object.filter ?? "";
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    return message;
  },
};

function createBaseListBackupsResponse(): ListBackupsResponse {
  return { backups: [], nextPageToken: "" };
}

export const ListBackupsResponse: MessageFns<ListBackupsResponse> = {
  encode(message: ListBackupsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.backups) {
      Backup.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListBackupsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListBackupsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.backups.push(Backup.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListBackupsResponse {
    return {
      backups: globalThis.Array.isArray(object?.backups) ? object.backups.map((e: any) => Backup.fromJSON(e)) : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: ListBackupsResponse): unknown {
    const obj: any = {};
    if (message.backups?.length) {
      obj.backups = message.backups.map((e) => Backup.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListBackupsResponse>): ListBackupsResponse {
    return ListBackupsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListBackupsResponse>): ListBackupsResponse {
    const message = createBaseListBackupsResponse();
    message.backups = object.backups?.map((e) => Backup.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBaseListBackupOperationsRequest(): ListBackupOperationsRequest {
  return { parent: "", filter: "", pageSize: 0, pageToken: "" };
}

export const ListBackupOperationsRequest: MessageFns<ListBackupOperationsRequest> = {
  encode(message: ListBackupOperationsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.filter !== "") {
      writer.uint32(18).string(message.filter);
    }
    if (message.pageSize !== 0) {
      writer.uint32(24).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(34).string(message.pageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListBackupOperationsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListBackupOperationsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.filter = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.pageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListBackupOperationsRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      filter: isSet(object.filter) ? globalThis.String(object.filter) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
    };
  },

  toJSON(message: ListBackupOperationsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.filter !== "") {
      obj.filter = message.filter;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListBackupOperationsRequest>): ListBackupOperationsRequest {
    return ListBackupOperationsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListBackupOperationsRequest>): ListBackupOperationsRequest {
    const message = createBaseListBackupOperationsRequest();
    message.parent = object.parent ?? "";
    message.filter = object.filter ?? "";
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    return message;
  },
};

function createBaseListBackupOperationsResponse(): ListBackupOperationsResponse {
  return { operations: [], nextPageToken: "" };
}

export const ListBackupOperationsResponse: MessageFns<ListBackupOperationsResponse> = {
  encode(message: ListBackupOperationsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.operations) {
      Operation.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListBackupOperationsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListBackupOperationsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.operations.push(Operation.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListBackupOperationsResponse {
    return {
      operations: globalThis.Array.isArray(object?.operations)
        ? object.operations.map((e: any) => Operation.fromJSON(e))
        : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: ListBackupOperationsResponse): unknown {
    const obj: any = {};
    if (message.operations?.length) {
      obj.operations = message.operations.map((e) => Operation.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListBackupOperationsResponse>): ListBackupOperationsResponse {
    return ListBackupOperationsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListBackupOperationsResponse>): ListBackupOperationsResponse {
    const message = createBaseListBackupOperationsResponse();
    message.operations = object.operations?.map((e) => Operation.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBaseBackupInfo(): BackupInfo {
  return { backup: "", versionTime: undefined, createTime: undefined, sourceDatabase: "" };
}

export const BackupInfo: MessageFns<BackupInfo> = {
  encode(message: BackupInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.backup !== "") {
      writer.uint32(10).string(message.backup);
    }
    if (message.versionTime !== undefined) {
      Timestamp.encode(toTimestamp(message.versionTime), writer.uint32(34).fork()).join();
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(18).fork()).join();
    }
    if (message.sourceDatabase !== "") {
      writer.uint32(26).string(message.sourceDatabase);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BackupInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackupInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.backup = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.versionTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.sourceDatabase = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BackupInfo {
    return {
      backup: isSet(object.backup) ? globalThis.String(object.backup) : "",
      versionTime: isSet(object.versionTime) ? fromJsonTimestamp(object.versionTime) : undefined,
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      sourceDatabase: isSet(object.sourceDatabase) ? globalThis.String(object.sourceDatabase) : "",
    };
  },

  toJSON(message: BackupInfo): unknown {
    const obj: any = {};
    if (message.backup !== "") {
      obj.backup = message.backup;
    }
    if (message.versionTime !== undefined) {
      obj.versionTime = message.versionTime.toISOString();
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.sourceDatabase !== "") {
      obj.sourceDatabase = message.sourceDatabase;
    }
    return obj;
  },

  create(base?: DeepPartial<BackupInfo>): BackupInfo {
    return BackupInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BackupInfo>): BackupInfo {
    const message = createBaseBackupInfo();
    message.backup = object.backup ?? "";
    message.versionTime = object.versionTime ?? undefined;
    message.createTime = object.createTime ?? undefined;
    message.sourceDatabase = object.sourceDatabase ?? "";
    return message;
  },
};

function createBaseCreateBackupEncryptionConfig(): CreateBackupEncryptionConfig {
  return { encryptionType: 0, kmsKeyName: "", kmsKeyNames: [] };
}

export const CreateBackupEncryptionConfig: MessageFns<CreateBackupEncryptionConfig> = {
  encode(message: CreateBackupEncryptionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.encryptionType !== 0) {
      writer.uint32(8).int32(message.encryptionType);
    }
    if (message.kmsKeyName !== "") {
      writer.uint32(18).string(message.kmsKeyName);
    }
    for (const v of message.kmsKeyNames) {
      writer.uint32(26).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateBackupEncryptionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateBackupEncryptionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.encryptionType = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.kmsKeyName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.kmsKeyNames.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateBackupEncryptionConfig {
    return {
      encryptionType: isSet(object.encryptionType)
        ? createBackupEncryptionConfig_EncryptionTypeFromJSON(object.encryptionType)
        : 0,
      kmsKeyName: isSet(object.kmsKeyName) ? globalThis.String(object.kmsKeyName) : "",
      kmsKeyNames: globalThis.Array.isArray(object?.kmsKeyNames)
        ? object.kmsKeyNames.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: CreateBackupEncryptionConfig): unknown {
    const obj: any = {};
    if (message.encryptionType !== 0) {
      obj.encryptionType = createBackupEncryptionConfig_EncryptionTypeToJSON(message.encryptionType);
    }
    if (message.kmsKeyName !== "") {
      obj.kmsKeyName = message.kmsKeyName;
    }
    if (message.kmsKeyNames?.length) {
      obj.kmsKeyNames = message.kmsKeyNames;
    }
    return obj;
  },

  create(base?: DeepPartial<CreateBackupEncryptionConfig>): CreateBackupEncryptionConfig {
    return CreateBackupEncryptionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateBackupEncryptionConfig>): CreateBackupEncryptionConfig {
    const message = createBaseCreateBackupEncryptionConfig();
    message.encryptionType = object.encryptionType ?? 0;
    message.kmsKeyName = object.kmsKeyName ?? "";
    message.kmsKeyNames = object.kmsKeyNames?.map((e) => e) || [];
    return message;
  },
};

function createBaseCopyBackupEncryptionConfig(): CopyBackupEncryptionConfig {
  return { encryptionType: 0, kmsKeyName: "", kmsKeyNames: [] };
}

export const CopyBackupEncryptionConfig: MessageFns<CopyBackupEncryptionConfig> = {
  encode(message: CopyBackupEncryptionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.encryptionType !== 0) {
      writer.uint32(8).int32(message.encryptionType);
    }
    if (message.kmsKeyName !== "") {
      writer.uint32(18).string(message.kmsKeyName);
    }
    for (const v of message.kmsKeyNames) {
      writer.uint32(26).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CopyBackupEncryptionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCopyBackupEncryptionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.encryptionType = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.kmsKeyName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.kmsKeyNames.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CopyBackupEncryptionConfig {
    return {
      encryptionType: isSet(object.encryptionType)
        ? copyBackupEncryptionConfig_EncryptionTypeFromJSON(object.encryptionType)
        : 0,
      kmsKeyName: isSet(object.kmsKeyName) ? globalThis.String(object.kmsKeyName) : "",
      kmsKeyNames: globalThis.Array.isArray(object?.kmsKeyNames)
        ? object.kmsKeyNames.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: CopyBackupEncryptionConfig): unknown {
    const obj: any = {};
    if (message.encryptionType !== 0) {
      obj.encryptionType = copyBackupEncryptionConfig_EncryptionTypeToJSON(message.encryptionType);
    }
    if (message.kmsKeyName !== "") {
      obj.kmsKeyName = message.kmsKeyName;
    }
    if (message.kmsKeyNames?.length) {
      obj.kmsKeyNames = message.kmsKeyNames;
    }
    return obj;
  },

  create(base?: DeepPartial<CopyBackupEncryptionConfig>): CopyBackupEncryptionConfig {
    return CopyBackupEncryptionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CopyBackupEncryptionConfig>): CopyBackupEncryptionConfig {
    const message = createBaseCopyBackupEncryptionConfig();
    message.encryptionType = object.encryptionType ?? 0;
    message.kmsKeyName = object.kmsKeyName ?? "";
    message.kmsKeyNames = object.kmsKeyNames?.map((e) => e) || [];
    return message;
  },
};

function createBaseFullBackupSpec(): FullBackupSpec {
  return {};
}

export const FullBackupSpec: MessageFns<FullBackupSpec> = {
  encode(_: FullBackupSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FullBackupSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFullBackupSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): FullBackupSpec {
    return {};
  },

  toJSON(_: FullBackupSpec): unknown {
    const obj: any = {};
    return obj;
  },

  create(base?: DeepPartial<FullBackupSpec>): FullBackupSpec {
    return FullBackupSpec.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<FullBackupSpec>): FullBackupSpec {
    const message = createBaseFullBackupSpec();
    return message;
  },
};

function createBaseIncrementalBackupSpec(): IncrementalBackupSpec {
  return {};
}

export const IncrementalBackupSpec: MessageFns<IncrementalBackupSpec> = {
  encode(_: IncrementalBackupSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): IncrementalBackupSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseIncrementalBackupSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): IncrementalBackupSpec {
    return {};
  },

  toJSON(_: IncrementalBackupSpec): unknown {
    const obj: any = {};
    return obj;
  },

  create(base?: DeepPartial<IncrementalBackupSpec>): IncrementalBackupSpec {
    return IncrementalBackupSpec.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<IncrementalBackupSpec>): IncrementalBackupSpec {
    const message = createBaseIncrementalBackupSpec();
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
