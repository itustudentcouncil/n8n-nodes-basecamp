// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/storagetransfer/v1/transfer_types.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Duration } from "../../protobuf/duration.js";
import { Timestamp } from "../../protobuf/timestamp.js";
import { Code, codeFromJSON, codeToJSON } from "../../rpc/code.js";
import { DateMessage } from "../../type/date.js";
import { TimeOfDay } from "../../type/timeofday.js";

export const protobufPackage = "google.storagetransfer.v1";

/** Google service account */
export interface GoogleServiceAccount {
  /** Email address of the service account. */
  accountEmail: string;
  /** Unique identifier for the service account. */
  subjectId: string;
}

/**
 * AWS access key (see
 * [AWS Security
 * Credentials](https://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html)).
 *
 * For information on our data retention policy for user credentials, see
 * [User credentials](/storage-transfer/docs/data-retention#user-credentials).
 */
export interface AwsAccessKey {
  /** Required. AWS access key ID. */
  accessKeyId: string;
  /**
   * Required. AWS secret access key. This field is not returned in RPC
   * responses.
   */
  secretAccessKey: string;
}

/**
 * Azure credentials
 *
 * For information on our data retention policy for user credentials, see
 * [User credentials](/storage-transfer/docs/data-retention#user-credentials).
 */
export interface AzureCredentials {
  /**
   * Required. Azure shared access signature (SAS).
   *
   * For more information about SAS, see
   * [Grant limited access to Azure Storage resources using shared access
   * signatures
   * (SAS)](https://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview).
   */
  sasToken: string;
}

/**
 * Conditions that determine which objects are transferred. Applies only
 * to Cloud Data Sources such as S3, Azure, and Cloud Storage.
 *
 * The "last modification time" refers to the time of the
 * last change to the object's content or metadata â€” specifically, this is
 * the `updated` property of Cloud Storage objects, the `LastModified` field
 * of S3 objects, and the `Last-Modified` header of Azure blobs.
 *
 * Transfers with a [PosixFilesystem][google.storagetransfer.v1.PosixFilesystem]
 * source or destination don't support `ObjectConditions`.
 */
export interface ObjectConditions {
  /**
   * Ensures that objects are not transferred until a specific minimum time
   * has elapsed after the "last modification time". When a
   * [TransferOperation][google.storagetransfer.v1.TransferOperation] begins,
   * objects with a "last modification time" are transferred only if the elapsed
   * time between the
   * [start_time][google.storagetransfer.v1.TransferOperation.start_time] of the
   * `TransferOperation` and the "last modification time" of the object is equal
   * to or greater than the value of min_time_elapsed_since_last_modification`.
   * Objects that do not have a "last modification time" are also transferred.
   */
  minTimeElapsedSinceLastModification:
    | Duration
    | undefined;
  /**
   * Ensures that objects are not transferred if a specific maximum time
   * has elapsed since the "last modification time".
   * When a [TransferOperation][google.storagetransfer.v1.TransferOperation]
   * begins, objects with a "last modification time" are transferred only if the
   * elapsed time between the
   * [start_time][google.storagetransfer.v1.TransferOperation.start_time] of the
   * `TransferOperation`and the "last modification time" of the object
   *  is less than the value of max_time_elapsed_since_last_modification`.
   * Objects that do not have a "last modification time" are also transferred.
   */
  maxTimeElapsedSinceLastModification:
    | Duration
    | undefined;
  /**
   * If you specify `include_prefixes`, Storage Transfer Service uses the items
   * in the `include_prefixes` array to determine which objects to include in a
   * transfer. Objects must start with one of the matching `include_prefixes`
   * for inclusion in the transfer. If
   * [exclude_prefixes][google.storagetransfer.v1.ObjectConditions.exclude_prefixes]
   * is specified, objects must not start with any of the `exclude_prefixes`
   * specified for inclusion in the transfer.
   *
   * The following are requirements of `include_prefixes`:
   *
   *   * Each include-prefix can contain any sequence of Unicode characters, to
   *     a max length of 1024 bytes when UTF8-encoded, and must not contain
   *     Carriage Return or Line Feed characters.  Wildcard matching and regular
   *     expression matching are not supported.
   *
   *   * Each include-prefix must omit the leading slash. For example, to
   *     include the object `s3://my-aws-bucket/logs/y=2015/requests.gz`,
   *     specify the include-prefix as `logs/y=2015/requests.gz`.
   *
   *   * None of the include-prefix values can be empty, if specified.
   *
   *   * Each include-prefix must include a distinct portion of the object
   *     namespace. No include-prefix may be a prefix of another
   *     include-prefix.
   *
   * The max size of `include_prefixes` is 1000.
   *
   * For more information, see [Filtering objects from
   * transfers](/storage-transfer/docs/filtering-objects-from-transfers).
   */
  includePrefixes: string[];
  /**
   * If you specify `exclude_prefixes`, Storage Transfer Service uses the items
   * in the `exclude_prefixes` array to determine which objects to exclude from
   * a transfer. Objects must not start with one of the matching
   * `exclude_prefixes` for inclusion in a transfer.
   *
   * The following are requirements of `exclude_prefixes`:
   *
   *   * Each exclude-prefix can contain any sequence of Unicode characters, to
   *     a max length of 1024 bytes when UTF8-encoded, and must not contain
   *     Carriage Return or Line Feed characters.  Wildcard matching and regular
   *     expression matching are not supported.
   *
   *   * Each exclude-prefix must omit the leading slash. For example, to
   *     exclude the object `s3://my-aws-bucket/logs/y=2015/requests.gz`,
   *     specify the exclude-prefix as `logs/y=2015/requests.gz`.
   *
   *   * None of the exclude-prefix values can be empty, if specified.
   *
   *   * Each exclude-prefix must exclude a distinct portion of the object
   *     namespace. No exclude-prefix may be a prefix of another
   *     exclude-prefix.
   *
   *   * If
   *   [include_prefixes][google.storagetransfer.v1.ObjectConditions.include_prefixes]
   *   is specified, then each exclude-prefix must start with the value of a
   *   path explicitly included by `include_prefixes`.
   *
   * The max size of `exclude_prefixes` is 1000.
   *
   * For more information, see [Filtering objects from
   * transfers](/storage-transfer/docs/filtering-objects-from-transfers).
   */
  excludePrefixes: string[];
  /**
   * If specified, only objects with a "last modification time" on or after
   * this timestamp and objects that don't have a "last modification time" are
   * transferred.
   *
   * The `last_modified_since` and `last_modified_before` fields can be used
   * together for chunked data processing. For example, consider a script that
   * processes each day's worth of data at a time. For that you'd set each
   * of the fields as follows:
   *
   * *  `last_modified_since` to the start of the day
   *
   * *  `last_modified_before` to the end of the day
   */
  lastModifiedSince:
    | Date
    | undefined;
  /**
   * If specified, only objects with a "last modification time" before this
   * timestamp and objects that don't have a "last modification time" are
   * transferred.
   */
  lastModifiedBefore: Date | undefined;
}

/**
 * In a GcsData resource, an object's name is the Cloud Storage object's
 * name and its "last modification time" refers to the object's `updated`
 * property of Cloud Storage objects, which changes when the content or the
 * metadata of the object is updated.
 */
export interface GcsData {
  /**
   * Required. Cloud Storage bucket name. Must meet
   * [Bucket Name Requirements](/storage/docs/naming#requirements).
   */
  bucketName: string;
  /**
   * Root path to transfer objects.
   *
   * Must be an empty string or full path name that ends with a '/'. This field
   * is treated as an object prefix. As such, it should generally not begin with
   * a '/'.
   *
   * The root path value must meet
   * [Object Name Requirements](/storage/docs/naming#objectnames).
   */
  path: string;
  /**
   * Preview. Enables the transfer of managed folders between Cloud Storage
   * buckets. Set this option on the gcs_data_source.
   *
   * If set to true:
   *
   * - Managed folders in the source bucket are transferred to the
   *   destination bucket.
   * - Managed folders in the destination bucket are overwritten. Other
   *   OVERWRITE options are not supported.
   *
   * See
   * [Transfer Cloud Storage managed
   * folders](/storage-transfer/docs/managed-folders).
   */
  managedFolderTransferEnabled: boolean;
}

/**
 * An AwsS3Data resource can be a data source, but not a data sink.
 * In an AwsS3Data resource, an object's name is the S3 object's key name.
 */
export interface AwsS3Data {
  /**
   * Required. S3 Bucket name (see
   * [Creating a
   * bucket](https://docs.aws.amazon.com/AmazonS3/latest/dev/create-bucket-get-location-example.html)).
   */
  bucketName: string;
  /**
   * Input only. AWS access key used to sign the API requests to the AWS S3
   * bucket. Permissions on the bucket must be granted to the access ID of the
   * AWS access key.
   *
   * For information on our data retention policy for user credentials, see
   * [User credentials](/storage-transfer/docs/data-retention#user-credentials).
   */
  awsAccessKey:
    | AwsAccessKey
    | undefined;
  /**
   * Root path to transfer objects.
   *
   * Must be an empty string or full path name that ends with a '/'. This field
   * is treated as an object prefix. As such, it should generally not begin with
   * a '/'.
   */
  path: string;
  /**
   * The Amazon Resource Name (ARN) of the role to support temporary
   * credentials via `AssumeRoleWithWebIdentity`. For more information about
   * ARNs, see [IAM
   * ARNs](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns).
   *
   * When a role ARN is provided, Transfer Service fetches temporary
   * credentials for the session using a `AssumeRoleWithWebIdentity` call for
   * the provided role using the
   * [GoogleServiceAccount][google.storagetransfer.v1.GoogleServiceAccount] for
   * this project.
   */
  roleArn: string;
  /**
   * Optional. The CloudFront distribution domain name pointing to this bucket,
   * to use when fetching.
   *
   * See
   * [Transfer from S3 via
   * CloudFront](https://cloud.google.com/storage-transfer/docs/s3-cloudfront)
   * for more information.
   *
   * Format: `https://{id}.cloudfront.net` or any valid custom domain. Must
   * begin with `https://`.
   */
  cloudfrontDomain: string;
  /**
   * Optional. The Resource name of a secret in Secret Manager.
   *
   * AWS credentials must be stored in Secret Manager in JSON format:
   *
   * {
   *  "access_key_id": "ACCESS_KEY_ID",
   *  "secret_access_key": "SECRET_ACCESS_KEY"
   * }
   *
   * [GoogleServiceAccount][google.storagetransfer.v1.GoogleServiceAccount] must
   * be granted `roles/secretmanager.secretAccessor` for the resource.
   *
   * See [Configure access to a source: Amazon S3]
   * (https://cloud.google.com/storage-transfer/docs/source-amazon-s3#secret_manager)
   * for more information.
   *
   * If `credentials_secret` is specified, do not specify
   * [role_arn][google.storagetransfer.v1.AwsS3Data.role_arn] or
   * [aws_access_key][google.storagetransfer.v1.AwsS3Data.aws_access_key].
   *
   * Format: `projects/{project_number}/secrets/{secret_name}`
   */
  credentialsSecret: string;
  /**
   * Egress bytes over a Google-managed private network.
   * This network is shared between other users of Storage Transfer Service.
   */
  managedPrivateNetwork?: boolean | undefined;
}

/**
 * An AzureBlobStorageData resource can be a data source, but not a data sink.
 * An AzureBlobStorageData resource represents one Azure container. The storage
 * account determines the [Azure
 * endpoint](https://docs.microsoft.com/en-us/azure/storage/common/storage-create-storage-account#storage-account-endpoints).
 * In an AzureBlobStorageData resource, a blobs's name is the [Azure Blob
 * Storage blob's key
 * name](https://docs.microsoft.com/en-us/rest/api/storageservices/naming-and-referencing-containers--blobs--and-metadata#blob-names).
 */
export interface AzureBlobStorageData {
  /** Required. The name of the Azure Storage account. */
  storageAccount: string;
  /**
   * Required. Input only. Credentials used to authenticate API requests to
   * Azure.
   *
   * For information on our data retention policy for user credentials, see
   * [User credentials](/storage-transfer/docs/data-retention#user-credentials).
   */
  azureCredentials:
    | AzureCredentials
    | undefined;
  /** Required. The container to transfer from the Azure Storage account. */
  container: string;
  /**
   * Root path to transfer objects.
   *
   * Must be an empty string or full path name that ends with a '/'. This field
   * is treated as an object prefix. As such, it should generally not begin with
   * a '/'.
   */
  path: string;
  /**
   * Optional. The Resource name of a secret in Secret Manager.
   *
   * The Azure SAS token must be stored in Secret Manager in JSON format:
   *
   * {
   *  "sas_token" : "SAS_TOKEN"
   * }
   *
   * [GoogleServiceAccount][google.storagetransfer.v1.GoogleServiceAccount] must
   * be granted `roles/secretmanager.secretAccessor` for the resource.
   *
   * See [Configure access to a source: Microsoft Azure Blob Storage]
   * (https://cloud.google.com/storage-transfer/docs/source-microsoft-azure#secret_manager)
   * for more information.
   *
   * If `credentials_secret` is specified, do not specify
   * [azure_credentials][google.storagetransfer.v1.AzureBlobStorageData.azure_credentials].
   *
   * Format: `projects/{project_number}/secrets/{secret_name}`
   */
  credentialsSecret: string;
}

/**
 * An HttpData resource specifies a list of objects on the web to be transferred
 * over HTTP.  The information of the objects to be transferred is contained in
 * a file referenced by a URL. The first line in the file must be
 * `"TsvHttpData-1.0"`, which specifies the format of the file.  Subsequent
 * lines specify the information of the list of objects, one object per list
 * entry. Each entry has the following tab-delimited fields:
 *
 * * **HTTP URL** â€” The location of the object.
 *
 * * **Length** â€” The size of the object in bytes.
 *
 * * **MD5** â€” The base64-encoded MD5 hash of the object.
 *
 * For an example of a valid TSV file, see
 * [Transferring data from
 * URLs](https://cloud.google.com/storage-transfer/docs/create-url-list).
 *
 * When transferring data based on a URL list, keep the following in mind:
 *
 * * When an object located at `http(s)://hostname:port/<URL-path>` is
 * transferred to a data sink, the name of the object at the data sink is
 * `<hostname>/<URL-path>`.
 *
 * * If the specified size of an object does not match the actual size of the
 * object fetched, the object is not transferred.
 *
 * * If the specified MD5 does not match the MD5 computed from the transferred
 * bytes, the object transfer fails.
 *
 * * Ensure that each URL you specify is publicly accessible. For
 * example, in Cloud Storage you can
 * [share an object publicly]
 * (/storage/docs/cloud-console#_sharingdata) and get a link to it.
 *
 * * Storage Transfer Service obeys `robots.txt` rules and requires the source
 * HTTP server to support `Range` requests and to return a `Content-Length`
 * header in each response.
 *
 * * [ObjectConditions][google.storagetransfer.v1.ObjectConditions] have no
 * effect when filtering objects to transfer.
 */
export interface HttpData {
  /**
   * Required. The URL that points to the file that stores the object list
   * entries. This file must allow public access.  Currently, only URLs with
   * HTTP and HTTPS schemes are supported.
   */
  listUrl: string;
}

/** A POSIX filesystem resource. */
export interface PosixFilesystem {
  /** Root directory path to the filesystem. */
  rootDirectory: string;
}

/**
 * An HdfsData resource specifies a path within an HDFS entity (e.g. a cluster).
 * All cluster-specific settings, such as namenodes and ports, are configured on
 * the transfer agents servicing requests, so HdfsData only contains the root
 * path to the data in our transfer.
 */
export interface HdfsData {
  /** Root path to transfer files. */
  path: string;
}

/** An AwsS3CompatibleData resource. */
export interface AwsS3CompatibleData {
  /** Required. Specifies the name of the bucket. */
  bucketName: string;
  /**
   * Specifies the root path to transfer objects.
   *
   * Must be an empty string or full path name that ends with a '/'. This
   * field is treated as an object prefix. As such, it should generally not
   * begin with a '/'.
   */
  path: string;
  /** Required. Specifies the endpoint of the storage service. */
  endpoint: string;
  /**
   * Specifies the region to sign requests with. This can be left blank if
   * requests should be signed with an empty region.
   */
  region: string;
  /** A S3 compatible metadata. */
  s3Metadata?: S3CompatibleMetadata | undefined;
}

/**
 * S3CompatibleMetadata contains the metadata fields that apply to the basic
 * types of S3-compatible data providers.
 */
export interface S3CompatibleMetadata {
  /**
   * Specifies the authentication and authorization method used by the storage
   * service. When not specified, Transfer Service will attempt to determine
   * right auth method to use.
   */
  authMethod: S3CompatibleMetadata_AuthMethod;
  /**
   * Specifies the API request model used to call the storage service. When not
   * specified, the default value of RequestModel
   * REQUEST_MODEL_VIRTUAL_HOSTED_STYLE is used.
   */
  requestModel: S3CompatibleMetadata_RequestModel;
  /**
   * Specifies the network protocol of the agent. When not specified, the
   * default value of NetworkProtocol NETWORK_PROTOCOL_HTTPS is used.
   */
  protocol: S3CompatibleMetadata_NetworkProtocol;
  /**
   * The Listing API to use for discovering objects. When not specified,
   * Transfer Service will attempt to determine the right API to use.
   */
  listApi: S3CompatibleMetadata_ListApi;
}

/** The authentication and authorization method used by the storage service. */
export enum S3CompatibleMetadata_AuthMethod {
  /** AUTH_METHOD_UNSPECIFIED - AuthMethod is not specified. */
  AUTH_METHOD_UNSPECIFIED = 0,
  /** AUTH_METHOD_AWS_SIGNATURE_V4 - Auth requests with AWS SigV4. */
  AUTH_METHOD_AWS_SIGNATURE_V4 = 1,
  /** AUTH_METHOD_AWS_SIGNATURE_V2 - Auth requests with AWS SigV2. */
  AUTH_METHOD_AWS_SIGNATURE_V2 = 2,
  UNRECOGNIZED = -1,
}

export function s3CompatibleMetadata_AuthMethodFromJSON(object: any): S3CompatibleMetadata_AuthMethod {
  switch (object) {
    case 0:
    case "AUTH_METHOD_UNSPECIFIED":
      return S3CompatibleMetadata_AuthMethod.AUTH_METHOD_UNSPECIFIED;
    case 1:
    case "AUTH_METHOD_AWS_SIGNATURE_V4":
      return S3CompatibleMetadata_AuthMethod.AUTH_METHOD_AWS_SIGNATURE_V4;
    case 2:
    case "AUTH_METHOD_AWS_SIGNATURE_V2":
      return S3CompatibleMetadata_AuthMethod.AUTH_METHOD_AWS_SIGNATURE_V2;
    case -1:
    case "UNRECOGNIZED":
    default:
      return S3CompatibleMetadata_AuthMethod.UNRECOGNIZED;
  }
}

export function s3CompatibleMetadata_AuthMethodToJSON(object: S3CompatibleMetadata_AuthMethod): string {
  switch (object) {
    case S3CompatibleMetadata_AuthMethod.AUTH_METHOD_UNSPECIFIED:
      return "AUTH_METHOD_UNSPECIFIED";
    case S3CompatibleMetadata_AuthMethod.AUTH_METHOD_AWS_SIGNATURE_V4:
      return "AUTH_METHOD_AWS_SIGNATURE_V4";
    case S3CompatibleMetadata_AuthMethod.AUTH_METHOD_AWS_SIGNATURE_V2:
      return "AUTH_METHOD_AWS_SIGNATURE_V2";
    case S3CompatibleMetadata_AuthMethod.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The request model of the API. */
export enum S3CompatibleMetadata_RequestModel {
  /** REQUEST_MODEL_UNSPECIFIED - RequestModel is not specified. */
  REQUEST_MODEL_UNSPECIFIED = 0,
  /**
   * REQUEST_MODEL_VIRTUAL_HOSTED_STYLE - Perform requests using Virtual Hosted Style.
   * Example: https://bucket-name.s3.region.amazonaws.com/key-name
   */
  REQUEST_MODEL_VIRTUAL_HOSTED_STYLE = 1,
  /**
   * REQUEST_MODEL_PATH_STYLE - Perform requests using Path Style.
   * Example: https://s3.region.amazonaws.com/bucket-name/key-name
   */
  REQUEST_MODEL_PATH_STYLE = 2,
  UNRECOGNIZED = -1,
}

export function s3CompatibleMetadata_RequestModelFromJSON(object: any): S3CompatibleMetadata_RequestModel {
  switch (object) {
    case 0:
    case "REQUEST_MODEL_UNSPECIFIED":
      return S3CompatibleMetadata_RequestModel.REQUEST_MODEL_UNSPECIFIED;
    case 1:
    case "REQUEST_MODEL_VIRTUAL_HOSTED_STYLE":
      return S3CompatibleMetadata_RequestModel.REQUEST_MODEL_VIRTUAL_HOSTED_STYLE;
    case 2:
    case "REQUEST_MODEL_PATH_STYLE":
      return S3CompatibleMetadata_RequestModel.REQUEST_MODEL_PATH_STYLE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return S3CompatibleMetadata_RequestModel.UNRECOGNIZED;
  }
}

export function s3CompatibleMetadata_RequestModelToJSON(object: S3CompatibleMetadata_RequestModel): string {
  switch (object) {
    case S3CompatibleMetadata_RequestModel.REQUEST_MODEL_UNSPECIFIED:
      return "REQUEST_MODEL_UNSPECIFIED";
    case S3CompatibleMetadata_RequestModel.REQUEST_MODEL_VIRTUAL_HOSTED_STYLE:
      return "REQUEST_MODEL_VIRTUAL_HOSTED_STYLE";
    case S3CompatibleMetadata_RequestModel.REQUEST_MODEL_PATH_STYLE:
      return "REQUEST_MODEL_PATH_STYLE";
    case S3CompatibleMetadata_RequestModel.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The agent network protocol to access the storage service. */
export enum S3CompatibleMetadata_NetworkProtocol {
  /** NETWORK_PROTOCOL_UNSPECIFIED - NetworkProtocol is not specified. */
  NETWORK_PROTOCOL_UNSPECIFIED = 0,
  /** NETWORK_PROTOCOL_HTTPS - Perform requests using HTTPS. */
  NETWORK_PROTOCOL_HTTPS = 1,
  /**
   * NETWORK_PROTOCOL_HTTP - Not recommended: This sends data in clear-text. This is only
   * appropriate within a closed network or for publicly available data.
   * Perform requests using HTTP.
   */
  NETWORK_PROTOCOL_HTTP = 2,
  UNRECOGNIZED = -1,
}

export function s3CompatibleMetadata_NetworkProtocolFromJSON(object: any): S3CompatibleMetadata_NetworkProtocol {
  switch (object) {
    case 0:
    case "NETWORK_PROTOCOL_UNSPECIFIED":
      return S3CompatibleMetadata_NetworkProtocol.NETWORK_PROTOCOL_UNSPECIFIED;
    case 1:
    case "NETWORK_PROTOCOL_HTTPS":
      return S3CompatibleMetadata_NetworkProtocol.NETWORK_PROTOCOL_HTTPS;
    case 2:
    case "NETWORK_PROTOCOL_HTTP":
      return S3CompatibleMetadata_NetworkProtocol.NETWORK_PROTOCOL_HTTP;
    case -1:
    case "UNRECOGNIZED":
    default:
      return S3CompatibleMetadata_NetworkProtocol.UNRECOGNIZED;
  }
}

export function s3CompatibleMetadata_NetworkProtocolToJSON(object: S3CompatibleMetadata_NetworkProtocol): string {
  switch (object) {
    case S3CompatibleMetadata_NetworkProtocol.NETWORK_PROTOCOL_UNSPECIFIED:
      return "NETWORK_PROTOCOL_UNSPECIFIED";
    case S3CompatibleMetadata_NetworkProtocol.NETWORK_PROTOCOL_HTTPS:
      return "NETWORK_PROTOCOL_HTTPS";
    case S3CompatibleMetadata_NetworkProtocol.NETWORK_PROTOCOL_HTTP:
      return "NETWORK_PROTOCOL_HTTP";
    case S3CompatibleMetadata_NetworkProtocol.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The Listing API to use for discovering objects. */
export enum S3CompatibleMetadata_ListApi {
  /** LIST_API_UNSPECIFIED - ListApi is not specified. */
  LIST_API_UNSPECIFIED = 0,
  /** LIST_OBJECTS_V2 - Perform listing using ListObjectsV2 API. */
  LIST_OBJECTS_V2 = 1,
  /** LIST_OBJECTS - Legacy ListObjects API. */
  LIST_OBJECTS = 2,
  UNRECOGNIZED = -1,
}

export function s3CompatibleMetadata_ListApiFromJSON(object: any): S3CompatibleMetadata_ListApi {
  switch (object) {
    case 0:
    case "LIST_API_UNSPECIFIED":
      return S3CompatibleMetadata_ListApi.LIST_API_UNSPECIFIED;
    case 1:
    case "LIST_OBJECTS_V2":
      return S3CompatibleMetadata_ListApi.LIST_OBJECTS_V2;
    case 2:
    case "LIST_OBJECTS":
      return S3CompatibleMetadata_ListApi.LIST_OBJECTS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return S3CompatibleMetadata_ListApi.UNRECOGNIZED;
  }
}

export function s3CompatibleMetadata_ListApiToJSON(object: S3CompatibleMetadata_ListApi): string {
  switch (object) {
    case S3CompatibleMetadata_ListApi.LIST_API_UNSPECIFIED:
      return "LIST_API_UNSPECIFIED";
    case S3CompatibleMetadata_ListApi.LIST_OBJECTS_V2:
      return "LIST_OBJECTS_V2";
    case S3CompatibleMetadata_ListApi.LIST_OBJECTS:
      return "LIST_OBJECTS";
    case S3CompatibleMetadata_ListApi.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Represents an agent pool. */
export interface AgentPool {
  /**
   * Required. Specifies a unique string that identifies the agent pool.
   *
   * Format: `projects/{project_id}/agentPools/{agent_pool_id}`
   */
  name: string;
  /** Specifies the client-specified AgentPool description. */
  displayName: string;
  /** Output only. Specifies the state of the AgentPool. */
  state: AgentPool_State;
  /**
   * Specifies the bandwidth limit details. If this field is unspecified, the
   * default value is set as 'No Limit'.
   */
  bandwidthLimit: AgentPool_BandwidthLimit | undefined;
}

/** The state of an AgentPool. */
export enum AgentPool_State {
  /** STATE_UNSPECIFIED - Default value. This value is unused. */
  STATE_UNSPECIFIED = 0,
  /**
   * CREATING - This is an initialization state. During this stage, resources are
   * allocated for the AgentPool.
   */
  CREATING = 1,
  /**
   * CREATED - Determines that the AgentPool is created for use. At this state, Agents
   * can join the AgentPool and participate in the transfer jobs in that pool.
   */
  CREATED = 2,
  /**
   * DELETING - Determines that the AgentPool deletion has been initiated, and all the
   * resources are scheduled to be cleaned up and freed.
   */
  DELETING = 3,
  UNRECOGNIZED = -1,
}

export function agentPool_StateFromJSON(object: any): AgentPool_State {
  switch (object) {
    case 0:
    case "STATE_UNSPECIFIED":
      return AgentPool_State.STATE_UNSPECIFIED;
    case 1:
    case "CREATING":
      return AgentPool_State.CREATING;
    case 2:
    case "CREATED":
      return AgentPool_State.CREATED;
    case 3:
    case "DELETING":
      return AgentPool_State.DELETING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AgentPool_State.UNRECOGNIZED;
  }
}

export function agentPool_StateToJSON(object: AgentPool_State): string {
  switch (object) {
    case AgentPool_State.STATE_UNSPECIFIED:
      return "STATE_UNSPECIFIED";
    case AgentPool_State.CREATING:
      return "CREATING";
    case AgentPool_State.CREATED:
      return "CREATED";
    case AgentPool_State.DELETING:
      return "DELETING";
    case AgentPool_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Specifies a bandwidth limit for an agent pool. */
export interface AgentPool_BandwidthLimit {
  /**
   * Bandwidth rate in megabytes per second, distributed across all the agents
   * in the pool.
   */
  limitMbps: Long;
}

/** TransferOptions define the actions to be performed on objects in a transfer. */
export interface TransferOptions {
  /**
   * When to overwrite objects that already exist in the sink. The default is
   * that only objects that are different from the source are ovewritten. If
   * true, all objects in the sink whose name matches an object in the source
   * are overwritten with the source object.
   */
  overwriteObjectsAlreadyExistingInSink: boolean;
  /**
   * Whether objects that exist only in the sink should be deleted.
   *
   * **Note:** This option and
   * [delete_objects_from_source_after_transfer][google.storagetransfer.v1.TransferOptions.delete_objects_from_source_after_transfer]
   * are mutually exclusive.
   */
  deleteObjectsUniqueInSink: boolean;
  /**
   * Whether objects should be deleted from the source after they are
   * transferred to the sink.
   *
   * **Note:** This option and
   * [delete_objects_unique_in_sink][google.storagetransfer.v1.TransferOptions.delete_objects_unique_in_sink]
   * are mutually exclusive.
   */
  deleteObjectsFromSourceAfterTransfer: boolean;
  /**
   * When to overwrite objects that already exist in the sink. If not set,
   * overwrite behavior is determined by
   * [overwrite_objects_already_existing_in_sink][google.storagetransfer.v1.TransferOptions.overwrite_objects_already_existing_in_sink].
   */
  overwriteWhen: TransferOptions_OverwriteWhen;
  /** Represents the selected metadata options for a transfer job. */
  metadataOptions: MetadataOptions | undefined;
}

/**
 * Specifies when to overwrite an object in the sink when an object with
 * matching name is found in the source.
 */
export enum TransferOptions_OverwriteWhen {
  /** OVERWRITE_WHEN_UNSPECIFIED - Overwrite behavior is unspecified. */
  OVERWRITE_WHEN_UNSPECIFIED = 0,
  /**
   * DIFFERENT - Overwrites destination objects with the source objects, only if the
   * objects have the same name but different HTTP ETags or checksum values.
   */
  DIFFERENT = 1,
  /**
   * NEVER - Never overwrites a destination object if a source object has the
   * same name. In this case, the source object is not transferred.
   */
  NEVER = 2,
  /**
   * ALWAYS - Always overwrite the destination object with the source object, even if
   * the HTTP Etags or checksum values are the same.
   */
  ALWAYS = 3,
  UNRECOGNIZED = -1,
}

export function transferOptions_OverwriteWhenFromJSON(object: any): TransferOptions_OverwriteWhen {
  switch (object) {
    case 0:
    case "OVERWRITE_WHEN_UNSPECIFIED":
      return TransferOptions_OverwriteWhen.OVERWRITE_WHEN_UNSPECIFIED;
    case 1:
    case "DIFFERENT":
      return TransferOptions_OverwriteWhen.DIFFERENT;
    case 2:
    case "NEVER":
      return TransferOptions_OverwriteWhen.NEVER;
    case 3:
    case "ALWAYS":
      return TransferOptions_OverwriteWhen.ALWAYS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return TransferOptions_OverwriteWhen.UNRECOGNIZED;
  }
}

export function transferOptions_OverwriteWhenToJSON(object: TransferOptions_OverwriteWhen): string {
  switch (object) {
    case TransferOptions_OverwriteWhen.OVERWRITE_WHEN_UNSPECIFIED:
      return "OVERWRITE_WHEN_UNSPECIFIED";
    case TransferOptions_OverwriteWhen.DIFFERENT:
      return "DIFFERENT";
    case TransferOptions_OverwriteWhen.NEVER:
      return "NEVER";
    case TransferOptions_OverwriteWhen.ALWAYS:
      return "ALWAYS";
    case TransferOptions_OverwriteWhen.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Configuration for running a transfer. */
export interface TransferSpec {
  /** A Cloud Storage data sink. */
  gcsDataSink?:
    | GcsData
    | undefined;
  /** A POSIX Filesystem data sink. */
  posixDataSink?:
    | PosixFilesystem
    | undefined;
  /** A Cloud Storage data source. */
  gcsDataSource?:
    | GcsData
    | undefined;
  /** An AWS S3 data source. */
  awsS3DataSource?:
    | AwsS3Data
    | undefined;
  /** An HTTP URL data source. */
  httpDataSource?:
    | HttpData
    | undefined;
  /** A POSIX Filesystem data source. */
  posixDataSource?:
    | PosixFilesystem
    | undefined;
  /** An Azure Blob Storage data source. */
  azureBlobStorageDataSource?:
    | AzureBlobStorageData
    | undefined;
  /** An AWS S3 compatible data source. */
  awsS3CompatibleDataSource?:
    | AwsS3CompatibleData
    | undefined;
  /** An HDFS cluster data source. */
  hdfsDataSource?:
    | HdfsData
    | undefined;
  /**
   * For transfers between file systems, specifies a Cloud Storage bucket
   * to be used as an intermediate location through which to transfer data.
   *
   * See [Transfer data between file
   * systems](https://cloud.google.com/storage-transfer/docs/file-to-file) for
   * more information.
   */
  gcsIntermediateDataLocation?:
    | GcsData
    | undefined;
  /**
   * Only objects that satisfy these object conditions are included in the set
   * of data source and data sink objects.  Object conditions based on
   * objects' "last modification time" do not exclude objects in a data sink.
   */
  objectConditions:
    | ObjectConditions
    | undefined;
  /**
   * If the option
   * [delete_objects_unique_in_sink][google.storagetransfer.v1.TransferOptions.delete_objects_unique_in_sink]
   * is `true` and time-based object conditions such as 'last modification time'
   * are specified, the request fails with an
   * [INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT] error.
   */
  transferOptions:
    | TransferOptions
    | undefined;
  /**
   * A manifest file provides a list of objects to be transferred from the data
   * source. This field points to the location of the manifest file.
   * Otherwise, the entire source bucket is used. ObjectConditions still apply.
   */
  transferManifest:
    | TransferManifest
    | undefined;
  /**
   * Specifies the agent pool name associated with the posix data source. When
   * unspecified, the default name is used.
   */
  sourceAgentPoolName: string;
  /**
   * Specifies the agent pool name associated with the posix data sink. When
   * unspecified, the default name is used.
   */
  sinkAgentPoolName: string;
}

/** Specifies the metadata options for running a transfer. */
export interface MetadataOptions {
  /**
   * Specifies how symlinks should be handled by the transfer. By default,
   * symlinks are not preserved. Only applicable to transfers involving
   * POSIX file systems, and ignored for other transfers.
   */
  symlink: MetadataOptions_Symlink;
  /**
   * Specifies how each file's mode attribute should be handled by the transfer.
   * By default, mode is not preserved. Only applicable to transfers involving
   * POSIX file systems, and ignored for other transfers.
   */
  mode: MetadataOptions_Mode;
  /**
   * Specifies how each file's POSIX group ID (GID) attribute should be handled
   * by the transfer. By default, GID is not preserved. Only applicable to
   * transfers involving POSIX file systems, and ignored for other transfers.
   */
  gid: MetadataOptions_GID;
  /**
   * Specifies how each file's POSIX user ID (UID) attribute should be handled
   * by the transfer. By default, UID is not preserved. Only applicable to
   * transfers involving POSIX file systems, and ignored for other transfers.
   */
  uid: MetadataOptions_UID;
  /**
   * Specifies how each object's ACLs should be preserved for transfers between
   * Google Cloud Storage buckets. If unspecified, the default behavior is the
   * same as ACL_DESTINATION_BUCKET_DEFAULT.
   */
  acl: MetadataOptions_Acl;
  /**
   * Specifies the storage class to set on objects being transferred to Google
   * Cloud Storage buckets.  If unspecified, the default behavior is the same as
   * [STORAGE_CLASS_DESTINATION_BUCKET_DEFAULT][google.storagetransfer.v1.MetadataOptions.StorageClass.STORAGE_CLASS_DESTINATION_BUCKET_DEFAULT].
   */
  storageClass: MetadataOptions_StorageClass;
  /**
   * Specifies how each object's temporary hold status should be preserved for
   * transfers between Google Cloud Storage buckets.  If unspecified, the
   * default behavior is the same as
   * [TEMPORARY_HOLD_PRESERVE][google.storagetransfer.v1.MetadataOptions.TemporaryHold.TEMPORARY_HOLD_PRESERVE].
   */
  temporaryHold: MetadataOptions_TemporaryHold;
  /**
   * Specifies how each object's Cloud KMS customer-managed encryption key
   * (CMEK) is preserved for transfers between Google Cloud Storage buckets.  If
   * unspecified, the default behavior is the same as
   * [KMS_KEY_DESTINATION_BUCKET_DEFAULT][google.storagetransfer.v1.MetadataOptions.KmsKey.KMS_KEY_DESTINATION_BUCKET_DEFAULT].
   */
  kmsKey: MetadataOptions_KmsKey;
  /**
   * Specifies how each object's `timeCreated` metadata is preserved for
   * transfers. If unspecified, the default behavior is the same as
   * [TIME_CREATED_SKIP][google.storagetransfer.v1.MetadataOptions.TimeCreated.TIME_CREATED_SKIP].
   * This behavior is supported for transfers to Cloud Storage buckets from
   * Cloud Storage, Amazon S3, S3-compatible storage, and Azure sources.
   */
  timeCreated: MetadataOptions_TimeCreated;
}

/** Whether symlinks should be skipped or preserved during a transfer job. */
export enum MetadataOptions_Symlink {
  /** SYMLINK_UNSPECIFIED - Symlink behavior is unspecified. */
  SYMLINK_UNSPECIFIED = 0,
  /** SYMLINK_SKIP - Do not preserve symlinks during a transfer job. */
  SYMLINK_SKIP = 1,
  /** SYMLINK_PRESERVE - Preserve symlinks during a transfer job. */
  SYMLINK_PRESERVE = 2,
  UNRECOGNIZED = -1,
}

export function metadataOptions_SymlinkFromJSON(object: any): MetadataOptions_Symlink {
  switch (object) {
    case 0:
    case "SYMLINK_UNSPECIFIED":
      return MetadataOptions_Symlink.SYMLINK_UNSPECIFIED;
    case 1:
    case "SYMLINK_SKIP":
      return MetadataOptions_Symlink.SYMLINK_SKIP;
    case 2:
    case "SYMLINK_PRESERVE":
      return MetadataOptions_Symlink.SYMLINK_PRESERVE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return MetadataOptions_Symlink.UNRECOGNIZED;
  }
}

export function metadataOptions_SymlinkToJSON(object: MetadataOptions_Symlink): string {
  switch (object) {
    case MetadataOptions_Symlink.SYMLINK_UNSPECIFIED:
      return "SYMLINK_UNSPECIFIED";
    case MetadataOptions_Symlink.SYMLINK_SKIP:
      return "SYMLINK_SKIP";
    case MetadataOptions_Symlink.SYMLINK_PRESERVE:
      return "SYMLINK_PRESERVE";
    case MetadataOptions_Symlink.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Options for handling file mode attribute. */
export enum MetadataOptions_Mode {
  /** MODE_UNSPECIFIED - Mode behavior is unspecified. */
  MODE_UNSPECIFIED = 0,
  /** MODE_SKIP - Do not preserve mode during a transfer job. */
  MODE_SKIP = 1,
  /** MODE_PRESERVE - Preserve mode during a transfer job. */
  MODE_PRESERVE = 2,
  UNRECOGNIZED = -1,
}

export function metadataOptions_ModeFromJSON(object: any): MetadataOptions_Mode {
  switch (object) {
    case 0:
    case "MODE_UNSPECIFIED":
      return MetadataOptions_Mode.MODE_UNSPECIFIED;
    case 1:
    case "MODE_SKIP":
      return MetadataOptions_Mode.MODE_SKIP;
    case 2:
    case "MODE_PRESERVE":
      return MetadataOptions_Mode.MODE_PRESERVE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return MetadataOptions_Mode.UNRECOGNIZED;
  }
}

export function metadataOptions_ModeToJSON(object: MetadataOptions_Mode): string {
  switch (object) {
    case MetadataOptions_Mode.MODE_UNSPECIFIED:
      return "MODE_UNSPECIFIED";
    case MetadataOptions_Mode.MODE_SKIP:
      return "MODE_SKIP";
    case MetadataOptions_Mode.MODE_PRESERVE:
      return "MODE_PRESERVE";
    case MetadataOptions_Mode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Options for handling file GID attribute. */
export enum MetadataOptions_GID {
  /** GID_UNSPECIFIED - GID behavior is unspecified. */
  GID_UNSPECIFIED = 0,
  /** GID_SKIP - Do not preserve GID during a transfer job. */
  GID_SKIP = 1,
  /** GID_NUMBER - Preserve GID during a transfer job. */
  GID_NUMBER = 2,
  UNRECOGNIZED = -1,
}

export function metadataOptions_GIDFromJSON(object: any): MetadataOptions_GID {
  switch (object) {
    case 0:
    case "GID_UNSPECIFIED":
      return MetadataOptions_GID.GID_UNSPECIFIED;
    case 1:
    case "GID_SKIP":
      return MetadataOptions_GID.GID_SKIP;
    case 2:
    case "GID_NUMBER":
      return MetadataOptions_GID.GID_NUMBER;
    case -1:
    case "UNRECOGNIZED":
    default:
      return MetadataOptions_GID.UNRECOGNIZED;
  }
}

export function metadataOptions_GIDToJSON(object: MetadataOptions_GID): string {
  switch (object) {
    case MetadataOptions_GID.GID_UNSPECIFIED:
      return "GID_UNSPECIFIED";
    case MetadataOptions_GID.GID_SKIP:
      return "GID_SKIP";
    case MetadataOptions_GID.GID_NUMBER:
      return "GID_NUMBER";
    case MetadataOptions_GID.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Options for handling file UID attribute. */
export enum MetadataOptions_UID {
  /** UID_UNSPECIFIED - UID behavior is unspecified. */
  UID_UNSPECIFIED = 0,
  /** UID_SKIP - Do not preserve UID during a transfer job. */
  UID_SKIP = 1,
  /** UID_NUMBER - Preserve UID during a transfer job. */
  UID_NUMBER = 2,
  UNRECOGNIZED = -1,
}

export function metadataOptions_UIDFromJSON(object: any): MetadataOptions_UID {
  switch (object) {
    case 0:
    case "UID_UNSPECIFIED":
      return MetadataOptions_UID.UID_UNSPECIFIED;
    case 1:
    case "UID_SKIP":
      return MetadataOptions_UID.UID_SKIP;
    case 2:
    case "UID_NUMBER":
      return MetadataOptions_UID.UID_NUMBER;
    case -1:
    case "UNRECOGNIZED":
    default:
      return MetadataOptions_UID.UNRECOGNIZED;
  }
}

export function metadataOptions_UIDToJSON(object: MetadataOptions_UID): string {
  switch (object) {
    case MetadataOptions_UID.UID_UNSPECIFIED:
      return "UID_UNSPECIFIED";
    case MetadataOptions_UID.UID_SKIP:
      return "UID_SKIP";
    case MetadataOptions_UID.UID_NUMBER:
      return "UID_NUMBER";
    case MetadataOptions_UID.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Options for handling Cloud Storage object ACLs. */
export enum MetadataOptions_Acl {
  /** ACL_UNSPECIFIED - ACL behavior is unspecified. */
  ACL_UNSPECIFIED = 0,
  /** ACL_DESTINATION_BUCKET_DEFAULT - Use the destination bucket's default object ACLS, if applicable. */
  ACL_DESTINATION_BUCKET_DEFAULT = 1,
  /**
   * ACL_PRESERVE - Preserve the object's original ACLs. This requires the service account
   * to have `storage.objects.getIamPolicy` permission for the source object.
   * [Uniform bucket-level
   * access](https://cloud.google.com/storage/docs/uniform-bucket-level-access)
   * must not be enabled on either the source or destination buckets.
   */
  ACL_PRESERVE = 2,
  UNRECOGNIZED = -1,
}

export function metadataOptions_AclFromJSON(object: any): MetadataOptions_Acl {
  switch (object) {
    case 0:
    case "ACL_UNSPECIFIED":
      return MetadataOptions_Acl.ACL_UNSPECIFIED;
    case 1:
    case "ACL_DESTINATION_BUCKET_DEFAULT":
      return MetadataOptions_Acl.ACL_DESTINATION_BUCKET_DEFAULT;
    case 2:
    case "ACL_PRESERVE":
      return MetadataOptions_Acl.ACL_PRESERVE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return MetadataOptions_Acl.UNRECOGNIZED;
  }
}

export function metadataOptions_AclToJSON(object: MetadataOptions_Acl): string {
  switch (object) {
    case MetadataOptions_Acl.ACL_UNSPECIFIED:
      return "ACL_UNSPECIFIED";
    case MetadataOptions_Acl.ACL_DESTINATION_BUCKET_DEFAULT:
      return "ACL_DESTINATION_BUCKET_DEFAULT";
    case MetadataOptions_Acl.ACL_PRESERVE:
      return "ACL_PRESERVE";
    case MetadataOptions_Acl.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Options for handling Google Cloud Storage object storage class. */
export enum MetadataOptions_StorageClass {
  /** STORAGE_CLASS_UNSPECIFIED - Storage class behavior is unspecified. */
  STORAGE_CLASS_UNSPECIFIED = 0,
  /** STORAGE_CLASS_DESTINATION_BUCKET_DEFAULT - Use the destination bucket's default storage class. */
  STORAGE_CLASS_DESTINATION_BUCKET_DEFAULT = 1,
  /**
   * STORAGE_CLASS_PRESERVE - Preserve the object's original storage class. This is only supported for
   * transfers from Google Cloud Storage buckets. REGIONAL and MULTI_REGIONAL
   * storage classes will be mapped to STANDARD to ensure they can be written
   * to the destination bucket.
   */
  STORAGE_CLASS_PRESERVE = 2,
  /** STORAGE_CLASS_STANDARD - Set the storage class to STANDARD. */
  STORAGE_CLASS_STANDARD = 3,
  /** STORAGE_CLASS_NEARLINE - Set the storage class to NEARLINE. */
  STORAGE_CLASS_NEARLINE = 4,
  /** STORAGE_CLASS_COLDLINE - Set the storage class to COLDLINE. */
  STORAGE_CLASS_COLDLINE = 5,
  /** STORAGE_CLASS_ARCHIVE - Set the storage class to ARCHIVE. */
  STORAGE_CLASS_ARCHIVE = 6,
  UNRECOGNIZED = -1,
}

export function metadataOptions_StorageClassFromJSON(object: any): MetadataOptions_StorageClass {
  switch (object) {
    case 0:
    case "STORAGE_CLASS_UNSPECIFIED":
      return MetadataOptions_StorageClass.STORAGE_CLASS_UNSPECIFIED;
    case 1:
    case "STORAGE_CLASS_DESTINATION_BUCKET_DEFAULT":
      return MetadataOptions_StorageClass.STORAGE_CLASS_DESTINATION_BUCKET_DEFAULT;
    case 2:
    case "STORAGE_CLASS_PRESERVE":
      return MetadataOptions_StorageClass.STORAGE_CLASS_PRESERVE;
    case 3:
    case "STORAGE_CLASS_STANDARD":
      return MetadataOptions_StorageClass.STORAGE_CLASS_STANDARD;
    case 4:
    case "STORAGE_CLASS_NEARLINE":
      return MetadataOptions_StorageClass.STORAGE_CLASS_NEARLINE;
    case 5:
    case "STORAGE_CLASS_COLDLINE":
      return MetadataOptions_StorageClass.STORAGE_CLASS_COLDLINE;
    case 6:
    case "STORAGE_CLASS_ARCHIVE":
      return MetadataOptions_StorageClass.STORAGE_CLASS_ARCHIVE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return MetadataOptions_StorageClass.UNRECOGNIZED;
  }
}

export function metadataOptions_StorageClassToJSON(object: MetadataOptions_StorageClass): string {
  switch (object) {
    case MetadataOptions_StorageClass.STORAGE_CLASS_UNSPECIFIED:
      return "STORAGE_CLASS_UNSPECIFIED";
    case MetadataOptions_StorageClass.STORAGE_CLASS_DESTINATION_BUCKET_DEFAULT:
      return "STORAGE_CLASS_DESTINATION_BUCKET_DEFAULT";
    case MetadataOptions_StorageClass.STORAGE_CLASS_PRESERVE:
      return "STORAGE_CLASS_PRESERVE";
    case MetadataOptions_StorageClass.STORAGE_CLASS_STANDARD:
      return "STORAGE_CLASS_STANDARD";
    case MetadataOptions_StorageClass.STORAGE_CLASS_NEARLINE:
      return "STORAGE_CLASS_NEARLINE";
    case MetadataOptions_StorageClass.STORAGE_CLASS_COLDLINE:
      return "STORAGE_CLASS_COLDLINE";
    case MetadataOptions_StorageClass.STORAGE_CLASS_ARCHIVE:
      return "STORAGE_CLASS_ARCHIVE";
    case MetadataOptions_StorageClass.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Options for handling temporary holds for Google Cloud Storage objects. */
export enum MetadataOptions_TemporaryHold {
  /** TEMPORARY_HOLD_UNSPECIFIED - Temporary hold behavior is unspecified. */
  TEMPORARY_HOLD_UNSPECIFIED = 0,
  /** TEMPORARY_HOLD_SKIP - Do not set a temporary hold on the destination object. */
  TEMPORARY_HOLD_SKIP = 1,
  /** TEMPORARY_HOLD_PRESERVE - Preserve the object's original temporary hold status. */
  TEMPORARY_HOLD_PRESERVE = 2,
  UNRECOGNIZED = -1,
}

export function metadataOptions_TemporaryHoldFromJSON(object: any): MetadataOptions_TemporaryHold {
  switch (object) {
    case 0:
    case "TEMPORARY_HOLD_UNSPECIFIED":
      return MetadataOptions_TemporaryHold.TEMPORARY_HOLD_UNSPECIFIED;
    case 1:
    case "TEMPORARY_HOLD_SKIP":
      return MetadataOptions_TemporaryHold.TEMPORARY_HOLD_SKIP;
    case 2:
    case "TEMPORARY_HOLD_PRESERVE":
      return MetadataOptions_TemporaryHold.TEMPORARY_HOLD_PRESERVE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return MetadataOptions_TemporaryHold.UNRECOGNIZED;
  }
}

export function metadataOptions_TemporaryHoldToJSON(object: MetadataOptions_TemporaryHold): string {
  switch (object) {
    case MetadataOptions_TemporaryHold.TEMPORARY_HOLD_UNSPECIFIED:
      return "TEMPORARY_HOLD_UNSPECIFIED";
    case MetadataOptions_TemporaryHold.TEMPORARY_HOLD_SKIP:
      return "TEMPORARY_HOLD_SKIP";
    case MetadataOptions_TemporaryHold.TEMPORARY_HOLD_PRESERVE:
      return "TEMPORARY_HOLD_PRESERVE";
    case MetadataOptions_TemporaryHold.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Options for handling the KmsKey setting for Google Cloud Storage objects. */
export enum MetadataOptions_KmsKey {
  /** KMS_KEY_UNSPECIFIED - KmsKey behavior is unspecified. */
  KMS_KEY_UNSPECIFIED = 0,
  /** KMS_KEY_DESTINATION_BUCKET_DEFAULT - Use the destination bucket's default encryption settings. */
  KMS_KEY_DESTINATION_BUCKET_DEFAULT = 1,
  /**
   * KMS_KEY_PRESERVE - Preserve the object's original Cloud KMS customer-managed encryption key
   * (CMEK) if present. Objects that do not use a Cloud KMS encryption key
   * will be encrypted using the destination bucket's encryption settings.
   */
  KMS_KEY_PRESERVE = 2,
  UNRECOGNIZED = -1,
}

export function metadataOptions_KmsKeyFromJSON(object: any): MetadataOptions_KmsKey {
  switch (object) {
    case 0:
    case "KMS_KEY_UNSPECIFIED":
      return MetadataOptions_KmsKey.KMS_KEY_UNSPECIFIED;
    case 1:
    case "KMS_KEY_DESTINATION_BUCKET_DEFAULT":
      return MetadataOptions_KmsKey.KMS_KEY_DESTINATION_BUCKET_DEFAULT;
    case 2:
    case "KMS_KEY_PRESERVE":
      return MetadataOptions_KmsKey.KMS_KEY_PRESERVE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return MetadataOptions_KmsKey.UNRECOGNIZED;
  }
}

export function metadataOptions_KmsKeyToJSON(object: MetadataOptions_KmsKey): string {
  switch (object) {
    case MetadataOptions_KmsKey.KMS_KEY_UNSPECIFIED:
      return "KMS_KEY_UNSPECIFIED";
    case MetadataOptions_KmsKey.KMS_KEY_DESTINATION_BUCKET_DEFAULT:
      return "KMS_KEY_DESTINATION_BUCKET_DEFAULT";
    case MetadataOptions_KmsKey.KMS_KEY_PRESERVE:
      return "KMS_KEY_PRESERVE";
    case MetadataOptions_KmsKey.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Options for handling `timeCreated` metadata for Google Cloud Storage
 * objects.
 */
export enum MetadataOptions_TimeCreated {
  /** TIME_CREATED_UNSPECIFIED - TimeCreated behavior is unspecified. */
  TIME_CREATED_UNSPECIFIED = 0,
  /** TIME_CREATED_SKIP - Do not preserve the `timeCreated` metadata from the source object. */
  TIME_CREATED_SKIP = 1,
  /**
   * TIME_CREATED_PRESERVE_AS_CUSTOM_TIME - Preserves the source object's `timeCreated` or `lastModified` metadata in
   * the `customTime` field in the destination object.  Note that any value
   * stored in the source object's `customTime` field will not be propagated
   * to the destination object.
   */
  TIME_CREATED_PRESERVE_AS_CUSTOM_TIME = 2,
  UNRECOGNIZED = -1,
}

export function metadataOptions_TimeCreatedFromJSON(object: any): MetadataOptions_TimeCreated {
  switch (object) {
    case 0:
    case "TIME_CREATED_UNSPECIFIED":
      return MetadataOptions_TimeCreated.TIME_CREATED_UNSPECIFIED;
    case 1:
    case "TIME_CREATED_SKIP":
      return MetadataOptions_TimeCreated.TIME_CREATED_SKIP;
    case 2:
    case "TIME_CREATED_PRESERVE_AS_CUSTOM_TIME":
      return MetadataOptions_TimeCreated.TIME_CREATED_PRESERVE_AS_CUSTOM_TIME;
    case -1:
    case "UNRECOGNIZED":
    default:
      return MetadataOptions_TimeCreated.UNRECOGNIZED;
  }
}

export function metadataOptions_TimeCreatedToJSON(object: MetadataOptions_TimeCreated): string {
  switch (object) {
    case MetadataOptions_TimeCreated.TIME_CREATED_UNSPECIFIED:
      return "TIME_CREATED_UNSPECIFIED";
    case MetadataOptions_TimeCreated.TIME_CREATED_SKIP:
      return "TIME_CREATED_SKIP";
    case MetadataOptions_TimeCreated.TIME_CREATED_PRESERVE_AS_CUSTOM_TIME:
      return "TIME_CREATED_PRESERVE_AS_CUSTOM_TIME";
    case MetadataOptions_TimeCreated.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Specifies where the manifest is located. */
export interface TransferManifest {
  /**
   * Specifies the path to the manifest in Cloud Storage. The Google-managed
   * service account for the transfer must have `storage.objects.get`
   * permission for this object. An example path is
   * `gs://bucket_name/path/manifest.csv`.
   */
  location: string;
}

/** Transfers can be scheduled to recur or to run just once. */
export interface Schedule {
  /**
   * Required. The start date of a transfer. Date boundaries are determined
   * relative to UTC time. If `schedule_start_date` and
   * [start_time_of_day][google.storagetransfer.v1.Schedule.start_time_of_day]
   * are in the past relative to the job's creation time, the transfer starts
   * the day after you schedule the transfer request.
   *
   * **Note:** When starting jobs at or near midnight UTC it is possible that
   * a job starts later than expected. For example, if you send an outbound
   * request on June 1 one millisecond prior to midnight UTC and the Storage
   * Transfer Service server receives the request on June 2, then it creates
   * a TransferJob with `schedule_start_date` set to June 2 and a
   * `start_time_of_day` set to midnight UTC. The first scheduled
   * [TransferOperation][google.storagetransfer.v1.TransferOperation] takes
   * place on June 3 at midnight UTC.
   */
  scheduleStartDate:
    | DateMessage
    | undefined;
  /**
   * The last day a transfer runs. Date boundaries are determined relative to
   * UTC time. A job runs once per 24 hours within the following guidelines:
   *
   * *   If `schedule_end_date` and
   * [schedule_start_date][google.storagetransfer.v1.Schedule.schedule_start_date]
   * are the same and in
   *     the future relative to UTC, the transfer is executed only one time.
   * *   If `schedule_end_date` is later than `schedule_start_date`  and
   *     `schedule_end_date` is in the future relative to UTC, the job runs each
   *     day at
   *     [start_time_of_day][google.storagetransfer.v1.Schedule.start_time_of_day]
   *     through `schedule_end_date`.
   */
  scheduleEndDate:
    | DateMessage
    | undefined;
  /**
   * The time in UTC that a transfer job is scheduled to run. Transfers may
   * start later than this time.
   *
   * If `start_time_of_day` is not specified:
   *
   * *   One-time transfers run immediately.
   * *   Recurring transfers run immediately, and each day at midnight UTC,
   *     through
   *     [schedule_end_date][google.storagetransfer.v1.Schedule.schedule_end_date].
   *
   * If `start_time_of_day` is specified:
   *
   * *   One-time transfers run at the specified time.
   * *   Recurring transfers run at the specified time each day, through
   *     `schedule_end_date`.
   */
  startTimeOfDay:
    | TimeOfDay
    | undefined;
  /**
   * The time in UTC that no further transfer operations are scheduled. Combined
   * with
   * [schedule_end_date][google.storagetransfer.v1.Schedule.schedule_end_date],
   * `end_time_of_day` specifies the end date and time for starting new transfer
   * operations. This field must be greater than or equal to the timestamp
   * corresponding to the combintation of
   * [schedule_start_date][google.storagetransfer.v1.Schedule.schedule_start_date]
   * and
   * [start_time_of_day][google.storagetransfer.v1.Schedule.start_time_of_day],
   * and is subject to the following:
   *
   * *   If `end_time_of_day` is not set and `schedule_end_date` is set, then
   *     a default value of `23:59:59` is used for `end_time_of_day`.
   *
   * *   If `end_time_of_day` is set and `schedule_end_date` is not set, then
   *     [INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT] is returned.
   */
  endTimeOfDay:
    | TimeOfDay
    | undefined;
  /**
   * Interval between the start of each scheduled TransferOperation. If
   * unspecified, the default value is 24 hours. This value may not be less than
   * 1 hour.
   */
  repeatInterval: Duration | undefined;
}

/**
 * Specifies the Event-driven transfer options. Event-driven transfers listen to
 * an event stream to transfer updated files.
 */
export interface EventStream {
  /**
   * Required. Specifies a unique name of the resource such as AWS SQS
   * ARN in the form 'arn:aws:sqs:region:account_id:queue_name',
   * or Pub/Sub subscription resource name in the form
   * 'projects/{project}/subscriptions/{sub}'.
   */
  name: string;
  /**
   * Specifies the date and time that Storage Transfer Service starts
   * listening for events from this stream. If no start time is specified or
   * start time is in the past, Storage Transfer Service starts listening
   * immediately.
   */
  eventStreamStartTime:
    | Date
    | undefined;
  /**
   * Specifies the data and time at which Storage Transfer Service stops
   * listening for events from this stream. After this time, any transfers in
   * progress will complete, but no new transfers are initiated.
   */
  eventStreamExpirationTime: Date | undefined;
}

/**
 * This resource represents the configuration of a transfer job that runs
 * periodically.
 */
export interface TransferJob {
  /**
   * A unique name (within the transfer project) assigned when the job is
   * created.  If this field is empty in a CreateTransferJobRequest, Storage
   * Transfer Service assigns a unique name. Otherwise, the specified name
   * is used as the unique name for this job.
   *
   * If the specified name is in use by a job, the creation request fails with
   * an [ALREADY_EXISTS][google.rpc.Code.ALREADY_EXISTS] error.
   *
   * This name must start with `"transferJobs/"` prefix and end with a letter or
   * a number, and should be no more than 128 characters. For transfers
   * involving PosixFilesystem, this name must start with `transferJobs/OPI`
   * specifically. For all other transfer types, this name must not start with
   * `transferJobs/OPI`.
   *
   * Non-PosixFilesystem example:
   * `"transferJobs/^(?!OPI)[A-Za-z0-9-._~]*[A-Za-z0-9]$"`
   *
   * PosixFilesystem example:
   * `"transferJobs/OPI^[A-Za-z0-9-._~]*[A-Za-z0-9]$"`
   *
   * Applications must not rely on the enforcement of naming requirements
   * involving OPI.
   *
   * Invalid job names fail with an
   * [INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT] error.
   */
  name: string;
  /**
   * A description provided by the user for the job. Its max length is 1024
   * bytes when Unicode-encoded.
   */
  description: string;
  /** The ID of the Google Cloud project that owns the job. */
  projectId: string;
  /** Transfer specification. */
  transferSpec:
    | TransferSpec
    | undefined;
  /** Notification configuration. */
  notificationConfig:
    | NotificationConfig
    | undefined;
  /** Logging configuration. */
  loggingConfig:
    | LoggingConfig
    | undefined;
  /**
   * Specifies schedule for the transfer job.
   * This is an optional field. When the field is not set, the job never
   * executes a transfer, unless you invoke RunTransferJob or update the job to
   * have a non-empty schedule.
   */
  schedule:
    | Schedule
    | undefined;
  /**
   * Specifies the event stream for the transfer job for event-driven transfers.
   * When EventStream is specified, the Schedule fields are ignored.
   */
  eventStream:
    | EventStream
    | undefined;
  /**
   * Status of the job. This value MUST be specified for
   * `CreateTransferJobRequests`.
   *
   * **Note:** The effect of the new job status takes place during a subsequent
   * job run. For example, if you change the job status from
   * [ENABLED][google.storagetransfer.v1.TransferJob.Status.ENABLED] to
   * [DISABLED][google.storagetransfer.v1.TransferJob.Status.DISABLED], and an
   * operation spawned by the transfer is running, the status change would not
   * affect the current operation.
   */
  status: TransferJob_Status;
  /** Output only. The time that the transfer job was created. */
  creationTime:
    | Date
    | undefined;
  /** Output only. The time that the transfer job was last modified. */
  lastModificationTime:
    | Date
    | undefined;
  /** Output only. The time that the transfer job was deleted. */
  deletionTime:
    | Date
    | undefined;
  /**
   * The name of the most recently started TransferOperation of this JobConfig.
   * Present if a TransferOperation has been created for this JobConfig.
   */
  latestOperationName: string;
}

/** The status of the transfer job. */
export enum TransferJob_Status {
  /** STATUS_UNSPECIFIED - Zero is an illegal value. */
  STATUS_UNSPECIFIED = 0,
  /** ENABLED - New transfers are performed based on the schedule. */
  ENABLED = 1,
  /** DISABLED - New transfers are not scheduled. */
  DISABLED = 2,
  /**
   * DELETED - This is a soft delete state. After a transfer job is set to this
   * state, the job and all the transfer executions are subject to
   * garbage collection. Transfer jobs become eligible for garbage collection
   * 30 days after their status is set to `DELETED`.
   */
  DELETED = 3,
  UNRECOGNIZED = -1,
}

export function transferJob_StatusFromJSON(object: any): TransferJob_Status {
  switch (object) {
    case 0:
    case "STATUS_UNSPECIFIED":
      return TransferJob_Status.STATUS_UNSPECIFIED;
    case 1:
    case "ENABLED":
      return TransferJob_Status.ENABLED;
    case 2:
    case "DISABLED":
      return TransferJob_Status.DISABLED;
    case 3:
    case "DELETED":
      return TransferJob_Status.DELETED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return TransferJob_Status.UNRECOGNIZED;
  }
}

export function transferJob_StatusToJSON(object: TransferJob_Status): string {
  switch (object) {
    case TransferJob_Status.STATUS_UNSPECIFIED:
      return "STATUS_UNSPECIFIED";
    case TransferJob_Status.ENABLED:
      return "ENABLED";
    case TransferJob_Status.DISABLED:
      return "DISABLED";
    case TransferJob_Status.DELETED:
      return "DELETED";
    case TransferJob_Status.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** An entry describing an error that has occurred. */
export interface ErrorLogEntry {
  /**
   * Required. A URL that refers to the target (a data source, a data sink,
   * or an object) with which the error is associated.
   */
  url: string;
  /** A list of messages that carry the error details. */
  errorDetails: string[];
}

/**
 * A summary of errors by error code, plus a count and sample error log
 * entries.
 */
export interface ErrorSummary {
  /** Required. */
  errorCode: Code;
  /** Required. Count of this type of error. */
  errorCount: Long;
  /**
   * Error samples.
   *
   * At most 5 error log entries are recorded for a given
   * error code for a single transfer operation.
   */
  errorLogEntries: ErrorLogEntry[];
}

/** A collection of counters that report the progress of a transfer operation. */
export interface TransferCounters {
  /**
   * Objects found in the data source that are scheduled to be transferred,
   * excluding any that are filtered based on object conditions or skipped due
   * to sync.
   */
  objectsFoundFromSource: Long;
  /**
   * Bytes found in the data source that are scheduled to be transferred,
   * excluding any that are filtered based on object conditions or skipped due
   * to sync.
   */
  bytesFoundFromSource: Long;
  /** Objects found only in the data sink that are scheduled to be deleted. */
  objectsFoundOnlyFromSink: Long;
  /** Bytes found only in the data sink that are scheduled to be deleted. */
  bytesFoundOnlyFromSink: Long;
  /**
   * Objects in the data source that are not transferred because they already
   * exist in the data sink.
   */
  objectsFromSourceSkippedBySync: Long;
  /**
   * Bytes in the data source that are not transferred because they already
   * exist in the data sink.
   */
  bytesFromSourceSkippedBySync: Long;
  /** Objects that are copied to the data sink. */
  objectsCopiedToSink: Long;
  /** Bytes that are copied to the data sink. */
  bytesCopiedToSink: Long;
  /** Objects that are deleted from the data source. */
  objectsDeletedFromSource: Long;
  /** Bytes that are deleted from the data source. */
  bytesDeletedFromSource: Long;
  /** Objects that are deleted from the data sink. */
  objectsDeletedFromSink: Long;
  /** Bytes that are deleted from the data sink. */
  bytesDeletedFromSink: Long;
  /**
   * Objects in the data source that failed to be transferred or that failed
   * to be deleted after being transferred.
   */
  objectsFromSourceFailed: Long;
  /**
   * Bytes in the data source that failed to be transferred or that failed to
   * be deleted after being transferred.
   */
  bytesFromSourceFailed: Long;
  /** Objects that failed to be deleted from the data sink. */
  objectsFailedToDeleteFromSink: Long;
  /** Bytes that failed to be deleted from the data sink. */
  bytesFailedToDeleteFromSink: Long;
  /**
   * For transfers involving PosixFilesystem only.
   *
   * Number of directories found while listing. For example, if the root
   * directory of the transfer is `base/` and there are two other directories,
   * `a/` and `b/` under this directory, the count after listing `base/`,
   * `base/a/` and `base/b/` is 3.
   */
  directoriesFoundFromSource: Long;
  /**
   * For transfers involving PosixFilesystem only.
   *
   * Number of listing failures for each directory found at the source.
   * Potential failures when listing a directory include permission failure or
   * block failure. If listing a directory fails, no files in the directory are
   * transferred.
   */
  directoriesFailedToListFromSource: Long;
  /**
   * For transfers involving PosixFilesystem only.
   *
   * Number of successful listings for each directory found at the source.
   */
  directoriesSuccessfullyListedFromSource: Long;
  /** Number of successfully cleaned up intermediate objects. */
  intermediateObjectsCleanedUp: Long;
  /** Number of intermediate objects failed cleaned up. */
  intermediateObjectsFailedCleanedUp: Long;
}

/**
 * Specification to configure notifications published to Pub/Sub.
 * Notifications are published to the customer-provided topic using the
 * following `PubsubMessage.attributes`:
 *
 * * `"eventType"`: one of the
 * [EventType][google.storagetransfer.v1.NotificationConfig.EventType] values
 * * `"payloadFormat"`: one of the
 * [PayloadFormat][google.storagetransfer.v1.NotificationConfig.PayloadFormat]
 * values
 * * `"projectId"`: the
 * [project_id][google.storagetransfer.v1.TransferOperation.project_id] of the
 * `TransferOperation`
 * * `"transferJobName"`: the
 * [transfer_job_name][google.storagetransfer.v1.TransferOperation.transfer_job_name]
 * of the `TransferOperation`
 * * `"transferOperationName"`: the
 * [name][google.storagetransfer.v1.TransferOperation.name] of the
 * `TransferOperation`
 *
 * The `PubsubMessage.data` contains a
 * [TransferOperation][google.storagetransfer.v1.TransferOperation] resource
 * formatted according to the specified `PayloadFormat`.
 */
export interface NotificationConfig {
  /**
   * Required. The `Topic.name` of the Pub/Sub topic to which to publish
   * notifications. Must be of the format: `projects/{project}/topics/{topic}`.
   * Not matching this format results in an
   * [INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT] error.
   */
  pubsubTopic: string;
  /**
   * Event types for which a notification is desired. If empty, send
   * notifications for all event types.
   */
  eventTypes: NotificationConfig_EventType[];
  /** Required. The desired format of the notification message payloads. */
  payloadFormat: NotificationConfig_PayloadFormat;
}

/**
 * Enum for specifying event types for which notifications are to be
 * published.
 *
 * Additional event types may be added in the future. Clients should either
 * safely ignore unrecognized event types or explicitly specify which event
 * types they are prepared to accept.
 */
export enum NotificationConfig_EventType {
  /** EVENT_TYPE_UNSPECIFIED - Illegal value, to avoid allowing a default. */
  EVENT_TYPE_UNSPECIFIED = 0,
  /**
   * TRANSFER_OPERATION_SUCCESS - `TransferOperation` completed with status
   * [SUCCESS][google.storagetransfer.v1.TransferOperation.Status.SUCCESS].
   */
  TRANSFER_OPERATION_SUCCESS = 1,
  /**
   * TRANSFER_OPERATION_FAILED - `TransferOperation` completed with status
   * [FAILED][google.storagetransfer.v1.TransferOperation.Status.FAILED].
   */
  TRANSFER_OPERATION_FAILED = 2,
  /**
   * TRANSFER_OPERATION_ABORTED - `TransferOperation` completed with status
   * [ABORTED][google.storagetransfer.v1.TransferOperation.Status.ABORTED].
   */
  TRANSFER_OPERATION_ABORTED = 3,
  UNRECOGNIZED = -1,
}

export function notificationConfig_EventTypeFromJSON(object: any): NotificationConfig_EventType {
  switch (object) {
    case 0:
    case "EVENT_TYPE_UNSPECIFIED":
      return NotificationConfig_EventType.EVENT_TYPE_UNSPECIFIED;
    case 1:
    case "TRANSFER_OPERATION_SUCCESS":
      return NotificationConfig_EventType.TRANSFER_OPERATION_SUCCESS;
    case 2:
    case "TRANSFER_OPERATION_FAILED":
      return NotificationConfig_EventType.TRANSFER_OPERATION_FAILED;
    case 3:
    case "TRANSFER_OPERATION_ABORTED":
      return NotificationConfig_EventType.TRANSFER_OPERATION_ABORTED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return NotificationConfig_EventType.UNRECOGNIZED;
  }
}

export function notificationConfig_EventTypeToJSON(object: NotificationConfig_EventType): string {
  switch (object) {
    case NotificationConfig_EventType.EVENT_TYPE_UNSPECIFIED:
      return "EVENT_TYPE_UNSPECIFIED";
    case NotificationConfig_EventType.TRANSFER_OPERATION_SUCCESS:
      return "TRANSFER_OPERATION_SUCCESS";
    case NotificationConfig_EventType.TRANSFER_OPERATION_FAILED:
      return "TRANSFER_OPERATION_FAILED";
    case NotificationConfig_EventType.TRANSFER_OPERATION_ABORTED:
      return "TRANSFER_OPERATION_ABORTED";
    case NotificationConfig_EventType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Enum for specifying the format of a notification message's payload. */
export enum NotificationConfig_PayloadFormat {
  /** PAYLOAD_FORMAT_UNSPECIFIED - Illegal value, to avoid allowing a default. */
  PAYLOAD_FORMAT_UNSPECIFIED = 0,
  /** NONE - No payload is included with the notification. */
  NONE = 1,
  /**
   * JSON - `TransferOperation` is [formatted as a JSON
   * response](https://developers.google.com/protocol-buffers/docs/proto3#json),
   * in application/json.
   */
  JSON = 2,
  UNRECOGNIZED = -1,
}

export function notificationConfig_PayloadFormatFromJSON(object: any): NotificationConfig_PayloadFormat {
  switch (object) {
    case 0:
    case "PAYLOAD_FORMAT_UNSPECIFIED":
      return NotificationConfig_PayloadFormat.PAYLOAD_FORMAT_UNSPECIFIED;
    case 1:
    case "NONE":
      return NotificationConfig_PayloadFormat.NONE;
    case 2:
    case "JSON":
      return NotificationConfig_PayloadFormat.JSON;
    case -1:
    case "UNRECOGNIZED":
    default:
      return NotificationConfig_PayloadFormat.UNRECOGNIZED;
  }
}

export function notificationConfig_PayloadFormatToJSON(object: NotificationConfig_PayloadFormat): string {
  switch (object) {
    case NotificationConfig_PayloadFormat.PAYLOAD_FORMAT_UNSPECIFIED:
      return "PAYLOAD_FORMAT_UNSPECIFIED";
    case NotificationConfig_PayloadFormat.NONE:
      return "NONE";
    case NotificationConfig_PayloadFormat.JSON:
      return "JSON";
    case NotificationConfig_PayloadFormat.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Specifies the logging behavior for transfer operations.
 *
 * Logs can be sent to Cloud Logging for all transfer types. See
 * [Read transfer
 * logs](https://cloud.google.com/storage-transfer/docs/read-transfer-logs) for
 * details.
 */
export interface LoggingConfig {
  /** Specifies the actions to be logged. If empty, no logs are generated. */
  logActions: LoggingConfig_LoggableAction[];
  /** States in which `log_actions` are logged. If empty, no logs are generated. */
  logActionStates: LoggingConfig_LoggableActionState[];
  /**
   * For PosixFilesystem transfers, enables
   * [file system transfer
   * logs](https://cloud.google.com/storage-transfer/docs/on-prem-transfer-log-format)
   * instead of, or in addition to, Cloud Logging.
   *
   * This option ignores [LoggableAction] and [LoggableActionState]. If these
   * are set, Cloud Logging will also be enabled for this transfer.
   */
  enableOnpremGcsTransferLogs: boolean;
}

/** Loggable actions. */
export enum LoggingConfig_LoggableAction {
  /** LOGGABLE_ACTION_UNSPECIFIED - Default value. This value is unused. */
  LOGGABLE_ACTION_UNSPECIFIED = 0,
  /** FIND - Listing objects in a bucket. */
  FIND = 1,
  /** DELETE - Deleting objects at the source or the destination. */
  DELETE = 2,
  /** COPY - Copying objects to Google Cloud Storage. */
  COPY = 3,
  UNRECOGNIZED = -1,
}

export function loggingConfig_LoggableActionFromJSON(object: any): LoggingConfig_LoggableAction {
  switch (object) {
    case 0:
    case "LOGGABLE_ACTION_UNSPECIFIED":
      return LoggingConfig_LoggableAction.LOGGABLE_ACTION_UNSPECIFIED;
    case 1:
    case "FIND":
      return LoggingConfig_LoggableAction.FIND;
    case 2:
    case "DELETE":
      return LoggingConfig_LoggableAction.DELETE;
    case 3:
    case "COPY":
      return LoggingConfig_LoggableAction.COPY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return LoggingConfig_LoggableAction.UNRECOGNIZED;
  }
}

export function loggingConfig_LoggableActionToJSON(object: LoggingConfig_LoggableAction): string {
  switch (object) {
    case LoggingConfig_LoggableAction.LOGGABLE_ACTION_UNSPECIFIED:
      return "LOGGABLE_ACTION_UNSPECIFIED";
    case LoggingConfig_LoggableAction.FIND:
      return "FIND";
    case LoggingConfig_LoggableAction.DELETE:
      return "DELETE";
    case LoggingConfig_LoggableAction.COPY:
      return "COPY";
    case LoggingConfig_LoggableAction.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Loggable action states. */
export enum LoggingConfig_LoggableActionState {
  /** LOGGABLE_ACTION_STATE_UNSPECIFIED - Default value. This value is unused. */
  LOGGABLE_ACTION_STATE_UNSPECIFIED = 0,
  /**
   * SUCCEEDED - `LoggableAction` completed successfully. `SUCCEEDED` actions are
   * logged as [INFO][google.logging.type.LogSeverity.INFO].
   */
  SUCCEEDED = 1,
  /**
   * FAILED - `LoggableAction` terminated in an error state. `FAILED` actions are
   * logged as [ERROR][google.logging.type.LogSeverity.ERROR].
   */
  FAILED = 2,
  UNRECOGNIZED = -1,
}

export function loggingConfig_LoggableActionStateFromJSON(object: any): LoggingConfig_LoggableActionState {
  switch (object) {
    case 0:
    case "LOGGABLE_ACTION_STATE_UNSPECIFIED":
      return LoggingConfig_LoggableActionState.LOGGABLE_ACTION_STATE_UNSPECIFIED;
    case 1:
    case "SUCCEEDED":
      return LoggingConfig_LoggableActionState.SUCCEEDED;
    case 2:
    case "FAILED":
      return LoggingConfig_LoggableActionState.FAILED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return LoggingConfig_LoggableActionState.UNRECOGNIZED;
  }
}

export function loggingConfig_LoggableActionStateToJSON(object: LoggingConfig_LoggableActionState): string {
  switch (object) {
    case LoggingConfig_LoggableActionState.LOGGABLE_ACTION_STATE_UNSPECIFIED:
      return "LOGGABLE_ACTION_STATE_UNSPECIFIED";
    case LoggingConfig_LoggableActionState.SUCCEEDED:
      return "SUCCEEDED";
    case LoggingConfig_LoggableActionState.FAILED:
      return "FAILED";
    case LoggingConfig_LoggableActionState.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** A description of the execution of a transfer. */
export interface TransferOperation {
  /** A globally unique ID assigned by the system. */
  name: string;
  /** The ID of the Google Cloud project that owns the operation. */
  projectId: string;
  /** Transfer specification. */
  transferSpec:
    | TransferSpec
    | undefined;
  /** Notification configuration. */
  notificationConfig:
    | NotificationConfig
    | undefined;
  /** Cloud Logging configuration. */
  loggingConfig:
    | LoggingConfig
    | undefined;
  /** Start time of this transfer execution. */
  startTime:
    | Date
    | undefined;
  /** End time of this transfer execution. */
  endTime:
    | Date
    | undefined;
  /** Status of the transfer operation. */
  status: TransferOperation_Status;
  /** Information about the progress of the transfer operation. */
  counters:
    | TransferCounters
    | undefined;
  /** Summarizes errors encountered with sample error log entries. */
  errorBreakdowns: ErrorSummary[];
  /** The name of the transfer job that triggers this transfer operation. */
  transferJobName: string;
}

/** The status of a TransferOperation. */
export enum TransferOperation_Status {
  /** STATUS_UNSPECIFIED - Zero is an illegal value. */
  STATUS_UNSPECIFIED = 0,
  /** IN_PROGRESS - In progress. */
  IN_PROGRESS = 1,
  /** PAUSED - Paused. */
  PAUSED = 2,
  /** SUCCESS - Completed successfully. */
  SUCCESS = 3,
  /** FAILED - Terminated due to an unrecoverable failure. */
  FAILED = 4,
  /** ABORTED - Aborted by the user. */
  ABORTED = 5,
  /** QUEUED - Temporarily delayed by the system. No user action is required. */
  QUEUED = 6,
  /** SUSPENDING - The operation is suspending and draining the ongoing work to completion. */
  SUSPENDING = 7,
  UNRECOGNIZED = -1,
}

export function transferOperation_StatusFromJSON(object: any): TransferOperation_Status {
  switch (object) {
    case 0:
    case "STATUS_UNSPECIFIED":
      return TransferOperation_Status.STATUS_UNSPECIFIED;
    case 1:
    case "IN_PROGRESS":
      return TransferOperation_Status.IN_PROGRESS;
    case 2:
    case "PAUSED":
      return TransferOperation_Status.PAUSED;
    case 3:
    case "SUCCESS":
      return TransferOperation_Status.SUCCESS;
    case 4:
    case "FAILED":
      return TransferOperation_Status.FAILED;
    case 5:
    case "ABORTED":
      return TransferOperation_Status.ABORTED;
    case 6:
    case "QUEUED":
      return TransferOperation_Status.QUEUED;
    case 7:
    case "SUSPENDING":
      return TransferOperation_Status.SUSPENDING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return TransferOperation_Status.UNRECOGNIZED;
  }
}

export function transferOperation_StatusToJSON(object: TransferOperation_Status): string {
  switch (object) {
    case TransferOperation_Status.STATUS_UNSPECIFIED:
      return "STATUS_UNSPECIFIED";
    case TransferOperation_Status.IN_PROGRESS:
      return "IN_PROGRESS";
    case TransferOperation_Status.PAUSED:
      return "PAUSED";
    case TransferOperation_Status.SUCCESS:
      return "SUCCESS";
    case TransferOperation_Status.FAILED:
      return "FAILED";
    case TransferOperation_Status.ABORTED:
      return "ABORTED";
    case TransferOperation_Status.QUEUED:
      return "QUEUED";
    case TransferOperation_Status.SUSPENDING:
      return "SUSPENDING";
    case TransferOperation_Status.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseGoogleServiceAccount(): GoogleServiceAccount {
  return { accountEmail: "", subjectId: "" };
}

export const GoogleServiceAccount: MessageFns<GoogleServiceAccount> = {
  encode(message: GoogleServiceAccount, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.accountEmail !== "") {
      writer.uint32(10).string(message.accountEmail);
    }
    if (message.subjectId !== "") {
      writer.uint32(18).string(message.subjectId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GoogleServiceAccount {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGoogleServiceAccount();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.accountEmail = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.subjectId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GoogleServiceAccount {
    return {
      accountEmail: isSet(object.accountEmail) ? globalThis.String(object.accountEmail) : "",
      subjectId: isSet(object.subjectId) ? globalThis.String(object.subjectId) : "",
    };
  },

  toJSON(message: GoogleServiceAccount): unknown {
    const obj: any = {};
    if (message.accountEmail !== "") {
      obj.accountEmail = message.accountEmail;
    }
    if (message.subjectId !== "") {
      obj.subjectId = message.subjectId;
    }
    return obj;
  },

  create(base?: DeepPartial<GoogleServiceAccount>): GoogleServiceAccount {
    return GoogleServiceAccount.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GoogleServiceAccount>): GoogleServiceAccount {
    const message = createBaseGoogleServiceAccount();
    message.accountEmail = object.accountEmail ?? "";
    message.subjectId = object.subjectId ?? "";
    return message;
  },
};

function createBaseAwsAccessKey(): AwsAccessKey {
  return { accessKeyId: "", secretAccessKey: "" };
}

export const AwsAccessKey: MessageFns<AwsAccessKey> = {
  encode(message: AwsAccessKey, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.accessKeyId !== "") {
      writer.uint32(10).string(message.accessKeyId);
    }
    if (message.secretAccessKey !== "") {
      writer.uint32(18).string(message.secretAccessKey);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AwsAccessKey {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAwsAccessKey();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.accessKeyId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.secretAccessKey = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AwsAccessKey {
    return {
      accessKeyId: isSet(object.accessKeyId) ? globalThis.String(object.accessKeyId) : "",
      secretAccessKey: isSet(object.secretAccessKey) ? globalThis.String(object.secretAccessKey) : "",
    };
  },

  toJSON(message: AwsAccessKey): unknown {
    const obj: any = {};
    if (message.accessKeyId !== "") {
      obj.accessKeyId = message.accessKeyId;
    }
    if (message.secretAccessKey !== "") {
      obj.secretAccessKey = message.secretAccessKey;
    }
    return obj;
  },

  create(base?: DeepPartial<AwsAccessKey>): AwsAccessKey {
    return AwsAccessKey.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AwsAccessKey>): AwsAccessKey {
    const message = createBaseAwsAccessKey();
    message.accessKeyId = object.accessKeyId ?? "";
    message.secretAccessKey = object.secretAccessKey ?? "";
    return message;
  },
};

function createBaseAzureCredentials(): AzureCredentials {
  return { sasToken: "" };
}

export const AzureCredentials: MessageFns<AzureCredentials> = {
  encode(message: AzureCredentials, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.sasToken !== "") {
      writer.uint32(18).string(message.sasToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AzureCredentials {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAzureCredentials();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.sasToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AzureCredentials {
    return { sasToken: isSet(object.sasToken) ? globalThis.String(object.sasToken) : "" };
  },

  toJSON(message: AzureCredentials): unknown {
    const obj: any = {};
    if (message.sasToken !== "") {
      obj.sasToken = message.sasToken;
    }
    return obj;
  },

  create(base?: DeepPartial<AzureCredentials>): AzureCredentials {
    return AzureCredentials.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AzureCredentials>): AzureCredentials {
    const message = createBaseAzureCredentials();
    message.sasToken = object.sasToken ?? "";
    return message;
  },
};

function createBaseObjectConditions(): ObjectConditions {
  return {
    minTimeElapsedSinceLastModification: undefined,
    maxTimeElapsedSinceLastModification: undefined,
    includePrefixes: [],
    excludePrefixes: [],
    lastModifiedSince: undefined,
    lastModifiedBefore: undefined,
  };
}

export const ObjectConditions: MessageFns<ObjectConditions> = {
  encode(message: ObjectConditions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.minTimeElapsedSinceLastModification !== undefined) {
      Duration.encode(message.minTimeElapsedSinceLastModification, writer.uint32(10).fork()).join();
    }
    if (message.maxTimeElapsedSinceLastModification !== undefined) {
      Duration.encode(message.maxTimeElapsedSinceLastModification, writer.uint32(18).fork()).join();
    }
    for (const v of message.includePrefixes) {
      writer.uint32(26).string(v!);
    }
    for (const v of message.excludePrefixes) {
      writer.uint32(34).string(v!);
    }
    if (message.lastModifiedSince !== undefined) {
      Timestamp.encode(toTimestamp(message.lastModifiedSince), writer.uint32(42).fork()).join();
    }
    if (message.lastModifiedBefore !== undefined) {
      Timestamp.encode(toTimestamp(message.lastModifiedBefore), writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ObjectConditions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseObjectConditions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.minTimeElapsedSinceLastModification = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.maxTimeElapsedSinceLastModification = Duration.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.includePrefixes.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.excludePrefixes.push(reader.string());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.lastModifiedSince = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.lastModifiedBefore = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ObjectConditions {
    return {
      minTimeElapsedSinceLastModification: isSet(object.minTimeElapsedSinceLastModification)
        ? Duration.fromJSON(object.minTimeElapsedSinceLastModification)
        : undefined,
      maxTimeElapsedSinceLastModification: isSet(object.maxTimeElapsedSinceLastModification)
        ? Duration.fromJSON(object.maxTimeElapsedSinceLastModification)
        : undefined,
      includePrefixes: globalThis.Array.isArray(object?.includePrefixes)
        ? object.includePrefixes.map((e: any) => globalThis.String(e))
        : [],
      excludePrefixes: globalThis.Array.isArray(object?.excludePrefixes)
        ? object.excludePrefixes.map((e: any) => globalThis.String(e))
        : [],
      lastModifiedSince: isSet(object.lastModifiedSince) ? fromJsonTimestamp(object.lastModifiedSince) : undefined,
      lastModifiedBefore: isSet(object.lastModifiedBefore) ? fromJsonTimestamp(object.lastModifiedBefore) : undefined,
    };
  },

  toJSON(message: ObjectConditions): unknown {
    const obj: any = {};
    if (message.minTimeElapsedSinceLastModification !== undefined) {
      obj.minTimeElapsedSinceLastModification = Duration.toJSON(message.minTimeElapsedSinceLastModification);
    }
    if (message.maxTimeElapsedSinceLastModification !== undefined) {
      obj.maxTimeElapsedSinceLastModification = Duration.toJSON(message.maxTimeElapsedSinceLastModification);
    }
    if (message.includePrefixes?.length) {
      obj.includePrefixes = message.includePrefixes;
    }
    if (message.excludePrefixes?.length) {
      obj.excludePrefixes = message.excludePrefixes;
    }
    if (message.lastModifiedSince !== undefined) {
      obj.lastModifiedSince = message.lastModifiedSince.toISOString();
    }
    if (message.lastModifiedBefore !== undefined) {
      obj.lastModifiedBefore = message.lastModifiedBefore.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<ObjectConditions>): ObjectConditions {
    return ObjectConditions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ObjectConditions>): ObjectConditions {
    const message = createBaseObjectConditions();
    message.minTimeElapsedSinceLastModification =
      (object.minTimeElapsedSinceLastModification !== undefined && object.minTimeElapsedSinceLastModification !== null)
        ? Duration.fromPartial(object.minTimeElapsedSinceLastModification)
        : undefined;
    message.maxTimeElapsedSinceLastModification =
      (object.maxTimeElapsedSinceLastModification !== undefined && object.maxTimeElapsedSinceLastModification !== null)
        ? Duration.fromPartial(object.maxTimeElapsedSinceLastModification)
        : undefined;
    message.includePrefixes = object.includePrefixes?.map((e) => e) || [];
    message.excludePrefixes = object.excludePrefixes?.map((e) => e) || [];
    message.lastModifiedSince = object.lastModifiedSince ?? undefined;
    message.lastModifiedBefore = object.lastModifiedBefore ?? undefined;
    return message;
  },
};

function createBaseGcsData(): GcsData {
  return { bucketName: "", path: "", managedFolderTransferEnabled: false };
}

export const GcsData: MessageFns<GcsData> = {
  encode(message: GcsData, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.bucketName !== "") {
      writer.uint32(10).string(message.bucketName);
    }
    if (message.path !== "") {
      writer.uint32(26).string(message.path);
    }
    if (message.managedFolderTransferEnabled !== false) {
      writer.uint32(32).bool(message.managedFolderTransferEnabled);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GcsData {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGcsData();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.bucketName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.path = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.managedFolderTransferEnabled = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GcsData {
    return {
      bucketName: isSet(object.bucketName) ? globalThis.String(object.bucketName) : "",
      path: isSet(object.path) ? globalThis.String(object.path) : "",
      managedFolderTransferEnabled: isSet(object.managedFolderTransferEnabled)
        ? globalThis.Boolean(object.managedFolderTransferEnabled)
        : false,
    };
  },

  toJSON(message: GcsData): unknown {
    const obj: any = {};
    if (message.bucketName !== "") {
      obj.bucketName = message.bucketName;
    }
    if (message.path !== "") {
      obj.path = message.path;
    }
    if (message.managedFolderTransferEnabled !== false) {
      obj.managedFolderTransferEnabled = message.managedFolderTransferEnabled;
    }
    return obj;
  },

  create(base?: DeepPartial<GcsData>): GcsData {
    return GcsData.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GcsData>): GcsData {
    const message = createBaseGcsData();
    message.bucketName = object.bucketName ?? "";
    message.path = object.path ?? "";
    message.managedFolderTransferEnabled = object.managedFolderTransferEnabled ?? false;
    return message;
  },
};

function createBaseAwsS3Data(): AwsS3Data {
  return {
    bucketName: "",
    awsAccessKey: undefined,
    path: "",
    roleArn: "",
    cloudfrontDomain: "",
    credentialsSecret: "",
    managedPrivateNetwork: undefined,
  };
}

export const AwsS3Data: MessageFns<AwsS3Data> = {
  encode(message: AwsS3Data, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.bucketName !== "") {
      writer.uint32(10).string(message.bucketName);
    }
    if (message.awsAccessKey !== undefined) {
      AwsAccessKey.encode(message.awsAccessKey, writer.uint32(18).fork()).join();
    }
    if (message.path !== "") {
      writer.uint32(26).string(message.path);
    }
    if (message.roleArn !== "") {
      writer.uint32(34).string(message.roleArn);
    }
    if (message.cloudfrontDomain !== "") {
      writer.uint32(50).string(message.cloudfrontDomain);
    }
    if (message.credentialsSecret !== "") {
      writer.uint32(58).string(message.credentialsSecret);
    }
    if (message.managedPrivateNetwork !== undefined) {
      writer.uint32(64).bool(message.managedPrivateNetwork);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AwsS3Data {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAwsS3Data();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.bucketName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.awsAccessKey = AwsAccessKey.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.path = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.roleArn = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.cloudfrontDomain = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.credentialsSecret = reader.string();
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.managedPrivateNetwork = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AwsS3Data {
    return {
      bucketName: isSet(object.bucketName) ? globalThis.String(object.bucketName) : "",
      awsAccessKey: isSet(object.awsAccessKey) ? AwsAccessKey.fromJSON(object.awsAccessKey) : undefined,
      path: isSet(object.path) ? globalThis.String(object.path) : "",
      roleArn: isSet(object.roleArn) ? globalThis.String(object.roleArn) : "",
      cloudfrontDomain: isSet(object.cloudfrontDomain) ? globalThis.String(object.cloudfrontDomain) : "",
      credentialsSecret: isSet(object.credentialsSecret) ? globalThis.String(object.credentialsSecret) : "",
      managedPrivateNetwork: isSet(object.managedPrivateNetwork)
        ? globalThis.Boolean(object.managedPrivateNetwork)
        : undefined,
    };
  },

  toJSON(message: AwsS3Data): unknown {
    const obj: any = {};
    if (message.bucketName !== "") {
      obj.bucketName = message.bucketName;
    }
    if (message.awsAccessKey !== undefined) {
      obj.awsAccessKey = AwsAccessKey.toJSON(message.awsAccessKey);
    }
    if (message.path !== "") {
      obj.path = message.path;
    }
    if (message.roleArn !== "") {
      obj.roleArn = message.roleArn;
    }
    if (message.cloudfrontDomain !== "") {
      obj.cloudfrontDomain = message.cloudfrontDomain;
    }
    if (message.credentialsSecret !== "") {
      obj.credentialsSecret = message.credentialsSecret;
    }
    if (message.managedPrivateNetwork !== undefined) {
      obj.managedPrivateNetwork = message.managedPrivateNetwork;
    }
    return obj;
  },

  create(base?: DeepPartial<AwsS3Data>): AwsS3Data {
    return AwsS3Data.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AwsS3Data>): AwsS3Data {
    const message = createBaseAwsS3Data();
    message.bucketName = object.bucketName ?? "";
    message.awsAccessKey = (object.awsAccessKey !== undefined && object.awsAccessKey !== null)
      ? AwsAccessKey.fromPartial(object.awsAccessKey)
      : undefined;
    message.path = object.path ?? "";
    message.roleArn = object.roleArn ?? "";
    message.cloudfrontDomain = object.cloudfrontDomain ?? "";
    message.credentialsSecret = object.credentialsSecret ?? "";
    message.managedPrivateNetwork = object.managedPrivateNetwork ?? undefined;
    return message;
  },
};

function createBaseAzureBlobStorageData(): AzureBlobStorageData {
  return { storageAccount: "", azureCredentials: undefined, container: "", path: "", credentialsSecret: "" };
}

export const AzureBlobStorageData: MessageFns<AzureBlobStorageData> = {
  encode(message: AzureBlobStorageData, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.storageAccount !== "") {
      writer.uint32(10).string(message.storageAccount);
    }
    if (message.azureCredentials !== undefined) {
      AzureCredentials.encode(message.azureCredentials, writer.uint32(18).fork()).join();
    }
    if (message.container !== "") {
      writer.uint32(34).string(message.container);
    }
    if (message.path !== "") {
      writer.uint32(42).string(message.path);
    }
    if (message.credentialsSecret !== "") {
      writer.uint32(58).string(message.credentialsSecret);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AzureBlobStorageData {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAzureBlobStorageData();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.storageAccount = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.azureCredentials = AzureCredentials.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.container = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.path = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.credentialsSecret = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AzureBlobStorageData {
    return {
      storageAccount: isSet(object.storageAccount) ? globalThis.String(object.storageAccount) : "",
      azureCredentials: isSet(object.azureCredentials) ? AzureCredentials.fromJSON(object.azureCredentials) : undefined,
      container: isSet(object.container) ? globalThis.String(object.container) : "",
      path: isSet(object.path) ? globalThis.String(object.path) : "",
      credentialsSecret: isSet(object.credentialsSecret) ? globalThis.String(object.credentialsSecret) : "",
    };
  },

  toJSON(message: AzureBlobStorageData): unknown {
    const obj: any = {};
    if (message.storageAccount !== "") {
      obj.storageAccount = message.storageAccount;
    }
    if (message.azureCredentials !== undefined) {
      obj.azureCredentials = AzureCredentials.toJSON(message.azureCredentials);
    }
    if (message.container !== "") {
      obj.container = message.container;
    }
    if (message.path !== "") {
      obj.path = message.path;
    }
    if (message.credentialsSecret !== "") {
      obj.credentialsSecret = message.credentialsSecret;
    }
    return obj;
  },

  create(base?: DeepPartial<AzureBlobStorageData>): AzureBlobStorageData {
    return AzureBlobStorageData.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AzureBlobStorageData>): AzureBlobStorageData {
    const message = createBaseAzureBlobStorageData();
    message.storageAccount = object.storageAccount ?? "";
    message.azureCredentials = (object.azureCredentials !== undefined && object.azureCredentials !== null)
      ? AzureCredentials.fromPartial(object.azureCredentials)
      : undefined;
    message.container = object.container ?? "";
    message.path = object.path ?? "";
    message.credentialsSecret = object.credentialsSecret ?? "";
    return message;
  },
};

function createBaseHttpData(): HttpData {
  return { listUrl: "" };
}

export const HttpData: MessageFns<HttpData> = {
  encode(message: HttpData, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.listUrl !== "") {
      writer.uint32(10).string(message.listUrl);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): HttpData {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseHttpData();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.listUrl = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): HttpData {
    return { listUrl: isSet(object.listUrl) ? globalThis.String(object.listUrl) : "" };
  },

  toJSON(message: HttpData): unknown {
    const obj: any = {};
    if (message.listUrl !== "") {
      obj.listUrl = message.listUrl;
    }
    return obj;
  },

  create(base?: DeepPartial<HttpData>): HttpData {
    return HttpData.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<HttpData>): HttpData {
    const message = createBaseHttpData();
    message.listUrl = object.listUrl ?? "";
    return message;
  },
};

function createBasePosixFilesystem(): PosixFilesystem {
  return { rootDirectory: "" };
}

export const PosixFilesystem: MessageFns<PosixFilesystem> = {
  encode(message: PosixFilesystem, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.rootDirectory !== "") {
      writer.uint32(10).string(message.rootDirectory);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PosixFilesystem {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePosixFilesystem();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.rootDirectory = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PosixFilesystem {
    return { rootDirectory: isSet(object.rootDirectory) ? globalThis.String(object.rootDirectory) : "" };
  },

  toJSON(message: PosixFilesystem): unknown {
    const obj: any = {};
    if (message.rootDirectory !== "") {
      obj.rootDirectory = message.rootDirectory;
    }
    return obj;
  },

  create(base?: DeepPartial<PosixFilesystem>): PosixFilesystem {
    return PosixFilesystem.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PosixFilesystem>): PosixFilesystem {
    const message = createBasePosixFilesystem();
    message.rootDirectory = object.rootDirectory ?? "";
    return message;
  },
};

function createBaseHdfsData(): HdfsData {
  return { path: "" };
}

export const HdfsData: MessageFns<HdfsData> = {
  encode(message: HdfsData, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.path !== "") {
      writer.uint32(10).string(message.path);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): HdfsData {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseHdfsData();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.path = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): HdfsData {
    return { path: isSet(object.path) ? globalThis.String(object.path) : "" };
  },

  toJSON(message: HdfsData): unknown {
    const obj: any = {};
    if (message.path !== "") {
      obj.path = message.path;
    }
    return obj;
  },

  create(base?: DeepPartial<HdfsData>): HdfsData {
    return HdfsData.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<HdfsData>): HdfsData {
    const message = createBaseHdfsData();
    message.path = object.path ?? "";
    return message;
  },
};

function createBaseAwsS3CompatibleData(): AwsS3CompatibleData {
  return { bucketName: "", path: "", endpoint: "", region: "", s3Metadata: undefined };
}

export const AwsS3CompatibleData: MessageFns<AwsS3CompatibleData> = {
  encode(message: AwsS3CompatibleData, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.bucketName !== "") {
      writer.uint32(10).string(message.bucketName);
    }
    if (message.path !== "") {
      writer.uint32(18).string(message.path);
    }
    if (message.endpoint !== "") {
      writer.uint32(26).string(message.endpoint);
    }
    if (message.region !== "") {
      writer.uint32(42).string(message.region);
    }
    if (message.s3Metadata !== undefined) {
      S3CompatibleMetadata.encode(message.s3Metadata, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AwsS3CompatibleData {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAwsS3CompatibleData();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.bucketName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.path = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.endpoint = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.region = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.s3Metadata = S3CompatibleMetadata.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AwsS3CompatibleData {
    return {
      bucketName: isSet(object.bucketName) ? globalThis.String(object.bucketName) : "",
      path: isSet(object.path) ? globalThis.String(object.path) : "",
      endpoint: isSet(object.endpoint) ? globalThis.String(object.endpoint) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      s3Metadata: isSet(object.s3Metadata) ? S3CompatibleMetadata.fromJSON(object.s3Metadata) : undefined,
    };
  },

  toJSON(message: AwsS3CompatibleData): unknown {
    const obj: any = {};
    if (message.bucketName !== "") {
      obj.bucketName = message.bucketName;
    }
    if (message.path !== "") {
      obj.path = message.path;
    }
    if (message.endpoint !== "") {
      obj.endpoint = message.endpoint;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.s3Metadata !== undefined) {
      obj.s3Metadata = S3CompatibleMetadata.toJSON(message.s3Metadata);
    }
    return obj;
  },

  create(base?: DeepPartial<AwsS3CompatibleData>): AwsS3CompatibleData {
    return AwsS3CompatibleData.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AwsS3CompatibleData>): AwsS3CompatibleData {
    const message = createBaseAwsS3CompatibleData();
    message.bucketName = object.bucketName ?? "";
    message.path = object.path ?? "";
    message.endpoint = object.endpoint ?? "";
    message.region = object.region ?? "";
    message.s3Metadata = (object.s3Metadata !== undefined && object.s3Metadata !== null)
      ? S3CompatibleMetadata.fromPartial(object.s3Metadata)
      : undefined;
    return message;
  },
};

function createBaseS3CompatibleMetadata(): S3CompatibleMetadata {
  return { authMethod: 0, requestModel: 0, protocol: 0, listApi: 0 };
}

export const S3CompatibleMetadata: MessageFns<S3CompatibleMetadata> = {
  encode(message: S3CompatibleMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.authMethod !== 0) {
      writer.uint32(8).int32(message.authMethod);
    }
    if (message.requestModel !== 0) {
      writer.uint32(16).int32(message.requestModel);
    }
    if (message.protocol !== 0) {
      writer.uint32(24).int32(message.protocol);
    }
    if (message.listApi !== 0) {
      writer.uint32(32).int32(message.listApi);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): S3CompatibleMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseS3CompatibleMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.authMethod = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.requestModel = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.protocol = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.listApi = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): S3CompatibleMetadata {
    return {
      authMethod: isSet(object.authMethod) ? s3CompatibleMetadata_AuthMethodFromJSON(object.authMethod) : 0,
      requestModel: isSet(object.requestModel) ? s3CompatibleMetadata_RequestModelFromJSON(object.requestModel) : 0,
      protocol: isSet(object.protocol) ? s3CompatibleMetadata_NetworkProtocolFromJSON(object.protocol) : 0,
      listApi: isSet(object.listApi) ? s3CompatibleMetadata_ListApiFromJSON(object.listApi) : 0,
    };
  },

  toJSON(message: S3CompatibleMetadata): unknown {
    const obj: any = {};
    if (message.authMethod !== 0) {
      obj.authMethod = s3CompatibleMetadata_AuthMethodToJSON(message.authMethod);
    }
    if (message.requestModel !== 0) {
      obj.requestModel = s3CompatibleMetadata_RequestModelToJSON(message.requestModel);
    }
    if (message.protocol !== 0) {
      obj.protocol = s3CompatibleMetadata_NetworkProtocolToJSON(message.protocol);
    }
    if (message.listApi !== 0) {
      obj.listApi = s3CompatibleMetadata_ListApiToJSON(message.listApi);
    }
    return obj;
  },

  create(base?: DeepPartial<S3CompatibleMetadata>): S3CompatibleMetadata {
    return S3CompatibleMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<S3CompatibleMetadata>): S3CompatibleMetadata {
    const message = createBaseS3CompatibleMetadata();
    message.authMethod = object.authMethod ?? 0;
    message.requestModel = object.requestModel ?? 0;
    message.protocol = object.protocol ?? 0;
    message.listApi = object.listApi ?? 0;
    return message;
  },
};

function createBaseAgentPool(): AgentPool {
  return { name: "", displayName: "", state: 0, bandwidthLimit: undefined };
}

export const AgentPool: MessageFns<AgentPool> = {
  encode(message: AgentPool, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    if (message.displayName !== "") {
      writer.uint32(26).string(message.displayName);
    }
    if (message.state !== 0) {
      writer.uint32(32).int32(message.state);
    }
    if (message.bandwidthLimit !== undefined) {
      AgentPool_BandwidthLimit.encode(message.bandwidthLimit, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AgentPool {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAgentPool();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.bandwidthLimit = AgentPool_BandwidthLimit.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AgentPool {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      state: isSet(object.state) ? agentPool_StateFromJSON(object.state) : 0,
      bandwidthLimit: isSet(object.bandwidthLimit)
        ? AgentPool_BandwidthLimit.fromJSON(object.bandwidthLimit)
        : undefined,
    };
  },

  toJSON(message: AgentPool): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.state !== 0) {
      obj.state = agentPool_StateToJSON(message.state);
    }
    if (message.bandwidthLimit !== undefined) {
      obj.bandwidthLimit = AgentPool_BandwidthLimit.toJSON(message.bandwidthLimit);
    }
    return obj;
  },

  create(base?: DeepPartial<AgentPool>): AgentPool {
    return AgentPool.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AgentPool>): AgentPool {
    const message = createBaseAgentPool();
    message.name = object.name ?? "";
    message.displayName = object.displayName ?? "";
    message.state = object.state ?? 0;
    message.bandwidthLimit = (object.bandwidthLimit !== undefined && object.bandwidthLimit !== null)
      ? AgentPool_BandwidthLimit.fromPartial(object.bandwidthLimit)
      : undefined;
    return message;
  },
};

function createBaseAgentPool_BandwidthLimit(): AgentPool_BandwidthLimit {
  return { limitMbps: Long.ZERO };
}

export const AgentPool_BandwidthLimit: MessageFns<AgentPool_BandwidthLimit> = {
  encode(message: AgentPool_BandwidthLimit, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.limitMbps.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.limitMbps.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AgentPool_BandwidthLimit {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAgentPool_BandwidthLimit();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.limitMbps = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AgentPool_BandwidthLimit {
    return { limitMbps: isSet(object.limitMbps) ? Long.fromValue(object.limitMbps) : Long.ZERO };
  },

  toJSON(message: AgentPool_BandwidthLimit): unknown {
    const obj: any = {};
    if (!message.limitMbps.equals(Long.ZERO)) {
      obj.limitMbps = (message.limitMbps || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<AgentPool_BandwidthLimit>): AgentPool_BandwidthLimit {
    return AgentPool_BandwidthLimit.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AgentPool_BandwidthLimit>): AgentPool_BandwidthLimit {
    const message = createBaseAgentPool_BandwidthLimit();
    message.limitMbps = (object.limitMbps !== undefined && object.limitMbps !== null)
      ? Long.fromValue(object.limitMbps)
      : Long.ZERO;
    return message;
  },
};

function createBaseTransferOptions(): TransferOptions {
  return {
    overwriteObjectsAlreadyExistingInSink: false,
    deleteObjectsUniqueInSink: false,
    deleteObjectsFromSourceAfterTransfer: false,
    overwriteWhen: 0,
    metadataOptions: undefined,
  };
}

export const TransferOptions: MessageFns<TransferOptions> = {
  encode(message: TransferOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.overwriteObjectsAlreadyExistingInSink !== false) {
      writer.uint32(8).bool(message.overwriteObjectsAlreadyExistingInSink);
    }
    if (message.deleteObjectsUniqueInSink !== false) {
      writer.uint32(16).bool(message.deleteObjectsUniqueInSink);
    }
    if (message.deleteObjectsFromSourceAfterTransfer !== false) {
      writer.uint32(24).bool(message.deleteObjectsFromSourceAfterTransfer);
    }
    if (message.overwriteWhen !== 0) {
      writer.uint32(32).int32(message.overwriteWhen);
    }
    if (message.metadataOptions !== undefined) {
      MetadataOptions.encode(message.metadataOptions, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TransferOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTransferOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.overwriteObjectsAlreadyExistingInSink = reader.bool();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.deleteObjectsUniqueInSink = reader.bool();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.deleteObjectsFromSourceAfterTransfer = reader.bool();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.overwriteWhen = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.metadataOptions = MetadataOptions.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TransferOptions {
    return {
      overwriteObjectsAlreadyExistingInSink: isSet(object.overwriteObjectsAlreadyExistingInSink)
        ? globalThis.Boolean(object.overwriteObjectsAlreadyExistingInSink)
        : false,
      deleteObjectsUniqueInSink: isSet(object.deleteObjectsUniqueInSink)
        ? globalThis.Boolean(object.deleteObjectsUniqueInSink)
        : false,
      deleteObjectsFromSourceAfterTransfer: isSet(object.deleteObjectsFromSourceAfterTransfer)
        ? globalThis.Boolean(object.deleteObjectsFromSourceAfterTransfer)
        : false,
      overwriteWhen: isSet(object.overwriteWhen) ? transferOptions_OverwriteWhenFromJSON(object.overwriteWhen) : 0,
      metadataOptions: isSet(object.metadataOptions) ? MetadataOptions.fromJSON(object.metadataOptions) : undefined,
    };
  },

  toJSON(message: TransferOptions): unknown {
    const obj: any = {};
    if (message.overwriteObjectsAlreadyExistingInSink !== false) {
      obj.overwriteObjectsAlreadyExistingInSink = message.overwriteObjectsAlreadyExistingInSink;
    }
    if (message.deleteObjectsUniqueInSink !== false) {
      obj.deleteObjectsUniqueInSink = message.deleteObjectsUniqueInSink;
    }
    if (message.deleteObjectsFromSourceAfterTransfer !== false) {
      obj.deleteObjectsFromSourceAfterTransfer = message.deleteObjectsFromSourceAfterTransfer;
    }
    if (message.overwriteWhen !== 0) {
      obj.overwriteWhen = transferOptions_OverwriteWhenToJSON(message.overwriteWhen);
    }
    if (message.metadataOptions !== undefined) {
      obj.metadataOptions = MetadataOptions.toJSON(message.metadataOptions);
    }
    return obj;
  },

  create(base?: DeepPartial<TransferOptions>): TransferOptions {
    return TransferOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TransferOptions>): TransferOptions {
    const message = createBaseTransferOptions();
    message.overwriteObjectsAlreadyExistingInSink = object.overwriteObjectsAlreadyExistingInSink ?? false;
    message.deleteObjectsUniqueInSink = object.deleteObjectsUniqueInSink ?? false;
    message.deleteObjectsFromSourceAfterTransfer = object.deleteObjectsFromSourceAfterTransfer ?? false;
    message.overwriteWhen = object.overwriteWhen ?? 0;
    message.metadataOptions = (object.metadataOptions !== undefined && object.metadataOptions !== null)
      ? MetadataOptions.fromPartial(object.metadataOptions)
      : undefined;
    return message;
  },
};

function createBaseTransferSpec(): TransferSpec {
  return {
    gcsDataSink: undefined,
    posixDataSink: undefined,
    gcsDataSource: undefined,
    awsS3DataSource: undefined,
    httpDataSource: undefined,
    posixDataSource: undefined,
    azureBlobStorageDataSource: undefined,
    awsS3CompatibleDataSource: undefined,
    hdfsDataSource: undefined,
    gcsIntermediateDataLocation: undefined,
    objectConditions: undefined,
    transferOptions: undefined,
    transferManifest: undefined,
    sourceAgentPoolName: "",
    sinkAgentPoolName: "",
  };
}

export const TransferSpec: MessageFns<TransferSpec> = {
  encode(message: TransferSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcsDataSink !== undefined) {
      GcsData.encode(message.gcsDataSink, writer.uint32(34).fork()).join();
    }
    if (message.posixDataSink !== undefined) {
      PosixFilesystem.encode(message.posixDataSink, writer.uint32(106).fork()).join();
    }
    if (message.gcsDataSource !== undefined) {
      GcsData.encode(message.gcsDataSource, writer.uint32(10).fork()).join();
    }
    if (message.awsS3DataSource !== undefined) {
      AwsS3Data.encode(message.awsS3DataSource, writer.uint32(18).fork()).join();
    }
    if (message.httpDataSource !== undefined) {
      HttpData.encode(message.httpDataSource, writer.uint32(26).fork()).join();
    }
    if (message.posixDataSource !== undefined) {
      PosixFilesystem.encode(message.posixDataSource, writer.uint32(114).fork()).join();
    }
    if (message.azureBlobStorageDataSource !== undefined) {
      AzureBlobStorageData.encode(message.azureBlobStorageDataSource, writer.uint32(66).fork()).join();
    }
    if (message.awsS3CompatibleDataSource !== undefined) {
      AwsS3CompatibleData.encode(message.awsS3CompatibleDataSource, writer.uint32(154).fork()).join();
    }
    if (message.hdfsDataSource !== undefined) {
      HdfsData.encode(message.hdfsDataSource, writer.uint32(162).fork()).join();
    }
    if (message.gcsIntermediateDataLocation !== undefined) {
      GcsData.encode(message.gcsIntermediateDataLocation, writer.uint32(130).fork()).join();
    }
    if (message.objectConditions !== undefined) {
      ObjectConditions.encode(message.objectConditions, writer.uint32(42).fork()).join();
    }
    if (message.transferOptions !== undefined) {
      TransferOptions.encode(message.transferOptions, writer.uint32(50).fork()).join();
    }
    if (message.transferManifest !== undefined) {
      TransferManifest.encode(message.transferManifest, writer.uint32(122).fork()).join();
    }
    if (message.sourceAgentPoolName !== "") {
      writer.uint32(138).string(message.sourceAgentPoolName);
    }
    if (message.sinkAgentPoolName !== "") {
      writer.uint32(146).string(message.sinkAgentPoolName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TransferSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTransferSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 4:
          if (tag !== 34) {
            break;
          }

          message.gcsDataSink = GcsData.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.posixDataSink = PosixFilesystem.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.gcsDataSource = GcsData.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.awsS3DataSource = AwsS3Data.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.httpDataSource = HttpData.decode(reader, reader.uint32());
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.posixDataSource = PosixFilesystem.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.azureBlobStorageDataSource = AzureBlobStorageData.decode(reader, reader.uint32());
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.awsS3CompatibleDataSource = AwsS3CompatibleData.decode(reader, reader.uint32());
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.hdfsDataSource = HdfsData.decode(reader, reader.uint32());
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.gcsIntermediateDataLocation = GcsData.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.objectConditions = ObjectConditions.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.transferOptions = TransferOptions.decode(reader, reader.uint32());
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.transferManifest = TransferManifest.decode(reader, reader.uint32());
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.sourceAgentPoolName = reader.string();
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.sinkAgentPoolName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TransferSpec {
    return {
      gcsDataSink: isSet(object.gcsDataSink) ? GcsData.fromJSON(object.gcsDataSink) : undefined,
      posixDataSink: isSet(object.posixDataSink) ? PosixFilesystem.fromJSON(object.posixDataSink) : undefined,
      gcsDataSource: isSet(object.gcsDataSource) ? GcsData.fromJSON(object.gcsDataSource) : undefined,
      awsS3DataSource: isSet(object.awsS3DataSource) ? AwsS3Data.fromJSON(object.awsS3DataSource) : undefined,
      httpDataSource: isSet(object.httpDataSource) ? HttpData.fromJSON(object.httpDataSource) : undefined,
      posixDataSource: isSet(object.posixDataSource) ? PosixFilesystem.fromJSON(object.posixDataSource) : undefined,
      azureBlobStorageDataSource: isSet(object.azureBlobStorageDataSource)
        ? AzureBlobStorageData.fromJSON(object.azureBlobStorageDataSource)
        : undefined,
      awsS3CompatibleDataSource: isSet(object.awsS3CompatibleDataSource)
        ? AwsS3CompatibleData.fromJSON(object.awsS3CompatibleDataSource)
        : undefined,
      hdfsDataSource: isSet(object.hdfsDataSource) ? HdfsData.fromJSON(object.hdfsDataSource) : undefined,
      gcsIntermediateDataLocation: isSet(object.gcsIntermediateDataLocation)
        ? GcsData.fromJSON(object.gcsIntermediateDataLocation)
        : undefined,
      objectConditions: isSet(object.objectConditions) ? ObjectConditions.fromJSON(object.objectConditions) : undefined,
      transferOptions: isSet(object.transferOptions) ? TransferOptions.fromJSON(object.transferOptions) : undefined,
      transferManifest: isSet(object.transferManifest) ? TransferManifest.fromJSON(object.transferManifest) : undefined,
      sourceAgentPoolName: isSet(object.sourceAgentPoolName) ? globalThis.String(object.sourceAgentPoolName) : "",
      sinkAgentPoolName: isSet(object.sinkAgentPoolName) ? globalThis.String(object.sinkAgentPoolName) : "",
    };
  },

  toJSON(message: TransferSpec): unknown {
    const obj: any = {};
    if (message.gcsDataSink !== undefined) {
      obj.gcsDataSink = GcsData.toJSON(message.gcsDataSink);
    }
    if (message.posixDataSink !== undefined) {
      obj.posixDataSink = PosixFilesystem.toJSON(message.posixDataSink);
    }
    if (message.gcsDataSource !== undefined) {
      obj.gcsDataSource = GcsData.toJSON(message.gcsDataSource);
    }
    if (message.awsS3DataSource !== undefined) {
      obj.awsS3DataSource = AwsS3Data.toJSON(message.awsS3DataSource);
    }
    if (message.httpDataSource !== undefined) {
      obj.httpDataSource = HttpData.toJSON(message.httpDataSource);
    }
    if (message.posixDataSource !== undefined) {
      obj.posixDataSource = PosixFilesystem.toJSON(message.posixDataSource);
    }
    if (message.azureBlobStorageDataSource !== undefined) {
      obj.azureBlobStorageDataSource = AzureBlobStorageData.toJSON(message.azureBlobStorageDataSource);
    }
    if (message.awsS3CompatibleDataSource !== undefined) {
      obj.awsS3CompatibleDataSource = AwsS3CompatibleData.toJSON(message.awsS3CompatibleDataSource);
    }
    if (message.hdfsDataSource !== undefined) {
      obj.hdfsDataSource = HdfsData.toJSON(message.hdfsDataSource);
    }
    if (message.gcsIntermediateDataLocation !== undefined) {
      obj.gcsIntermediateDataLocation = GcsData.toJSON(message.gcsIntermediateDataLocation);
    }
    if (message.objectConditions !== undefined) {
      obj.objectConditions = ObjectConditions.toJSON(message.objectConditions);
    }
    if (message.transferOptions !== undefined) {
      obj.transferOptions = TransferOptions.toJSON(message.transferOptions);
    }
    if (message.transferManifest !== undefined) {
      obj.transferManifest = TransferManifest.toJSON(message.transferManifest);
    }
    if (message.sourceAgentPoolName !== "") {
      obj.sourceAgentPoolName = message.sourceAgentPoolName;
    }
    if (message.sinkAgentPoolName !== "") {
      obj.sinkAgentPoolName = message.sinkAgentPoolName;
    }
    return obj;
  },

  create(base?: DeepPartial<TransferSpec>): TransferSpec {
    return TransferSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TransferSpec>): TransferSpec {
    const message = createBaseTransferSpec();
    message.gcsDataSink = (object.gcsDataSink !== undefined && object.gcsDataSink !== null)
      ? GcsData.fromPartial(object.gcsDataSink)
      : undefined;
    message.posixDataSink = (object.posixDataSink !== undefined && object.posixDataSink !== null)
      ? PosixFilesystem.fromPartial(object.posixDataSink)
      : undefined;
    message.gcsDataSource = (object.gcsDataSource !== undefined && object.gcsDataSource !== null)
      ? GcsData.fromPartial(object.gcsDataSource)
      : undefined;
    message.awsS3DataSource = (object.awsS3DataSource !== undefined && object.awsS3DataSource !== null)
      ? AwsS3Data.fromPartial(object.awsS3DataSource)
      : undefined;
    message.httpDataSource = (object.httpDataSource !== undefined && object.httpDataSource !== null)
      ? HttpData.fromPartial(object.httpDataSource)
      : undefined;
    message.posixDataSource = (object.posixDataSource !== undefined && object.posixDataSource !== null)
      ? PosixFilesystem.fromPartial(object.posixDataSource)
      : undefined;
    message.azureBlobStorageDataSource =
      (object.azureBlobStorageDataSource !== undefined && object.azureBlobStorageDataSource !== null)
        ? AzureBlobStorageData.fromPartial(object.azureBlobStorageDataSource)
        : undefined;
    message.awsS3CompatibleDataSource =
      (object.awsS3CompatibleDataSource !== undefined && object.awsS3CompatibleDataSource !== null)
        ? AwsS3CompatibleData.fromPartial(object.awsS3CompatibleDataSource)
        : undefined;
    message.hdfsDataSource = (object.hdfsDataSource !== undefined && object.hdfsDataSource !== null)
      ? HdfsData.fromPartial(object.hdfsDataSource)
      : undefined;
    message.gcsIntermediateDataLocation =
      (object.gcsIntermediateDataLocation !== undefined && object.gcsIntermediateDataLocation !== null)
        ? GcsData.fromPartial(object.gcsIntermediateDataLocation)
        : undefined;
    message.objectConditions = (object.objectConditions !== undefined && object.objectConditions !== null)
      ? ObjectConditions.fromPartial(object.objectConditions)
      : undefined;
    message.transferOptions = (object.transferOptions !== undefined && object.transferOptions !== null)
      ? TransferOptions.fromPartial(object.transferOptions)
      : undefined;
    message.transferManifest = (object.transferManifest !== undefined && object.transferManifest !== null)
      ? TransferManifest.fromPartial(object.transferManifest)
      : undefined;
    message.sourceAgentPoolName = object.sourceAgentPoolName ?? "";
    message.sinkAgentPoolName = object.sinkAgentPoolName ?? "";
    return message;
  },
};

function createBaseMetadataOptions(): MetadataOptions {
  return { symlink: 0, mode: 0, gid: 0, uid: 0, acl: 0, storageClass: 0, temporaryHold: 0, kmsKey: 0, timeCreated: 0 };
}

export const MetadataOptions: MessageFns<MetadataOptions> = {
  encode(message: MetadataOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.symlink !== 0) {
      writer.uint32(8).int32(message.symlink);
    }
    if (message.mode !== 0) {
      writer.uint32(16).int32(message.mode);
    }
    if (message.gid !== 0) {
      writer.uint32(24).int32(message.gid);
    }
    if (message.uid !== 0) {
      writer.uint32(32).int32(message.uid);
    }
    if (message.acl !== 0) {
      writer.uint32(40).int32(message.acl);
    }
    if (message.storageClass !== 0) {
      writer.uint32(48).int32(message.storageClass);
    }
    if (message.temporaryHold !== 0) {
      writer.uint32(56).int32(message.temporaryHold);
    }
    if (message.kmsKey !== 0) {
      writer.uint32(64).int32(message.kmsKey);
    }
    if (message.timeCreated !== 0) {
      writer.uint32(72).int32(message.timeCreated);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MetadataOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMetadataOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.symlink = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.mode = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.gid = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.uid = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.acl = reader.int32() as any;
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.storageClass = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.temporaryHold = reader.int32() as any;
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.kmsKey = reader.int32() as any;
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.timeCreated = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MetadataOptions {
    return {
      symlink: isSet(object.symlink) ? metadataOptions_SymlinkFromJSON(object.symlink) : 0,
      mode: isSet(object.mode) ? metadataOptions_ModeFromJSON(object.mode) : 0,
      gid: isSet(object.gid) ? metadataOptions_GIDFromJSON(object.gid) : 0,
      uid: isSet(object.uid) ? metadataOptions_UIDFromJSON(object.uid) : 0,
      acl: isSet(object.acl) ? metadataOptions_AclFromJSON(object.acl) : 0,
      storageClass: isSet(object.storageClass) ? metadataOptions_StorageClassFromJSON(object.storageClass) : 0,
      temporaryHold: isSet(object.temporaryHold) ? metadataOptions_TemporaryHoldFromJSON(object.temporaryHold) : 0,
      kmsKey: isSet(object.kmsKey) ? metadataOptions_KmsKeyFromJSON(object.kmsKey) : 0,
      timeCreated: isSet(object.timeCreated) ? metadataOptions_TimeCreatedFromJSON(object.timeCreated) : 0,
    };
  },

  toJSON(message: MetadataOptions): unknown {
    const obj: any = {};
    if (message.symlink !== 0) {
      obj.symlink = metadataOptions_SymlinkToJSON(message.symlink);
    }
    if (message.mode !== 0) {
      obj.mode = metadataOptions_ModeToJSON(message.mode);
    }
    if (message.gid !== 0) {
      obj.gid = metadataOptions_GIDToJSON(message.gid);
    }
    if (message.uid !== 0) {
      obj.uid = metadataOptions_UIDToJSON(message.uid);
    }
    if (message.acl !== 0) {
      obj.acl = metadataOptions_AclToJSON(message.acl);
    }
    if (message.storageClass !== 0) {
      obj.storageClass = metadataOptions_StorageClassToJSON(message.storageClass);
    }
    if (message.temporaryHold !== 0) {
      obj.temporaryHold = metadataOptions_TemporaryHoldToJSON(message.temporaryHold);
    }
    if (message.kmsKey !== 0) {
      obj.kmsKey = metadataOptions_KmsKeyToJSON(message.kmsKey);
    }
    if (message.timeCreated !== 0) {
      obj.timeCreated = metadataOptions_TimeCreatedToJSON(message.timeCreated);
    }
    return obj;
  },

  create(base?: DeepPartial<MetadataOptions>): MetadataOptions {
    return MetadataOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MetadataOptions>): MetadataOptions {
    const message = createBaseMetadataOptions();
    message.symlink = object.symlink ?? 0;
    message.mode = object.mode ?? 0;
    message.gid = object.gid ?? 0;
    message.uid = object.uid ?? 0;
    message.acl = object.acl ?? 0;
    message.storageClass = object.storageClass ?? 0;
    message.temporaryHold = object.temporaryHold ?? 0;
    message.kmsKey = object.kmsKey ?? 0;
    message.timeCreated = object.timeCreated ?? 0;
    return message;
  },
};

function createBaseTransferManifest(): TransferManifest {
  return { location: "" };
}

export const TransferManifest: MessageFns<TransferManifest> = {
  encode(message: TransferManifest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.location !== "") {
      writer.uint32(10).string(message.location);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TransferManifest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTransferManifest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.location = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TransferManifest {
    return { location: isSet(object.location) ? globalThis.String(object.location) : "" };
  },

  toJSON(message: TransferManifest): unknown {
    const obj: any = {};
    if (message.location !== "") {
      obj.location = message.location;
    }
    return obj;
  },

  create(base?: DeepPartial<TransferManifest>): TransferManifest {
    return TransferManifest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TransferManifest>): TransferManifest {
    const message = createBaseTransferManifest();
    message.location = object.location ?? "";
    return message;
  },
};

function createBaseSchedule(): Schedule {
  return {
    scheduleStartDate: undefined,
    scheduleEndDate: undefined,
    startTimeOfDay: undefined,
    endTimeOfDay: undefined,
    repeatInterval: undefined,
  };
}

export const Schedule: MessageFns<Schedule> = {
  encode(message: Schedule, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.scheduleStartDate !== undefined) {
      DateMessage.encode(message.scheduleStartDate, writer.uint32(10).fork()).join();
    }
    if (message.scheduleEndDate !== undefined) {
      DateMessage.encode(message.scheduleEndDate, writer.uint32(18).fork()).join();
    }
    if (message.startTimeOfDay !== undefined) {
      TimeOfDay.encode(message.startTimeOfDay, writer.uint32(26).fork()).join();
    }
    if (message.endTimeOfDay !== undefined) {
      TimeOfDay.encode(message.endTimeOfDay, writer.uint32(34).fork()).join();
    }
    if (message.repeatInterval !== undefined) {
      Duration.encode(message.repeatInterval, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Schedule {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSchedule();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.scheduleStartDate = DateMessage.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.scheduleEndDate = DateMessage.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.startTimeOfDay = TimeOfDay.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.endTimeOfDay = TimeOfDay.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.repeatInterval = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Schedule {
    return {
      scheduleStartDate: isSet(object.scheduleStartDate) ? DateMessage.fromJSON(object.scheduleStartDate) : undefined,
      scheduleEndDate: isSet(object.scheduleEndDate) ? DateMessage.fromJSON(object.scheduleEndDate) : undefined,
      startTimeOfDay: isSet(object.startTimeOfDay) ? TimeOfDay.fromJSON(object.startTimeOfDay) : undefined,
      endTimeOfDay: isSet(object.endTimeOfDay) ? TimeOfDay.fromJSON(object.endTimeOfDay) : undefined,
      repeatInterval: isSet(object.repeatInterval) ? Duration.fromJSON(object.repeatInterval) : undefined,
    };
  },

  toJSON(message: Schedule): unknown {
    const obj: any = {};
    if (message.scheduleStartDate !== undefined) {
      obj.scheduleStartDate = DateMessage.toJSON(message.scheduleStartDate);
    }
    if (message.scheduleEndDate !== undefined) {
      obj.scheduleEndDate = DateMessage.toJSON(message.scheduleEndDate);
    }
    if (message.startTimeOfDay !== undefined) {
      obj.startTimeOfDay = TimeOfDay.toJSON(message.startTimeOfDay);
    }
    if (message.endTimeOfDay !== undefined) {
      obj.endTimeOfDay = TimeOfDay.toJSON(message.endTimeOfDay);
    }
    if (message.repeatInterval !== undefined) {
      obj.repeatInterval = Duration.toJSON(message.repeatInterval);
    }
    return obj;
  },

  create(base?: DeepPartial<Schedule>): Schedule {
    return Schedule.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Schedule>): Schedule {
    const message = createBaseSchedule();
    message.scheduleStartDate = (object.scheduleStartDate !== undefined && object.scheduleStartDate !== null)
      ? DateMessage.fromPartial(object.scheduleStartDate)
      : undefined;
    message.scheduleEndDate = (object.scheduleEndDate !== undefined && object.scheduleEndDate !== null)
      ? DateMessage.fromPartial(object.scheduleEndDate)
      : undefined;
    message.startTimeOfDay = (object.startTimeOfDay !== undefined && object.startTimeOfDay !== null)
      ? TimeOfDay.fromPartial(object.startTimeOfDay)
      : undefined;
    message.endTimeOfDay = (object.endTimeOfDay !== undefined && object.endTimeOfDay !== null)
      ? TimeOfDay.fromPartial(object.endTimeOfDay)
      : undefined;
    message.repeatInterval = (object.repeatInterval !== undefined && object.repeatInterval !== null)
      ? Duration.fromPartial(object.repeatInterval)
      : undefined;
    return message;
  },
};

function createBaseEventStream(): EventStream {
  return { name: "", eventStreamStartTime: undefined, eventStreamExpirationTime: undefined };
}

export const EventStream: MessageFns<EventStream> = {
  encode(message: EventStream, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.eventStreamStartTime !== undefined) {
      Timestamp.encode(toTimestamp(message.eventStreamStartTime), writer.uint32(18).fork()).join();
    }
    if (message.eventStreamExpirationTime !== undefined) {
      Timestamp.encode(toTimestamp(message.eventStreamExpirationTime), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EventStream {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEventStream();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.eventStreamStartTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.eventStreamExpirationTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EventStream {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      eventStreamStartTime: isSet(object.eventStreamStartTime)
        ? fromJsonTimestamp(object.eventStreamStartTime)
        : undefined,
      eventStreamExpirationTime: isSet(object.eventStreamExpirationTime)
        ? fromJsonTimestamp(object.eventStreamExpirationTime)
        : undefined,
    };
  },

  toJSON(message: EventStream): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.eventStreamStartTime !== undefined) {
      obj.eventStreamStartTime = message.eventStreamStartTime.toISOString();
    }
    if (message.eventStreamExpirationTime !== undefined) {
      obj.eventStreamExpirationTime = message.eventStreamExpirationTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<EventStream>): EventStream {
    return EventStream.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EventStream>): EventStream {
    const message = createBaseEventStream();
    message.name = object.name ?? "";
    message.eventStreamStartTime = object.eventStreamStartTime ?? undefined;
    message.eventStreamExpirationTime = object.eventStreamExpirationTime ?? undefined;
    return message;
  },
};

function createBaseTransferJob(): TransferJob {
  return {
    name: "",
    description: "",
    projectId: "",
    transferSpec: undefined,
    notificationConfig: undefined,
    loggingConfig: undefined,
    schedule: undefined,
    eventStream: undefined,
    status: 0,
    creationTime: undefined,
    lastModificationTime: undefined,
    deletionTime: undefined,
    latestOperationName: "",
  };
}

export const TransferJob: MessageFns<TransferJob> = {
  encode(message: TransferJob, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.description !== "") {
      writer.uint32(18).string(message.description);
    }
    if (message.projectId !== "") {
      writer.uint32(26).string(message.projectId);
    }
    if (message.transferSpec !== undefined) {
      TransferSpec.encode(message.transferSpec, writer.uint32(34).fork()).join();
    }
    if (message.notificationConfig !== undefined) {
      NotificationConfig.encode(message.notificationConfig, writer.uint32(90).fork()).join();
    }
    if (message.loggingConfig !== undefined) {
      LoggingConfig.encode(message.loggingConfig, writer.uint32(114).fork()).join();
    }
    if (message.schedule !== undefined) {
      Schedule.encode(message.schedule, writer.uint32(42).fork()).join();
    }
    if (message.eventStream !== undefined) {
      EventStream.encode(message.eventStream, writer.uint32(122).fork()).join();
    }
    if (message.status !== 0) {
      writer.uint32(48).int32(message.status);
    }
    if (message.creationTime !== undefined) {
      Timestamp.encode(toTimestamp(message.creationTime), writer.uint32(58).fork()).join();
    }
    if (message.lastModificationTime !== undefined) {
      Timestamp.encode(toTimestamp(message.lastModificationTime), writer.uint32(66).fork()).join();
    }
    if (message.deletionTime !== undefined) {
      Timestamp.encode(toTimestamp(message.deletionTime), writer.uint32(74).fork()).join();
    }
    if (message.latestOperationName !== "") {
      writer.uint32(98).string(message.latestOperationName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TransferJob {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTransferJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.description = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.transferSpec = TransferSpec.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.notificationConfig = NotificationConfig.decode(reader, reader.uint32());
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.loggingConfig = LoggingConfig.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.schedule = Schedule.decode(reader, reader.uint32());
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.eventStream = EventStream.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.status = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.creationTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.lastModificationTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.deletionTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.latestOperationName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TransferJob {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      transferSpec: isSet(object.transferSpec) ? TransferSpec.fromJSON(object.transferSpec) : undefined,
      notificationConfig: isSet(object.notificationConfig)
        ? NotificationConfig.fromJSON(object.notificationConfig)
        : undefined,
      loggingConfig: isSet(object.loggingConfig) ? LoggingConfig.fromJSON(object.loggingConfig) : undefined,
      schedule: isSet(object.schedule) ? Schedule.fromJSON(object.schedule) : undefined,
      eventStream: isSet(object.eventStream) ? EventStream.fromJSON(object.eventStream) : undefined,
      status: isSet(object.status) ? transferJob_StatusFromJSON(object.status) : 0,
      creationTime: isSet(object.creationTime) ? fromJsonTimestamp(object.creationTime) : undefined,
      lastModificationTime: isSet(object.lastModificationTime)
        ? fromJsonTimestamp(object.lastModificationTime)
        : undefined,
      deletionTime: isSet(object.deletionTime) ? fromJsonTimestamp(object.deletionTime) : undefined,
      latestOperationName: isSet(object.latestOperationName) ? globalThis.String(object.latestOperationName) : "",
    };
  },

  toJSON(message: TransferJob): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.transferSpec !== undefined) {
      obj.transferSpec = TransferSpec.toJSON(message.transferSpec);
    }
    if (message.notificationConfig !== undefined) {
      obj.notificationConfig = NotificationConfig.toJSON(message.notificationConfig);
    }
    if (message.loggingConfig !== undefined) {
      obj.loggingConfig = LoggingConfig.toJSON(message.loggingConfig);
    }
    if (message.schedule !== undefined) {
      obj.schedule = Schedule.toJSON(message.schedule);
    }
    if (message.eventStream !== undefined) {
      obj.eventStream = EventStream.toJSON(message.eventStream);
    }
    if (message.status !== 0) {
      obj.status = transferJob_StatusToJSON(message.status);
    }
    if (message.creationTime !== undefined) {
      obj.creationTime = message.creationTime.toISOString();
    }
    if (message.lastModificationTime !== undefined) {
      obj.lastModificationTime = message.lastModificationTime.toISOString();
    }
    if (message.deletionTime !== undefined) {
      obj.deletionTime = message.deletionTime.toISOString();
    }
    if (message.latestOperationName !== "") {
      obj.latestOperationName = message.latestOperationName;
    }
    return obj;
  },

  create(base?: DeepPartial<TransferJob>): TransferJob {
    return TransferJob.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TransferJob>): TransferJob {
    const message = createBaseTransferJob();
    message.name = object.name ?? "";
    message.description = object.description ?? "";
    message.projectId = object.projectId ?? "";
    message.transferSpec = (object.transferSpec !== undefined && object.transferSpec !== null)
      ? TransferSpec.fromPartial(object.transferSpec)
      : undefined;
    message.notificationConfig = (object.notificationConfig !== undefined && object.notificationConfig !== null)
      ? NotificationConfig.fromPartial(object.notificationConfig)
      : undefined;
    message.loggingConfig = (object.loggingConfig !== undefined && object.loggingConfig !== null)
      ? LoggingConfig.fromPartial(object.loggingConfig)
      : undefined;
    message.schedule = (object.schedule !== undefined && object.schedule !== null)
      ? Schedule.fromPartial(object.schedule)
      : undefined;
    message.eventStream = (object.eventStream !== undefined && object.eventStream !== null)
      ? EventStream.fromPartial(object.eventStream)
      : undefined;
    message.status = object.status ?? 0;
    message.creationTime = object.creationTime ?? undefined;
    message.lastModificationTime = object.lastModificationTime ?? undefined;
    message.deletionTime = object.deletionTime ?? undefined;
    message.latestOperationName = object.latestOperationName ?? "";
    return message;
  },
};

function createBaseErrorLogEntry(): ErrorLogEntry {
  return { url: "", errorDetails: [] };
}

export const ErrorLogEntry: MessageFns<ErrorLogEntry> = {
  encode(message: ErrorLogEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.url !== "") {
      writer.uint32(10).string(message.url);
    }
    for (const v of message.errorDetails) {
      writer.uint32(26).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ErrorLogEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseErrorLogEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.url = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.errorDetails.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ErrorLogEntry {
    return {
      url: isSet(object.url) ? globalThis.String(object.url) : "",
      errorDetails: globalThis.Array.isArray(object?.errorDetails)
        ? object.errorDetails.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: ErrorLogEntry): unknown {
    const obj: any = {};
    if (message.url !== "") {
      obj.url = message.url;
    }
    if (message.errorDetails?.length) {
      obj.errorDetails = message.errorDetails;
    }
    return obj;
  },

  create(base?: DeepPartial<ErrorLogEntry>): ErrorLogEntry {
    return ErrorLogEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ErrorLogEntry>): ErrorLogEntry {
    const message = createBaseErrorLogEntry();
    message.url = object.url ?? "";
    message.errorDetails = object.errorDetails?.map((e) => e) || [];
    return message;
  },
};

function createBaseErrorSummary(): ErrorSummary {
  return { errorCode: 0, errorCount: Long.ZERO, errorLogEntries: [] };
}

export const ErrorSummary: MessageFns<ErrorSummary> = {
  encode(message: ErrorSummary, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.errorCode !== 0) {
      writer.uint32(8).int32(message.errorCode);
    }
    if (!message.errorCount.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.errorCount.toString());
    }
    for (const v of message.errorLogEntries) {
      ErrorLogEntry.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ErrorSummary {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseErrorSummary();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.errorCode = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.errorCount = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.errorLogEntries.push(ErrorLogEntry.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ErrorSummary {
    return {
      errorCode: isSet(object.errorCode) ? codeFromJSON(object.errorCode) : 0,
      errorCount: isSet(object.errorCount) ? Long.fromValue(object.errorCount) : Long.ZERO,
      errorLogEntries: globalThis.Array.isArray(object?.errorLogEntries)
        ? object.errorLogEntries.map((e: any) => ErrorLogEntry.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ErrorSummary): unknown {
    const obj: any = {};
    if (message.errorCode !== 0) {
      obj.errorCode = codeToJSON(message.errorCode);
    }
    if (!message.errorCount.equals(Long.ZERO)) {
      obj.errorCount = (message.errorCount || Long.ZERO).toString();
    }
    if (message.errorLogEntries?.length) {
      obj.errorLogEntries = message.errorLogEntries.map((e) => ErrorLogEntry.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ErrorSummary>): ErrorSummary {
    return ErrorSummary.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ErrorSummary>): ErrorSummary {
    const message = createBaseErrorSummary();
    message.errorCode = object.errorCode ?? 0;
    message.errorCount = (object.errorCount !== undefined && object.errorCount !== null)
      ? Long.fromValue(object.errorCount)
      : Long.ZERO;
    message.errorLogEntries = object.errorLogEntries?.map((e) => ErrorLogEntry.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTransferCounters(): TransferCounters {
  return {
    objectsFoundFromSource: Long.ZERO,
    bytesFoundFromSource: Long.ZERO,
    objectsFoundOnlyFromSink: Long.ZERO,
    bytesFoundOnlyFromSink: Long.ZERO,
    objectsFromSourceSkippedBySync: Long.ZERO,
    bytesFromSourceSkippedBySync: Long.ZERO,
    objectsCopiedToSink: Long.ZERO,
    bytesCopiedToSink: Long.ZERO,
    objectsDeletedFromSource: Long.ZERO,
    bytesDeletedFromSource: Long.ZERO,
    objectsDeletedFromSink: Long.ZERO,
    bytesDeletedFromSink: Long.ZERO,
    objectsFromSourceFailed: Long.ZERO,
    bytesFromSourceFailed: Long.ZERO,
    objectsFailedToDeleteFromSink: Long.ZERO,
    bytesFailedToDeleteFromSink: Long.ZERO,
    directoriesFoundFromSource: Long.ZERO,
    directoriesFailedToListFromSource: Long.ZERO,
    directoriesSuccessfullyListedFromSource: Long.ZERO,
    intermediateObjectsCleanedUp: Long.ZERO,
    intermediateObjectsFailedCleanedUp: Long.ZERO,
  };
}

export const TransferCounters: MessageFns<TransferCounters> = {
  encode(message: TransferCounters, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.objectsFoundFromSource.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.objectsFoundFromSource.toString());
    }
    if (!message.bytesFoundFromSource.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.bytesFoundFromSource.toString());
    }
    if (!message.objectsFoundOnlyFromSink.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.objectsFoundOnlyFromSink.toString());
    }
    if (!message.bytesFoundOnlyFromSink.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.bytesFoundOnlyFromSink.toString());
    }
    if (!message.objectsFromSourceSkippedBySync.equals(Long.ZERO)) {
      writer.uint32(40).int64(message.objectsFromSourceSkippedBySync.toString());
    }
    if (!message.bytesFromSourceSkippedBySync.equals(Long.ZERO)) {
      writer.uint32(48).int64(message.bytesFromSourceSkippedBySync.toString());
    }
    if (!message.objectsCopiedToSink.equals(Long.ZERO)) {
      writer.uint32(56).int64(message.objectsCopiedToSink.toString());
    }
    if (!message.bytesCopiedToSink.equals(Long.ZERO)) {
      writer.uint32(64).int64(message.bytesCopiedToSink.toString());
    }
    if (!message.objectsDeletedFromSource.equals(Long.ZERO)) {
      writer.uint32(72).int64(message.objectsDeletedFromSource.toString());
    }
    if (!message.bytesDeletedFromSource.equals(Long.ZERO)) {
      writer.uint32(80).int64(message.bytesDeletedFromSource.toString());
    }
    if (!message.objectsDeletedFromSink.equals(Long.ZERO)) {
      writer.uint32(88).int64(message.objectsDeletedFromSink.toString());
    }
    if (!message.bytesDeletedFromSink.equals(Long.ZERO)) {
      writer.uint32(96).int64(message.bytesDeletedFromSink.toString());
    }
    if (!message.objectsFromSourceFailed.equals(Long.ZERO)) {
      writer.uint32(104).int64(message.objectsFromSourceFailed.toString());
    }
    if (!message.bytesFromSourceFailed.equals(Long.ZERO)) {
      writer.uint32(112).int64(message.bytesFromSourceFailed.toString());
    }
    if (!message.objectsFailedToDeleteFromSink.equals(Long.ZERO)) {
      writer.uint32(120).int64(message.objectsFailedToDeleteFromSink.toString());
    }
    if (!message.bytesFailedToDeleteFromSink.equals(Long.ZERO)) {
      writer.uint32(128).int64(message.bytesFailedToDeleteFromSink.toString());
    }
    if (!message.directoriesFoundFromSource.equals(Long.ZERO)) {
      writer.uint32(136).int64(message.directoriesFoundFromSource.toString());
    }
    if (!message.directoriesFailedToListFromSource.equals(Long.ZERO)) {
      writer.uint32(144).int64(message.directoriesFailedToListFromSource.toString());
    }
    if (!message.directoriesSuccessfullyListedFromSource.equals(Long.ZERO)) {
      writer.uint32(152).int64(message.directoriesSuccessfullyListedFromSource.toString());
    }
    if (!message.intermediateObjectsCleanedUp.equals(Long.ZERO)) {
      writer.uint32(176).int64(message.intermediateObjectsCleanedUp.toString());
    }
    if (!message.intermediateObjectsFailedCleanedUp.equals(Long.ZERO)) {
      writer.uint32(184).int64(message.intermediateObjectsFailedCleanedUp.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TransferCounters {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTransferCounters();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.objectsFoundFromSource = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.bytesFoundFromSource = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.objectsFoundOnlyFromSink = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.bytesFoundOnlyFromSink = Long.fromString(reader.int64().toString());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.objectsFromSourceSkippedBySync = Long.fromString(reader.int64().toString());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.bytesFromSourceSkippedBySync = Long.fromString(reader.int64().toString());
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.objectsCopiedToSink = Long.fromString(reader.int64().toString());
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.bytesCopiedToSink = Long.fromString(reader.int64().toString());
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.objectsDeletedFromSource = Long.fromString(reader.int64().toString());
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.bytesDeletedFromSource = Long.fromString(reader.int64().toString());
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.objectsDeletedFromSink = Long.fromString(reader.int64().toString());
          continue;
        case 12:
          if (tag !== 96) {
            break;
          }

          message.bytesDeletedFromSink = Long.fromString(reader.int64().toString());
          continue;
        case 13:
          if (tag !== 104) {
            break;
          }

          message.objectsFromSourceFailed = Long.fromString(reader.int64().toString());
          continue;
        case 14:
          if (tag !== 112) {
            break;
          }

          message.bytesFromSourceFailed = Long.fromString(reader.int64().toString());
          continue;
        case 15:
          if (tag !== 120) {
            break;
          }

          message.objectsFailedToDeleteFromSink = Long.fromString(reader.int64().toString());
          continue;
        case 16:
          if (tag !== 128) {
            break;
          }

          message.bytesFailedToDeleteFromSink = Long.fromString(reader.int64().toString());
          continue;
        case 17:
          if (tag !== 136) {
            break;
          }

          message.directoriesFoundFromSource = Long.fromString(reader.int64().toString());
          continue;
        case 18:
          if (tag !== 144) {
            break;
          }

          message.directoriesFailedToListFromSource = Long.fromString(reader.int64().toString());
          continue;
        case 19:
          if (tag !== 152) {
            break;
          }

          message.directoriesSuccessfullyListedFromSource = Long.fromString(reader.int64().toString());
          continue;
        case 22:
          if (tag !== 176) {
            break;
          }

          message.intermediateObjectsCleanedUp = Long.fromString(reader.int64().toString());
          continue;
        case 23:
          if (tag !== 184) {
            break;
          }

          message.intermediateObjectsFailedCleanedUp = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TransferCounters {
    return {
      objectsFoundFromSource: isSet(object.objectsFoundFromSource)
        ? Long.fromValue(object.objectsFoundFromSource)
        : Long.ZERO,
      bytesFoundFromSource: isSet(object.bytesFoundFromSource)
        ? Long.fromValue(object.bytesFoundFromSource)
        : Long.ZERO,
      objectsFoundOnlyFromSink: isSet(object.objectsFoundOnlyFromSink)
        ? Long.fromValue(object.objectsFoundOnlyFromSink)
        : Long.ZERO,
      bytesFoundOnlyFromSink: isSet(object.bytesFoundOnlyFromSink)
        ? Long.fromValue(object.bytesFoundOnlyFromSink)
        : Long.ZERO,
      objectsFromSourceSkippedBySync: isSet(object.objectsFromSourceSkippedBySync)
        ? Long.fromValue(object.objectsFromSourceSkippedBySync)
        : Long.ZERO,
      bytesFromSourceSkippedBySync: isSet(object.bytesFromSourceSkippedBySync)
        ? Long.fromValue(object.bytesFromSourceSkippedBySync)
        : Long.ZERO,
      objectsCopiedToSink: isSet(object.objectsCopiedToSink) ? Long.fromValue(object.objectsCopiedToSink) : Long.ZERO,
      bytesCopiedToSink: isSet(object.bytesCopiedToSink) ? Long.fromValue(object.bytesCopiedToSink) : Long.ZERO,
      objectsDeletedFromSource: isSet(object.objectsDeletedFromSource)
        ? Long.fromValue(object.objectsDeletedFromSource)
        : Long.ZERO,
      bytesDeletedFromSource: isSet(object.bytesDeletedFromSource)
        ? Long.fromValue(object.bytesDeletedFromSource)
        : Long.ZERO,
      objectsDeletedFromSink: isSet(object.objectsDeletedFromSink)
        ? Long.fromValue(object.objectsDeletedFromSink)
        : Long.ZERO,
      bytesDeletedFromSink: isSet(object.bytesDeletedFromSink)
        ? Long.fromValue(object.bytesDeletedFromSink)
        : Long.ZERO,
      objectsFromSourceFailed: isSet(object.objectsFromSourceFailed)
        ? Long.fromValue(object.objectsFromSourceFailed)
        : Long.ZERO,
      bytesFromSourceFailed: isSet(object.bytesFromSourceFailed)
        ? Long.fromValue(object.bytesFromSourceFailed)
        : Long.ZERO,
      objectsFailedToDeleteFromSink: isSet(object.objectsFailedToDeleteFromSink)
        ? Long.fromValue(object.objectsFailedToDeleteFromSink)
        : Long.ZERO,
      bytesFailedToDeleteFromSink: isSet(object.bytesFailedToDeleteFromSink)
        ? Long.fromValue(object.bytesFailedToDeleteFromSink)
        : Long.ZERO,
      directoriesFoundFromSource: isSet(object.directoriesFoundFromSource)
        ? Long.fromValue(object.directoriesFoundFromSource)
        : Long.ZERO,
      directoriesFailedToListFromSource: isSet(object.directoriesFailedToListFromSource)
        ? Long.fromValue(object.directoriesFailedToListFromSource)
        : Long.ZERO,
      directoriesSuccessfullyListedFromSource: isSet(object.directoriesSuccessfullyListedFromSource)
        ? Long.fromValue(object.directoriesSuccessfullyListedFromSource)
        : Long.ZERO,
      intermediateObjectsCleanedUp: isSet(object.intermediateObjectsCleanedUp)
        ? Long.fromValue(object.intermediateObjectsCleanedUp)
        : Long.ZERO,
      intermediateObjectsFailedCleanedUp: isSet(object.intermediateObjectsFailedCleanedUp)
        ? Long.fromValue(object.intermediateObjectsFailedCleanedUp)
        : Long.ZERO,
    };
  },

  toJSON(message: TransferCounters): unknown {
    const obj: any = {};
    if (!message.objectsFoundFromSource.equals(Long.ZERO)) {
      obj.objectsFoundFromSource = (message.objectsFoundFromSource || Long.ZERO).toString();
    }
    if (!message.bytesFoundFromSource.equals(Long.ZERO)) {
      obj.bytesFoundFromSource = (message.bytesFoundFromSource || Long.ZERO).toString();
    }
    if (!message.objectsFoundOnlyFromSink.equals(Long.ZERO)) {
      obj.objectsFoundOnlyFromSink = (message.objectsFoundOnlyFromSink || Long.ZERO).toString();
    }
    if (!message.bytesFoundOnlyFromSink.equals(Long.ZERO)) {
      obj.bytesFoundOnlyFromSink = (message.bytesFoundOnlyFromSink || Long.ZERO).toString();
    }
    if (!message.objectsFromSourceSkippedBySync.equals(Long.ZERO)) {
      obj.objectsFromSourceSkippedBySync = (message.objectsFromSourceSkippedBySync || Long.ZERO).toString();
    }
    if (!message.bytesFromSourceSkippedBySync.equals(Long.ZERO)) {
      obj.bytesFromSourceSkippedBySync = (message.bytesFromSourceSkippedBySync || Long.ZERO).toString();
    }
    if (!message.objectsCopiedToSink.equals(Long.ZERO)) {
      obj.objectsCopiedToSink = (message.objectsCopiedToSink || Long.ZERO).toString();
    }
    if (!message.bytesCopiedToSink.equals(Long.ZERO)) {
      obj.bytesCopiedToSink = (message.bytesCopiedToSink || Long.ZERO).toString();
    }
    if (!message.objectsDeletedFromSource.equals(Long.ZERO)) {
      obj.objectsDeletedFromSource = (message.objectsDeletedFromSource || Long.ZERO).toString();
    }
    if (!message.bytesDeletedFromSource.equals(Long.ZERO)) {
      obj.bytesDeletedFromSource = (message.bytesDeletedFromSource || Long.ZERO).toString();
    }
    if (!message.objectsDeletedFromSink.equals(Long.ZERO)) {
      obj.objectsDeletedFromSink = (message.objectsDeletedFromSink || Long.ZERO).toString();
    }
    if (!message.bytesDeletedFromSink.equals(Long.ZERO)) {
      obj.bytesDeletedFromSink = (message.bytesDeletedFromSink || Long.ZERO).toString();
    }
    if (!message.objectsFromSourceFailed.equals(Long.ZERO)) {
      obj.objectsFromSourceFailed = (message.objectsFromSourceFailed || Long.ZERO).toString();
    }
    if (!message.bytesFromSourceFailed.equals(Long.ZERO)) {
      obj.bytesFromSourceFailed = (message.bytesFromSourceFailed || Long.ZERO).toString();
    }
    if (!message.objectsFailedToDeleteFromSink.equals(Long.ZERO)) {
      obj.objectsFailedToDeleteFromSink = (message.objectsFailedToDeleteFromSink || Long.ZERO).toString();
    }
    if (!message.bytesFailedToDeleteFromSink.equals(Long.ZERO)) {
      obj.bytesFailedToDeleteFromSink = (message.bytesFailedToDeleteFromSink || Long.ZERO).toString();
    }
    if (!message.directoriesFoundFromSource.equals(Long.ZERO)) {
      obj.directoriesFoundFromSource = (message.directoriesFoundFromSource || Long.ZERO).toString();
    }
    if (!message.directoriesFailedToListFromSource.equals(Long.ZERO)) {
      obj.directoriesFailedToListFromSource = (message.directoriesFailedToListFromSource || Long.ZERO).toString();
    }
    if (!message.directoriesSuccessfullyListedFromSource.equals(Long.ZERO)) {
      obj.directoriesSuccessfullyListedFromSource = (message.directoriesSuccessfullyListedFromSource || Long.ZERO)
        .toString();
    }
    if (!message.intermediateObjectsCleanedUp.equals(Long.ZERO)) {
      obj.intermediateObjectsCleanedUp = (message.intermediateObjectsCleanedUp || Long.ZERO).toString();
    }
    if (!message.intermediateObjectsFailedCleanedUp.equals(Long.ZERO)) {
      obj.intermediateObjectsFailedCleanedUp = (message.intermediateObjectsFailedCleanedUp || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<TransferCounters>): TransferCounters {
    return TransferCounters.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TransferCounters>): TransferCounters {
    const message = createBaseTransferCounters();
    message.objectsFoundFromSource =
      (object.objectsFoundFromSource !== undefined && object.objectsFoundFromSource !== null)
        ? Long.fromValue(object.objectsFoundFromSource)
        : Long.ZERO;
    message.bytesFoundFromSource = (object.bytesFoundFromSource !== undefined && object.bytesFoundFromSource !== null)
      ? Long.fromValue(object.bytesFoundFromSource)
      : Long.ZERO;
    message.objectsFoundOnlyFromSink =
      (object.objectsFoundOnlyFromSink !== undefined && object.objectsFoundOnlyFromSink !== null)
        ? Long.fromValue(object.objectsFoundOnlyFromSink)
        : Long.ZERO;
    message.bytesFoundOnlyFromSink =
      (object.bytesFoundOnlyFromSink !== undefined && object.bytesFoundOnlyFromSink !== null)
        ? Long.fromValue(object.bytesFoundOnlyFromSink)
        : Long.ZERO;
    message.objectsFromSourceSkippedBySync =
      (object.objectsFromSourceSkippedBySync !== undefined && object.objectsFromSourceSkippedBySync !== null)
        ? Long.fromValue(object.objectsFromSourceSkippedBySync)
        : Long.ZERO;
    message.bytesFromSourceSkippedBySync =
      (object.bytesFromSourceSkippedBySync !== undefined && object.bytesFromSourceSkippedBySync !== null)
        ? Long.fromValue(object.bytesFromSourceSkippedBySync)
        : Long.ZERO;
    message.objectsCopiedToSink = (object.objectsCopiedToSink !== undefined && object.objectsCopiedToSink !== null)
      ? Long.fromValue(object.objectsCopiedToSink)
      : Long.ZERO;
    message.bytesCopiedToSink = (object.bytesCopiedToSink !== undefined && object.bytesCopiedToSink !== null)
      ? Long.fromValue(object.bytesCopiedToSink)
      : Long.ZERO;
    message.objectsDeletedFromSource =
      (object.objectsDeletedFromSource !== undefined && object.objectsDeletedFromSource !== null)
        ? Long.fromValue(object.objectsDeletedFromSource)
        : Long.ZERO;
    message.bytesDeletedFromSource =
      (object.bytesDeletedFromSource !== undefined && object.bytesDeletedFromSource !== null)
        ? Long.fromValue(object.bytesDeletedFromSource)
        : Long.ZERO;
    message.objectsDeletedFromSink =
      (object.objectsDeletedFromSink !== undefined && object.objectsDeletedFromSink !== null)
        ? Long.fromValue(object.objectsDeletedFromSink)
        : Long.ZERO;
    message.bytesDeletedFromSink = (object.bytesDeletedFromSink !== undefined && object.bytesDeletedFromSink !== null)
      ? Long.fromValue(object.bytesDeletedFromSink)
      : Long.ZERO;
    message.objectsFromSourceFailed =
      (object.objectsFromSourceFailed !== undefined && object.objectsFromSourceFailed !== null)
        ? Long.fromValue(object.objectsFromSourceFailed)
        : Long.ZERO;
    message.bytesFromSourceFailed =
      (object.bytesFromSourceFailed !== undefined && object.bytesFromSourceFailed !== null)
        ? Long.fromValue(object.bytesFromSourceFailed)
        : Long.ZERO;
    message.objectsFailedToDeleteFromSink =
      (object.objectsFailedToDeleteFromSink !== undefined && object.objectsFailedToDeleteFromSink !== null)
        ? Long.fromValue(object.objectsFailedToDeleteFromSink)
        : Long.ZERO;
    message.bytesFailedToDeleteFromSink =
      (object.bytesFailedToDeleteFromSink !== undefined && object.bytesFailedToDeleteFromSink !== null)
        ? Long.fromValue(object.bytesFailedToDeleteFromSink)
        : Long.ZERO;
    message.directoriesFoundFromSource =
      (object.directoriesFoundFromSource !== undefined && object.directoriesFoundFromSource !== null)
        ? Long.fromValue(object.directoriesFoundFromSource)
        : Long.ZERO;
    message.directoriesFailedToListFromSource =
      (object.directoriesFailedToListFromSource !== undefined && object.directoriesFailedToListFromSource !== null)
        ? Long.fromValue(object.directoriesFailedToListFromSource)
        : Long.ZERO;
    message.directoriesSuccessfullyListedFromSource =
      (object.directoriesSuccessfullyListedFromSource !== undefined &&
          object.directoriesSuccessfullyListedFromSource !== null)
        ? Long.fromValue(object.directoriesSuccessfullyListedFromSource)
        : Long.ZERO;
    message.intermediateObjectsCleanedUp =
      (object.intermediateObjectsCleanedUp !== undefined && object.intermediateObjectsCleanedUp !== null)
        ? Long.fromValue(object.intermediateObjectsCleanedUp)
        : Long.ZERO;
    message.intermediateObjectsFailedCleanedUp =
      (object.intermediateObjectsFailedCleanedUp !== undefined && object.intermediateObjectsFailedCleanedUp !== null)
        ? Long.fromValue(object.intermediateObjectsFailedCleanedUp)
        : Long.ZERO;
    return message;
  },
};

function createBaseNotificationConfig(): NotificationConfig {
  return { pubsubTopic: "", eventTypes: [], payloadFormat: 0 };
}

export const NotificationConfig: MessageFns<NotificationConfig> = {
  encode(message: NotificationConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.pubsubTopic !== "") {
      writer.uint32(10).string(message.pubsubTopic);
    }
    writer.uint32(18).fork();
    for (const v of message.eventTypes) {
      writer.int32(v);
    }
    writer.join();
    if (message.payloadFormat !== 0) {
      writer.uint32(24).int32(message.payloadFormat);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NotificationConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNotificationConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.pubsubTopic = reader.string();
          continue;
        case 2:
          if (tag === 16) {
            message.eventTypes.push(reader.int32() as any);

            continue;
          }

          if (tag === 18) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.eventTypes.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.payloadFormat = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NotificationConfig {
    return {
      pubsubTopic: isSet(object.pubsubTopic) ? globalThis.String(object.pubsubTopic) : "",
      eventTypes: globalThis.Array.isArray(object?.eventTypes)
        ? object.eventTypes.map((e: any) => notificationConfig_EventTypeFromJSON(e))
        : [],
      payloadFormat: isSet(object.payloadFormat) ? notificationConfig_PayloadFormatFromJSON(object.payloadFormat) : 0,
    };
  },

  toJSON(message: NotificationConfig): unknown {
    const obj: any = {};
    if (message.pubsubTopic !== "") {
      obj.pubsubTopic = message.pubsubTopic;
    }
    if (message.eventTypes?.length) {
      obj.eventTypes = message.eventTypes.map((e) => notificationConfig_EventTypeToJSON(e));
    }
    if (message.payloadFormat !== 0) {
      obj.payloadFormat = notificationConfig_PayloadFormatToJSON(message.payloadFormat);
    }
    return obj;
  },

  create(base?: DeepPartial<NotificationConfig>): NotificationConfig {
    return NotificationConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<NotificationConfig>): NotificationConfig {
    const message = createBaseNotificationConfig();
    message.pubsubTopic = object.pubsubTopic ?? "";
    message.eventTypes = object.eventTypes?.map((e) => e) || [];
    message.payloadFormat = object.payloadFormat ?? 0;
    return message;
  },
};

function createBaseLoggingConfig(): LoggingConfig {
  return { logActions: [], logActionStates: [], enableOnpremGcsTransferLogs: false };
}

export const LoggingConfig: MessageFns<LoggingConfig> = {
  encode(message: LoggingConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    writer.uint32(10).fork();
    for (const v of message.logActions) {
      writer.int32(v);
    }
    writer.join();
    writer.uint32(18).fork();
    for (const v of message.logActionStates) {
      writer.int32(v);
    }
    writer.join();
    if (message.enableOnpremGcsTransferLogs !== false) {
      writer.uint32(24).bool(message.enableOnpremGcsTransferLogs);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LoggingConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLoggingConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag === 8) {
            message.logActions.push(reader.int32() as any);

            continue;
          }

          if (tag === 10) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.logActions.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 2:
          if (tag === 16) {
            message.logActionStates.push(reader.int32() as any);

            continue;
          }

          if (tag === 18) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.logActionStates.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.enableOnpremGcsTransferLogs = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LoggingConfig {
    return {
      logActions: globalThis.Array.isArray(object?.logActions)
        ? object.logActions.map((e: any) => loggingConfig_LoggableActionFromJSON(e))
        : [],
      logActionStates: globalThis.Array.isArray(object?.logActionStates)
        ? object.logActionStates.map((e: any) => loggingConfig_LoggableActionStateFromJSON(e))
        : [],
      enableOnpremGcsTransferLogs: isSet(object.enableOnpremGcsTransferLogs)
        ? globalThis.Boolean(object.enableOnpremGcsTransferLogs)
        : false,
    };
  },

  toJSON(message: LoggingConfig): unknown {
    const obj: any = {};
    if (message.logActions?.length) {
      obj.logActions = message.logActions.map((e) => loggingConfig_LoggableActionToJSON(e));
    }
    if (message.logActionStates?.length) {
      obj.logActionStates = message.logActionStates.map((e) => loggingConfig_LoggableActionStateToJSON(e));
    }
    if (message.enableOnpremGcsTransferLogs !== false) {
      obj.enableOnpremGcsTransferLogs = message.enableOnpremGcsTransferLogs;
    }
    return obj;
  },

  create(base?: DeepPartial<LoggingConfig>): LoggingConfig {
    return LoggingConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LoggingConfig>): LoggingConfig {
    const message = createBaseLoggingConfig();
    message.logActions = object.logActions?.map((e) => e) || [];
    message.logActionStates = object.logActionStates?.map((e) => e) || [];
    message.enableOnpremGcsTransferLogs = object.enableOnpremGcsTransferLogs ?? false;
    return message;
  },
};

function createBaseTransferOperation(): TransferOperation {
  return {
    name: "",
    projectId: "",
    transferSpec: undefined,
    notificationConfig: undefined,
    loggingConfig: undefined,
    startTime: undefined,
    endTime: undefined,
    status: 0,
    counters: undefined,
    errorBreakdowns: [],
    transferJobName: "",
  };
}

export const TransferOperation: MessageFns<TransferOperation> = {
  encode(message: TransferOperation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.projectId !== "") {
      writer.uint32(18).string(message.projectId);
    }
    if (message.transferSpec !== undefined) {
      TransferSpec.encode(message.transferSpec, writer.uint32(26).fork()).join();
    }
    if (message.notificationConfig !== undefined) {
      NotificationConfig.encode(message.notificationConfig, writer.uint32(82).fork()).join();
    }
    if (message.loggingConfig !== undefined) {
      LoggingConfig.encode(message.loggingConfig, writer.uint32(98).fork()).join();
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(34).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(42).fork()).join();
    }
    if (message.status !== 0) {
      writer.uint32(48).int32(message.status);
    }
    if (message.counters !== undefined) {
      TransferCounters.encode(message.counters, writer.uint32(58).fork()).join();
    }
    for (const v of message.errorBreakdowns) {
      ErrorSummary.encode(v!, writer.uint32(66).fork()).join();
    }
    if (message.transferJobName !== "") {
      writer.uint32(74).string(message.transferJobName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TransferOperation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTransferOperation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.transferSpec = TransferSpec.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.notificationConfig = NotificationConfig.decode(reader, reader.uint32());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.loggingConfig = LoggingConfig.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.status = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.counters = TransferCounters.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.errorBreakdowns.push(ErrorSummary.decode(reader, reader.uint32()));
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.transferJobName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TransferOperation {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      transferSpec: isSet(object.transferSpec) ? TransferSpec.fromJSON(object.transferSpec) : undefined,
      notificationConfig: isSet(object.notificationConfig)
        ? NotificationConfig.fromJSON(object.notificationConfig)
        : undefined,
      loggingConfig: isSet(object.loggingConfig) ? LoggingConfig.fromJSON(object.loggingConfig) : undefined,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      status: isSet(object.status) ? transferOperation_StatusFromJSON(object.status) : 0,
      counters: isSet(object.counters) ? TransferCounters.fromJSON(object.counters) : undefined,
      errorBreakdowns: globalThis.Array.isArray(object?.errorBreakdowns)
        ? object.errorBreakdowns.map((e: any) => ErrorSummary.fromJSON(e))
        : [],
      transferJobName: isSet(object.transferJobName) ? globalThis.String(object.transferJobName) : "",
    };
  },

  toJSON(message: TransferOperation): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.transferSpec !== undefined) {
      obj.transferSpec = TransferSpec.toJSON(message.transferSpec);
    }
    if (message.notificationConfig !== undefined) {
      obj.notificationConfig = NotificationConfig.toJSON(message.notificationConfig);
    }
    if (message.loggingConfig !== undefined) {
      obj.loggingConfig = LoggingConfig.toJSON(message.loggingConfig);
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (message.status !== 0) {
      obj.status = transferOperation_StatusToJSON(message.status);
    }
    if (message.counters !== undefined) {
      obj.counters = TransferCounters.toJSON(message.counters);
    }
    if (message.errorBreakdowns?.length) {
      obj.errorBreakdowns = message.errorBreakdowns.map((e) => ErrorSummary.toJSON(e));
    }
    if (message.transferJobName !== "") {
      obj.transferJobName = message.transferJobName;
    }
    return obj;
  },

  create(base?: DeepPartial<TransferOperation>): TransferOperation {
    return TransferOperation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TransferOperation>): TransferOperation {
    const message = createBaseTransferOperation();
    message.name = object.name ?? "";
    message.projectId = object.projectId ?? "";
    message.transferSpec = (object.transferSpec !== undefined && object.transferSpec !== null)
      ? TransferSpec.fromPartial(object.transferSpec)
      : undefined;
    message.notificationConfig = (object.notificationConfig !== undefined && object.notificationConfig !== null)
      ? NotificationConfig.fromPartial(object.notificationConfig)
      : undefined;
    message.loggingConfig = (object.loggingConfig !== undefined && object.loggingConfig !== null)
      ? LoggingConfig.fromPartial(object.loggingConfig)
      : undefined;
    message.startTime = object.startTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    message.status = object.status ?? 0;
    message.counters = (object.counters !== undefined && object.counters !== null)
      ? TransferCounters.fromPartial(object.counters)
      : undefined;
    message.errorBreakdowns = object.errorBreakdowns?.map((e) => ErrorSummary.fromPartial(e)) || [];
    message.transferJobName = object.transferJobName ?? "";
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
