// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/bigtable/v2/data.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Timestamp } from "../../protobuf/timestamp.js";
import { DateMessage } from "../../type/date.js";
import { Type } from "./types.js";

export const protobufPackage = "google.bigtable.v2";

/**
 * Specifies the complete (requested) contents of a single row of a table.
 * Rows which exceed 256MiB in size cannot be read in full.
 */
export interface Row {
  /**
   * The unique key which identifies this row within its table. This is the same
   * key that's used to identify the row in, for example, a MutateRowRequest.
   * May contain any non-empty byte string up to 4KiB in length.
   */
  key: Buffer;
  /**
   * May be empty, but only if the entire row is empty.
   * The mutual ordering of column families is not specified.
   */
  families: Family[];
}

/**
 * Specifies (some of) the contents of a single row/column family intersection
 * of a table.
 */
export interface Family {
  /**
   * The unique key which identifies this family within its row. This is the
   * same key that's used to identify the family in, for example, a RowFilter
   * which sets its "family_name_regex_filter" field.
   * Must match `[-_.a-zA-Z0-9]+`, except that AggregatingRowProcessors may
   * produce cells in a sentinel family with an empty name.
   * Must be no greater than 64 characters in length.
   */
  name: string;
  /** Must not be empty. Sorted in order of increasing "qualifier". */
  columns: Column[];
}

/**
 * Specifies (some of) the contents of a single row/column intersection of a
 * table.
 */
export interface Column {
  /**
   * The unique key which identifies this column within its family. This is the
   * same key that's used to identify the column in, for example, a RowFilter
   * which sets its `column_qualifier_regex_filter` field.
   * May contain any byte string, including the empty string, up to 16kiB in
   * length.
   */
  qualifier: Buffer;
  /** Must not be empty. Sorted in order of decreasing "timestamp_micros". */
  cells: Cell[];
}

/** Specifies (some of) the contents of a single row/column/timestamp of a table. */
export interface Cell {
  /**
   * The cell's stored timestamp, which also uniquely identifies it within
   * its column.
   * Values are always expressed in microseconds, but individual tables may set
   * a coarser granularity to further restrict the allowed values. For
   * example, a table which specifies millisecond granularity will only allow
   * values of `timestamp_micros` which are multiples of 1000.
   */
  timestampMicros: Long;
  /**
   * The value stored in the cell.
   * May contain any byte string, including the empty string, up to 100MiB in
   * length.
   */
  value: Buffer;
  /** Labels applied to the cell by a [RowFilter][google.bigtable.v2.RowFilter]. */
  labels: string[];
}

/**
 * `Value` represents a dynamically typed value.
 * The typed fields in `Value` are used as a transport encoding for the actual
 * value (which may be of a more complex type). See the documentation of the
 * `Type` message for more details.
 */
export interface Value {
  /**
   * The verified `Type` of this `Value`, if it cannot be inferred.
   *
   * Read results will never specify the encoding for `type` since the value
   * will already have been decoded by the server. Furthermore, the `type` will
   * be omitted entirely if it can be inferred from a previous response. The
   * exact semantics for inferring `type` will vary, and are therefore
   * documented separately for each read method.
   *
   * When using composite types (Struct, Array, Map) only the outermost `Value`
   * will specify the `type`. This top-level `type` will define the types for
   * any nested `Struct' fields, `Array` elements, or `Map` key/value pairs.
   * If a nested `Value` provides a `type` on write, the request will be
   * rejected with INVALID_ARGUMENT.
   */
  type:
    | Type
    | undefined;
  /**
   * Represents a raw byte sequence with no type information.
   * The `type` field must be omitted.
   */
  rawValue?:
    | Buffer
    | undefined;
  /**
   * Represents a raw cell timestamp with no type information.
   * The `type` field must be omitted.
   */
  rawTimestampMicros?:
    | Long
    | undefined;
  /** Represents a typed value transported as a byte sequence. */
  bytesValue?:
    | Buffer
    | undefined;
  /** Represents a typed value transported as a string. */
  stringValue?:
    | string
    | undefined;
  /** Represents a typed value transported as an integer. */
  intValue?:
    | Long
    | undefined;
  /** Represents a typed value transported as a boolean. */
  boolValue?:
    | boolean
    | undefined;
  /** Represents a typed value transported as a floating point number. */
  floatValue?:
    | number
    | undefined;
  /** Represents a typed value transported as a timestamp. */
  timestampValue?:
    | Date
    | undefined;
  /** Represents a typed value transported as a date. */
  dateValue?:
    | DateMessage
    | undefined;
  /**
   * Represents a typed value transported as a sequence of values.
   * To differentiate between `Struct`, `Array`, and `Map`, the outermost
   * `Value` must provide an explicit `type` on write. This `type` will
   * apply recursively to the nested `Struct` fields, `Array` elements,
   * or `Map` key/value pairs, which *must not* supply their own `type`.
   */
  arrayValue?: ArrayValue | undefined;
}

/** `ArrayValue` is an ordered list of `Value`. */
export interface ArrayValue {
  /** The ordered elements in the array. */
  values: Value[];
}

/** Specifies a contiguous range of rows. */
export interface RowRange {
  /** Used when giving an inclusive lower bound for the range. */
  startKeyClosed?:
    | Buffer
    | undefined;
  /** Used when giving an exclusive lower bound for the range. */
  startKeyOpen?:
    | Buffer
    | undefined;
  /** Used when giving an exclusive upper bound for the range. */
  endKeyOpen?:
    | Buffer
    | undefined;
  /** Used when giving an inclusive upper bound for the range. */
  endKeyClosed?: Buffer | undefined;
}

/** Specifies a non-contiguous set of rows. */
export interface RowSet {
  /** Single rows included in the set. */
  rowKeys: Buffer[];
  /** Contiguous row ranges included in the set. */
  rowRanges: RowRange[];
}

/**
 * Specifies a contiguous range of columns within a single column family.
 * The range spans from &lt;column_family&gt;:&lt;start_qualifier&gt; to
 * &lt;column_family&gt;:&lt;end_qualifier&gt;, where both bounds can be either
 * inclusive or exclusive.
 */
export interface ColumnRange {
  /** The name of the column family within which this range falls. */
  familyName: string;
  /** Used when giving an inclusive lower bound for the range. */
  startQualifierClosed?:
    | Buffer
    | undefined;
  /** Used when giving an exclusive lower bound for the range. */
  startQualifierOpen?:
    | Buffer
    | undefined;
  /** Used when giving an inclusive upper bound for the range. */
  endQualifierClosed?:
    | Buffer
    | undefined;
  /** Used when giving an exclusive upper bound for the range. */
  endQualifierOpen?: Buffer | undefined;
}

/** Specified a contiguous range of microsecond timestamps. */
export interface TimestampRange {
  /** Inclusive lower bound. If left empty, interpreted as 0. */
  startTimestampMicros: Long;
  /** Exclusive upper bound. If left empty, interpreted as infinity. */
  endTimestampMicros: Long;
}

/** Specifies a contiguous range of raw byte values. */
export interface ValueRange {
  /** Used when giving an inclusive lower bound for the range. */
  startValueClosed?:
    | Buffer
    | undefined;
  /** Used when giving an exclusive lower bound for the range. */
  startValueOpen?:
    | Buffer
    | undefined;
  /** Used when giving an inclusive upper bound for the range. */
  endValueClosed?:
    | Buffer
    | undefined;
  /** Used when giving an exclusive upper bound for the range. */
  endValueOpen?: Buffer | undefined;
}

/**
 * Takes a row as input and produces an alternate view of the row based on
 * specified rules. For example, a RowFilter might trim down a row to include
 * just the cells from columns matching a given regular expression, or might
 * return all the cells of a row but not their values. More complicated filters
 * can be composed out of these components to express requests such as, "within
 * every column of a particular family, give just the two most recent cells
 * which are older than timestamp X."
 *
 * There are two broad categories of RowFilters (true filters and transformers),
 * as well as two ways to compose simple filters into more complex ones
 * (chains and interleaves). They work as follows:
 *
 * * True filters alter the input row by excluding some of its cells wholesale
 * from the output row. An example of a true filter is the `value_regex_filter`,
 * which excludes cells whose values don't match the specified pattern. All
 * regex true filters use RE2 syntax (https://github.com/google/re2/wiki/Syntax)
 * in raw byte mode (RE2::Latin1), and are evaluated as full matches. An
 * important point to keep in mind is that `RE2(.)` is equivalent by default to
 * `RE2([^\n])`, meaning that it does not match newlines. When attempting to
 * match an arbitrary byte, you should therefore use the escape sequence `\C`,
 * which may need to be further escaped as `\\C` in your client language.
 *
 * * Transformers alter the input row by changing the values of some of its
 * cells in the output, without excluding them completely. Currently, the only
 * supported transformer is the `strip_value_transformer`, which replaces every
 * cell's value with the empty string.
 *
 * * Chains and interleaves are described in more detail in the
 * RowFilter.Chain and RowFilter.Interleave documentation.
 *
 * The total serialized size of a RowFilter message must not
 * exceed 20480 bytes, and RowFilters may not be nested within each other
 * (in Chains or Interleaves) to a depth of more than 20.
 */
export interface RowFilter {
  /**
   * Applies several RowFilters to the data in sequence, progressively
   * narrowing the results.
   */
  chain?:
    | RowFilter_Chain
    | undefined;
  /**
   * Applies several RowFilters to the data in parallel and combines the
   * results.
   */
  interleave?:
    | RowFilter_Interleave
    | undefined;
  /**
   * Applies one of two possible RowFilters to the data based on the output of
   * a predicate RowFilter.
   */
  condition?:
    | RowFilter_Condition
    | undefined;
  /**
   * ADVANCED USE ONLY.
   * Hook for introspection into the RowFilter. Outputs all cells directly to
   * the output of the read rather than to any parent filter. Consider the
   * following example:
   *
   *     Chain(
   *       FamilyRegex("A"),
   *       Interleave(
   *         All(),
   *         Chain(Label("foo"), Sink())
   *       ),
   *       QualifierRegex("B")
   *     )
   *
   *                         A,A,1,w
   *                         A,B,2,x
   *                         B,B,4,z
   *                            |
   *                     FamilyRegex("A")
   *                            |
   *                         A,A,1,w
   *                         A,B,2,x
   *                            |
   *               +------------+-------------+
   *               |                          |
   *             All()                    Label(foo)
   *               |                          |
   *            A,A,1,w              A,A,1,w,labels:[foo]
   *            A,B,2,x              A,B,2,x,labels:[foo]
   *               |                          |
   *               |                        Sink() --------------+
   *               |                          |                  |
   *               +------------+      x------+          A,A,1,w,labels:[foo]
   *                            |                        A,B,2,x,labels:[foo]
   *                         A,A,1,w                             |
   *                         A,B,2,x                             |
   *                            |                                |
   *                    QualifierRegex("B")                      |
   *                            |                                |
   *                         A,B,2,x                             |
   *                            |                                |
   *                            +--------------------------------+
   *                            |
   *                         A,A,1,w,labels:[foo]
   *                         A,B,2,x,labels:[foo]  // could be switched
   *                         A,B,2,x               // could be switched
   *
   * Despite being excluded by the qualifier filter, a copy of every cell
   * that reaches the sink is present in the final result.
   *
   * As with an [Interleave][google.bigtable.v2.RowFilter.Interleave],
   * duplicate cells are possible, and appear in an unspecified mutual order.
   * In this case we have a duplicate with column "A:B" and timestamp 2,
   * because one copy passed through the all filter while the other was
   * passed through the label and sink. Note that one copy has label "foo",
   * while the other does not.
   *
   * Cannot be used within the `predicate_filter`, `true_filter`, or
   * `false_filter` of a [Condition][google.bigtable.v2.RowFilter.Condition].
   */
  sink?:
    | boolean
    | undefined;
  /**
   * Matches all cells, regardless of input. Functionally equivalent to
   * leaving `filter` unset, but included for completeness.
   */
  passAllFilter?:
    | boolean
    | undefined;
  /**
   * Does not match any cells, regardless of input. Useful for temporarily
   * disabling just part of a filter.
   */
  blockAllFilter?:
    | boolean
    | undefined;
  /**
   * Matches only cells from rows whose keys satisfy the given RE2 regex. In
   * other words, passes through the entire row when the key matches, and
   * otherwise produces an empty row.
   * Note that, since row keys can contain arbitrary bytes, the `\C` escape
   * sequence must be used if a true wildcard is desired. The `.` character
   * will not match the new line character `\n`, which may be present in a
   * binary key.
   */
  rowKeyRegexFilter?:
    | Buffer
    | undefined;
  /**
   * Matches all cells from a row with probability p, and matches no cells
   * from the row with probability 1-p.
   */
  rowSampleFilter?:
    | number
    | undefined;
  /**
   * Matches only cells from columns whose families satisfy the given RE2
   * regex. For technical reasons, the regex must not contain the `:`
   * character, even if it is not being used as a literal.
   * Note that, since column families cannot contain the new line character
   * `\n`, it is sufficient to use `.` as a full wildcard when matching
   * column family names.
   */
  familyNameRegexFilter?:
    | string
    | undefined;
  /**
   * Matches only cells from columns whose qualifiers satisfy the given RE2
   * regex.
   * Note that, since column qualifiers can contain arbitrary bytes, the `\C`
   * escape sequence must be used if a true wildcard is desired. The `.`
   * character will not match the new line character `\n`, which may be
   * present in a binary qualifier.
   */
  columnQualifierRegexFilter?:
    | Buffer
    | undefined;
  /** Matches only cells from columns within the given range. */
  columnRangeFilter?:
    | ColumnRange
    | undefined;
  /** Matches only cells with timestamps within the given range. */
  timestampRangeFilter?:
    | TimestampRange
    | undefined;
  /**
   * Matches only cells with values that satisfy the given regular expression.
   * Note that, since cell values can contain arbitrary bytes, the `\C` escape
   * sequence must be used if a true wildcard is desired. The `.` character
   * will not match the new line character `\n`, which may be present in a
   * binary value.
   */
  valueRegexFilter?:
    | Buffer
    | undefined;
  /** Matches only cells with values that fall within the given range. */
  valueRangeFilter?:
    | ValueRange
    | undefined;
  /**
   * Skips the first N cells of each row, matching all subsequent cells.
   * If duplicate cells are present, as is possible when using an Interleave,
   * each copy of the cell is counted separately.
   */
  cellsPerRowOffsetFilter?:
    | number
    | undefined;
  /**
   * Matches only the first N cells of each row.
   * If duplicate cells are present, as is possible when using an Interleave,
   * each copy of the cell is counted separately.
   */
  cellsPerRowLimitFilter?:
    | number
    | undefined;
  /**
   * Matches only the most recent N cells within each column. For example,
   * if N=2, this filter would match column `foo:bar` at timestamps 10 and 9,
   * skip all earlier cells in `foo:bar`, and then begin matching again in
   * column `foo:bar2`.
   * If duplicate cells are present, as is possible when using an Interleave,
   * each copy of the cell is counted separately.
   */
  cellsPerColumnLimitFilter?:
    | number
    | undefined;
  /** Replaces each cell's value with the empty string. */
  stripValueTransformer?:
    | boolean
    | undefined;
  /**
   * Applies the given label to all cells in the output row. This allows
   * the client to determine which results were produced from which part of
   * the filter.
   *
   * Values must be at most 15 characters in length, and match the RE2
   * pattern `[a-z0-9\\-]+`
   *
   * Due to a technical limitation, it is not currently possible to apply
   * multiple labels to a cell. As a result, a Chain may have no more than
   * one sub-filter which contains a `apply_label_transformer`. It is okay for
   * an Interleave to contain multiple `apply_label_transformers`, as they
   * will be applied to separate copies of the input. This may be relaxed in
   * the future.
   */
  applyLabelTransformer?: string | undefined;
}

/** A RowFilter which sends rows through several RowFilters in sequence. */
export interface RowFilter_Chain {
  /**
   * The elements of "filters" are chained together to process the input row:
   * in row -> f(0) -> intermediate row -> f(1) -> ... -> f(N) -> out row
   * The full chain is executed atomically.
   */
  filters: RowFilter[];
}

/**
 * A RowFilter which sends each row to each of several component
 * RowFilters and interleaves the results.
 */
export interface RowFilter_Interleave {
  /**
   * The elements of "filters" all process a copy of the input row, and the
   * results are pooled, sorted, and combined into a single output row.
   * If multiple cells are produced with the same column and timestamp,
   * they will all appear in the output row in an unspecified mutual order.
   * Consider the following example, with three filters:
   *
   *                                  input row
   *                                      |
   *            -----------------------------------------------------
   *            |                         |                         |
   *           f(0)                      f(1)                      f(2)
   *            |                         |                         |
   *     1: foo,bar,10,x             foo,bar,10,z              far,bar,7,a
   *     2: foo,blah,11,z            far,blah,5,x              far,blah,5,x
   *            |                         |                         |
   *            -----------------------------------------------------
   *                                      |
   *     1:                      foo,bar,10,z   // could have switched with #2
   *     2:                      foo,bar,10,x   // could have switched with #1
   *     3:                      foo,blah,11,z
   *     4:                      far,bar,7,a
   *     5:                      far,blah,5,x   // identical to #6
   *     6:                      far,blah,5,x   // identical to #5
   *
   * All interleaved filters are executed atomically.
   */
  filters: RowFilter[];
}

/**
 * A RowFilter which evaluates one of two possible RowFilters, depending on
 * whether or not a predicate RowFilter outputs any cells from the input row.
 *
 * IMPORTANT NOTE: The predicate filter does not execute atomically with the
 * true and false filters, which may lead to inconsistent or unexpected
 * results. Additionally, Condition filters have poor performance, especially
 * when filters are set for the false condition.
 */
export interface RowFilter_Condition {
  /**
   * If `predicate_filter` outputs any cells, then `true_filter` will be
   * evaluated on the input row. Otherwise, `false_filter` will be evaluated.
   */
  predicateFilter:
    | RowFilter
    | undefined;
  /**
   * The filter to apply to the input row if `predicate_filter` returns any
   * results. If not provided, no results will be returned in the true case.
   */
  trueFilter:
    | RowFilter
    | undefined;
  /**
   * The filter to apply to the input row if `predicate_filter` does not
   * return any results. If not provided, no results will be returned in the
   * false case.
   */
  falseFilter: RowFilter | undefined;
}

/** Specifies a particular change to be made to the contents of a row. */
export interface Mutation {
  /** Set a cell's value. */
  setCell?:
    | Mutation_SetCell
    | undefined;
  /** Incrementally updates an `Aggregate` cell. */
  addToCell?:
    | Mutation_AddToCell
    | undefined;
  /** Merges accumulated state to an `Aggregate` cell. */
  mergeToCell?:
    | Mutation_MergeToCell
    | undefined;
  /** Deletes cells from a column. */
  deleteFromColumn?:
    | Mutation_DeleteFromColumn
    | undefined;
  /** Deletes cells from a column family. */
  deleteFromFamily?:
    | Mutation_DeleteFromFamily
    | undefined;
  /** Deletes cells from the entire row. */
  deleteFromRow?: Mutation_DeleteFromRow | undefined;
}

/** A Mutation which sets the value of the specified cell. */
export interface Mutation_SetCell {
  /**
   * The name of the family into which new data should be written.
   * Must match `[-_.a-zA-Z0-9]+`
   */
  familyName: string;
  /**
   * The qualifier of the column into which new data should be written.
   * Can be any byte string, including the empty string.
   */
  columnQualifier: Buffer;
  /**
   * The timestamp of the cell into which new data should be written.
   * Use -1 for current Bigtable server time.
   * Otherwise, the client should set this value itself, noting that the
   * default value is a timestamp of zero if the field is left unspecified.
   * Values must match the granularity of the table (e.g. micros, millis).
   */
  timestampMicros: Long;
  /** The value to be written into the specified cell. */
  value: Buffer;
}

/** A Mutation which incrementally updates a cell in an `Aggregate` family. */
export interface Mutation_AddToCell {
  /**
   * The name of the `Aggregate` family into which new data should be added.
   * This must be a family with a `value_type` of `Aggregate`.
   * Format: `[-_.a-zA-Z0-9]+`
   */
  familyName: string;
  /**
   * The qualifier of the column into which new data should be added. This
   * must be a `raw_value`.
   */
  columnQualifier:
    | Value
    | undefined;
  /**
   * The timestamp of the cell to which new data should be added. This must
   * be a `raw_timestamp_micros` that matches the table's `granularity`.
   */
  timestamp:
    | Value
    | undefined;
  /**
   * The input value to be accumulated into the specified cell. This must be
   * compatible with the family's `value_type.input_type`.
   */
  input: Value | undefined;
}

/**
 * A Mutation which merges accumulated state into a cell in an `Aggregate`
 * family.
 */
export interface Mutation_MergeToCell {
  /**
   * The name of the `Aggregate` family into which new data should be added.
   * This must be a family with a `value_type` of `Aggregate`.
   * Format: `[-_.a-zA-Z0-9]+`
   */
  familyName: string;
  /**
   * The qualifier of the column into which new data should be added. This
   * must be a `raw_value`.
   */
  columnQualifier:
    | Value
    | undefined;
  /**
   * The timestamp of the cell to which new data should be added. This must
   * be a `raw_timestamp_micros` that matches the table's `granularity`.
   */
  timestamp:
    | Value
    | undefined;
  /**
   * The input value to be merged into the specified cell. This must be
   * compatible with the family's `value_type.state_type`. Merging `NULL` is
   * allowed, but has no effect.
   */
  input: Value | undefined;
}

/**
 * A Mutation which deletes cells from the specified column, optionally
 * restricting the deletions to a given timestamp range.
 */
export interface Mutation_DeleteFromColumn {
  /**
   * The name of the family from which cells should be deleted.
   * Must match `[-_.a-zA-Z0-9]+`
   */
  familyName: string;
  /**
   * The qualifier of the column from which cells should be deleted.
   * Can be any byte string, including the empty string.
   */
  columnQualifier: Buffer;
  /** The range of timestamps within which cells should be deleted. */
  timeRange: TimestampRange | undefined;
}

/** A Mutation which deletes all cells from the specified column family. */
export interface Mutation_DeleteFromFamily {
  /**
   * The name of the family from which cells should be deleted.
   * Must match `[-_.a-zA-Z0-9]+`
   */
  familyName: string;
}

/** A Mutation which deletes all cells from the containing row. */
export interface Mutation_DeleteFromRow {
}

/**
 * Specifies an atomic read/modify/write operation on the latest value of the
 * specified column.
 */
export interface ReadModifyWriteRule {
  /**
   * The name of the family to which the read/modify/write should be applied.
   * Must match `[-_.a-zA-Z0-9]+`
   */
  familyName: string;
  /**
   * The qualifier of the column to which the read/modify/write should be
   * applied.
   * Can be any byte string, including the empty string.
   */
  columnQualifier: Buffer;
  /**
   * Rule specifying that `append_value` be appended to the existing value.
   * If the targeted cell is unset, it will be treated as containing the
   * empty string.
   */
  appendValue?:
    | Buffer
    | undefined;
  /**
   * Rule specifying that `increment_amount` be added to the existing value.
   * If the targeted cell is unset, it will be treated as containing a zero.
   * Otherwise, the targeted cell must contain an 8-byte value (interpreted
   * as a 64-bit big-endian signed integer), or the entire request will fail.
   */
  incrementAmount?: Long | undefined;
}

/**
 * NOTE: This API is intended to be used by Apache Beam BigtableIO.
 * A partition of a change stream.
 */
export interface StreamPartition {
  /**
   * The row range covered by this partition and is specified by
   * [`start_key_closed`, `end_key_open`).
   */
  rowRange: RowRange | undefined;
}

/**
 * NOTE: This API is intended to be used by Apache Beam BigtableIO.
 * The information required to continue reading the data from multiple
 * `StreamPartitions` from where a previous read left off.
 */
export interface StreamContinuationTokens {
  /** List of continuation tokens. */
  tokens: StreamContinuationToken[];
}

/**
 * NOTE: This API is intended to be used by Apache Beam BigtableIO.
 * The information required to continue reading the data from a
 * `StreamPartition` from where a previous read left off.
 */
export interface StreamContinuationToken {
  /** The partition that this token applies to. */
  partition:
    | StreamPartition
    | undefined;
  /** An encoded position in the stream to restart reading from. */
  token: string;
}

/**
 * Protocol buffers format descriptor, as described by Messages ProtoSchema and
 * ProtoRows
 */
export interface ProtoFormat {
}

/** Describes a column in a Bigtable Query Language result set. */
export interface ColumnMetadata {
  /** The name of the column. */
  name: string;
  /** The type of the column. */
  type: Type | undefined;
}

/** ResultSet schema in proto format */
export interface ProtoSchema {
  /** The columns in the result set. */
  columns: ColumnMetadata[];
}

/** Describes the structure of a Bigtable result set. */
export interface ResultSetMetadata {
  /** Schema in proto format */
  protoSchema?: ProtoSchema | undefined;
}

/**
 * Rows represented in proto format.
 *
 * This should be constructed by concatenating the `batch_data` from each
 * of the relevant `ProtoRowsBatch` messages and parsing the result as a
 * `ProtoRows` message.
 */
export interface ProtoRows {
  /**
   * A proto rows message consists of a list of values. Every N complete values
   * defines a row, where N is equal to the  number of entries in the
   * `metadata.proto_schema.columns` value received in the first response.
   */
  values: Value[];
}

/** Batch of serialized ProtoRows. */
export interface ProtoRowsBatch {
  /**
   * Merge partial results by concatenating these bytes, then parsing the
   * overall value as a `ProtoRows` message.
   */
  batchData: Buffer;
}

/**
 * A partial result set from the streaming query API.
 * CBT client will buffer partial_rows from result_sets until it gets a
 * resumption_token.
 */
export interface PartialResultSet {
  /** Partial rows in serialized ProtoRows format. */
  protoRowsBatch?:
    | ProtoRowsBatch
    | undefined;
  /**
   * An opaque token sent by the server to allow query resumption and signal
   * the client to accumulate `partial_rows` since the last non-empty
   * `resume_token`. On resumption, the resumed query will return the remaining
   * rows for this query.
   *
   * If there is a batch in progress, a non-empty `resume_token`
   * means that that the batch of `partial_rows` will be complete after merging
   * the `partial_rows` from this response. The client must only yield
   * completed batches to the application, and must ensure that any future
   * retries send the latest token to avoid returning duplicate data.
   *
   * The server may set 'resume_token' without a 'partial_rows'. If there is a
   * batch in progress the client should yield it.
   *
   * The server will also send a sentinel `resume_token` when last batch of
   * `partial_rows` is sent. If the client retries the ExecuteQueryRequest with
   * the sentinel `resume_token`, the server will emit it again without any
   * `partial_rows`, then return OK.
   */
  resumeToken: Buffer;
  /**
   * Estimated size of a new batch. The server will always set this when
   * returning the first `partial_rows` of a batch, and will not set it at any
   * other time.
   *
   * The client can use this estimate to allocate an initial buffer for the
   * batched results. This helps minimize the number of allocations required,
   * though the buffer size may still need to be increased if the estimate is
   * too low.
   */
  estimatedBatchSize: number;
}

function createBaseRow(): Row {
  return { key: Buffer.alloc(0), families: [] };
}

export const Row: MessageFns<Row> = {
  encode(message: Row, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key.length !== 0) {
      writer.uint32(10).bytes(message.key);
    }
    for (const v of message.families) {
      Family.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Row {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRow();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = Buffer.from(reader.bytes());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.families.push(Family.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Row {
    return {
      key: isSet(object.key) ? Buffer.from(bytesFromBase64(object.key)) : Buffer.alloc(0),
      families: globalThis.Array.isArray(object?.families) ? object.families.map((e: any) => Family.fromJSON(e)) : [],
    };
  },

  toJSON(message: Row): unknown {
    const obj: any = {};
    if (message.key.length !== 0) {
      obj.key = base64FromBytes(message.key);
    }
    if (message.families?.length) {
      obj.families = message.families.map((e) => Family.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<Row>): Row {
    return Row.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Row>): Row {
    const message = createBaseRow();
    message.key = object.key ?? Buffer.alloc(0);
    message.families = object.families?.map((e) => Family.fromPartial(e)) || [];
    return message;
  },
};

function createBaseFamily(): Family {
  return { name: "", columns: [] };
}

export const Family: MessageFns<Family> = {
  encode(message: Family, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    for (const v of message.columns) {
      Column.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Family {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFamily();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.columns.push(Column.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Family {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      columns: globalThis.Array.isArray(object?.columns) ? object.columns.map((e: any) => Column.fromJSON(e)) : [],
    };
  },

  toJSON(message: Family): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.columns?.length) {
      obj.columns = message.columns.map((e) => Column.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<Family>): Family {
    return Family.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Family>): Family {
    const message = createBaseFamily();
    message.name = object.name ?? "";
    message.columns = object.columns?.map((e) => Column.fromPartial(e)) || [];
    return message;
  },
};

function createBaseColumn(): Column {
  return { qualifier: Buffer.alloc(0), cells: [] };
}

export const Column: MessageFns<Column> = {
  encode(message: Column, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.qualifier.length !== 0) {
      writer.uint32(10).bytes(message.qualifier);
    }
    for (const v of message.cells) {
      Cell.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Column {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseColumn();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.qualifier = Buffer.from(reader.bytes());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.cells.push(Cell.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Column {
    return {
      qualifier: isSet(object.qualifier) ? Buffer.from(bytesFromBase64(object.qualifier)) : Buffer.alloc(0),
      cells: globalThis.Array.isArray(object?.cells) ? object.cells.map((e: any) => Cell.fromJSON(e)) : [],
    };
  },

  toJSON(message: Column): unknown {
    const obj: any = {};
    if (message.qualifier.length !== 0) {
      obj.qualifier = base64FromBytes(message.qualifier);
    }
    if (message.cells?.length) {
      obj.cells = message.cells.map((e) => Cell.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<Column>): Column {
    return Column.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Column>): Column {
    const message = createBaseColumn();
    message.qualifier = object.qualifier ?? Buffer.alloc(0);
    message.cells = object.cells?.map((e) => Cell.fromPartial(e)) || [];
    return message;
  },
};

function createBaseCell(): Cell {
  return { timestampMicros: Long.ZERO, value: Buffer.alloc(0), labels: [] };
}

export const Cell: MessageFns<Cell> = {
  encode(message: Cell, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.timestampMicros.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.timestampMicros.toString());
    }
    if (message.value.length !== 0) {
      writer.uint32(18).bytes(message.value);
    }
    for (const v of message.labels) {
      writer.uint32(26).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cell {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCell();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.timestampMicros = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = Buffer.from(reader.bytes());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.labels.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cell {
    return {
      timestampMicros: isSet(object.timestampMicros) ? Long.fromValue(object.timestampMicros) : Long.ZERO,
      value: isSet(object.value) ? Buffer.from(bytesFromBase64(object.value)) : Buffer.alloc(0),
      labels: globalThis.Array.isArray(object?.labels) ? object.labels.map((e: any) => globalThis.String(e)) : [],
    };
  },

  toJSON(message: Cell): unknown {
    const obj: any = {};
    if (!message.timestampMicros.equals(Long.ZERO)) {
      obj.timestampMicros = (message.timestampMicros || Long.ZERO).toString();
    }
    if (message.value.length !== 0) {
      obj.value = base64FromBytes(message.value);
    }
    if (message.labels?.length) {
      obj.labels = message.labels;
    }
    return obj;
  },

  create(base?: DeepPartial<Cell>): Cell {
    return Cell.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cell>): Cell {
    const message = createBaseCell();
    message.timestampMicros = (object.timestampMicros !== undefined && object.timestampMicros !== null)
      ? Long.fromValue(object.timestampMicros)
      : Long.ZERO;
    message.value = object.value ?? Buffer.alloc(0);
    message.labels = object.labels?.map((e) => e) || [];
    return message;
  },
};

function createBaseValue(): Value {
  return {
    type: undefined,
    rawValue: undefined,
    rawTimestampMicros: undefined,
    bytesValue: undefined,
    stringValue: undefined,
    intValue: undefined,
    boolValue: undefined,
    floatValue: undefined,
    timestampValue: undefined,
    dateValue: undefined,
    arrayValue: undefined,
  };
}

export const Value: MessageFns<Value> = {
  encode(message: Value, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.type !== undefined) {
      Type.encode(message.type, writer.uint32(58).fork()).join();
    }
    if (message.rawValue !== undefined) {
      writer.uint32(66).bytes(message.rawValue);
    }
    if (message.rawTimestampMicros !== undefined) {
      writer.uint32(72).int64(message.rawTimestampMicros.toString());
    }
    if (message.bytesValue !== undefined) {
      writer.uint32(18).bytes(message.bytesValue);
    }
    if (message.stringValue !== undefined) {
      writer.uint32(26).string(message.stringValue);
    }
    if (message.intValue !== undefined) {
      writer.uint32(48).int64(message.intValue.toString());
    }
    if (message.boolValue !== undefined) {
      writer.uint32(80).bool(message.boolValue);
    }
    if (message.floatValue !== undefined) {
      writer.uint32(89).double(message.floatValue);
    }
    if (message.timestampValue !== undefined) {
      Timestamp.encode(toTimestamp(message.timestampValue), writer.uint32(98).fork()).join();
    }
    if (message.dateValue !== undefined) {
      DateMessage.encode(message.dateValue, writer.uint32(106).fork()).join();
    }
    if (message.arrayValue !== undefined) {
      ArrayValue.encode(message.arrayValue, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Value {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseValue();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 7:
          if (tag !== 58) {
            break;
          }

          message.type = Type.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.rawValue = Buffer.from(reader.bytes());
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.rawTimestampMicros = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.bytesValue = Buffer.from(reader.bytes());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.stringValue = reader.string();
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.intValue = Long.fromString(reader.int64().toString());
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.boolValue = reader.bool();
          continue;
        case 11:
          if (tag !== 89) {
            break;
          }

          message.floatValue = reader.double();
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.timestampValue = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.dateValue = DateMessage.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.arrayValue = ArrayValue.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Value {
    return {
      type: isSet(object.type) ? Type.fromJSON(object.type) : undefined,
      rawValue: isSet(object.rawValue) ? Buffer.from(bytesFromBase64(object.rawValue)) : undefined,
      rawTimestampMicros: isSet(object.rawTimestampMicros) ? Long.fromValue(object.rawTimestampMicros) : undefined,
      bytesValue: isSet(object.bytesValue) ? Buffer.from(bytesFromBase64(object.bytesValue)) : undefined,
      stringValue: isSet(object.stringValue) ? globalThis.String(object.stringValue) : undefined,
      intValue: isSet(object.intValue) ? Long.fromValue(object.intValue) : undefined,
      boolValue: isSet(object.boolValue) ? globalThis.Boolean(object.boolValue) : undefined,
      floatValue: isSet(object.floatValue) ? globalThis.Number(object.floatValue) : undefined,
      timestampValue: isSet(object.timestampValue) ? fromJsonTimestamp(object.timestampValue) : undefined,
      dateValue: isSet(object.dateValue) ? DateMessage.fromJSON(object.dateValue) : undefined,
      arrayValue: isSet(object.arrayValue) ? ArrayValue.fromJSON(object.arrayValue) : undefined,
    };
  },

  toJSON(message: Value): unknown {
    const obj: any = {};
    if (message.type !== undefined) {
      obj.type = Type.toJSON(message.type);
    }
    if (message.rawValue !== undefined) {
      obj.rawValue = base64FromBytes(message.rawValue);
    }
    if (message.rawTimestampMicros !== undefined) {
      obj.rawTimestampMicros = (message.rawTimestampMicros || Long.ZERO).toString();
    }
    if (message.bytesValue !== undefined) {
      obj.bytesValue = base64FromBytes(message.bytesValue);
    }
    if (message.stringValue !== undefined) {
      obj.stringValue = message.stringValue;
    }
    if (message.intValue !== undefined) {
      obj.intValue = (message.intValue || Long.ZERO).toString();
    }
    if (message.boolValue !== undefined) {
      obj.boolValue = message.boolValue;
    }
    if (message.floatValue !== undefined) {
      obj.floatValue = message.floatValue;
    }
    if (message.timestampValue !== undefined) {
      obj.timestampValue = message.timestampValue.toISOString();
    }
    if (message.dateValue !== undefined) {
      obj.dateValue = DateMessage.toJSON(message.dateValue);
    }
    if (message.arrayValue !== undefined) {
      obj.arrayValue = ArrayValue.toJSON(message.arrayValue);
    }
    return obj;
  },

  create(base?: DeepPartial<Value>): Value {
    return Value.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Value>): Value {
    const message = createBaseValue();
    message.type = (object.type !== undefined && object.type !== null) ? Type.fromPartial(object.type) : undefined;
    message.rawValue = object.rawValue ?? undefined;
    message.rawTimestampMicros = (object.rawTimestampMicros !== undefined && object.rawTimestampMicros !== null)
      ? Long.fromValue(object.rawTimestampMicros)
      : undefined;
    message.bytesValue = object.bytesValue ?? undefined;
    message.stringValue = object.stringValue ?? undefined;
    message.intValue = (object.intValue !== undefined && object.intValue !== null)
      ? Long.fromValue(object.intValue)
      : undefined;
    message.boolValue = object.boolValue ?? undefined;
    message.floatValue = object.floatValue ?? undefined;
    message.timestampValue = object.timestampValue ?? undefined;
    message.dateValue = (object.dateValue !== undefined && object.dateValue !== null)
      ? DateMessage.fromPartial(object.dateValue)
      : undefined;
    message.arrayValue = (object.arrayValue !== undefined && object.arrayValue !== null)
      ? ArrayValue.fromPartial(object.arrayValue)
      : undefined;
    return message;
  },
};

function createBaseArrayValue(): ArrayValue {
  return { values: [] };
}

export const ArrayValue: MessageFns<ArrayValue> = {
  encode(message: ArrayValue, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.values) {
      Value.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ArrayValue {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseArrayValue();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.values.push(Value.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ArrayValue {
    return { values: globalThis.Array.isArray(object?.values) ? object.values.map((e: any) => Value.fromJSON(e)) : [] };
  },

  toJSON(message: ArrayValue): unknown {
    const obj: any = {};
    if (message.values?.length) {
      obj.values = message.values.map((e) => Value.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ArrayValue>): ArrayValue {
    return ArrayValue.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ArrayValue>): ArrayValue {
    const message = createBaseArrayValue();
    message.values = object.values?.map((e) => Value.fromPartial(e)) || [];
    return message;
  },
};

function createBaseRowRange(): RowRange {
  return { startKeyClosed: undefined, startKeyOpen: undefined, endKeyOpen: undefined, endKeyClosed: undefined };
}

export const RowRange: MessageFns<RowRange> = {
  encode(message: RowRange, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startKeyClosed !== undefined) {
      writer.uint32(10).bytes(message.startKeyClosed);
    }
    if (message.startKeyOpen !== undefined) {
      writer.uint32(18).bytes(message.startKeyOpen);
    }
    if (message.endKeyOpen !== undefined) {
      writer.uint32(26).bytes(message.endKeyOpen);
    }
    if (message.endKeyClosed !== undefined) {
      writer.uint32(34).bytes(message.endKeyClosed);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RowRange {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRowRange();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startKeyClosed = Buffer.from(reader.bytes());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.startKeyOpen = Buffer.from(reader.bytes());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.endKeyOpen = Buffer.from(reader.bytes());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.endKeyClosed = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RowRange {
    return {
      startKeyClosed: isSet(object.startKeyClosed) ? Buffer.from(bytesFromBase64(object.startKeyClosed)) : undefined,
      startKeyOpen: isSet(object.startKeyOpen) ? Buffer.from(bytesFromBase64(object.startKeyOpen)) : undefined,
      endKeyOpen: isSet(object.endKeyOpen) ? Buffer.from(bytesFromBase64(object.endKeyOpen)) : undefined,
      endKeyClosed: isSet(object.endKeyClosed) ? Buffer.from(bytesFromBase64(object.endKeyClosed)) : undefined,
    };
  },

  toJSON(message: RowRange): unknown {
    const obj: any = {};
    if (message.startKeyClosed !== undefined) {
      obj.startKeyClosed = base64FromBytes(message.startKeyClosed);
    }
    if (message.startKeyOpen !== undefined) {
      obj.startKeyOpen = base64FromBytes(message.startKeyOpen);
    }
    if (message.endKeyOpen !== undefined) {
      obj.endKeyOpen = base64FromBytes(message.endKeyOpen);
    }
    if (message.endKeyClosed !== undefined) {
      obj.endKeyClosed = base64FromBytes(message.endKeyClosed);
    }
    return obj;
  },

  create(base?: DeepPartial<RowRange>): RowRange {
    return RowRange.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RowRange>): RowRange {
    const message = createBaseRowRange();
    message.startKeyClosed = object.startKeyClosed ?? undefined;
    message.startKeyOpen = object.startKeyOpen ?? undefined;
    message.endKeyOpen = object.endKeyOpen ?? undefined;
    message.endKeyClosed = object.endKeyClosed ?? undefined;
    return message;
  },
};

function createBaseRowSet(): RowSet {
  return { rowKeys: [], rowRanges: [] };
}

export const RowSet: MessageFns<RowSet> = {
  encode(message: RowSet, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.rowKeys) {
      writer.uint32(10).bytes(v!);
    }
    for (const v of message.rowRanges) {
      RowRange.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RowSet {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRowSet();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.rowKeys.push(Buffer.from(reader.bytes()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.rowRanges.push(RowRange.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RowSet {
    return {
      rowKeys: globalThis.Array.isArray(object?.rowKeys)
        ? object.rowKeys.map((e: any) => Buffer.from(bytesFromBase64(e)))
        : [],
      rowRanges: globalThis.Array.isArray(object?.rowRanges)
        ? object.rowRanges.map((e: any) => RowRange.fromJSON(e))
        : [],
    };
  },

  toJSON(message: RowSet): unknown {
    const obj: any = {};
    if (message.rowKeys?.length) {
      obj.rowKeys = message.rowKeys.map((e) => base64FromBytes(e));
    }
    if (message.rowRanges?.length) {
      obj.rowRanges = message.rowRanges.map((e) => RowRange.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<RowSet>): RowSet {
    return RowSet.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RowSet>): RowSet {
    const message = createBaseRowSet();
    message.rowKeys = object.rowKeys?.map((e) => e) || [];
    message.rowRanges = object.rowRanges?.map((e) => RowRange.fromPartial(e)) || [];
    return message;
  },
};

function createBaseColumnRange(): ColumnRange {
  return {
    familyName: "",
    startQualifierClosed: undefined,
    startQualifierOpen: undefined,
    endQualifierClosed: undefined,
    endQualifierOpen: undefined,
  };
}

export const ColumnRange: MessageFns<ColumnRange> = {
  encode(message: ColumnRange, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.familyName !== "") {
      writer.uint32(10).string(message.familyName);
    }
    if (message.startQualifierClosed !== undefined) {
      writer.uint32(18).bytes(message.startQualifierClosed);
    }
    if (message.startQualifierOpen !== undefined) {
      writer.uint32(26).bytes(message.startQualifierOpen);
    }
    if (message.endQualifierClosed !== undefined) {
      writer.uint32(34).bytes(message.endQualifierClosed);
    }
    if (message.endQualifierOpen !== undefined) {
      writer.uint32(42).bytes(message.endQualifierOpen);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ColumnRange {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseColumnRange();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.familyName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.startQualifierClosed = Buffer.from(reader.bytes());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.startQualifierOpen = Buffer.from(reader.bytes());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.endQualifierClosed = Buffer.from(reader.bytes());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.endQualifierOpen = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ColumnRange {
    return {
      familyName: isSet(object.familyName) ? globalThis.String(object.familyName) : "",
      startQualifierClosed: isSet(object.startQualifierClosed)
        ? Buffer.from(bytesFromBase64(object.startQualifierClosed))
        : undefined,
      startQualifierOpen: isSet(object.startQualifierOpen)
        ? Buffer.from(bytesFromBase64(object.startQualifierOpen))
        : undefined,
      endQualifierClosed: isSet(object.endQualifierClosed)
        ? Buffer.from(bytesFromBase64(object.endQualifierClosed))
        : undefined,
      endQualifierOpen: isSet(object.endQualifierOpen)
        ? Buffer.from(bytesFromBase64(object.endQualifierOpen))
        : undefined,
    };
  },

  toJSON(message: ColumnRange): unknown {
    const obj: any = {};
    if (message.familyName !== "") {
      obj.familyName = message.familyName;
    }
    if (message.startQualifierClosed !== undefined) {
      obj.startQualifierClosed = base64FromBytes(message.startQualifierClosed);
    }
    if (message.startQualifierOpen !== undefined) {
      obj.startQualifierOpen = base64FromBytes(message.startQualifierOpen);
    }
    if (message.endQualifierClosed !== undefined) {
      obj.endQualifierClosed = base64FromBytes(message.endQualifierClosed);
    }
    if (message.endQualifierOpen !== undefined) {
      obj.endQualifierOpen = base64FromBytes(message.endQualifierOpen);
    }
    return obj;
  },

  create(base?: DeepPartial<ColumnRange>): ColumnRange {
    return ColumnRange.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ColumnRange>): ColumnRange {
    const message = createBaseColumnRange();
    message.familyName = object.familyName ?? "";
    message.startQualifierClosed = object.startQualifierClosed ?? undefined;
    message.startQualifierOpen = object.startQualifierOpen ?? undefined;
    message.endQualifierClosed = object.endQualifierClosed ?? undefined;
    message.endQualifierOpen = object.endQualifierOpen ?? undefined;
    return message;
  },
};

function createBaseTimestampRange(): TimestampRange {
  return { startTimestampMicros: Long.ZERO, endTimestampMicros: Long.ZERO };
}

export const TimestampRange: MessageFns<TimestampRange> = {
  encode(message: TimestampRange, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.startTimestampMicros.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.startTimestampMicros.toString());
    }
    if (!message.endTimestampMicros.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.endTimestampMicros.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TimestampRange {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTimestampRange();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.startTimestampMicros = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.endTimestampMicros = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TimestampRange {
    return {
      startTimestampMicros: isSet(object.startTimestampMicros)
        ? Long.fromValue(object.startTimestampMicros)
        : Long.ZERO,
      endTimestampMicros: isSet(object.endTimestampMicros) ? Long.fromValue(object.endTimestampMicros) : Long.ZERO,
    };
  },

  toJSON(message: TimestampRange): unknown {
    const obj: any = {};
    if (!message.startTimestampMicros.equals(Long.ZERO)) {
      obj.startTimestampMicros = (message.startTimestampMicros || Long.ZERO).toString();
    }
    if (!message.endTimestampMicros.equals(Long.ZERO)) {
      obj.endTimestampMicros = (message.endTimestampMicros || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<TimestampRange>): TimestampRange {
    return TimestampRange.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TimestampRange>): TimestampRange {
    const message = createBaseTimestampRange();
    message.startTimestampMicros = (object.startTimestampMicros !== undefined && object.startTimestampMicros !== null)
      ? Long.fromValue(object.startTimestampMicros)
      : Long.ZERO;
    message.endTimestampMicros = (object.endTimestampMicros !== undefined && object.endTimestampMicros !== null)
      ? Long.fromValue(object.endTimestampMicros)
      : Long.ZERO;
    return message;
  },
};

function createBaseValueRange(): ValueRange {
  return { startValueClosed: undefined, startValueOpen: undefined, endValueClosed: undefined, endValueOpen: undefined };
}

export const ValueRange: MessageFns<ValueRange> = {
  encode(message: ValueRange, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startValueClosed !== undefined) {
      writer.uint32(10).bytes(message.startValueClosed);
    }
    if (message.startValueOpen !== undefined) {
      writer.uint32(18).bytes(message.startValueOpen);
    }
    if (message.endValueClosed !== undefined) {
      writer.uint32(26).bytes(message.endValueClosed);
    }
    if (message.endValueOpen !== undefined) {
      writer.uint32(34).bytes(message.endValueOpen);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ValueRange {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseValueRange();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startValueClosed = Buffer.from(reader.bytes());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.startValueOpen = Buffer.from(reader.bytes());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.endValueClosed = Buffer.from(reader.bytes());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.endValueOpen = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ValueRange {
    return {
      startValueClosed: isSet(object.startValueClosed)
        ? Buffer.from(bytesFromBase64(object.startValueClosed))
        : undefined,
      startValueOpen: isSet(object.startValueOpen) ? Buffer.from(bytesFromBase64(object.startValueOpen)) : undefined,
      endValueClosed: isSet(object.endValueClosed) ? Buffer.from(bytesFromBase64(object.endValueClosed)) : undefined,
      endValueOpen: isSet(object.endValueOpen) ? Buffer.from(bytesFromBase64(object.endValueOpen)) : undefined,
    };
  },

  toJSON(message: ValueRange): unknown {
    const obj: any = {};
    if (message.startValueClosed !== undefined) {
      obj.startValueClosed = base64FromBytes(message.startValueClosed);
    }
    if (message.startValueOpen !== undefined) {
      obj.startValueOpen = base64FromBytes(message.startValueOpen);
    }
    if (message.endValueClosed !== undefined) {
      obj.endValueClosed = base64FromBytes(message.endValueClosed);
    }
    if (message.endValueOpen !== undefined) {
      obj.endValueOpen = base64FromBytes(message.endValueOpen);
    }
    return obj;
  },

  create(base?: DeepPartial<ValueRange>): ValueRange {
    return ValueRange.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ValueRange>): ValueRange {
    const message = createBaseValueRange();
    message.startValueClosed = object.startValueClosed ?? undefined;
    message.startValueOpen = object.startValueOpen ?? undefined;
    message.endValueClosed = object.endValueClosed ?? undefined;
    message.endValueOpen = object.endValueOpen ?? undefined;
    return message;
  },
};

function createBaseRowFilter(): RowFilter {
  return {
    chain: undefined,
    interleave: undefined,
    condition: undefined,
    sink: undefined,
    passAllFilter: undefined,
    blockAllFilter: undefined,
    rowKeyRegexFilter: undefined,
    rowSampleFilter: undefined,
    familyNameRegexFilter: undefined,
    columnQualifierRegexFilter: undefined,
    columnRangeFilter: undefined,
    timestampRangeFilter: undefined,
    valueRegexFilter: undefined,
    valueRangeFilter: undefined,
    cellsPerRowOffsetFilter: undefined,
    cellsPerRowLimitFilter: undefined,
    cellsPerColumnLimitFilter: undefined,
    stripValueTransformer: undefined,
    applyLabelTransformer: undefined,
  };
}

export const RowFilter: MessageFns<RowFilter> = {
  encode(message: RowFilter, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.chain !== undefined) {
      RowFilter_Chain.encode(message.chain, writer.uint32(10).fork()).join();
    }
    if (message.interleave !== undefined) {
      RowFilter_Interleave.encode(message.interleave, writer.uint32(18).fork()).join();
    }
    if (message.condition !== undefined) {
      RowFilter_Condition.encode(message.condition, writer.uint32(26).fork()).join();
    }
    if (message.sink !== undefined) {
      writer.uint32(128).bool(message.sink);
    }
    if (message.passAllFilter !== undefined) {
      writer.uint32(136).bool(message.passAllFilter);
    }
    if (message.blockAllFilter !== undefined) {
      writer.uint32(144).bool(message.blockAllFilter);
    }
    if (message.rowKeyRegexFilter !== undefined) {
      writer.uint32(34).bytes(message.rowKeyRegexFilter);
    }
    if (message.rowSampleFilter !== undefined) {
      writer.uint32(113).double(message.rowSampleFilter);
    }
    if (message.familyNameRegexFilter !== undefined) {
      writer.uint32(42).string(message.familyNameRegexFilter);
    }
    if (message.columnQualifierRegexFilter !== undefined) {
      writer.uint32(50).bytes(message.columnQualifierRegexFilter);
    }
    if (message.columnRangeFilter !== undefined) {
      ColumnRange.encode(message.columnRangeFilter, writer.uint32(58).fork()).join();
    }
    if (message.timestampRangeFilter !== undefined) {
      TimestampRange.encode(message.timestampRangeFilter, writer.uint32(66).fork()).join();
    }
    if (message.valueRegexFilter !== undefined) {
      writer.uint32(74).bytes(message.valueRegexFilter);
    }
    if (message.valueRangeFilter !== undefined) {
      ValueRange.encode(message.valueRangeFilter, writer.uint32(122).fork()).join();
    }
    if (message.cellsPerRowOffsetFilter !== undefined) {
      writer.uint32(80).int32(message.cellsPerRowOffsetFilter);
    }
    if (message.cellsPerRowLimitFilter !== undefined) {
      writer.uint32(88).int32(message.cellsPerRowLimitFilter);
    }
    if (message.cellsPerColumnLimitFilter !== undefined) {
      writer.uint32(96).int32(message.cellsPerColumnLimitFilter);
    }
    if (message.stripValueTransformer !== undefined) {
      writer.uint32(104).bool(message.stripValueTransformer);
    }
    if (message.applyLabelTransformer !== undefined) {
      writer.uint32(154).string(message.applyLabelTransformer);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RowFilter {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRowFilter();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.chain = RowFilter_Chain.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.interleave = RowFilter_Interleave.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.condition = RowFilter_Condition.decode(reader, reader.uint32());
          continue;
        case 16:
          if (tag !== 128) {
            break;
          }

          message.sink = reader.bool();
          continue;
        case 17:
          if (tag !== 136) {
            break;
          }

          message.passAllFilter = reader.bool();
          continue;
        case 18:
          if (tag !== 144) {
            break;
          }

          message.blockAllFilter = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.rowKeyRegexFilter = Buffer.from(reader.bytes());
          continue;
        case 14:
          if (tag !== 113) {
            break;
          }

          message.rowSampleFilter = reader.double();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.familyNameRegexFilter = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.columnQualifierRegexFilter = Buffer.from(reader.bytes());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.columnRangeFilter = ColumnRange.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.timestampRangeFilter = TimestampRange.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.valueRegexFilter = Buffer.from(reader.bytes());
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.valueRangeFilter = ValueRange.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.cellsPerRowOffsetFilter = reader.int32();
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.cellsPerRowLimitFilter = reader.int32();
          continue;
        case 12:
          if (tag !== 96) {
            break;
          }

          message.cellsPerColumnLimitFilter = reader.int32();
          continue;
        case 13:
          if (tag !== 104) {
            break;
          }

          message.stripValueTransformer = reader.bool();
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.applyLabelTransformer = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RowFilter {
    return {
      chain: isSet(object.chain) ? RowFilter_Chain.fromJSON(object.chain) : undefined,
      interleave: isSet(object.interleave) ? RowFilter_Interleave.fromJSON(object.interleave) : undefined,
      condition: isSet(object.condition) ? RowFilter_Condition.fromJSON(object.condition) : undefined,
      sink: isSet(object.sink) ? globalThis.Boolean(object.sink) : undefined,
      passAllFilter: isSet(object.passAllFilter) ? globalThis.Boolean(object.passAllFilter) : undefined,
      blockAllFilter: isSet(object.blockAllFilter) ? globalThis.Boolean(object.blockAllFilter) : undefined,
      rowKeyRegexFilter: isSet(object.rowKeyRegexFilter)
        ? Buffer.from(bytesFromBase64(object.rowKeyRegexFilter))
        : undefined,
      rowSampleFilter: isSet(object.rowSampleFilter) ? globalThis.Number(object.rowSampleFilter) : undefined,
      familyNameRegexFilter: isSet(object.familyNameRegexFilter)
        ? globalThis.String(object.familyNameRegexFilter)
        : undefined,
      columnQualifierRegexFilter: isSet(object.columnQualifierRegexFilter)
        ? Buffer.from(bytesFromBase64(object.columnQualifierRegexFilter))
        : undefined,
      columnRangeFilter: isSet(object.columnRangeFilter) ? ColumnRange.fromJSON(object.columnRangeFilter) : undefined,
      timestampRangeFilter: isSet(object.timestampRangeFilter)
        ? TimestampRange.fromJSON(object.timestampRangeFilter)
        : undefined,
      valueRegexFilter: isSet(object.valueRegexFilter)
        ? Buffer.from(bytesFromBase64(object.valueRegexFilter))
        : undefined,
      valueRangeFilter: isSet(object.valueRangeFilter) ? ValueRange.fromJSON(object.valueRangeFilter) : undefined,
      cellsPerRowOffsetFilter: isSet(object.cellsPerRowOffsetFilter)
        ? globalThis.Number(object.cellsPerRowOffsetFilter)
        : undefined,
      cellsPerRowLimitFilter: isSet(object.cellsPerRowLimitFilter)
        ? globalThis.Number(object.cellsPerRowLimitFilter)
        : undefined,
      cellsPerColumnLimitFilter: isSet(object.cellsPerColumnLimitFilter)
        ? globalThis.Number(object.cellsPerColumnLimitFilter)
        : undefined,
      stripValueTransformer: isSet(object.stripValueTransformer)
        ? globalThis.Boolean(object.stripValueTransformer)
        : undefined,
      applyLabelTransformer: isSet(object.applyLabelTransformer)
        ? globalThis.String(object.applyLabelTransformer)
        : undefined,
    };
  },

  toJSON(message: RowFilter): unknown {
    const obj: any = {};
    if (message.chain !== undefined) {
      obj.chain = RowFilter_Chain.toJSON(message.chain);
    }
    if (message.interleave !== undefined) {
      obj.interleave = RowFilter_Interleave.toJSON(message.interleave);
    }
    if (message.condition !== undefined) {
      obj.condition = RowFilter_Condition.toJSON(message.condition);
    }
    if (message.sink !== undefined) {
      obj.sink = message.sink;
    }
    if (message.passAllFilter !== undefined) {
      obj.passAllFilter = message.passAllFilter;
    }
    if (message.blockAllFilter !== undefined) {
      obj.blockAllFilter = message.blockAllFilter;
    }
    if (message.rowKeyRegexFilter !== undefined) {
      obj.rowKeyRegexFilter = base64FromBytes(message.rowKeyRegexFilter);
    }
    if (message.rowSampleFilter !== undefined) {
      obj.rowSampleFilter = message.rowSampleFilter;
    }
    if (message.familyNameRegexFilter !== undefined) {
      obj.familyNameRegexFilter = message.familyNameRegexFilter;
    }
    if (message.columnQualifierRegexFilter !== undefined) {
      obj.columnQualifierRegexFilter = base64FromBytes(message.columnQualifierRegexFilter);
    }
    if (message.columnRangeFilter !== undefined) {
      obj.columnRangeFilter = ColumnRange.toJSON(message.columnRangeFilter);
    }
    if (message.timestampRangeFilter !== undefined) {
      obj.timestampRangeFilter = TimestampRange.toJSON(message.timestampRangeFilter);
    }
    if (message.valueRegexFilter !== undefined) {
      obj.valueRegexFilter = base64FromBytes(message.valueRegexFilter);
    }
    if (message.valueRangeFilter !== undefined) {
      obj.valueRangeFilter = ValueRange.toJSON(message.valueRangeFilter);
    }
    if (message.cellsPerRowOffsetFilter !== undefined) {
      obj.cellsPerRowOffsetFilter = Math.round(message.cellsPerRowOffsetFilter);
    }
    if (message.cellsPerRowLimitFilter !== undefined) {
      obj.cellsPerRowLimitFilter = Math.round(message.cellsPerRowLimitFilter);
    }
    if (message.cellsPerColumnLimitFilter !== undefined) {
      obj.cellsPerColumnLimitFilter = Math.round(message.cellsPerColumnLimitFilter);
    }
    if (message.stripValueTransformer !== undefined) {
      obj.stripValueTransformer = message.stripValueTransformer;
    }
    if (message.applyLabelTransformer !== undefined) {
      obj.applyLabelTransformer = message.applyLabelTransformer;
    }
    return obj;
  },

  create(base?: DeepPartial<RowFilter>): RowFilter {
    return RowFilter.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RowFilter>): RowFilter {
    const message = createBaseRowFilter();
    message.chain = (object.chain !== undefined && object.chain !== null)
      ? RowFilter_Chain.fromPartial(object.chain)
      : undefined;
    message.interleave = (object.interleave !== undefined && object.interleave !== null)
      ? RowFilter_Interleave.fromPartial(object.interleave)
      : undefined;
    message.condition = (object.condition !== undefined && object.condition !== null)
      ? RowFilter_Condition.fromPartial(object.condition)
      : undefined;
    message.sink = object.sink ?? undefined;
    message.passAllFilter = object.passAllFilter ?? undefined;
    message.blockAllFilter = object.blockAllFilter ?? undefined;
    message.rowKeyRegexFilter = object.rowKeyRegexFilter ?? undefined;
    message.rowSampleFilter = object.rowSampleFilter ?? undefined;
    message.familyNameRegexFilter = object.familyNameRegexFilter ?? undefined;
    message.columnQualifierRegexFilter = object.columnQualifierRegexFilter ?? undefined;
    message.columnRangeFilter = (object.columnRangeFilter !== undefined && object.columnRangeFilter !== null)
      ? ColumnRange.fromPartial(object.columnRangeFilter)
      : undefined;
    message.timestampRangeFilter = (object.timestampRangeFilter !== undefined && object.timestampRangeFilter !== null)
      ? TimestampRange.fromPartial(object.timestampRangeFilter)
      : undefined;
    message.valueRegexFilter = object.valueRegexFilter ?? undefined;
    message.valueRangeFilter = (object.valueRangeFilter !== undefined && object.valueRangeFilter !== null)
      ? ValueRange.fromPartial(object.valueRangeFilter)
      : undefined;
    message.cellsPerRowOffsetFilter = object.cellsPerRowOffsetFilter ?? undefined;
    message.cellsPerRowLimitFilter = object.cellsPerRowLimitFilter ?? undefined;
    message.cellsPerColumnLimitFilter = object.cellsPerColumnLimitFilter ?? undefined;
    message.stripValueTransformer = object.stripValueTransformer ?? undefined;
    message.applyLabelTransformer = object.applyLabelTransformer ?? undefined;
    return message;
  },
};

function createBaseRowFilter_Chain(): RowFilter_Chain {
  return { filters: [] };
}

export const RowFilter_Chain: MessageFns<RowFilter_Chain> = {
  encode(message: RowFilter_Chain, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.filters) {
      RowFilter.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RowFilter_Chain {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRowFilter_Chain();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.filters.push(RowFilter.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RowFilter_Chain {
    return {
      filters: globalThis.Array.isArray(object?.filters) ? object.filters.map((e: any) => RowFilter.fromJSON(e)) : [],
    };
  },

  toJSON(message: RowFilter_Chain): unknown {
    const obj: any = {};
    if (message.filters?.length) {
      obj.filters = message.filters.map((e) => RowFilter.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<RowFilter_Chain>): RowFilter_Chain {
    return RowFilter_Chain.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RowFilter_Chain>): RowFilter_Chain {
    const message = createBaseRowFilter_Chain();
    message.filters = object.filters?.map((e) => RowFilter.fromPartial(e)) || [];
    return message;
  },
};

function createBaseRowFilter_Interleave(): RowFilter_Interleave {
  return { filters: [] };
}

export const RowFilter_Interleave: MessageFns<RowFilter_Interleave> = {
  encode(message: RowFilter_Interleave, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.filters) {
      RowFilter.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RowFilter_Interleave {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRowFilter_Interleave();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.filters.push(RowFilter.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RowFilter_Interleave {
    return {
      filters: globalThis.Array.isArray(object?.filters) ? object.filters.map((e: any) => RowFilter.fromJSON(e)) : [],
    };
  },

  toJSON(message: RowFilter_Interleave): unknown {
    const obj: any = {};
    if (message.filters?.length) {
      obj.filters = message.filters.map((e) => RowFilter.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<RowFilter_Interleave>): RowFilter_Interleave {
    return RowFilter_Interleave.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RowFilter_Interleave>): RowFilter_Interleave {
    const message = createBaseRowFilter_Interleave();
    message.filters = object.filters?.map((e) => RowFilter.fromPartial(e)) || [];
    return message;
  },
};

function createBaseRowFilter_Condition(): RowFilter_Condition {
  return { predicateFilter: undefined, trueFilter: undefined, falseFilter: undefined };
}

export const RowFilter_Condition: MessageFns<RowFilter_Condition> = {
  encode(message: RowFilter_Condition, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.predicateFilter !== undefined) {
      RowFilter.encode(message.predicateFilter, writer.uint32(10).fork()).join();
    }
    if (message.trueFilter !== undefined) {
      RowFilter.encode(message.trueFilter, writer.uint32(18).fork()).join();
    }
    if (message.falseFilter !== undefined) {
      RowFilter.encode(message.falseFilter, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RowFilter_Condition {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRowFilter_Condition();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.predicateFilter = RowFilter.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.trueFilter = RowFilter.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.falseFilter = RowFilter.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RowFilter_Condition {
    return {
      predicateFilter: isSet(object.predicateFilter) ? RowFilter.fromJSON(object.predicateFilter) : undefined,
      trueFilter: isSet(object.trueFilter) ? RowFilter.fromJSON(object.trueFilter) : undefined,
      falseFilter: isSet(object.falseFilter) ? RowFilter.fromJSON(object.falseFilter) : undefined,
    };
  },

  toJSON(message: RowFilter_Condition): unknown {
    const obj: any = {};
    if (message.predicateFilter !== undefined) {
      obj.predicateFilter = RowFilter.toJSON(message.predicateFilter);
    }
    if (message.trueFilter !== undefined) {
      obj.trueFilter = RowFilter.toJSON(message.trueFilter);
    }
    if (message.falseFilter !== undefined) {
      obj.falseFilter = RowFilter.toJSON(message.falseFilter);
    }
    return obj;
  },

  create(base?: DeepPartial<RowFilter_Condition>): RowFilter_Condition {
    return RowFilter_Condition.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RowFilter_Condition>): RowFilter_Condition {
    const message = createBaseRowFilter_Condition();
    message.predicateFilter = (object.predicateFilter !== undefined && object.predicateFilter !== null)
      ? RowFilter.fromPartial(object.predicateFilter)
      : undefined;
    message.trueFilter = (object.trueFilter !== undefined && object.trueFilter !== null)
      ? RowFilter.fromPartial(object.trueFilter)
      : undefined;
    message.falseFilter = (object.falseFilter !== undefined && object.falseFilter !== null)
      ? RowFilter.fromPartial(object.falseFilter)
      : undefined;
    return message;
  },
};

function createBaseMutation(): Mutation {
  return {
    setCell: undefined,
    addToCell: undefined,
    mergeToCell: undefined,
    deleteFromColumn: undefined,
    deleteFromFamily: undefined,
    deleteFromRow: undefined,
  };
}

export const Mutation: MessageFns<Mutation> = {
  encode(message: Mutation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.setCell !== undefined) {
      Mutation_SetCell.encode(message.setCell, writer.uint32(10).fork()).join();
    }
    if (message.addToCell !== undefined) {
      Mutation_AddToCell.encode(message.addToCell, writer.uint32(42).fork()).join();
    }
    if (message.mergeToCell !== undefined) {
      Mutation_MergeToCell.encode(message.mergeToCell, writer.uint32(50).fork()).join();
    }
    if (message.deleteFromColumn !== undefined) {
      Mutation_DeleteFromColumn.encode(message.deleteFromColumn, writer.uint32(18).fork()).join();
    }
    if (message.deleteFromFamily !== undefined) {
      Mutation_DeleteFromFamily.encode(message.deleteFromFamily, writer.uint32(26).fork()).join();
    }
    if (message.deleteFromRow !== undefined) {
      Mutation_DeleteFromRow.encode(message.deleteFromRow, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Mutation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMutation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.setCell = Mutation_SetCell.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.addToCell = Mutation_AddToCell.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.mergeToCell = Mutation_MergeToCell.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.deleteFromColumn = Mutation_DeleteFromColumn.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.deleteFromFamily = Mutation_DeleteFromFamily.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.deleteFromRow = Mutation_DeleteFromRow.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Mutation {
    return {
      setCell: isSet(object.setCell) ? Mutation_SetCell.fromJSON(object.setCell) : undefined,
      addToCell: isSet(object.addToCell) ? Mutation_AddToCell.fromJSON(object.addToCell) : undefined,
      mergeToCell: isSet(object.mergeToCell) ? Mutation_MergeToCell.fromJSON(object.mergeToCell) : undefined,
      deleteFromColumn: isSet(object.deleteFromColumn)
        ? Mutation_DeleteFromColumn.fromJSON(object.deleteFromColumn)
        : undefined,
      deleteFromFamily: isSet(object.deleteFromFamily)
        ? Mutation_DeleteFromFamily.fromJSON(object.deleteFromFamily)
        : undefined,
      deleteFromRow: isSet(object.deleteFromRow) ? Mutation_DeleteFromRow.fromJSON(object.deleteFromRow) : undefined,
    };
  },

  toJSON(message: Mutation): unknown {
    const obj: any = {};
    if (message.setCell !== undefined) {
      obj.setCell = Mutation_SetCell.toJSON(message.setCell);
    }
    if (message.addToCell !== undefined) {
      obj.addToCell = Mutation_AddToCell.toJSON(message.addToCell);
    }
    if (message.mergeToCell !== undefined) {
      obj.mergeToCell = Mutation_MergeToCell.toJSON(message.mergeToCell);
    }
    if (message.deleteFromColumn !== undefined) {
      obj.deleteFromColumn = Mutation_DeleteFromColumn.toJSON(message.deleteFromColumn);
    }
    if (message.deleteFromFamily !== undefined) {
      obj.deleteFromFamily = Mutation_DeleteFromFamily.toJSON(message.deleteFromFamily);
    }
    if (message.deleteFromRow !== undefined) {
      obj.deleteFromRow = Mutation_DeleteFromRow.toJSON(message.deleteFromRow);
    }
    return obj;
  },

  create(base?: DeepPartial<Mutation>): Mutation {
    return Mutation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Mutation>): Mutation {
    const message = createBaseMutation();
    message.setCell = (object.setCell !== undefined && object.setCell !== null)
      ? Mutation_SetCell.fromPartial(object.setCell)
      : undefined;
    message.addToCell = (object.addToCell !== undefined && object.addToCell !== null)
      ? Mutation_AddToCell.fromPartial(object.addToCell)
      : undefined;
    message.mergeToCell = (object.mergeToCell !== undefined && object.mergeToCell !== null)
      ? Mutation_MergeToCell.fromPartial(object.mergeToCell)
      : undefined;
    message.deleteFromColumn = (object.deleteFromColumn !== undefined && object.deleteFromColumn !== null)
      ? Mutation_DeleteFromColumn.fromPartial(object.deleteFromColumn)
      : undefined;
    message.deleteFromFamily = (object.deleteFromFamily !== undefined && object.deleteFromFamily !== null)
      ? Mutation_DeleteFromFamily.fromPartial(object.deleteFromFamily)
      : undefined;
    message.deleteFromRow = (object.deleteFromRow !== undefined && object.deleteFromRow !== null)
      ? Mutation_DeleteFromRow.fromPartial(object.deleteFromRow)
      : undefined;
    return message;
  },
};

function createBaseMutation_SetCell(): Mutation_SetCell {
  return { familyName: "", columnQualifier: Buffer.alloc(0), timestampMicros: Long.ZERO, value: Buffer.alloc(0) };
}

export const Mutation_SetCell: MessageFns<Mutation_SetCell> = {
  encode(message: Mutation_SetCell, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.familyName !== "") {
      writer.uint32(10).string(message.familyName);
    }
    if (message.columnQualifier.length !== 0) {
      writer.uint32(18).bytes(message.columnQualifier);
    }
    if (!message.timestampMicros.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.timestampMicros.toString());
    }
    if (message.value.length !== 0) {
      writer.uint32(34).bytes(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Mutation_SetCell {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMutation_SetCell();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.familyName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.columnQualifier = Buffer.from(reader.bytes());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.timestampMicros = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.value = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Mutation_SetCell {
    return {
      familyName: isSet(object.familyName) ? globalThis.String(object.familyName) : "",
      columnQualifier: isSet(object.columnQualifier)
        ? Buffer.from(bytesFromBase64(object.columnQualifier))
        : Buffer.alloc(0),
      timestampMicros: isSet(object.timestampMicros) ? Long.fromValue(object.timestampMicros) : Long.ZERO,
      value: isSet(object.value) ? Buffer.from(bytesFromBase64(object.value)) : Buffer.alloc(0),
    };
  },

  toJSON(message: Mutation_SetCell): unknown {
    const obj: any = {};
    if (message.familyName !== "") {
      obj.familyName = message.familyName;
    }
    if (message.columnQualifier.length !== 0) {
      obj.columnQualifier = base64FromBytes(message.columnQualifier);
    }
    if (!message.timestampMicros.equals(Long.ZERO)) {
      obj.timestampMicros = (message.timestampMicros || Long.ZERO).toString();
    }
    if (message.value.length !== 0) {
      obj.value = base64FromBytes(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<Mutation_SetCell>): Mutation_SetCell {
    return Mutation_SetCell.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Mutation_SetCell>): Mutation_SetCell {
    const message = createBaseMutation_SetCell();
    message.familyName = object.familyName ?? "";
    message.columnQualifier = object.columnQualifier ?? Buffer.alloc(0);
    message.timestampMicros = (object.timestampMicros !== undefined && object.timestampMicros !== null)
      ? Long.fromValue(object.timestampMicros)
      : Long.ZERO;
    message.value = object.value ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseMutation_AddToCell(): Mutation_AddToCell {
  return { familyName: "", columnQualifier: undefined, timestamp: undefined, input: undefined };
}

export const Mutation_AddToCell: MessageFns<Mutation_AddToCell> = {
  encode(message: Mutation_AddToCell, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.familyName !== "") {
      writer.uint32(10).string(message.familyName);
    }
    if (message.columnQualifier !== undefined) {
      Value.encode(message.columnQualifier, writer.uint32(18).fork()).join();
    }
    if (message.timestamp !== undefined) {
      Value.encode(message.timestamp, writer.uint32(26).fork()).join();
    }
    if (message.input !== undefined) {
      Value.encode(message.input, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Mutation_AddToCell {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMutation_AddToCell();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.familyName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.columnQualifier = Value.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.timestamp = Value.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.input = Value.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Mutation_AddToCell {
    return {
      familyName: isSet(object.familyName) ? globalThis.String(object.familyName) : "",
      columnQualifier: isSet(object.columnQualifier) ? Value.fromJSON(object.columnQualifier) : undefined,
      timestamp: isSet(object.timestamp) ? Value.fromJSON(object.timestamp) : undefined,
      input: isSet(object.input) ? Value.fromJSON(object.input) : undefined,
    };
  },

  toJSON(message: Mutation_AddToCell): unknown {
    const obj: any = {};
    if (message.familyName !== "") {
      obj.familyName = message.familyName;
    }
    if (message.columnQualifier !== undefined) {
      obj.columnQualifier = Value.toJSON(message.columnQualifier);
    }
    if (message.timestamp !== undefined) {
      obj.timestamp = Value.toJSON(message.timestamp);
    }
    if (message.input !== undefined) {
      obj.input = Value.toJSON(message.input);
    }
    return obj;
  },

  create(base?: DeepPartial<Mutation_AddToCell>): Mutation_AddToCell {
    return Mutation_AddToCell.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Mutation_AddToCell>): Mutation_AddToCell {
    const message = createBaseMutation_AddToCell();
    message.familyName = object.familyName ?? "";
    message.columnQualifier = (object.columnQualifier !== undefined && object.columnQualifier !== null)
      ? Value.fromPartial(object.columnQualifier)
      : undefined;
    message.timestamp = (object.timestamp !== undefined && object.timestamp !== null)
      ? Value.fromPartial(object.timestamp)
      : undefined;
    message.input = (object.input !== undefined && object.input !== null) ? Value.fromPartial(object.input) : undefined;
    return message;
  },
};

function createBaseMutation_MergeToCell(): Mutation_MergeToCell {
  return { familyName: "", columnQualifier: undefined, timestamp: undefined, input: undefined };
}

export const Mutation_MergeToCell: MessageFns<Mutation_MergeToCell> = {
  encode(message: Mutation_MergeToCell, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.familyName !== "") {
      writer.uint32(10).string(message.familyName);
    }
    if (message.columnQualifier !== undefined) {
      Value.encode(message.columnQualifier, writer.uint32(18).fork()).join();
    }
    if (message.timestamp !== undefined) {
      Value.encode(message.timestamp, writer.uint32(26).fork()).join();
    }
    if (message.input !== undefined) {
      Value.encode(message.input, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Mutation_MergeToCell {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMutation_MergeToCell();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.familyName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.columnQualifier = Value.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.timestamp = Value.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.input = Value.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Mutation_MergeToCell {
    return {
      familyName: isSet(object.familyName) ? globalThis.String(object.familyName) : "",
      columnQualifier: isSet(object.columnQualifier) ? Value.fromJSON(object.columnQualifier) : undefined,
      timestamp: isSet(object.timestamp) ? Value.fromJSON(object.timestamp) : undefined,
      input: isSet(object.input) ? Value.fromJSON(object.input) : undefined,
    };
  },

  toJSON(message: Mutation_MergeToCell): unknown {
    const obj: any = {};
    if (message.familyName !== "") {
      obj.familyName = message.familyName;
    }
    if (message.columnQualifier !== undefined) {
      obj.columnQualifier = Value.toJSON(message.columnQualifier);
    }
    if (message.timestamp !== undefined) {
      obj.timestamp = Value.toJSON(message.timestamp);
    }
    if (message.input !== undefined) {
      obj.input = Value.toJSON(message.input);
    }
    return obj;
  },

  create(base?: DeepPartial<Mutation_MergeToCell>): Mutation_MergeToCell {
    return Mutation_MergeToCell.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Mutation_MergeToCell>): Mutation_MergeToCell {
    const message = createBaseMutation_MergeToCell();
    message.familyName = object.familyName ?? "";
    message.columnQualifier = (object.columnQualifier !== undefined && object.columnQualifier !== null)
      ? Value.fromPartial(object.columnQualifier)
      : undefined;
    message.timestamp = (object.timestamp !== undefined && object.timestamp !== null)
      ? Value.fromPartial(object.timestamp)
      : undefined;
    message.input = (object.input !== undefined && object.input !== null) ? Value.fromPartial(object.input) : undefined;
    return message;
  },
};

function createBaseMutation_DeleteFromColumn(): Mutation_DeleteFromColumn {
  return { familyName: "", columnQualifier: Buffer.alloc(0), timeRange: undefined };
}

export const Mutation_DeleteFromColumn: MessageFns<Mutation_DeleteFromColumn> = {
  encode(message: Mutation_DeleteFromColumn, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.familyName !== "") {
      writer.uint32(10).string(message.familyName);
    }
    if (message.columnQualifier.length !== 0) {
      writer.uint32(18).bytes(message.columnQualifier);
    }
    if (message.timeRange !== undefined) {
      TimestampRange.encode(message.timeRange, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Mutation_DeleteFromColumn {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMutation_DeleteFromColumn();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.familyName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.columnQualifier = Buffer.from(reader.bytes());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.timeRange = TimestampRange.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Mutation_DeleteFromColumn {
    return {
      familyName: isSet(object.familyName) ? globalThis.String(object.familyName) : "",
      columnQualifier: isSet(object.columnQualifier)
        ? Buffer.from(bytesFromBase64(object.columnQualifier))
        : Buffer.alloc(0),
      timeRange: isSet(object.timeRange) ? TimestampRange.fromJSON(object.timeRange) : undefined,
    };
  },

  toJSON(message: Mutation_DeleteFromColumn): unknown {
    const obj: any = {};
    if (message.familyName !== "") {
      obj.familyName = message.familyName;
    }
    if (message.columnQualifier.length !== 0) {
      obj.columnQualifier = base64FromBytes(message.columnQualifier);
    }
    if (message.timeRange !== undefined) {
      obj.timeRange = TimestampRange.toJSON(message.timeRange);
    }
    return obj;
  },

  create(base?: DeepPartial<Mutation_DeleteFromColumn>): Mutation_DeleteFromColumn {
    return Mutation_DeleteFromColumn.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Mutation_DeleteFromColumn>): Mutation_DeleteFromColumn {
    const message = createBaseMutation_DeleteFromColumn();
    message.familyName = object.familyName ?? "";
    message.columnQualifier = object.columnQualifier ?? Buffer.alloc(0);
    message.timeRange = (object.timeRange !== undefined && object.timeRange !== null)
      ? TimestampRange.fromPartial(object.timeRange)
      : undefined;
    return message;
  },
};

function createBaseMutation_DeleteFromFamily(): Mutation_DeleteFromFamily {
  return { familyName: "" };
}

export const Mutation_DeleteFromFamily: MessageFns<Mutation_DeleteFromFamily> = {
  encode(message: Mutation_DeleteFromFamily, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.familyName !== "") {
      writer.uint32(10).string(message.familyName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Mutation_DeleteFromFamily {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMutation_DeleteFromFamily();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.familyName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Mutation_DeleteFromFamily {
    return { familyName: isSet(object.familyName) ? globalThis.String(object.familyName) : "" };
  },

  toJSON(message: Mutation_DeleteFromFamily): unknown {
    const obj: any = {};
    if (message.familyName !== "") {
      obj.familyName = message.familyName;
    }
    return obj;
  },

  create(base?: DeepPartial<Mutation_DeleteFromFamily>): Mutation_DeleteFromFamily {
    return Mutation_DeleteFromFamily.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Mutation_DeleteFromFamily>): Mutation_DeleteFromFamily {
    const message = createBaseMutation_DeleteFromFamily();
    message.familyName = object.familyName ?? "";
    return message;
  },
};

function createBaseMutation_DeleteFromRow(): Mutation_DeleteFromRow {
  return {};
}

export const Mutation_DeleteFromRow: MessageFns<Mutation_DeleteFromRow> = {
  encode(_: Mutation_DeleteFromRow, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Mutation_DeleteFromRow {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMutation_DeleteFromRow();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): Mutation_DeleteFromRow {
    return {};
  },

  toJSON(_: Mutation_DeleteFromRow): unknown {
    const obj: any = {};
    return obj;
  },

  create(base?: DeepPartial<Mutation_DeleteFromRow>): Mutation_DeleteFromRow {
    return Mutation_DeleteFromRow.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<Mutation_DeleteFromRow>): Mutation_DeleteFromRow {
    const message = createBaseMutation_DeleteFromRow();
    return message;
  },
};

function createBaseReadModifyWriteRule(): ReadModifyWriteRule {
  return { familyName: "", columnQualifier: Buffer.alloc(0), appendValue: undefined, incrementAmount: undefined };
}

export const ReadModifyWriteRule: MessageFns<ReadModifyWriteRule> = {
  encode(message: ReadModifyWriteRule, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.familyName !== "") {
      writer.uint32(10).string(message.familyName);
    }
    if (message.columnQualifier.length !== 0) {
      writer.uint32(18).bytes(message.columnQualifier);
    }
    if (message.appendValue !== undefined) {
      writer.uint32(26).bytes(message.appendValue);
    }
    if (message.incrementAmount !== undefined) {
      writer.uint32(32).int64(message.incrementAmount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReadModifyWriteRule {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReadModifyWriteRule();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.familyName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.columnQualifier = Buffer.from(reader.bytes());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.appendValue = Buffer.from(reader.bytes());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.incrementAmount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReadModifyWriteRule {
    return {
      familyName: isSet(object.familyName) ? globalThis.String(object.familyName) : "",
      columnQualifier: isSet(object.columnQualifier)
        ? Buffer.from(bytesFromBase64(object.columnQualifier))
        : Buffer.alloc(0),
      appendValue: isSet(object.appendValue) ? Buffer.from(bytesFromBase64(object.appendValue)) : undefined,
      incrementAmount: isSet(object.incrementAmount) ? Long.fromValue(object.incrementAmount) : undefined,
    };
  },

  toJSON(message: ReadModifyWriteRule): unknown {
    const obj: any = {};
    if (message.familyName !== "") {
      obj.familyName = message.familyName;
    }
    if (message.columnQualifier.length !== 0) {
      obj.columnQualifier = base64FromBytes(message.columnQualifier);
    }
    if (message.appendValue !== undefined) {
      obj.appendValue = base64FromBytes(message.appendValue);
    }
    if (message.incrementAmount !== undefined) {
      obj.incrementAmount = (message.incrementAmount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ReadModifyWriteRule>): ReadModifyWriteRule {
    return ReadModifyWriteRule.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReadModifyWriteRule>): ReadModifyWriteRule {
    const message = createBaseReadModifyWriteRule();
    message.familyName = object.familyName ?? "";
    message.columnQualifier = object.columnQualifier ?? Buffer.alloc(0);
    message.appendValue = object.appendValue ?? undefined;
    message.incrementAmount = (object.incrementAmount !== undefined && object.incrementAmount !== null)
      ? Long.fromValue(object.incrementAmount)
      : undefined;
    return message;
  },
};

function createBaseStreamPartition(): StreamPartition {
  return { rowRange: undefined };
}

export const StreamPartition: MessageFns<StreamPartition> = {
  encode(message: StreamPartition, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.rowRange !== undefined) {
      RowRange.encode(message.rowRange, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamPartition {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamPartition();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.rowRange = RowRange.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamPartition {
    return { rowRange: isSet(object.rowRange) ? RowRange.fromJSON(object.rowRange) : undefined };
  },

  toJSON(message: StreamPartition): unknown {
    const obj: any = {};
    if (message.rowRange !== undefined) {
      obj.rowRange = RowRange.toJSON(message.rowRange);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamPartition>): StreamPartition {
    return StreamPartition.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamPartition>): StreamPartition {
    const message = createBaseStreamPartition();
    message.rowRange = (object.rowRange !== undefined && object.rowRange !== null)
      ? RowRange.fromPartial(object.rowRange)
      : undefined;
    return message;
  },
};

function createBaseStreamContinuationTokens(): StreamContinuationTokens {
  return { tokens: [] };
}

export const StreamContinuationTokens: MessageFns<StreamContinuationTokens> = {
  encode(message: StreamContinuationTokens, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.tokens) {
      StreamContinuationToken.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamContinuationTokens {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamContinuationTokens();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.tokens.push(StreamContinuationToken.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamContinuationTokens {
    return {
      tokens: globalThis.Array.isArray(object?.tokens)
        ? object.tokens.map((e: any) => StreamContinuationToken.fromJSON(e))
        : [],
    };
  },

  toJSON(message: StreamContinuationTokens): unknown {
    const obj: any = {};
    if (message.tokens?.length) {
      obj.tokens = message.tokens.map((e) => StreamContinuationToken.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<StreamContinuationTokens>): StreamContinuationTokens {
    return StreamContinuationTokens.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamContinuationTokens>): StreamContinuationTokens {
    const message = createBaseStreamContinuationTokens();
    message.tokens = object.tokens?.map((e) => StreamContinuationToken.fromPartial(e)) || [];
    return message;
  },
};

function createBaseStreamContinuationToken(): StreamContinuationToken {
  return { partition: undefined, token: "" };
}

export const StreamContinuationToken: MessageFns<StreamContinuationToken> = {
  encode(message: StreamContinuationToken, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.partition !== undefined) {
      StreamPartition.encode(message.partition, writer.uint32(10).fork()).join();
    }
    if (message.token !== "") {
      writer.uint32(18).string(message.token);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamContinuationToken {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamContinuationToken();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.partition = StreamPartition.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.token = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamContinuationToken {
    return {
      partition: isSet(object.partition) ? StreamPartition.fromJSON(object.partition) : undefined,
      token: isSet(object.token) ? globalThis.String(object.token) : "",
    };
  },

  toJSON(message: StreamContinuationToken): unknown {
    const obj: any = {};
    if (message.partition !== undefined) {
      obj.partition = StreamPartition.toJSON(message.partition);
    }
    if (message.token !== "") {
      obj.token = message.token;
    }
    return obj;
  },

  create(base?: DeepPartial<StreamContinuationToken>): StreamContinuationToken {
    return StreamContinuationToken.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamContinuationToken>): StreamContinuationToken {
    const message = createBaseStreamContinuationToken();
    message.partition = (object.partition !== undefined && object.partition !== null)
      ? StreamPartition.fromPartial(object.partition)
      : undefined;
    message.token = object.token ?? "";
    return message;
  },
};

function createBaseProtoFormat(): ProtoFormat {
  return {};
}

export const ProtoFormat: MessageFns<ProtoFormat> = {
  encode(_: ProtoFormat, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ProtoFormat {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProtoFormat();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): ProtoFormat {
    return {};
  },

  toJSON(_: ProtoFormat): unknown {
    const obj: any = {};
    return obj;
  },

  create(base?: DeepPartial<ProtoFormat>): ProtoFormat {
    return ProtoFormat.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<ProtoFormat>): ProtoFormat {
    const message = createBaseProtoFormat();
    return message;
  },
};

function createBaseColumnMetadata(): ColumnMetadata {
  return { name: "", type: undefined };
}

export const ColumnMetadata: MessageFns<ColumnMetadata> = {
  encode(message: ColumnMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.type !== undefined) {
      Type.encode(message.type, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ColumnMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseColumnMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.type = Type.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ColumnMetadata {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      type: isSet(object.type) ? Type.fromJSON(object.type) : undefined,
    };
  },

  toJSON(message: ColumnMetadata): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.type !== undefined) {
      obj.type = Type.toJSON(message.type);
    }
    return obj;
  },

  create(base?: DeepPartial<ColumnMetadata>): ColumnMetadata {
    return ColumnMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ColumnMetadata>): ColumnMetadata {
    const message = createBaseColumnMetadata();
    message.name = object.name ?? "";
    message.type = (object.type !== undefined && object.type !== null) ? Type.fromPartial(object.type) : undefined;
    return message;
  },
};

function createBaseProtoSchema(): ProtoSchema {
  return { columns: [] };
}

export const ProtoSchema: MessageFns<ProtoSchema> = {
  encode(message: ProtoSchema, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.columns) {
      ColumnMetadata.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ProtoSchema {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProtoSchema();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.columns.push(ColumnMetadata.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ProtoSchema {
    return {
      columns: globalThis.Array.isArray(object?.columns)
        ? object.columns.map((e: any) => ColumnMetadata.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ProtoSchema): unknown {
    const obj: any = {};
    if (message.columns?.length) {
      obj.columns = message.columns.map((e) => ColumnMetadata.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ProtoSchema>): ProtoSchema {
    return ProtoSchema.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ProtoSchema>): ProtoSchema {
    const message = createBaseProtoSchema();
    message.columns = object.columns?.map((e) => ColumnMetadata.fromPartial(e)) || [];
    return message;
  },
};

function createBaseResultSetMetadata(): ResultSetMetadata {
  return { protoSchema: undefined };
}

export const ResultSetMetadata: MessageFns<ResultSetMetadata> = {
  encode(message: ResultSetMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.protoSchema !== undefined) {
      ProtoSchema.encode(message.protoSchema, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ResultSetMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseResultSetMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.protoSchema = ProtoSchema.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ResultSetMetadata {
    return { protoSchema: isSet(object.protoSchema) ? ProtoSchema.fromJSON(object.protoSchema) : undefined };
  },

  toJSON(message: ResultSetMetadata): unknown {
    const obj: any = {};
    if (message.protoSchema !== undefined) {
      obj.protoSchema = ProtoSchema.toJSON(message.protoSchema);
    }
    return obj;
  },

  create(base?: DeepPartial<ResultSetMetadata>): ResultSetMetadata {
    return ResultSetMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ResultSetMetadata>): ResultSetMetadata {
    const message = createBaseResultSetMetadata();
    message.protoSchema = (object.protoSchema !== undefined && object.protoSchema !== null)
      ? ProtoSchema.fromPartial(object.protoSchema)
      : undefined;
    return message;
  },
};

function createBaseProtoRows(): ProtoRows {
  return { values: [] };
}

export const ProtoRows: MessageFns<ProtoRows> = {
  encode(message: ProtoRows, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.values) {
      Value.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ProtoRows {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProtoRows();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.values.push(Value.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ProtoRows {
    return { values: globalThis.Array.isArray(object?.values) ? object.values.map((e: any) => Value.fromJSON(e)) : [] };
  },

  toJSON(message: ProtoRows): unknown {
    const obj: any = {};
    if (message.values?.length) {
      obj.values = message.values.map((e) => Value.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ProtoRows>): ProtoRows {
    return ProtoRows.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ProtoRows>): ProtoRows {
    const message = createBaseProtoRows();
    message.values = object.values?.map((e) => Value.fromPartial(e)) || [];
    return message;
  },
};

function createBaseProtoRowsBatch(): ProtoRowsBatch {
  return { batchData: Buffer.alloc(0) };
}

export const ProtoRowsBatch: MessageFns<ProtoRowsBatch> = {
  encode(message: ProtoRowsBatch, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.batchData.length !== 0) {
      writer.uint32(10).bytes(message.batchData);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ProtoRowsBatch {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProtoRowsBatch();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.batchData = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ProtoRowsBatch {
    return { batchData: isSet(object.batchData) ? Buffer.from(bytesFromBase64(object.batchData)) : Buffer.alloc(0) };
  },

  toJSON(message: ProtoRowsBatch): unknown {
    const obj: any = {};
    if (message.batchData.length !== 0) {
      obj.batchData = base64FromBytes(message.batchData);
    }
    return obj;
  },

  create(base?: DeepPartial<ProtoRowsBatch>): ProtoRowsBatch {
    return ProtoRowsBatch.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ProtoRowsBatch>): ProtoRowsBatch {
    const message = createBaseProtoRowsBatch();
    message.batchData = object.batchData ?? Buffer.alloc(0);
    return message;
  },
};

function createBasePartialResultSet(): PartialResultSet {
  return { protoRowsBatch: undefined, resumeToken: Buffer.alloc(0), estimatedBatchSize: 0 };
}

export const PartialResultSet: MessageFns<PartialResultSet> = {
  encode(message: PartialResultSet, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.protoRowsBatch !== undefined) {
      ProtoRowsBatch.encode(message.protoRowsBatch, writer.uint32(26).fork()).join();
    }
    if (message.resumeToken.length !== 0) {
      writer.uint32(42).bytes(message.resumeToken);
    }
    if (message.estimatedBatchSize !== 0) {
      writer.uint32(32).int32(message.estimatedBatchSize);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PartialResultSet {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePartialResultSet();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 26) {
            break;
          }

          message.protoRowsBatch = ProtoRowsBatch.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.resumeToken = Buffer.from(reader.bytes());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.estimatedBatchSize = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PartialResultSet {
    return {
      protoRowsBatch: isSet(object.protoRowsBatch) ? ProtoRowsBatch.fromJSON(object.protoRowsBatch) : undefined,
      resumeToken: isSet(object.resumeToken) ? Buffer.from(bytesFromBase64(object.resumeToken)) : Buffer.alloc(0),
      estimatedBatchSize: isSet(object.estimatedBatchSize) ? globalThis.Number(object.estimatedBatchSize) : 0,
    };
  },

  toJSON(message: PartialResultSet): unknown {
    const obj: any = {};
    if (message.protoRowsBatch !== undefined) {
      obj.protoRowsBatch = ProtoRowsBatch.toJSON(message.protoRowsBatch);
    }
    if (message.resumeToken.length !== 0) {
      obj.resumeToken = base64FromBytes(message.resumeToken);
    }
    if (message.estimatedBatchSize !== 0) {
      obj.estimatedBatchSize = Math.round(message.estimatedBatchSize);
    }
    return obj;
  },

  create(base?: DeepPartial<PartialResultSet>): PartialResultSet {
    return PartialResultSet.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PartialResultSet>): PartialResultSet {
    const message = createBasePartialResultSet();
    message.protoRowsBatch = (object.protoRowsBatch !== undefined && object.protoRowsBatch !== null)
      ? ProtoRowsBatch.fromPartial(object.protoRowsBatch)
      : undefined;
    message.resumeToken = object.resumeToken ?? Buffer.alloc(0);
    message.estimatedBatchSize = object.estimatedBatchSize ?? 0;
    return message;
  },
};

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
