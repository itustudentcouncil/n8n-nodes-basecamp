// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/bigtable/admin/v2/instance.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { StorageType, storageTypeFromJSON, storageTypeToJSON } from "./common.js";

export const protobufPackage = "google.bigtable.admin.v2";

/**
 * A collection of Bigtable [Tables][google.bigtable.admin.v2.Table] and
 * the resources that serve them.
 * All tables in an instance are served from all
 * [Clusters][google.bigtable.admin.v2.Cluster] in the instance.
 */
export interface Instance {
  /**
   * The unique name of the instance. Values are of the form
   * `projects/{project}/instances/[a-z][a-z0-9\\-]+[a-z0-9]`.
   */
  name: string;
  /**
   * Required. The descriptive name for this instance as it appears in UIs.
   * Can be changed at any time, but should be kept globally unique
   * to avoid confusion.
   */
  displayName: string;
  /**
   * (`OutputOnly`)
   * The current state of the instance.
   */
  state: Instance_State;
  /** The type of the instance. Defaults to `PRODUCTION`. */
  type: Instance_Type;
  /**
   * Labels are a flexible and lightweight mechanism for organizing cloud
   * resources into groups that reflect a customer's organizational needs and
   * deployment strategies. They can be used to filter resources and aggregate
   * metrics.
   *
   * * Label keys must be between 1 and 63 characters long and must conform to
   *   the regular expression: `[\p{Ll}\p{Lo}][\p{Ll}\p{Lo}\p{N}_-]{0,62}`.
   * * Label values must be between 0 and 63 characters long and must conform to
   *   the regular expression: `[\p{Ll}\p{Lo}\p{N}_-]{0,63}`.
   * * No more than 64 labels can be associated with a given resource.
   * * Keys and values must both be under 128 bytes.
   */
  labels: { [key: string]: string };
  /**
   * Output only. A server-assigned timestamp representing when this Instance
   * was created. For instances created before this field was added (August
   * 2021), this value is `seconds: 0, nanos: 1`.
   */
  createTime:
    | Date
    | undefined;
  /** Output only. Reserved for future use. */
  satisfiesPzs?: boolean | undefined;
}

/** Possible states of an instance. */
export enum Instance_State {
  /** STATE_NOT_KNOWN - The state of the instance could not be determined. */
  STATE_NOT_KNOWN = 0,
  /**
   * READY - The instance has been successfully created and can serve requests
   * to its tables.
   */
  READY = 1,
  /**
   * CREATING - The instance is currently being created, and may be destroyed
   * if the creation process encounters an error.
   */
  CREATING = 2,
  UNRECOGNIZED = -1,
}

export function instance_StateFromJSON(object: any): Instance_State {
  switch (object) {
    case 0:
    case "STATE_NOT_KNOWN":
      return Instance_State.STATE_NOT_KNOWN;
    case 1:
    case "READY":
      return Instance_State.READY;
    case 2:
    case "CREATING":
      return Instance_State.CREATING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Instance_State.UNRECOGNIZED;
  }
}

export function instance_StateToJSON(object: Instance_State): string {
  switch (object) {
    case Instance_State.STATE_NOT_KNOWN:
      return "STATE_NOT_KNOWN";
    case Instance_State.READY:
      return "READY";
    case Instance_State.CREATING:
      return "CREATING";
    case Instance_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The type of the instance. */
export enum Instance_Type {
  /**
   * TYPE_UNSPECIFIED - The type of the instance is unspecified. If set when creating an
   * instance, a `PRODUCTION` instance will be created. If set when updating
   * an instance, the type will be left unchanged.
   */
  TYPE_UNSPECIFIED = 0,
  /**
   * PRODUCTION - An instance meant for production use. `serve_nodes` must be set
   * on the cluster.
   */
  PRODUCTION = 1,
  /**
   * DEVELOPMENT - DEPRECATED: Prefer PRODUCTION for all use cases, as it no longer enforces
   * a higher minimum node count than DEVELOPMENT.
   */
  DEVELOPMENT = 2,
  UNRECOGNIZED = -1,
}

export function instance_TypeFromJSON(object: any): Instance_Type {
  switch (object) {
    case 0:
    case "TYPE_UNSPECIFIED":
      return Instance_Type.TYPE_UNSPECIFIED;
    case 1:
    case "PRODUCTION":
      return Instance_Type.PRODUCTION;
    case 2:
    case "DEVELOPMENT":
      return Instance_Type.DEVELOPMENT;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Instance_Type.UNRECOGNIZED;
  }
}

export function instance_TypeToJSON(object: Instance_Type): string {
  switch (object) {
    case Instance_Type.TYPE_UNSPECIFIED:
      return "TYPE_UNSPECIFIED";
    case Instance_Type.PRODUCTION:
      return "PRODUCTION";
    case Instance_Type.DEVELOPMENT:
      return "DEVELOPMENT";
    case Instance_Type.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export interface Instance_LabelsEntry {
  key: string;
  value: string;
}

/** The Autoscaling targets for a Cluster. These determine the recommended nodes. */
export interface AutoscalingTargets {
  /**
   * The cpu utilization that the Autoscaler should be trying to achieve.
   * This number is on a scale from 0 (no utilization) to
   * 100 (total utilization), and is limited between 10 and 80, otherwise it
   * will return INVALID_ARGUMENT error.
   */
  cpuUtilizationPercent: number;
  /**
   * The storage utilization that the Autoscaler should be trying to achieve.
   * This number is limited between 2560 (2.5TiB) and 5120 (5TiB) for a SSD
   * cluster and between 8192 (8TiB) and 16384 (16TiB) for an HDD cluster,
   * otherwise it will return INVALID_ARGUMENT error. If this value is set to 0,
   * it will be treated as if it were set to the default value: 2560 for SSD,
   * 8192 for HDD.
   */
  storageUtilizationGibPerNode: number;
}

/** Limits for the number of nodes a Cluster can autoscale up/down to. */
export interface AutoscalingLimits {
  /** Required. Minimum number of nodes to scale down to. */
  minServeNodes: number;
  /** Required. Maximum number of nodes to scale up to. */
  maxServeNodes: number;
}

/**
 * A resizable group of nodes in a particular cloud location, capable
 * of serving all [Tables][google.bigtable.admin.v2.Table] in the parent
 * [Instance][google.bigtable.admin.v2.Instance].
 */
export interface Cluster {
  /**
   * The unique name of the cluster. Values are of the form
   * `projects/{project}/instances/{instance}/clusters/[a-z][-a-z0-9]*`.
   */
  name: string;
  /**
   * Immutable. The location where this cluster's nodes and storage reside. For
   * best performance, clients should be located as close as possible to this
   * cluster. Currently only zones are supported, so values should be of the
   * form `projects/{project}/locations/{zone}`.
   */
  location: string;
  /** Output only. The current state of the cluster. */
  state: Cluster_State;
  /**
   * The number of nodes allocated to this cluster. More nodes enable higher
   * throughput and more consistent performance.
   */
  serveNodes: number;
  /** Immutable. The node scaling factor of this cluster. */
  nodeScalingFactor: Cluster_NodeScalingFactor;
  /** Configuration for this cluster. */
  clusterConfig?:
    | Cluster_ClusterConfig
    | undefined;
  /**
   * Immutable. The type of storage used by this cluster to serve its
   * parent instance's tables, unless explicitly overridden.
   */
  defaultStorageType: StorageType;
  /** Immutable. The encryption configuration for CMEK-protected clusters. */
  encryptionConfig: Cluster_EncryptionConfig | undefined;
}

/** Possible states of a cluster. */
export enum Cluster_State {
  /** STATE_NOT_KNOWN - The state of the cluster could not be determined. */
  STATE_NOT_KNOWN = 0,
  /** READY - The cluster has been successfully created and is ready to serve requests. */
  READY = 1,
  /**
   * CREATING - The cluster is currently being created, and may be destroyed
   * if the creation process encounters an error.
   * A cluster may not be able to serve requests while being created.
   */
  CREATING = 2,
  /**
   * RESIZING - The cluster is currently being resized, and may revert to its previous
   * node count if the process encounters an error.
   * A cluster is still capable of serving requests while being resized,
   * but may exhibit performance as if its number of allocated nodes is
   * between the starting and requested states.
   */
  RESIZING = 3,
  /**
   * DISABLED - The cluster has no backing nodes. The data (tables) still
   * exist, but no operations can be performed on the cluster.
   */
  DISABLED = 4,
  UNRECOGNIZED = -1,
}

export function cluster_StateFromJSON(object: any): Cluster_State {
  switch (object) {
    case 0:
    case "STATE_NOT_KNOWN":
      return Cluster_State.STATE_NOT_KNOWN;
    case 1:
    case "READY":
      return Cluster_State.READY;
    case 2:
    case "CREATING":
      return Cluster_State.CREATING;
    case 3:
    case "RESIZING":
      return Cluster_State.RESIZING;
    case 4:
    case "DISABLED":
      return Cluster_State.DISABLED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Cluster_State.UNRECOGNIZED;
  }
}

export function cluster_StateToJSON(object: Cluster_State): string {
  switch (object) {
    case Cluster_State.STATE_NOT_KNOWN:
      return "STATE_NOT_KNOWN";
    case Cluster_State.READY:
      return "READY";
    case Cluster_State.CREATING:
      return "CREATING";
    case Cluster_State.RESIZING:
      return "RESIZING";
    case Cluster_State.DISABLED:
      return "DISABLED";
    case Cluster_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Possible node scaling factors of the clusters. Node scaling delivers better
 * latency and more throughput by removing node boundaries.
 */
export enum Cluster_NodeScalingFactor {
  /** NODE_SCALING_FACTOR_UNSPECIFIED - No node scaling specified. Defaults to NODE_SCALING_FACTOR_1X. */
  NODE_SCALING_FACTOR_UNSPECIFIED = 0,
  /** NODE_SCALING_FACTOR_1X - The cluster is running with a scaling factor of 1. */
  NODE_SCALING_FACTOR_1X = 1,
  /**
   * NODE_SCALING_FACTOR_2X - The cluster is running with a scaling factor of 2.
   * All node count values must be in increments of 2 with this scaling factor
   * enabled, otherwise an INVALID_ARGUMENT error will be returned.
   */
  NODE_SCALING_FACTOR_2X = 2,
  UNRECOGNIZED = -1,
}

export function cluster_NodeScalingFactorFromJSON(object: any): Cluster_NodeScalingFactor {
  switch (object) {
    case 0:
    case "NODE_SCALING_FACTOR_UNSPECIFIED":
      return Cluster_NodeScalingFactor.NODE_SCALING_FACTOR_UNSPECIFIED;
    case 1:
    case "NODE_SCALING_FACTOR_1X":
      return Cluster_NodeScalingFactor.NODE_SCALING_FACTOR_1X;
    case 2:
    case "NODE_SCALING_FACTOR_2X":
      return Cluster_NodeScalingFactor.NODE_SCALING_FACTOR_2X;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Cluster_NodeScalingFactor.UNRECOGNIZED;
  }
}

export function cluster_NodeScalingFactorToJSON(object: Cluster_NodeScalingFactor): string {
  switch (object) {
    case Cluster_NodeScalingFactor.NODE_SCALING_FACTOR_UNSPECIFIED:
      return "NODE_SCALING_FACTOR_UNSPECIFIED";
    case Cluster_NodeScalingFactor.NODE_SCALING_FACTOR_1X:
      return "NODE_SCALING_FACTOR_1X";
    case Cluster_NodeScalingFactor.NODE_SCALING_FACTOR_2X:
      return "NODE_SCALING_FACTOR_2X";
    case Cluster_NodeScalingFactor.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Autoscaling config for a cluster. */
export interface Cluster_ClusterAutoscalingConfig {
  /** Required. Autoscaling limits for this cluster. */
  autoscalingLimits:
    | AutoscalingLimits
    | undefined;
  /** Required. Autoscaling targets for this cluster. */
  autoscalingTargets: AutoscalingTargets | undefined;
}

/** Configuration for a cluster. */
export interface Cluster_ClusterConfig {
  /** Autoscaling configuration for this cluster. */
  clusterAutoscalingConfig: Cluster_ClusterAutoscalingConfig | undefined;
}

/**
 * Cloud Key Management Service (Cloud KMS) settings for a CMEK-protected
 * cluster.
 */
export interface Cluster_EncryptionConfig {
  /**
   * Describes the Cloud KMS encryption key that will be used to protect the
   * destination Bigtable cluster. The requirements for this key are:
   *  1) The Cloud Bigtable service account associated with the project that
   *  contains this cluster must be granted the
   *  `cloudkms.cryptoKeyEncrypterDecrypter` role on the CMEK key.
   *  2) Only regional keys can be used and the region of the CMEK key must
   *  match the region of the cluster.
   *  3) All clusters within an instance must use the same CMEK key.
   * Values are of the form
   * `projects/{project}/locations/{location}/keyRings/{keyring}/cryptoKeys/{key}`
   */
  kmsKeyName: string;
}

/**
 * A configuration object describing how Cloud Bigtable should treat traffic
 * from a particular end user application.
 */
export interface AppProfile {
  /**
   * The unique name of the app profile. Values are of the form
   * `projects/{project}/instances/{instance}/appProfiles/[_a-zA-Z0-9][-_.a-zA-Z0-9]*`.
   */
  name: string;
  /**
   * Strongly validated etag for optimistic concurrency control. Preserve the
   * value returned from `GetAppProfile` when calling `UpdateAppProfile` to
   * fail the request if there has been a modification in the mean time. The
   * `update_mask` of the request need not include `etag` for this protection
   * to apply.
   * See [Wikipedia](https://en.wikipedia.org/wiki/HTTP_ETag) and
   * [RFC 7232](https://tools.ietf.org/html/rfc7232#section-2.3) for more
   * details.
   */
  etag: string;
  /** Long form description of the use case for this AppProfile. */
  description: string;
  /** Use a multi-cluster routing policy. */
  multiClusterRoutingUseAny?:
    | AppProfile_MultiClusterRoutingUseAny
    | undefined;
  /** Use a single-cluster routing policy. */
  singleClusterRouting?:
    | AppProfile_SingleClusterRouting
    | undefined;
  /**
   * This field has been deprecated in favor of `standard_isolation.priority`.
   * If you set this field, `standard_isolation.priority` will be set instead.
   *
   * The priority of requests sent using this app profile.
   *
   * @deprecated
   */
  priority?:
    | AppProfile_Priority
    | undefined;
  /**
   * The standard options used for isolating this app profile's traffic from
   * other use cases.
   */
  standardIsolation?:
    | AppProfile_StandardIsolation
    | undefined;
  /**
   * Specifies that this app profile is intended for read-only usage via the
   * Data Boost feature.
   */
  dataBoostIsolationReadOnly?: AppProfile_DataBoostIsolationReadOnly | undefined;
}

/**
 * Possible priorities for an app profile. Note that higher priority writes
 * can sometimes queue behind lower priority writes to the same tablet, as
 * writes must be strictly sequenced in the durability log.
 */
export enum AppProfile_Priority {
  /** PRIORITY_UNSPECIFIED - Default value. Mapped to PRIORITY_HIGH (the legacy behavior) on creation. */
  PRIORITY_UNSPECIFIED = 0,
  PRIORITY_LOW = 1,
  PRIORITY_MEDIUM = 2,
  PRIORITY_HIGH = 3,
  UNRECOGNIZED = -1,
}

export function appProfile_PriorityFromJSON(object: any): AppProfile_Priority {
  switch (object) {
    case 0:
    case "PRIORITY_UNSPECIFIED":
      return AppProfile_Priority.PRIORITY_UNSPECIFIED;
    case 1:
    case "PRIORITY_LOW":
      return AppProfile_Priority.PRIORITY_LOW;
    case 2:
    case "PRIORITY_MEDIUM":
      return AppProfile_Priority.PRIORITY_MEDIUM;
    case 3:
    case "PRIORITY_HIGH":
      return AppProfile_Priority.PRIORITY_HIGH;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AppProfile_Priority.UNRECOGNIZED;
  }
}

export function appProfile_PriorityToJSON(object: AppProfile_Priority): string {
  switch (object) {
    case AppProfile_Priority.PRIORITY_UNSPECIFIED:
      return "PRIORITY_UNSPECIFIED";
    case AppProfile_Priority.PRIORITY_LOW:
      return "PRIORITY_LOW";
    case AppProfile_Priority.PRIORITY_MEDIUM:
      return "PRIORITY_MEDIUM";
    case AppProfile_Priority.PRIORITY_HIGH:
      return "PRIORITY_HIGH";
    case AppProfile_Priority.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Read/write requests are routed to the nearest cluster in the instance, and
 * will fail over to the nearest cluster that is available in the event of
 * transient errors or delays. Clusters in a region are considered
 * equidistant. Choosing this option sacrifices read-your-writes consistency
 * to improve availability.
 */
export interface AppProfile_MultiClusterRoutingUseAny {
  /**
   * The set of clusters to route to. The order is ignored; clusters will be
   * tried in order of distance. If left empty, all clusters are eligible.
   */
  clusterIds: string[];
  /**
   * Row affinity sticky routing based on the row key of the request.
   * Requests that span multiple rows are routed non-deterministically.
   */
  rowAffinity?: AppProfile_MultiClusterRoutingUseAny_RowAffinity | undefined;
}

/**
 * If enabled, Bigtable will route the request based on the row key of the
 * request, rather than randomly. Instead, each row key will be assigned
 * to a cluster, and will stick to that cluster. If clusters are added or
 * removed, then this may affect which row keys stick to which clusters.
 * To avoid this, users can use a cluster group to specify which clusters
 * are to be used. In this case, new clusters that are not a part of the
 * cluster group will not be routed to, and routing will be unaffected by
 * the new cluster. Moreover, clusters specified in the cluster group cannot
 * be deleted unless removed from the cluster group.
 */
export interface AppProfile_MultiClusterRoutingUseAny_RowAffinity {
}

/**
 * Unconditionally routes all read/write requests to a specific cluster.
 * This option preserves read-your-writes consistency but does not improve
 * availability.
 */
export interface AppProfile_SingleClusterRouting {
  /** The cluster to which read/write requests should be routed. */
  clusterId: string;
  /**
   * Whether or not `CheckAndMutateRow` and `ReadModifyWriteRow` requests are
   * allowed by this app profile. It is unsafe to send these requests to
   * the same table/row/column in multiple clusters.
   */
  allowTransactionalWrites: boolean;
}

/**
 * Standard options for isolating this app profile's traffic from other use
 * cases.
 */
export interface AppProfile_StandardIsolation {
  /** The priority of requests sent using this app profile. */
  priority: AppProfile_Priority;
}

/**
 * Data Boost is a serverless compute capability that lets you run
 * high-throughput read jobs on your Bigtable data, without impacting the
 * performance of the clusters that handle your application traffic.
 * Currently, Data Boost exclusively supports read-only use-cases with
 * single-cluster routing.
 *
 * Data Boost reads are only guaranteed to see the results of writes that
 * were written at least 30 minutes ago. This means newly written values may
 * not become visible for up to 30m, and also means that old values may
 * remain visible for up to 30m after being deleted or overwritten. To
 * mitigate the staleness of the data, users may either wait 30m, or use
 * CheckConsistency.
 */
export interface AppProfile_DataBoostIsolationReadOnly {
  /** The Compute Billing Owner for this Data Boost App Profile. */
  computeBillingOwner?: AppProfile_DataBoostIsolationReadOnly_ComputeBillingOwner | undefined;
}

/**
 * Compute Billing Owner specifies how usage should be accounted when using
 * Data Boost. Compute Billing Owner also configures which Cloud Project is
 * charged for relevant quota.
 */
export enum AppProfile_DataBoostIsolationReadOnly_ComputeBillingOwner {
  /** COMPUTE_BILLING_OWNER_UNSPECIFIED - Unspecified value. */
  COMPUTE_BILLING_OWNER_UNSPECIFIED = 0,
  /**
   * HOST_PAYS - The host Cloud Project containing the targeted Bigtable Instance /
   * Table pays for compute.
   */
  HOST_PAYS = 1,
  UNRECOGNIZED = -1,
}

export function appProfile_DataBoostIsolationReadOnly_ComputeBillingOwnerFromJSON(
  object: any,
): AppProfile_DataBoostIsolationReadOnly_ComputeBillingOwner {
  switch (object) {
    case 0:
    case "COMPUTE_BILLING_OWNER_UNSPECIFIED":
      return AppProfile_DataBoostIsolationReadOnly_ComputeBillingOwner.COMPUTE_BILLING_OWNER_UNSPECIFIED;
    case 1:
    case "HOST_PAYS":
      return AppProfile_DataBoostIsolationReadOnly_ComputeBillingOwner.HOST_PAYS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AppProfile_DataBoostIsolationReadOnly_ComputeBillingOwner.UNRECOGNIZED;
  }
}

export function appProfile_DataBoostIsolationReadOnly_ComputeBillingOwnerToJSON(
  object: AppProfile_DataBoostIsolationReadOnly_ComputeBillingOwner,
): string {
  switch (object) {
    case AppProfile_DataBoostIsolationReadOnly_ComputeBillingOwner.COMPUTE_BILLING_OWNER_UNSPECIFIED:
      return "COMPUTE_BILLING_OWNER_UNSPECIFIED";
    case AppProfile_DataBoostIsolationReadOnly_ComputeBillingOwner.HOST_PAYS:
      return "HOST_PAYS";
    case AppProfile_DataBoostIsolationReadOnly_ComputeBillingOwner.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * A tablet is a defined by a start and end key and is explained in
 * https://cloud.google.com/bigtable/docs/overview#architecture and
 * https://cloud.google.com/bigtable/docs/performance#optimization.
 * A Hot tablet is a tablet that exhibits high average cpu usage during the time
 * interval from start time to end time.
 */
export interface HotTablet {
  /**
   * The unique name of the hot tablet. Values are of the form
   * `projects/{project}/instances/{instance}/clusters/{cluster}/hotTablets/[a-zA-Z0-9_-]*`.
   */
  name: string;
  /**
   * Name of the table that contains the tablet. Values are of the form
   * `projects/{project}/instances/{instance}/tables/[_a-zA-Z0-9][-_.a-zA-Z0-9]*`.
   */
  tableName: string;
  /** Output only. The start time of the hot tablet. */
  startTime:
    | Date
    | undefined;
  /** Output only. The end time of the hot tablet. */
  endTime:
    | Date
    | undefined;
  /** Tablet Start Key (inclusive). */
  startKey: string;
  /** Tablet End Key (inclusive). */
  endKey: string;
  /**
   * Output only. The average CPU usage spent by a node on this tablet over the
   * start_time to end_time time range. The percentage is the amount of CPU used
   * by the node to serve the tablet, from 0% (tablet was not interacted with)
   * to 100% (the node spent all cycles serving the hot tablet).
   */
  nodeCpuUsagePercent: number;
}

function createBaseInstance(): Instance {
  return { name: "", displayName: "", state: 0, type: 0, labels: {}, createTime: undefined, satisfiesPzs: undefined };
}

export const Instance: MessageFns<Instance> = {
  encode(message: Instance, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.displayName !== "") {
      writer.uint32(18).string(message.displayName);
    }
    if (message.state !== 0) {
      writer.uint32(24).int32(message.state);
    }
    if (message.type !== 0) {
      writer.uint32(32).int32(message.type);
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      Instance_LabelsEntry.encode({ key: key as any, value }, writer.uint32(42).fork()).join();
    });
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(58).fork()).join();
    }
    if (message.satisfiesPzs !== undefined) {
      writer.uint32(64).bool(message.satisfiesPzs);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Instance {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstance();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          const entry5 = Instance_LabelsEntry.decode(reader, reader.uint32());
          if (entry5.value !== undefined) {
            message.labels[entry5.key] = entry5.value;
          }
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.satisfiesPzs = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Instance {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      state: isSet(object.state) ? instance_StateFromJSON(object.state) : 0,
      type: isSet(object.type) ? instance_TypeFromJSON(object.type) : 0,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      satisfiesPzs: isSet(object.satisfiesPzs) ? globalThis.Boolean(object.satisfiesPzs) : undefined,
    };
  },

  toJSON(message: Instance): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.state !== 0) {
      obj.state = instance_StateToJSON(message.state);
    }
    if (message.type !== 0) {
      obj.type = instance_TypeToJSON(message.type);
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.satisfiesPzs !== undefined) {
      obj.satisfiesPzs = message.satisfiesPzs;
    }
    return obj;
  },

  create(base?: DeepPartial<Instance>): Instance {
    return Instance.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Instance>): Instance {
    const message = createBaseInstance();
    message.name = object.name ?? "";
    message.displayName = object.displayName ?? "";
    message.state = object.state ?? 0;
    message.type = object.type ?? 0;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.createTime = object.createTime ?? undefined;
    message.satisfiesPzs = object.satisfiesPzs ?? undefined;
    return message;
  },
};

function createBaseInstance_LabelsEntry(): Instance_LabelsEntry {
  return { key: "", value: "" };
}

export const Instance_LabelsEntry: MessageFns<Instance_LabelsEntry> = {
  encode(message: Instance_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Instance_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstance_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Instance_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Instance_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Instance_LabelsEntry>): Instance_LabelsEntry {
    return Instance_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Instance_LabelsEntry>): Instance_LabelsEntry {
    const message = createBaseInstance_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseAutoscalingTargets(): AutoscalingTargets {
  return { cpuUtilizationPercent: 0, storageUtilizationGibPerNode: 0 };
}

export const AutoscalingTargets: MessageFns<AutoscalingTargets> = {
  encode(message: AutoscalingTargets, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.cpuUtilizationPercent !== 0) {
      writer.uint32(16).int32(message.cpuUtilizationPercent);
    }
    if (message.storageUtilizationGibPerNode !== 0) {
      writer.uint32(24).int32(message.storageUtilizationGibPerNode);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutoscalingTargets {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutoscalingTargets();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 16) {
            break;
          }

          message.cpuUtilizationPercent = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.storageUtilizationGibPerNode = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutoscalingTargets {
    return {
      cpuUtilizationPercent: isSet(object.cpuUtilizationPercent) ? globalThis.Number(object.cpuUtilizationPercent) : 0,
      storageUtilizationGibPerNode: isSet(object.storageUtilizationGibPerNode)
        ? globalThis.Number(object.storageUtilizationGibPerNode)
        : 0,
    };
  },

  toJSON(message: AutoscalingTargets): unknown {
    const obj: any = {};
    if (message.cpuUtilizationPercent !== 0) {
      obj.cpuUtilizationPercent = Math.round(message.cpuUtilizationPercent);
    }
    if (message.storageUtilizationGibPerNode !== 0) {
      obj.storageUtilizationGibPerNode = Math.round(message.storageUtilizationGibPerNode);
    }
    return obj;
  },

  create(base?: DeepPartial<AutoscalingTargets>): AutoscalingTargets {
    return AutoscalingTargets.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutoscalingTargets>): AutoscalingTargets {
    const message = createBaseAutoscalingTargets();
    message.cpuUtilizationPercent = object.cpuUtilizationPercent ?? 0;
    message.storageUtilizationGibPerNode = object.storageUtilizationGibPerNode ?? 0;
    return message;
  },
};

function createBaseAutoscalingLimits(): AutoscalingLimits {
  return { minServeNodes: 0, maxServeNodes: 0 };
}

export const AutoscalingLimits: MessageFns<AutoscalingLimits> = {
  encode(message: AutoscalingLimits, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.minServeNodes !== 0) {
      writer.uint32(8).int32(message.minServeNodes);
    }
    if (message.maxServeNodes !== 0) {
      writer.uint32(16).int32(message.maxServeNodes);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutoscalingLimits {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutoscalingLimits();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.minServeNodes = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.maxServeNodes = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutoscalingLimits {
    return {
      minServeNodes: isSet(object.minServeNodes) ? globalThis.Number(object.minServeNodes) : 0,
      maxServeNodes: isSet(object.maxServeNodes) ? globalThis.Number(object.maxServeNodes) : 0,
    };
  },

  toJSON(message: AutoscalingLimits): unknown {
    const obj: any = {};
    if (message.minServeNodes !== 0) {
      obj.minServeNodes = Math.round(message.minServeNodes);
    }
    if (message.maxServeNodes !== 0) {
      obj.maxServeNodes = Math.round(message.maxServeNodes);
    }
    return obj;
  },

  create(base?: DeepPartial<AutoscalingLimits>): AutoscalingLimits {
    return AutoscalingLimits.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutoscalingLimits>): AutoscalingLimits {
    const message = createBaseAutoscalingLimits();
    message.minServeNodes = object.minServeNodes ?? 0;
    message.maxServeNodes = object.maxServeNodes ?? 0;
    return message;
  },
};

function createBaseCluster(): Cluster {
  return {
    name: "",
    location: "",
    state: 0,
    serveNodes: 0,
    nodeScalingFactor: 0,
    clusterConfig: undefined,
    defaultStorageType: 0,
    encryptionConfig: undefined,
  };
}

export const Cluster: MessageFns<Cluster> = {
  encode(message: Cluster, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.location !== "") {
      writer.uint32(18).string(message.location);
    }
    if (message.state !== 0) {
      writer.uint32(24).int32(message.state);
    }
    if (message.serveNodes !== 0) {
      writer.uint32(32).int32(message.serveNodes);
    }
    if (message.nodeScalingFactor !== 0) {
      writer.uint32(72).int32(message.nodeScalingFactor);
    }
    if (message.clusterConfig !== undefined) {
      Cluster_ClusterConfig.encode(message.clusterConfig, writer.uint32(58).fork()).join();
    }
    if (message.defaultStorageType !== 0) {
      writer.uint32(40).int32(message.defaultStorageType);
    }
    if (message.encryptionConfig !== undefined) {
      Cluster_EncryptionConfig.encode(message.encryptionConfig, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.location = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.serveNodes = reader.int32();
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.nodeScalingFactor = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.clusterConfig = Cluster_ClusterConfig.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.defaultStorageType = reader.int32() as any;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.encryptionConfig = Cluster_EncryptionConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      location: isSet(object.location) ? globalThis.String(object.location) : "",
      state: isSet(object.state) ? cluster_StateFromJSON(object.state) : 0,
      serveNodes: isSet(object.serveNodes) ? globalThis.Number(object.serveNodes) : 0,
      nodeScalingFactor: isSet(object.nodeScalingFactor)
        ? cluster_NodeScalingFactorFromJSON(object.nodeScalingFactor)
        : 0,
      clusterConfig: isSet(object.clusterConfig) ? Cluster_ClusterConfig.fromJSON(object.clusterConfig) : undefined,
      defaultStorageType: isSet(object.defaultStorageType) ? storageTypeFromJSON(object.defaultStorageType) : 0,
      encryptionConfig: isSet(object.encryptionConfig)
        ? Cluster_EncryptionConfig.fromJSON(object.encryptionConfig)
        : undefined,
    };
  },

  toJSON(message: Cluster): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    if (message.state !== 0) {
      obj.state = cluster_StateToJSON(message.state);
    }
    if (message.serveNodes !== 0) {
      obj.serveNodes = Math.round(message.serveNodes);
    }
    if (message.nodeScalingFactor !== 0) {
      obj.nodeScalingFactor = cluster_NodeScalingFactorToJSON(message.nodeScalingFactor);
    }
    if (message.clusterConfig !== undefined) {
      obj.clusterConfig = Cluster_ClusterConfig.toJSON(message.clusterConfig);
    }
    if (message.defaultStorageType !== 0) {
      obj.defaultStorageType = storageTypeToJSON(message.defaultStorageType);
    }
    if (message.encryptionConfig !== undefined) {
      obj.encryptionConfig = Cluster_EncryptionConfig.toJSON(message.encryptionConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster>): Cluster {
    return Cluster.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster>): Cluster {
    const message = createBaseCluster();
    message.name = object.name ?? "";
    message.location = object.location ?? "";
    message.state = object.state ?? 0;
    message.serveNodes = object.serveNodes ?? 0;
    message.nodeScalingFactor = object.nodeScalingFactor ?? 0;
    message.clusterConfig = (object.clusterConfig !== undefined && object.clusterConfig !== null)
      ? Cluster_ClusterConfig.fromPartial(object.clusterConfig)
      : undefined;
    message.defaultStorageType = object.defaultStorageType ?? 0;
    message.encryptionConfig = (object.encryptionConfig !== undefined && object.encryptionConfig !== null)
      ? Cluster_EncryptionConfig.fromPartial(object.encryptionConfig)
      : undefined;
    return message;
  },
};

function createBaseCluster_ClusterAutoscalingConfig(): Cluster_ClusterAutoscalingConfig {
  return { autoscalingLimits: undefined, autoscalingTargets: undefined };
}

export const Cluster_ClusterAutoscalingConfig: MessageFns<Cluster_ClusterAutoscalingConfig> = {
  encode(message: Cluster_ClusterAutoscalingConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.autoscalingLimits !== undefined) {
      AutoscalingLimits.encode(message.autoscalingLimits, writer.uint32(10).fork()).join();
    }
    if (message.autoscalingTargets !== undefined) {
      AutoscalingTargets.encode(message.autoscalingTargets, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster_ClusterAutoscalingConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster_ClusterAutoscalingConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.autoscalingLimits = AutoscalingLimits.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.autoscalingTargets = AutoscalingTargets.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster_ClusterAutoscalingConfig {
    return {
      autoscalingLimits: isSet(object.autoscalingLimits)
        ? AutoscalingLimits.fromJSON(object.autoscalingLimits)
        : undefined,
      autoscalingTargets: isSet(object.autoscalingTargets)
        ? AutoscalingTargets.fromJSON(object.autoscalingTargets)
        : undefined,
    };
  },

  toJSON(message: Cluster_ClusterAutoscalingConfig): unknown {
    const obj: any = {};
    if (message.autoscalingLimits !== undefined) {
      obj.autoscalingLimits = AutoscalingLimits.toJSON(message.autoscalingLimits);
    }
    if (message.autoscalingTargets !== undefined) {
      obj.autoscalingTargets = AutoscalingTargets.toJSON(message.autoscalingTargets);
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster_ClusterAutoscalingConfig>): Cluster_ClusterAutoscalingConfig {
    return Cluster_ClusterAutoscalingConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster_ClusterAutoscalingConfig>): Cluster_ClusterAutoscalingConfig {
    const message = createBaseCluster_ClusterAutoscalingConfig();
    message.autoscalingLimits = (object.autoscalingLimits !== undefined && object.autoscalingLimits !== null)
      ? AutoscalingLimits.fromPartial(object.autoscalingLimits)
      : undefined;
    message.autoscalingTargets = (object.autoscalingTargets !== undefined && object.autoscalingTargets !== null)
      ? AutoscalingTargets.fromPartial(object.autoscalingTargets)
      : undefined;
    return message;
  },
};

function createBaseCluster_ClusterConfig(): Cluster_ClusterConfig {
  return { clusterAutoscalingConfig: undefined };
}

export const Cluster_ClusterConfig: MessageFns<Cluster_ClusterConfig> = {
  encode(message: Cluster_ClusterConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.clusterAutoscalingConfig !== undefined) {
      Cluster_ClusterAutoscalingConfig.encode(message.clusterAutoscalingConfig, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster_ClusterConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster_ClusterConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.clusterAutoscalingConfig = Cluster_ClusterAutoscalingConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster_ClusterConfig {
    return {
      clusterAutoscalingConfig: isSet(object.clusterAutoscalingConfig)
        ? Cluster_ClusterAutoscalingConfig.fromJSON(object.clusterAutoscalingConfig)
        : undefined,
    };
  },

  toJSON(message: Cluster_ClusterConfig): unknown {
    const obj: any = {};
    if (message.clusterAutoscalingConfig !== undefined) {
      obj.clusterAutoscalingConfig = Cluster_ClusterAutoscalingConfig.toJSON(message.clusterAutoscalingConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster_ClusterConfig>): Cluster_ClusterConfig {
    return Cluster_ClusterConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster_ClusterConfig>): Cluster_ClusterConfig {
    const message = createBaseCluster_ClusterConfig();
    message.clusterAutoscalingConfig =
      (object.clusterAutoscalingConfig !== undefined && object.clusterAutoscalingConfig !== null)
        ? Cluster_ClusterAutoscalingConfig.fromPartial(object.clusterAutoscalingConfig)
        : undefined;
    return message;
  },
};

function createBaseCluster_EncryptionConfig(): Cluster_EncryptionConfig {
  return { kmsKeyName: "" };
}

export const Cluster_EncryptionConfig: MessageFns<Cluster_EncryptionConfig> = {
  encode(message: Cluster_EncryptionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kmsKeyName !== "") {
      writer.uint32(10).string(message.kmsKeyName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster_EncryptionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster_EncryptionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kmsKeyName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster_EncryptionConfig {
    return { kmsKeyName: isSet(object.kmsKeyName) ? globalThis.String(object.kmsKeyName) : "" };
  },

  toJSON(message: Cluster_EncryptionConfig): unknown {
    const obj: any = {};
    if (message.kmsKeyName !== "") {
      obj.kmsKeyName = message.kmsKeyName;
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster_EncryptionConfig>): Cluster_EncryptionConfig {
    return Cluster_EncryptionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster_EncryptionConfig>): Cluster_EncryptionConfig {
    const message = createBaseCluster_EncryptionConfig();
    message.kmsKeyName = object.kmsKeyName ?? "";
    return message;
  },
};

function createBaseAppProfile(): AppProfile {
  return {
    name: "",
    etag: "",
    description: "",
    multiClusterRoutingUseAny: undefined,
    singleClusterRouting: undefined,
    priority: undefined,
    standardIsolation: undefined,
    dataBoostIsolationReadOnly: undefined,
  };
}

export const AppProfile: MessageFns<AppProfile> = {
  encode(message: AppProfile, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.etag !== "") {
      writer.uint32(18).string(message.etag);
    }
    if (message.description !== "") {
      writer.uint32(26).string(message.description);
    }
    if (message.multiClusterRoutingUseAny !== undefined) {
      AppProfile_MultiClusterRoutingUseAny.encode(message.multiClusterRoutingUseAny, writer.uint32(42).fork()).join();
    }
    if (message.singleClusterRouting !== undefined) {
      AppProfile_SingleClusterRouting.encode(message.singleClusterRouting, writer.uint32(50).fork()).join();
    }
    if (message.priority !== undefined) {
      writer.uint32(56).int32(message.priority);
    }
    if (message.standardIsolation !== undefined) {
      AppProfile_StandardIsolation.encode(message.standardIsolation, writer.uint32(90).fork()).join();
    }
    if (message.dataBoostIsolationReadOnly !== undefined) {
      AppProfile_DataBoostIsolationReadOnly.encode(message.dataBoostIsolationReadOnly, writer.uint32(82).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppProfile {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppProfile();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.etag = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.description = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.multiClusterRoutingUseAny = AppProfile_MultiClusterRoutingUseAny.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.singleClusterRouting = AppProfile_SingleClusterRouting.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.priority = reader.int32() as any;
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.standardIsolation = AppProfile_StandardIsolation.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.dataBoostIsolationReadOnly = AppProfile_DataBoostIsolationReadOnly.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AppProfile {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      etag: isSet(object.etag) ? globalThis.String(object.etag) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      multiClusterRoutingUseAny: isSet(object.multiClusterRoutingUseAny)
        ? AppProfile_MultiClusterRoutingUseAny.fromJSON(object.multiClusterRoutingUseAny)
        : undefined,
      singleClusterRouting: isSet(object.singleClusterRouting)
        ? AppProfile_SingleClusterRouting.fromJSON(object.singleClusterRouting)
        : undefined,
      priority: isSet(object.priority) ? appProfile_PriorityFromJSON(object.priority) : undefined,
      standardIsolation: isSet(object.standardIsolation)
        ? AppProfile_StandardIsolation.fromJSON(object.standardIsolation)
        : undefined,
      dataBoostIsolationReadOnly: isSet(object.dataBoostIsolationReadOnly)
        ? AppProfile_DataBoostIsolationReadOnly.fromJSON(object.dataBoostIsolationReadOnly)
        : undefined,
    };
  },

  toJSON(message: AppProfile): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.etag !== "") {
      obj.etag = message.etag;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.multiClusterRoutingUseAny !== undefined) {
      obj.multiClusterRoutingUseAny = AppProfile_MultiClusterRoutingUseAny.toJSON(message.multiClusterRoutingUseAny);
    }
    if (message.singleClusterRouting !== undefined) {
      obj.singleClusterRouting = AppProfile_SingleClusterRouting.toJSON(message.singleClusterRouting);
    }
    if (message.priority !== undefined) {
      obj.priority = appProfile_PriorityToJSON(message.priority);
    }
    if (message.standardIsolation !== undefined) {
      obj.standardIsolation = AppProfile_StandardIsolation.toJSON(message.standardIsolation);
    }
    if (message.dataBoostIsolationReadOnly !== undefined) {
      obj.dataBoostIsolationReadOnly = AppProfile_DataBoostIsolationReadOnly.toJSON(message.dataBoostIsolationReadOnly);
    }
    return obj;
  },

  create(base?: DeepPartial<AppProfile>): AppProfile {
    return AppProfile.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AppProfile>): AppProfile {
    const message = createBaseAppProfile();
    message.name = object.name ?? "";
    message.etag = object.etag ?? "";
    message.description = object.description ?? "";
    message.multiClusterRoutingUseAny =
      (object.multiClusterRoutingUseAny !== undefined && object.multiClusterRoutingUseAny !== null)
        ? AppProfile_MultiClusterRoutingUseAny.fromPartial(object.multiClusterRoutingUseAny)
        : undefined;
    message.singleClusterRouting = (object.singleClusterRouting !== undefined && object.singleClusterRouting !== null)
      ? AppProfile_SingleClusterRouting.fromPartial(object.singleClusterRouting)
      : undefined;
    message.priority = object.priority ?? undefined;
    message.standardIsolation = (object.standardIsolation !== undefined && object.standardIsolation !== null)
      ? AppProfile_StandardIsolation.fromPartial(object.standardIsolation)
      : undefined;
    message.dataBoostIsolationReadOnly =
      (object.dataBoostIsolationReadOnly !== undefined && object.dataBoostIsolationReadOnly !== null)
        ? AppProfile_DataBoostIsolationReadOnly.fromPartial(object.dataBoostIsolationReadOnly)
        : undefined;
    return message;
  },
};

function createBaseAppProfile_MultiClusterRoutingUseAny(): AppProfile_MultiClusterRoutingUseAny {
  return { clusterIds: [], rowAffinity: undefined };
}

export const AppProfile_MultiClusterRoutingUseAny: MessageFns<AppProfile_MultiClusterRoutingUseAny> = {
  encode(message: AppProfile_MultiClusterRoutingUseAny, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.clusterIds) {
      writer.uint32(10).string(v!);
    }
    if (message.rowAffinity !== undefined) {
      AppProfile_MultiClusterRoutingUseAny_RowAffinity.encode(message.rowAffinity, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppProfile_MultiClusterRoutingUseAny {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppProfile_MultiClusterRoutingUseAny();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.clusterIds.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.rowAffinity = AppProfile_MultiClusterRoutingUseAny_RowAffinity.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AppProfile_MultiClusterRoutingUseAny {
    return {
      clusterIds: globalThis.Array.isArray(object?.clusterIds)
        ? object.clusterIds.map((e: any) => globalThis.String(e))
        : [],
      rowAffinity: isSet(object.rowAffinity)
        ? AppProfile_MultiClusterRoutingUseAny_RowAffinity.fromJSON(object.rowAffinity)
        : undefined,
    };
  },

  toJSON(message: AppProfile_MultiClusterRoutingUseAny): unknown {
    const obj: any = {};
    if (message.clusterIds?.length) {
      obj.clusterIds = message.clusterIds;
    }
    if (message.rowAffinity !== undefined) {
      obj.rowAffinity = AppProfile_MultiClusterRoutingUseAny_RowAffinity.toJSON(message.rowAffinity);
    }
    return obj;
  },

  create(base?: DeepPartial<AppProfile_MultiClusterRoutingUseAny>): AppProfile_MultiClusterRoutingUseAny {
    return AppProfile_MultiClusterRoutingUseAny.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AppProfile_MultiClusterRoutingUseAny>): AppProfile_MultiClusterRoutingUseAny {
    const message = createBaseAppProfile_MultiClusterRoutingUseAny();
    message.clusterIds = object.clusterIds?.map((e) => e) || [];
    message.rowAffinity = (object.rowAffinity !== undefined && object.rowAffinity !== null)
      ? AppProfile_MultiClusterRoutingUseAny_RowAffinity.fromPartial(object.rowAffinity)
      : undefined;
    return message;
  },
};

function createBaseAppProfile_MultiClusterRoutingUseAny_RowAffinity(): AppProfile_MultiClusterRoutingUseAny_RowAffinity {
  return {};
}

export const AppProfile_MultiClusterRoutingUseAny_RowAffinity: MessageFns<
  AppProfile_MultiClusterRoutingUseAny_RowAffinity
> = {
  encode(_: AppProfile_MultiClusterRoutingUseAny_RowAffinity, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppProfile_MultiClusterRoutingUseAny_RowAffinity {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppProfile_MultiClusterRoutingUseAny_RowAffinity();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): AppProfile_MultiClusterRoutingUseAny_RowAffinity {
    return {};
  },

  toJSON(_: AppProfile_MultiClusterRoutingUseAny_RowAffinity): unknown {
    const obj: any = {};
    return obj;
  },

  create(
    base?: DeepPartial<AppProfile_MultiClusterRoutingUseAny_RowAffinity>,
  ): AppProfile_MultiClusterRoutingUseAny_RowAffinity {
    return AppProfile_MultiClusterRoutingUseAny_RowAffinity.fromPartial(base ?? {});
  },
  fromPartial(
    _: DeepPartial<AppProfile_MultiClusterRoutingUseAny_RowAffinity>,
  ): AppProfile_MultiClusterRoutingUseAny_RowAffinity {
    const message = createBaseAppProfile_MultiClusterRoutingUseAny_RowAffinity();
    return message;
  },
};

function createBaseAppProfile_SingleClusterRouting(): AppProfile_SingleClusterRouting {
  return { clusterId: "", allowTransactionalWrites: false };
}

export const AppProfile_SingleClusterRouting: MessageFns<AppProfile_SingleClusterRouting> = {
  encode(message: AppProfile_SingleClusterRouting, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.clusterId !== "") {
      writer.uint32(10).string(message.clusterId);
    }
    if (message.allowTransactionalWrites !== false) {
      writer.uint32(16).bool(message.allowTransactionalWrites);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppProfile_SingleClusterRouting {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppProfile_SingleClusterRouting();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.clusterId = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.allowTransactionalWrites = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AppProfile_SingleClusterRouting {
    return {
      clusterId: isSet(object.clusterId) ? globalThis.String(object.clusterId) : "",
      allowTransactionalWrites: isSet(object.allowTransactionalWrites)
        ? globalThis.Boolean(object.allowTransactionalWrites)
        : false,
    };
  },

  toJSON(message: AppProfile_SingleClusterRouting): unknown {
    const obj: any = {};
    if (message.clusterId !== "") {
      obj.clusterId = message.clusterId;
    }
    if (message.allowTransactionalWrites !== false) {
      obj.allowTransactionalWrites = message.allowTransactionalWrites;
    }
    return obj;
  },

  create(base?: DeepPartial<AppProfile_SingleClusterRouting>): AppProfile_SingleClusterRouting {
    return AppProfile_SingleClusterRouting.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AppProfile_SingleClusterRouting>): AppProfile_SingleClusterRouting {
    const message = createBaseAppProfile_SingleClusterRouting();
    message.clusterId = object.clusterId ?? "";
    message.allowTransactionalWrites = object.allowTransactionalWrites ?? false;
    return message;
  },
};

function createBaseAppProfile_StandardIsolation(): AppProfile_StandardIsolation {
  return { priority: 0 };
}

export const AppProfile_StandardIsolation: MessageFns<AppProfile_StandardIsolation> = {
  encode(message: AppProfile_StandardIsolation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.priority !== 0) {
      writer.uint32(8).int32(message.priority);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppProfile_StandardIsolation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppProfile_StandardIsolation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.priority = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AppProfile_StandardIsolation {
    return { priority: isSet(object.priority) ? appProfile_PriorityFromJSON(object.priority) : 0 };
  },

  toJSON(message: AppProfile_StandardIsolation): unknown {
    const obj: any = {};
    if (message.priority !== 0) {
      obj.priority = appProfile_PriorityToJSON(message.priority);
    }
    return obj;
  },

  create(base?: DeepPartial<AppProfile_StandardIsolation>): AppProfile_StandardIsolation {
    return AppProfile_StandardIsolation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AppProfile_StandardIsolation>): AppProfile_StandardIsolation {
    const message = createBaseAppProfile_StandardIsolation();
    message.priority = object.priority ?? 0;
    return message;
  },
};

function createBaseAppProfile_DataBoostIsolationReadOnly(): AppProfile_DataBoostIsolationReadOnly {
  return { computeBillingOwner: undefined };
}

export const AppProfile_DataBoostIsolationReadOnly: MessageFns<AppProfile_DataBoostIsolationReadOnly> = {
  encode(message: AppProfile_DataBoostIsolationReadOnly, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.computeBillingOwner !== undefined) {
      writer.uint32(8).int32(message.computeBillingOwner);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppProfile_DataBoostIsolationReadOnly {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppProfile_DataBoostIsolationReadOnly();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.computeBillingOwner = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AppProfile_DataBoostIsolationReadOnly {
    return {
      computeBillingOwner: isSet(object.computeBillingOwner)
        ? appProfile_DataBoostIsolationReadOnly_ComputeBillingOwnerFromJSON(object.computeBillingOwner)
        : undefined,
    };
  },

  toJSON(message: AppProfile_DataBoostIsolationReadOnly): unknown {
    const obj: any = {};
    if (message.computeBillingOwner !== undefined) {
      obj.computeBillingOwner = appProfile_DataBoostIsolationReadOnly_ComputeBillingOwnerToJSON(
        message.computeBillingOwner,
      );
    }
    return obj;
  },

  create(base?: DeepPartial<AppProfile_DataBoostIsolationReadOnly>): AppProfile_DataBoostIsolationReadOnly {
    return AppProfile_DataBoostIsolationReadOnly.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AppProfile_DataBoostIsolationReadOnly>): AppProfile_DataBoostIsolationReadOnly {
    const message = createBaseAppProfile_DataBoostIsolationReadOnly();
    message.computeBillingOwner = object.computeBillingOwner ?? undefined;
    return message;
  },
};

function createBaseHotTablet(): HotTablet {
  return {
    name: "",
    tableName: "",
    startTime: undefined,
    endTime: undefined,
    startKey: "",
    endKey: "",
    nodeCpuUsagePercent: 0,
  };
}

export const HotTablet: MessageFns<HotTablet> = {
  encode(message: HotTablet, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.tableName !== "") {
      writer.uint32(18).string(message.tableName);
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(26).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(34).fork()).join();
    }
    if (message.startKey !== "") {
      writer.uint32(42).string(message.startKey);
    }
    if (message.endKey !== "") {
      writer.uint32(50).string(message.endKey);
    }
    if (message.nodeCpuUsagePercent !== 0) {
      writer.uint32(61).float(message.nodeCpuUsagePercent);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): HotTablet {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseHotTablet();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.tableName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.startKey = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.endKey = reader.string();
          continue;
        case 7:
          if (tag !== 61) {
            break;
          }

          message.nodeCpuUsagePercent = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): HotTablet {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      tableName: isSet(object.tableName) ? globalThis.String(object.tableName) : "",
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      startKey: isSet(object.startKey) ? globalThis.String(object.startKey) : "",
      endKey: isSet(object.endKey) ? globalThis.String(object.endKey) : "",
      nodeCpuUsagePercent: isSet(object.nodeCpuUsagePercent) ? globalThis.Number(object.nodeCpuUsagePercent) : 0,
    };
  },

  toJSON(message: HotTablet): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.tableName !== "") {
      obj.tableName = message.tableName;
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (message.startKey !== "") {
      obj.startKey = message.startKey;
    }
    if (message.endKey !== "") {
      obj.endKey = message.endKey;
    }
    if (message.nodeCpuUsagePercent !== 0) {
      obj.nodeCpuUsagePercent = message.nodeCpuUsagePercent;
    }
    return obj;
  },

  create(base?: DeepPartial<HotTablet>): HotTablet {
    return HotTablet.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<HotTablet>): HotTablet {
    const message = createBaseHotTablet();
    message.name = object.name ?? "";
    message.tableName = object.tableName ?? "";
    message.startTime = object.startTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    message.startKey = object.startKey ?? "";
    message.endKey = object.endKey ?? "";
    message.nodeCpuUsagePercent = object.nodeCpuUsagePercent ?? 0;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
