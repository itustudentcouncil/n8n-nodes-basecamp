// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/bigtable/admin/v2/table.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Duration } from "../../../protobuf/duration.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { Status } from "../../../rpc/status.js";
import { Type } from "./types.js";

export const protobufPackage = "google.bigtable.admin.v2";

/** Indicates the type of the restore source. */
export enum RestoreSourceType {
  /** RESTORE_SOURCE_TYPE_UNSPECIFIED - No restore associated. */
  RESTORE_SOURCE_TYPE_UNSPECIFIED = 0,
  /** BACKUP - A backup was used as the source of the restore. */
  BACKUP = 1,
  UNRECOGNIZED = -1,
}

export function restoreSourceTypeFromJSON(object: any): RestoreSourceType {
  switch (object) {
    case 0:
    case "RESTORE_SOURCE_TYPE_UNSPECIFIED":
      return RestoreSourceType.RESTORE_SOURCE_TYPE_UNSPECIFIED;
    case 1:
    case "BACKUP":
      return RestoreSourceType.BACKUP;
    case -1:
    case "UNRECOGNIZED":
    default:
      return RestoreSourceType.UNRECOGNIZED;
  }
}

export function restoreSourceTypeToJSON(object: RestoreSourceType): string {
  switch (object) {
    case RestoreSourceType.RESTORE_SOURCE_TYPE_UNSPECIFIED:
      return "RESTORE_SOURCE_TYPE_UNSPECIFIED";
    case RestoreSourceType.BACKUP:
      return "BACKUP";
    case RestoreSourceType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Information about a table restore. */
export interface RestoreInfo {
  /** The type of the restore source. */
  sourceType: RestoreSourceType;
  /**
   * Information about the backup used to restore the table. The backup
   * may no longer exist.
   */
  backupInfo?: BackupInfo | undefined;
}

/** Change stream configuration. */
export interface ChangeStreamConfig {
  /**
   * How long the change stream should be retained. Change stream data older
   * than the retention period will not be returned when reading the change
   * stream from the table.
   * Values must be at least 1 day and at most 7 days, and will be truncated to
   * microsecond granularity.
   */
  retentionPeriod: Duration | undefined;
}

/**
 * A collection of user data indexed by row, column, and timestamp.
 * Each table is served using the resources of its parent cluster.
 */
export interface Table {
  /**
   * The unique name of the table. Values are of the form
   * `projects/{project}/instances/{instance}/tables/[_a-zA-Z0-9][-_.a-zA-Z0-9]*`.
   * Views: `NAME_ONLY`, `SCHEMA_VIEW`, `REPLICATION_VIEW`, `FULL`
   */
  name: string;
  /**
   * Output only. Map from cluster ID to per-cluster table state.
   * If it could not be determined whether or not the table has data in a
   * particular cluster (for example, if its zone is unavailable), then
   * there will be an entry for the cluster with UNKNOWN `replication_status`.
   * Views: `REPLICATION_VIEW`, `ENCRYPTION_VIEW`, `FULL`
   */
  clusterStates: { [key: string]: Table_ClusterState };
  /**
   * The column families configured for this table, mapped by column family ID.
   * Views: `SCHEMA_VIEW`, `STATS_VIEW`, `FULL`
   */
  columnFamilies: { [key: string]: ColumnFamily };
  /**
   * Immutable. The granularity (i.e. `MILLIS`) at which timestamps are stored
   * in this table. Timestamps not matching the granularity will be rejected. If
   * unspecified at creation time, the value will be set to `MILLIS`. Views:
   * `SCHEMA_VIEW`, `FULL`.
   */
  granularity: Table_TimestampGranularity;
  /**
   * Output only. If this table was restored from another data source (e.g. a
   * backup), this field will be populated with information about the restore.
   */
  restoreInfo:
    | RestoreInfo
    | undefined;
  /**
   * If specified, enable the change stream on this table.
   * Otherwise, the change stream is disabled and the change stream is not
   * retained.
   */
  changeStreamConfig:
    | ChangeStreamConfig
    | undefined;
  /**
   * Set to true to make the table protected against data loss. i.e. deleting
   * the following resources through Admin APIs are prohibited:
   *
   * * The table.
   * * The column families in the table.
   * * The instance containing the table.
   *
   * Note one can still delete the data stored in the table through Data APIs.
   */
  deletionProtection: boolean;
  /**
   * If specified, automated backups are enabled for this table.
   * Otherwise, automated backups are disabled.
   */
  automatedBackupPolicy?: Table_AutomatedBackupPolicy | undefined;
}

/**
 * Possible timestamp granularities to use when keeping multiple versions
 * of data in a table.
 */
export enum Table_TimestampGranularity {
  /**
   * TIMESTAMP_GRANULARITY_UNSPECIFIED - The user did not specify a granularity. Should not be returned.
   * When specified during table creation, MILLIS will be used.
   */
  TIMESTAMP_GRANULARITY_UNSPECIFIED = 0,
  /** MILLIS - The table keeps data versioned at a granularity of 1ms. */
  MILLIS = 1,
  UNRECOGNIZED = -1,
}

export function table_TimestampGranularityFromJSON(object: any): Table_TimestampGranularity {
  switch (object) {
    case 0:
    case "TIMESTAMP_GRANULARITY_UNSPECIFIED":
      return Table_TimestampGranularity.TIMESTAMP_GRANULARITY_UNSPECIFIED;
    case 1:
    case "MILLIS":
      return Table_TimestampGranularity.MILLIS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Table_TimestampGranularity.UNRECOGNIZED;
  }
}

export function table_TimestampGranularityToJSON(object: Table_TimestampGranularity): string {
  switch (object) {
    case Table_TimestampGranularity.TIMESTAMP_GRANULARITY_UNSPECIFIED:
      return "TIMESTAMP_GRANULARITY_UNSPECIFIED";
    case Table_TimestampGranularity.MILLIS:
      return "MILLIS";
    case Table_TimestampGranularity.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Defines a view over a table's fields. */
export enum Table_View {
  /** VIEW_UNSPECIFIED - Uses the default view for each method as documented in its request. */
  VIEW_UNSPECIFIED = 0,
  /** NAME_ONLY - Only populates `name`. */
  NAME_ONLY = 1,
  /** SCHEMA_VIEW - Only populates `name` and fields related to the table's schema. */
  SCHEMA_VIEW = 2,
  /**
   * REPLICATION_VIEW - Only populates `name` and fields related to the table's replication
   * state.
   */
  REPLICATION_VIEW = 3,
  /** ENCRYPTION_VIEW - Only populates `name` and fields related to the table's encryption state. */
  ENCRYPTION_VIEW = 5,
  /** FULL - Populates all fields. */
  FULL = 4,
  UNRECOGNIZED = -1,
}

export function table_ViewFromJSON(object: any): Table_View {
  switch (object) {
    case 0:
    case "VIEW_UNSPECIFIED":
      return Table_View.VIEW_UNSPECIFIED;
    case 1:
    case "NAME_ONLY":
      return Table_View.NAME_ONLY;
    case 2:
    case "SCHEMA_VIEW":
      return Table_View.SCHEMA_VIEW;
    case 3:
    case "REPLICATION_VIEW":
      return Table_View.REPLICATION_VIEW;
    case 5:
    case "ENCRYPTION_VIEW":
      return Table_View.ENCRYPTION_VIEW;
    case 4:
    case "FULL":
      return Table_View.FULL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Table_View.UNRECOGNIZED;
  }
}

export function table_ViewToJSON(object: Table_View): string {
  switch (object) {
    case Table_View.VIEW_UNSPECIFIED:
      return "VIEW_UNSPECIFIED";
    case Table_View.NAME_ONLY:
      return "NAME_ONLY";
    case Table_View.SCHEMA_VIEW:
      return "SCHEMA_VIEW";
    case Table_View.REPLICATION_VIEW:
      return "REPLICATION_VIEW";
    case Table_View.ENCRYPTION_VIEW:
      return "ENCRYPTION_VIEW";
    case Table_View.FULL:
      return "FULL";
    case Table_View.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The state of a table's data in a particular cluster. */
export interface Table_ClusterState {
  /** Output only. The state of replication for the table in this cluster. */
  replicationState: Table_ClusterState_ReplicationState;
  /**
   * Output only. The encryption information for the table in this cluster.
   * If the encryption key protecting this resource is customer managed, then
   * its version can be rotated in Cloud Key Management Service (Cloud KMS).
   * The primary version of the key and its status will be reflected here when
   * changes propagate from Cloud KMS.
   */
  encryptionInfo: EncryptionInfo[];
}

/** Table replication states. */
export enum Table_ClusterState_ReplicationState {
  /** STATE_NOT_KNOWN - The replication state of the table is unknown in this cluster. */
  STATE_NOT_KNOWN = 0,
  /**
   * INITIALIZING - The cluster was recently created, and the table must finish copying
   * over pre-existing data from other clusters before it can begin
   * receiving live replication updates and serving Data API requests.
   */
  INITIALIZING = 1,
  /**
   * PLANNED_MAINTENANCE - The table is temporarily unable to serve Data API requests from this
   * cluster due to planned internal maintenance.
   */
  PLANNED_MAINTENANCE = 2,
  /**
   * UNPLANNED_MAINTENANCE - The table is temporarily unable to serve Data API requests from this
   * cluster due to unplanned or emergency maintenance.
   */
  UNPLANNED_MAINTENANCE = 3,
  /**
   * READY - The table can serve Data API requests from this cluster. Depending on
   * replication delay, reads may not immediately reflect the state of the
   * table in other clusters.
   */
  READY = 4,
  /**
   * READY_OPTIMIZING - The table is fully created and ready for use after a restore, and is
   * being optimized for performance. When optimizations are complete, the
   * table will transition to `READY` state.
   */
  READY_OPTIMIZING = 5,
  UNRECOGNIZED = -1,
}

export function table_ClusterState_ReplicationStateFromJSON(object: any): Table_ClusterState_ReplicationState {
  switch (object) {
    case 0:
    case "STATE_NOT_KNOWN":
      return Table_ClusterState_ReplicationState.STATE_NOT_KNOWN;
    case 1:
    case "INITIALIZING":
      return Table_ClusterState_ReplicationState.INITIALIZING;
    case 2:
    case "PLANNED_MAINTENANCE":
      return Table_ClusterState_ReplicationState.PLANNED_MAINTENANCE;
    case 3:
    case "UNPLANNED_MAINTENANCE":
      return Table_ClusterState_ReplicationState.UNPLANNED_MAINTENANCE;
    case 4:
    case "READY":
      return Table_ClusterState_ReplicationState.READY;
    case 5:
    case "READY_OPTIMIZING":
      return Table_ClusterState_ReplicationState.READY_OPTIMIZING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Table_ClusterState_ReplicationState.UNRECOGNIZED;
  }
}

export function table_ClusterState_ReplicationStateToJSON(object: Table_ClusterState_ReplicationState): string {
  switch (object) {
    case Table_ClusterState_ReplicationState.STATE_NOT_KNOWN:
      return "STATE_NOT_KNOWN";
    case Table_ClusterState_ReplicationState.INITIALIZING:
      return "INITIALIZING";
    case Table_ClusterState_ReplicationState.PLANNED_MAINTENANCE:
      return "PLANNED_MAINTENANCE";
    case Table_ClusterState_ReplicationState.UNPLANNED_MAINTENANCE:
      return "UNPLANNED_MAINTENANCE";
    case Table_ClusterState_ReplicationState.READY:
      return "READY";
    case Table_ClusterState_ReplicationState.READY_OPTIMIZING:
      return "READY_OPTIMIZING";
    case Table_ClusterState_ReplicationState.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Defines an automated backup policy for a table */
export interface Table_AutomatedBackupPolicy {
  /**
   * Required. How long the automated backups should be retained. The only
   * supported value at this time is 3 days.
   */
  retentionPeriod:
    | Duration
    | undefined;
  /**
   * Required. How frequently automated backups should occur. The only
   * supported value at this time is 24 hours.
   */
  frequency: Duration | undefined;
}

export interface Table_ClusterStatesEntry {
  key: string;
  value: Table_ClusterState | undefined;
}

export interface Table_ColumnFamiliesEntry {
  key: string;
  value: ColumnFamily | undefined;
}

/**
 * AuthorizedViews represent subsets of a particular Cloud Bigtable table. Users
 * can configure access to each Authorized View independently from the table and
 * use the existing Data APIs to access the subset of data.
 */
export interface AuthorizedView {
  /**
   * Identifier. The name of this AuthorizedView.
   * Values are of the form
   * `projects/{project}/instances/{instance}/tables/{table}/authorizedViews/{authorized_view}`
   */
  name: string;
  /** An AuthorizedView permitting access to an explicit subset of a Table. */
  subsetView?:
    | AuthorizedView_SubsetView
    | undefined;
  /**
   * The etag for this AuthorizedView.
   * If this is provided on update, it must match the server's etag. The server
   * returns ABORTED error on a mismatched etag.
   */
  etag: string;
  /**
   * Set to true to make the AuthorizedView protected against deletion.
   * The parent Table and containing Instance cannot be deleted if an
   * AuthorizedView has this bit set.
   */
  deletionProtection: boolean;
}

/** Defines a subset of an AuthorizedView's fields. */
export enum AuthorizedView_ResponseView {
  /** RESPONSE_VIEW_UNSPECIFIED - Uses the default view for each method as documented in the request. */
  RESPONSE_VIEW_UNSPECIFIED = 0,
  /** NAME_ONLY - Only populates `name`. */
  NAME_ONLY = 1,
  /**
   * BASIC - Only populates the AuthorizedView's basic metadata. This includes:
   * name, deletion_protection, etag.
   */
  BASIC = 2,
  /** FULL - Populates every fields. */
  FULL = 3,
  UNRECOGNIZED = -1,
}

export function authorizedView_ResponseViewFromJSON(object: any): AuthorizedView_ResponseView {
  switch (object) {
    case 0:
    case "RESPONSE_VIEW_UNSPECIFIED":
      return AuthorizedView_ResponseView.RESPONSE_VIEW_UNSPECIFIED;
    case 1:
    case "NAME_ONLY":
      return AuthorizedView_ResponseView.NAME_ONLY;
    case 2:
    case "BASIC":
      return AuthorizedView_ResponseView.BASIC;
    case 3:
    case "FULL":
      return AuthorizedView_ResponseView.FULL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AuthorizedView_ResponseView.UNRECOGNIZED;
  }
}

export function authorizedView_ResponseViewToJSON(object: AuthorizedView_ResponseView): string {
  switch (object) {
    case AuthorizedView_ResponseView.RESPONSE_VIEW_UNSPECIFIED:
      return "RESPONSE_VIEW_UNSPECIFIED";
    case AuthorizedView_ResponseView.NAME_ONLY:
      return "NAME_ONLY";
    case AuthorizedView_ResponseView.BASIC:
      return "BASIC";
    case AuthorizedView_ResponseView.FULL:
      return "FULL";
    case AuthorizedView_ResponseView.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Subsets of a column family that are included in this AuthorizedView. */
export interface AuthorizedView_FamilySubsets {
  /** Individual exact column qualifiers to be included in the AuthorizedView. */
  qualifiers: Buffer[];
  /**
   * Prefixes for qualifiers to be included in the AuthorizedView. Every
   * qualifier starting with one of these prefixes is included in the
   * AuthorizedView. To provide access to all qualifiers, include the empty
   * string as a prefix
   * ("").
   */
  qualifierPrefixes: Buffer[];
}

/** Defines a simple AuthorizedView that is a subset of the underlying Table. */
export interface AuthorizedView_SubsetView {
  /**
   * Row prefixes to be included in the AuthorizedView.
   * To provide access to all rows, include the empty string as a prefix ("").
   */
  rowPrefixes: Buffer[];
  /**
   * Map from column family name to the columns in this family to be included
   * in the AuthorizedView.
   */
  familySubsets: { [key: string]: AuthorizedView_FamilySubsets };
}

export interface AuthorizedView_SubsetView_FamilySubsetsEntry {
  key: string;
  value: AuthorizedView_FamilySubsets | undefined;
}

/** A set of columns within a table which share a common configuration. */
export interface ColumnFamily {
  /**
   * Garbage collection rule specified as a protobuf.
   * Must serialize to at most 500 bytes.
   *
   * NOTE: Garbage collection executes opportunistically in the background, and
   * so it's possible for reads to return a cell even if it matches the active
   * GC expression for its family.
   */
  gcRule:
    | GcRule
    | undefined;
  /**
   * The type of data stored in each of this family's cell values, including its
   * full encoding. If omitted, the family only serves raw untyped bytes.
   *
   * For now, only the `Aggregate` type is supported.
   *
   * `Aggregate` can only be set at family creation and is immutable afterwards.
   *
   * If `value_type` is `Aggregate`, written data must be compatible with:
   *  * `value_type.input_type` for `AddInput` mutations
   */
  valueType: Type | undefined;
}

/** Rule for determining which cells to delete during garbage collection. */
export interface GcRule {
  /** Delete all cells in a column except the most recent N. */
  maxNumVersions?:
    | number
    | undefined;
  /**
   * Delete cells in a column older than the given age.
   * Values must be at least one millisecond, and will be truncated to
   * microsecond granularity.
   */
  maxAge?:
    | Duration
    | undefined;
  /** Delete cells that would be deleted by every nested rule. */
  intersection?:
    | GcRule_Intersection
    | undefined;
  /** Delete cells that would be deleted by any nested rule. */
  union?: GcRule_Union | undefined;
}

/** A GcRule which deletes cells matching all of the given rules. */
export interface GcRule_Intersection {
  /** Only delete cells which would be deleted by every element of `rules`. */
  rules: GcRule[];
}

/** A GcRule which deletes cells matching any of the given rules. */
export interface GcRule_Union {
  /** Delete cells which would be deleted by any element of `rules`. */
  rules: GcRule[];
}

/**
 * Encryption information for a given resource.
 * If this resource is protected with customer managed encryption, the in-use
 * Cloud Key Management Service (Cloud KMS) key version is specified along with
 * its status.
 */
export interface EncryptionInfo {
  /** Output only. The type of encryption used to protect this resource. */
  encryptionType: EncryptionInfo_EncryptionType;
  /**
   * Output only. The status of encrypt/decrypt calls on underlying data for
   * this resource. Regardless of status, the existing data is always encrypted
   * at rest.
   */
  encryptionStatus:
    | Status
    | undefined;
  /**
   * Output only. The version of the Cloud KMS key specified in the parent
   * cluster that is in use for the data underlying this table.
   */
  kmsKeyVersion: string;
}

/** Possible encryption types for a resource. */
export enum EncryptionInfo_EncryptionType {
  /** ENCRYPTION_TYPE_UNSPECIFIED - Encryption type was not specified, though data at rest remains encrypted. */
  ENCRYPTION_TYPE_UNSPECIFIED = 0,
  /**
   * GOOGLE_DEFAULT_ENCRYPTION - The data backing this resource is encrypted at rest with a key that is
   * fully managed by Google. No key version or status will be populated.
   * This is the default state.
   */
  GOOGLE_DEFAULT_ENCRYPTION = 1,
  /**
   * CUSTOMER_MANAGED_ENCRYPTION - The data backing this resource is encrypted at rest with a key that is
   * managed by the customer.
   * The in-use version of the key and its status are populated for
   * CMEK-protected tables.
   * CMEK-protected backups are pinned to the key version that was in use at
   * the time the backup was taken. This key version is populated but its
   * status is not tracked and is reported as `UNKNOWN`.
   */
  CUSTOMER_MANAGED_ENCRYPTION = 2,
  UNRECOGNIZED = -1,
}

export function encryptionInfo_EncryptionTypeFromJSON(object: any): EncryptionInfo_EncryptionType {
  switch (object) {
    case 0:
    case "ENCRYPTION_TYPE_UNSPECIFIED":
      return EncryptionInfo_EncryptionType.ENCRYPTION_TYPE_UNSPECIFIED;
    case 1:
    case "GOOGLE_DEFAULT_ENCRYPTION":
      return EncryptionInfo_EncryptionType.GOOGLE_DEFAULT_ENCRYPTION;
    case 2:
    case "CUSTOMER_MANAGED_ENCRYPTION":
      return EncryptionInfo_EncryptionType.CUSTOMER_MANAGED_ENCRYPTION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return EncryptionInfo_EncryptionType.UNRECOGNIZED;
  }
}

export function encryptionInfo_EncryptionTypeToJSON(object: EncryptionInfo_EncryptionType): string {
  switch (object) {
    case EncryptionInfo_EncryptionType.ENCRYPTION_TYPE_UNSPECIFIED:
      return "ENCRYPTION_TYPE_UNSPECIFIED";
    case EncryptionInfo_EncryptionType.GOOGLE_DEFAULT_ENCRYPTION:
      return "GOOGLE_DEFAULT_ENCRYPTION";
    case EncryptionInfo_EncryptionType.CUSTOMER_MANAGED_ENCRYPTION:
      return "CUSTOMER_MANAGED_ENCRYPTION";
    case EncryptionInfo_EncryptionType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * A snapshot of a table at a particular time. A snapshot can be used as a
 * checkpoint for data restoration or a data source for a new table.
 *
 * Note: This is a private alpha release of Cloud Bigtable snapshots. This
 * feature is not currently available to most Cloud Bigtable customers. This
 * feature might be changed in backward-incompatible ways and is not recommended
 * for production use. It is not subject to any SLA or deprecation policy.
 */
export interface Snapshot {
  /**
   * The unique name of the snapshot.
   * Values are of the form
   * `projects/{project}/instances/{instance}/clusters/{cluster}/snapshots/{snapshot}`.
   */
  name: string;
  /** Output only. The source table at the time the snapshot was taken. */
  sourceTable:
    | Table
    | undefined;
  /**
   * Output only. The size of the data in the source table at the time the
   * snapshot was taken. In some cases, this value may be computed
   * asynchronously via a background process and a placeholder of 0 will be used
   * in the meantime.
   */
  dataSizeBytes: Long;
  /** Output only. The time when the snapshot is created. */
  createTime:
    | Date
    | undefined;
  /**
   * The time when the snapshot will be deleted. The maximum amount of time a
   * snapshot can stay active is 365 days. If 'ttl' is not specified,
   * the default maximum of 365 days will be used.
   */
  deleteTime:
    | Date
    | undefined;
  /** Output only. The current state of the snapshot. */
  state: Snapshot_State;
  /** Description of the snapshot. */
  description: string;
}

/** Possible states of a snapshot. */
export enum Snapshot_State {
  /** STATE_NOT_KNOWN - The state of the snapshot could not be determined. */
  STATE_NOT_KNOWN = 0,
  /** READY - The snapshot has been successfully created and can serve all requests. */
  READY = 1,
  /**
   * CREATING - The snapshot is currently being created, and may be destroyed if the
   * creation process encounters an error. A snapshot may not be restored to a
   * table while it is being created.
   */
  CREATING = 2,
  UNRECOGNIZED = -1,
}

export function snapshot_StateFromJSON(object: any): Snapshot_State {
  switch (object) {
    case 0:
    case "STATE_NOT_KNOWN":
      return Snapshot_State.STATE_NOT_KNOWN;
    case 1:
    case "READY":
      return Snapshot_State.READY;
    case 2:
    case "CREATING":
      return Snapshot_State.CREATING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Snapshot_State.UNRECOGNIZED;
  }
}

export function snapshot_StateToJSON(object: Snapshot_State): string {
  switch (object) {
    case Snapshot_State.STATE_NOT_KNOWN:
      return "STATE_NOT_KNOWN";
    case Snapshot_State.READY:
      return "READY";
    case Snapshot_State.CREATING:
      return "CREATING";
    case Snapshot_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** A backup of a Cloud Bigtable table. */
export interface Backup {
  /**
   * A globally unique identifier for the backup which cannot be
   * changed. Values are of the form
   * `projects/{project}/instances/{instance}/clusters/{cluster}/
   *    backups/[_a-zA-Z0-9][-_.a-zA-Z0-9]*`
   * The final segment of the name must be between 1 and 50 characters
   * in length.
   *
   * The backup is stored in the cluster identified by the prefix of the backup
   * name of the form
   * `projects/{project}/instances/{instance}/clusters/{cluster}`.
   */
  name: string;
  /**
   * Required. Immutable. Name of the table from which this backup was created.
   * This needs to be in the same instance as the backup. Values are of the form
   * `projects/{project}/instances/{instance}/tables/{source_table}`.
   */
  sourceTable: string;
  /**
   * Output only. Name of the backup from which this backup was copied. If a
   * backup is not created by copying a backup, this field will be empty. Values
   * are of the form:
   * projects/<project>/instances/<instance>/clusters/<cluster>/backups/<backup>
   */
  sourceBackup: string;
  /**
   * Required. The expiration time of the backup.
   * When creating a backup or updating its `expire_time`, the value must be
   * greater than the backup creation time by:
   * - At least 6 hours
   * - At most 90 days
   *
   * Once the `expire_time` has passed, Cloud Bigtable will delete the backup.
   */
  expireTime:
    | Date
    | undefined;
  /**
   * Output only. `start_time` is the time that the backup was started
   * (i.e. approximately the time the
   * [CreateBackup][google.bigtable.admin.v2.BigtableTableAdmin.CreateBackup]
   * request is received).  The row data in this backup will be no older than
   * this timestamp.
   */
  startTime:
    | Date
    | undefined;
  /**
   * Output only. `end_time` is the time that the backup was finished. The row
   * data in the backup will be no newer than this timestamp.
   */
  endTime:
    | Date
    | undefined;
  /** Output only. Size of the backup in bytes. */
  sizeBytes: Long;
  /** Output only. The current state of the backup. */
  state: Backup_State;
  /** Output only. The encryption information for the backup. */
  encryptionInfo:
    | EncryptionInfo
    | undefined;
  /** Indicates the backup type of the backup. */
  backupType: Backup_BackupType;
  /**
   * The time at which the hot backup will be converted to a standard backup.
   * Once the `hot_to_standard_time` has passed, Cloud Bigtable will convert the
   * hot backup to a standard backup. This value must be greater than the backup
   * creation time by:
   * - At least 24 hours
   *
   * This field only applies for hot backups. When creating or updating a
   * standard backup, attempting to set this field will fail the request.
   */
  hotToStandardTime: Date | undefined;
}

/** Indicates the current state of the backup. */
export enum Backup_State {
  /** STATE_UNSPECIFIED - Not specified. */
  STATE_UNSPECIFIED = 0,
  /**
   * CREATING - The pending backup is still being created. Operations on the
   * backup may fail with `FAILED_PRECONDITION` in this state.
   */
  CREATING = 1,
  /** READY - The backup is complete and ready for use. */
  READY = 2,
  UNRECOGNIZED = -1,
}

export function backup_StateFromJSON(object: any): Backup_State {
  switch (object) {
    case 0:
    case "STATE_UNSPECIFIED":
      return Backup_State.STATE_UNSPECIFIED;
    case 1:
    case "CREATING":
      return Backup_State.CREATING;
    case 2:
    case "READY":
      return Backup_State.READY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Backup_State.UNRECOGNIZED;
  }
}

export function backup_StateToJSON(object: Backup_State): string {
  switch (object) {
    case Backup_State.STATE_UNSPECIFIED:
      return "STATE_UNSPECIFIED";
    case Backup_State.CREATING:
      return "CREATING";
    case Backup_State.READY:
      return "READY";
    case Backup_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The type of the backup. */
export enum Backup_BackupType {
  /** BACKUP_TYPE_UNSPECIFIED - Not specified. */
  BACKUP_TYPE_UNSPECIFIED = 0,
  /**
   * STANDARD - The default type for Cloud Bigtable managed backups. Supported for
   * backups created in both HDD and SSD instances. Requires optimization when
   * restored to a table in an SSD instance.
   */
  STANDARD = 1,
  /**
   * HOT - A backup type with faster restore to SSD performance. Only supported for
   * backups created in SSD instances. A new SSD table restored from a hot
   * backup reaches production performance more quickly than a standard
   * backup.
   */
  HOT = 2,
  UNRECOGNIZED = -1,
}

export function backup_BackupTypeFromJSON(object: any): Backup_BackupType {
  switch (object) {
    case 0:
    case "BACKUP_TYPE_UNSPECIFIED":
      return Backup_BackupType.BACKUP_TYPE_UNSPECIFIED;
    case 1:
    case "STANDARD":
      return Backup_BackupType.STANDARD;
    case 2:
    case "HOT":
      return Backup_BackupType.HOT;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Backup_BackupType.UNRECOGNIZED;
  }
}

export function backup_BackupTypeToJSON(object: Backup_BackupType): string {
  switch (object) {
    case Backup_BackupType.BACKUP_TYPE_UNSPECIFIED:
      return "BACKUP_TYPE_UNSPECIFIED";
    case Backup_BackupType.STANDARD:
      return "STANDARD";
    case Backup_BackupType.HOT:
      return "HOT";
    case Backup_BackupType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Information about a backup. */
export interface BackupInfo {
  /** Output only. Name of the backup. */
  backup: string;
  /**
   * Output only. The time that the backup was started. Row data in the backup
   * will be no older than this timestamp.
   */
  startTime:
    | Date
    | undefined;
  /**
   * Output only. This time that the backup was finished. Row data in the
   * backup will be no newer than this timestamp.
   */
  endTime:
    | Date
    | undefined;
  /** Output only. Name of the table the backup was created from. */
  sourceTable: string;
  /**
   * Output only. Name of the backup from which this backup was copied. If a
   * backup is not created by copying a backup, this field will be empty. Values
   * are of the form:
   * projects/<project>/instances/<instance>/clusters/<cluster>/backups/<backup>
   */
  sourceBackup: string;
}

function createBaseRestoreInfo(): RestoreInfo {
  return { sourceType: 0, backupInfo: undefined };
}

export const RestoreInfo: MessageFns<RestoreInfo> = {
  encode(message: RestoreInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.sourceType !== 0) {
      writer.uint32(8).int32(message.sourceType);
    }
    if (message.backupInfo !== undefined) {
      BackupInfo.encode(message.backupInfo, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RestoreInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRestoreInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.sourceType = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.backupInfo = BackupInfo.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RestoreInfo {
    return {
      sourceType: isSet(object.sourceType) ? restoreSourceTypeFromJSON(object.sourceType) : 0,
      backupInfo: isSet(object.backupInfo) ? BackupInfo.fromJSON(object.backupInfo) : undefined,
    };
  },

  toJSON(message: RestoreInfo): unknown {
    const obj: any = {};
    if (message.sourceType !== 0) {
      obj.sourceType = restoreSourceTypeToJSON(message.sourceType);
    }
    if (message.backupInfo !== undefined) {
      obj.backupInfo = BackupInfo.toJSON(message.backupInfo);
    }
    return obj;
  },

  create(base?: DeepPartial<RestoreInfo>): RestoreInfo {
    return RestoreInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RestoreInfo>): RestoreInfo {
    const message = createBaseRestoreInfo();
    message.sourceType = object.sourceType ?? 0;
    message.backupInfo = (object.backupInfo !== undefined && object.backupInfo !== null)
      ? BackupInfo.fromPartial(object.backupInfo)
      : undefined;
    return message;
  },
};

function createBaseChangeStreamConfig(): ChangeStreamConfig {
  return { retentionPeriod: undefined };
}

export const ChangeStreamConfig: MessageFns<ChangeStreamConfig> = {
  encode(message: ChangeStreamConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.retentionPeriod !== undefined) {
      Duration.encode(message.retentionPeriod, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ChangeStreamConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseChangeStreamConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.retentionPeriod = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ChangeStreamConfig {
    return { retentionPeriod: isSet(object.retentionPeriod) ? Duration.fromJSON(object.retentionPeriod) : undefined };
  },

  toJSON(message: ChangeStreamConfig): unknown {
    const obj: any = {};
    if (message.retentionPeriod !== undefined) {
      obj.retentionPeriod = Duration.toJSON(message.retentionPeriod);
    }
    return obj;
  },

  create(base?: DeepPartial<ChangeStreamConfig>): ChangeStreamConfig {
    return ChangeStreamConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ChangeStreamConfig>): ChangeStreamConfig {
    const message = createBaseChangeStreamConfig();
    message.retentionPeriod = (object.retentionPeriod !== undefined && object.retentionPeriod !== null)
      ? Duration.fromPartial(object.retentionPeriod)
      : undefined;
    return message;
  },
};

function createBaseTable(): Table {
  return {
    name: "",
    clusterStates: {},
    columnFamilies: {},
    granularity: 0,
    restoreInfo: undefined,
    changeStreamConfig: undefined,
    deletionProtection: false,
    automatedBackupPolicy: undefined,
  };
}

export const Table: MessageFns<Table> = {
  encode(message: Table, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    Object.entries(message.clusterStates).forEach(([key, value]) => {
      Table_ClusterStatesEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    Object.entries(message.columnFamilies).forEach(([key, value]) => {
      Table_ColumnFamiliesEntry.encode({ key: key as any, value }, writer.uint32(26).fork()).join();
    });
    if (message.granularity !== 0) {
      writer.uint32(32).int32(message.granularity);
    }
    if (message.restoreInfo !== undefined) {
      RestoreInfo.encode(message.restoreInfo, writer.uint32(50).fork()).join();
    }
    if (message.changeStreamConfig !== undefined) {
      ChangeStreamConfig.encode(message.changeStreamConfig, writer.uint32(66).fork()).join();
    }
    if (message.deletionProtection !== false) {
      writer.uint32(72).bool(message.deletionProtection);
    }
    if (message.automatedBackupPolicy !== undefined) {
      Table_AutomatedBackupPolicy.encode(message.automatedBackupPolicy, writer.uint32(106).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Table {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTable();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = Table_ClusterStatesEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.clusterStates[entry2.key] = entry2.value;
          }
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          const entry3 = Table_ColumnFamiliesEntry.decode(reader, reader.uint32());
          if (entry3.value !== undefined) {
            message.columnFamilies[entry3.key] = entry3.value;
          }
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.granularity = reader.int32() as any;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.restoreInfo = RestoreInfo.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.changeStreamConfig = ChangeStreamConfig.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.deletionProtection = reader.bool();
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.automatedBackupPolicy = Table_AutomatedBackupPolicy.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Table {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      clusterStates: isObject(object.clusterStates)
        ? Object.entries(object.clusterStates).reduce<{ [key: string]: Table_ClusterState }>((acc, [key, value]) => {
          acc[key] = Table_ClusterState.fromJSON(value);
          return acc;
        }, {})
        : {},
      columnFamilies: isObject(object.columnFamilies)
        ? Object.entries(object.columnFamilies).reduce<{ [key: string]: ColumnFamily }>((acc, [key, value]) => {
          acc[key] = ColumnFamily.fromJSON(value);
          return acc;
        }, {})
        : {},
      granularity: isSet(object.granularity) ? table_TimestampGranularityFromJSON(object.granularity) : 0,
      restoreInfo: isSet(object.restoreInfo) ? RestoreInfo.fromJSON(object.restoreInfo) : undefined,
      changeStreamConfig: isSet(object.changeStreamConfig)
        ? ChangeStreamConfig.fromJSON(object.changeStreamConfig)
        : undefined,
      deletionProtection: isSet(object.deletionProtection) ? globalThis.Boolean(object.deletionProtection) : false,
      automatedBackupPolicy: isSet(object.automatedBackupPolicy)
        ? Table_AutomatedBackupPolicy.fromJSON(object.automatedBackupPolicy)
        : undefined,
    };
  },

  toJSON(message: Table): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.clusterStates) {
      const entries = Object.entries(message.clusterStates);
      if (entries.length > 0) {
        obj.clusterStates = {};
        entries.forEach(([k, v]) => {
          obj.clusterStates[k] = Table_ClusterState.toJSON(v);
        });
      }
    }
    if (message.columnFamilies) {
      const entries = Object.entries(message.columnFamilies);
      if (entries.length > 0) {
        obj.columnFamilies = {};
        entries.forEach(([k, v]) => {
          obj.columnFamilies[k] = ColumnFamily.toJSON(v);
        });
      }
    }
    if (message.granularity !== 0) {
      obj.granularity = table_TimestampGranularityToJSON(message.granularity);
    }
    if (message.restoreInfo !== undefined) {
      obj.restoreInfo = RestoreInfo.toJSON(message.restoreInfo);
    }
    if (message.changeStreamConfig !== undefined) {
      obj.changeStreamConfig = ChangeStreamConfig.toJSON(message.changeStreamConfig);
    }
    if (message.deletionProtection !== false) {
      obj.deletionProtection = message.deletionProtection;
    }
    if (message.automatedBackupPolicy !== undefined) {
      obj.automatedBackupPolicy = Table_AutomatedBackupPolicy.toJSON(message.automatedBackupPolicy);
    }
    return obj;
  },

  create(base?: DeepPartial<Table>): Table {
    return Table.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Table>): Table {
    const message = createBaseTable();
    message.name = object.name ?? "";
    message.clusterStates = Object.entries(object.clusterStates ?? {}).reduce<{ [key: string]: Table_ClusterState }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = Table_ClusterState.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    message.columnFamilies = Object.entries(object.columnFamilies ?? {}).reduce<{ [key: string]: ColumnFamily }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = ColumnFamily.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    message.granularity = object.granularity ?? 0;
    message.restoreInfo = (object.restoreInfo !== undefined && object.restoreInfo !== null)
      ? RestoreInfo.fromPartial(object.restoreInfo)
      : undefined;
    message.changeStreamConfig = (object.changeStreamConfig !== undefined && object.changeStreamConfig !== null)
      ? ChangeStreamConfig.fromPartial(object.changeStreamConfig)
      : undefined;
    message.deletionProtection = object.deletionProtection ?? false;
    message.automatedBackupPolicy =
      (object.automatedBackupPolicy !== undefined && object.automatedBackupPolicy !== null)
        ? Table_AutomatedBackupPolicy.fromPartial(object.automatedBackupPolicy)
        : undefined;
    return message;
  },
};

function createBaseTable_ClusterState(): Table_ClusterState {
  return { replicationState: 0, encryptionInfo: [] };
}

export const Table_ClusterState: MessageFns<Table_ClusterState> = {
  encode(message: Table_ClusterState, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.replicationState !== 0) {
      writer.uint32(8).int32(message.replicationState);
    }
    for (const v of message.encryptionInfo) {
      EncryptionInfo.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Table_ClusterState {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTable_ClusterState();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.replicationState = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.encryptionInfo.push(EncryptionInfo.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Table_ClusterState {
    return {
      replicationState: isSet(object.replicationState)
        ? table_ClusterState_ReplicationStateFromJSON(object.replicationState)
        : 0,
      encryptionInfo: globalThis.Array.isArray(object?.encryptionInfo)
        ? object.encryptionInfo.map((e: any) => EncryptionInfo.fromJSON(e))
        : [],
    };
  },

  toJSON(message: Table_ClusterState): unknown {
    const obj: any = {};
    if (message.replicationState !== 0) {
      obj.replicationState = table_ClusterState_ReplicationStateToJSON(message.replicationState);
    }
    if (message.encryptionInfo?.length) {
      obj.encryptionInfo = message.encryptionInfo.map((e) => EncryptionInfo.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<Table_ClusterState>): Table_ClusterState {
    return Table_ClusterState.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Table_ClusterState>): Table_ClusterState {
    const message = createBaseTable_ClusterState();
    message.replicationState = object.replicationState ?? 0;
    message.encryptionInfo = object.encryptionInfo?.map((e) => EncryptionInfo.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTable_AutomatedBackupPolicy(): Table_AutomatedBackupPolicy {
  return { retentionPeriod: undefined, frequency: undefined };
}

export const Table_AutomatedBackupPolicy: MessageFns<Table_AutomatedBackupPolicy> = {
  encode(message: Table_AutomatedBackupPolicy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.retentionPeriod !== undefined) {
      Duration.encode(message.retentionPeriod, writer.uint32(10).fork()).join();
    }
    if (message.frequency !== undefined) {
      Duration.encode(message.frequency, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Table_AutomatedBackupPolicy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTable_AutomatedBackupPolicy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.retentionPeriod = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.frequency = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Table_AutomatedBackupPolicy {
    return {
      retentionPeriod: isSet(object.retentionPeriod) ? Duration.fromJSON(object.retentionPeriod) : undefined,
      frequency: isSet(object.frequency) ? Duration.fromJSON(object.frequency) : undefined,
    };
  },

  toJSON(message: Table_AutomatedBackupPolicy): unknown {
    const obj: any = {};
    if (message.retentionPeriod !== undefined) {
      obj.retentionPeriod = Duration.toJSON(message.retentionPeriod);
    }
    if (message.frequency !== undefined) {
      obj.frequency = Duration.toJSON(message.frequency);
    }
    return obj;
  },

  create(base?: DeepPartial<Table_AutomatedBackupPolicy>): Table_AutomatedBackupPolicy {
    return Table_AutomatedBackupPolicy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Table_AutomatedBackupPolicy>): Table_AutomatedBackupPolicy {
    const message = createBaseTable_AutomatedBackupPolicy();
    message.retentionPeriod = (object.retentionPeriod !== undefined && object.retentionPeriod !== null)
      ? Duration.fromPartial(object.retentionPeriod)
      : undefined;
    message.frequency = (object.frequency !== undefined && object.frequency !== null)
      ? Duration.fromPartial(object.frequency)
      : undefined;
    return message;
  },
};

function createBaseTable_ClusterStatesEntry(): Table_ClusterStatesEntry {
  return { key: "", value: undefined };
}

export const Table_ClusterStatesEntry: MessageFns<Table_ClusterStatesEntry> = {
  encode(message: Table_ClusterStatesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      Table_ClusterState.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Table_ClusterStatesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTable_ClusterStatesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = Table_ClusterState.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Table_ClusterStatesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? Table_ClusterState.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: Table_ClusterStatesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = Table_ClusterState.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<Table_ClusterStatesEntry>): Table_ClusterStatesEntry {
    return Table_ClusterStatesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Table_ClusterStatesEntry>): Table_ClusterStatesEntry {
    const message = createBaseTable_ClusterStatesEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? Table_ClusterState.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseTable_ColumnFamiliesEntry(): Table_ColumnFamiliesEntry {
  return { key: "", value: undefined };
}

export const Table_ColumnFamiliesEntry: MessageFns<Table_ColumnFamiliesEntry> = {
  encode(message: Table_ColumnFamiliesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      ColumnFamily.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Table_ColumnFamiliesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTable_ColumnFamiliesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = ColumnFamily.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Table_ColumnFamiliesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? ColumnFamily.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: Table_ColumnFamiliesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = ColumnFamily.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<Table_ColumnFamiliesEntry>): Table_ColumnFamiliesEntry {
    return Table_ColumnFamiliesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Table_ColumnFamiliesEntry>): Table_ColumnFamiliesEntry {
    const message = createBaseTable_ColumnFamiliesEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? ColumnFamily.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseAuthorizedView(): AuthorizedView {
  return { name: "", subsetView: undefined, etag: "", deletionProtection: false };
}

export const AuthorizedView: MessageFns<AuthorizedView> = {
  encode(message: AuthorizedView, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.subsetView !== undefined) {
      AuthorizedView_SubsetView.encode(message.subsetView, writer.uint32(18).fork()).join();
    }
    if (message.etag !== "") {
      writer.uint32(26).string(message.etag);
    }
    if (message.deletionProtection !== false) {
      writer.uint32(32).bool(message.deletionProtection);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AuthorizedView {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAuthorizedView();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.subsetView = AuthorizedView_SubsetView.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.etag = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.deletionProtection = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AuthorizedView {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      subsetView: isSet(object.subsetView) ? AuthorizedView_SubsetView.fromJSON(object.subsetView) : undefined,
      etag: isSet(object.etag) ? globalThis.String(object.etag) : "",
      deletionProtection: isSet(object.deletionProtection) ? globalThis.Boolean(object.deletionProtection) : false,
    };
  },

  toJSON(message: AuthorizedView): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.subsetView !== undefined) {
      obj.subsetView = AuthorizedView_SubsetView.toJSON(message.subsetView);
    }
    if (message.etag !== "") {
      obj.etag = message.etag;
    }
    if (message.deletionProtection !== false) {
      obj.deletionProtection = message.deletionProtection;
    }
    return obj;
  },

  create(base?: DeepPartial<AuthorizedView>): AuthorizedView {
    return AuthorizedView.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AuthorizedView>): AuthorizedView {
    const message = createBaseAuthorizedView();
    message.name = object.name ?? "";
    message.subsetView = (object.subsetView !== undefined && object.subsetView !== null)
      ? AuthorizedView_SubsetView.fromPartial(object.subsetView)
      : undefined;
    message.etag = object.etag ?? "";
    message.deletionProtection = object.deletionProtection ?? false;
    return message;
  },
};

function createBaseAuthorizedView_FamilySubsets(): AuthorizedView_FamilySubsets {
  return { qualifiers: [], qualifierPrefixes: [] };
}

export const AuthorizedView_FamilySubsets: MessageFns<AuthorizedView_FamilySubsets> = {
  encode(message: AuthorizedView_FamilySubsets, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.qualifiers) {
      writer.uint32(10).bytes(v!);
    }
    for (const v of message.qualifierPrefixes) {
      writer.uint32(18).bytes(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AuthorizedView_FamilySubsets {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAuthorizedView_FamilySubsets();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.qualifiers.push(Buffer.from(reader.bytes()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.qualifierPrefixes.push(Buffer.from(reader.bytes()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AuthorizedView_FamilySubsets {
    return {
      qualifiers: globalThis.Array.isArray(object?.qualifiers)
        ? object.qualifiers.map((e: any) => Buffer.from(bytesFromBase64(e)))
        : [],
      qualifierPrefixes: globalThis.Array.isArray(object?.qualifierPrefixes)
        ? object.qualifierPrefixes.map((e: any) => Buffer.from(bytesFromBase64(e)))
        : [],
    };
  },

  toJSON(message: AuthorizedView_FamilySubsets): unknown {
    const obj: any = {};
    if (message.qualifiers?.length) {
      obj.qualifiers = message.qualifiers.map((e) => base64FromBytes(e));
    }
    if (message.qualifierPrefixes?.length) {
      obj.qualifierPrefixes = message.qualifierPrefixes.map((e) => base64FromBytes(e));
    }
    return obj;
  },

  create(base?: DeepPartial<AuthorizedView_FamilySubsets>): AuthorizedView_FamilySubsets {
    return AuthorizedView_FamilySubsets.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AuthorizedView_FamilySubsets>): AuthorizedView_FamilySubsets {
    const message = createBaseAuthorizedView_FamilySubsets();
    message.qualifiers = object.qualifiers?.map((e) => e) || [];
    message.qualifierPrefixes = object.qualifierPrefixes?.map((e) => e) || [];
    return message;
  },
};

function createBaseAuthorizedView_SubsetView(): AuthorizedView_SubsetView {
  return { rowPrefixes: [], familySubsets: {} };
}

export const AuthorizedView_SubsetView: MessageFns<AuthorizedView_SubsetView> = {
  encode(message: AuthorizedView_SubsetView, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.rowPrefixes) {
      writer.uint32(10).bytes(v!);
    }
    Object.entries(message.familySubsets).forEach(([key, value]) => {
      AuthorizedView_SubsetView_FamilySubsetsEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AuthorizedView_SubsetView {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAuthorizedView_SubsetView();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.rowPrefixes.push(Buffer.from(reader.bytes()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = AuthorizedView_SubsetView_FamilySubsetsEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.familySubsets[entry2.key] = entry2.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AuthorizedView_SubsetView {
    return {
      rowPrefixes: globalThis.Array.isArray(object?.rowPrefixes)
        ? object.rowPrefixes.map((e: any) => Buffer.from(bytesFromBase64(e)))
        : [],
      familySubsets: isObject(object.familySubsets)
        ? Object.entries(object.familySubsets).reduce<{ [key: string]: AuthorizedView_FamilySubsets }>(
          (acc, [key, value]) => {
            acc[key] = AuthorizedView_FamilySubsets.fromJSON(value);
            return acc;
          },
          {},
        )
        : {},
    };
  },

  toJSON(message: AuthorizedView_SubsetView): unknown {
    const obj: any = {};
    if (message.rowPrefixes?.length) {
      obj.rowPrefixes = message.rowPrefixes.map((e) => base64FromBytes(e));
    }
    if (message.familySubsets) {
      const entries = Object.entries(message.familySubsets);
      if (entries.length > 0) {
        obj.familySubsets = {};
        entries.forEach(([k, v]) => {
          obj.familySubsets[k] = AuthorizedView_FamilySubsets.toJSON(v);
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<AuthorizedView_SubsetView>): AuthorizedView_SubsetView {
    return AuthorizedView_SubsetView.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AuthorizedView_SubsetView>): AuthorizedView_SubsetView {
    const message = createBaseAuthorizedView_SubsetView();
    message.rowPrefixes = object.rowPrefixes?.map((e) => e) || [];
    message.familySubsets = Object.entries(object.familySubsets ?? {}).reduce<
      { [key: string]: AuthorizedView_FamilySubsets }
    >((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = AuthorizedView_FamilySubsets.fromPartial(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseAuthorizedView_SubsetView_FamilySubsetsEntry(): AuthorizedView_SubsetView_FamilySubsetsEntry {
  return { key: "", value: undefined };
}

export const AuthorizedView_SubsetView_FamilySubsetsEntry: MessageFns<AuthorizedView_SubsetView_FamilySubsetsEntry> = {
  encode(
    message: AuthorizedView_SubsetView_FamilySubsetsEntry,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      AuthorizedView_FamilySubsets.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AuthorizedView_SubsetView_FamilySubsetsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAuthorizedView_SubsetView_FamilySubsetsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = AuthorizedView_FamilySubsets.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AuthorizedView_SubsetView_FamilySubsetsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? AuthorizedView_FamilySubsets.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: AuthorizedView_SubsetView_FamilySubsetsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = AuthorizedView_FamilySubsets.toJSON(message.value);
    }
    return obj;
  },

  create(
    base?: DeepPartial<AuthorizedView_SubsetView_FamilySubsetsEntry>,
  ): AuthorizedView_SubsetView_FamilySubsetsEntry {
    return AuthorizedView_SubsetView_FamilySubsetsEntry.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<AuthorizedView_SubsetView_FamilySubsetsEntry>,
  ): AuthorizedView_SubsetView_FamilySubsetsEntry {
    const message = createBaseAuthorizedView_SubsetView_FamilySubsetsEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? AuthorizedView_FamilySubsets.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseColumnFamily(): ColumnFamily {
  return { gcRule: undefined, valueType: undefined };
}

export const ColumnFamily: MessageFns<ColumnFamily> = {
  encode(message: ColumnFamily, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcRule !== undefined) {
      GcRule.encode(message.gcRule, writer.uint32(10).fork()).join();
    }
    if (message.valueType !== undefined) {
      Type.encode(message.valueType, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ColumnFamily {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseColumnFamily();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.gcRule = GcRule.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.valueType = Type.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ColumnFamily {
    return {
      gcRule: isSet(object.gcRule) ? GcRule.fromJSON(object.gcRule) : undefined,
      valueType: isSet(object.valueType) ? Type.fromJSON(object.valueType) : undefined,
    };
  },

  toJSON(message: ColumnFamily): unknown {
    const obj: any = {};
    if (message.gcRule !== undefined) {
      obj.gcRule = GcRule.toJSON(message.gcRule);
    }
    if (message.valueType !== undefined) {
      obj.valueType = Type.toJSON(message.valueType);
    }
    return obj;
  },

  create(base?: DeepPartial<ColumnFamily>): ColumnFamily {
    return ColumnFamily.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ColumnFamily>): ColumnFamily {
    const message = createBaseColumnFamily();
    message.gcRule = (object.gcRule !== undefined && object.gcRule !== null)
      ? GcRule.fromPartial(object.gcRule)
      : undefined;
    message.valueType = (object.valueType !== undefined && object.valueType !== null)
      ? Type.fromPartial(object.valueType)
      : undefined;
    return message;
  },
};

function createBaseGcRule(): GcRule {
  return { maxNumVersions: undefined, maxAge: undefined, intersection: undefined, union: undefined };
}

export const GcRule: MessageFns<GcRule> = {
  encode(message: GcRule, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.maxNumVersions !== undefined) {
      writer.uint32(8).int32(message.maxNumVersions);
    }
    if (message.maxAge !== undefined) {
      Duration.encode(message.maxAge, writer.uint32(18).fork()).join();
    }
    if (message.intersection !== undefined) {
      GcRule_Intersection.encode(message.intersection, writer.uint32(26).fork()).join();
    }
    if (message.union !== undefined) {
      GcRule_Union.encode(message.union, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GcRule {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGcRule();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.maxNumVersions = reader.int32();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.maxAge = Duration.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.intersection = GcRule_Intersection.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.union = GcRule_Union.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GcRule {
    return {
      maxNumVersions: isSet(object.maxNumVersions) ? globalThis.Number(object.maxNumVersions) : undefined,
      maxAge: isSet(object.maxAge) ? Duration.fromJSON(object.maxAge) : undefined,
      intersection: isSet(object.intersection) ? GcRule_Intersection.fromJSON(object.intersection) : undefined,
      union: isSet(object.union) ? GcRule_Union.fromJSON(object.union) : undefined,
    };
  },

  toJSON(message: GcRule): unknown {
    const obj: any = {};
    if (message.maxNumVersions !== undefined) {
      obj.maxNumVersions = Math.round(message.maxNumVersions);
    }
    if (message.maxAge !== undefined) {
      obj.maxAge = Duration.toJSON(message.maxAge);
    }
    if (message.intersection !== undefined) {
      obj.intersection = GcRule_Intersection.toJSON(message.intersection);
    }
    if (message.union !== undefined) {
      obj.union = GcRule_Union.toJSON(message.union);
    }
    return obj;
  },

  create(base?: DeepPartial<GcRule>): GcRule {
    return GcRule.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GcRule>): GcRule {
    const message = createBaseGcRule();
    message.maxNumVersions = object.maxNumVersions ?? undefined;
    message.maxAge = (object.maxAge !== undefined && object.maxAge !== null)
      ? Duration.fromPartial(object.maxAge)
      : undefined;
    message.intersection = (object.intersection !== undefined && object.intersection !== null)
      ? GcRule_Intersection.fromPartial(object.intersection)
      : undefined;
    message.union = (object.union !== undefined && object.union !== null)
      ? GcRule_Union.fromPartial(object.union)
      : undefined;
    return message;
  },
};

function createBaseGcRule_Intersection(): GcRule_Intersection {
  return { rules: [] };
}

export const GcRule_Intersection: MessageFns<GcRule_Intersection> = {
  encode(message: GcRule_Intersection, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.rules) {
      GcRule.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GcRule_Intersection {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGcRule_Intersection();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.rules.push(GcRule.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GcRule_Intersection {
    return { rules: globalThis.Array.isArray(object?.rules) ? object.rules.map((e: any) => GcRule.fromJSON(e)) : [] };
  },

  toJSON(message: GcRule_Intersection): unknown {
    const obj: any = {};
    if (message.rules?.length) {
      obj.rules = message.rules.map((e) => GcRule.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<GcRule_Intersection>): GcRule_Intersection {
    return GcRule_Intersection.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GcRule_Intersection>): GcRule_Intersection {
    const message = createBaseGcRule_Intersection();
    message.rules = object.rules?.map((e) => GcRule.fromPartial(e)) || [];
    return message;
  },
};

function createBaseGcRule_Union(): GcRule_Union {
  return { rules: [] };
}

export const GcRule_Union: MessageFns<GcRule_Union> = {
  encode(message: GcRule_Union, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.rules) {
      GcRule.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GcRule_Union {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGcRule_Union();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.rules.push(GcRule.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GcRule_Union {
    return { rules: globalThis.Array.isArray(object?.rules) ? object.rules.map((e: any) => GcRule.fromJSON(e)) : [] };
  },

  toJSON(message: GcRule_Union): unknown {
    const obj: any = {};
    if (message.rules?.length) {
      obj.rules = message.rules.map((e) => GcRule.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<GcRule_Union>): GcRule_Union {
    return GcRule_Union.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GcRule_Union>): GcRule_Union {
    const message = createBaseGcRule_Union();
    message.rules = object.rules?.map((e) => GcRule.fromPartial(e)) || [];
    return message;
  },
};

function createBaseEncryptionInfo(): EncryptionInfo {
  return { encryptionType: 0, encryptionStatus: undefined, kmsKeyVersion: "" };
}

export const EncryptionInfo: MessageFns<EncryptionInfo> = {
  encode(message: EncryptionInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.encryptionType !== 0) {
      writer.uint32(24).int32(message.encryptionType);
    }
    if (message.encryptionStatus !== undefined) {
      Status.encode(message.encryptionStatus, writer.uint32(34).fork()).join();
    }
    if (message.kmsKeyVersion !== "") {
      writer.uint32(18).string(message.kmsKeyVersion);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EncryptionInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEncryptionInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 24) {
            break;
          }

          message.encryptionType = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.encryptionStatus = Status.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.kmsKeyVersion = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EncryptionInfo {
    return {
      encryptionType: isSet(object.encryptionType) ? encryptionInfo_EncryptionTypeFromJSON(object.encryptionType) : 0,
      encryptionStatus: isSet(object.encryptionStatus) ? Status.fromJSON(object.encryptionStatus) : undefined,
      kmsKeyVersion: isSet(object.kmsKeyVersion) ? globalThis.String(object.kmsKeyVersion) : "",
    };
  },

  toJSON(message: EncryptionInfo): unknown {
    const obj: any = {};
    if (message.encryptionType !== 0) {
      obj.encryptionType = encryptionInfo_EncryptionTypeToJSON(message.encryptionType);
    }
    if (message.encryptionStatus !== undefined) {
      obj.encryptionStatus = Status.toJSON(message.encryptionStatus);
    }
    if (message.kmsKeyVersion !== "") {
      obj.kmsKeyVersion = message.kmsKeyVersion;
    }
    return obj;
  },

  create(base?: DeepPartial<EncryptionInfo>): EncryptionInfo {
    return EncryptionInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EncryptionInfo>): EncryptionInfo {
    const message = createBaseEncryptionInfo();
    message.encryptionType = object.encryptionType ?? 0;
    message.encryptionStatus = (object.encryptionStatus !== undefined && object.encryptionStatus !== null)
      ? Status.fromPartial(object.encryptionStatus)
      : undefined;
    message.kmsKeyVersion = object.kmsKeyVersion ?? "";
    return message;
  },
};

function createBaseSnapshot(): Snapshot {
  return {
    name: "",
    sourceTable: undefined,
    dataSizeBytes: Long.ZERO,
    createTime: undefined,
    deleteTime: undefined,
    state: 0,
    description: "",
  };
}

export const Snapshot: MessageFns<Snapshot> = {
  encode(message: Snapshot, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.sourceTable !== undefined) {
      Table.encode(message.sourceTable, writer.uint32(18).fork()).join();
    }
    if (!message.dataSizeBytes.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.dataSizeBytes.toString());
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(34).fork()).join();
    }
    if (message.deleteTime !== undefined) {
      Timestamp.encode(toTimestamp(message.deleteTime), writer.uint32(42).fork()).join();
    }
    if (message.state !== 0) {
      writer.uint32(48).int32(message.state);
    }
    if (message.description !== "") {
      writer.uint32(58).string(message.description);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Snapshot {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSnapshot();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.sourceTable = Table.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.dataSizeBytes = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.deleteTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.description = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Snapshot {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      sourceTable: isSet(object.sourceTable) ? Table.fromJSON(object.sourceTable) : undefined,
      dataSizeBytes: isSet(object.dataSizeBytes) ? Long.fromValue(object.dataSizeBytes) : Long.ZERO,
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      deleteTime: isSet(object.deleteTime) ? fromJsonTimestamp(object.deleteTime) : undefined,
      state: isSet(object.state) ? snapshot_StateFromJSON(object.state) : 0,
      description: isSet(object.description) ? globalThis.String(object.description) : "",
    };
  },

  toJSON(message: Snapshot): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.sourceTable !== undefined) {
      obj.sourceTable = Table.toJSON(message.sourceTable);
    }
    if (!message.dataSizeBytes.equals(Long.ZERO)) {
      obj.dataSizeBytes = (message.dataSizeBytes || Long.ZERO).toString();
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.deleteTime !== undefined) {
      obj.deleteTime = message.deleteTime.toISOString();
    }
    if (message.state !== 0) {
      obj.state = snapshot_StateToJSON(message.state);
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    return obj;
  },

  create(base?: DeepPartial<Snapshot>): Snapshot {
    return Snapshot.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Snapshot>): Snapshot {
    const message = createBaseSnapshot();
    message.name = object.name ?? "";
    message.sourceTable = (object.sourceTable !== undefined && object.sourceTable !== null)
      ? Table.fromPartial(object.sourceTable)
      : undefined;
    message.dataSizeBytes = (object.dataSizeBytes !== undefined && object.dataSizeBytes !== null)
      ? Long.fromValue(object.dataSizeBytes)
      : Long.ZERO;
    message.createTime = object.createTime ?? undefined;
    message.deleteTime = object.deleteTime ?? undefined;
    message.state = object.state ?? 0;
    message.description = object.description ?? "";
    return message;
  },
};

function createBaseBackup(): Backup {
  return {
    name: "",
    sourceTable: "",
    sourceBackup: "",
    expireTime: undefined,
    startTime: undefined,
    endTime: undefined,
    sizeBytes: Long.ZERO,
    state: 0,
    encryptionInfo: undefined,
    backupType: 0,
    hotToStandardTime: undefined,
  };
}

export const Backup: MessageFns<Backup> = {
  encode(message: Backup, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.sourceTable !== "") {
      writer.uint32(18).string(message.sourceTable);
    }
    if (message.sourceBackup !== "") {
      writer.uint32(82).string(message.sourceBackup);
    }
    if (message.expireTime !== undefined) {
      Timestamp.encode(toTimestamp(message.expireTime), writer.uint32(26).fork()).join();
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(34).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(42).fork()).join();
    }
    if (!message.sizeBytes.equals(Long.ZERO)) {
      writer.uint32(48).int64(message.sizeBytes.toString());
    }
    if (message.state !== 0) {
      writer.uint32(56).int32(message.state);
    }
    if (message.encryptionInfo !== undefined) {
      EncryptionInfo.encode(message.encryptionInfo, writer.uint32(74).fork()).join();
    }
    if (message.backupType !== 0) {
      writer.uint32(88).int32(message.backupType);
    }
    if (message.hotToStandardTime !== undefined) {
      Timestamp.encode(toTimestamp(message.hotToStandardTime), writer.uint32(98).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Backup {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackup();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.sourceTable = reader.string();
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.sourceBackup = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.expireTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.sizeBytes = Long.fromString(reader.int64().toString());
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.encryptionInfo = EncryptionInfo.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.backupType = reader.int32() as any;
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.hotToStandardTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Backup {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      sourceTable: isSet(object.sourceTable) ? globalThis.String(object.sourceTable) : "",
      sourceBackup: isSet(object.sourceBackup) ? globalThis.String(object.sourceBackup) : "",
      expireTime: isSet(object.expireTime) ? fromJsonTimestamp(object.expireTime) : undefined,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      sizeBytes: isSet(object.sizeBytes) ? Long.fromValue(object.sizeBytes) : Long.ZERO,
      state: isSet(object.state) ? backup_StateFromJSON(object.state) : 0,
      encryptionInfo: isSet(object.encryptionInfo) ? EncryptionInfo.fromJSON(object.encryptionInfo) : undefined,
      backupType: isSet(object.backupType) ? backup_BackupTypeFromJSON(object.backupType) : 0,
      hotToStandardTime: isSet(object.hotToStandardTime) ? fromJsonTimestamp(object.hotToStandardTime) : undefined,
    };
  },

  toJSON(message: Backup): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.sourceTable !== "") {
      obj.sourceTable = message.sourceTable;
    }
    if (message.sourceBackup !== "") {
      obj.sourceBackup = message.sourceBackup;
    }
    if (message.expireTime !== undefined) {
      obj.expireTime = message.expireTime.toISOString();
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (!message.sizeBytes.equals(Long.ZERO)) {
      obj.sizeBytes = (message.sizeBytes || Long.ZERO).toString();
    }
    if (message.state !== 0) {
      obj.state = backup_StateToJSON(message.state);
    }
    if (message.encryptionInfo !== undefined) {
      obj.encryptionInfo = EncryptionInfo.toJSON(message.encryptionInfo);
    }
    if (message.backupType !== 0) {
      obj.backupType = backup_BackupTypeToJSON(message.backupType);
    }
    if (message.hotToStandardTime !== undefined) {
      obj.hotToStandardTime = message.hotToStandardTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<Backup>): Backup {
    return Backup.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Backup>): Backup {
    const message = createBaseBackup();
    message.name = object.name ?? "";
    message.sourceTable = object.sourceTable ?? "";
    message.sourceBackup = object.sourceBackup ?? "";
    message.expireTime = object.expireTime ?? undefined;
    message.startTime = object.startTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    message.sizeBytes = (object.sizeBytes !== undefined && object.sizeBytes !== null)
      ? Long.fromValue(object.sizeBytes)
      : Long.ZERO;
    message.state = object.state ?? 0;
    message.encryptionInfo = (object.encryptionInfo !== undefined && object.encryptionInfo !== null)
      ? EncryptionInfo.fromPartial(object.encryptionInfo)
      : undefined;
    message.backupType = object.backupType ?? 0;
    message.hotToStandardTime = object.hotToStandardTime ?? undefined;
    return message;
  },
};

function createBaseBackupInfo(): BackupInfo {
  return { backup: "", startTime: undefined, endTime: undefined, sourceTable: "", sourceBackup: "" };
}

export const BackupInfo: MessageFns<BackupInfo> = {
  encode(message: BackupInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.backup !== "") {
      writer.uint32(10).string(message.backup);
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(18).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(26).fork()).join();
    }
    if (message.sourceTable !== "") {
      writer.uint32(34).string(message.sourceTable);
    }
    if (message.sourceBackup !== "") {
      writer.uint32(82).string(message.sourceBackup);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BackupInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackupInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.backup = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.sourceTable = reader.string();
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.sourceBackup = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BackupInfo {
    return {
      backup: isSet(object.backup) ? globalThis.String(object.backup) : "",
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      sourceTable: isSet(object.sourceTable) ? globalThis.String(object.sourceTable) : "",
      sourceBackup: isSet(object.sourceBackup) ? globalThis.String(object.sourceBackup) : "",
    };
  },

  toJSON(message: BackupInfo): unknown {
    const obj: any = {};
    if (message.backup !== "") {
      obj.backup = message.backup;
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (message.sourceTable !== "") {
      obj.sourceTable = message.sourceTable;
    }
    if (message.sourceBackup !== "") {
      obj.sourceBackup = message.sourceBackup;
    }
    return obj;
  },

  create(base?: DeepPartial<BackupInfo>): BackupInfo {
    return BackupInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BackupInfo>): BackupInfo {
    const message = createBaseBackupInfo();
    message.backup = object.backup ?? "";
    message.startTime = object.startTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    message.sourceTable = object.sourceTable ?? "";
    message.sourceBackup = object.sourceBackup ?? "";
    return message;
  },
};

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
