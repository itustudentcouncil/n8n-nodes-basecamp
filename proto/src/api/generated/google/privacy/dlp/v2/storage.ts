// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/privacy/dlp/v2/storage.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Timestamp } from "../../../protobuf/timestamp.js";

export const protobufPackage = "google.privacy.dlp.v2";

/**
 * Coarse-grained confidence level of how well a particular finding
 * satisfies the criteria to match a particular infoType.
 *
 * Likelihood is calculated based on the number of signals a
 * finding has that implies that the finding matches the infoType. For
 * example, a string that has an '@' and a '.com' is more likely to be a
 * match for an email address than a string that only has an '@'.
 *
 * In general, the highest likelihood level has the strongest signals that
 * indicate a match. That is, a finding with a high likelihood has a low chance
 * of being a false positive.
 *
 * For more information about each likelihood level
 * and how likelihood works, see [Match
 * likelihood](https://cloud.google.com/sensitive-data-protection/docs/likelihood).
 */
export enum Likelihood {
  /** LIKELIHOOD_UNSPECIFIED - Default value; same as POSSIBLE. */
  LIKELIHOOD_UNSPECIFIED = 0,
  /** VERY_UNLIKELY - Highest chance of a false positive. */
  VERY_UNLIKELY = 1,
  /** UNLIKELY - High chance of a false positive. */
  UNLIKELY = 2,
  /** POSSIBLE - Some matching signals. The default value. */
  POSSIBLE = 3,
  /** LIKELY - Low chance of a false positive. */
  LIKELY = 4,
  /** VERY_LIKELY - Confidence level is high. Lowest chance of a false positive. */
  VERY_LIKELY = 5,
  UNRECOGNIZED = -1,
}

export function likelihoodFromJSON(object: any): Likelihood {
  switch (object) {
    case 0:
    case "LIKELIHOOD_UNSPECIFIED":
      return Likelihood.LIKELIHOOD_UNSPECIFIED;
    case 1:
    case "VERY_UNLIKELY":
      return Likelihood.VERY_UNLIKELY;
    case 2:
    case "UNLIKELY":
      return Likelihood.UNLIKELY;
    case 3:
    case "POSSIBLE":
      return Likelihood.POSSIBLE;
    case 4:
    case "LIKELY":
      return Likelihood.LIKELY;
    case 5:
    case "VERY_LIKELY":
      return Likelihood.VERY_LIKELY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Likelihood.UNRECOGNIZED;
  }
}

export function likelihoodToJSON(object: Likelihood): string {
  switch (object) {
    case Likelihood.LIKELIHOOD_UNSPECIFIED:
      return "LIKELIHOOD_UNSPECIFIED";
    case Likelihood.VERY_UNLIKELY:
      return "VERY_UNLIKELY";
    case Likelihood.UNLIKELY:
      return "UNLIKELY";
    case Likelihood.POSSIBLE:
      return "POSSIBLE";
    case Likelihood.LIKELY:
      return "LIKELY";
    case Likelihood.VERY_LIKELY:
      return "VERY_LIKELY";
    case Likelihood.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Definitions of file type groups to scan. New types will be added to this
 * list.
 */
export enum FileType {
  /** FILE_TYPE_UNSPECIFIED - Includes all files. */
  FILE_TYPE_UNSPECIFIED = 0,
  /**
   * BINARY_FILE - Includes all file extensions not covered by another entry. Binary
   * scanning attempts to convert the content of the file to utf_8 to scan
   * the file.
   * If you wish to avoid this fall back, specify one or more of the other
   * file types in your storage scan.
   */
  BINARY_FILE = 1,
  /**
   * TEXT_FILE - Included file extensions:
   *   asc,asp, aspx, brf, c, cc,cfm, cgi, cpp, csv, cxx, c++, cs, css, dart,
   *   dat, dot, eml,, epbub, ged, go, h, hh, hpp, hxx, h++, hs, html, htm,
   *   mkd, markdown, m, ml, mli, perl, pl, plist, pm, php, phtml, pht,
   *   properties, py, pyw, rb, rbw, rs, rss,  rc, scala, sh, sql, swift, tex,
   *   shtml, shtm, xhtml, lhs, ics, ini, java, js, json, jsonl, kix, kml,
   *   ocaml, md, txt, text, tsv, vb, vcard, vcs, wml, xcodeproj, xml, xsl, xsd,
   *   yml, yaml.
   */
  TEXT_FILE = 2,
  /**
   * IMAGE - Included file extensions:
   *   bmp, gif, jpg, jpeg, jpe, png. Setting
   * [bytes_limit_per_file][google.privacy.dlp.v2.CloudStorageOptions.bytes_limit_per_file]
   * or
   * [bytes_limit_per_file_percent][google.privacy.dlp.v2.CloudStorageOptions.bytes_limit_per_file]
   * has no effect on image files. Image inspection is restricted to the
   * `global`, `us`, `asia`, and `europe` regions.
   */
  IMAGE = 3,
  /**
   * WORD - Microsoft Word files larger than 30 MB will be scanned as binary files.
   * Included file extensions:
   *   docx, dotx, docm, dotm. Setting `bytes_limit_per_file` or
   *   `bytes_limit_per_file_percent` has no effect on Word files.
   */
  WORD = 5,
  /**
   * PDF - PDF files larger than 30 MB will be scanned as binary files.
   * Included file extensions:
   *   pdf. Setting `bytes_limit_per_file` or `bytes_limit_per_file_percent`
   * has no effect on PDF files.
   */
  PDF = 6,
  /**
   * AVRO - Included file extensions:
   *   avro
   */
  AVRO = 7,
  /**
   * CSV - Included file extensions:
   *   csv
   */
  CSV = 8,
  /**
   * TSV - Included file extensions:
   *   tsv
   */
  TSV = 9,
  /**
   * POWERPOINT - Microsoft PowerPoint files larger than 30 MB will be scanned as binary
   * files. Included file extensions:
   *   pptx, pptm, potx, potm, pot. Setting `bytes_limit_per_file` or
   *   `bytes_limit_per_file_percent` has no effect on PowerPoint files.
   */
  POWERPOINT = 11,
  /**
   * EXCEL - Microsoft Excel files larger than 30 MB will be scanned as binary files.
   * Included file extensions:
   *   xlsx, xlsm, xltx, xltm. Setting `bytes_limit_per_file` or
   *   `bytes_limit_per_file_percent` has no effect on Excel files.
   */
  EXCEL = 12,
  UNRECOGNIZED = -1,
}

export function fileTypeFromJSON(object: any): FileType {
  switch (object) {
    case 0:
    case "FILE_TYPE_UNSPECIFIED":
      return FileType.FILE_TYPE_UNSPECIFIED;
    case 1:
    case "BINARY_FILE":
      return FileType.BINARY_FILE;
    case 2:
    case "TEXT_FILE":
      return FileType.TEXT_FILE;
    case 3:
    case "IMAGE":
      return FileType.IMAGE;
    case 5:
    case "WORD":
      return FileType.WORD;
    case 6:
    case "PDF":
      return FileType.PDF;
    case 7:
    case "AVRO":
      return FileType.AVRO;
    case 8:
    case "CSV":
      return FileType.CSV;
    case 9:
    case "TSV":
      return FileType.TSV;
    case 11:
    case "POWERPOINT":
      return FileType.POWERPOINT;
    case 12:
    case "EXCEL":
      return FileType.EXCEL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return FileType.UNRECOGNIZED;
  }
}

export function fileTypeToJSON(object: FileType): string {
  switch (object) {
    case FileType.FILE_TYPE_UNSPECIFIED:
      return "FILE_TYPE_UNSPECIFIED";
    case FileType.BINARY_FILE:
      return "BINARY_FILE";
    case FileType.TEXT_FILE:
      return "TEXT_FILE";
    case FileType.IMAGE:
      return "IMAGE";
    case FileType.WORD:
      return "WORD";
    case FileType.PDF:
      return "PDF";
    case FileType.AVRO:
      return "AVRO";
    case FileType.CSV:
      return "CSV";
    case FileType.TSV:
      return "TSV";
    case FileType.POWERPOINT:
      return "POWERPOINT";
    case FileType.EXCEL:
      return "EXCEL";
    case FileType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Type of information detected by the API. */
export interface InfoType {
  /**
   * Name of the information type. Either a name of your choosing when
   * creating a CustomInfoType, or one of the names listed
   * at
   * https://cloud.google.com/sensitive-data-protection/docs/infotypes-reference
   * when specifying a built-in type.  When sending Cloud DLP results to Data
   * Catalog, infoType names should conform to the pattern
   * `[A-Za-z0-9$_-]{1,64}`.
   */
  name: string;
  /** Optional version name for this InfoType. */
  version: string;
  /**
   * Optional custom sensitivity for this InfoType.
   * This only applies to data profiling.
   */
  sensitivityScore: SensitivityScore | undefined;
}

/**
 * Score is calculated from of all elements in the data profile.
 * A higher level means the data is more sensitive.
 */
export interface SensitivityScore {
  /** The sensitivity score applied to the resource. */
  score: SensitivityScore_SensitivityScoreLevel;
}

/** Various sensitivity score levels for resources. */
export enum SensitivityScore_SensitivityScoreLevel {
  /** SENSITIVITY_SCORE_UNSPECIFIED - Unused. */
  SENSITIVITY_SCORE_UNSPECIFIED = 0,
  /**
   * SENSITIVITY_LOW - No sensitive information detected. The resource isn't publicly
   * accessible.
   */
  SENSITIVITY_LOW = 10,
  /** SENSITIVITY_UNKNOWN - Unable to determine sensitivity. */
  SENSITIVITY_UNKNOWN = 12,
  /**
   * SENSITIVITY_MODERATE - Medium risk. Contains personally identifiable information (PII),
   * potentially sensitive data, or fields with free-text data that are at a
   * higher risk of having intermittent sensitive data. Consider limiting
   * access.
   */
  SENSITIVITY_MODERATE = 20,
  /**
   * SENSITIVITY_HIGH - High risk. Sensitive personally identifiable information (SPII) can be
   * present. Exfiltration of data can lead to user data loss.
   * Re-identification of users might be possible. Consider limiting usage and
   * or removing SPII.
   */
  SENSITIVITY_HIGH = 30,
  UNRECOGNIZED = -1,
}

export function sensitivityScore_SensitivityScoreLevelFromJSON(object: any): SensitivityScore_SensitivityScoreLevel {
  switch (object) {
    case 0:
    case "SENSITIVITY_SCORE_UNSPECIFIED":
      return SensitivityScore_SensitivityScoreLevel.SENSITIVITY_SCORE_UNSPECIFIED;
    case 10:
    case "SENSITIVITY_LOW":
      return SensitivityScore_SensitivityScoreLevel.SENSITIVITY_LOW;
    case 12:
    case "SENSITIVITY_UNKNOWN":
      return SensitivityScore_SensitivityScoreLevel.SENSITIVITY_UNKNOWN;
    case 20:
    case "SENSITIVITY_MODERATE":
      return SensitivityScore_SensitivityScoreLevel.SENSITIVITY_MODERATE;
    case 30:
    case "SENSITIVITY_HIGH":
      return SensitivityScore_SensitivityScoreLevel.SENSITIVITY_HIGH;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SensitivityScore_SensitivityScoreLevel.UNRECOGNIZED;
  }
}

export function sensitivityScore_SensitivityScoreLevelToJSON(object: SensitivityScore_SensitivityScoreLevel): string {
  switch (object) {
    case SensitivityScore_SensitivityScoreLevel.SENSITIVITY_SCORE_UNSPECIFIED:
      return "SENSITIVITY_SCORE_UNSPECIFIED";
    case SensitivityScore_SensitivityScoreLevel.SENSITIVITY_LOW:
      return "SENSITIVITY_LOW";
    case SensitivityScore_SensitivityScoreLevel.SENSITIVITY_UNKNOWN:
      return "SENSITIVITY_UNKNOWN";
    case SensitivityScore_SensitivityScoreLevel.SENSITIVITY_MODERATE:
      return "SENSITIVITY_MODERATE";
    case SensitivityScore_SensitivityScoreLevel.SENSITIVITY_HIGH:
      return "SENSITIVITY_HIGH";
    case SensitivityScore_SensitivityScoreLevel.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** A reference to a StoredInfoType to use with scanning. */
export interface StoredType {
  /**
   * Resource name of the requested `StoredInfoType`, for example
   * `organizations/433245324/storedInfoTypes/432452342` or
   * `projects/project-id/storedInfoTypes/432452342`.
   */
  name: string;
  /**
   * Timestamp indicating when the version of the `StoredInfoType` used for
   * inspection was created. Output-only field, populated by the system.
   */
  createTime: Date | undefined;
}

/**
 * Custom information type provided by the user. Used to find domain-specific
 * sensitive information configurable to the data in question.
 */
export interface CustomInfoType {
  /**
   * CustomInfoType can either be a new infoType, or an extension of built-in
   * infoType, when the name matches one of existing infoTypes and that infoType
   * is specified in `InspectContent.info_types` field. Specifying the latter
   * adds findings to the one detected by the system. If built-in info type is
   * not specified in `InspectContent.info_types` list then the name is treated
   * as a custom info type.
   */
  infoType:
    | InfoType
    | undefined;
  /**
   * Likelihood to return for this CustomInfoType. This base value can be
   * altered by a detection rule if the finding meets the criteria specified by
   * the rule. Defaults to `VERY_LIKELY` if not specified.
   */
  likelihood: Likelihood;
  /** A list of phrases to detect as a CustomInfoType. */
  dictionary?:
    | CustomInfoType_Dictionary
    | undefined;
  /** Regular expression based CustomInfoType. */
  regex?:
    | CustomInfoType_Regex
    | undefined;
  /**
   * Message for detecting output from deidentification transformations that
   * support reversing.
   */
  surrogateType?:
    | CustomInfoType_SurrogateType
    | undefined;
  /**
   * Load an existing `StoredInfoType` resource for use in
   * `InspectDataSource`. Not currently supported in `InspectContent`.
   */
  storedType?:
    | StoredType
    | undefined;
  /**
   * Set of detection rules to apply to all findings of this CustomInfoType.
   * Rules are applied in order that they are specified. Not supported for the
   * `surrogate_type` CustomInfoType.
   */
  detectionRules: CustomInfoType_DetectionRule[];
  /**
   * If set to EXCLUSION_TYPE_EXCLUDE this infoType will not cause a finding
   * to be returned. It still can be used for rules matching.
   */
  exclusionType: CustomInfoType_ExclusionType;
  /**
   * Sensitivity for this CustomInfoType. If this CustomInfoType extends an
   * existing InfoType, the sensitivity here will take precedence over that of
   * the original InfoType. If unset for a CustomInfoType, it will default to
   * HIGH.
   * This only applies to data profiling.
   */
  sensitivityScore: SensitivityScore | undefined;
}

/** Type of exclusion rule. */
export enum CustomInfoType_ExclusionType {
  /** EXCLUSION_TYPE_UNSPECIFIED - A finding of this custom info type will not be excluded from results. */
  EXCLUSION_TYPE_UNSPECIFIED = 0,
  /**
   * EXCLUSION_TYPE_EXCLUDE - A finding of this custom info type will be excluded from final results,
   * but can still affect rule execution.
   */
  EXCLUSION_TYPE_EXCLUDE = 1,
  UNRECOGNIZED = -1,
}

export function customInfoType_ExclusionTypeFromJSON(object: any): CustomInfoType_ExclusionType {
  switch (object) {
    case 0:
    case "EXCLUSION_TYPE_UNSPECIFIED":
      return CustomInfoType_ExclusionType.EXCLUSION_TYPE_UNSPECIFIED;
    case 1:
    case "EXCLUSION_TYPE_EXCLUDE":
      return CustomInfoType_ExclusionType.EXCLUSION_TYPE_EXCLUDE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return CustomInfoType_ExclusionType.UNRECOGNIZED;
  }
}

export function customInfoType_ExclusionTypeToJSON(object: CustomInfoType_ExclusionType): string {
  switch (object) {
    case CustomInfoType_ExclusionType.EXCLUSION_TYPE_UNSPECIFIED:
      return "EXCLUSION_TYPE_UNSPECIFIED";
    case CustomInfoType_ExclusionType.EXCLUSION_TYPE_EXCLUDE:
      return "EXCLUSION_TYPE_EXCLUDE";
    case CustomInfoType_ExclusionType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Custom information type based on a dictionary of words or phrases. This can
 * be used to match sensitive information specific to the data, such as a list
 * of employee IDs or job titles.
 *
 * Dictionary words are case-insensitive and all characters other than letters
 * and digits in the unicode [Basic Multilingual
 * Plane](https://en.wikipedia.org/wiki/Plane_%28Unicode%29#Basic_Multilingual_Plane)
 * will be replaced with whitespace when scanning for matches, so the
 * dictionary phrase "Sam Johnson" will match all three phrases "sam johnson",
 * "Sam, Johnson", and "Sam (Johnson)". Additionally, the characters
 * surrounding any match must be of a different type than the adjacent
 * characters within the word, so letters must be next to non-letters and
 * digits next to non-digits. For example, the dictionary word "jen" will
 * match the first three letters of the text "jen123" but will return no
 * matches for "jennifer".
 *
 * Dictionary words containing a large number of characters that are not
 * letters or digits may result in unexpected findings because such characters
 * are treated as whitespace. The
 * [limits](https://cloud.google.com/sensitive-data-protection/limits) page
 * contains details about the size limits of dictionaries. For dictionaries
 * that do not fit within these constraints, consider using
 * `LargeCustomDictionaryConfig` in the `StoredInfoType` API.
 */
export interface CustomInfoType_Dictionary {
  /** List of words or phrases to search for. */
  wordList?:
    | CustomInfoType_Dictionary_WordList
    | undefined;
  /**
   * Newline-delimited file of words in Cloud Storage. Only a single file
   * is accepted.
   */
  cloudStoragePath?: CloudStoragePath | undefined;
}

/** Message defining a list of words or phrases to search for in the data. */
export interface CustomInfoType_Dictionary_WordList {
  /**
   * Words or phrases defining the dictionary. The dictionary must contain
   * at least one phrase and every phrase must contain at least 2 characters
   * that are letters or digits. [required]
   */
  words: string[];
}

/** Message defining a custom regular expression. */
export interface CustomInfoType_Regex {
  /**
   * Pattern defining the regular expression. Its syntax
   * (https://github.com/google/re2/wiki/Syntax) can be found under the
   * google/re2 repository on GitHub.
   */
  pattern: string;
  /**
   * The index of the submatch to extract as findings. When not
   * specified, the entire match is returned. No more than 3 may be included.
   */
  groupIndexes: number[];
}

/**
 * Message for detecting output from deidentification transformations
 * such as
 * [`CryptoReplaceFfxFpeConfig`](https://cloud.google.com/sensitive-data-protection/docs/reference/rest/v2/organizations.deidentifyTemplates#cryptoreplaceffxfpeconfig).
 * These types of transformations are
 * those that perform pseudonymization, thereby producing a "surrogate" as
 * output. This should be used in conjunction with a field on the
 * transformation such as `surrogate_info_type`. This CustomInfoType does
 * not support the use of `detection_rules`.
 */
export interface CustomInfoType_SurrogateType {
}

/**
 * Deprecated; use `InspectionRuleSet` instead. Rule for modifying a
 * `CustomInfoType` to alter behavior under certain circumstances, depending
 * on the specific details of the rule. Not supported for the `surrogate_type`
 * custom infoType.
 */
export interface CustomInfoType_DetectionRule {
  /** Hotword-based detection rule. */
  hotwordRule?: CustomInfoType_DetectionRule_HotwordRule | undefined;
}

/**
 * Message for specifying a window around a finding to apply a detection
 * rule.
 */
export interface CustomInfoType_DetectionRule_Proximity {
  /**
   * Number of characters before the finding to consider. For tabular data,
   * if you want to modify the likelihood of an entire column of findngs,
   * set this to 1. For more information, see
   * [Hotword example: Set the match likelihood of a table column]
   * (https://cloud.google.com/sensitive-data-protection/docs/creating-custom-infotypes-likelihood#match-column-values).
   */
  windowBefore: number;
  /** Number of characters after the finding to consider. */
  windowAfter: number;
}

/**
 * Message for specifying an adjustment to the likelihood of a finding as
 * part of a detection rule.
 */
export interface CustomInfoType_DetectionRule_LikelihoodAdjustment {
  /** Set the likelihood of a finding to a fixed value. */
  fixedLikelihood?:
    | Likelihood
    | undefined;
  /**
   * Increase or decrease the likelihood by the specified number of
   * levels. For example, if a finding would be `POSSIBLE` without the
   * detection rule and `relative_likelihood` is 1, then it is upgraded to
   * `LIKELY`, while a value of -1 would downgrade it to `UNLIKELY`.
   * Likelihood may never drop below `VERY_UNLIKELY` or exceed
   * `VERY_LIKELY`, so applying an adjustment of 1 followed by an
   * adjustment of -1 when base likelihood is `VERY_LIKELY` will result in
   * a final likelihood of `LIKELY`.
   */
  relativeLikelihood?: number | undefined;
}

/**
 * The rule that adjusts the likelihood of findings within a certain
 * proximity of hotwords.
 */
export interface CustomInfoType_DetectionRule_HotwordRule {
  /** Regular expression pattern defining what qualifies as a hotword. */
  hotwordRegex:
    | CustomInfoType_Regex
    | undefined;
  /**
   * Range of characters within which the entire hotword must reside.
   * The total length of the window cannot exceed 1000 characters.
   * The finding itself will be included in the window, so that hotwords can
   * be used to match substrings of the finding itself. Suppose you
   * want Cloud DLP to promote the likelihood of the phone number
   * regex "\(\d{3}\) \d{3}-\d{4}" if the area code is known to be the
   * area code of a company's office. In this case, use the hotword regex
   * "\(xxx\)", where "xxx" is the area code in question.
   *
   * For tabular data, if you want to modify the likelihood of an entire
   * column of findngs, see
   * [Hotword example: Set the match likelihood of a table column]
   * (https://cloud.google.com/sensitive-data-protection/docs/creating-custom-infotypes-likelihood#match-column-values).
   */
  proximity:
    | CustomInfoType_DetectionRule_Proximity
    | undefined;
  /** Likelihood adjustment to apply to all matching findings. */
  likelihoodAdjustment: CustomInfoType_DetectionRule_LikelihoodAdjustment | undefined;
}

/** General identifier of a data field in a storage service. */
export interface FieldId {
  /** Name describing the field. */
  name: string;
}

/**
 * Datastore partition ID.
 * A partition ID identifies a grouping of entities. The grouping is always
 * by project and namespace, however the namespace ID may be empty.
 *
 * A partition ID contains several dimensions:
 * project ID and namespace ID.
 */
export interface PartitionId {
  /** The ID of the project to which the entities belong. */
  projectId: string;
  /** If not empty, the ID of the namespace to which the entities belong. */
  namespaceId: string;
}

/** A representation of a Datastore kind. */
export interface KindExpression {
  /** The name of the kind. */
  name: string;
}

/** Options defining a data set within Google Cloud Datastore. */
export interface DatastoreOptions {
  /**
   * A partition ID identifies a grouping of entities. The grouping is always
   * by project and namespace, however the namespace ID may be empty.
   */
  partitionId:
    | PartitionId
    | undefined;
  /** The kind to process. */
  kind: KindExpression | undefined;
}

/**
 * Message representing a set of files in a Cloud Storage bucket. Regular
 * expressions are used to allow fine-grained control over which files in the
 * bucket to include.
 *
 * Included files are those that match at least one item in `include_regex` and
 * do not match any items in `exclude_regex`. Note that a file that matches
 * items from both lists will _not_ be included. For a match to occur, the
 * entire file path (i.e., everything in the url after the bucket name) must
 * match the regular expression.
 *
 * For example, given the input `{bucket_name: "mybucket", include_regex:
 * ["directory1/.*"], exclude_regex:
 * ["directory1/excluded.*"]}`:
 *
 * * `gs://mybucket/directory1/myfile` will be included
 * * `gs://mybucket/directory1/directory2/myfile` will be included (`.*` matches
 * across `/`)
 * * `gs://mybucket/directory0/directory1/myfile` will _not_ be included (the
 * full path doesn't match any items in `include_regex`)
 * * `gs://mybucket/directory1/excludedfile` will _not_ be included (the path
 * matches an item in `exclude_regex`)
 *
 * If `include_regex` is left empty, it will match all files by default
 * (this is equivalent to setting `include_regex: [".*"]`).
 *
 * Some other common use cases:
 *
 * * `{bucket_name: "mybucket", exclude_regex: [".*\.pdf"]}` will include all
 * files in `mybucket` except for .pdf files
 * * `{bucket_name: "mybucket", include_regex: ["directory/[^/]+"]}` will
 * include all files directly under `gs://mybucket/directory/`, without matching
 * across `/`
 */
export interface CloudStorageRegexFileSet {
  /** The name of a Cloud Storage bucket. Required. */
  bucketName: string;
  /**
   * A list of regular expressions matching file paths to include. All files in
   * the bucket that match at least one of these regular expressions will be
   * included in the set of files, except for those that also match an item in
   * `exclude_regex`. Leaving this field empty will match all files by default
   * (this is equivalent to including `.*` in the list).
   *
   * Regular expressions use RE2
   * [syntax](https://github.com/google/re2/wiki/Syntax); a guide can be found
   * under the google/re2 repository on GitHub.
   */
  includeRegex: string[];
  /**
   * A list of regular expressions matching file paths to exclude. All files in
   * the bucket that match at least one of these regular expressions will be
   * excluded from the scan.
   *
   * Regular expressions use RE2
   * [syntax](https://github.com/google/re2/wiki/Syntax); a guide can be found
   * under the google/re2 repository on GitHub.
   */
  excludeRegex: string[];
}

/**
 * Options defining a file or a set of files within a Cloud Storage
 * bucket.
 */
export interface CloudStorageOptions {
  /** The set of one or more files to scan. */
  fileSet:
    | CloudStorageOptions_FileSet
    | undefined;
  /**
   * Max number of bytes to scan from a file. If a scanned file's size is bigger
   * than this value then the rest of the bytes are omitted. Only one of
   * `bytes_limit_per_file` and `bytes_limit_per_file_percent` can be specified.
   * This field can't be set if de-identification is requested. For certain file
   * types, setting this field has no effect. For more information, see [Limits
   * on bytes scanned per
   * file](https://cloud.google.com/sensitive-data-protection/docs/supported-file-types#max-byte-size-per-file).
   */
  bytesLimitPerFile: Long;
  /**
   * Max percentage of bytes to scan from a file. The rest are omitted. The
   * number of bytes scanned is rounded down. Must be between 0 and 100,
   * inclusively. Both 0 and 100 means no limit. Defaults to 0. Only one of
   * bytes_limit_per_file and bytes_limit_per_file_percent can be specified.
   * This field can't be set if de-identification is requested. For certain file
   * types, setting this field has no effect. For more information, see [Limits
   * on bytes scanned per
   * file](https://cloud.google.com/sensitive-data-protection/docs/supported-file-types#max-byte-size-per-file).
   */
  bytesLimitPerFilePercent: number;
  /**
   * List of file type groups to include in the scan.
   * If empty, all files are scanned and available data format processors
   * are applied. In addition, the binary content of the selected files
   * is always scanned as well.
   * Images are scanned only as binary if the specified region
   * does not support image inspection and no file_types were specified.
   * Image inspection is restricted to 'global', 'us', 'asia', and 'europe'.
   */
  fileTypes: FileType[];
  /** How to sample the data. */
  sampleMethod: CloudStorageOptions_SampleMethod;
  /**
   * Limits the number of files to scan to this percentage of the input FileSet.
   * Number of files scanned is rounded down. Must be between 0 and 100,
   * inclusively. Both 0 and 100 means no limit. Defaults to 0.
   */
  filesLimitPercent: number;
}

/**
 * How to sample bytes if not all bytes are scanned. Meaningful only when used
 * in conjunction with bytes_limit_per_file. If not specified, scanning would
 * start from the top.
 */
export enum CloudStorageOptions_SampleMethod {
  /** SAMPLE_METHOD_UNSPECIFIED - No sampling. */
  SAMPLE_METHOD_UNSPECIFIED = 0,
  /** TOP - Scan from the top (default). */
  TOP = 1,
  /**
   * RANDOM_START - For each file larger than bytes_limit_per_file, randomly pick the offset
   * to start scanning. The scanned bytes are contiguous.
   */
  RANDOM_START = 2,
  UNRECOGNIZED = -1,
}

export function cloudStorageOptions_SampleMethodFromJSON(object: any): CloudStorageOptions_SampleMethod {
  switch (object) {
    case 0:
    case "SAMPLE_METHOD_UNSPECIFIED":
      return CloudStorageOptions_SampleMethod.SAMPLE_METHOD_UNSPECIFIED;
    case 1:
    case "TOP":
      return CloudStorageOptions_SampleMethod.TOP;
    case 2:
    case "RANDOM_START":
      return CloudStorageOptions_SampleMethod.RANDOM_START;
    case -1:
    case "UNRECOGNIZED":
    default:
      return CloudStorageOptions_SampleMethod.UNRECOGNIZED;
  }
}

export function cloudStorageOptions_SampleMethodToJSON(object: CloudStorageOptions_SampleMethod): string {
  switch (object) {
    case CloudStorageOptions_SampleMethod.SAMPLE_METHOD_UNSPECIFIED:
      return "SAMPLE_METHOD_UNSPECIFIED";
    case CloudStorageOptions_SampleMethod.TOP:
      return "TOP";
    case CloudStorageOptions_SampleMethod.RANDOM_START:
      return "RANDOM_START";
    case CloudStorageOptions_SampleMethod.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Set of files to scan. */
export interface CloudStorageOptions_FileSet {
  /**
   * The Cloud Storage url of the file(s) to scan, in the format
   * `gs://<bucket>/<path>`. Trailing wildcard in the path is allowed.
   *
   * If the url ends in a trailing slash, the bucket or directory represented
   * by the url will be scanned non-recursively (content in sub-directories
   * will not be scanned). This means that `gs://mybucket/` is equivalent to
   * `gs://mybucket/*`, and `gs://mybucket/directory/` is equivalent to
   * `gs://mybucket/directory/*`.
   *
   * Exactly one of `url` or `regex_file_set` must be set.
   */
  url: string;
  /**
   * The regex-filtered set of files to scan. Exactly one of `url` or
   * `regex_file_set` must be set.
   */
  regexFileSet: CloudStorageRegexFileSet | undefined;
}

/** Message representing a set of files in Cloud Storage. */
export interface CloudStorageFileSet {
  /**
   * The url, in the format `gs://<bucket>/<path>`. Trailing wildcard in the
   * path is allowed.
   */
  url: string;
}

/** Message representing a single file or path in Cloud Storage. */
export interface CloudStoragePath {
  /**
   * A URL representing a file or path (no wildcards) in Cloud Storage.
   * Example: `gs://[BUCKET_NAME]/dictionary.txt`
   */
  path: string;
}

/** Options defining BigQuery table and row identifiers. */
export interface BigQueryOptions {
  /** Complete BigQuery table reference. */
  tableReference:
    | BigQueryTable
    | undefined;
  /**
   * Table fields that may uniquely identify a row within the table. When
   * `actions.saveFindings.outputConfig.table` is specified, the values of
   * columns specified here are available in the output table under
   * `location.content_locations.record_location.record_key.id_values`. Nested
   * fields such as `person.birthdate.year` are allowed.
   */
  identifyingFields: FieldId[];
  /**
   * Max number of rows to scan. If the table has more rows than this value, the
   * rest of the rows are omitted. If not set, or if set to 0, all rows will be
   * scanned. Only one of rows_limit and rows_limit_percent can be specified.
   * Cannot be used in conjunction with TimespanConfig.
   */
  rowsLimit: Long;
  /**
   * Max percentage of rows to scan. The rest are omitted. The number of rows
   * scanned is rounded down. Must be between 0 and 100, inclusively. Both 0 and
   * 100 means no limit. Defaults to 0. Only one of rows_limit and
   * rows_limit_percent can be specified. Cannot be used in conjunction with
   * TimespanConfig.
   *
   * Caution: A [known
   * issue](https://cloud.google.com/sensitive-data-protection/docs/known-issues#bq-sampling)
   * is causing the `rowsLimitPercent` field to behave unexpectedly. We
   * recommend using `rowsLimit` instead.
   */
  rowsLimitPercent: number;
  /** How to sample the data. */
  sampleMethod: BigQueryOptions_SampleMethod;
  /**
   * References to fields excluded from scanning. This allows you to skip
   * inspection of entire columns which you know have no findings.
   * When inspecting a table, we recommend that you inspect all columns.
   * Otherwise, findings might be affected because hints from excluded columns
   * will not be used.
   */
  excludedFields: FieldId[];
  /**
   * Limit scanning only to these fields.
   * When inspecting a table, we recommend that you inspect all columns.
   * Otherwise, findings might be affected because hints from excluded columns
   * will not be used.
   */
  includedFields: FieldId[];
}

/**
 * How to sample rows if not all rows are scanned. Meaningful only when used
 * in conjunction with either rows_limit or rows_limit_percent. If not
 * specified, rows are scanned in the order BigQuery reads them.
 */
export enum BigQueryOptions_SampleMethod {
  /** SAMPLE_METHOD_UNSPECIFIED - No sampling. */
  SAMPLE_METHOD_UNSPECIFIED = 0,
  /**
   * TOP - Scan groups of rows in the order BigQuery provides (default). Multiple
   * groups of rows may be scanned in parallel, so results may not appear in
   * the same order the rows are read.
   */
  TOP = 1,
  /** RANDOM_START - Randomly pick groups of rows to scan. */
  RANDOM_START = 2,
  UNRECOGNIZED = -1,
}

export function bigQueryOptions_SampleMethodFromJSON(object: any): BigQueryOptions_SampleMethod {
  switch (object) {
    case 0:
    case "SAMPLE_METHOD_UNSPECIFIED":
      return BigQueryOptions_SampleMethod.SAMPLE_METHOD_UNSPECIFIED;
    case 1:
    case "TOP":
      return BigQueryOptions_SampleMethod.TOP;
    case 2:
    case "RANDOM_START":
      return BigQueryOptions_SampleMethod.RANDOM_START;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BigQueryOptions_SampleMethod.UNRECOGNIZED;
  }
}

export function bigQueryOptions_SampleMethodToJSON(object: BigQueryOptions_SampleMethod): string {
  switch (object) {
    case BigQueryOptions_SampleMethod.SAMPLE_METHOD_UNSPECIFIED:
      return "SAMPLE_METHOD_UNSPECIFIED";
    case BigQueryOptions_SampleMethod.TOP:
      return "TOP";
    case BigQueryOptions_SampleMethod.RANDOM_START:
      return "RANDOM_START";
    case BigQueryOptions_SampleMethod.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Shared message indicating Cloud storage type. */
export interface StorageConfig {
  /** Google Cloud Datastore options. */
  datastoreOptions?:
    | DatastoreOptions
    | undefined;
  /** Cloud Storage options. */
  cloudStorageOptions?:
    | CloudStorageOptions
    | undefined;
  /** BigQuery options. */
  bigQueryOptions?:
    | BigQueryOptions
    | undefined;
  /** Hybrid inspection options. */
  hybridOptions?:
    | HybridOptions
    | undefined;
  /** Configuration of the timespan of the items to include in scanning. */
  timespanConfig: StorageConfig_TimespanConfig | undefined;
}

/**
 * Configuration of the timespan of the items to include in scanning.
 * Currently only supported when inspecting Cloud Storage and BigQuery.
 */
export interface StorageConfig_TimespanConfig {
  /**
   * Exclude files, tables, or rows older than this value.
   * If not set, no lower time limit is applied.
   */
  startTime:
    | Date
    | undefined;
  /**
   * Exclude files, tables, or rows newer than this value.
   * If not set, no upper time limit is applied.
   */
  endTime:
    | Date
    | undefined;
  /**
   * Specification of the field containing the timestamp of scanned items.
   * Used for data sources like Datastore and BigQuery.
   *
   * **For BigQuery**
   *
   * If this value is not specified and the table was modified between the
   * given start and end times, the entire table will be scanned. If this
   * value is specified, then rows are filtered based on the given start and
   * end times. Rows with a `NULL` value in the provided BigQuery column are
   * skipped.
   * Valid data types of the provided BigQuery column are: `INTEGER`, `DATE`,
   * `TIMESTAMP`, and `DATETIME`.
   *
   * If your BigQuery table is [partitioned at ingestion
   * time](https://cloud.google.com/bigquery/docs/partitioned-tables#ingestion_time),
   * you can use any of the following pseudo-columns as your timestamp field.
   * When used with Cloud DLP, these pseudo-column names are case sensitive.
   *
   * - `_PARTITIONTIME`
   * - `_PARTITIONDATE`
   * - `_PARTITION_LOAD_TIME`
   *
   * **For Datastore**
   *
   * If this value is specified, then entities are filtered based on the given
   * start and end times. If an entity does not contain the provided timestamp
   * property or contains empty or invalid values, then it is included.
   * Valid data types of the provided timestamp property are: `TIMESTAMP`.
   *
   * See the
   * [known
   * issue](https://cloud.google.com/sensitive-data-protection/docs/known-issues#bq-timespan)
   * related to this operation.
   */
  timestampField:
    | FieldId
    | undefined;
  /**
   * When the job is started by a JobTrigger we will automatically figure out
   * a valid start_time to avoid scanning files that have not been modified
   * since the last time the JobTrigger executed. This will be based on the
   * time of the execution of the last run of the JobTrigger or the timespan
   * end_time used in the last run of the JobTrigger.
   *
   * **For BigQuery**
   *
   * Inspect jobs triggered by automatic population will scan data that is at
   * least three hours old when the job starts. This is because streaming
   * buffer rows are not read during inspection and reading up to the current
   * timestamp will result in skipped rows.
   *
   * See the [known
   * issue](https://cloud.google.com/sensitive-data-protection/docs/known-issues#recently-streamed-data)
   * related to this operation.
   */
  enableAutoPopulationOfTimespanConfig: boolean;
}

/**
 * Configuration to control jobs where the content being inspected is outside
 * of Google Cloud Platform.
 */
export interface HybridOptions {
  /**
   * A short description of where the data is coming from. Will be stored once
   * in the job. 256 max length.
   */
  description: string;
  /**
   * These are labels that each inspection request must include within their
   * 'finding_labels' map. Request may contain others, but any missing one of
   * these will be rejected.
   *
   * Label keys must be between 1 and 63 characters long and must conform
   * to the following regular expression: `[a-z]([-a-z0-9]*[a-z0-9])?`.
   *
   * No more than 10 keys can be required.
   */
  requiredFindingLabelKeys: string[];
  /**
   * To organize findings, these labels will be added to each finding.
   *
   * Label keys must be between 1 and 63 characters long and must conform
   * to the following regular expression: `[a-z]([-a-z0-9]*[a-z0-9])?`.
   *
   * Label values must be between 0 and 63 characters long and must conform
   * to the regular expression `([a-z]([-a-z0-9]*[a-z0-9])?)?`.
   *
   * No more than 10 labels can be associated with a given finding.
   *
   * Examples:
   *
   * * `"environment" : "production"`
   * * `"pipeline" : "etl"`
   */
  labels: { [key: string]: string };
  /**
   * If the container is a table, additional information to make findings
   * meaningful such as the columns that are primary keys.
   */
  tableOptions: TableOptions | undefined;
}

export interface HybridOptions_LabelsEntry {
  key: string;
  value: string;
}

/** Row key for identifying a record in BigQuery table. */
export interface BigQueryKey {
  /** Complete BigQuery table reference. */
  tableReference:
    | BigQueryTable
    | undefined;
  /**
   * Row number inferred at the time the table was scanned. This value is
   * nondeterministic, cannot be queried, and may be null for inspection
   * jobs. To locate findings within a table, specify
   * `inspect_job.storage_config.big_query_options.identifying_fields` in
   * `CreateDlpJobRequest`.
   */
  rowNumber: Long;
}

/** Record key for a finding in Cloud Datastore. */
export interface DatastoreKey {
  /** Datastore entity key. */
  entityKey: Key | undefined;
}

/**
 * A unique identifier for a Datastore entity.
 * If a key's partition ID or any of its path kinds or names are
 * reserved/read-only, the key is reserved/read-only.
 * A reserved/read-only key is forbidden in certain documented contexts.
 */
export interface Key {
  /**
   * Entities are partitioned into subsets, currently identified by a project
   * ID and namespace ID.
   * Queries are scoped to a single partition.
   */
  partitionId:
    | PartitionId
    | undefined;
  /**
   * The entity path.
   * An entity path consists of one or more elements composed of a kind and a
   * string or numerical identifier, which identify entities. The first
   * element identifies a _root entity_, the second element identifies
   * a _child_ of the root entity, the third element identifies a child of the
   * second entity, and so forth. The entities identified by all prefixes of
   * the path are called the element's _ancestors_.
   *
   * A path can never be empty, and a path can have at most 100 elements.
   */
  path: Key_PathElement[];
}

/**
 * A (kind, ID/name) pair used to construct a key path.
 *
 * If either name or ID is set, the element is complete.
 * If neither is set, the element is incomplete.
 */
export interface Key_PathElement {
  /**
   * The kind of the entity.
   * A kind matching regex `__.*__` is reserved/read-only.
   * A kind must not contain more than 1500 bytes when UTF-8 encoded.
   * Cannot be `""`.
   */
  kind: string;
  /**
   * The auto-allocated ID of the entity.
   * Never equal to zero. Values less than zero are discouraged and may not
   * be supported in the future.
   */
  id?:
    | Long
    | undefined;
  /**
   * The name of the entity.
   * A name matching regex `__.*__` is reserved/read-only.
   * A name must not be more than 1500 bytes when UTF-8 encoded.
   * Cannot be `""`.
   */
  name?: string | undefined;
}

/** Message for a unique key indicating a record that contains a finding. */
export interface RecordKey {
  /** BigQuery key */
  datastoreKey?:
    | DatastoreKey
    | undefined;
  /** Datastore key */
  bigQueryKey?:
    | BigQueryKey
    | undefined;
  /**
   * Values of identifying columns in the given row. Order of values matches
   * the order of `identifying_fields` specified in the scanning request.
   */
  idValues: string[];
}

/**
 * Message defining the location of a BigQuery table. A table is uniquely
 * identified  by its project_id, dataset_id, and table_name. Within a query
 * a table is often referenced with a string in the format of:
 * `<project_id>:<dataset_id>.<table_id>` or
 * `<project_id>.<dataset_id>.<table_id>`.
 */
export interface BigQueryTable {
  /**
   * The Google Cloud Platform project ID of the project containing the table.
   * If omitted, project ID is inferred from the API call.
   */
  projectId: string;
  /** Dataset ID of the table. */
  datasetId: string;
  /** Name of the table. */
  tableId: string;
}

/**
 * Message defining the location of a BigQuery table with the projectId inferred
 * from the parent project.
 */
export interface TableReference {
  /** Dataset ID of the table. */
  datasetId: string;
  /** Name of the table. */
  tableId: string;
}

/** Message defining a field of a BigQuery table. */
export interface BigQueryField {
  /** Source table of the field. */
  table:
    | BigQueryTable
    | undefined;
  /** Designated field in the BigQuery table. */
  field: FieldId | undefined;
}

/**
 * An entity in a dataset is a field or set of fields that correspond to a
 * single person. For example, in medical records the `EntityId` might be a
 * patient identifier, or for financial records it might be an account
 * identifier. This message is used when generalizations or analysis must take
 * into account that multiple rows correspond to the same entity.
 */
export interface EntityId {
  /** Composite key indicating which field contains the entity identifier. */
  field: FieldId | undefined;
}

/** Instructions regarding the table content being inspected. */
export interface TableOptions {
  /**
   * The columns that are the primary keys for table objects included in
   * ContentItem. A copy of this cell's value will stored alongside alongside
   * each finding so that the finding can be traced to the specific row it came
   * from. No more than 3 may be provided.
   */
  identifyingFields: FieldId[];
}

function createBaseInfoType(): InfoType {
  return { name: "", version: "", sensitivityScore: undefined };
}

export const InfoType: MessageFns<InfoType> = {
  encode(message: InfoType, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.version !== "") {
      writer.uint32(18).string(message.version);
    }
    if (message.sensitivityScore !== undefined) {
      SensitivityScore.encode(message.sensitivityScore, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InfoType {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInfoType();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.version = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.sensitivityScore = SensitivityScore.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InfoType {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      version: isSet(object.version) ? globalThis.String(object.version) : "",
      sensitivityScore: isSet(object.sensitivityScore) ? SensitivityScore.fromJSON(object.sensitivityScore) : undefined,
    };
  },

  toJSON(message: InfoType): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.version !== "") {
      obj.version = message.version;
    }
    if (message.sensitivityScore !== undefined) {
      obj.sensitivityScore = SensitivityScore.toJSON(message.sensitivityScore);
    }
    return obj;
  },

  create(base?: DeepPartial<InfoType>): InfoType {
    return InfoType.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InfoType>): InfoType {
    const message = createBaseInfoType();
    message.name = object.name ?? "";
    message.version = object.version ?? "";
    message.sensitivityScore = (object.sensitivityScore !== undefined && object.sensitivityScore !== null)
      ? SensitivityScore.fromPartial(object.sensitivityScore)
      : undefined;
    return message;
  },
};

function createBaseSensitivityScore(): SensitivityScore {
  return { score: 0 };
}

export const SensitivityScore: MessageFns<SensitivityScore> = {
  encode(message: SensitivityScore, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.score !== 0) {
      writer.uint32(8).int32(message.score);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SensitivityScore {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSensitivityScore();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.score = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SensitivityScore {
    return { score: isSet(object.score) ? sensitivityScore_SensitivityScoreLevelFromJSON(object.score) : 0 };
  },

  toJSON(message: SensitivityScore): unknown {
    const obj: any = {};
    if (message.score !== 0) {
      obj.score = sensitivityScore_SensitivityScoreLevelToJSON(message.score);
    }
    return obj;
  },

  create(base?: DeepPartial<SensitivityScore>): SensitivityScore {
    return SensitivityScore.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SensitivityScore>): SensitivityScore {
    const message = createBaseSensitivityScore();
    message.score = object.score ?? 0;
    return message;
  },
};

function createBaseStoredType(): StoredType {
  return { name: "", createTime: undefined };
}

export const StoredType: MessageFns<StoredType> = {
  encode(message: StoredType, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StoredType {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStoredType();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StoredType {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
    };
  },

  toJSON(message: StoredType): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<StoredType>): StoredType {
    return StoredType.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StoredType>): StoredType {
    const message = createBaseStoredType();
    message.name = object.name ?? "";
    message.createTime = object.createTime ?? undefined;
    return message;
  },
};

function createBaseCustomInfoType(): CustomInfoType {
  return {
    infoType: undefined,
    likelihood: 0,
    dictionary: undefined,
    regex: undefined,
    surrogateType: undefined,
    storedType: undefined,
    detectionRules: [],
    exclusionType: 0,
    sensitivityScore: undefined,
  };
}

export const CustomInfoType: MessageFns<CustomInfoType> = {
  encode(message: CustomInfoType, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.infoType !== undefined) {
      InfoType.encode(message.infoType, writer.uint32(10).fork()).join();
    }
    if (message.likelihood !== 0) {
      writer.uint32(48).int32(message.likelihood);
    }
    if (message.dictionary !== undefined) {
      CustomInfoType_Dictionary.encode(message.dictionary, writer.uint32(18).fork()).join();
    }
    if (message.regex !== undefined) {
      CustomInfoType_Regex.encode(message.regex, writer.uint32(26).fork()).join();
    }
    if (message.surrogateType !== undefined) {
      CustomInfoType_SurrogateType.encode(message.surrogateType, writer.uint32(34).fork()).join();
    }
    if (message.storedType !== undefined) {
      StoredType.encode(message.storedType, writer.uint32(42).fork()).join();
    }
    for (const v of message.detectionRules) {
      CustomInfoType_DetectionRule.encode(v!, writer.uint32(58).fork()).join();
    }
    if (message.exclusionType !== 0) {
      writer.uint32(64).int32(message.exclusionType);
    }
    if (message.sensitivityScore !== undefined) {
      SensitivityScore.encode(message.sensitivityScore, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CustomInfoType {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCustomInfoType();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.infoType = InfoType.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.likelihood = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.dictionary = CustomInfoType_Dictionary.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.regex = CustomInfoType_Regex.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.surrogateType = CustomInfoType_SurrogateType.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.storedType = StoredType.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.detectionRules.push(CustomInfoType_DetectionRule.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.exclusionType = reader.int32() as any;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.sensitivityScore = SensitivityScore.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CustomInfoType {
    return {
      infoType: isSet(object.infoType) ? InfoType.fromJSON(object.infoType) : undefined,
      likelihood: isSet(object.likelihood) ? likelihoodFromJSON(object.likelihood) : 0,
      dictionary: isSet(object.dictionary) ? CustomInfoType_Dictionary.fromJSON(object.dictionary) : undefined,
      regex: isSet(object.regex) ? CustomInfoType_Regex.fromJSON(object.regex) : undefined,
      surrogateType: isSet(object.surrogateType)
        ? CustomInfoType_SurrogateType.fromJSON(object.surrogateType)
        : undefined,
      storedType: isSet(object.storedType) ? StoredType.fromJSON(object.storedType) : undefined,
      detectionRules: globalThis.Array.isArray(object?.detectionRules)
        ? object.detectionRules.map((e: any) => CustomInfoType_DetectionRule.fromJSON(e))
        : [],
      exclusionType: isSet(object.exclusionType) ? customInfoType_ExclusionTypeFromJSON(object.exclusionType) : 0,
      sensitivityScore: isSet(object.sensitivityScore) ? SensitivityScore.fromJSON(object.sensitivityScore) : undefined,
    };
  },

  toJSON(message: CustomInfoType): unknown {
    const obj: any = {};
    if (message.infoType !== undefined) {
      obj.infoType = InfoType.toJSON(message.infoType);
    }
    if (message.likelihood !== 0) {
      obj.likelihood = likelihoodToJSON(message.likelihood);
    }
    if (message.dictionary !== undefined) {
      obj.dictionary = CustomInfoType_Dictionary.toJSON(message.dictionary);
    }
    if (message.regex !== undefined) {
      obj.regex = CustomInfoType_Regex.toJSON(message.regex);
    }
    if (message.surrogateType !== undefined) {
      obj.surrogateType = CustomInfoType_SurrogateType.toJSON(message.surrogateType);
    }
    if (message.storedType !== undefined) {
      obj.storedType = StoredType.toJSON(message.storedType);
    }
    if (message.detectionRules?.length) {
      obj.detectionRules = message.detectionRules.map((e) => CustomInfoType_DetectionRule.toJSON(e));
    }
    if (message.exclusionType !== 0) {
      obj.exclusionType = customInfoType_ExclusionTypeToJSON(message.exclusionType);
    }
    if (message.sensitivityScore !== undefined) {
      obj.sensitivityScore = SensitivityScore.toJSON(message.sensitivityScore);
    }
    return obj;
  },

  create(base?: DeepPartial<CustomInfoType>): CustomInfoType {
    return CustomInfoType.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CustomInfoType>): CustomInfoType {
    const message = createBaseCustomInfoType();
    message.infoType = (object.infoType !== undefined && object.infoType !== null)
      ? InfoType.fromPartial(object.infoType)
      : undefined;
    message.likelihood = object.likelihood ?? 0;
    message.dictionary = (object.dictionary !== undefined && object.dictionary !== null)
      ? CustomInfoType_Dictionary.fromPartial(object.dictionary)
      : undefined;
    message.regex = (object.regex !== undefined && object.regex !== null)
      ? CustomInfoType_Regex.fromPartial(object.regex)
      : undefined;
    message.surrogateType = (object.surrogateType !== undefined && object.surrogateType !== null)
      ? CustomInfoType_SurrogateType.fromPartial(object.surrogateType)
      : undefined;
    message.storedType = (object.storedType !== undefined && object.storedType !== null)
      ? StoredType.fromPartial(object.storedType)
      : undefined;
    message.detectionRules = object.detectionRules?.map((e) => CustomInfoType_DetectionRule.fromPartial(e)) || [];
    message.exclusionType = object.exclusionType ?? 0;
    message.sensitivityScore = (object.sensitivityScore !== undefined && object.sensitivityScore !== null)
      ? SensitivityScore.fromPartial(object.sensitivityScore)
      : undefined;
    return message;
  },
};

function createBaseCustomInfoType_Dictionary(): CustomInfoType_Dictionary {
  return { wordList: undefined, cloudStoragePath: undefined };
}

export const CustomInfoType_Dictionary: MessageFns<CustomInfoType_Dictionary> = {
  encode(message: CustomInfoType_Dictionary, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.wordList !== undefined) {
      CustomInfoType_Dictionary_WordList.encode(message.wordList, writer.uint32(10).fork()).join();
    }
    if (message.cloudStoragePath !== undefined) {
      CloudStoragePath.encode(message.cloudStoragePath, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CustomInfoType_Dictionary {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCustomInfoType_Dictionary();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.wordList = CustomInfoType_Dictionary_WordList.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.cloudStoragePath = CloudStoragePath.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CustomInfoType_Dictionary {
    return {
      wordList: isSet(object.wordList) ? CustomInfoType_Dictionary_WordList.fromJSON(object.wordList) : undefined,
      cloudStoragePath: isSet(object.cloudStoragePath) ? CloudStoragePath.fromJSON(object.cloudStoragePath) : undefined,
    };
  },

  toJSON(message: CustomInfoType_Dictionary): unknown {
    const obj: any = {};
    if (message.wordList !== undefined) {
      obj.wordList = CustomInfoType_Dictionary_WordList.toJSON(message.wordList);
    }
    if (message.cloudStoragePath !== undefined) {
      obj.cloudStoragePath = CloudStoragePath.toJSON(message.cloudStoragePath);
    }
    return obj;
  },

  create(base?: DeepPartial<CustomInfoType_Dictionary>): CustomInfoType_Dictionary {
    return CustomInfoType_Dictionary.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CustomInfoType_Dictionary>): CustomInfoType_Dictionary {
    const message = createBaseCustomInfoType_Dictionary();
    message.wordList = (object.wordList !== undefined && object.wordList !== null)
      ? CustomInfoType_Dictionary_WordList.fromPartial(object.wordList)
      : undefined;
    message.cloudStoragePath = (object.cloudStoragePath !== undefined && object.cloudStoragePath !== null)
      ? CloudStoragePath.fromPartial(object.cloudStoragePath)
      : undefined;
    return message;
  },
};

function createBaseCustomInfoType_Dictionary_WordList(): CustomInfoType_Dictionary_WordList {
  return { words: [] };
}

export const CustomInfoType_Dictionary_WordList: MessageFns<CustomInfoType_Dictionary_WordList> = {
  encode(message: CustomInfoType_Dictionary_WordList, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.words) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CustomInfoType_Dictionary_WordList {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCustomInfoType_Dictionary_WordList();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.words.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CustomInfoType_Dictionary_WordList {
    return { words: globalThis.Array.isArray(object?.words) ? object.words.map((e: any) => globalThis.String(e)) : [] };
  },

  toJSON(message: CustomInfoType_Dictionary_WordList): unknown {
    const obj: any = {};
    if (message.words?.length) {
      obj.words = message.words;
    }
    return obj;
  },

  create(base?: DeepPartial<CustomInfoType_Dictionary_WordList>): CustomInfoType_Dictionary_WordList {
    return CustomInfoType_Dictionary_WordList.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CustomInfoType_Dictionary_WordList>): CustomInfoType_Dictionary_WordList {
    const message = createBaseCustomInfoType_Dictionary_WordList();
    message.words = object.words?.map((e) => e) || [];
    return message;
  },
};

function createBaseCustomInfoType_Regex(): CustomInfoType_Regex {
  return { pattern: "", groupIndexes: [] };
}

export const CustomInfoType_Regex: MessageFns<CustomInfoType_Regex> = {
  encode(message: CustomInfoType_Regex, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.pattern !== "") {
      writer.uint32(10).string(message.pattern);
    }
    writer.uint32(18).fork();
    for (const v of message.groupIndexes) {
      writer.int32(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CustomInfoType_Regex {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCustomInfoType_Regex();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.pattern = reader.string();
          continue;
        case 2:
          if (tag === 16) {
            message.groupIndexes.push(reader.int32());

            continue;
          }

          if (tag === 18) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.groupIndexes.push(reader.int32());
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CustomInfoType_Regex {
    return {
      pattern: isSet(object.pattern) ? globalThis.String(object.pattern) : "",
      groupIndexes: globalThis.Array.isArray(object?.groupIndexes)
        ? object.groupIndexes.map((e: any) => globalThis.Number(e))
        : [],
    };
  },

  toJSON(message: CustomInfoType_Regex): unknown {
    const obj: any = {};
    if (message.pattern !== "") {
      obj.pattern = message.pattern;
    }
    if (message.groupIndexes?.length) {
      obj.groupIndexes = message.groupIndexes.map((e) => Math.round(e));
    }
    return obj;
  },

  create(base?: DeepPartial<CustomInfoType_Regex>): CustomInfoType_Regex {
    return CustomInfoType_Regex.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CustomInfoType_Regex>): CustomInfoType_Regex {
    const message = createBaseCustomInfoType_Regex();
    message.pattern = object.pattern ?? "";
    message.groupIndexes = object.groupIndexes?.map((e) => e) || [];
    return message;
  },
};

function createBaseCustomInfoType_SurrogateType(): CustomInfoType_SurrogateType {
  return {};
}

export const CustomInfoType_SurrogateType: MessageFns<CustomInfoType_SurrogateType> = {
  encode(_: CustomInfoType_SurrogateType, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CustomInfoType_SurrogateType {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCustomInfoType_SurrogateType();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): CustomInfoType_SurrogateType {
    return {};
  },

  toJSON(_: CustomInfoType_SurrogateType): unknown {
    const obj: any = {};
    return obj;
  },

  create(base?: DeepPartial<CustomInfoType_SurrogateType>): CustomInfoType_SurrogateType {
    return CustomInfoType_SurrogateType.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<CustomInfoType_SurrogateType>): CustomInfoType_SurrogateType {
    const message = createBaseCustomInfoType_SurrogateType();
    return message;
  },
};

function createBaseCustomInfoType_DetectionRule(): CustomInfoType_DetectionRule {
  return { hotwordRule: undefined };
}

export const CustomInfoType_DetectionRule: MessageFns<CustomInfoType_DetectionRule> = {
  encode(message: CustomInfoType_DetectionRule, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.hotwordRule !== undefined) {
      CustomInfoType_DetectionRule_HotwordRule.encode(message.hotwordRule, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CustomInfoType_DetectionRule {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCustomInfoType_DetectionRule();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.hotwordRule = CustomInfoType_DetectionRule_HotwordRule.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CustomInfoType_DetectionRule {
    return {
      hotwordRule: isSet(object.hotwordRule)
        ? CustomInfoType_DetectionRule_HotwordRule.fromJSON(object.hotwordRule)
        : undefined,
    };
  },

  toJSON(message: CustomInfoType_DetectionRule): unknown {
    const obj: any = {};
    if (message.hotwordRule !== undefined) {
      obj.hotwordRule = CustomInfoType_DetectionRule_HotwordRule.toJSON(message.hotwordRule);
    }
    return obj;
  },

  create(base?: DeepPartial<CustomInfoType_DetectionRule>): CustomInfoType_DetectionRule {
    return CustomInfoType_DetectionRule.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CustomInfoType_DetectionRule>): CustomInfoType_DetectionRule {
    const message = createBaseCustomInfoType_DetectionRule();
    message.hotwordRule = (object.hotwordRule !== undefined && object.hotwordRule !== null)
      ? CustomInfoType_DetectionRule_HotwordRule.fromPartial(object.hotwordRule)
      : undefined;
    return message;
  },
};

function createBaseCustomInfoType_DetectionRule_Proximity(): CustomInfoType_DetectionRule_Proximity {
  return { windowBefore: 0, windowAfter: 0 };
}

export const CustomInfoType_DetectionRule_Proximity: MessageFns<CustomInfoType_DetectionRule_Proximity> = {
  encode(message: CustomInfoType_DetectionRule_Proximity, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.windowBefore !== 0) {
      writer.uint32(8).int32(message.windowBefore);
    }
    if (message.windowAfter !== 0) {
      writer.uint32(16).int32(message.windowAfter);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CustomInfoType_DetectionRule_Proximity {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCustomInfoType_DetectionRule_Proximity();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.windowBefore = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.windowAfter = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CustomInfoType_DetectionRule_Proximity {
    return {
      windowBefore: isSet(object.windowBefore) ? globalThis.Number(object.windowBefore) : 0,
      windowAfter: isSet(object.windowAfter) ? globalThis.Number(object.windowAfter) : 0,
    };
  },

  toJSON(message: CustomInfoType_DetectionRule_Proximity): unknown {
    const obj: any = {};
    if (message.windowBefore !== 0) {
      obj.windowBefore = Math.round(message.windowBefore);
    }
    if (message.windowAfter !== 0) {
      obj.windowAfter = Math.round(message.windowAfter);
    }
    return obj;
  },

  create(base?: DeepPartial<CustomInfoType_DetectionRule_Proximity>): CustomInfoType_DetectionRule_Proximity {
    return CustomInfoType_DetectionRule_Proximity.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CustomInfoType_DetectionRule_Proximity>): CustomInfoType_DetectionRule_Proximity {
    const message = createBaseCustomInfoType_DetectionRule_Proximity();
    message.windowBefore = object.windowBefore ?? 0;
    message.windowAfter = object.windowAfter ?? 0;
    return message;
  },
};

function createBaseCustomInfoType_DetectionRule_LikelihoodAdjustment(): CustomInfoType_DetectionRule_LikelihoodAdjustment {
  return { fixedLikelihood: undefined, relativeLikelihood: undefined };
}

export const CustomInfoType_DetectionRule_LikelihoodAdjustment: MessageFns<
  CustomInfoType_DetectionRule_LikelihoodAdjustment
> = {
  encode(
    message: CustomInfoType_DetectionRule_LikelihoodAdjustment,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.fixedLikelihood !== undefined) {
      writer.uint32(8).int32(message.fixedLikelihood);
    }
    if (message.relativeLikelihood !== undefined) {
      writer.uint32(16).int32(message.relativeLikelihood);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CustomInfoType_DetectionRule_LikelihoodAdjustment {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCustomInfoType_DetectionRule_LikelihoodAdjustment();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.fixedLikelihood = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.relativeLikelihood = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CustomInfoType_DetectionRule_LikelihoodAdjustment {
    return {
      fixedLikelihood: isSet(object.fixedLikelihood) ? likelihoodFromJSON(object.fixedLikelihood) : undefined,
      relativeLikelihood: isSet(object.relativeLikelihood) ? globalThis.Number(object.relativeLikelihood) : undefined,
    };
  },

  toJSON(message: CustomInfoType_DetectionRule_LikelihoodAdjustment): unknown {
    const obj: any = {};
    if (message.fixedLikelihood !== undefined) {
      obj.fixedLikelihood = likelihoodToJSON(message.fixedLikelihood);
    }
    if (message.relativeLikelihood !== undefined) {
      obj.relativeLikelihood = Math.round(message.relativeLikelihood);
    }
    return obj;
  },

  create(
    base?: DeepPartial<CustomInfoType_DetectionRule_LikelihoodAdjustment>,
  ): CustomInfoType_DetectionRule_LikelihoodAdjustment {
    return CustomInfoType_DetectionRule_LikelihoodAdjustment.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<CustomInfoType_DetectionRule_LikelihoodAdjustment>,
  ): CustomInfoType_DetectionRule_LikelihoodAdjustment {
    const message = createBaseCustomInfoType_DetectionRule_LikelihoodAdjustment();
    message.fixedLikelihood = object.fixedLikelihood ?? undefined;
    message.relativeLikelihood = object.relativeLikelihood ?? undefined;
    return message;
  },
};

function createBaseCustomInfoType_DetectionRule_HotwordRule(): CustomInfoType_DetectionRule_HotwordRule {
  return { hotwordRegex: undefined, proximity: undefined, likelihoodAdjustment: undefined };
}

export const CustomInfoType_DetectionRule_HotwordRule: MessageFns<CustomInfoType_DetectionRule_HotwordRule> = {
  encode(message: CustomInfoType_DetectionRule_HotwordRule, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.hotwordRegex !== undefined) {
      CustomInfoType_Regex.encode(message.hotwordRegex, writer.uint32(10).fork()).join();
    }
    if (message.proximity !== undefined) {
      CustomInfoType_DetectionRule_Proximity.encode(message.proximity, writer.uint32(18).fork()).join();
    }
    if (message.likelihoodAdjustment !== undefined) {
      CustomInfoType_DetectionRule_LikelihoodAdjustment.encode(message.likelihoodAdjustment, writer.uint32(26).fork())
        .join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CustomInfoType_DetectionRule_HotwordRule {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCustomInfoType_DetectionRule_HotwordRule();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.hotwordRegex = CustomInfoType_Regex.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.proximity = CustomInfoType_DetectionRule_Proximity.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.likelihoodAdjustment = CustomInfoType_DetectionRule_LikelihoodAdjustment.decode(
            reader,
            reader.uint32(),
          );
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CustomInfoType_DetectionRule_HotwordRule {
    return {
      hotwordRegex: isSet(object.hotwordRegex) ? CustomInfoType_Regex.fromJSON(object.hotwordRegex) : undefined,
      proximity: isSet(object.proximity)
        ? CustomInfoType_DetectionRule_Proximity.fromJSON(object.proximity)
        : undefined,
      likelihoodAdjustment: isSet(object.likelihoodAdjustment)
        ? CustomInfoType_DetectionRule_LikelihoodAdjustment.fromJSON(object.likelihoodAdjustment)
        : undefined,
    };
  },

  toJSON(message: CustomInfoType_DetectionRule_HotwordRule): unknown {
    const obj: any = {};
    if (message.hotwordRegex !== undefined) {
      obj.hotwordRegex = CustomInfoType_Regex.toJSON(message.hotwordRegex);
    }
    if (message.proximity !== undefined) {
      obj.proximity = CustomInfoType_DetectionRule_Proximity.toJSON(message.proximity);
    }
    if (message.likelihoodAdjustment !== undefined) {
      obj.likelihoodAdjustment = CustomInfoType_DetectionRule_LikelihoodAdjustment.toJSON(message.likelihoodAdjustment);
    }
    return obj;
  },

  create(base?: DeepPartial<CustomInfoType_DetectionRule_HotwordRule>): CustomInfoType_DetectionRule_HotwordRule {
    return CustomInfoType_DetectionRule_HotwordRule.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CustomInfoType_DetectionRule_HotwordRule>): CustomInfoType_DetectionRule_HotwordRule {
    const message = createBaseCustomInfoType_DetectionRule_HotwordRule();
    message.hotwordRegex = (object.hotwordRegex !== undefined && object.hotwordRegex !== null)
      ? CustomInfoType_Regex.fromPartial(object.hotwordRegex)
      : undefined;
    message.proximity = (object.proximity !== undefined && object.proximity !== null)
      ? CustomInfoType_DetectionRule_Proximity.fromPartial(object.proximity)
      : undefined;
    message.likelihoodAdjustment = (object.likelihoodAdjustment !== undefined && object.likelihoodAdjustment !== null)
      ? CustomInfoType_DetectionRule_LikelihoodAdjustment.fromPartial(object.likelihoodAdjustment)
      : undefined;
    return message;
  },
};

function createBaseFieldId(): FieldId {
  return { name: "" };
}

export const FieldId: MessageFns<FieldId> = {
  encode(message: FieldId, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FieldId {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFieldId();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FieldId {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: FieldId): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<FieldId>): FieldId {
    return FieldId.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FieldId>): FieldId {
    const message = createBaseFieldId();
    message.name = object.name ?? "";
    return message;
  },
};

function createBasePartitionId(): PartitionId {
  return { projectId: "", namespaceId: "" };
}

export const PartitionId: MessageFns<PartitionId> = {
  encode(message: PartitionId, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(18).string(message.projectId);
    }
    if (message.namespaceId !== "") {
      writer.uint32(34).string(message.namespaceId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PartitionId {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePartitionId();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.namespaceId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PartitionId {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      namespaceId: isSet(object.namespaceId) ? globalThis.String(object.namespaceId) : "",
    };
  },

  toJSON(message: PartitionId): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.namespaceId !== "") {
      obj.namespaceId = message.namespaceId;
    }
    return obj;
  },

  create(base?: DeepPartial<PartitionId>): PartitionId {
    return PartitionId.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PartitionId>): PartitionId {
    const message = createBasePartitionId();
    message.projectId = object.projectId ?? "";
    message.namespaceId = object.namespaceId ?? "";
    return message;
  },
};

function createBaseKindExpression(): KindExpression {
  return { name: "" };
}

export const KindExpression: MessageFns<KindExpression> = {
  encode(message: KindExpression, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KindExpression {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKindExpression();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KindExpression {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: KindExpression): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<KindExpression>): KindExpression {
    return KindExpression.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<KindExpression>): KindExpression {
    const message = createBaseKindExpression();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseDatastoreOptions(): DatastoreOptions {
  return { partitionId: undefined, kind: undefined };
}

export const DatastoreOptions: MessageFns<DatastoreOptions> = {
  encode(message: DatastoreOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.partitionId !== undefined) {
      PartitionId.encode(message.partitionId, writer.uint32(10).fork()).join();
    }
    if (message.kind !== undefined) {
      KindExpression.encode(message.kind, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatastoreOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatastoreOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.partitionId = PartitionId.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.kind = KindExpression.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatastoreOptions {
    return {
      partitionId: isSet(object.partitionId) ? PartitionId.fromJSON(object.partitionId) : undefined,
      kind: isSet(object.kind) ? KindExpression.fromJSON(object.kind) : undefined,
    };
  },

  toJSON(message: DatastoreOptions): unknown {
    const obj: any = {};
    if (message.partitionId !== undefined) {
      obj.partitionId = PartitionId.toJSON(message.partitionId);
    }
    if (message.kind !== undefined) {
      obj.kind = KindExpression.toJSON(message.kind);
    }
    return obj;
  },

  create(base?: DeepPartial<DatastoreOptions>): DatastoreOptions {
    return DatastoreOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatastoreOptions>): DatastoreOptions {
    const message = createBaseDatastoreOptions();
    message.partitionId = (object.partitionId !== undefined && object.partitionId !== null)
      ? PartitionId.fromPartial(object.partitionId)
      : undefined;
    message.kind = (object.kind !== undefined && object.kind !== null)
      ? KindExpression.fromPartial(object.kind)
      : undefined;
    return message;
  },
};

function createBaseCloudStorageRegexFileSet(): CloudStorageRegexFileSet {
  return { bucketName: "", includeRegex: [], excludeRegex: [] };
}

export const CloudStorageRegexFileSet: MessageFns<CloudStorageRegexFileSet> = {
  encode(message: CloudStorageRegexFileSet, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.bucketName !== "") {
      writer.uint32(10).string(message.bucketName);
    }
    for (const v of message.includeRegex) {
      writer.uint32(18).string(v!);
    }
    for (const v of message.excludeRegex) {
      writer.uint32(26).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CloudStorageRegexFileSet {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCloudStorageRegexFileSet();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.bucketName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.includeRegex.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.excludeRegex.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CloudStorageRegexFileSet {
    return {
      bucketName: isSet(object.bucketName) ? globalThis.String(object.bucketName) : "",
      includeRegex: globalThis.Array.isArray(object?.includeRegex)
        ? object.includeRegex.map((e: any) => globalThis.String(e))
        : [],
      excludeRegex: globalThis.Array.isArray(object?.excludeRegex)
        ? object.excludeRegex.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: CloudStorageRegexFileSet): unknown {
    const obj: any = {};
    if (message.bucketName !== "") {
      obj.bucketName = message.bucketName;
    }
    if (message.includeRegex?.length) {
      obj.includeRegex = message.includeRegex;
    }
    if (message.excludeRegex?.length) {
      obj.excludeRegex = message.excludeRegex;
    }
    return obj;
  },

  create(base?: DeepPartial<CloudStorageRegexFileSet>): CloudStorageRegexFileSet {
    return CloudStorageRegexFileSet.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CloudStorageRegexFileSet>): CloudStorageRegexFileSet {
    const message = createBaseCloudStorageRegexFileSet();
    message.bucketName = object.bucketName ?? "";
    message.includeRegex = object.includeRegex?.map((e) => e) || [];
    message.excludeRegex = object.excludeRegex?.map((e) => e) || [];
    return message;
  },
};

function createBaseCloudStorageOptions(): CloudStorageOptions {
  return {
    fileSet: undefined,
    bytesLimitPerFile: Long.ZERO,
    bytesLimitPerFilePercent: 0,
    fileTypes: [],
    sampleMethod: 0,
    filesLimitPercent: 0,
  };
}

export const CloudStorageOptions: MessageFns<CloudStorageOptions> = {
  encode(message: CloudStorageOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.fileSet !== undefined) {
      CloudStorageOptions_FileSet.encode(message.fileSet, writer.uint32(10).fork()).join();
    }
    if (!message.bytesLimitPerFile.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.bytesLimitPerFile.toString());
    }
    if (message.bytesLimitPerFilePercent !== 0) {
      writer.uint32(64).int32(message.bytesLimitPerFilePercent);
    }
    writer.uint32(42).fork();
    for (const v of message.fileTypes) {
      writer.int32(v);
    }
    writer.join();
    if (message.sampleMethod !== 0) {
      writer.uint32(48).int32(message.sampleMethod);
    }
    if (message.filesLimitPercent !== 0) {
      writer.uint32(56).int32(message.filesLimitPercent);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CloudStorageOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCloudStorageOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.fileSet = CloudStorageOptions_FileSet.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.bytesLimitPerFile = Long.fromString(reader.int64().toString());
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.bytesLimitPerFilePercent = reader.int32();
          continue;
        case 5:
          if (tag === 40) {
            message.fileTypes.push(reader.int32() as any);

            continue;
          }

          if (tag === 42) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.fileTypes.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.sampleMethod = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.filesLimitPercent = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CloudStorageOptions {
    return {
      fileSet: isSet(object.fileSet) ? CloudStorageOptions_FileSet.fromJSON(object.fileSet) : undefined,
      bytesLimitPerFile: isSet(object.bytesLimitPerFile) ? Long.fromValue(object.bytesLimitPerFile) : Long.ZERO,
      bytesLimitPerFilePercent: isSet(object.bytesLimitPerFilePercent)
        ? globalThis.Number(object.bytesLimitPerFilePercent)
        : 0,
      fileTypes: globalThis.Array.isArray(object?.fileTypes)
        ? object.fileTypes.map((e: any) => fileTypeFromJSON(e))
        : [],
      sampleMethod: isSet(object.sampleMethod) ? cloudStorageOptions_SampleMethodFromJSON(object.sampleMethod) : 0,
      filesLimitPercent: isSet(object.filesLimitPercent) ? globalThis.Number(object.filesLimitPercent) : 0,
    };
  },

  toJSON(message: CloudStorageOptions): unknown {
    const obj: any = {};
    if (message.fileSet !== undefined) {
      obj.fileSet = CloudStorageOptions_FileSet.toJSON(message.fileSet);
    }
    if (!message.bytesLimitPerFile.equals(Long.ZERO)) {
      obj.bytesLimitPerFile = (message.bytesLimitPerFile || Long.ZERO).toString();
    }
    if (message.bytesLimitPerFilePercent !== 0) {
      obj.bytesLimitPerFilePercent = Math.round(message.bytesLimitPerFilePercent);
    }
    if (message.fileTypes?.length) {
      obj.fileTypes = message.fileTypes.map((e) => fileTypeToJSON(e));
    }
    if (message.sampleMethod !== 0) {
      obj.sampleMethod = cloudStorageOptions_SampleMethodToJSON(message.sampleMethod);
    }
    if (message.filesLimitPercent !== 0) {
      obj.filesLimitPercent = Math.round(message.filesLimitPercent);
    }
    return obj;
  },

  create(base?: DeepPartial<CloudStorageOptions>): CloudStorageOptions {
    return CloudStorageOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CloudStorageOptions>): CloudStorageOptions {
    const message = createBaseCloudStorageOptions();
    message.fileSet = (object.fileSet !== undefined && object.fileSet !== null)
      ? CloudStorageOptions_FileSet.fromPartial(object.fileSet)
      : undefined;
    message.bytesLimitPerFile = (object.bytesLimitPerFile !== undefined && object.bytesLimitPerFile !== null)
      ? Long.fromValue(object.bytesLimitPerFile)
      : Long.ZERO;
    message.bytesLimitPerFilePercent = object.bytesLimitPerFilePercent ?? 0;
    message.fileTypes = object.fileTypes?.map((e) => e) || [];
    message.sampleMethod = object.sampleMethod ?? 0;
    message.filesLimitPercent = object.filesLimitPercent ?? 0;
    return message;
  },
};

function createBaseCloudStorageOptions_FileSet(): CloudStorageOptions_FileSet {
  return { url: "", regexFileSet: undefined };
}

export const CloudStorageOptions_FileSet: MessageFns<CloudStorageOptions_FileSet> = {
  encode(message: CloudStorageOptions_FileSet, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.url !== "") {
      writer.uint32(10).string(message.url);
    }
    if (message.regexFileSet !== undefined) {
      CloudStorageRegexFileSet.encode(message.regexFileSet, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CloudStorageOptions_FileSet {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCloudStorageOptions_FileSet();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.url = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.regexFileSet = CloudStorageRegexFileSet.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CloudStorageOptions_FileSet {
    return {
      url: isSet(object.url) ? globalThis.String(object.url) : "",
      regexFileSet: isSet(object.regexFileSet) ? CloudStorageRegexFileSet.fromJSON(object.regexFileSet) : undefined,
    };
  },

  toJSON(message: CloudStorageOptions_FileSet): unknown {
    const obj: any = {};
    if (message.url !== "") {
      obj.url = message.url;
    }
    if (message.regexFileSet !== undefined) {
      obj.regexFileSet = CloudStorageRegexFileSet.toJSON(message.regexFileSet);
    }
    return obj;
  },

  create(base?: DeepPartial<CloudStorageOptions_FileSet>): CloudStorageOptions_FileSet {
    return CloudStorageOptions_FileSet.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CloudStorageOptions_FileSet>): CloudStorageOptions_FileSet {
    const message = createBaseCloudStorageOptions_FileSet();
    message.url = object.url ?? "";
    message.regexFileSet = (object.regexFileSet !== undefined && object.regexFileSet !== null)
      ? CloudStorageRegexFileSet.fromPartial(object.regexFileSet)
      : undefined;
    return message;
  },
};

function createBaseCloudStorageFileSet(): CloudStorageFileSet {
  return { url: "" };
}

export const CloudStorageFileSet: MessageFns<CloudStorageFileSet> = {
  encode(message: CloudStorageFileSet, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.url !== "") {
      writer.uint32(10).string(message.url);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CloudStorageFileSet {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCloudStorageFileSet();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.url = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CloudStorageFileSet {
    return { url: isSet(object.url) ? globalThis.String(object.url) : "" };
  },

  toJSON(message: CloudStorageFileSet): unknown {
    const obj: any = {};
    if (message.url !== "") {
      obj.url = message.url;
    }
    return obj;
  },

  create(base?: DeepPartial<CloudStorageFileSet>): CloudStorageFileSet {
    return CloudStorageFileSet.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CloudStorageFileSet>): CloudStorageFileSet {
    const message = createBaseCloudStorageFileSet();
    message.url = object.url ?? "";
    return message;
  },
};

function createBaseCloudStoragePath(): CloudStoragePath {
  return { path: "" };
}

export const CloudStoragePath: MessageFns<CloudStoragePath> = {
  encode(message: CloudStoragePath, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.path !== "") {
      writer.uint32(10).string(message.path);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CloudStoragePath {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCloudStoragePath();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.path = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CloudStoragePath {
    return { path: isSet(object.path) ? globalThis.String(object.path) : "" };
  },

  toJSON(message: CloudStoragePath): unknown {
    const obj: any = {};
    if (message.path !== "") {
      obj.path = message.path;
    }
    return obj;
  },

  create(base?: DeepPartial<CloudStoragePath>): CloudStoragePath {
    return CloudStoragePath.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CloudStoragePath>): CloudStoragePath {
    const message = createBaseCloudStoragePath();
    message.path = object.path ?? "";
    return message;
  },
};

function createBaseBigQueryOptions(): BigQueryOptions {
  return {
    tableReference: undefined,
    identifyingFields: [],
    rowsLimit: Long.ZERO,
    rowsLimitPercent: 0,
    sampleMethod: 0,
    excludedFields: [],
    includedFields: [],
  };
}

export const BigQueryOptions: MessageFns<BigQueryOptions> = {
  encode(message: BigQueryOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.tableReference !== undefined) {
      BigQueryTable.encode(message.tableReference, writer.uint32(10).fork()).join();
    }
    for (const v of message.identifyingFields) {
      FieldId.encode(v!, writer.uint32(18).fork()).join();
    }
    if (!message.rowsLimit.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.rowsLimit.toString());
    }
    if (message.rowsLimitPercent !== 0) {
      writer.uint32(48).int32(message.rowsLimitPercent);
    }
    if (message.sampleMethod !== 0) {
      writer.uint32(32).int32(message.sampleMethod);
    }
    for (const v of message.excludedFields) {
      FieldId.encode(v!, writer.uint32(42).fork()).join();
    }
    for (const v of message.includedFields) {
      FieldId.encode(v!, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigQueryOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigQueryOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.tableReference = BigQueryTable.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.identifyingFields.push(FieldId.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.rowsLimit = Long.fromString(reader.int64().toString());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.rowsLimitPercent = reader.int32();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.sampleMethod = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.excludedFields.push(FieldId.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.includedFields.push(FieldId.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigQueryOptions {
    return {
      tableReference: isSet(object.tableReference) ? BigQueryTable.fromJSON(object.tableReference) : undefined,
      identifyingFields: globalThis.Array.isArray(object?.identifyingFields)
        ? object.identifyingFields.map((e: any) => FieldId.fromJSON(e))
        : [],
      rowsLimit: isSet(object.rowsLimit) ? Long.fromValue(object.rowsLimit) : Long.ZERO,
      rowsLimitPercent: isSet(object.rowsLimitPercent) ? globalThis.Number(object.rowsLimitPercent) : 0,
      sampleMethod: isSet(object.sampleMethod) ? bigQueryOptions_SampleMethodFromJSON(object.sampleMethod) : 0,
      excludedFields: globalThis.Array.isArray(object?.excludedFields)
        ? object.excludedFields.map((e: any) => FieldId.fromJSON(e))
        : [],
      includedFields: globalThis.Array.isArray(object?.includedFields)
        ? object.includedFields.map((e: any) => FieldId.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BigQueryOptions): unknown {
    const obj: any = {};
    if (message.tableReference !== undefined) {
      obj.tableReference = BigQueryTable.toJSON(message.tableReference);
    }
    if (message.identifyingFields?.length) {
      obj.identifyingFields = message.identifyingFields.map((e) => FieldId.toJSON(e));
    }
    if (!message.rowsLimit.equals(Long.ZERO)) {
      obj.rowsLimit = (message.rowsLimit || Long.ZERO).toString();
    }
    if (message.rowsLimitPercent !== 0) {
      obj.rowsLimitPercent = Math.round(message.rowsLimitPercent);
    }
    if (message.sampleMethod !== 0) {
      obj.sampleMethod = bigQueryOptions_SampleMethodToJSON(message.sampleMethod);
    }
    if (message.excludedFields?.length) {
      obj.excludedFields = message.excludedFields.map((e) => FieldId.toJSON(e));
    }
    if (message.includedFields?.length) {
      obj.includedFields = message.includedFields.map((e) => FieldId.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BigQueryOptions>): BigQueryOptions {
    return BigQueryOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigQueryOptions>): BigQueryOptions {
    const message = createBaseBigQueryOptions();
    message.tableReference = (object.tableReference !== undefined && object.tableReference !== null)
      ? BigQueryTable.fromPartial(object.tableReference)
      : undefined;
    message.identifyingFields = object.identifyingFields?.map((e) => FieldId.fromPartial(e)) || [];
    message.rowsLimit = (object.rowsLimit !== undefined && object.rowsLimit !== null)
      ? Long.fromValue(object.rowsLimit)
      : Long.ZERO;
    message.rowsLimitPercent = object.rowsLimitPercent ?? 0;
    message.sampleMethod = object.sampleMethod ?? 0;
    message.excludedFields = object.excludedFields?.map((e) => FieldId.fromPartial(e)) || [];
    message.includedFields = object.includedFields?.map((e) => FieldId.fromPartial(e)) || [];
    return message;
  },
};

function createBaseStorageConfig(): StorageConfig {
  return {
    datastoreOptions: undefined,
    cloudStorageOptions: undefined,
    bigQueryOptions: undefined,
    hybridOptions: undefined,
    timespanConfig: undefined,
  };
}

export const StorageConfig: MessageFns<StorageConfig> = {
  encode(message: StorageConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.datastoreOptions !== undefined) {
      DatastoreOptions.encode(message.datastoreOptions, writer.uint32(18).fork()).join();
    }
    if (message.cloudStorageOptions !== undefined) {
      CloudStorageOptions.encode(message.cloudStorageOptions, writer.uint32(26).fork()).join();
    }
    if (message.bigQueryOptions !== undefined) {
      BigQueryOptions.encode(message.bigQueryOptions, writer.uint32(34).fork()).join();
    }
    if (message.hybridOptions !== undefined) {
      HybridOptions.encode(message.hybridOptions, writer.uint32(74).fork()).join();
    }
    if (message.timespanConfig !== undefined) {
      StorageConfig_TimespanConfig.encode(message.timespanConfig, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StorageConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStorageConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.datastoreOptions = DatastoreOptions.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.cloudStorageOptions = CloudStorageOptions.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.bigQueryOptions = BigQueryOptions.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.hybridOptions = HybridOptions.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.timespanConfig = StorageConfig_TimespanConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StorageConfig {
    return {
      datastoreOptions: isSet(object.datastoreOptions) ? DatastoreOptions.fromJSON(object.datastoreOptions) : undefined,
      cloudStorageOptions: isSet(object.cloudStorageOptions)
        ? CloudStorageOptions.fromJSON(object.cloudStorageOptions)
        : undefined,
      bigQueryOptions: isSet(object.bigQueryOptions) ? BigQueryOptions.fromJSON(object.bigQueryOptions) : undefined,
      hybridOptions: isSet(object.hybridOptions) ? HybridOptions.fromJSON(object.hybridOptions) : undefined,
      timespanConfig: isSet(object.timespanConfig)
        ? StorageConfig_TimespanConfig.fromJSON(object.timespanConfig)
        : undefined,
    };
  },

  toJSON(message: StorageConfig): unknown {
    const obj: any = {};
    if (message.datastoreOptions !== undefined) {
      obj.datastoreOptions = DatastoreOptions.toJSON(message.datastoreOptions);
    }
    if (message.cloudStorageOptions !== undefined) {
      obj.cloudStorageOptions = CloudStorageOptions.toJSON(message.cloudStorageOptions);
    }
    if (message.bigQueryOptions !== undefined) {
      obj.bigQueryOptions = BigQueryOptions.toJSON(message.bigQueryOptions);
    }
    if (message.hybridOptions !== undefined) {
      obj.hybridOptions = HybridOptions.toJSON(message.hybridOptions);
    }
    if (message.timespanConfig !== undefined) {
      obj.timespanConfig = StorageConfig_TimespanConfig.toJSON(message.timespanConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<StorageConfig>): StorageConfig {
    return StorageConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StorageConfig>): StorageConfig {
    const message = createBaseStorageConfig();
    message.datastoreOptions = (object.datastoreOptions !== undefined && object.datastoreOptions !== null)
      ? DatastoreOptions.fromPartial(object.datastoreOptions)
      : undefined;
    message.cloudStorageOptions = (object.cloudStorageOptions !== undefined && object.cloudStorageOptions !== null)
      ? CloudStorageOptions.fromPartial(object.cloudStorageOptions)
      : undefined;
    message.bigQueryOptions = (object.bigQueryOptions !== undefined && object.bigQueryOptions !== null)
      ? BigQueryOptions.fromPartial(object.bigQueryOptions)
      : undefined;
    message.hybridOptions = (object.hybridOptions !== undefined && object.hybridOptions !== null)
      ? HybridOptions.fromPartial(object.hybridOptions)
      : undefined;
    message.timespanConfig = (object.timespanConfig !== undefined && object.timespanConfig !== null)
      ? StorageConfig_TimespanConfig.fromPartial(object.timespanConfig)
      : undefined;
    return message;
  },
};

function createBaseStorageConfig_TimespanConfig(): StorageConfig_TimespanConfig {
  return {
    startTime: undefined,
    endTime: undefined,
    timestampField: undefined,
    enableAutoPopulationOfTimespanConfig: false,
  };
}

export const StorageConfig_TimespanConfig: MessageFns<StorageConfig_TimespanConfig> = {
  encode(message: StorageConfig_TimespanConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(10).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(18).fork()).join();
    }
    if (message.timestampField !== undefined) {
      FieldId.encode(message.timestampField, writer.uint32(26).fork()).join();
    }
    if (message.enableAutoPopulationOfTimespanConfig !== false) {
      writer.uint32(32).bool(message.enableAutoPopulationOfTimespanConfig);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StorageConfig_TimespanConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStorageConfig_TimespanConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.timestampField = FieldId.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.enableAutoPopulationOfTimespanConfig = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StorageConfig_TimespanConfig {
    return {
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      timestampField: isSet(object.timestampField) ? FieldId.fromJSON(object.timestampField) : undefined,
      enableAutoPopulationOfTimespanConfig: isSet(object.enableAutoPopulationOfTimespanConfig)
        ? globalThis.Boolean(object.enableAutoPopulationOfTimespanConfig)
        : false,
    };
  },

  toJSON(message: StorageConfig_TimespanConfig): unknown {
    const obj: any = {};
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (message.timestampField !== undefined) {
      obj.timestampField = FieldId.toJSON(message.timestampField);
    }
    if (message.enableAutoPopulationOfTimespanConfig !== false) {
      obj.enableAutoPopulationOfTimespanConfig = message.enableAutoPopulationOfTimespanConfig;
    }
    return obj;
  },

  create(base?: DeepPartial<StorageConfig_TimespanConfig>): StorageConfig_TimespanConfig {
    return StorageConfig_TimespanConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StorageConfig_TimespanConfig>): StorageConfig_TimespanConfig {
    const message = createBaseStorageConfig_TimespanConfig();
    message.startTime = object.startTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    message.timestampField = (object.timestampField !== undefined && object.timestampField !== null)
      ? FieldId.fromPartial(object.timestampField)
      : undefined;
    message.enableAutoPopulationOfTimespanConfig = object.enableAutoPopulationOfTimespanConfig ?? false;
    return message;
  },
};

function createBaseHybridOptions(): HybridOptions {
  return { description: "", requiredFindingLabelKeys: [], labels: {}, tableOptions: undefined };
}

export const HybridOptions: MessageFns<HybridOptions> = {
  encode(message: HybridOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.description !== "") {
      writer.uint32(10).string(message.description);
    }
    for (const v of message.requiredFindingLabelKeys) {
      writer.uint32(18).string(v!);
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      HybridOptions_LabelsEntry.encode({ key: key as any, value }, writer.uint32(26).fork()).join();
    });
    if (message.tableOptions !== undefined) {
      TableOptions.encode(message.tableOptions, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): HybridOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseHybridOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.description = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.requiredFindingLabelKeys.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          const entry3 = HybridOptions_LabelsEntry.decode(reader, reader.uint32());
          if (entry3.value !== undefined) {
            message.labels[entry3.key] = entry3.value;
          }
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.tableOptions = TableOptions.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): HybridOptions {
    return {
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      requiredFindingLabelKeys: globalThis.Array.isArray(object?.requiredFindingLabelKeys)
        ? object.requiredFindingLabelKeys.map((e: any) => globalThis.String(e))
        : [],
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      tableOptions: isSet(object.tableOptions) ? TableOptions.fromJSON(object.tableOptions) : undefined,
    };
  },

  toJSON(message: HybridOptions): unknown {
    const obj: any = {};
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.requiredFindingLabelKeys?.length) {
      obj.requiredFindingLabelKeys = message.requiredFindingLabelKeys;
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.tableOptions !== undefined) {
      obj.tableOptions = TableOptions.toJSON(message.tableOptions);
    }
    return obj;
  },

  create(base?: DeepPartial<HybridOptions>): HybridOptions {
    return HybridOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<HybridOptions>): HybridOptions {
    const message = createBaseHybridOptions();
    message.description = object.description ?? "";
    message.requiredFindingLabelKeys = object.requiredFindingLabelKeys?.map((e) => e) || [];
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.tableOptions = (object.tableOptions !== undefined && object.tableOptions !== null)
      ? TableOptions.fromPartial(object.tableOptions)
      : undefined;
    return message;
  },
};

function createBaseHybridOptions_LabelsEntry(): HybridOptions_LabelsEntry {
  return { key: "", value: "" };
}

export const HybridOptions_LabelsEntry: MessageFns<HybridOptions_LabelsEntry> = {
  encode(message: HybridOptions_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): HybridOptions_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseHybridOptions_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): HybridOptions_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: HybridOptions_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<HybridOptions_LabelsEntry>): HybridOptions_LabelsEntry {
    return HybridOptions_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<HybridOptions_LabelsEntry>): HybridOptions_LabelsEntry {
    const message = createBaseHybridOptions_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseBigQueryKey(): BigQueryKey {
  return { tableReference: undefined, rowNumber: Long.ZERO };
}

export const BigQueryKey: MessageFns<BigQueryKey> = {
  encode(message: BigQueryKey, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.tableReference !== undefined) {
      BigQueryTable.encode(message.tableReference, writer.uint32(10).fork()).join();
    }
    if (!message.rowNumber.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.rowNumber.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigQueryKey {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigQueryKey();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.tableReference = BigQueryTable.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.rowNumber = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigQueryKey {
    return {
      tableReference: isSet(object.tableReference) ? BigQueryTable.fromJSON(object.tableReference) : undefined,
      rowNumber: isSet(object.rowNumber) ? Long.fromValue(object.rowNumber) : Long.ZERO,
    };
  },

  toJSON(message: BigQueryKey): unknown {
    const obj: any = {};
    if (message.tableReference !== undefined) {
      obj.tableReference = BigQueryTable.toJSON(message.tableReference);
    }
    if (!message.rowNumber.equals(Long.ZERO)) {
      obj.rowNumber = (message.rowNumber || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<BigQueryKey>): BigQueryKey {
    return BigQueryKey.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigQueryKey>): BigQueryKey {
    const message = createBaseBigQueryKey();
    message.tableReference = (object.tableReference !== undefined && object.tableReference !== null)
      ? BigQueryTable.fromPartial(object.tableReference)
      : undefined;
    message.rowNumber = (object.rowNumber !== undefined && object.rowNumber !== null)
      ? Long.fromValue(object.rowNumber)
      : Long.ZERO;
    return message;
  },
};

function createBaseDatastoreKey(): DatastoreKey {
  return { entityKey: undefined };
}

export const DatastoreKey: MessageFns<DatastoreKey> = {
  encode(message: DatastoreKey, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.entityKey !== undefined) {
      Key.encode(message.entityKey, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatastoreKey {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatastoreKey();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.entityKey = Key.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatastoreKey {
    return { entityKey: isSet(object.entityKey) ? Key.fromJSON(object.entityKey) : undefined };
  },

  toJSON(message: DatastoreKey): unknown {
    const obj: any = {};
    if (message.entityKey !== undefined) {
      obj.entityKey = Key.toJSON(message.entityKey);
    }
    return obj;
  },

  create(base?: DeepPartial<DatastoreKey>): DatastoreKey {
    return DatastoreKey.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatastoreKey>): DatastoreKey {
    const message = createBaseDatastoreKey();
    message.entityKey = (object.entityKey !== undefined && object.entityKey !== null)
      ? Key.fromPartial(object.entityKey)
      : undefined;
    return message;
  },
};

function createBaseKey(): Key {
  return { partitionId: undefined, path: [] };
}

export const Key: MessageFns<Key> = {
  encode(message: Key, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.partitionId !== undefined) {
      PartitionId.encode(message.partitionId, writer.uint32(10).fork()).join();
    }
    for (const v of message.path) {
      Key_PathElement.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Key {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKey();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.partitionId = PartitionId.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.path.push(Key_PathElement.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Key {
    return {
      partitionId: isSet(object.partitionId) ? PartitionId.fromJSON(object.partitionId) : undefined,
      path: globalThis.Array.isArray(object?.path) ? object.path.map((e: any) => Key_PathElement.fromJSON(e)) : [],
    };
  },

  toJSON(message: Key): unknown {
    const obj: any = {};
    if (message.partitionId !== undefined) {
      obj.partitionId = PartitionId.toJSON(message.partitionId);
    }
    if (message.path?.length) {
      obj.path = message.path.map((e) => Key_PathElement.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<Key>): Key {
    return Key.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Key>): Key {
    const message = createBaseKey();
    message.partitionId = (object.partitionId !== undefined && object.partitionId !== null)
      ? PartitionId.fromPartial(object.partitionId)
      : undefined;
    message.path = object.path?.map((e) => Key_PathElement.fromPartial(e)) || [];
    return message;
  },
};

function createBaseKey_PathElement(): Key_PathElement {
  return { kind: "", id: undefined, name: undefined };
}

export const Key_PathElement: MessageFns<Key_PathElement> = {
  encode(message: Key_PathElement, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.id !== undefined) {
      writer.uint32(16).int64(message.id.toString());
    }
    if (message.name !== undefined) {
      writer.uint32(26).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Key_PathElement {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKey_PathElement();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.id = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Key_PathElement {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      id: isSet(object.id) ? Long.fromValue(object.id) : undefined,
      name: isSet(object.name) ? globalThis.String(object.name) : undefined,
    };
  },

  toJSON(message: Key_PathElement): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.id !== undefined) {
      obj.id = (message.id || Long.ZERO).toString();
    }
    if (message.name !== undefined) {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<Key_PathElement>): Key_PathElement {
    return Key_PathElement.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Key_PathElement>): Key_PathElement {
    const message = createBaseKey_PathElement();
    message.kind = object.kind ?? "";
    message.id = (object.id !== undefined && object.id !== null) ? Long.fromValue(object.id) : undefined;
    message.name = object.name ?? undefined;
    return message;
  },
};

function createBaseRecordKey(): RecordKey {
  return { datastoreKey: undefined, bigQueryKey: undefined, idValues: [] };
}

export const RecordKey: MessageFns<RecordKey> = {
  encode(message: RecordKey, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.datastoreKey !== undefined) {
      DatastoreKey.encode(message.datastoreKey, writer.uint32(18).fork()).join();
    }
    if (message.bigQueryKey !== undefined) {
      BigQueryKey.encode(message.bigQueryKey, writer.uint32(26).fork()).join();
    }
    for (const v of message.idValues) {
      writer.uint32(42).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RecordKey {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRecordKey();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.datastoreKey = DatastoreKey.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.bigQueryKey = BigQueryKey.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.idValues.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RecordKey {
    return {
      datastoreKey: isSet(object.datastoreKey) ? DatastoreKey.fromJSON(object.datastoreKey) : undefined,
      bigQueryKey: isSet(object.bigQueryKey) ? BigQueryKey.fromJSON(object.bigQueryKey) : undefined,
      idValues: globalThis.Array.isArray(object?.idValues) ? object.idValues.map((e: any) => globalThis.String(e)) : [],
    };
  },

  toJSON(message: RecordKey): unknown {
    const obj: any = {};
    if (message.datastoreKey !== undefined) {
      obj.datastoreKey = DatastoreKey.toJSON(message.datastoreKey);
    }
    if (message.bigQueryKey !== undefined) {
      obj.bigQueryKey = BigQueryKey.toJSON(message.bigQueryKey);
    }
    if (message.idValues?.length) {
      obj.idValues = message.idValues;
    }
    return obj;
  },

  create(base?: DeepPartial<RecordKey>): RecordKey {
    return RecordKey.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RecordKey>): RecordKey {
    const message = createBaseRecordKey();
    message.datastoreKey = (object.datastoreKey !== undefined && object.datastoreKey !== null)
      ? DatastoreKey.fromPartial(object.datastoreKey)
      : undefined;
    message.bigQueryKey = (object.bigQueryKey !== undefined && object.bigQueryKey !== null)
      ? BigQueryKey.fromPartial(object.bigQueryKey)
      : undefined;
    message.idValues = object.idValues?.map((e) => e) || [];
    return message;
  },
};

function createBaseBigQueryTable(): BigQueryTable {
  return { projectId: "", datasetId: "", tableId: "" };
}

export const BigQueryTable: MessageFns<BigQueryTable> = {
  encode(message: BigQueryTable, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.datasetId !== "") {
      writer.uint32(18).string(message.datasetId);
    }
    if (message.tableId !== "") {
      writer.uint32(26).string(message.tableId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigQueryTable {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigQueryTable();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.tableId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigQueryTable {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      datasetId: isSet(object.datasetId) ? globalThis.String(object.datasetId) : "",
      tableId: isSet(object.tableId) ? globalThis.String(object.tableId) : "",
    };
  },

  toJSON(message: BigQueryTable): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    if (message.tableId !== "") {
      obj.tableId = message.tableId;
    }
    return obj;
  },

  create(base?: DeepPartial<BigQueryTable>): BigQueryTable {
    return BigQueryTable.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigQueryTable>): BigQueryTable {
    const message = createBaseBigQueryTable();
    message.projectId = object.projectId ?? "";
    message.datasetId = object.datasetId ?? "";
    message.tableId = object.tableId ?? "";
    return message;
  },
};

function createBaseTableReference(): TableReference {
  return { datasetId: "", tableId: "" };
}

export const TableReference: MessageFns<TableReference> = {
  encode(message: TableReference, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.datasetId !== "") {
      writer.uint32(10).string(message.datasetId);
    }
    if (message.tableId !== "") {
      writer.uint32(18).string(message.tableId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TableReference {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTableReference();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.tableId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TableReference {
    return {
      datasetId: isSet(object.datasetId) ? globalThis.String(object.datasetId) : "",
      tableId: isSet(object.tableId) ? globalThis.String(object.tableId) : "",
    };
  },

  toJSON(message: TableReference): unknown {
    const obj: any = {};
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    if (message.tableId !== "") {
      obj.tableId = message.tableId;
    }
    return obj;
  },

  create(base?: DeepPartial<TableReference>): TableReference {
    return TableReference.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TableReference>): TableReference {
    const message = createBaseTableReference();
    message.datasetId = object.datasetId ?? "";
    message.tableId = object.tableId ?? "";
    return message;
  },
};

function createBaseBigQueryField(): BigQueryField {
  return { table: undefined, field: undefined };
}

export const BigQueryField: MessageFns<BigQueryField> = {
  encode(message: BigQueryField, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.table !== undefined) {
      BigQueryTable.encode(message.table, writer.uint32(10).fork()).join();
    }
    if (message.field !== undefined) {
      FieldId.encode(message.field, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigQueryField {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigQueryField();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.table = BigQueryTable.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.field = FieldId.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigQueryField {
    return {
      table: isSet(object.table) ? BigQueryTable.fromJSON(object.table) : undefined,
      field: isSet(object.field) ? FieldId.fromJSON(object.field) : undefined,
    };
  },

  toJSON(message: BigQueryField): unknown {
    const obj: any = {};
    if (message.table !== undefined) {
      obj.table = BigQueryTable.toJSON(message.table);
    }
    if (message.field !== undefined) {
      obj.field = FieldId.toJSON(message.field);
    }
    return obj;
  },

  create(base?: DeepPartial<BigQueryField>): BigQueryField {
    return BigQueryField.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigQueryField>): BigQueryField {
    const message = createBaseBigQueryField();
    message.table = (object.table !== undefined && object.table !== null)
      ? BigQueryTable.fromPartial(object.table)
      : undefined;
    message.field = (object.field !== undefined && object.field !== null)
      ? FieldId.fromPartial(object.field)
      : undefined;
    return message;
  },
};

function createBaseEntityId(): EntityId {
  return { field: undefined };
}

export const EntityId: MessageFns<EntityId> = {
  encode(message: EntityId, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.field !== undefined) {
      FieldId.encode(message.field, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EntityId {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEntityId();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.field = FieldId.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EntityId {
    return { field: isSet(object.field) ? FieldId.fromJSON(object.field) : undefined };
  },

  toJSON(message: EntityId): unknown {
    const obj: any = {};
    if (message.field !== undefined) {
      obj.field = FieldId.toJSON(message.field);
    }
    return obj;
  },

  create(base?: DeepPartial<EntityId>): EntityId {
    return EntityId.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EntityId>): EntityId {
    const message = createBaseEntityId();
    message.field = (object.field !== undefined && object.field !== null)
      ? FieldId.fromPartial(object.field)
      : undefined;
    return message;
  },
};

function createBaseTableOptions(): TableOptions {
  return { identifyingFields: [] };
}

export const TableOptions: MessageFns<TableOptions> = {
  encode(message: TableOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.identifyingFields) {
      FieldId.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TableOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTableOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.identifyingFields.push(FieldId.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TableOptions {
    return {
      identifyingFields: globalThis.Array.isArray(object?.identifyingFields)
        ? object.identifyingFields.map((e: any) => FieldId.fromJSON(e))
        : [],
    };
  },

  toJSON(message: TableOptions): unknown {
    const obj: any = {};
    if (message.identifyingFields?.length) {
      obj.identifyingFields = message.identifyingFields.map((e) => FieldId.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<TableOptions>): TableOptions {
    return TableOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TableOptions>): TableOptions {
    const message = createBaseTableOptions();
    message.identifyingFields = object.identifyingFields?.map((e) => FieldId.fromPartial(e)) || [];
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
