// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/ai/generativelanguage/v1beta2/safety.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";

export const protobufPackage = "google.ai.generativelanguage.v1beta2";

/**
 * The category of a rating.
 *
 * These categories cover various kinds of harms that developers
 * may wish to adjust.
 */
export enum HarmCategory {
  /** HARM_CATEGORY_UNSPECIFIED - Category is unspecified. */
  HARM_CATEGORY_UNSPECIFIED = 0,
  /** HARM_CATEGORY_DEROGATORY - Negative or harmful comments targeting identity and/or protected attribute. */
  HARM_CATEGORY_DEROGATORY = 1,
  /** HARM_CATEGORY_TOXICITY - Content that is rude, disrepspectful, or profane. */
  HARM_CATEGORY_TOXICITY = 2,
  /**
   * HARM_CATEGORY_VIOLENCE - Describes scenarios depictng violence against an individual or group, or
   * general descriptions of gore.
   */
  HARM_CATEGORY_VIOLENCE = 3,
  /** HARM_CATEGORY_SEXUAL - Contains references to sexual acts or other lewd content. */
  HARM_CATEGORY_SEXUAL = 4,
  /** HARM_CATEGORY_MEDICAL - Promotes unchecked medical advice. */
  HARM_CATEGORY_MEDICAL = 5,
  /** HARM_CATEGORY_DANGEROUS - Dangerous content that promotes, facilitates, or encourages harmful acts. */
  HARM_CATEGORY_DANGEROUS = 6,
  UNRECOGNIZED = -1,
}

export function harmCategoryFromJSON(object: any): HarmCategory {
  switch (object) {
    case 0:
    case "HARM_CATEGORY_UNSPECIFIED":
      return HarmCategory.HARM_CATEGORY_UNSPECIFIED;
    case 1:
    case "HARM_CATEGORY_DEROGATORY":
      return HarmCategory.HARM_CATEGORY_DEROGATORY;
    case 2:
    case "HARM_CATEGORY_TOXICITY":
      return HarmCategory.HARM_CATEGORY_TOXICITY;
    case 3:
    case "HARM_CATEGORY_VIOLENCE":
      return HarmCategory.HARM_CATEGORY_VIOLENCE;
    case 4:
    case "HARM_CATEGORY_SEXUAL":
      return HarmCategory.HARM_CATEGORY_SEXUAL;
    case 5:
    case "HARM_CATEGORY_MEDICAL":
      return HarmCategory.HARM_CATEGORY_MEDICAL;
    case 6:
    case "HARM_CATEGORY_DANGEROUS":
      return HarmCategory.HARM_CATEGORY_DANGEROUS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return HarmCategory.UNRECOGNIZED;
  }
}

export function harmCategoryToJSON(object: HarmCategory): string {
  switch (object) {
    case HarmCategory.HARM_CATEGORY_UNSPECIFIED:
      return "HARM_CATEGORY_UNSPECIFIED";
    case HarmCategory.HARM_CATEGORY_DEROGATORY:
      return "HARM_CATEGORY_DEROGATORY";
    case HarmCategory.HARM_CATEGORY_TOXICITY:
      return "HARM_CATEGORY_TOXICITY";
    case HarmCategory.HARM_CATEGORY_VIOLENCE:
      return "HARM_CATEGORY_VIOLENCE";
    case HarmCategory.HARM_CATEGORY_SEXUAL:
      return "HARM_CATEGORY_SEXUAL";
    case HarmCategory.HARM_CATEGORY_MEDICAL:
      return "HARM_CATEGORY_MEDICAL";
    case HarmCategory.HARM_CATEGORY_DANGEROUS:
      return "HARM_CATEGORY_DANGEROUS";
    case HarmCategory.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Content filtering metadata associated with processing a single request.
 *
 * ContentFilter contains a reason and an optional supporting string. The reason
 * may be unspecified.
 */
export interface ContentFilter {
  /** The reason content was blocked during request processing. */
  reason: ContentFilter_BlockedReason;
  /** A string that describes the filtering behavior in more detail. */
  message?: string | undefined;
}

/** A list of reasons why content may have been blocked. */
export enum ContentFilter_BlockedReason {
  /** BLOCKED_REASON_UNSPECIFIED - A blocked reason was not specified. */
  BLOCKED_REASON_UNSPECIFIED = 0,
  /** SAFETY - Content was blocked by safety settings. */
  SAFETY = 1,
  /** OTHER - Content was blocked, but the reason is uncategorized. */
  OTHER = 2,
  UNRECOGNIZED = -1,
}

export function contentFilter_BlockedReasonFromJSON(object: any): ContentFilter_BlockedReason {
  switch (object) {
    case 0:
    case "BLOCKED_REASON_UNSPECIFIED":
      return ContentFilter_BlockedReason.BLOCKED_REASON_UNSPECIFIED;
    case 1:
    case "SAFETY":
      return ContentFilter_BlockedReason.SAFETY;
    case 2:
    case "OTHER":
      return ContentFilter_BlockedReason.OTHER;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ContentFilter_BlockedReason.UNRECOGNIZED;
  }
}

export function contentFilter_BlockedReasonToJSON(object: ContentFilter_BlockedReason): string {
  switch (object) {
    case ContentFilter_BlockedReason.BLOCKED_REASON_UNSPECIFIED:
      return "BLOCKED_REASON_UNSPECIFIED";
    case ContentFilter_BlockedReason.SAFETY:
      return "SAFETY";
    case ContentFilter_BlockedReason.OTHER:
      return "OTHER";
    case ContentFilter_BlockedReason.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Safety feedback for an entire request.
 *
 * This field is populated if content in the input and/or response is blocked
 * due to safety settings. SafetyFeedback may not exist for every HarmCategory.
 * Each SafetyFeedback will return the safety settings used by the request as
 * well as the lowest HarmProbability that should be allowed in order to return
 * a result.
 */
export interface SafetyFeedback {
  /** Safety rating evaluated from content. */
  rating:
    | SafetyRating
    | undefined;
  /** Safety settings applied to the request. */
  setting: SafetySetting | undefined;
}

/**
 * Safety rating for a piece of content.
 *
 * The safety rating contains the category of harm and the
 * harm probability level in that category for a piece of content.
 * Content is classified for safety across a number of
 * harm categories and the probability of the harm classification is included
 * here.
 */
export interface SafetyRating {
  /** Required. The category for this rating. */
  category: HarmCategory;
  /** Required. The probability of harm for this content. */
  probability: SafetyRating_HarmProbability;
}

/**
 * The probability that a piece of content is harmful.
 *
 * The classification system gives the probability of the content being
 * unsafe. This does not indicate the severity of harm for a piece of content.
 */
export enum SafetyRating_HarmProbability {
  /** HARM_PROBABILITY_UNSPECIFIED - Probability is unspecified. */
  HARM_PROBABILITY_UNSPECIFIED = 0,
  /** NEGLIGIBLE - Content has a negligible chance of being unsafe. */
  NEGLIGIBLE = 1,
  /** LOW - Content has a low chance of being unsafe. */
  LOW = 2,
  /** MEDIUM - Content has a medium chance of being unsafe. */
  MEDIUM = 3,
  /** HIGH - Content has a high chance of being unsafe. */
  HIGH = 4,
  UNRECOGNIZED = -1,
}

export function safetyRating_HarmProbabilityFromJSON(object: any): SafetyRating_HarmProbability {
  switch (object) {
    case 0:
    case "HARM_PROBABILITY_UNSPECIFIED":
      return SafetyRating_HarmProbability.HARM_PROBABILITY_UNSPECIFIED;
    case 1:
    case "NEGLIGIBLE":
      return SafetyRating_HarmProbability.NEGLIGIBLE;
    case 2:
    case "LOW":
      return SafetyRating_HarmProbability.LOW;
    case 3:
    case "MEDIUM":
      return SafetyRating_HarmProbability.MEDIUM;
    case 4:
    case "HIGH":
      return SafetyRating_HarmProbability.HIGH;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SafetyRating_HarmProbability.UNRECOGNIZED;
  }
}

export function safetyRating_HarmProbabilityToJSON(object: SafetyRating_HarmProbability): string {
  switch (object) {
    case SafetyRating_HarmProbability.HARM_PROBABILITY_UNSPECIFIED:
      return "HARM_PROBABILITY_UNSPECIFIED";
    case SafetyRating_HarmProbability.NEGLIGIBLE:
      return "NEGLIGIBLE";
    case SafetyRating_HarmProbability.LOW:
      return "LOW";
    case SafetyRating_HarmProbability.MEDIUM:
      return "MEDIUM";
    case SafetyRating_HarmProbability.HIGH:
      return "HIGH";
    case SafetyRating_HarmProbability.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Safety setting, affecting the safety-blocking behavior.
 *
 * Passing a safety setting for a category changes the allowed proability that
 * content is blocked.
 */
export interface SafetySetting {
  /** Required. The category for this setting. */
  category: HarmCategory;
  /** Required. Controls the probability threshold at which harm is blocked. */
  threshold: SafetySetting_HarmBlockThreshold;
}

/** Block at and beyond a specified harm probability. */
export enum SafetySetting_HarmBlockThreshold {
  /** HARM_BLOCK_THRESHOLD_UNSPECIFIED - Threshold is unspecified. */
  HARM_BLOCK_THRESHOLD_UNSPECIFIED = 0,
  /** BLOCK_LOW_AND_ABOVE - Content with NEGLIGIBLE will be allowed. */
  BLOCK_LOW_AND_ABOVE = 1,
  /** BLOCK_MEDIUM_AND_ABOVE - Content with NEGLIGIBLE and LOW will be allowed. */
  BLOCK_MEDIUM_AND_ABOVE = 2,
  /** BLOCK_ONLY_HIGH - Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed. */
  BLOCK_ONLY_HIGH = 3,
  UNRECOGNIZED = -1,
}

export function safetySetting_HarmBlockThresholdFromJSON(object: any): SafetySetting_HarmBlockThreshold {
  switch (object) {
    case 0:
    case "HARM_BLOCK_THRESHOLD_UNSPECIFIED":
      return SafetySetting_HarmBlockThreshold.HARM_BLOCK_THRESHOLD_UNSPECIFIED;
    case 1:
    case "BLOCK_LOW_AND_ABOVE":
      return SafetySetting_HarmBlockThreshold.BLOCK_LOW_AND_ABOVE;
    case 2:
    case "BLOCK_MEDIUM_AND_ABOVE":
      return SafetySetting_HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE;
    case 3:
    case "BLOCK_ONLY_HIGH":
      return SafetySetting_HarmBlockThreshold.BLOCK_ONLY_HIGH;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SafetySetting_HarmBlockThreshold.UNRECOGNIZED;
  }
}

export function safetySetting_HarmBlockThresholdToJSON(object: SafetySetting_HarmBlockThreshold): string {
  switch (object) {
    case SafetySetting_HarmBlockThreshold.HARM_BLOCK_THRESHOLD_UNSPECIFIED:
      return "HARM_BLOCK_THRESHOLD_UNSPECIFIED";
    case SafetySetting_HarmBlockThreshold.BLOCK_LOW_AND_ABOVE:
      return "BLOCK_LOW_AND_ABOVE";
    case SafetySetting_HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE:
      return "BLOCK_MEDIUM_AND_ABOVE";
    case SafetySetting_HarmBlockThreshold.BLOCK_ONLY_HIGH:
      return "BLOCK_ONLY_HIGH";
    case SafetySetting_HarmBlockThreshold.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseContentFilter(): ContentFilter {
  return { reason: 0, message: undefined };
}

export const ContentFilter: MessageFns<ContentFilter> = {
  encode(message: ContentFilter, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.reason !== 0) {
      writer.uint32(8).int32(message.reason);
    }
    if (message.message !== undefined) {
      writer.uint32(18).string(message.message);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ContentFilter {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseContentFilter();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.reason = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.message = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ContentFilter {
    return {
      reason: isSet(object.reason) ? contentFilter_BlockedReasonFromJSON(object.reason) : 0,
      message: isSet(object.message) ? globalThis.String(object.message) : undefined,
    };
  },

  toJSON(message: ContentFilter): unknown {
    const obj: any = {};
    if (message.reason !== 0) {
      obj.reason = contentFilter_BlockedReasonToJSON(message.reason);
    }
    if (message.message !== undefined) {
      obj.message = message.message;
    }
    return obj;
  },

  create(base?: DeepPartial<ContentFilter>): ContentFilter {
    return ContentFilter.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ContentFilter>): ContentFilter {
    const message = createBaseContentFilter();
    message.reason = object.reason ?? 0;
    message.message = object.message ?? undefined;
    return message;
  },
};

function createBaseSafetyFeedback(): SafetyFeedback {
  return { rating: undefined, setting: undefined };
}

export const SafetyFeedback: MessageFns<SafetyFeedback> = {
  encode(message: SafetyFeedback, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.rating !== undefined) {
      SafetyRating.encode(message.rating, writer.uint32(10).fork()).join();
    }
    if (message.setting !== undefined) {
      SafetySetting.encode(message.setting, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SafetyFeedback {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSafetyFeedback();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.rating = SafetyRating.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.setting = SafetySetting.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SafetyFeedback {
    return {
      rating: isSet(object.rating) ? SafetyRating.fromJSON(object.rating) : undefined,
      setting: isSet(object.setting) ? SafetySetting.fromJSON(object.setting) : undefined,
    };
  },

  toJSON(message: SafetyFeedback): unknown {
    const obj: any = {};
    if (message.rating !== undefined) {
      obj.rating = SafetyRating.toJSON(message.rating);
    }
    if (message.setting !== undefined) {
      obj.setting = SafetySetting.toJSON(message.setting);
    }
    return obj;
  },

  create(base?: DeepPartial<SafetyFeedback>): SafetyFeedback {
    return SafetyFeedback.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SafetyFeedback>): SafetyFeedback {
    const message = createBaseSafetyFeedback();
    message.rating = (object.rating !== undefined && object.rating !== null)
      ? SafetyRating.fromPartial(object.rating)
      : undefined;
    message.setting = (object.setting !== undefined && object.setting !== null)
      ? SafetySetting.fromPartial(object.setting)
      : undefined;
    return message;
  },
};

function createBaseSafetyRating(): SafetyRating {
  return { category: 0, probability: 0 };
}

export const SafetyRating: MessageFns<SafetyRating> = {
  encode(message: SafetyRating, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.category !== 0) {
      writer.uint32(24).int32(message.category);
    }
    if (message.probability !== 0) {
      writer.uint32(32).int32(message.probability);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SafetyRating {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSafetyRating();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 24) {
            break;
          }

          message.category = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.probability = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SafetyRating {
    return {
      category: isSet(object.category) ? harmCategoryFromJSON(object.category) : 0,
      probability: isSet(object.probability) ? safetyRating_HarmProbabilityFromJSON(object.probability) : 0,
    };
  },

  toJSON(message: SafetyRating): unknown {
    const obj: any = {};
    if (message.category !== 0) {
      obj.category = harmCategoryToJSON(message.category);
    }
    if (message.probability !== 0) {
      obj.probability = safetyRating_HarmProbabilityToJSON(message.probability);
    }
    return obj;
  },

  create(base?: DeepPartial<SafetyRating>): SafetyRating {
    return SafetyRating.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SafetyRating>): SafetyRating {
    const message = createBaseSafetyRating();
    message.category = object.category ?? 0;
    message.probability = object.probability ?? 0;
    return message;
  },
};

function createBaseSafetySetting(): SafetySetting {
  return { category: 0, threshold: 0 };
}

export const SafetySetting: MessageFns<SafetySetting> = {
  encode(message: SafetySetting, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.category !== 0) {
      writer.uint32(24).int32(message.category);
    }
    if (message.threshold !== 0) {
      writer.uint32(32).int32(message.threshold);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SafetySetting {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSafetySetting();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 24) {
            break;
          }

          message.category = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.threshold = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SafetySetting {
    return {
      category: isSet(object.category) ? harmCategoryFromJSON(object.category) : 0,
      threshold: isSet(object.threshold) ? safetySetting_HarmBlockThresholdFromJSON(object.threshold) : 0,
    };
  },

  toJSON(message: SafetySetting): unknown {
    const obj: any = {};
    if (message.category !== 0) {
      obj.category = harmCategoryToJSON(message.category);
    }
    if (message.threshold !== 0) {
      obj.threshold = safetySetting_HarmBlockThresholdToJSON(message.threshold);
    }
    return obj;
  },

  create(base?: DeepPartial<SafetySetting>): SafetySetting {
    return SafetySetting.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SafetySetting>): SafetySetting {
    const message = createBaseSafetySetting();
    message.category = object.category ?? 0;
    message.threshold = object.threshold ?? 0;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
