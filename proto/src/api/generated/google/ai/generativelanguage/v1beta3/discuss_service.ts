// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/ai/generativelanguage/v1beta3/discuss_service.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { CitationMetadata } from "./citation.js";
import { ContentFilter } from "./safety.js";

export const protobufPackage = "google.ai.generativelanguage.v1beta3";

/** Request to generate a message response from the model. */
export interface GenerateMessageRequest {
  /**
   * Required. The name of the model to use.
   *
   * Format: `name=models/{model}`.
   */
  model: string;
  /**
   * Required. The structured textual input given to the model as a prompt.
   *
   * Given a
   * prompt, the model will return what it predicts is the next message in the
   * discussion.
   */
  prompt:
    | MessagePrompt
    | undefined;
  /**
   * Optional. Controls the randomness of the output.
   *
   * Values can range over `[0.0,1.0]`,
   * inclusive. A value closer to `1.0` will produce responses that are more
   * varied, while a value closer to `0.0` will typically result in
   * less surprising responses from the model.
   */
  temperature?:
    | number
    | undefined;
  /**
   * Optional. The number of generated response messages to return.
   *
   * This value must be between
   * `[1, 8]`, inclusive. If unset, this will default to `1`.
   */
  candidateCount?:
    | number
    | undefined;
  /**
   * Optional. The maximum cumulative probability of tokens to consider when
   * sampling.
   *
   * The model uses combined Top-k and nucleus sampling.
   *
   * Nucleus sampling considers the smallest set of tokens whose probability
   * sum is at least `top_p`.
   */
  topP?:
    | number
    | undefined;
  /**
   * Optional. The maximum number of tokens to consider when sampling.
   *
   * The model uses combined Top-k and nucleus sampling.
   *
   * Top-k sampling considers the set of `top_k` most probable tokens.
   */
  topK?: number | undefined;
}

/**
 * The response from the model.
 *
 * This includes candidate messages and
 * conversation history in the form of chronologically-ordered messages.
 */
export interface GenerateMessageResponse {
  /** Candidate response messages from the model. */
  candidates: Message[];
  /** The conversation history used by the model. */
  messages: Message[];
  /**
   * A set of content filtering metadata for the prompt and response
   * text.
   *
   * This indicates which `SafetyCategory`(s) blocked a
   * candidate from this response, the lowest `HarmProbability`
   * that triggered a block, and the HarmThreshold setting for that category.
   */
  filters: ContentFilter[];
}

/**
 * The base unit of structured text.
 *
 * A `Message` includes an `author` and the `content` of
 * the `Message`.
 *
 * The `author` is used to tag messages when they are fed to the
 * model as text.
 */
export interface Message {
  /**
   * Optional. The author of this Message.
   *
   * This serves as a key for tagging
   * the content of this Message when it is fed to the model as text.
   *
   * The author can be any alphanumeric string.
   */
  author: string;
  /** Required. The text content of the structured `Message`. */
  content: string;
  /**
   * Output only. Citation information for model-generated `content` in this
   * `Message`.
   *
   * If this `Message` was generated as output from the model, this field may be
   * populated with attribution information for any text included in the
   * `content`. This field is used only on output.
   */
  citationMetadata?: CitationMetadata | undefined;
}

/**
 * All of the structured input text passed to the model as a prompt.
 *
 * A `MessagePrompt` contains a structured set of fields that provide context
 * for the conversation, examples of user input/model output message pairs that
 * prime the model to respond in different ways, and the conversation history
 * or list of messages representing the alternating turns of the conversation
 * between the user and the model.
 */
export interface MessagePrompt {
  /**
   * Optional. Text that should be provided to the model first to ground the
   * response.
   *
   * If not empty, this `context` will be given to the model first before the
   * `examples` and `messages`. When using a `context` be sure to provide it
   * with every request to maintain continuity.
   *
   * This field can be a description of your prompt to the model to help provide
   * context and guide the responses. Examples: "Translate the phrase from
   * English to French." or "Given a statement, classify the sentiment as happy,
   * sad or neutral."
   *
   * Anything included in this field will take precedence over message history
   * if the total input size exceeds the model's `input_token_limit` and the
   * input request is truncated.
   */
  context: string;
  /**
   * Optional. Examples of what the model should generate.
   *
   * This includes both user input and the response that the model should
   * emulate.
   *
   * These `examples` are treated identically to conversation messages except
   * that they take precedence over the history in `messages`:
   * If the total input size exceeds the model's `input_token_limit` the input
   * will be truncated. Items will be dropped from `messages` before `examples`.
   */
  examples: Example[];
  /**
   * Required. A snapshot of the recent conversation history sorted
   * chronologically.
   *
   * Turns alternate between two authors.
   *
   * If the total input size exceeds the model's `input_token_limit` the input
   * will be truncated: The oldest items will be dropped from `messages`.
   */
  messages: Message[];
}

/**
 * An input/output example used to instruct the Model.
 *
 * It demonstrates how the model should respond or format its response.
 */
export interface Example {
  /** Required. An example of an input `Message` from the user. */
  input:
    | Message
    | undefined;
  /** Required. An example of what the model should output given the input. */
  output: Message | undefined;
}

/**
 * Counts the number of tokens in the `prompt` sent to a model.
 *
 * Models may tokenize text differently, so each model may return a different
 * `token_count`.
 */
export interface CountMessageTokensRequest {
  /**
   * Required. The model's resource name. This serves as an ID for the Model to
   * use.
   *
   * This name should match a model name returned by the `ListModels` method.
   *
   * Format: `models/{model}`
   */
  model: string;
  /** Required. The prompt, whose token count is to be returned. */
  prompt: MessagePrompt | undefined;
}

/**
 * A response from `CountMessageTokens`.
 *
 * It returns the model's `token_count` for the `prompt`.
 */
export interface CountMessageTokensResponse {
  /**
   * The number of tokens that the `model` tokenizes the `prompt` into.
   *
   * Always non-negative.
   */
  tokenCount: number;
}

function createBaseGenerateMessageRequest(): GenerateMessageRequest {
  return {
    model: "",
    prompt: undefined,
    temperature: undefined,
    candidateCount: undefined,
    topP: undefined,
    topK: undefined,
  };
}

export const GenerateMessageRequest: MessageFns<GenerateMessageRequest> = {
  encode(message: GenerateMessageRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    if (message.prompt !== undefined) {
      MessagePrompt.encode(message.prompt, writer.uint32(18).fork()).join();
    }
    if (message.temperature !== undefined) {
      writer.uint32(29).float(message.temperature);
    }
    if (message.candidateCount !== undefined) {
      writer.uint32(32).int32(message.candidateCount);
    }
    if (message.topP !== undefined) {
      writer.uint32(45).float(message.topP);
    }
    if (message.topK !== undefined) {
      writer.uint32(48).int32(message.topK);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerateMessageRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerateMessageRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.prompt = MessagePrompt.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 29) {
            break;
          }

          message.temperature = reader.float();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.candidateCount = reader.int32();
          continue;
        case 5:
          if (tag !== 45) {
            break;
          }

          message.topP = reader.float();
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.topK = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerateMessageRequest {
    return {
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      prompt: isSet(object.prompt) ? MessagePrompt.fromJSON(object.prompt) : undefined,
      temperature: isSet(object.temperature) ? globalThis.Number(object.temperature) : undefined,
      candidateCount: isSet(object.candidateCount) ? globalThis.Number(object.candidateCount) : undefined,
      topP: isSet(object.topP) ? globalThis.Number(object.topP) : undefined,
      topK: isSet(object.topK) ? globalThis.Number(object.topK) : undefined,
    };
  },

  toJSON(message: GenerateMessageRequest): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.prompt !== undefined) {
      obj.prompt = MessagePrompt.toJSON(message.prompt);
    }
    if (message.temperature !== undefined) {
      obj.temperature = message.temperature;
    }
    if (message.candidateCount !== undefined) {
      obj.candidateCount = Math.round(message.candidateCount);
    }
    if (message.topP !== undefined) {
      obj.topP = message.topP;
    }
    if (message.topK !== undefined) {
      obj.topK = Math.round(message.topK);
    }
    return obj;
  },

  create(base?: DeepPartial<GenerateMessageRequest>): GenerateMessageRequest {
    return GenerateMessageRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerateMessageRequest>): GenerateMessageRequest {
    const message = createBaseGenerateMessageRequest();
    message.model = object.model ?? "";
    message.prompt = (object.prompt !== undefined && object.prompt !== null)
      ? MessagePrompt.fromPartial(object.prompt)
      : undefined;
    message.temperature = object.temperature ?? undefined;
    message.candidateCount = object.candidateCount ?? undefined;
    message.topP = object.topP ?? undefined;
    message.topK = object.topK ?? undefined;
    return message;
  },
};

function createBaseGenerateMessageResponse(): GenerateMessageResponse {
  return { candidates: [], messages: [], filters: [] };
}

export const GenerateMessageResponse: MessageFns<GenerateMessageResponse> = {
  encode(message: GenerateMessageResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.candidates) {
      Message.encode(v!, writer.uint32(10).fork()).join();
    }
    for (const v of message.messages) {
      Message.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.filters) {
      ContentFilter.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerateMessageResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerateMessageResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.candidates.push(Message.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.messages.push(Message.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.filters.push(ContentFilter.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerateMessageResponse {
    return {
      candidates: globalThis.Array.isArray(object?.candidates)
        ? object.candidates.map((e: any) => Message.fromJSON(e))
        : [],
      messages: globalThis.Array.isArray(object?.messages) ? object.messages.map((e: any) => Message.fromJSON(e)) : [],
      filters: globalThis.Array.isArray(object?.filters)
        ? object.filters.map((e: any) => ContentFilter.fromJSON(e))
        : [],
    };
  },

  toJSON(message: GenerateMessageResponse): unknown {
    const obj: any = {};
    if (message.candidates?.length) {
      obj.candidates = message.candidates.map((e) => Message.toJSON(e));
    }
    if (message.messages?.length) {
      obj.messages = message.messages.map((e) => Message.toJSON(e));
    }
    if (message.filters?.length) {
      obj.filters = message.filters.map((e) => ContentFilter.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<GenerateMessageResponse>): GenerateMessageResponse {
    return GenerateMessageResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerateMessageResponse>): GenerateMessageResponse {
    const message = createBaseGenerateMessageResponse();
    message.candidates = object.candidates?.map((e) => Message.fromPartial(e)) || [];
    message.messages = object.messages?.map((e) => Message.fromPartial(e)) || [];
    message.filters = object.filters?.map((e) => ContentFilter.fromPartial(e)) || [];
    return message;
  },
};

function createBaseMessage(): Message {
  return { author: "", content: "", citationMetadata: undefined };
}

export const Message: MessageFns<Message> = {
  encode(message: Message, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.author !== "") {
      writer.uint32(10).string(message.author);
    }
    if (message.content !== "") {
      writer.uint32(18).string(message.content);
    }
    if (message.citationMetadata !== undefined) {
      CitationMetadata.encode(message.citationMetadata, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Message {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMessage();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.author = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.content = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.citationMetadata = CitationMetadata.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Message {
    return {
      author: isSet(object.author) ? globalThis.String(object.author) : "",
      content: isSet(object.content) ? globalThis.String(object.content) : "",
      citationMetadata: isSet(object.citationMetadata) ? CitationMetadata.fromJSON(object.citationMetadata) : undefined,
    };
  },

  toJSON(message: Message): unknown {
    const obj: any = {};
    if (message.author !== "") {
      obj.author = message.author;
    }
    if (message.content !== "") {
      obj.content = message.content;
    }
    if (message.citationMetadata !== undefined) {
      obj.citationMetadata = CitationMetadata.toJSON(message.citationMetadata);
    }
    return obj;
  },

  create(base?: DeepPartial<Message>): Message {
    return Message.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Message>): Message {
    const message = createBaseMessage();
    message.author = object.author ?? "";
    message.content = object.content ?? "";
    message.citationMetadata = (object.citationMetadata !== undefined && object.citationMetadata !== null)
      ? CitationMetadata.fromPartial(object.citationMetadata)
      : undefined;
    return message;
  },
};

function createBaseMessagePrompt(): MessagePrompt {
  return { context: "", examples: [], messages: [] };
}

export const MessagePrompt: MessageFns<MessagePrompt> = {
  encode(message: MessagePrompt, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.context !== "") {
      writer.uint32(10).string(message.context);
    }
    for (const v of message.examples) {
      Example.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.messages) {
      Message.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MessagePrompt {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMessagePrompt();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.context = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.examples.push(Example.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.messages.push(Message.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MessagePrompt {
    return {
      context: isSet(object.context) ? globalThis.String(object.context) : "",
      examples: globalThis.Array.isArray(object?.examples) ? object.examples.map((e: any) => Example.fromJSON(e)) : [],
      messages: globalThis.Array.isArray(object?.messages) ? object.messages.map((e: any) => Message.fromJSON(e)) : [],
    };
  },

  toJSON(message: MessagePrompt): unknown {
    const obj: any = {};
    if (message.context !== "") {
      obj.context = message.context;
    }
    if (message.examples?.length) {
      obj.examples = message.examples.map((e) => Example.toJSON(e));
    }
    if (message.messages?.length) {
      obj.messages = message.messages.map((e) => Message.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<MessagePrompt>): MessagePrompt {
    return MessagePrompt.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MessagePrompt>): MessagePrompt {
    const message = createBaseMessagePrompt();
    message.context = object.context ?? "";
    message.examples = object.examples?.map((e) => Example.fromPartial(e)) || [];
    message.messages = object.messages?.map((e) => Message.fromPartial(e)) || [];
    return message;
  },
};

function createBaseExample(): Example {
  return { input: undefined, output: undefined };
}

export const Example: MessageFns<Example> = {
  encode(message: Example, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.input !== undefined) {
      Message.encode(message.input, writer.uint32(10).fork()).join();
    }
    if (message.output !== undefined) {
      Message.encode(message.output, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Example {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExample();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.input = Message.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.output = Message.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Example {
    return {
      input: isSet(object.input) ? Message.fromJSON(object.input) : undefined,
      output: isSet(object.output) ? Message.fromJSON(object.output) : undefined,
    };
  },

  toJSON(message: Example): unknown {
    const obj: any = {};
    if (message.input !== undefined) {
      obj.input = Message.toJSON(message.input);
    }
    if (message.output !== undefined) {
      obj.output = Message.toJSON(message.output);
    }
    return obj;
  },

  create(base?: DeepPartial<Example>): Example {
    return Example.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Example>): Example {
    const message = createBaseExample();
    message.input = (object.input !== undefined && object.input !== null)
      ? Message.fromPartial(object.input)
      : undefined;
    message.output = (object.output !== undefined && object.output !== null)
      ? Message.fromPartial(object.output)
      : undefined;
    return message;
  },
};

function createBaseCountMessageTokensRequest(): CountMessageTokensRequest {
  return { model: "", prompt: undefined };
}

export const CountMessageTokensRequest: MessageFns<CountMessageTokensRequest> = {
  encode(message: CountMessageTokensRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    if (message.prompt !== undefined) {
      MessagePrompt.encode(message.prompt, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CountMessageTokensRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCountMessageTokensRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.prompt = MessagePrompt.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CountMessageTokensRequest {
    return {
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      prompt: isSet(object.prompt) ? MessagePrompt.fromJSON(object.prompt) : undefined,
    };
  },

  toJSON(message: CountMessageTokensRequest): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.prompt !== undefined) {
      obj.prompt = MessagePrompt.toJSON(message.prompt);
    }
    return obj;
  },

  create(base?: DeepPartial<CountMessageTokensRequest>): CountMessageTokensRequest {
    return CountMessageTokensRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CountMessageTokensRequest>): CountMessageTokensRequest {
    const message = createBaseCountMessageTokensRequest();
    message.model = object.model ?? "";
    message.prompt = (object.prompt !== undefined && object.prompt !== null)
      ? MessagePrompt.fromPartial(object.prompt)
      : undefined;
    return message;
  },
};

function createBaseCountMessageTokensResponse(): CountMessageTokensResponse {
  return { tokenCount: 0 };
}

export const CountMessageTokensResponse: MessageFns<CountMessageTokensResponse> = {
  encode(message: CountMessageTokensResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.tokenCount !== 0) {
      writer.uint32(8).int32(message.tokenCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CountMessageTokensResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCountMessageTokensResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.tokenCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CountMessageTokensResponse {
    return { tokenCount: isSet(object.tokenCount) ? globalThis.Number(object.tokenCount) : 0 };
  },

  toJSON(message: CountMessageTokensResponse): unknown {
    const obj: any = {};
    if (message.tokenCount !== 0) {
      obj.tokenCount = Math.round(message.tokenCount);
    }
    return obj;
  },

  create(base?: DeepPartial<CountMessageTokensResponse>): CountMessageTokensResponse {
    return CountMessageTokensResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CountMessageTokensResponse>): CountMessageTokensResponse {
    const message = createBaseCountMessageTokensResponse();
    message.tokenCount = object.tokenCount ?? 0;
    return message;
  },
};

/**
 * An API for using Generative Language Models (GLMs) in dialog applications.
 *
 * Also known as large language models (LLMs), this API provides models that
 * are trained for multi-turn dialog.
 */
export type DiscussServiceDefinition = typeof DiscussServiceDefinition;
export const DiscussServiceDefinition = {
  name: "DiscussService",
  fullName: "google.ai.generativelanguage.v1beta3.DiscussService",
  methods: {
    /** Generates a response from the model given an input `MessagePrompt`. */
    generateMessage: {
      name: "GenerateMessage",
      requestType: GenerateMessageRequest,
      requestStream: false,
      responseType: GenerateMessageResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              52,
              109,
              111,
              100,
              101,
              108,
              44,
              112,
              114,
              111,
              109,
              112,
              116,
              44,
              116,
              101,
              109,
              112,
              101,
              114,
              97,
              116,
              117,
              114,
              101,
              44,
              99,
              97,
              110,
              100,
              105,
              100,
              97,
              116,
              101,
              95,
              99,
              111,
              117,
              110,
              116,
              44,
              116,
              111,
              112,
              95,
              112,
              44,
              116,
              111,
              112,
              95,
              107,
            ]),
          ],
          578365826: [
            Buffer.from([
              46,
              58,
              1,
              42,
              34,
              41,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              51,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              103,
              101,
              110,
              101,
              114,
              97,
              116,
              101,
              77,
              101,
              115,
              115,
              97,
              103,
              101,
            ]),
          ],
        },
      },
    },
    /** Runs a model's tokenizer on a string and returns the token count. */
    countMessageTokens: {
      name: "CountMessageTokens",
      requestType: CountMessageTokensRequest,
      requestStream: false,
      responseType: CountMessageTokensResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([12, 109, 111, 100, 101, 108, 44, 112, 114, 111, 109, 112, 116])],
          578365826: [
            Buffer.from([
              49,
              58,
              1,
              42,
              34,
              44,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              51,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              99,
              111,
              117,
              110,
              116,
              77,
              101,
              115,
              115,
              97,
              103,
              101,
              84,
              111,
              107,
              101,
              110,
              115,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface DiscussServiceImplementation<CallContextExt = {}> {
  /** Generates a response from the model given an input `MessagePrompt`. */
  generateMessage(
    request: GenerateMessageRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<GenerateMessageResponse>>;
  /** Runs a model's tokenizer on a string and returns the token count. */
  countMessageTokens(
    request: CountMessageTokensRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<CountMessageTokensResponse>>;
}

export interface DiscussServiceClient<CallOptionsExt = {}> {
  /** Generates a response from the model given an input `MessagePrompt`. */
  generateMessage(
    request: DeepPartial<GenerateMessageRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<GenerateMessageResponse>;
  /** Runs a model's tokenizer on a string and returns the token count. */
  countMessageTokens(
    request: DeepPartial<CountMessageTokensRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<CountMessageTokensResponse>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
