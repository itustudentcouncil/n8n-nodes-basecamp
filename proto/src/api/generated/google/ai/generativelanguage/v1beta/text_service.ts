// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/ai/generativelanguage/v1beta/text_service.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { CitationMetadata } from "./citation.js";
import { ContentFilter, SafetyFeedback, SafetyRating, SafetySetting } from "./safety.js";

export const protobufPackage = "google.ai.generativelanguage.v1beta";

/** Request to generate a text completion response from the model. */
export interface GenerateTextRequest {
  /**
   * Required. The name of the `Model` or `TunedModel` to use for generating the
   * completion.
   * Examples:
   *  models/text-bison-001
   *  tunedModels/sentence-translator-u3b7m
   */
  model: string;
  /**
   * Required. The free-form input text given to the model as a prompt.
   *
   * Given a prompt, the model will generate a TextCompletion response it
   * predicts as the completion of the input text.
   */
  prompt:
    | TextPrompt
    | undefined;
  /**
   * Optional. Controls the randomness of the output.
   * Note: The default value varies by model, see the `Model.temperature`
   * attribute of the `Model` returned the `getModel` function.
   *
   * Values can range from [0.0,1.0],
   * inclusive. A value closer to 1.0 will produce responses that are more
   * varied and creative, while a value closer to 0.0 will typically result in
   * more straightforward responses from the model.
   */
  temperature?:
    | number
    | undefined;
  /**
   * Optional. Number of generated responses to return.
   *
   * This value must be between [1, 8], inclusive. If unset, this will default
   * to 1.
   */
  candidateCount?:
    | number
    | undefined;
  /**
   * Optional. The maximum number of tokens to include in a candidate.
   *
   * If unset, this will default to output_token_limit specified in the `Model`
   * specification.
   */
  maxOutputTokens?:
    | number
    | undefined;
  /**
   * Optional. The maximum cumulative probability of tokens to consider when
   * sampling.
   *
   * The model uses combined Top-k and nucleus sampling.
   *
   * Tokens are sorted based on their assigned probabilities so that only the
   * most likely tokens are considered. Top-k sampling directly limits the
   * maximum number of tokens to consider, while Nucleus sampling limits number
   * of tokens based on the cumulative probability.
   *
   * Note: The default value varies by model, see the `Model.top_p`
   * attribute of the `Model` returned the `getModel` function.
   */
  topP?:
    | number
    | undefined;
  /**
   * Optional. The maximum number of tokens to consider when sampling.
   *
   * The model uses combined Top-k and nucleus sampling.
   *
   * Top-k sampling considers the set of `top_k` most probable tokens.
   * Defaults to 40.
   *
   * Note: The default value varies by model, see the `Model.top_k`
   * attribute of the `Model` returned the `getModel` function.
   */
  topK?:
    | number
    | undefined;
  /**
   * Optional. A list of unique `SafetySetting` instances for blocking unsafe
   * content.
   *
   * that will be enforced on the `GenerateTextRequest.prompt` and
   * `GenerateTextResponse.candidates`. There should not be more than one
   * setting for each `SafetyCategory` type. The API will block any prompts and
   * responses that fail to meet the thresholds set by these settings. This list
   * overrides the default settings for each `SafetyCategory` specified in the
   * safety_settings. If there is no `SafetySetting` for a given
   * `SafetyCategory` provided in the list, the API will use the default safety
   * setting for that category. Harm categories HARM_CATEGORY_DEROGATORY,
   * HARM_CATEGORY_TOXICITY, HARM_CATEGORY_VIOLENCE, HARM_CATEGORY_SEXUAL,
   * HARM_CATEGORY_MEDICAL, HARM_CATEGORY_DANGEROUS are supported in text
   * service.
   */
  safetySettings: SafetySetting[];
  /**
   * The set of character sequences (up to 5) that will stop output generation.
   * If specified, the API will stop at the first appearance of a stop
   * sequence. The stop sequence will not be included as part of the response.
   */
  stopSequences: string[];
}

/** The response from the model, including candidate completions. */
export interface GenerateTextResponse {
  /** Candidate responses from the model. */
  candidates: TextCompletion[];
  /**
   * A set of content filtering metadata for the prompt and response
   * text.
   *
   * This indicates which `SafetyCategory`(s) blocked a
   * candidate from this response, the lowest `HarmProbability`
   * that triggered a block, and the HarmThreshold setting for that category.
   * This indicates the smallest change to the `SafetySettings` that would be
   * necessary to unblock at least 1 response.
   *
   * The blocking is configured by the `SafetySettings` in the request (or the
   * default `SafetySettings` of the API).
   */
  filters: ContentFilter[];
  /** Returns any safety feedback related to content filtering. */
  safetyFeedback: SafetyFeedback[];
}

/**
 * Text given to the model as a prompt.
 *
 * The Model will use this TextPrompt to Generate a text completion.
 */
export interface TextPrompt {
  /** Required. The prompt text. */
  text: string;
}

/** Output text returned from a model. */
export interface TextCompletion {
  /** Output only. The generated text returned from the model. */
  output: string;
  /**
   * Ratings for the safety of a response.
   *
   * There is at most one rating per category.
   */
  safetyRatings: SafetyRating[];
  /**
   * Output only. Citation information for model-generated `output` in this
   * `TextCompletion`.
   *
   * This field may be populated with attribution information for any text
   * included in the `output`.
   */
  citationMetadata?: CitationMetadata | undefined;
}

/** Request to get a text embedding from the model. */
export interface EmbedTextRequest {
  /** Required. The model name to use with the format model=models/{model}. */
  model: string;
  /**
   * Optional. The free-form input text that the model will turn into an
   * embedding.
   */
  text: string;
}

/** The response to a EmbedTextRequest. */
export interface EmbedTextResponse {
  /** Output only. The embedding generated from the input text. */
  embedding?: Embedding | undefined;
}

/** Batch request to get a text embedding from the model. */
export interface BatchEmbedTextRequest {
  /**
   * Required. The name of the `Model` to use for generating the embedding.
   * Examples:
   *  models/embedding-gecko-001
   */
  model: string;
  /**
   * Optional. The free-form input texts that the model will turn into an
   * embedding. The current limit is 100 texts, over which an error will be
   * thrown.
   */
  texts: string[];
  /**
   * Optional. Embed requests for the batch. Only one of `texts` or `requests`
   * can be set.
   */
  requests: EmbedTextRequest[];
}

/** The response to a EmbedTextRequest. */
export interface BatchEmbedTextResponse {
  /** Output only. The embeddings generated from the input text. */
  embeddings: Embedding[];
}

/** A list of floats representing the embedding. */
export interface Embedding {
  /** The embedding values. */
  value: number[];
}

/**
 * Counts the number of tokens in the `prompt` sent to a model.
 *
 * Models may tokenize text differently, so each model may return a different
 * `token_count`.
 */
export interface CountTextTokensRequest {
  /**
   * Required. The model's resource name. This serves as an ID for the Model to
   * use.
   *
   * This name should match a model name returned by the `ListModels` method.
   *
   * Format: `models/{model}`
   */
  model: string;
  /** Required. The free-form input text given to the model as a prompt. */
  prompt: TextPrompt | undefined;
}

/**
 * A response from `CountTextTokens`.
 *
 * It returns the model's `token_count` for the `prompt`.
 */
export interface CountTextTokensResponse {
  /**
   * The number of tokens that the `model` tokenizes the `prompt` into.
   *
   * Always non-negative.
   */
  tokenCount: number;
}

function createBaseGenerateTextRequest(): GenerateTextRequest {
  return {
    model: "",
    prompt: undefined,
    temperature: undefined,
    candidateCount: undefined,
    maxOutputTokens: undefined,
    topP: undefined,
    topK: undefined,
    safetySettings: [],
    stopSequences: [],
  };
}

export const GenerateTextRequest: MessageFns<GenerateTextRequest> = {
  encode(message: GenerateTextRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    if (message.prompt !== undefined) {
      TextPrompt.encode(message.prompt, writer.uint32(18).fork()).join();
    }
    if (message.temperature !== undefined) {
      writer.uint32(29).float(message.temperature);
    }
    if (message.candidateCount !== undefined) {
      writer.uint32(32).int32(message.candidateCount);
    }
    if (message.maxOutputTokens !== undefined) {
      writer.uint32(40).int32(message.maxOutputTokens);
    }
    if (message.topP !== undefined) {
      writer.uint32(53).float(message.topP);
    }
    if (message.topK !== undefined) {
      writer.uint32(56).int32(message.topK);
    }
    for (const v of message.safetySettings) {
      SafetySetting.encode(v!, writer.uint32(66).fork()).join();
    }
    for (const v of message.stopSequences) {
      writer.uint32(74).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerateTextRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerateTextRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.prompt = TextPrompt.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 29) {
            break;
          }

          message.temperature = reader.float();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.candidateCount = reader.int32();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.maxOutputTokens = reader.int32();
          continue;
        case 6:
          if (tag !== 53) {
            break;
          }

          message.topP = reader.float();
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.topK = reader.int32();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.safetySettings.push(SafetySetting.decode(reader, reader.uint32()));
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.stopSequences.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerateTextRequest {
    return {
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      prompt: isSet(object.prompt) ? TextPrompt.fromJSON(object.prompt) : undefined,
      temperature: isSet(object.temperature) ? globalThis.Number(object.temperature) : undefined,
      candidateCount: isSet(object.candidateCount) ? globalThis.Number(object.candidateCount) : undefined,
      maxOutputTokens: isSet(object.maxOutputTokens) ? globalThis.Number(object.maxOutputTokens) : undefined,
      topP: isSet(object.topP) ? globalThis.Number(object.topP) : undefined,
      topK: isSet(object.topK) ? globalThis.Number(object.topK) : undefined,
      safetySettings: globalThis.Array.isArray(object?.safetySettings)
        ? object.safetySettings.map((e: any) => SafetySetting.fromJSON(e))
        : [],
      stopSequences: globalThis.Array.isArray(object?.stopSequences)
        ? object.stopSequences.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: GenerateTextRequest): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.prompt !== undefined) {
      obj.prompt = TextPrompt.toJSON(message.prompt);
    }
    if (message.temperature !== undefined) {
      obj.temperature = message.temperature;
    }
    if (message.candidateCount !== undefined) {
      obj.candidateCount = Math.round(message.candidateCount);
    }
    if (message.maxOutputTokens !== undefined) {
      obj.maxOutputTokens = Math.round(message.maxOutputTokens);
    }
    if (message.topP !== undefined) {
      obj.topP = message.topP;
    }
    if (message.topK !== undefined) {
      obj.topK = Math.round(message.topK);
    }
    if (message.safetySettings?.length) {
      obj.safetySettings = message.safetySettings.map((e) => SafetySetting.toJSON(e));
    }
    if (message.stopSequences?.length) {
      obj.stopSequences = message.stopSequences;
    }
    return obj;
  },

  create(base?: DeepPartial<GenerateTextRequest>): GenerateTextRequest {
    return GenerateTextRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerateTextRequest>): GenerateTextRequest {
    const message = createBaseGenerateTextRequest();
    message.model = object.model ?? "";
    message.prompt = (object.prompt !== undefined && object.prompt !== null)
      ? TextPrompt.fromPartial(object.prompt)
      : undefined;
    message.temperature = object.temperature ?? undefined;
    message.candidateCount = object.candidateCount ?? undefined;
    message.maxOutputTokens = object.maxOutputTokens ?? undefined;
    message.topP = object.topP ?? undefined;
    message.topK = object.topK ?? undefined;
    message.safetySettings = object.safetySettings?.map((e) => SafetySetting.fromPartial(e)) || [];
    message.stopSequences = object.stopSequences?.map((e) => e) || [];
    return message;
  },
};

function createBaseGenerateTextResponse(): GenerateTextResponse {
  return { candidates: [], filters: [], safetyFeedback: [] };
}

export const GenerateTextResponse: MessageFns<GenerateTextResponse> = {
  encode(message: GenerateTextResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.candidates) {
      TextCompletion.encode(v!, writer.uint32(10).fork()).join();
    }
    for (const v of message.filters) {
      ContentFilter.encode(v!, writer.uint32(26).fork()).join();
    }
    for (const v of message.safetyFeedback) {
      SafetyFeedback.encode(v!, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerateTextResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerateTextResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.candidates.push(TextCompletion.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.filters.push(ContentFilter.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.safetyFeedback.push(SafetyFeedback.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerateTextResponse {
    return {
      candidates: globalThis.Array.isArray(object?.candidates)
        ? object.candidates.map((e: any) => TextCompletion.fromJSON(e))
        : [],
      filters: globalThis.Array.isArray(object?.filters)
        ? object.filters.map((e: any) => ContentFilter.fromJSON(e))
        : [],
      safetyFeedback: globalThis.Array.isArray(object?.safetyFeedback)
        ? object.safetyFeedback.map((e: any) => SafetyFeedback.fromJSON(e))
        : [],
    };
  },

  toJSON(message: GenerateTextResponse): unknown {
    const obj: any = {};
    if (message.candidates?.length) {
      obj.candidates = message.candidates.map((e) => TextCompletion.toJSON(e));
    }
    if (message.filters?.length) {
      obj.filters = message.filters.map((e) => ContentFilter.toJSON(e));
    }
    if (message.safetyFeedback?.length) {
      obj.safetyFeedback = message.safetyFeedback.map((e) => SafetyFeedback.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<GenerateTextResponse>): GenerateTextResponse {
    return GenerateTextResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerateTextResponse>): GenerateTextResponse {
    const message = createBaseGenerateTextResponse();
    message.candidates = object.candidates?.map((e) => TextCompletion.fromPartial(e)) || [];
    message.filters = object.filters?.map((e) => ContentFilter.fromPartial(e)) || [];
    message.safetyFeedback = object.safetyFeedback?.map((e) => SafetyFeedback.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTextPrompt(): TextPrompt {
  return { text: "" };
}

export const TextPrompt: MessageFns<TextPrompt> = {
  encode(message: TextPrompt, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.text !== "") {
      writer.uint32(10).string(message.text);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TextPrompt {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextPrompt();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.text = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextPrompt {
    return { text: isSet(object.text) ? globalThis.String(object.text) : "" };
  },

  toJSON(message: TextPrompt): unknown {
    const obj: any = {};
    if (message.text !== "") {
      obj.text = message.text;
    }
    return obj;
  },

  create(base?: DeepPartial<TextPrompt>): TextPrompt {
    return TextPrompt.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TextPrompt>): TextPrompt {
    const message = createBaseTextPrompt();
    message.text = object.text ?? "";
    return message;
  },
};

function createBaseTextCompletion(): TextCompletion {
  return { output: "", safetyRatings: [], citationMetadata: undefined };
}

export const TextCompletion: MessageFns<TextCompletion> = {
  encode(message: TextCompletion, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.output !== "") {
      writer.uint32(10).string(message.output);
    }
    for (const v of message.safetyRatings) {
      SafetyRating.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.citationMetadata !== undefined) {
      CitationMetadata.encode(message.citationMetadata, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TextCompletion {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextCompletion();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.output = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.safetyRatings.push(SafetyRating.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.citationMetadata = CitationMetadata.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextCompletion {
    return {
      output: isSet(object.output) ? globalThis.String(object.output) : "",
      safetyRatings: globalThis.Array.isArray(object?.safetyRatings)
        ? object.safetyRatings.map((e: any) => SafetyRating.fromJSON(e))
        : [],
      citationMetadata: isSet(object.citationMetadata) ? CitationMetadata.fromJSON(object.citationMetadata) : undefined,
    };
  },

  toJSON(message: TextCompletion): unknown {
    const obj: any = {};
    if (message.output !== "") {
      obj.output = message.output;
    }
    if (message.safetyRatings?.length) {
      obj.safetyRatings = message.safetyRatings.map((e) => SafetyRating.toJSON(e));
    }
    if (message.citationMetadata !== undefined) {
      obj.citationMetadata = CitationMetadata.toJSON(message.citationMetadata);
    }
    return obj;
  },

  create(base?: DeepPartial<TextCompletion>): TextCompletion {
    return TextCompletion.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TextCompletion>): TextCompletion {
    const message = createBaseTextCompletion();
    message.output = object.output ?? "";
    message.safetyRatings = object.safetyRatings?.map((e) => SafetyRating.fromPartial(e)) || [];
    message.citationMetadata = (object.citationMetadata !== undefined && object.citationMetadata !== null)
      ? CitationMetadata.fromPartial(object.citationMetadata)
      : undefined;
    return message;
  },
};

function createBaseEmbedTextRequest(): EmbedTextRequest {
  return { model: "", text: "" };
}

export const EmbedTextRequest: MessageFns<EmbedTextRequest> = {
  encode(message: EmbedTextRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    if (message.text !== "") {
      writer.uint32(18).string(message.text);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EmbedTextRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEmbedTextRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.text = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EmbedTextRequest {
    return {
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      text: isSet(object.text) ? globalThis.String(object.text) : "",
    };
  },

  toJSON(message: EmbedTextRequest): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.text !== "") {
      obj.text = message.text;
    }
    return obj;
  },

  create(base?: DeepPartial<EmbedTextRequest>): EmbedTextRequest {
    return EmbedTextRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EmbedTextRequest>): EmbedTextRequest {
    const message = createBaseEmbedTextRequest();
    message.model = object.model ?? "";
    message.text = object.text ?? "";
    return message;
  },
};

function createBaseEmbedTextResponse(): EmbedTextResponse {
  return { embedding: undefined };
}

export const EmbedTextResponse: MessageFns<EmbedTextResponse> = {
  encode(message: EmbedTextResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.embedding !== undefined) {
      Embedding.encode(message.embedding, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EmbedTextResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEmbedTextResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.embedding = Embedding.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EmbedTextResponse {
    return { embedding: isSet(object.embedding) ? Embedding.fromJSON(object.embedding) : undefined };
  },

  toJSON(message: EmbedTextResponse): unknown {
    const obj: any = {};
    if (message.embedding !== undefined) {
      obj.embedding = Embedding.toJSON(message.embedding);
    }
    return obj;
  },

  create(base?: DeepPartial<EmbedTextResponse>): EmbedTextResponse {
    return EmbedTextResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EmbedTextResponse>): EmbedTextResponse {
    const message = createBaseEmbedTextResponse();
    message.embedding = (object.embedding !== undefined && object.embedding !== null)
      ? Embedding.fromPartial(object.embedding)
      : undefined;
    return message;
  },
};

function createBaseBatchEmbedTextRequest(): BatchEmbedTextRequest {
  return { model: "", texts: [], requests: [] };
}

export const BatchEmbedTextRequest: MessageFns<BatchEmbedTextRequest> = {
  encode(message: BatchEmbedTextRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    for (const v of message.texts) {
      writer.uint32(18).string(v!);
    }
    for (const v of message.requests) {
      EmbedTextRequest.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchEmbedTextRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchEmbedTextRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.texts.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.requests.push(EmbedTextRequest.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchEmbedTextRequest {
    return {
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      texts: globalThis.Array.isArray(object?.texts) ? object.texts.map((e: any) => globalThis.String(e)) : [],
      requests: globalThis.Array.isArray(object?.requests)
        ? object.requests.map((e: any) => EmbedTextRequest.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchEmbedTextRequest): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.texts?.length) {
      obj.texts = message.texts;
    }
    if (message.requests?.length) {
      obj.requests = message.requests.map((e) => EmbedTextRequest.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchEmbedTextRequest>): BatchEmbedTextRequest {
    return BatchEmbedTextRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchEmbedTextRequest>): BatchEmbedTextRequest {
    const message = createBaseBatchEmbedTextRequest();
    message.model = object.model ?? "";
    message.texts = object.texts?.map((e) => e) || [];
    message.requests = object.requests?.map((e) => EmbedTextRequest.fromPartial(e)) || [];
    return message;
  },
};

function createBaseBatchEmbedTextResponse(): BatchEmbedTextResponse {
  return { embeddings: [] };
}

export const BatchEmbedTextResponse: MessageFns<BatchEmbedTextResponse> = {
  encode(message: BatchEmbedTextResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.embeddings) {
      Embedding.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchEmbedTextResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchEmbedTextResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.embeddings.push(Embedding.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchEmbedTextResponse {
    return {
      embeddings: globalThis.Array.isArray(object?.embeddings)
        ? object.embeddings.map((e: any) => Embedding.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchEmbedTextResponse): unknown {
    const obj: any = {};
    if (message.embeddings?.length) {
      obj.embeddings = message.embeddings.map((e) => Embedding.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchEmbedTextResponse>): BatchEmbedTextResponse {
    return BatchEmbedTextResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchEmbedTextResponse>): BatchEmbedTextResponse {
    const message = createBaseBatchEmbedTextResponse();
    message.embeddings = object.embeddings?.map((e) => Embedding.fromPartial(e)) || [];
    return message;
  },
};

function createBaseEmbedding(): Embedding {
  return { value: [] };
}

export const Embedding: MessageFns<Embedding> = {
  encode(message: Embedding, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    writer.uint32(10).fork();
    for (const v of message.value) {
      writer.float(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Embedding {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEmbedding();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag === 13) {
            message.value.push(reader.float());

            continue;
          }

          if (tag === 10) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.value.push(reader.float());
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Embedding {
    return { value: globalThis.Array.isArray(object?.value) ? object.value.map((e: any) => globalThis.Number(e)) : [] };
  },

  toJSON(message: Embedding): unknown {
    const obj: any = {};
    if (message.value?.length) {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Embedding>): Embedding {
    return Embedding.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Embedding>): Embedding {
    const message = createBaseEmbedding();
    message.value = object.value?.map((e) => e) || [];
    return message;
  },
};

function createBaseCountTextTokensRequest(): CountTextTokensRequest {
  return { model: "", prompt: undefined };
}

export const CountTextTokensRequest: MessageFns<CountTextTokensRequest> = {
  encode(message: CountTextTokensRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    if (message.prompt !== undefined) {
      TextPrompt.encode(message.prompt, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CountTextTokensRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCountTextTokensRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.prompt = TextPrompt.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CountTextTokensRequest {
    return {
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      prompt: isSet(object.prompt) ? TextPrompt.fromJSON(object.prompt) : undefined,
    };
  },

  toJSON(message: CountTextTokensRequest): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.prompt !== undefined) {
      obj.prompt = TextPrompt.toJSON(message.prompt);
    }
    return obj;
  },

  create(base?: DeepPartial<CountTextTokensRequest>): CountTextTokensRequest {
    return CountTextTokensRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CountTextTokensRequest>): CountTextTokensRequest {
    const message = createBaseCountTextTokensRequest();
    message.model = object.model ?? "";
    message.prompt = (object.prompt !== undefined && object.prompt !== null)
      ? TextPrompt.fromPartial(object.prompt)
      : undefined;
    return message;
  },
};

function createBaseCountTextTokensResponse(): CountTextTokensResponse {
  return { tokenCount: 0 };
}

export const CountTextTokensResponse: MessageFns<CountTextTokensResponse> = {
  encode(message: CountTextTokensResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.tokenCount !== 0) {
      writer.uint32(8).int32(message.tokenCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CountTextTokensResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCountTextTokensResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.tokenCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CountTextTokensResponse {
    return { tokenCount: isSet(object.tokenCount) ? globalThis.Number(object.tokenCount) : 0 };
  },

  toJSON(message: CountTextTokensResponse): unknown {
    const obj: any = {};
    if (message.tokenCount !== 0) {
      obj.tokenCount = Math.round(message.tokenCount);
    }
    return obj;
  },

  create(base?: DeepPartial<CountTextTokensResponse>): CountTextTokensResponse {
    return CountTextTokensResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CountTextTokensResponse>): CountTextTokensResponse {
    const message = createBaseCountTextTokensResponse();
    message.tokenCount = object.tokenCount ?? 0;
    return message;
  },
};

/**
 * API for using Generative Language Models (GLMs) trained to generate text.
 *
 * Also known as Large Language Models (LLM)s, these generate text given an
 * input prompt from the user.
 */
export type TextServiceDefinition = typeof TextServiceDefinition;
export const TextServiceDefinition = {
  name: "TextService",
  fullName: "google.ai.generativelanguage.v1beta.TextService",
  methods: {
    /** Generates a response from the model given an input message. */
    generateText: {
      name: "GenerateText",
      requestType: GenerateTextRequest,
      requestStream: false,
      responseType: GenerateTextResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              70,
              109,
              111,
              100,
              101,
              108,
              44,
              112,
              114,
              111,
              109,
              112,
              116,
              44,
              116,
              101,
              109,
              112,
              101,
              114,
              97,
              116,
              117,
              114,
              101,
              44,
              99,
              97,
              110,
              100,
              105,
              100,
              97,
              116,
              101,
              95,
              99,
              111,
              117,
              110,
              116,
              44,
              109,
              97,
              120,
              95,
              111,
              117,
              116,
              112,
              117,
              116,
              95,
              116,
              111,
              107,
              101,
              110,
              115,
              44,
              116,
              111,
              112,
              95,
              112,
              44,
              116,
              111,
              112,
              95,
              107,
            ]),
          ],
          578365826: [
            Buffer.from([
              91,
              58,
              1,
              42,
              90,
              47,
              58,
              1,
              42,
              34,
              42,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              116,
              117,
              110,
              101,
              100,
              77,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              103,
              101,
              110,
              101,
              114,
              97,
              116,
              101,
              84,
              101,
              120,
              116,
              34,
              37,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              103,
              101,
              110,
              101,
              114,
              97,
              116,
              101,
              84,
              101,
              120,
              116,
            ]),
          ],
        },
      },
    },
    /** Generates an embedding from the model given an input message. */
    embedText: {
      name: "EmbedText",
      requestType: EmbedTextRequest,
      requestStream: false,
      responseType: EmbedTextResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([10, 109, 111, 100, 101, 108, 44, 116, 101, 120, 116])],
          578365826: [
            Buffer.from([
              39,
              58,
              1,
              42,
              34,
              34,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              101,
              109,
              98,
              101,
              100,
              84,
              101,
              120,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * Generates multiple embeddings from the model given input text in a
     * synchronous call.
     */
    batchEmbedText: {
      name: "BatchEmbedText",
      requestType: BatchEmbedTextRequest,
      requestStream: false,
      responseType: BatchEmbedTextResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([11, 109, 111, 100, 101, 108, 44, 116, 101, 120, 116, 115])],
          578365826: [
            Buffer.from([
              44,
              58,
              1,
              42,
              34,
              39,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              98,
              97,
              116,
              99,
              104,
              69,
              109,
              98,
              101,
              100,
              84,
              101,
              120,
              116,
            ]),
          ],
        },
      },
    },
    /** Runs a model's tokenizer on a text and returns the token count. */
    countTextTokens: {
      name: "CountTextTokens",
      requestType: CountTextTokensRequest,
      requestStream: false,
      responseType: CountTextTokensResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([12, 109, 111, 100, 101, 108, 44, 112, 114, 111, 109, 112, 116])],
          578365826: [
            Buffer.from([
              45,
              58,
              1,
              42,
              34,
              40,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              99,
              111,
              117,
              110,
              116,
              84,
              101,
              120,
              116,
              84,
              111,
              107,
              101,
              110,
              115,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface TextServiceImplementation<CallContextExt = {}> {
  /** Generates a response from the model given an input message. */
  generateText(
    request: GenerateTextRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<GenerateTextResponse>>;
  /** Generates an embedding from the model given an input message. */
  embedText(request: EmbedTextRequest, context: CallContext & CallContextExt): Promise<DeepPartial<EmbedTextResponse>>;
  /**
   * Generates multiple embeddings from the model given input text in a
   * synchronous call.
   */
  batchEmbedText(
    request: BatchEmbedTextRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<BatchEmbedTextResponse>>;
  /** Runs a model's tokenizer on a text and returns the token count. */
  countTextTokens(
    request: CountTextTokensRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<CountTextTokensResponse>>;
}

export interface TextServiceClient<CallOptionsExt = {}> {
  /** Generates a response from the model given an input message. */
  generateText(
    request: DeepPartial<GenerateTextRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<GenerateTextResponse>;
  /** Generates an embedding from the model given an input message. */
  embedText(request: DeepPartial<EmbedTextRequest>, options?: CallOptions & CallOptionsExt): Promise<EmbedTextResponse>;
  /**
   * Generates multiple embeddings from the model given input text in a
   * synchronous call.
   */
  batchEmbedText(
    request: DeepPartial<BatchEmbedTextRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<BatchEmbedTextResponse>;
  /** Runs a model's tokenizer on a text and returns the token count. */
  countTextTokens(
    request: DeepPartial<CountTextTokensRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<CountTextTokensResponse>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
