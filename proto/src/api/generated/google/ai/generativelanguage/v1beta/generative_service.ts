// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/ai/generativelanguage/v1beta/generative_service.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { CitationMetadata } from "./citation.js";
import { Content, GroundingPassages, Schema, Tool, ToolConfig } from "./content.js";
import { MetadataFilter } from "./retriever.js";
import { SafetyRating, SafetySetting } from "./safety.js";

export const protobufPackage = "google.ai.generativelanguage.v1beta";

/** Type of task for which the embedding will be used. */
export enum TaskType {
  /** TASK_TYPE_UNSPECIFIED - Unset value, which will default to one of the other enum values. */
  TASK_TYPE_UNSPECIFIED = 0,
  /** RETRIEVAL_QUERY - Specifies the given text is a query in a search/retrieval setting. */
  RETRIEVAL_QUERY = 1,
  /** RETRIEVAL_DOCUMENT - Specifies the given text is a document from the corpus being searched. */
  RETRIEVAL_DOCUMENT = 2,
  /** SEMANTIC_SIMILARITY - Specifies the given text will be used for STS. */
  SEMANTIC_SIMILARITY = 3,
  /** CLASSIFICATION - Specifies that the given text will be classified. */
  CLASSIFICATION = 4,
  /** CLUSTERING - Specifies that the embeddings will be used for clustering. */
  CLUSTERING = 5,
  /** QUESTION_ANSWERING - Specifies that the given text will be used for question answering. */
  QUESTION_ANSWERING = 6,
  /** FACT_VERIFICATION - Specifies that the given text will be used for fact verification. */
  FACT_VERIFICATION = 7,
  UNRECOGNIZED = -1,
}

export function taskTypeFromJSON(object: any): TaskType {
  switch (object) {
    case 0:
    case "TASK_TYPE_UNSPECIFIED":
      return TaskType.TASK_TYPE_UNSPECIFIED;
    case 1:
    case "RETRIEVAL_QUERY":
      return TaskType.RETRIEVAL_QUERY;
    case 2:
    case "RETRIEVAL_DOCUMENT":
      return TaskType.RETRIEVAL_DOCUMENT;
    case 3:
    case "SEMANTIC_SIMILARITY":
      return TaskType.SEMANTIC_SIMILARITY;
    case 4:
    case "CLASSIFICATION":
      return TaskType.CLASSIFICATION;
    case 5:
    case "CLUSTERING":
      return TaskType.CLUSTERING;
    case 6:
    case "QUESTION_ANSWERING":
      return TaskType.QUESTION_ANSWERING;
    case 7:
    case "FACT_VERIFICATION":
      return TaskType.FACT_VERIFICATION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return TaskType.UNRECOGNIZED;
  }
}

export function taskTypeToJSON(object: TaskType): string {
  switch (object) {
    case TaskType.TASK_TYPE_UNSPECIFIED:
      return "TASK_TYPE_UNSPECIFIED";
    case TaskType.RETRIEVAL_QUERY:
      return "RETRIEVAL_QUERY";
    case TaskType.RETRIEVAL_DOCUMENT:
      return "RETRIEVAL_DOCUMENT";
    case TaskType.SEMANTIC_SIMILARITY:
      return "SEMANTIC_SIMILARITY";
    case TaskType.CLASSIFICATION:
      return "CLASSIFICATION";
    case TaskType.CLUSTERING:
      return "CLUSTERING";
    case TaskType.QUESTION_ANSWERING:
      return "QUESTION_ANSWERING";
    case TaskType.FACT_VERIFICATION:
      return "FACT_VERIFICATION";
    case TaskType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Request to generate a completion from the model. */
export interface GenerateContentRequest {
  /**
   * Required. The name of the `Model` to use for generating the completion.
   *
   * Format: `name=models/{model}`.
   */
  model: string;
  /**
   * Optional. Developer set [system
   * instruction(s)](https://ai.google.dev/gemini-api/docs/system-instructions).
   * Currently, text only.
   */
  systemInstruction?:
    | Content
    | undefined;
  /**
   * Required. The content of the current conversation with the model.
   *
   * For single-turn queries, this is a single instance. For multi-turn queries
   * like [chat](https://ai.google.dev/gemini-api/docs/text-generation#chat),
   * this is a repeated field that contains the conversation history and the
   * latest request.
   */
  contents: Content[];
  /**
   * Optional. A list of `Tools` the `Model` may use to generate the next
   * response.
   *
   * A `Tool` is a piece of code that enables the system to interact with
   * external systems to perform an action, or set of actions, outside of
   * knowledge and scope of the `Model`. Supported `Tool`s are `Function` and
   * `code_execution`. Refer to the [Function
   * calling](https://ai.google.dev/gemini-api/docs/function-calling) and the
   * [Code execution](https://ai.google.dev/gemini-api/docs/code-execution)
   * guides to learn more.
   */
  tools: Tool[];
  /**
   * Optional. Tool configuration for any `Tool` specified in the request. Refer
   * to the [Function calling
   * guide](https://ai.google.dev/gemini-api/docs/function-calling#function_calling_mode)
   * for a usage example.
   */
  toolConfig:
    | ToolConfig
    | undefined;
  /**
   * Optional. A list of unique `SafetySetting` instances for blocking unsafe
   * content.
   *
   * This will be enforced on the `GenerateContentRequest.contents` and
   * `GenerateContentResponse.candidates`. There should not be more than one
   * setting for each `SafetyCategory` type. The API will block any contents and
   * responses that fail to meet the thresholds set by these settings. This list
   * overrides the default settings for each `SafetyCategory` specified in the
   * safety_settings. If there is no `SafetySetting` for a given
   * `SafetyCategory` provided in the list, the API will use the default safety
   * setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH,
   * HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT,
   * HARM_CATEGORY_HARASSMENT are supported. Refer to the
   * [guide](https://ai.google.dev/gemini-api/docs/safety-settings)
   * for detailed information on available safety settings. Also refer to the
   * [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance) to
   * learn how to incorporate safety considerations in your AI applications.
   */
  safetySettings: SafetySetting[];
  /** Optional. Configuration options for model generation and outputs. */
  generationConfig?:
    | GenerationConfig
    | undefined;
  /**
   * Optional. The name of the content
   * [cached](https://ai.google.dev/gemini-api/docs/caching) to use as context
   * to serve the prediction. Format: `cachedContents/{cachedContent}`
   */
  cachedContent?: string | undefined;
}

/**
 * Configuration options for model generation and outputs. Not all parameters
 * are configurable for every model.
 */
export interface GenerationConfig {
  /**
   * Optional. Number of generated responses to return.
   *
   * Currently, this value can only be set to 1. If unset, this will default
   * to 1.
   */
  candidateCount?:
    | number
    | undefined;
  /**
   * Optional. The set of character sequences (up to 5) that will stop output
   * generation. If specified, the API will stop at the first appearance of a
   * `stop_sequence`. The stop sequence will not be included as part of the
   * response.
   */
  stopSequences: string[];
  /**
   * Optional. The maximum number of tokens to include in a response candidate.
   *
   * Note: The default value varies by model, see the `Model.output_token_limit`
   * attribute of the `Model` returned from the `getModel` function.
   */
  maxOutputTokens?:
    | number
    | undefined;
  /**
   * Optional. Controls the randomness of the output.
   *
   * Note: The default value varies by model, see the `Model.temperature`
   * attribute of the `Model` returned from the `getModel` function.
   *
   * Values can range from [0.0, 2.0].
   */
  temperature?:
    | number
    | undefined;
  /**
   * Optional. The maximum cumulative probability of tokens to consider when
   * sampling.
   *
   * The model uses combined Top-k and Top-p (nucleus) sampling.
   *
   * Tokens are sorted based on their assigned probabilities so that only the
   * most likely tokens are considered. Top-k sampling directly limits the
   * maximum number of tokens to consider, while Nucleus sampling limits the
   * number of tokens based on the cumulative probability.
   *
   * Note: The default value varies by `Model` and is specified by
   * the`Model.top_p` attribute returned from the `getModel` function. An empty
   * `top_k` attribute indicates that the model doesn't apply top-k sampling
   * and doesn't allow setting `top_k` on requests.
   */
  topP?:
    | number
    | undefined;
  /**
   * Optional. The maximum number of tokens to consider when sampling.
   *
   * Gemini models use Top-p (nucleus) sampling or a combination of Top-k and
   * nucleus sampling. Top-k sampling considers the set of `top_k` most probable
   * tokens. Models running with nucleus sampling don't allow top_k setting.
   *
   * Note: The default value varies by `Model` and is specified by
   * the`Model.top_p` attribute returned from the `getModel` function. An empty
   * `top_k` attribute indicates that the model doesn't apply top-k sampling
   * and doesn't allow setting `top_k` on requests.
   */
  topK?:
    | number
    | undefined;
  /**
   * Optional. MIME type of the generated candidate text.
   * Supported MIME types are:
   * `text/plain`: (default) Text output.
   * `application/json`: JSON response in the response candidates.
   * `text/x.enum`: ENUM as a string response in the response candidates.
   * Refer to the
   * [docs](https://ai.google.dev/gemini-api/docs/prompting_with_media#plain_text_formats)
   * for a list of all supported text MIME types.
   */
  responseMimeType: string;
  /**
   * Optional. Output schema of the generated candidate text. Schemas must be a
   * subset of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schema)
   * and can be objects, primitives or arrays.
   *
   * If set, a compatible `response_mime_type` must also be set.
   * Compatible MIME types:
   * `application/json`: Schema for JSON response.
   * Refer to the [JSON text generation
   * guide](https://ai.google.dev/gemini-api/docs/json-mode) for more details.
   */
  responseSchema:
    | Schema
    | undefined;
  /**
   * Optional. Presence penalty applied to the next token's logprobs if the
   * token has already been seen in the response.
   *
   * This penalty is binary on/off and not dependant on the number of times the
   * token is used (after the first). Use
   * [frequency_penalty][google.ai.generativelanguage.v1beta.GenerationConfig.frequency_penalty]
   * for a penalty that increases with each use.
   *
   * A positive penalty will discourage the use of tokens that have already
   * been used in the response, increasing the vocabulary.
   *
   * A negative penalty will encourage the use of tokens that have already been
   * used in the response, decreasing the vocabulary.
   */
  presencePenalty?:
    | number
    | undefined;
  /**
   * Optional. Frequency penalty applied to the next token's logprobs,
   * multiplied by the number of times each token has been seen in the respponse
   * so far.
   *
   * A positive penalty will discourage the use of tokens that have already
   * been used, proportional to the number of times the token has been used:
   * The more a token is used, the more dificult it is for the model to use
   * that token again increasing the vocabulary of responses.
   *
   * Caution: A _negative_ penalty will encourage the model to reuse tokens
   * proportional to the number of times the token has been used. Small
   * negative values will reduce the vocabulary of a response. Larger negative
   * values will cause the model to start repeating a common token  until it
   * hits the
   * [max_output_tokens][google.ai.generativelanguage.v1beta.GenerationConfig.max_output_tokens]
   * limit: "...the the the the the...".
   */
  frequencyPenalty?:
    | number
    | undefined;
  /** Optional. If true, export the logprobs results in response. */
  responseLogprobs?:
    | boolean
    | undefined;
  /**
   * Optional. Only valid if
   * [response_logprobs=True][google.ai.generativelanguage.v1beta.GenerationConfig.response_logprobs].
   * This sets the number of top logprobs to return at each decoding step in the
   * [Candidate.logprobs_result][google.ai.generativelanguage.v1beta.Candidate.logprobs_result].
   */
  logprobs?: number | undefined;
}

/**
 * Configuration for retrieving grounding content from a `Corpus` or
 * `Document` created using the Semantic Retriever API.
 */
export interface SemanticRetrieverConfig {
  /**
   * Required. Name of the resource for retrieval. Example: `corpora/123` or
   * `corpora/123/documents/abc`.
   */
  source: string;
  /**
   * Required. Query to use for matching `Chunk`s in the given resource by
   * similarity.
   */
  query:
    | Content
    | undefined;
  /**
   * Optional. Filters for selecting `Document`s and/or `Chunk`s from the
   * resource.
   */
  metadataFilters: MetadataFilter[];
  /** Optional. Maximum number of relevant `Chunk`s to retrieve. */
  maxChunksCount?:
    | number
    | undefined;
  /** Optional. Minimum relevance score for retrieved relevant `Chunk`s. */
  minimumRelevanceScore?: number | undefined;
}

/**
 * Response from the model supporting multiple candidate responses.
 *
 * Safety ratings and content filtering are reported for both
 * prompt in `GenerateContentResponse.prompt_feedback` and for each candidate
 * in `finish_reason` and in `safety_ratings`. The API:
 *  - Returns either all requested candidates or none of them
 *  - Returns no candidates at all only if there was something wrong with the
 *    prompt (check `prompt_feedback`)
 *  - Reports feedback on each candidate in `finish_reason` and
 *    `safety_ratings`.
 */
export interface GenerateContentResponse {
  /** Candidate responses from the model. */
  candidates: Candidate[];
  /** Returns the prompt's feedback related to the content filters. */
  promptFeedback:
    | GenerateContentResponse_PromptFeedback
    | undefined;
  /** Output only. Metadata on the generation requests' token usage. */
  usageMetadata: GenerateContentResponse_UsageMetadata | undefined;
}

/**
 * A set of the feedback metadata the prompt specified in
 * `GenerateContentRequest.content`.
 */
export interface GenerateContentResponse_PromptFeedback {
  /**
   * Optional. If set, the prompt was blocked and no candidates are returned.
   * Rephrase the prompt.
   */
  blockReason: GenerateContentResponse_PromptFeedback_BlockReason;
  /**
   * Ratings for safety of the prompt.
   * There is at most one rating per category.
   */
  safetyRatings: SafetyRating[];
}

/** Specifies the reason why the prompt was blocked. */
export enum GenerateContentResponse_PromptFeedback_BlockReason {
  /** BLOCK_REASON_UNSPECIFIED - Default value. This value is unused. */
  BLOCK_REASON_UNSPECIFIED = 0,
  /**
   * SAFETY - Prompt was blocked due to safety reasons. Inspect `safety_ratings`
   * to understand which safety category blocked it.
   */
  SAFETY = 1,
  /** OTHER - Prompt was blocked due to unknown reasons. */
  OTHER = 2,
  /**
   * BLOCKLIST - Prompt was blocked due to the terms which are included from the
   * terminology blocklist.
   */
  BLOCKLIST = 3,
  /** PROHIBITED_CONTENT - Prompt was blocked due to prohibited content. */
  PROHIBITED_CONTENT = 4,
  UNRECOGNIZED = -1,
}

export function generateContentResponse_PromptFeedback_BlockReasonFromJSON(
  object: any,
): GenerateContentResponse_PromptFeedback_BlockReason {
  switch (object) {
    case 0:
    case "BLOCK_REASON_UNSPECIFIED":
      return GenerateContentResponse_PromptFeedback_BlockReason.BLOCK_REASON_UNSPECIFIED;
    case 1:
    case "SAFETY":
      return GenerateContentResponse_PromptFeedback_BlockReason.SAFETY;
    case 2:
    case "OTHER":
      return GenerateContentResponse_PromptFeedback_BlockReason.OTHER;
    case 3:
    case "BLOCKLIST":
      return GenerateContentResponse_PromptFeedback_BlockReason.BLOCKLIST;
    case 4:
    case "PROHIBITED_CONTENT":
      return GenerateContentResponse_PromptFeedback_BlockReason.PROHIBITED_CONTENT;
    case -1:
    case "UNRECOGNIZED":
    default:
      return GenerateContentResponse_PromptFeedback_BlockReason.UNRECOGNIZED;
  }
}

export function generateContentResponse_PromptFeedback_BlockReasonToJSON(
  object: GenerateContentResponse_PromptFeedback_BlockReason,
): string {
  switch (object) {
    case GenerateContentResponse_PromptFeedback_BlockReason.BLOCK_REASON_UNSPECIFIED:
      return "BLOCK_REASON_UNSPECIFIED";
    case GenerateContentResponse_PromptFeedback_BlockReason.SAFETY:
      return "SAFETY";
    case GenerateContentResponse_PromptFeedback_BlockReason.OTHER:
      return "OTHER";
    case GenerateContentResponse_PromptFeedback_BlockReason.BLOCKLIST:
      return "BLOCKLIST";
    case GenerateContentResponse_PromptFeedback_BlockReason.PROHIBITED_CONTENT:
      return "PROHIBITED_CONTENT";
    case GenerateContentResponse_PromptFeedback_BlockReason.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Metadata on the generation request's token usage. */
export interface GenerateContentResponse_UsageMetadata {
  /**
   * Number of tokens in the prompt. When `cached_content` is set, this is
   * still the total effective prompt size meaning this includes the number of
   * tokens in the cached content.
   */
  promptTokenCount: number;
  /** Number of tokens in the cached part of the prompt (the cached content) */
  cachedContentTokenCount: number;
  /** Total number of tokens across all the generated response candidates. */
  candidatesTokenCount: number;
  /**
   * Total token count for the generation request (prompt + response
   * candidates).
   */
  totalTokenCount: number;
}

/** A response candidate generated from the model. */
export interface Candidate {
  /** Output only. Index of the candidate in the list of response candidates. */
  index?:
    | number
    | undefined;
  /** Output only. Generated content returned from the model. */
  content:
    | Content
    | undefined;
  /**
   * Optional. Output only. The reason why the model stopped generating tokens.
   *
   * If empty, the model has not stopped generating tokens.
   */
  finishReason: Candidate_FinishReason;
  /**
   * List of ratings for the safety of a response candidate.
   *
   * There is at most one rating per category.
   */
  safetyRatings: SafetyRating[];
  /**
   * Output only. Citation information for model-generated candidate.
   *
   * This field may be populated with recitation information for any text
   * included in the `content`. These are passages that are "recited" from
   * copyrighted material in the foundational LLM's training data.
   */
  citationMetadata:
    | CitationMetadata
    | undefined;
  /** Output only. Token count for this candidate. */
  tokenCount: number;
  /**
   * Output only. Attribution information for sources that contributed to a
   * grounded answer.
   *
   * This field is populated for `GenerateAnswer` calls.
   */
  groundingAttributions: GroundingAttribution[];
  /**
   * Output only. Grounding metadata for the candidate.
   *
   * This field is populated for `GenerateContent` calls.
   */
  groundingMetadata:
    | GroundingMetadata
    | undefined;
  /** Output only. */
  avgLogprobs: number;
  /** Output only. Log-likelihood scores for the response tokens and top tokens */
  logprobsResult: LogprobsResult | undefined;
}

/** Defines the reason why the model stopped generating tokens. */
export enum Candidate_FinishReason {
  /** FINISH_REASON_UNSPECIFIED - Default value. This value is unused. */
  FINISH_REASON_UNSPECIFIED = 0,
  /** STOP - Natural stop point of the model or provided stop sequence. */
  STOP = 1,
  /** MAX_TOKENS - The maximum number of tokens as specified in the request was reached. */
  MAX_TOKENS = 2,
  /** SAFETY - The response candidate content was flagged for safety reasons. */
  SAFETY = 3,
  /** RECITATION - The response candidate content was flagged for recitation reasons. */
  RECITATION = 4,
  /**
   * LANGUAGE - The response candidate content was flagged for using an unsupported
   * language.
   */
  LANGUAGE = 6,
  /** OTHER - Unknown reason. */
  OTHER = 5,
  /** BLOCKLIST - Token generation stopped because the content contains forbidden terms. */
  BLOCKLIST = 7,
  /** PROHIBITED_CONTENT - Token generation stopped for potentially containing prohibited content. */
  PROHIBITED_CONTENT = 8,
  /**
   * SPII - Token generation stopped because the content potentially contains
   * Sensitive Personally Identifiable Information (SPII).
   */
  SPII = 9,
  /** MALFORMED_FUNCTION_CALL - The function call generated by the model is invalid. */
  MALFORMED_FUNCTION_CALL = 10,
  UNRECOGNIZED = -1,
}

export function candidate_FinishReasonFromJSON(object: any): Candidate_FinishReason {
  switch (object) {
    case 0:
    case "FINISH_REASON_UNSPECIFIED":
      return Candidate_FinishReason.FINISH_REASON_UNSPECIFIED;
    case 1:
    case "STOP":
      return Candidate_FinishReason.STOP;
    case 2:
    case "MAX_TOKENS":
      return Candidate_FinishReason.MAX_TOKENS;
    case 3:
    case "SAFETY":
      return Candidate_FinishReason.SAFETY;
    case 4:
    case "RECITATION":
      return Candidate_FinishReason.RECITATION;
    case 6:
    case "LANGUAGE":
      return Candidate_FinishReason.LANGUAGE;
    case 5:
    case "OTHER":
      return Candidate_FinishReason.OTHER;
    case 7:
    case "BLOCKLIST":
      return Candidate_FinishReason.BLOCKLIST;
    case 8:
    case "PROHIBITED_CONTENT":
      return Candidate_FinishReason.PROHIBITED_CONTENT;
    case 9:
    case "SPII":
      return Candidate_FinishReason.SPII;
    case 10:
    case "MALFORMED_FUNCTION_CALL":
      return Candidate_FinishReason.MALFORMED_FUNCTION_CALL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Candidate_FinishReason.UNRECOGNIZED;
  }
}

export function candidate_FinishReasonToJSON(object: Candidate_FinishReason): string {
  switch (object) {
    case Candidate_FinishReason.FINISH_REASON_UNSPECIFIED:
      return "FINISH_REASON_UNSPECIFIED";
    case Candidate_FinishReason.STOP:
      return "STOP";
    case Candidate_FinishReason.MAX_TOKENS:
      return "MAX_TOKENS";
    case Candidate_FinishReason.SAFETY:
      return "SAFETY";
    case Candidate_FinishReason.RECITATION:
      return "RECITATION";
    case Candidate_FinishReason.LANGUAGE:
      return "LANGUAGE";
    case Candidate_FinishReason.OTHER:
      return "OTHER";
    case Candidate_FinishReason.BLOCKLIST:
      return "BLOCKLIST";
    case Candidate_FinishReason.PROHIBITED_CONTENT:
      return "PROHIBITED_CONTENT";
    case Candidate_FinishReason.SPII:
      return "SPII";
    case Candidate_FinishReason.MALFORMED_FUNCTION_CALL:
      return "MALFORMED_FUNCTION_CALL";
    case Candidate_FinishReason.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Logprobs Result */
export interface LogprobsResult {
  /** Length = total number of decoding steps. */
  topCandidates: LogprobsResult_TopCandidates[];
  /**
   * Length = total number of decoding steps.
   * The chosen candidates may or may not be in top_candidates.
   */
  chosenCandidates: LogprobsResult_Candidate[];
}

/** Candidate for the logprobs token and score. */
export interface LogprobsResult_Candidate {
  /** The candidate’s token string value. */
  token?:
    | string
    | undefined;
  /** The candidate’s token id value. */
  tokenId?:
    | number
    | undefined;
  /** The candidate's log probability. */
  logProbability?: number | undefined;
}

/** Candidates with top log probabilities at each decoding step. */
export interface LogprobsResult_TopCandidates {
  /** Sorted by log probability in descending order. */
  candidates: LogprobsResult_Candidate[];
}

/** Identifier for the source contributing to this attribution. */
export interface AttributionSourceId {
  /** Identifier for an inline passage. */
  groundingPassage?:
    | AttributionSourceId_GroundingPassageId
    | undefined;
  /** Identifier for a `Chunk` fetched via Semantic Retriever. */
  semanticRetrieverChunk?: AttributionSourceId_SemanticRetrieverChunk | undefined;
}

/** Identifier for a part within a `GroundingPassage`. */
export interface AttributionSourceId_GroundingPassageId {
  /**
   * Output only. ID of the passage matching the `GenerateAnswerRequest`'s
   * `GroundingPassage.id`.
   */
  passageId: string;
  /**
   * Output only. Index of the part within the `GenerateAnswerRequest`'s
   * `GroundingPassage.content`.
   */
  partIndex: number;
}

/**
 * Identifier for a `Chunk` retrieved via Semantic Retriever specified in the
 * `GenerateAnswerRequest` using `SemanticRetrieverConfig`.
 */
export interface AttributionSourceId_SemanticRetrieverChunk {
  /**
   * Output only. Name of the source matching the request's
   * `SemanticRetrieverConfig.source`. Example: `corpora/123` or
   * `corpora/123/documents/abc`
   */
  source: string;
  /**
   * Output only. Name of the `Chunk` containing the attributed text.
   * Example: `corpora/123/documents/abc/chunks/xyz`
   */
  chunk: string;
}

/** Attribution for a source that contributed to an answer. */
export interface GroundingAttribution {
  /** Output only. Identifier for the source contributing to this attribution. */
  sourceId:
    | AttributionSourceId
    | undefined;
  /** Grounding source content that makes up this attribution. */
  content: Content | undefined;
}

/** Metadata related to retrieval in the grounding flow. */
export interface RetrievalMetadata {
  /**
   * Optional. Score indicating how likely information from google search could
   * help answer the prompt. The score is in the range [0, 1], where 0 is the
   * least likely and 1 is the most likely. This score is only populated when
   * google search grounding and dynamic retrieval is enabled. It will be
   * compared to the threshold to determine whether to trigger google search.
   */
  googleSearchDynamicRetrievalScore: number;
}

/** Metadata returned to client when grounding is enabled. */
export interface GroundingMetadata {
  /** Optional. Google search entry for the following-up web searches. */
  searchEntryPoint?:
    | SearchEntryPoint
    | undefined;
  /** List of supporting references retrieved from specified grounding source. */
  groundingChunks: GroundingChunk[];
  /** List of grounding support. */
  groundingSupports: GroundingSupport[];
  /** Metadata related to retrieval in the grounding flow. */
  retrievalMetadata?: RetrievalMetadata | undefined;
}

/** Google search entry point. */
export interface SearchEntryPoint {
  /**
   * Optional. Web content snippet that can be embedded in a web page or an app
   * webview.
   */
  renderedContent: string;
  /**
   * Optional. Base64 encoded JSON representing array of <search term, search
   * url> tuple.
   */
  sdkBlob: Buffer;
}

/** Grounding chunk. */
export interface GroundingChunk {
  /** Grounding chunk from the web. */
  web?: GroundingChunk_Web | undefined;
}

/** Chunk from the web. */
export interface GroundingChunk_Web {
  /** URI reference of the chunk. */
  uri?:
    | string
    | undefined;
  /** Title of the chunk. */
  title?: string | undefined;
}

/** Segment of the content. */
export interface Segment {
  /** Output only. The index of a Part object within its parent Content object. */
  partIndex: number;
  /**
   * Output only. Start index in the given Part, measured in bytes. Offset from
   * the start of the Part, inclusive, starting at zero.
   */
  startIndex: number;
  /**
   * Output only. End index in the given Part, measured in bytes. Offset from
   * the start of the Part, exclusive, starting at zero.
   */
  endIndex: number;
  /** Output only. The text corresponding to the segment from the response. */
  text: string;
}

/** Grounding support. */
export interface GroundingSupport {
  /** Segment of the content this support belongs to. */
  segment?:
    | Segment
    | undefined;
  /**
   * A list of indices (into 'grounding_chunk') specifying the
   * citations associated with the claim. For instance [1,3,4] means
   * that grounding_chunk[1], grounding_chunk[3],
   * grounding_chunk[4] are the retrieved content attributed to the claim.
   */
  groundingChunkIndices: number[];
  /**
   * Confidence score of the support references. Ranges from 0 to 1. 1 is the
   * most confident. This list must have the same size as the
   * grounding_chunk_indices.
   */
  confidenceScores: number[];
}

/** Request to generate a grounded answer from the `Model`. */
export interface GenerateAnswerRequest {
  /** Passages provided inline with the request. */
  inlinePassages?:
    | GroundingPassages
    | undefined;
  /**
   * Content retrieved from resources created via the Semantic Retriever
   * API.
   */
  semanticRetriever?:
    | SemanticRetrieverConfig
    | undefined;
  /**
   * Required. The name of the `Model` to use for generating the grounded
   * response.
   *
   * Format: `model=models/{model}`.
   */
  model: string;
  /**
   * Required. The content of the current conversation with the `Model`. For
   * single-turn queries, this is a single question to answer. For multi-turn
   * queries, this is a repeated field that contains conversation history and
   * the last `Content` in the list containing the question.
   *
   * Note: `GenerateAnswer` only supports queries in English.
   */
  contents: Content[];
  /** Required. Style in which answers should be returned. */
  answerStyle: GenerateAnswerRequest_AnswerStyle;
  /**
   * Optional. A list of unique `SafetySetting` instances for blocking unsafe
   * content.
   *
   * This will be enforced on the `GenerateAnswerRequest.contents` and
   * `GenerateAnswerResponse.candidate`. There should not be more than one
   * setting for each `SafetyCategory` type. The API will block any contents and
   * responses that fail to meet the thresholds set by these settings. This list
   * overrides the default settings for each `SafetyCategory` specified in the
   * safety_settings. If there is no `SafetySetting` for a given
   * `SafetyCategory` provided in the list, the API will use the default safety
   * setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH,
   * HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT,
   * HARM_CATEGORY_HARASSMENT are supported.
   * Refer to the
   * [guide](https://ai.google.dev/gemini-api/docs/safety-settings)
   * for detailed information on available safety settings. Also refer to the
   * [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance) to
   * learn how to incorporate safety considerations in your AI applications.
   */
  safetySettings: SafetySetting[];
  /**
   * Optional. Controls the randomness of the output.
   *
   * Values can range from [0.0,1.0], inclusive. A value closer to 1.0 will
   * produce responses that are more varied and creative, while a value closer
   * to 0.0 will typically result in more straightforward responses from the
   * model. A low temperature (~0.2) is usually recommended for
   * Attributed-Question-Answering use cases.
   */
  temperature?: number | undefined;
}

/** Style for grounded answers. */
export enum GenerateAnswerRequest_AnswerStyle {
  /** ANSWER_STYLE_UNSPECIFIED - Unspecified answer style. */
  ANSWER_STYLE_UNSPECIFIED = 0,
  /** ABSTRACTIVE - Succint but abstract style. */
  ABSTRACTIVE = 1,
  /** EXTRACTIVE - Very brief and extractive style. */
  EXTRACTIVE = 2,
  /**
   * VERBOSE - Verbose style including extra details. The response may be formatted as a
   * sentence, paragraph, multiple paragraphs, or bullet points, etc.
   */
  VERBOSE = 3,
  UNRECOGNIZED = -1,
}

export function generateAnswerRequest_AnswerStyleFromJSON(object: any): GenerateAnswerRequest_AnswerStyle {
  switch (object) {
    case 0:
    case "ANSWER_STYLE_UNSPECIFIED":
      return GenerateAnswerRequest_AnswerStyle.ANSWER_STYLE_UNSPECIFIED;
    case 1:
    case "ABSTRACTIVE":
      return GenerateAnswerRequest_AnswerStyle.ABSTRACTIVE;
    case 2:
    case "EXTRACTIVE":
      return GenerateAnswerRequest_AnswerStyle.EXTRACTIVE;
    case 3:
    case "VERBOSE":
      return GenerateAnswerRequest_AnswerStyle.VERBOSE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return GenerateAnswerRequest_AnswerStyle.UNRECOGNIZED;
  }
}

export function generateAnswerRequest_AnswerStyleToJSON(object: GenerateAnswerRequest_AnswerStyle): string {
  switch (object) {
    case GenerateAnswerRequest_AnswerStyle.ANSWER_STYLE_UNSPECIFIED:
      return "ANSWER_STYLE_UNSPECIFIED";
    case GenerateAnswerRequest_AnswerStyle.ABSTRACTIVE:
      return "ABSTRACTIVE";
    case GenerateAnswerRequest_AnswerStyle.EXTRACTIVE:
      return "EXTRACTIVE";
    case GenerateAnswerRequest_AnswerStyle.VERBOSE:
      return "VERBOSE";
    case GenerateAnswerRequest_AnswerStyle.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Response from the model for a grounded answer. */
export interface GenerateAnswerResponse {
  /**
   * Candidate answer from the model.
   *
   * Note: The model *always* attempts to provide a grounded answer, even when
   * the answer is unlikely to be answerable from the given passages.
   * In that case, a low-quality or ungrounded answer may be provided, along
   * with a low `answerable_probability`.
   */
  answer:
    | Candidate
    | undefined;
  /**
   * Output only. The model's estimate of the probability that its answer is
   * correct and grounded in the input passages.
   *
   * A low `answerable_probability` indicates that the answer might not be
   * grounded in the sources.
   *
   * When `answerable_probability` is low, you may want to:
   *
   * * Display a message to the effect of "We couldn’t answer that question" to
   * the user.
   * * Fall back to a general-purpose LLM that answers the question from world
   * knowledge. The threshold and nature of such fallbacks will depend on
   * individual use cases. `0.5` is a good starting threshold.
   */
  answerableProbability?:
    | number
    | undefined;
  /**
   * Output only. Feedback related to the input data used to answer the
   * question, as opposed to the model-generated response to the question.
   *
   * The input data can be one or more of the following:
   *
   * - Question specified by the last entry in `GenerateAnswerRequest.content`
   * - Conversation history specified by the other entries in
   * `GenerateAnswerRequest.content`
   * - Grounding sources (`GenerateAnswerRequest.semantic_retriever` or
   * `GenerateAnswerRequest.inline_passages`)
   */
  inputFeedback?: GenerateAnswerResponse_InputFeedback | undefined;
}

/**
 * Feedback related to the input data used to answer the question, as opposed
 * to the model-generated response to the question.
 */
export interface GenerateAnswerResponse_InputFeedback {
  /**
   * Optional. If set, the input was blocked and no candidates are returned.
   * Rephrase the input.
   */
  blockReason?:
    | GenerateAnswerResponse_InputFeedback_BlockReason
    | undefined;
  /**
   * Ratings for safety of the input.
   * There is at most one rating per category.
   */
  safetyRatings: SafetyRating[];
}

/** Specifies what was the reason why input was blocked. */
export enum GenerateAnswerResponse_InputFeedback_BlockReason {
  /** BLOCK_REASON_UNSPECIFIED - Default value. This value is unused. */
  BLOCK_REASON_UNSPECIFIED = 0,
  /**
   * SAFETY - Input was blocked due to safety reasons. Inspect
   * `safety_ratings` to understand which safety category blocked it.
   */
  SAFETY = 1,
  /** OTHER - Input was blocked due to other reasons. */
  OTHER = 2,
  UNRECOGNIZED = -1,
}

export function generateAnswerResponse_InputFeedback_BlockReasonFromJSON(
  object: any,
): GenerateAnswerResponse_InputFeedback_BlockReason {
  switch (object) {
    case 0:
    case "BLOCK_REASON_UNSPECIFIED":
      return GenerateAnswerResponse_InputFeedback_BlockReason.BLOCK_REASON_UNSPECIFIED;
    case 1:
    case "SAFETY":
      return GenerateAnswerResponse_InputFeedback_BlockReason.SAFETY;
    case 2:
    case "OTHER":
      return GenerateAnswerResponse_InputFeedback_BlockReason.OTHER;
    case -1:
    case "UNRECOGNIZED":
    default:
      return GenerateAnswerResponse_InputFeedback_BlockReason.UNRECOGNIZED;
  }
}

export function generateAnswerResponse_InputFeedback_BlockReasonToJSON(
  object: GenerateAnswerResponse_InputFeedback_BlockReason,
): string {
  switch (object) {
    case GenerateAnswerResponse_InputFeedback_BlockReason.BLOCK_REASON_UNSPECIFIED:
      return "BLOCK_REASON_UNSPECIFIED";
    case GenerateAnswerResponse_InputFeedback_BlockReason.SAFETY:
      return "SAFETY";
    case GenerateAnswerResponse_InputFeedback_BlockReason.OTHER:
      return "OTHER";
    case GenerateAnswerResponse_InputFeedback_BlockReason.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Request containing the `Content` for the model to embed. */
export interface EmbedContentRequest {
  /**
   * Required. The model's resource name. This serves as an ID for the Model to
   * use.
   *
   * This name should match a model name returned by the `ListModels` method.
   *
   * Format: `models/{model}`
   */
  model: string;
  /**
   * Required. The content to embed. Only the `parts.text` fields will be
   * counted.
   */
  content:
    | Content
    | undefined;
  /**
   * Optional. Optional task type for which the embeddings will be used. Can
   * only be set for `models/embedding-001`.
   */
  taskType?:
    | TaskType
    | undefined;
  /**
   * Optional. An optional title for the text. Only applicable when TaskType is
   * `RETRIEVAL_DOCUMENT`.
   *
   * Note: Specifying a `title` for `RETRIEVAL_DOCUMENT` provides better quality
   * embeddings for retrieval.
   */
  title?:
    | string
    | undefined;
  /**
   * Optional. Optional reduced dimension for the output embedding. If set,
   * excessive values in the output embedding are truncated from the end.
   * Supported by newer models since 2024 only. You cannot set this value if
   * using the earlier model (`models/embedding-001`).
   */
  outputDimensionality?: number | undefined;
}

/** A list of floats representing an embedding. */
export interface ContentEmbedding {
  /** The embedding values. */
  values: number[];
}

/** The response to an `EmbedContentRequest`. */
export interface EmbedContentResponse {
  /** Output only. The embedding generated from the input content. */
  embedding: ContentEmbedding | undefined;
}

/** Batch request to get embeddings from the model for a list of prompts. */
export interface BatchEmbedContentsRequest {
  /**
   * Required. The model's resource name. This serves as an ID for the Model to
   * use.
   *
   * This name should match a model name returned by the `ListModels` method.
   *
   * Format: `models/{model}`
   */
  model: string;
  /**
   * Required. Embed requests for the batch. The model in each of these requests
   * must match the model specified `BatchEmbedContentsRequest.model`.
   */
  requests: EmbedContentRequest[];
}

/** The response to a `BatchEmbedContentsRequest`. */
export interface BatchEmbedContentsResponse {
  /**
   * Output only. The embeddings for each request, in the same order as provided
   * in the batch request.
   */
  embeddings: ContentEmbedding[];
}

/**
 * Counts the number of tokens in the `prompt` sent to a model.
 *
 * Models may tokenize text differently, so each model may return a different
 * `token_count`.
 */
export interface CountTokensRequest {
  /**
   * Required. The model's resource name. This serves as an ID for the Model to
   * use.
   *
   * This name should match a model name returned by the `ListModels` method.
   *
   * Format: `models/{model}`
   */
  model: string;
  /**
   * Optional. The input given to the model as a prompt. This field is ignored
   * when `generate_content_request` is set.
   */
  contents: Content[];
  /**
   * Optional. The overall input given to the `Model`. This includes the prompt
   * as well as other model steering information like [system
   * instructions](https://ai.google.dev/gemini-api/docs/system-instructions),
   * and/or function declarations for [function
   * calling](https://ai.google.dev/gemini-api/docs/function-calling).
   * `Model`s/`Content`s and `generate_content_request`s are mutually
   * exclusive. You can either send `Model` + `Content`s or a
   * `generate_content_request`, but never both.
   */
  generateContentRequest: GenerateContentRequest | undefined;
}

/**
 * A response from `CountTokens`.
 *
 * It returns the model's `token_count` for the `prompt`.
 */
export interface CountTokensResponse {
  /**
   * The number of tokens that the `Model` tokenizes the `prompt` into. Always
   * non-negative.
   */
  totalTokens: number;
  /** Number of tokens in the cached part of the prompt (the cached content). */
  cachedContentTokenCount: number;
}

function createBaseGenerateContentRequest(): GenerateContentRequest {
  return {
    model: "",
    systemInstruction: undefined,
    contents: [],
    tools: [],
    toolConfig: undefined,
    safetySettings: [],
    generationConfig: undefined,
    cachedContent: undefined,
  };
}

export const GenerateContentRequest: MessageFns<GenerateContentRequest> = {
  encode(message: GenerateContentRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    if (message.systemInstruction !== undefined) {
      Content.encode(message.systemInstruction, writer.uint32(66).fork()).join();
    }
    for (const v of message.contents) {
      Content.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.tools) {
      Tool.encode(v!, writer.uint32(42).fork()).join();
    }
    if (message.toolConfig !== undefined) {
      ToolConfig.encode(message.toolConfig, writer.uint32(58).fork()).join();
    }
    for (const v of message.safetySettings) {
      SafetySetting.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.generationConfig !== undefined) {
      GenerationConfig.encode(message.generationConfig, writer.uint32(34).fork()).join();
    }
    if (message.cachedContent !== undefined) {
      writer.uint32(74).string(message.cachedContent);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerateContentRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerateContentRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.systemInstruction = Content.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.contents.push(Content.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.tools.push(Tool.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.toolConfig = ToolConfig.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.safetySettings.push(SafetySetting.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.generationConfig = GenerationConfig.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.cachedContent = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerateContentRequest {
    return {
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      systemInstruction: isSet(object.systemInstruction) ? Content.fromJSON(object.systemInstruction) : undefined,
      contents: globalThis.Array.isArray(object?.contents) ? object.contents.map((e: any) => Content.fromJSON(e)) : [],
      tools: globalThis.Array.isArray(object?.tools) ? object.tools.map((e: any) => Tool.fromJSON(e)) : [],
      toolConfig: isSet(object.toolConfig) ? ToolConfig.fromJSON(object.toolConfig) : undefined,
      safetySettings: globalThis.Array.isArray(object?.safetySettings)
        ? object.safetySettings.map((e: any) => SafetySetting.fromJSON(e))
        : [],
      generationConfig: isSet(object.generationConfig) ? GenerationConfig.fromJSON(object.generationConfig) : undefined,
      cachedContent: isSet(object.cachedContent) ? globalThis.String(object.cachedContent) : undefined,
    };
  },

  toJSON(message: GenerateContentRequest): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.systemInstruction !== undefined) {
      obj.systemInstruction = Content.toJSON(message.systemInstruction);
    }
    if (message.contents?.length) {
      obj.contents = message.contents.map((e) => Content.toJSON(e));
    }
    if (message.tools?.length) {
      obj.tools = message.tools.map((e) => Tool.toJSON(e));
    }
    if (message.toolConfig !== undefined) {
      obj.toolConfig = ToolConfig.toJSON(message.toolConfig);
    }
    if (message.safetySettings?.length) {
      obj.safetySettings = message.safetySettings.map((e) => SafetySetting.toJSON(e));
    }
    if (message.generationConfig !== undefined) {
      obj.generationConfig = GenerationConfig.toJSON(message.generationConfig);
    }
    if (message.cachedContent !== undefined) {
      obj.cachedContent = message.cachedContent;
    }
    return obj;
  },

  create(base?: DeepPartial<GenerateContentRequest>): GenerateContentRequest {
    return GenerateContentRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerateContentRequest>): GenerateContentRequest {
    const message = createBaseGenerateContentRequest();
    message.model = object.model ?? "";
    message.systemInstruction = (object.systemInstruction !== undefined && object.systemInstruction !== null)
      ? Content.fromPartial(object.systemInstruction)
      : undefined;
    message.contents = object.contents?.map((e) => Content.fromPartial(e)) || [];
    message.tools = object.tools?.map((e) => Tool.fromPartial(e)) || [];
    message.toolConfig = (object.toolConfig !== undefined && object.toolConfig !== null)
      ? ToolConfig.fromPartial(object.toolConfig)
      : undefined;
    message.safetySettings = object.safetySettings?.map((e) => SafetySetting.fromPartial(e)) || [];
    message.generationConfig = (object.generationConfig !== undefined && object.generationConfig !== null)
      ? GenerationConfig.fromPartial(object.generationConfig)
      : undefined;
    message.cachedContent = object.cachedContent ?? undefined;
    return message;
  },
};

function createBaseGenerationConfig(): GenerationConfig {
  return {
    candidateCount: undefined,
    stopSequences: [],
    maxOutputTokens: undefined,
    temperature: undefined,
    topP: undefined,
    topK: undefined,
    responseMimeType: "",
    responseSchema: undefined,
    presencePenalty: undefined,
    frequencyPenalty: undefined,
    responseLogprobs: undefined,
    logprobs: undefined,
  };
}

export const GenerationConfig: MessageFns<GenerationConfig> = {
  encode(message: GenerationConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.candidateCount !== undefined) {
      writer.uint32(8).int32(message.candidateCount);
    }
    for (const v of message.stopSequences) {
      writer.uint32(18).string(v!);
    }
    if (message.maxOutputTokens !== undefined) {
      writer.uint32(32).int32(message.maxOutputTokens);
    }
    if (message.temperature !== undefined) {
      writer.uint32(45).float(message.temperature);
    }
    if (message.topP !== undefined) {
      writer.uint32(53).float(message.topP);
    }
    if (message.topK !== undefined) {
      writer.uint32(56).int32(message.topK);
    }
    if (message.responseMimeType !== "") {
      writer.uint32(106).string(message.responseMimeType);
    }
    if (message.responseSchema !== undefined) {
      Schema.encode(message.responseSchema, writer.uint32(114).fork()).join();
    }
    if (message.presencePenalty !== undefined) {
      writer.uint32(125).float(message.presencePenalty);
    }
    if (message.frequencyPenalty !== undefined) {
      writer.uint32(133).float(message.frequencyPenalty);
    }
    if (message.responseLogprobs !== undefined) {
      writer.uint32(136).bool(message.responseLogprobs);
    }
    if (message.logprobs !== undefined) {
      writer.uint32(144).int32(message.logprobs);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerationConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerationConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.candidateCount = reader.int32();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.stopSequences.push(reader.string());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.maxOutputTokens = reader.int32();
          continue;
        case 5:
          if (tag !== 45) {
            break;
          }

          message.temperature = reader.float();
          continue;
        case 6:
          if (tag !== 53) {
            break;
          }

          message.topP = reader.float();
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.topK = reader.int32();
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.responseMimeType = reader.string();
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.responseSchema = Schema.decode(reader, reader.uint32());
          continue;
        case 15:
          if (tag !== 125) {
            break;
          }

          message.presencePenalty = reader.float();
          continue;
        case 16:
          if (tag !== 133) {
            break;
          }

          message.frequencyPenalty = reader.float();
          continue;
        case 17:
          if (tag !== 136) {
            break;
          }

          message.responseLogprobs = reader.bool();
          continue;
        case 18:
          if (tag !== 144) {
            break;
          }

          message.logprobs = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerationConfig {
    return {
      candidateCount: isSet(object.candidateCount) ? globalThis.Number(object.candidateCount) : undefined,
      stopSequences: globalThis.Array.isArray(object?.stopSequences)
        ? object.stopSequences.map((e: any) => globalThis.String(e))
        : [],
      maxOutputTokens: isSet(object.maxOutputTokens) ? globalThis.Number(object.maxOutputTokens) : undefined,
      temperature: isSet(object.temperature) ? globalThis.Number(object.temperature) : undefined,
      topP: isSet(object.topP) ? globalThis.Number(object.topP) : undefined,
      topK: isSet(object.topK) ? globalThis.Number(object.topK) : undefined,
      responseMimeType: isSet(object.responseMimeType) ? globalThis.String(object.responseMimeType) : "",
      responseSchema: isSet(object.responseSchema) ? Schema.fromJSON(object.responseSchema) : undefined,
      presencePenalty: isSet(object.presencePenalty) ? globalThis.Number(object.presencePenalty) : undefined,
      frequencyPenalty: isSet(object.frequencyPenalty) ? globalThis.Number(object.frequencyPenalty) : undefined,
      responseLogprobs: isSet(object.responseLogprobs) ? globalThis.Boolean(object.responseLogprobs) : undefined,
      logprobs: isSet(object.logprobs) ? globalThis.Number(object.logprobs) : undefined,
    };
  },

  toJSON(message: GenerationConfig): unknown {
    const obj: any = {};
    if (message.candidateCount !== undefined) {
      obj.candidateCount = Math.round(message.candidateCount);
    }
    if (message.stopSequences?.length) {
      obj.stopSequences = message.stopSequences;
    }
    if (message.maxOutputTokens !== undefined) {
      obj.maxOutputTokens = Math.round(message.maxOutputTokens);
    }
    if (message.temperature !== undefined) {
      obj.temperature = message.temperature;
    }
    if (message.topP !== undefined) {
      obj.topP = message.topP;
    }
    if (message.topK !== undefined) {
      obj.topK = Math.round(message.topK);
    }
    if (message.responseMimeType !== "") {
      obj.responseMimeType = message.responseMimeType;
    }
    if (message.responseSchema !== undefined) {
      obj.responseSchema = Schema.toJSON(message.responseSchema);
    }
    if (message.presencePenalty !== undefined) {
      obj.presencePenalty = message.presencePenalty;
    }
    if (message.frequencyPenalty !== undefined) {
      obj.frequencyPenalty = message.frequencyPenalty;
    }
    if (message.responseLogprobs !== undefined) {
      obj.responseLogprobs = message.responseLogprobs;
    }
    if (message.logprobs !== undefined) {
      obj.logprobs = Math.round(message.logprobs);
    }
    return obj;
  },

  create(base?: DeepPartial<GenerationConfig>): GenerationConfig {
    return GenerationConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerationConfig>): GenerationConfig {
    const message = createBaseGenerationConfig();
    message.candidateCount = object.candidateCount ?? undefined;
    message.stopSequences = object.stopSequences?.map((e) => e) || [];
    message.maxOutputTokens = object.maxOutputTokens ?? undefined;
    message.temperature = object.temperature ?? undefined;
    message.topP = object.topP ?? undefined;
    message.topK = object.topK ?? undefined;
    message.responseMimeType = object.responseMimeType ?? "";
    message.responseSchema = (object.responseSchema !== undefined && object.responseSchema !== null)
      ? Schema.fromPartial(object.responseSchema)
      : undefined;
    message.presencePenalty = object.presencePenalty ?? undefined;
    message.frequencyPenalty = object.frequencyPenalty ?? undefined;
    message.responseLogprobs = object.responseLogprobs ?? undefined;
    message.logprobs = object.logprobs ?? undefined;
    return message;
  },
};

function createBaseSemanticRetrieverConfig(): SemanticRetrieverConfig {
  return {
    source: "",
    query: undefined,
    metadataFilters: [],
    maxChunksCount: undefined,
    minimumRelevanceScore: undefined,
  };
}

export const SemanticRetrieverConfig: MessageFns<SemanticRetrieverConfig> = {
  encode(message: SemanticRetrieverConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.source !== "") {
      writer.uint32(10).string(message.source);
    }
    if (message.query !== undefined) {
      Content.encode(message.query, writer.uint32(18).fork()).join();
    }
    for (const v of message.metadataFilters) {
      MetadataFilter.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.maxChunksCount !== undefined) {
      writer.uint32(32).int32(message.maxChunksCount);
    }
    if (message.minimumRelevanceScore !== undefined) {
      writer.uint32(45).float(message.minimumRelevanceScore);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SemanticRetrieverConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSemanticRetrieverConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.source = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.query = Content.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.metadataFilters.push(MetadataFilter.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.maxChunksCount = reader.int32();
          continue;
        case 5:
          if (tag !== 45) {
            break;
          }

          message.minimumRelevanceScore = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SemanticRetrieverConfig {
    return {
      source: isSet(object.source) ? globalThis.String(object.source) : "",
      query: isSet(object.query) ? Content.fromJSON(object.query) : undefined,
      metadataFilters: globalThis.Array.isArray(object?.metadataFilters)
        ? object.metadataFilters.map((e: any) => MetadataFilter.fromJSON(e))
        : [],
      maxChunksCount: isSet(object.maxChunksCount) ? globalThis.Number(object.maxChunksCount) : undefined,
      minimumRelevanceScore: isSet(object.minimumRelevanceScore)
        ? globalThis.Number(object.minimumRelevanceScore)
        : undefined,
    };
  },

  toJSON(message: SemanticRetrieverConfig): unknown {
    const obj: any = {};
    if (message.source !== "") {
      obj.source = message.source;
    }
    if (message.query !== undefined) {
      obj.query = Content.toJSON(message.query);
    }
    if (message.metadataFilters?.length) {
      obj.metadataFilters = message.metadataFilters.map((e) => MetadataFilter.toJSON(e));
    }
    if (message.maxChunksCount !== undefined) {
      obj.maxChunksCount = Math.round(message.maxChunksCount);
    }
    if (message.minimumRelevanceScore !== undefined) {
      obj.minimumRelevanceScore = message.minimumRelevanceScore;
    }
    return obj;
  },

  create(base?: DeepPartial<SemanticRetrieverConfig>): SemanticRetrieverConfig {
    return SemanticRetrieverConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SemanticRetrieverConfig>): SemanticRetrieverConfig {
    const message = createBaseSemanticRetrieverConfig();
    message.source = object.source ?? "";
    message.query = (object.query !== undefined && object.query !== null)
      ? Content.fromPartial(object.query)
      : undefined;
    message.metadataFilters = object.metadataFilters?.map((e) => MetadataFilter.fromPartial(e)) || [];
    message.maxChunksCount = object.maxChunksCount ?? undefined;
    message.minimumRelevanceScore = object.minimumRelevanceScore ?? undefined;
    return message;
  },
};

function createBaseGenerateContentResponse(): GenerateContentResponse {
  return { candidates: [], promptFeedback: undefined, usageMetadata: undefined };
}

export const GenerateContentResponse: MessageFns<GenerateContentResponse> = {
  encode(message: GenerateContentResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.candidates) {
      Candidate.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.promptFeedback !== undefined) {
      GenerateContentResponse_PromptFeedback.encode(message.promptFeedback, writer.uint32(18).fork()).join();
    }
    if (message.usageMetadata !== undefined) {
      GenerateContentResponse_UsageMetadata.encode(message.usageMetadata, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerateContentResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerateContentResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.candidates.push(Candidate.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.promptFeedback = GenerateContentResponse_PromptFeedback.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.usageMetadata = GenerateContentResponse_UsageMetadata.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerateContentResponse {
    return {
      candidates: globalThis.Array.isArray(object?.candidates)
        ? object.candidates.map((e: any) => Candidate.fromJSON(e))
        : [],
      promptFeedback: isSet(object.promptFeedback)
        ? GenerateContentResponse_PromptFeedback.fromJSON(object.promptFeedback)
        : undefined,
      usageMetadata: isSet(object.usageMetadata)
        ? GenerateContentResponse_UsageMetadata.fromJSON(object.usageMetadata)
        : undefined,
    };
  },

  toJSON(message: GenerateContentResponse): unknown {
    const obj: any = {};
    if (message.candidates?.length) {
      obj.candidates = message.candidates.map((e) => Candidate.toJSON(e));
    }
    if (message.promptFeedback !== undefined) {
      obj.promptFeedback = GenerateContentResponse_PromptFeedback.toJSON(message.promptFeedback);
    }
    if (message.usageMetadata !== undefined) {
      obj.usageMetadata = GenerateContentResponse_UsageMetadata.toJSON(message.usageMetadata);
    }
    return obj;
  },

  create(base?: DeepPartial<GenerateContentResponse>): GenerateContentResponse {
    return GenerateContentResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerateContentResponse>): GenerateContentResponse {
    const message = createBaseGenerateContentResponse();
    message.candidates = object.candidates?.map((e) => Candidate.fromPartial(e)) || [];
    message.promptFeedback = (object.promptFeedback !== undefined && object.promptFeedback !== null)
      ? GenerateContentResponse_PromptFeedback.fromPartial(object.promptFeedback)
      : undefined;
    message.usageMetadata = (object.usageMetadata !== undefined && object.usageMetadata !== null)
      ? GenerateContentResponse_UsageMetadata.fromPartial(object.usageMetadata)
      : undefined;
    return message;
  },
};

function createBaseGenerateContentResponse_PromptFeedback(): GenerateContentResponse_PromptFeedback {
  return { blockReason: 0, safetyRatings: [] };
}

export const GenerateContentResponse_PromptFeedback: MessageFns<GenerateContentResponse_PromptFeedback> = {
  encode(message: GenerateContentResponse_PromptFeedback, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.blockReason !== 0) {
      writer.uint32(8).int32(message.blockReason);
    }
    for (const v of message.safetyRatings) {
      SafetyRating.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerateContentResponse_PromptFeedback {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerateContentResponse_PromptFeedback();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.blockReason = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.safetyRatings.push(SafetyRating.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerateContentResponse_PromptFeedback {
    return {
      blockReason: isSet(object.blockReason)
        ? generateContentResponse_PromptFeedback_BlockReasonFromJSON(object.blockReason)
        : 0,
      safetyRatings: globalThis.Array.isArray(object?.safetyRatings)
        ? object.safetyRatings.map((e: any) => SafetyRating.fromJSON(e))
        : [],
    };
  },

  toJSON(message: GenerateContentResponse_PromptFeedback): unknown {
    const obj: any = {};
    if (message.blockReason !== 0) {
      obj.blockReason = generateContentResponse_PromptFeedback_BlockReasonToJSON(message.blockReason);
    }
    if (message.safetyRatings?.length) {
      obj.safetyRatings = message.safetyRatings.map((e) => SafetyRating.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<GenerateContentResponse_PromptFeedback>): GenerateContentResponse_PromptFeedback {
    return GenerateContentResponse_PromptFeedback.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerateContentResponse_PromptFeedback>): GenerateContentResponse_PromptFeedback {
    const message = createBaseGenerateContentResponse_PromptFeedback();
    message.blockReason = object.blockReason ?? 0;
    message.safetyRatings = object.safetyRatings?.map((e) => SafetyRating.fromPartial(e)) || [];
    return message;
  },
};

function createBaseGenerateContentResponse_UsageMetadata(): GenerateContentResponse_UsageMetadata {
  return { promptTokenCount: 0, cachedContentTokenCount: 0, candidatesTokenCount: 0, totalTokenCount: 0 };
}

export const GenerateContentResponse_UsageMetadata: MessageFns<GenerateContentResponse_UsageMetadata> = {
  encode(message: GenerateContentResponse_UsageMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.promptTokenCount !== 0) {
      writer.uint32(8).int32(message.promptTokenCount);
    }
    if (message.cachedContentTokenCount !== 0) {
      writer.uint32(32).int32(message.cachedContentTokenCount);
    }
    if (message.candidatesTokenCount !== 0) {
      writer.uint32(16).int32(message.candidatesTokenCount);
    }
    if (message.totalTokenCount !== 0) {
      writer.uint32(24).int32(message.totalTokenCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerateContentResponse_UsageMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerateContentResponse_UsageMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.promptTokenCount = reader.int32();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.cachedContentTokenCount = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.candidatesTokenCount = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.totalTokenCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerateContentResponse_UsageMetadata {
    return {
      promptTokenCount: isSet(object.promptTokenCount) ? globalThis.Number(object.promptTokenCount) : 0,
      cachedContentTokenCount: isSet(object.cachedContentTokenCount)
        ? globalThis.Number(object.cachedContentTokenCount)
        : 0,
      candidatesTokenCount: isSet(object.candidatesTokenCount) ? globalThis.Number(object.candidatesTokenCount) : 0,
      totalTokenCount: isSet(object.totalTokenCount) ? globalThis.Number(object.totalTokenCount) : 0,
    };
  },

  toJSON(message: GenerateContentResponse_UsageMetadata): unknown {
    const obj: any = {};
    if (message.promptTokenCount !== 0) {
      obj.promptTokenCount = Math.round(message.promptTokenCount);
    }
    if (message.cachedContentTokenCount !== 0) {
      obj.cachedContentTokenCount = Math.round(message.cachedContentTokenCount);
    }
    if (message.candidatesTokenCount !== 0) {
      obj.candidatesTokenCount = Math.round(message.candidatesTokenCount);
    }
    if (message.totalTokenCount !== 0) {
      obj.totalTokenCount = Math.round(message.totalTokenCount);
    }
    return obj;
  },

  create(base?: DeepPartial<GenerateContentResponse_UsageMetadata>): GenerateContentResponse_UsageMetadata {
    return GenerateContentResponse_UsageMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerateContentResponse_UsageMetadata>): GenerateContentResponse_UsageMetadata {
    const message = createBaseGenerateContentResponse_UsageMetadata();
    message.promptTokenCount = object.promptTokenCount ?? 0;
    message.cachedContentTokenCount = object.cachedContentTokenCount ?? 0;
    message.candidatesTokenCount = object.candidatesTokenCount ?? 0;
    message.totalTokenCount = object.totalTokenCount ?? 0;
    return message;
  },
};

function createBaseCandidate(): Candidate {
  return {
    index: undefined,
    content: undefined,
    finishReason: 0,
    safetyRatings: [],
    citationMetadata: undefined,
    tokenCount: 0,
    groundingAttributions: [],
    groundingMetadata: undefined,
    avgLogprobs: 0,
    logprobsResult: undefined,
  };
}

export const Candidate: MessageFns<Candidate> = {
  encode(message: Candidate, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.index !== undefined) {
      writer.uint32(24).int32(message.index);
    }
    if (message.content !== undefined) {
      Content.encode(message.content, writer.uint32(10).fork()).join();
    }
    if (message.finishReason !== 0) {
      writer.uint32(16).int32(message.finishReason);
    }
    for (const v of message.safetyRatings) {
      SafetyRating.encode(v!, writer.uint32(42).fork()).join();
    }
    if (message.citationMetadata !== undefined) {
      CitationMetadata.encode(message.citationMetadata, writer.uint32(50).fork()).join();
    }
    if (message.tokenCount !== 0) {
      writer.uint32(56).int32(message.tokenCount);
    }
    for (const v of message.groundingAttributions) {
      GroundingAttribution.encode(v!, writer.uint32(66).fork()).join();
    }
    if (message.groundingMetadata !== undefined) {
      GroundingMetadata.encode(message.groundingMetadata, writer.uint32(74).fork()).join();
    }
    if (message.avgLogprobs !== 0) {
      writer.uint32(81).double(message.avgLogprobs);
    }
    if (message.logprobsResult !== undefined) {
      LogprobsResult.encode(message.logprobsResult, writer.uint32(90).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Candidate {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCandidate();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 24) {
            break;
          }

          message.index = reader.int32();
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.content = Content.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.finishReason = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.safetyRatings.push(SafetyRating.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.citationMetadata = CitationMetadata.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.tokenCount = reader.int32();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.groundingAttributions.push(GroundingAttribution.decode(reader, reader.uint32()));
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.groundingMetadata = GroundingMetadata.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 81) {
            break;
          }

          message.avgLogprobs = reader.double();
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.logprobsResult = LogprobsResult.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Candidate {
    return {
      index: isSet(object.index) ? globalThis.Number(object.index) : undefined,
      content: isSet(object.content) ? Content.fromJSON(object.content) : undefined,
      finishReason: isSet(object.finishReason) ? candidate_FinishReasonFromJSON(object.finishReason) : 0,
      safetyRatings: globalThis.Array.isArray(object?.safetyRatings)
        ? object.safetyRatings.map((e: any) => SafetyRating.fromJSON(e))
        : [],
      citationMetadata: isSet(object.citationMetadata) ? CitationMetadata.fromJSON(object.citationMetadata) : undefined,
      tokenCount: isSet(object.tokenCount) ? globalThis.Number(object.tokenCount) : 0,
      groundingAttributions: globalThis.Array.isArray(object?.groundingAttributions)
        ? object.groundingAttributions.map((e: any) => GroundingAttribution.fromJSON(e))
        : [],
      groundingMetadata: isSet(object.groundingMetadata)
        ? GroundingMetadata.fromJSON(object.groundingMetadata)
        : undefined,
      avgLogprobs: isSet(object.avgLogprobs) ? globalThis.Number(object.avgLogprobs) : 0,
      logprobsResult: isSet(object.logprobsResult) ? LogprobsResult.fromJSON(object.logprobsResult) : undefined,
    };
  },

  toJSON(message: Candidate): unknown {
    const obj: any = {};
    if (message.index !== undefined) {
      obj.index = Math.round(message.index);
    }
    if (message.content !== undefined) {
      obj.content = Content.toJSON(message.content);
    }
    if (message.finishReason !== 0) {
      obj.finishReason = candidate_FinishReasonToJSON(message.finishReason);
    }
    if (message.safetyRatings?.length) {
      obj.safetyRatings = message.safetyRatings.map((e) => SafetyRating.toJSON(e));
    }
    if (message.citationMetadata !== undefined) {
      obj.citationMetadata = CitationMetadata.toJSON(message.citationMetadata);
    }
    if (message.tokenCount !== 0) {
      obj.tokenCount = Math.round(message.tokenCount);
    }
    if (message.groundingAttributions?.length) {
      obj.groundingAttributions = message.groundingAttributions.map((e) => GroundingAttribution.toJSON(e));
    }
    if (message.groundingMetadata !== undefined) {
      obj.groundingMetadata = GroundingMetadata.toJSON(message.groundingMetadata);
    }
    if (message.avgLogprobs !== 0) {
      obj.avgLogprobs = message.avgLogprobs;
    }
    if (message.logprobsResult !== undefined) {
      obj.logprobsResult = LogprobsResult.toJSON(message.logprobsResult);
    }
    return obj;
  },

  create(base?: DeepPartial<Candidate>): Candidate {
    return Candidate.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Candidate>): Candidate {
    const message = createBaseCandidate();
    message.index = object.index ?? undefined;
    message.content = (object.content !== undefined && object.content !== null)
      ? Content.fromPartial(object.content)
      : undefined;
    message.finishReason = object.finishReason ?? 0;
    message.safetyRatings = object.safetyRatings?.map((e) => SafetyRating.fromPartial(e)) || [];
    message.citationMetadata = (object.citationMetadata !== undefined && object.citationMetadata !== null)
      ? CitationMetadata.fromPartial(object.citationMetadata)
      : undefined;
    message.tokenCount = object.tokenCount ?? 0;
    message.groundingAttributions = object.groundingAttributions?.map((e) => GroundingAttribution.fromPartial(e)) || [];
    message.groundingMetadata = (object.groundingMetadata !== undefined && object.groundingMetadata !== null)
      ? GroundingMetadata.fromPartial(object.groundingMetadata)
      : undefined;
    message.avgLogprobs = object.avgLogprobs ?? 0;
    message.logprobsResult = (object.logprobsResult !== undefined && object.logprobsResult !== null)
      ? LogprobsResult.fromPartial(object.logprobsResult)
      : undefined;
    return message;
  },
};

function createBaseLogprobsResult(): LogprobsResult {
  return { topCandidates: [], chosenCandidates: [] };
}

export const LogprobsResult: MessageFns<LogprobsResult> = {
  encode(message: LogprobsResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.topCandidates) {
      LogprobsResult_TopCandidates.encode(v!, writer.uint32(10).fork()).join();
    }
    for (const v of message.chosenCandidates) {
      LogprobsResult_Candidate.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LogprobsResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLogprobsResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.topCandidates.push(LogprobsResult_TopCandidates.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.chosenCandidates.push(LogprobsResult_Candidate.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LogprobsResult {
    return {
      topCandidates: globalThis.Array.isArray(object?.topCandidates)
        ? object.topCandidates.map((e: any) => LogprobsResult_TopCandidates.fromJSON(e))
        : [],
      chosenCandidates: globalThis.Array.isArray(object?.chosenCandidates)
        ? object.chosenCandidates.map((e: any) => LogprobsResult_Candidate.fromJSON(e))
        : [],
    };
  },

  toJSON(message: LogprobsResult): unknown {
    const obj: any = {};
    if (message.topCandidates?.length) {
      obj.topCandidates = message.topCandidates.map((e) => LogprobsResult_TopCandidates.toJSON(e));
    }
    if (message.chosenCandidates?.length) {
      obj.chosenCandidates = message.chosenCandidates.map((e) => LogprobsResult_Candidate.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<LogprobsResult>): LogprobsResult {
    return LogprobsResult.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LogprobsResult>): LogprobsResult {
    const message = createBaseLogprobsResult();
    message.topCandidates = object.topCandidates?.map((e) => LogprobsResult_TopCandidates.fromPartial(e)) || [];
    message.chosenCandidates = object.chosenCandidates?.map((e) => LogprobsResult_Candidate.fromPartial(e)) || [];
    return message;
  },
};

function createBaseLogprobsResult_Candidate(): LogprobsResult_Candidate {
  return { token: undefined, tokenId: undefined, logProbability: undefined };
}

export const LogprobsResult_Candidate: MessageFns<LogprobsResult_Candidate> = {
  encode(message: LogprobsResult_Candidate, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.token !== undefined) {
      writer.uint32(10).string(message.token);
    }
    if (message.tokenId !== undefined) {
      writer.uint32(24).int32(message.tokenId);
    }
    if (message.logProbability !== undefined) {
      writer.uint32(21).float(message.logProbability);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LogprobsResult_Candidate {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLogprobsResult_Candidate();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.token = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.tokenId = reader.int32();
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.logProbability = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LogprobsResult_Candidate {
    return {
      token: isSet(object.token) ? globalThis.String(object.token) : undefined,
      tokenId: isSet(object.tokenId) ? globalThis.Number(object.tokenId) : undefined,
      logProbability: isSet(object.logProbability) ? globalThis.Number(object.logProbability) : undefined,
    };
  },

  toJSON(message: LogprobsResult_Candidate): unknown {
    const obj: any = {};
    if (message.token !== undefined) {
      obj.token = message.token;
    }
    if (message.tokenId !== undefined) {
      obj.tokenId = Math.round(message.tokenId);
    }
    if (message.logProbability !== undefined) {
      obj.logProbability = message.logProbability;
    }
    return obj;
  },

  create(base?: DeepPartial<LogprobsResult_Candidate>): LogprobsResult_Candidate {
    return LogprobsResult_Candidate.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LogprobsResult_Candidate>): LogprobsResult_Candidate {
    const message = createBaseLogprobsResult_Candidate();
    message.token = object.token ?? undefined;
    message.tokenId = object.tokenId ?? undefined;
    message.logProbability = object.logProbability ?? undefined;
    return message;
  },
};

function createBaseLogprobsResult_TopCandidates(): LogprobsResult_TopCandidates {
  return { candidates: [] };
}

export const LogprobsResult_TopCandidates: MessageFns<LogprobsResult_TopCandidates> = {
  encode(message: LogprobsResult_TopCandidates, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.candidates) {
      LogprobsResult_Candidate.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LogprobsResult_TopCandidates {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLogprobsResult_TopCandidates();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.candidates.push(LogprobsResult_Candidate.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LogprobsResult_TopCandidates {
    return {
      candidates: globalThis.Array.isArray(object?.candidates)
        ? object.candidates.map((e: any) => LogprobsResult_Candidate.fromJSON(e))
        : [],
    };
  },

  toJSON(message: LogprobsResult_TopCandidates): unknown {
    const obj: any = {};
    if (message.candidates?.length) {
      obj.candidates = message.candidates.map((e) => LogprobsResult_Candidate.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<LogprobsResult_TopCandidates>): LogprobsResult_TopCandidates {
    return LogprobsResult_TopCandidates.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LogprobsResult_TopCandidates>): LogprobsResult_TopCandidates {
    const message = createBaseLogprobsResult_TopCandidates();
    message.candidates = object.candidates?.map((e) => LogprobsResult_Candidate.fromPartial(e)) || [];
    return message;
  },
};

function createBaseAttributionSourceId(): AttributionSourceId {
  return { groundingPassage: undefined, semanticRetrieverChunk: undefined };
}

export const AttributionSourceId: MessageFns<AttributionSourceId> = {
  encode(message: AttributionSourceId, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.groundingPassage !== undefined) {
      AttributionSourceId_GroundingPassageId.encode(message.groundingPassage, writer.uint32(10).fork()).join();
    }
    if (message.semanticRetrieverChunk !== undefined) {
      AttributionSourceId_SemanticRetrieverChunk.encode(message.semanticRetrieverChunk, writer.uint32(18).fork())
        .join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AttributionSourceId {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAttributionSourceId();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.groundingPassage = AttributionSourceId_GroundingPassageId.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.semanticRetrieverChunk = AttributionSourceId_SemanticRetrieverChunk.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AttributionSourceId {
    return {
      groundingPassage: isSet(object.groundingPassage)
        ? AttributionSourceId_GroundingPassageId.fromJSON(object.groundingPassage)
        : undefined,
      semanticRetrieverChunk: isSet(object.semanticRetrieverChunk)
        ? AttributionSourceId_SemanticRetrieverChunk.fromJSON(object.semanticRetrieverChunk)
        : undefined,
    };
  },

  toJSON(message: AttributionSourceId): unknown {
    const obj: any = {};
    if (message.groundingPassage !== undefined) {
      obj.groundingPassage = AttributionSourceId_GroundingPassageId.toJSON(message.groundingPassage);
    }
    if (message.semanticRetrieverChunk !== undefined) {
      obj.semanticRetrieverChunk = AttributionSourceId_SemanticRetrieverChunk.toJSON(message.semanticRetrieverChunk);
    }
    return obj;
  },

  create(base?: DeepPartial<AttributionSourceId>): AttributionSourceId {
    return AttributionSourceId.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AttributionSourceId>): AttributionSourceId {
    const message = createBaseAttributionSourceId();
    message.groundingPassage = (object.groundingPassage !== undefined && object.groundingPassage !== null)
      ? AttributionSourceId_GroundingPassageId.fromPartial(object.groundingPassage)
      : undefined;
    message.semanticRetrieverChunk =
      (object.semanticRetrieverChunk !== undefined && object.semanticRetrieverChunk !== null)
        ? AttributionSourceId_SemanticRetrieverChunk.fromPartial(object.semanticRetrieverChunk)
        : undefined;
    return message;
  },
};

function createBaseAttributionSourceId_GroundingPassageId(): AttributionSourceId_GroundingPassageId {
  return { passageId: "", partIndex: 0 };
}

export const AttributionSourceId_GroundingPassageId: MessageFns<AttributionSourceId_GroundingPassageId> = {
  encode(message: AttributionSourceId_GroundingPassageId, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.passageId !== "") {
      writer.uint32(10).string(message.passageId);
    }
    if (message.partIndex !== 0) {
      writer.uint32(16).int32(message.partIndex);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AttributionSourceId_GroundingPassageId {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAttributionSourceId_GroundingPassageId();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.passageId = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.partIndex = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AttributionSourceId_GroundingPassageId {
    return {
      passageId: isSet(object.passageId) ? globalThis.String(object.passageId) : "",
      partIndex: isSet(object.partIndex) ? globalThis.Number(object.partIndex) : 0,
    };
  },

  toJSON(message: AttributionSourceId_GroundingPassageId): unknown {
    const obj: any = {};
    if (message.passageId !== "") {
      obj.passageId = message.passageId;
    }
    if (message.partIndex !== 0) {
      obj.partIndex = Math.round(message.partIndex);
    }
    return obj;
  },

  create(base?: DeepPartial<AttributionSourceId_GroundingPassageId>): AttributionSourceId_GroundingPassageId {
    return AttributionSourceId_GroundingPassageId.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AttributionSourceId_GroundingPassageId>): AttributionSourceId_GroundingPassageId {
    const message = createBaseAttributionSourceId_GroundingPassageId();
    message.passageId = object.passageId ?? "";
    message.partIndex = object.partIndex ?? 0;
    return message;
  },
};

function createBaseAttributionSourceId_SemanticRetrieverChunk(): AttributionSourceId_SemanticRetrieverChunk {
  return { source: "", chunk: "" };
}

export const AttributionSourceId_SemanticRetrieverChunk: MessageFns<AttributionSourceId_SemanticRetrieverChunk> = {
  encode(message: AttributionSourceId_SemanticRetrieverChunk, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.source !== "") {
      writer.uint32(10).string(message.source);
    }
    if (message.chunk !== "") {
      writer.uint32(18).string(message.chunk);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AttributionSourceId_SemanticRetrieverChunk {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAttributionSourceId_SemanticRetrieverChunk();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.source = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.chunk = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AttributionSourceId_SemanticRetrieverChunk {
    return {
      source: isSet(object.source) ? globalThis.String(object.source) : "",
      chunk: isSet(object.chunk) ? globalThis.String(object.chunk) : "",
    };
  },

  toJSON(message: AttributionSourceId_SemanticRetrieverChunk): unknown {
    const obj: any = {};
    if (message.source !== "") {
      obj.source = message.source;
    }
    if (message.chunk !== "") {
      obj.chunk = message.chunk;
    }
    return obj;
  },

  create(base?: DeepPartial<AttributionSourceId_SemanticRetrieverChunk>): AttributionSourceId_SemanticRetrieverChunk {
    return AttributionSourceId_SemanticRetrieverChunk.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<AttributionSourceId_SemanticRetrieverChunk>,
  ): AttributionSourceId_SemanticRetrieverChunk {
    const message = createBaseAttributionSourceId_SemanticRetrieverChunk();
    message.source = object.source ?? "";
    message.chunk = object.chunk ?? "";
    return message;
  },
};

function createBaseGroundingAttribution(): GroundingAttribution {
  return { sourceId: undefined, content: undefined };
}

export const GroundingAttribution: MessageFns<GroundingAttribution> = {
  encode(message: GroundingAttribution, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.sourceId !== undefined) {
      AttributionSourceId.encode(message.sourceId, writer.uint32(26).fork()).join();
    }
    if (message.content !== undefined) {
      Content.encode(message.content, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GroundingAttribution {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGroundingAttribution();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 26) {
            break;
          }

          message.sourceId = AttributionSourceId.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.content = Content.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GroundingAttribution {
    return {
      sourceId: isSet(object.sourceId) ? AttributionSourceId.fromJSON(object.sourceId) : undefined,
      content: isSet(object.content) ? Content.fromJSON(object.content) : undefined,
    };
  },

  toJSON(message: GroundingAttribution): unknown {
    const obj: any = {};
    if (message.sourceId !== undefined) {
      obj.sourceId = AttributionSourceId.toJSON(message.sourceId);
    }
    if (message.content !== undefined) {
      obj.content = Content.toJSON(message.content);
    }
    return obj;
  },

  create(base?: DeepPartial<GroundingAttribution>): GroundingAttribution {
    return GroundingAttribution.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GroundingAttribution>): GroundingAttribution {
    const message = createBaseGroundingAttribution();
    message.sourceId = (object.sourceId !== undefined && object.sourceId !== null)
      ? AttributionSourceId.fromPartial(object.sourceId)
      : undefined;
    message.content = (object.content !== undefined && object.content !== null)
      ? Content.fromPartial(object.content)
      : undefined;
    return message;
  },
};

function createBaseRetrievalMetadata(): RetrievalMetadata {
  return { googleSearchDynamicRetrievalScore: 0 };
}

export const RetrievalMetadata: MessageFns<RetrievalMetadata> = {
  encode(message: RetrievalMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.googleSearchDynamicRetrievalScore !== 0) {
      writer.uint32(21).float(message.googleSearchDynamicRetrievalScore);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RetrievalMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRetrievalMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 21) {
            break;
          }

          message.googleSearchDynamicRetrievalScore = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RetrievalMetadata {
    return {
      googleSearchDynamicRetrievalScore: isSet(object.googleSearchDynamicRetrievalScore)
        ? globalThis.Number(object.googleSearchDynamicRetrievalScore)
        : 0,
    };
  },

  toJSON(message: RetrievalMetadata): unknown {
    const obj: any = {};
    if (message.googleSearchDynamicRetrievalScore !== 0) {
      obj.googleSearchDynamicRetrievalScore = message.googleSearchDynamicRetrievalScore;
    }
    return obj;
  },

  create(base?: DeepPartial<RetrievalMetadata>): RetrievalMetadata {
    return RetrievalMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RetrievalMetadata>): RetrievalMetadata {
    const message = createBaseRetrievalMetadata();
    message.googleSearchDynamicRetrievalScore = object.googleSearchDynamicRetrievalScore ?? 0;
    return message;
  },
};

function createBaseGroundingMetadata(): GroundingMetadata {
  return { searchEntryPoint: undefined, groundingChunks: [], groundingSupports: [], retrievalMetadata: undefined };
}

export const GroundingMetadata: MessageFns<GroundingMetadata> = {
  encode(message: GroundingMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.searchEntryPoint !== undefined) {
      SearchEntryPoint.encode(message.searchEntryPoint, writer.uint32(10).fork()).join();
    }
    for (const v of message.groundingChunks) {
      GroundingChunk.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.groundingSupports) {
      GroundingSupport.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.retrievalMetadata !== undefined) {
      RetrievalMetadata.encode(message.retrievalMetadata, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GroundingMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGroundingMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.searchEntryPoint = SearchEntryPoint.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.groundingChunks.push(GroundingChunk.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.groundingSupports.push(GroundingSupport.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.retrievalMetadata = RetrievalMetadata.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GroundingMetadata {
    return {
      searchEntryPoint: isSet(object.searchEntryPoint) ? SearchEntryPoint.fromJSON(object.searchEntryPoint) : undefined,
      groundingChunks: globalThis.Array.isArray(object?.groundingChunks)
        ? object.groundingChunks.map((e: any) => GroundingChunk.fromJSON(e))
        : [],
      groundingSupports: globalThis.Array.isArray(object?.groundingSupports)
        ? object.groundingSupports.map((e: any) => GroundingSupport.fromJSON(e))
        : [],
      retrievalMetadata: isSet(object.retrievalMetadata)
        ? RetrievalMetadata.fromJSON(object.retrievalMetadata)
        : undefined,
    };
  },

  toJSON(message: GroundingMetadata): unknown {
    const obj: any = {};
    if (message.searchEntryPoint !== undefined) {
      obj.searchEntryPoint = SearchEntryPoint.toJSON(message.searchEntryPoint);
    }
    if (message.groundingChunks?.length) {
      obj.groundingChunks = message.groundingChunks.map((e) => GroundingChunk.toJSON(e));
    }
    if (message.groundingSupports?.length) {
      obj.groundingSupports = message.groundingSupports.map((e) => GroundingSupport.toJSON(e));
    }
    if (message.retrievalMetadata !== undefined) {
      obj.retrievalMetadata = RetrievalMetadata.toJSON(message.retrievalMetadata);
    }
    return obj;
  },

  create(base?: DeepPartial<GroundingMetadata>): GroundingMetadata {
    return GroundingMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GroundingMetadata>): GroundingMetadata {
    const message = createBaseGroundingMetadata();
    message.searchEntryPoint = (object.searchEntryPoint !== undefined && object.searchEntryPoint !== null)
      ? SearchEntryPoint.fromPartial(object.searchEntryPoint)
      : undefined;
    message.groundingChunks = object.groundingChunks?.map((e) => GroundingChunk.fromPartial(e)) || [];
    message.groundingSupports = object.groundingSupports?.map((e) => GroundingSupport.fromPartial(e)) || [];
    message.retrievalMetadata = (object.retrievalMetadata !== undefined && object.retrievalMetadata !== null)
      ? RetrievalMetadata.fromPartial(object.retrievalMetadata)
      : undefined;
    return message;
  },
};

function createBaseSearchEntryPoint(): SearchEntryPoint {
  return { renderedContent: "", sdkBlob: Buffer.alloc(0) };
}

export const SearchEntryPoint: MessageFns<SearchEntryPoint> = {
  encode(message: SearchEntryPoint, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.renderedContent !== "") {
      writer.uint32(10).string(message.renderedContent);
    }
    if (message.sdkBlob.length !== 0) {
      writer.uint32(18).bytes(message.sdkBlob);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SearchEntryPoint {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSearchEntryPoint();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.renderedContent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.sdkBlob = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SearchEntryPoint {
    return {
      renderedContent: isSet(object.renderedContent) ? globalThis.String(object.renderedContent) : "",
      sdkBlob: isSet(object.sdkBlob) ? Buffer.from(bytesFromBase64(object.sdkBlob)) : Buffer.alloc(0),
    };
  },

  toJSON(message: SearchEntryPoint): unknown {
    const obj: any = {};
    if (message.renderedContent !== "") {
      obj.renderedContent = message.renderedContent;
    }
    if (message.sdkBlob.length !== 0) {
      obj.sdkBlob = base64FromBytes(message.sdkBlob);
    }
    return obj;
  },

  create(base?: DeepPartial<SearchEntryPoint>): SearchEntryPoint {
    return SearchEntryPoint.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SearchEntryPoint>): SearchEntryPoint {
    const message = createBaseSearchEntryPoint();
    message.renderedContent = object.renderedContent ?? "";
    message.sdkBlob = object.sdkBlob ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseGroundingChunk(): GroundingChunk {
  return { web: undefined };
}

export const GroundingChunk: MessageFns<GroundingChunk> = {
  encode(message: GroundingChunk, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.web !== undefined) {
      GroundingChunk_Web.encode(message.web, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GroundingChunk {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGroundingChunk();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.web = GroundingChunk_Web.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GroundingChunk {
    return { web: isSet(object.web) ? GroundingChunk_Web.fromJSON(object.web) : undefined };
  },

  toJSON(message: GroundingChunk): unknown {
    const obj: any = {};
    if (message.web !== undefined) {
      obj.web = GroundingChunk_Web.toJSON(message.web);
    }
    return obj;
  },

  create(base?: DeepPartial<GroundingChunk>): GroundingChunk {
    return GroundingChunk.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GroundingChunk>): GroundingChunk {
    const message = createBaseGroundingChunk();
    message.web = (object.web !== undefined && object.web !== null)
      ? GroundingChunk_Web.fromPartial(object.web)
      : undefined;
    return message;
  },
};

function createBaseGroundingChunk_Web(): GroundingChunk_Web {
  return { uri: undefined, title: undefined };
}

export const GroundingChunk_Web: MessageFns<GroundingChunk_Web> = {
  encode(message: GroundingChunk_Web, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.uri !== undefined) {
      writer.uint32(10).string(message.uri);
    }
    if (message.title !== undefined) {
      writer.uint32(18).string(message.title);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GroundingChunk_Web {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGroundingChunk_Web();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.uri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.title = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GroundingChunk_Web {
    return {
      uri: isSet(object.uri) ? globalThis.String(object.uri) : undefined,
      title: isSet(object.title) ? globalThis.String(object.title) : undefined,
    };
  },

  toJSON(message: GroundingChunk_Web): unknown {
    const obj: any = {};
    if (message.uri !== undefined) {
      obj.uri = message.uri;
    }
    if (message.title !== undefined) {
      obj.title = message.title;
    }
    return obj;
  },

  create(base?: DeepPartial<GroundingChunk_Web>): GroundingChunk_Web {
    return GroundingChunk_Web.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GroundingChunk_Web>): GroundingChunk_Web {
    const message = createBaseGroundingChunk_Web();
    message.uri = object.uri ?? undefined;
    message.title = object.title ?? undefined;
    return message;
  },
};

function createBaseSegment(): Segment {
  return { partIndex: 0, startIndex: 0, endIndex: 0, text: "" };
}

export const Segment: MessageFns<Segment> = {
  encode(message: Segment, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.partIndex !== 0) {
      writer.uint32(8).int32(message.partIndex);
    }
    if (message.startIndex !== 0) {
      writer.uint32(16).int32(message.startIndex);
    }
    if (message.endIndex !== 0) {
      writer.uint32(24).int32(message.endIndex);
    }
    if (message.text !== "") {
      writer.uint32(34).string(message.text);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Segment {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSegment();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.partIndex = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.startIndex = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.endIndex = reader.int32();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.text = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Segment {
    return {
      partIndex: isSet(object.partIndex) ? globalThis.Number(object.partIndex) : 0,
      startIndex: isSet(object.startIndex) ? globalThis.Number(object.startIndex) : 0,
      endIndex: isSet(object.endIndex) ? globalThis.Number(object.endIndex) : 0,
      text: isSet(object.text) ? globalThis.String(object.text) : "",
    };
  },

  toJSON(message: Segment): unknown {
    const obj: any = {};
    if (message.partIndex !== 0) {
      obj.partIndex = Math.round(message.partIndex);
    }
    if (message.startIndex !== 0) {
      obj.startIndex = Math.round(message.startIndex);
    }
    if (message.endIndex !== 0) {
      obj.endIndex = Math.round(message.endIndex);
    }
    if (message.text !== "") {
      obj.text = message.text;
    }
    return obj;
  },

  create(base?: DeepPartial<Segment>): Segment {
    return Segment.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Segment>): Segment {
    const message = createBaseSegment();
    message.partIndex = object.partIndex ?? 0;
    message.startIndex = object.startIndex ?? 0;
    message.endIndex = object.endIndex ?? 0;
    message.text = object.text ?? "";
    return message;
  },
};

function createBaseGroundingSupport(): GroundingSupport {
  return { segment: undefined, groundingChunkIndices: [], confidenceScores: [] };
}

export const GroundingSupport: MessageFns<GroundingSupport> = {
  encode(message: GroundingSupport, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.segment !== undefined) {
      Segment.encode(message.segment, writer.uint32(10).fork()).join();
    }
    writer.uint32(18).fork();
    for (const v of message.groundingChunkIndices) {
      writer.int32(v);
    }
    writer.join();
    writer.uint32(26).fork();
    for (const v of message.confidenceScores) {
      writer.float(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GroundingSupport {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGroundingSupport();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.segment = Segment.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag === 16) {
            message.groundingChunkIndices.push(reader.int32());

            continue;
          }

          if (tag === 18) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.groundingChunkIndices.push(reader.int32());
            }

            continue;
          }

          break;
        case 3:
          if (tag === 29) {
            message.confidenceScores.push(reader.float());

            continue;
          }

          if (tag === 26) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.confidenceScores.push(reader.float());
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GroundingSupport {
    return {
      segment: isSet(object.segment) ? Segment.fromJSON(object.segment) : undefined,
      groundingChunkIndices: globalThis.Array.isArray(object?.groundingChunkIndices)
        ? object.groundingChunkIndices.map((e: any) => globalThis.Number(e))
        : [],
      confidenceScores: globalThis.Array.isArray(object?.confidenceScores)
        ? object.confidenceScores.map((e: any) => globalThis.Number(e))
        : [],
    };
  },

  toJSON(message: GroundingSupport): unknown {
    const obj: any = {};
    if (message.segment !== undefined) {
      obj.segment = Segment.toJSON(message.segment);
    }
    if (message.groundingChunkIndices?.length) {
      obj.groundingChunkIndices = message.groundingChunkIndices.map((e) => Math.round(e));
    }
    if (message.confidenceScores?.length) {
      obj.confidenceScores = message.confidenceScores;
    }
    return obj;
  },

  create(base?: DeepPartial<GroundingSupport>): GroundingSupport {
    return GroundingSupport.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GroundingSupport>): GroundingSupport {
    const message = createBaseGroundingSupport();
    message.segment = (object.segment !== undefined && object.segment !== null)
      ? Segment.fromPartial(object.segment)
      : undefined;
    message.groundingChunkIndices = object.groundingChunkIndices?.map((e) => e) || [];
    message.confidenceScores = object.confidenceScores?.map((e) => e) || [];
    return message;
  },
};

function createBaseGenerateAnswerRequest(): GenerateAnswerRequest {
  return {
    inlinePassages: undefined,
    semanticRetriever: undefined,
    model: "",
    contents: [],
    answerStyle: 0,
    safetySettings: [],
    temperature: undefined,
  };
}

export const GenerateAnswerRequest: MessageFns<GenerateAnswerRequest> = {
  encode(message: GenerateAnswerRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inlinePassages !== undefined) {
      GroundingPassages.encode(message.inlinePassages, writer.uint32(50).fork()).join();
    }
    if (message.semanticRetriever !== undefined) {
      SemanticRetrieverConfig.encode(message.semanticRetriever, writer.uint32(58).fork()).join();
    }
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    for (const v of message.contents) {
      Content.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.answerStyle !== 0) {
      writer.uint32(40).int32(message.answerStyle);
    }
    for (const v of message.safetySettings) {
      SafetySetting.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.temperature !== undefined) {
      writer.uint32(37).float(message.temperature);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerateAnswerRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerateAnswerRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 6:
          if (tag !== 50) {
            break;
          }

          message.inlinePassages = GroundingPassages.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.semanticRetriever = SemanticRetrieverConfig.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.contents.push(Content.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.answerStyle = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.safetySettings.push(SafetySetting.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.temperature = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerateAnswerRequest {
    return {
      inlinePassages: isSet(object.inlinePassages) ? GroundingPassages.fromJSON(object.inlinePassages) : undefined,
      semanticRetriever: isSet(object.semanticRetriever)
        ? SemanticRetrieverConfig.fromJSON(object.semanticRetriever)
        : undefined,
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      contents: globalThis.Array.isArray(object?.contents) ? object.contents.map((e: any) => Content.fromJSON(e)) : [],
      answerStyle: isSet(object.answerStyle) ? generateAnswerRequest_AnswerStyleFromJSON(object.answerStyle) : 0,
      safetySettings: globalThis.Array.isArray(object?.safetySettings)
        ? object.safetySettings.map((e: any) => SafetySetting.fromJSON(e))
        : [],
      temperature: isSet(object.temperature) ? globalThis.Number(object.temperature) : undefined,
    };
  },

  toJSON(message: GenerateAnswerRequest): unknown {
    const obj: any = {};
    if (message.inlinePassages !== undefined) {
      obj.inlinePassages = GroundingPassages.toJSON(message.inlinePassages);
    }
    if (message.semanticRetriever !== undefined) {
      obj.semanticRetriever = SemanticRetrieverConfig.toJSON(message.semanticRetriever);
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.contents?.length) {
      obj.contents = message.contents.map((e) => Content.toJSON(e));
    }
    if (message.answerStyle !== 0) {
      obj.answerStyle = generateAnswerRequest_AnswerStyleToJSON(message.answerStyle);
    }
    if (message.safetySettings?.length) {
      obj.safetySettings = message.safetySettings.map((e) => SafetySetting.toJSON(e));
    }
    if (message.temperature !== undefined) {
      obj.temperature = message.temperature;
    }
    return obj;
  },

  create(base?: DeepPartial<GenerateAnswerRequest>): GenerateAnswerRequest {
    return GenerateAnswerRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerateAnswerRequest>): GenerateAnswerRequest {
    const message = createBaseGenerateAnswerRequest();
    message.inlinePassages = (object.inlinePassages !== undefined && object.inlinePassages !== null)
      ? GroundingPassages.fromPartial(object.inlinePassages)
      : undefined;
    message.semanticRetriever = (object.semanticRetriever !== undefined && object.semanticRetriever !== null)
      ? SemanticRetrieverConfig.fromPartial(object.semanticRetriever)
      : undefined;
    message.model = object.model ?? "";
    message.contents = object.contents?.map((e) => Content.fromPartial(e)) || [];
    message.answerStyle = object.answerStyle ?? 0;
    message.safetySettings = object.safetySettings?.map((e) => SafetySetting.fromPartial(e)) || [];
    message.temperature = object.temperature ?? undefined;
    return message;
  },
};

function createBaseGenerateAnswerResponse(): GenerateAnswerResponse {
  return { answer: undefined, answerableProbability: undefined, inputFeedback: undefined };
}

export const GenerateAnswerResponse: MessageFns<GenerateAnswerResponse> = {
  encode(message: GenerateAnswerResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.answer !== undefined) {
      Candidate.encode(message.answer, writer.uint32(10).fork()).join();
    }
    if (message.answerableProbability !== undefined) {
      writer.uint32(21).float(message.answerableProbability);
    }
    if (message.inputFeedback !== undefined) {
      GenerateAnswerResponse_InputFeedback.encode(message.inputFeedback, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerateAnswerResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerateAnswerResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.answer = Candidate.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.answerableProbability = reader.float();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.inputFeedback = GenerateAnswerResponse_InputFeedback.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerateAnswerResponse {
    return {
      answer: isSet(object.answer) ? Candidate.fromJSON(object.answer) : undefined,
      answerableProbability: isSet(object.answerableProbability)
        ? globalThis.Number(object.answerableProbability)
        : undefined,
      inputFeedback: isSet(object.inputFeedback)
        ? GenerateAnswerResponse_InputFeedback.fromJSON(object.inputFeedback)
        : undefined,
    };
  },

  toJSON(message: GenerateAnswerResponse): unknown {
    const obj: any = {};
    if (message.answer !== undefined) {
      obj.answer = Candidate.toJSON(message.answer);
    }
    if (message.answerableProbability !== undefined) {
      obj.answerableProbability = message.answerableProbability;
    }
    if (message.inputFeedback !== undefined) {
      obj.inputFeedback = GenerateAnswerResponse_InputFeedback.toJSON(message.inputFeedback);
    }
    return obj;
  },

  create(base?: DeepPartial<GenerateAnswerResponse>): GenerateAnswerResponse {
    return GenerateAnswerResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerateAnswerResponse>): GenerateAnswerResponse {
    const message = createBaseGenerateAnswerResponse();
    message.answer = (object.answer !== undefined && object.answer !== null)
      ? Candidate.fromPartial(object.answer)
      : undefined;
    message.answerableProbability = object.answerableProbability ?? undefined;
    message.inputFeedback = (object.inputFeedback !== undefined && object.inputFeedback !== null)
      ? GenerateAnswerResponse_InputFeedback.fromPartial(object.inputFeedback)
      : undefined;
    return message;
  },
};

function createBaseGenerateAnswerResponse_InputFeedback(): GenerateAnswerResponse_InputFeedback {
  return { blockReason: undefined, safetyRatings: [] };
}

export const GenerateAnswerResponse_InputFeedback: MessageFns<GenerateAnswerResponse_InputFeedback> = {
  encode(message: GenerateAnswerResponse_InputFeedback, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.blockReason !== undefined) {
      writer.uint32(8).int32(message.blockReason);
    }
    for (const v of message.safetyRatings) {
      SafetyRating.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GenerateAnswerResponse_InputFeedback {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGenerateAnswerResponse_InputFeedback();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.blockReason = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.safetyRatings.push(SafetyRating.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GenerateAnswerResponse_InputFeedback {
    return {
      blockReason: isSet(object.blockReason)
        ? generateAnswerResponse_InputFeedback_BlockReasonFromJSON(object.blockReason)
        : undefined,
      safetyRatings: globalThis.Array.isArray(object?.safetyRatings)
        ? object.safetyRatings.map((e: any) => SafetyRating.fromJSON(e))
        : [],
    };
  },

  toJSON(message: GenerateAnswerResponse_InputFeedback): unknown {
    const obj: any = {};
    if (message.blockReason !== undefined) {
      obj.blockReason = generateAnswerResponse_InputFeedback_BlockReasonToJSON(message.blockReason);
    }
    if (message.safetyRatings?.length) {
      obj.safetyRatings = message.safetyRatings.map((e) => SafetyRating.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<GenerateAnswerResponse_InputFeedback>): GenerateAnswerResponse_InputFeedback {
    return GenerateAnswerResponse_InputFeedback.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GenerateAnswerResponse_InputFeedback>): GenerateAnswerResponse_InputFeedback {
    const message = createBaseGenerateAnswerResponse_InputFeedback();
    message.blockReason = object.blockReason ?? undefined;
    message.safetyRatings = object.safetyRatings?.map((e) => SafetyRating.fromPartial(e)) || [];
    return message;
  },
};

function createBaseEmbedContentRequest(): EmbedContentRequest {
  return { model: "", content: undefined, taskType: undefined, title: undefined, outputDimensionality: undefined };
}

export const EmbedContentRequest: MessageFns<EmbedContentRequest> = {
  encode(message: EmbedContentRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    if (message.content !== undefined) {
      Content.encode(message.content, writer.uint32(18).fork()).join();
    }
    if (message.taskType !== undefined) {
      writer.uint32(24).int32(message.taskType);
    }
    if (message.title !== undefined) {
      writer.uint32(34).string(message.title);
    }
    if (message.outputDimensionality !== undefined) {
      writer.uint32(40).int32(message.outputDimensionality);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EmbedContentRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEmbedContentRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.content = Content.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.taskType = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.title = reader.string();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.outputDimensionality = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EmbedContentRequest {
    return {
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      content: isSet(object.content) ? Content.fromJSON(object.content) : undefined,
      taskType: isSet(object.taskType) ? taskTypeFromJSON(object.taskType) : undefined,
      title: isSet(object.title) ? globalThis.String(object.title) : undefined,
      outputDimensionality: isSet(object.outputDimensionality)
        ? globalThis.Number(object.outputDimensionality)
        : undefined,
    };
  },

  toJSON(message: EmbedContentRequest): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.content !== undefined) {
      obj.content = Content.toJSON(message.content);
    }
    if (message.taskType !== undefined) {
      obj.taskType = taskTypeToJSON(message.taskType);
    }
    if (message.title !== undefined) {
      obj.title = message.title;
    }
    if (message.outputDimensionality !== undefined) {
      obj.outputDimensionality = Math.round(message.outputDimensionality);
    }
    return obj;
  },

  create(base?: DeepPartial<EmbedContentRequest>): EmbedContentRequest {
    return EmbedContentRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EmbedContentRequest>): EmbedContentRequest {
    const message = createBaseEmbedContentRequest();
    message.model = object.model ?? "";
    message.content = (object.content !== undefined && object.content !== null)
      ? Content.fromPartial(object.content)
      : undefined;
    message.taskType = object.taskType ?? undefined;
    message.title = object.title ?? undefined;
    message.outputDimensionality = object.outputDimensionality ?? undefined;
    return message;
  },
};

function createBaseContentEmbedding(): ContentEmbedding {
  return { values: [] };
}

export const ContentEmbedding: MessageFns<ContentEmbedding> = {
  encode(message: ContentEmbedding, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    writer.uint32(10).fork();
    for (const v of message.values) {
      writer.float(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ContentEmbedding {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseContentEmbedding();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag === 13) {
            message.values.push(reader.float());

            continue;
          }

          if (tag === 10) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.values.push(reader.float());
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ContentEmbedding {
    return {
      values: globalThis.Array.isArray(object?.values) ? object.values.map((e: any) => globalThis.Number(e)) : [],
    };
  },

  toJSON(message: ContentEmbedding): unknown {
    const obj: any = {};
    if (message.values?.length) {
      obj.values = message.values;
    }
    return obj;
  },

  create(base?: DeepPartial<ContentEmbedding>): ContentEmbedding {
    return ContentEmbedding.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ContentEmbedding>): ContentEmbedding {
    const message = createBaseContentEmbedding();
    message.values = object.values?.map((e) => e) || [];
    return message;
  },
};

function createBaseEmbedContentResponse(): EmbedContentResponse {
  return { embedding: undefined };
}

export const EmbedContentResponse: MessageFns<EmbedContentResponse> = {
  encode(message: EmbedContentResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.embedding !== undefined) {
      ContentEmbedding.encode(message.embedding, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EmbedContentResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEmbedContentResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.embedding = ContentEmbedding.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EmbedContentResponse {
    return { embedding: isSet(object.embedding) ? ContentEmbedding.fromJSON(object.embedding) : undefined };
  },

  toJSON(message: EmbedContentResponse): unknown {
    const obj: any = {};
    if (message.embedding !== undefined) {
      obj.embedding = ContentEmbedding.toJSON(message.embedding);
    }
    return obj;
  },

  create(base?: DeepPartial<EmbedContentResponse>): EmbedContentResponse {
    return EmbedContentResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EmbedContentResponse>): EmbedContentResponse {
    const message = createBaseEmbedContentResponse();
    message.embedding = (object.embedding !== undefined && object.embedding !== null)
      ? ContentEmbedding.fromPartial(object.embedding)
      : undefined;
    return message;
  },
};

function createBaseBatchEmbedContentsRequest(): BatchEmbedContentsRequest {
  return { model: "", requests: [] };
}

export const BatchEmbedContentsRequest: MessageFns<BatchEmbedContentsRequest> = {
  encode(message: BatchEmbedContentsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    for (const v of message.requests) {
      EmbedContentRequest.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchEmbedContentsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchEmbedContentsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.requests.push(EmbedContentRequest.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchEmbedContentsRequest {
    return {
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      requests: globalThis.Array.isArray(object?.requests)
        ? object.requests.map((e: any) => EmbedContentRequest.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchEmbedContentsRequest): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.requests?.length) {
      obj.requests = message.requests.map((e) => EmbedContentRequest.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchEmbedContentsRequest>): BatchEmbedContentsRequest {
    return BatchEmbedContentsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchEmbedContentsRequest>): BatchEmbedContentsRequest {
    const message = createBaseBatchEmbedContentsRequest();
    message.model = object.model ?? "";
    message.requests = object.requests?.map((e) => EmbedContentRequest.fromPartial(e)) || [];
    return message;
  },
};

function createBaseBatchEmbedContentsResponse(): BatchEmbedContentsResponse {
  return { embeddings: [] };
}

export const BatchEmbedContentsResponse: MessageFns<BatchEmbedContentsResponse> = {
  encode(message: BatchEmbedContentsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.embeddings) {
      ContentEmbedding.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchEmbedContentsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchEmbedContentsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.embeddings.push(ContentEmbedding.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchEmbedContentsResponse {
    return {
      embeddings: globalThis.Array.isArray(object?.embeddings)
        ? object.embeddings.map((e: any) => ContentEmbedding.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchEmbedContentsResponse): unknown {
    const obj: any = {};
    if (message.embeddings?.length) {
      obj.embeddings = message.embeddings.map((e) => ContentEmbedding.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchEmbedContentsResponse>): BatchEmbedContentsResponse {
    return BatchEmbedContentsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchEmbedContentsResponse>): BatchEmbedContentsResponse {
    const message = createBaseBatchEmbedContentsResponse();
    message.embeddings = object.embeddings?.map((e) => ContentEmbedding.fromPartial(e)) || [];
    return message;
  },
};

function createBaseCountTokensRequest(): CountTokensRequest {
  return { model: "", contents: [], generateContentRequest: undefined };
}

export const CountTokensRequest: MessageFns<CountTokensRequest> = {
  encode(message: CountTokensRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    for (const v of message.contents) {
      Content.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.generateContentRequest !== undefined) {
      GenerateContentRequest.encode(message.generateContentRequest, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CountTokensRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCountTokensRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.contents.push(Content.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.generateContentRequest = GenerateContentRequest.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CountTokensRequest {
    return {
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      contents: globalThis.Array.isArray(object?.contents) ? object.contents.map((e: any) => Content.fromJSON(e)) : [],
      generateContentRequest: isSet(object.generateContentRequest)
        ? GenerateContentRequest.fromJSON(object.generateContentRequest)
        : undefined,
    };
  },

  toJSON(message: CountTokensRequest): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.contents?.length) {
      obj.contents = message.contents.map((e) => Content.toJSON(e));
    }
    if (message.generateContentRequest !== undefined) {
      obj.generateContentRequest = GenerateContentRequest.toJSON(message.generateContentRequest);
    }
    return obj;
  },

  create(base?: DeepPartial<CountTokensRequest>): CountTokensRequest {
    return CountTokensRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CountTokensRequest>): CountTokensRequest {
    const message = createBaseCountTokensRequest();
    message.model = object.model ?? "";
    message.contents = object.contents?.map((e) => Content.fromPartial(e)) || [];
    message.generateContentRequest =
      (object.generateContentRequest !== undefined && object.generateContentRequest !== null)
        ? GenerateContentRequest.fromPartial(object.generateContentRequest)
        : undefined;
    return message;
  },
};

function createBaseCountTokensResponse(): CountTokensResponse {
  return { totalTokens: 0, cachedContentTokenCount: 0 };
}

export const CountTokensResponse: MessageFns<CountTokensResponse> = {
  encode(message: CountTokensResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.totalTokens !== 0) {
      writer.uint32(8).int32(message.totalTokens);
    }
    if (message.cachedContentTokenCount !== 0) {
      writer.uint32(40).int32(message.cachedContentTokenCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CountTokensResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCountTokensResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.totalTokens = reader.int32();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.cachedContentTokenCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CountTokensResponse {
    return {
      totalTokens: isSet(object.totalTokens) ? globalThis.Number(object.totalTokens) : 0,
      cachedContentTokenCount: isSet(object.cachedContentTokenCount)
        ? globalThis.Number(object.cachedContentTokenCount)
        : 0,
    };
  },

  toJSON(message: CountTokensResponse): unknown {
    const obj: any = {};
    if (message.totalTokens !== 0) {
      obj.totalTokens = Math.round(message.totalTokens);
    }
    if (message.cachedContentTokenCount !== 0) {
      obj.cachedContentTokenCount = Math.round(message.cachedContentTokenCount);
    }
    return obj;
  },

  create(base?: DeepPartial<CountTokensResponse>): CountTokensResponse {
    return CountTokensResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CountTokensResponse>): CountTokensResponse {
    const message = createBaseCountTokensResponse();
    message.totalTokens = object.totalTokens ?? 0;
    message.cachedContentTokenCount = object.cachedContentTokenCount ?? 0;
    return message;
  },
};

/**
 * API for using Large Models that generate multimodal content and have
 * additional capabilities beyond text generation.
 */
export type GenerativeServiceDefinition = typeof GenerativeServiceDefinition;
export const GenerativeServiceDefinition = {
  name: "GenerativeService",
  fullName: "google.ai.generativelanguage.v1beta.GenerativeService",
  methods: {
    /**
     * Generates a model response given an input `GenerateContentRequest`.
     * Refer to the [text generation
     * guide](https://ai.google.dev/gemini-api/docs/text-generation) for detailed
     * usage information. Input capabilities differ between models, including
     * tuned models. Refer to the [model
     * guide](https://ai.google.dev/gemini-api/docs/models/gemini) and [tuning
     * guide](https://ai.google.dev/gemini-api/docs/model-tuning) for details.
     */
    generateContent: {
      name: "GenerateContent",
      requestType: GenerateContentRequest,
      requestStream: false,
      responseType: GenerateContentResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([14, 109, 111, 100, 101, 108, 44, 99, 111, 110, 116, 101, 110, 116, 115])],
          578365826: [
            Buffer.from([
              97,
              58,
              1,
              42,
              90,
              50,
              58,
              1,
              42,
              34,
              45,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              116,
              117,
              110,
              101,
              100,
              77,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              103,
              101,
              110,
              101,
              114,
              97,
              116,
              101,
              67,
              111,
              110,
              116,
              101,
              110,
              116,
              34,
              40,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              103,
              101,
              110,
              101,
              114,
              97,
              116,
              101,
              67,
              111,
              110,
              116,
              101,
              110,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * Generates a grounded answer from the model given an input
     * `GenerateAnswerRequest`.
     */
    generateAnswer: {
      name: "GenerateAnswer",
      requestType: GenerateAnswerRequest,
      requestStream: false,
      responseType: GenerateAnswerResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              43,
              109,
              111,
              100,
              101,
              108,
              44,
              99,
              111,
              110,
              116,
              101,
              110,
              116,
              115,
              44,
              115,
              97,
              102,
              101,
              116,
              121,
              95,
              115,
              101,
              116,
              116,
              105,
              110,
              103,
              115,
              44,
              97,
              110,
              115,
              119,
              101,
              114,
              95,
              115,
              116,
              121,
              108,
              101,
            ]),
          ],
          578365826: [
            Buffer.from([
              44,
              58,
              1,
              42,
              34,
              39,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              103,
              101,
              110,
              101,
              114,
              97,
              116,
              101,
              65,
              110,
              115,
              119,
              101,
              114,
            ]),
          ],
        },
      },
    },
    /**
     * Generates a [streamed
     * response](https://ai.google.dev/gemini-api/docs/text-generation?lang=python#generate-a-text-stream)
     * from the model given an input `GenerateContentRequest`.
     */
    streamGenerateContent: {
      name: "StreamGenerateContent",
      requestType: GenerateContentRequest,
      requestStream: false,
      responseType: GenerateContentResponse,
      responseStream: true,
      options: {
        _unknownFields: {
          8410: [Buffer.from([14, 109, 111, 100, 101, 108, 44, 99, 111, 110, 116, 101, 110, 116, 115])],
          578365826: [
            Buffer.from([
              51,
              58,
              1,
              42,
              34,
              46,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              115,
              116,
              114,
              101,
              97,
              109,
              71,
              101,
              110,
              101,
              114,
              97,
              116,
              101,
              67,
              111,
              110,
              116,
              101,
              110,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * Generates a text embedding vector from the input `Content` using the
     * specified [Gemini Embedding
     * model](https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding).
     */
    embedContent: {
      name: "EmbedContent",
      requestType: EmbedContentRequest,
      requestStream: false,
      responseType: EmbedContentResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([13, 109, 111, 100, 101, 108, 44, 99, 111, 110, 116, 101, 110, 116])],
          578365826: [
            Buffer.from([
              42,
              58,
              1,
              42,
              34,
              37,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              101,
              109,
              98,
              101,
              100,
              67,
              111,
              110,
              116,
              101,
              110,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * Generates multiple embedding vectors from the input `Content` which
     * consists of a batch of strings represented as `EmbedContentRequest`
     * objects.
     */
    batchEmbedContents: {
      name: "BatchEmbedContents",
      requestType: BatchEmbedContentsRequest,
      requestStream: false,
      responseType: BatchEmbedContentsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([14, 109, 111, 100, 101, 108, 44, 114, 101, 113, 117, 101, 115, 116, 115])],
          578365826: [
            Buffer.from([
              48,
              58,
              1,
              42,
              34,
              43,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              98,
              97,
              116,
              99,
              104,
              69,
              109,
              98,
              101,
              100,
              67,
              111,
              110,
              116,
              101,
              110,
              116,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Runs a model's tokenizer on input `Content` and returns the token count.
     * Refer to the [tokens guide](https://ai.google.dev/gemini-api/docs/tokens)
     * to learn more about tokens.
     */
    countTokens: {
      name: "CountTokens",
      requestType: CountTokensRequest,
      requestStream: false,
      responseType: CountTokensResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([14, 109, 111, 100, 101, 108, 44, 99, 111, 110, 116, 101, 110, 116, 115])],
          578365826: [
            Buffer.from([
              41,
              58,
              1,
              42,
              34,
              36,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              47,
              123,
              109,
              111,
              100,
              101,
              108,
              61,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              99,
              111,
              117,
              110,
              116,
              84,
              111,
              107,
              101,
              110,
              115,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface GenerativeServiceImplementation<CallContextExt = {}> {
  /**
   * Generates a model response given an input `GenerateContentRequest`.
   * Refer to the [text generation
   * guide](https://ai.google.dev/gemini-api/docs/text-generation) for detailed
   * usage information. Input capabilities differ between models, including
   * tuned models. Refer to the [model
   * guide](https://ai.google.dev/gemini-api/docs/models/gemini) and [tuning
   * guide](https://ai.google.dev/gemini-api/docs/model-tuning) for details.
   */
  generateContent(
    request: GenerateContentRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<GenerateContentResponse>>;
  /**
   * Generates a grounded answer from the model given an input
   * `GenerateAnswerRequest`.
   */
  generateAnswer(
    request: GenerateAnswerRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<GenerateAnswerResponse>>;
  /**
   * Generates a [streamed
   * response](https://ai.google.dev/gemini-api/docs/text-generation?lang=python#generate-a-text-stream)
   * from the model given an input `GenerateContentRequest`.
   */
  streamGenerateContent(
    request: GenerateContentRequest,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<GenerateContentResponse>>;
  /**
   * Generates a text embedding vector from the input `Content` using the
   * specified [Gemini Embedding
   * model](https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding).
   */
  embedContent(
    request: EmbedContentRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<EmbedContentResponse>>;
  /**
   * Generates multiple embedding vectors from the input `Content` which
   * consists of a batch of strings represented as `EmbedContentRequest`
   * objects.
   */
  batchEmbedContents(
    request: BatchEmbedContentsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<BatchEmbedContentsResponse>>;
  /**
   * Runs a model's tokenizer on input `Content` and returns the token count.
   * Refer to the [tokens guide](https://ai.google.dev/gemini-api/docs/tokens)
   * to learn more about tokens.
   */
  countTokens(
    request: CountTokensRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<CountTokensResponse>>;
}

export interface GenerativeServiceClient<CallOptionsExt = {}> {
  /**
   * Generates a model response given an input `GenerateContentRequest`.
   * Refer to the [text generation
   * guide](https://ai.google.dev/gemini-api/docs/text-generation) for detailed
   * usage information. Input capabilities differ between models, including
   * tuned models. Refer to the [model
   * guide](https://ai.google.dev/gemini-api/docs/models/gemini) and [tuning
   * guide](https://ai.google.dev/gemini-api/docs/model-tuning) for details.
   */
  generateContent(
    request: DeepPartial<GenerateContentRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<GenerateContentResponse>;
  /**
   * Generates a grounded answer from the model given an input
   * `GenerateAnswerRequest`.
   */
  generateAnswer(
    request: DeepPartial<GenerateAnswerRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<GenerateAnswerResponse>;
  /**
   * Generates a [streamed
   * response](https://ai.google.dev/gemini-api/docs/text-generation?lang=python#generate-a-text-stream)
   * from the model given an input `GenerateContentRequest`.
   */
  streamGenerateContent(
    request: DeepPartial<GenerateContentRequest>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<GenerateContentResponse>;
  /**
   * Generates a text embedding vector from the input `Content` using the
   * specified [Gemini Embedding
   * model](https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding).
   */
  embedContent(
    request: DeepPartial<EmbedContentRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<EmbedContentResponse>;
  /**
   * Generates multiple embedding vectors from the input `Content` which
   * consists of a batch of strings represented as `EmbedContentRequest`
   * objects.
   */
  batchEmbedContents(
    request: DeepPartial<BatchEmbedContentsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<BatchEmbedContentsResponse>;
  /**
   * Runs a model's tokenizer on input `Content` and returns the token count.
   * Refer to the [tokens guide](https://ai.google.dev/gemini-api/docs/tokens)
   * to learn more about tokens.
   */
  countTokens(
    request: DeepPartial<CountTokensRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<CountTokensResponse>;
}

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export type ServerStreamingMethodResult<Response> = { [Symbol.asyncIterator](): AsyncIterator<Response, void> };

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
