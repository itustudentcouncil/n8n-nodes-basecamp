// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/dataflow/v1beta3/jobs.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Duration } from "../../protobuf/duration.js";
import { Struct } from "../../protobuf/struct.js";
import { Timestamp } from "../../protobuf/timestamp.js";
import { Environment, JobType, jobTypeFromJSON, jobTypeToJSON } from "./environment.js";
import { Snapshot } from "./snapshots.js";

export const protobufPackage = "google.dataflow.v1beta3";

/** Type of transform or stage operation. */
export enum KindType {
  /** UNKNOWN_KIND - Unrecognized transform type. */
  UNKNOWN_KIND = 0,
  /** PAR_DO_KIND - ParDo transform. */
  PAR_DO_KIND = 1,
  /** GROUP_BY_KEY_KIND - Group By Key transform. */
  GROUP_BY_KEY_KIND = 2,
  /** FLATTEN_KIND - Flatten transform. */
  FLATTEN_KIND = 3,
  /** READ_KIND - Read transform. */
  READ_KIND = 4,
  /** WRITE_KIND - Write transform. */
  WRITE_KIND = 5,
  /** CONSTANT_KIND - Constructs from a constant value, such as with Create.of. */
  CONSTANT_KIND = 6,
  /** SINGLETON_KIND - Creates a Singleton view of a collection. */
  SINGLETON_KIND = 7,
  /** SHUFFLE_KIND - Opening or closing a shuffle session, often as part of a GroupByKey. */
  SHUFFLE_KIND = 8,
  UNRECOGNIZED = -1,
}

export function kindTypeFromJSON(object: any): KindType {
  switch (object) {
    case 0:
    case "UNKNOWN_KIND":
      return KindType.UNKNOWN_KIND;
    case 1:
    case "PAR_DO_KIND":
      return KindType.PAR_DO_KIND;
    case 2:
    case "GROUP_BY_KEY_KIND":
      return KindType.GROUP_BY_KEY_KIND;
    case 3:
    case "FLATTEN_KIND":
      return KindType.FLATTEN_KIND;
    case 4:
    case "READ_KIND":
      return KindType.READ_KIND;
    case 5:
    case "WRITE_KIND":
      return KindType.WRITE_KIND;
    case 6:
    case "CONSTANT_KIND":
      return KindType.CONSTANT_KIND;
    case 7:
    case "SINGLETON_KIND":
      return KindType.SINGLETON_KIND;
    case 8:
    case "SHUFFLE_KIND":
      return KindType.SHUFFLE_KIND;
    case -1:
    case "UNRECOGNIZED":
    default:
      return KindType.UNRECOGNIZED;
  }
}

export function kindTypeToJSON(object: KindType): string {
  switch (object) {
    case KindType.UNKNOWN_KIND:
      return "UNKNOWN_KIND";
    case KindType.PAR_DO_KIND:
      return "PAR_DO_KIND";
    case KindType.GROUP_BY_KEY_KIND:
      return "GROUP_BY_KEY_KIND";
    case KindType.FLATTEN_KIND:
      return "FLATTEN_KIND";
    case KindType.READ_KIND:
      return "READ_KIND";
    case KindType.WRITE_KIND:
      return "WRITE_KIND";
    case KindType.CONSTANT_KIND:
      return "CONSTANT_KIND";
    case KindType.SINGLETON_KIND:
      return "SINGLETON_KIND";
    case KindType.SHUFFLE_KIND:
      return "SHUFFLE_KIND";
    case KindType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Describes the overall state of a [google.dataflow.v1beta3.Job][google.dataflow.v1beta3.Job]. */
export enum JobState {
  /** JOB_STATE_UNKNOWN - The job's run state isn't specified. */
  JOB_STATE_UNKNOWN = 0,
  /**
   * JOB_STATE_STOPPED - `JOB_STATE_STOPPED` indicates that the job has not
   * yet started to run.
   */
  JOB_STATE_STOPPED = 1,
  /** JOB_STATE_RUNNING - `JOB_STATE_RUNNING` indicates that the job is currently running. */
  JOB_STATE_RUNNING = 2,
  /**
   * JOB_STATE_DONE - `JOB_STATE_DONE` indicates that the job has successfully completed.
   * This is a terminal job state.  This state may be set by the Cloud Dataflow
   * service, as a transition from `JOB_STATE_RUNNING`. It may also be set via a
   * Cloud Dataflow `UpdateJob` call, if the job has not yet reached a terminal
   * state.
   */
  JOB_STATE_DONE = 3,
  /**
   * JOB_STATE_FAILED - `JOB_STATE_FAILED` indicates that the job has failed.  This is a
   * terminal job state.  This state may only be set by the Cloud Dataflow
   * service, and only as a transition from `JOB_STATE_RUNNING`.
   */
  JOB_STATE_FAILED = 4,
  /**
   * JOB_STATE_CANCELLED - `JOB_STATE_CANCELLED` indicates that the job has been explicitly
   * cancelled. This is a terminal job state. This state may only be
   * set via a Cloud Dataflow `UpdateJob` call, and only if the job has not
   * yet reached another terminal state.
   */
  JOB_STATE_CANCELLED = 5,
  /**
   * JOB_STATE_UPDATED - `JOB_STATE_UPDATED` indicates that the job was successfully updated,
   * meaning that this job was stopped and another job was started, inheriting
   * state from this one. This is a terminal job state. This state may only be
   * set by the Cloud Dataflow service, and only as a transition from
   * `JOB_STATE_RUNNING`.
   */
  JOB_STATE_UPDATED = 6,
  /**
   * JOB_STATE_DRAINING - `JOB_STATE_DRAINING` indicates that the job is in the process of draining.
   * A draining job has stopped pulling from its input sources and is processing
   * any data that remains in-flight. This state may be set via a Cloud Dataflow
   * `UpdateJob` call, but only as a transition from `JOB_STATE_RUNNING`. Jobs
   * that are draining may only transition to `JOB_STATE_DRAINED`,
   * `JOB_STATE_CANCELLED`, or `JOB_STATE_FAILED`.
   */
  JOB_STATE_DRAINING = 7,
  /**
   * JOB_STATE_DRAINED - `JOB_STATE_DRAINED` indicates that the job has been drained.
   * A drained job terminated by stopping pulling from its input sources and
   * processing any data that remained in-flight when draining was requested.
   * This state is a terminal state, may only be set by the Cloud Dataflow
   * service, and only as a transition from `JOB_STATE_DRAINING`.
   */
  JOB_STATE_DRAINED = 8,
  /**
   * JOB_STATE_PENDING - `JOB_STATE_PENDING` indicates that the job has been created but is not yet
   * running.  Jobs that are pending may only transition to `JOB_STATE_RUNNING`,
   * or `JOB_STATE_FAILED`.
   */
  JOB_STATE_PENDING = 9,
  /**
   * JOB_STATE_CANCELLING - `JOB_STATE_CANCELLING` indicates that the job has been explicitly cancelled
   * and is in the process of stopping.  Jobs that are cancelling may only
   * transition to `JOB_STATE_CANCELLED` or `JOB_STATE_FAILED`.
   */
  JOB_STATE_CANCELLING = 10,
  /**
   * JOB_STATE_QUEUED - `JOB_STATE_QUEUED` indicates that the job has been created but is being
   * delayed until launch. Jobs that are queued may only transition to
   * `JOB_STATE_PENDING` or `JOB_STATE_CANCELLED`.
   */
  JOB_STATE_QUEUED = 11,
  /**
   * JOB_STATE_RESOURCE_CLEANING_UP - `JOB_STATE_RESOURCE_CLEANING_UP` indicates that the batch job's associated
   * resources are currently being cleaned up after a successful run.
   * Currently, this is an opt-in feature, please reach out to Cloud support
   * team if you are interested.
   */
  JOB_STATE_RESOURCE_CLEANING_UP = 12,
  UNRECOGNIZED = -1,
}

export function jobStateFromJSON(object: any): JobState {
  switch (object) {
    case 0:
    case "JOB_STATE_UNKNOWN":
      return JobState.JOB_STATE_UNKNOWN;
    case 1:
    case "JOB_STATE_STOPPED":
      return JobState.JOB_STATE_STOPPED;
    case 2:
    case "JOB_STATE_RUNNING":
      return JobState.JOB_STATE_RUNNING;
    case 3:
    case "JOB_STATE_DONE":
      return JobState.JOB_STATE_DONE;
    case 4:
    case "JOB_STATE_FAILED":
      return JobState.JOB_STATE_FAILED;
    case 5:
    case "JOB_STATE_CANCELLED":
      return JobState.JOB_STATE_CANCELLED;
    case 6:
    case "JOB_STATE_UPDATED":
      return JobState.JOB_STATE_UPDATED;
    case 7:
    case "JOB_STATE_DRAINING":
      return JobState.JOB_STATE_DRAINING;
    case 8:
    case "JOB_STATE_DRAINED":
      return JobState.JOB_STATE_DRAINED;
    case 9:
    case "JOB_STATE_PENDING":
      return JobState.JOB_STATE_PENDING;
    case 10:
    case "JOB_STATE_CANCELLING":
      return JobState.JOB_STATE_CANCELLING;
    case 11:
    case "JOB_STATE_QUEUED":
      return JobState.JOB_STATE_QUEUED;
    case 12:
    case "JOB_STATE_RESOURCE_CLEANING_UP":
      return JobState.JOB_STATE_RESOURCE_CLEANING_UP;
    case -1:
    case "UNRECOGNIZED":
    default:
      return JobState.UNRECOGNIZED;
  }
}

export function jobStateToJSON(object: JobState): string {
  switch (object) {
    case JobState.JOB_STATE_UNKNOWN:
      return "JOB_STATE_UNKNOWN";
    case JobState.JOB_STATE_STOPPED:
      return "JOB_STATE_STOPPED";
    case JobState.JOB_STATE_RUNNING:
      return "JOB_STATE_RUNNING";
    case JobState.JOB_STATE_DONE:
      return "JOB_STATE_DONE";
    case JobState.JOB_STATE_FAILED:
      return "JOB_STATE_FAILED";
    case JobState.JOB_STATE_CANCELLED:
      return "JOB_STATE_CANCELLED";
    case JobState.JOB_STATE_UPDATED:
      return "JOB_STATE_UPDATED";
    case JobState.JOB_STATE_DRAINING:
      return "JOB_STATE_DRAINING";
    case JobState.JOB_STATE_DRAINED:
      return "JOB_STATE_DRAINED";
    case JobState.JOB_STATE_PENDING:
      return "JOB_STATE_PENDING";
    case JobState.JOB_STATE_CANCELLING:
      return "JOB_STATE_CANCELLING";
    case JobState.JOB_STATE_QUEUED:
      return "JOB_STATE_QUEUED";
    case JobState.JOB_STATE_RESOURCE_CLEANING_UP:
      return "JOB_STATE_RESOURCE_CLEANING_UP";
    case JobState.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Selector for how much information is returned in Job responses. */
export enum JobView {
  /**
   * JOB_VIEW_UNKNOWN - The job view to return isn't specified, or is unknown.
   * Responses will contain at least the `JOB_VIEW_SUMMARY` information,
   * and may contain additional information.
   */
  JOB_VIEW_UNKNOWN = 0,
  /**
   * JOB_VIEW_SUMMARY - Request summary information only:
   * Project ID, Job ID, job name, job type, job status, start/end time,
   * and Cloud SDK version details.
   */
  JOB_VIEW_SUMMARY = 1,
  /** JOB_VIEW_ALL - Request all information available for this job. */
  JOB_VIEW_ALL = 2,
  /**
   * JOB_VIEW_DESCRIPTION - Request summary info and limited job description data for steps, labels and
   * environment.
   */
  JOB_VIEW_DESCRIPTION = 3,
  UNRECOGNIZED = -1,
}

export function jobViewFromJSON(object: any): JobView {
  switch (object) {
    case 0:
    case "JOB_VIEW_UNKNOWN":
      return JobView.JOB_VIEW_UNKNOWN;
    case 1:
    case "JOB_VIEW_SUMMARY":
      return JobView.JOB_VIEW_SUMMARY;
    case 2:
    case "JOB_VIEW_ALL":
      return JobView.JOB_VIEW_ALL;
    case 3:
    case "JOB_VIEW_DESCRIPTION":
      return JobView.JOB_VIEW_DESCRIPTION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return JobView.UNRECOGNIZED;
  }
}

export function jobViewToJSON(object: JobView): string {
  switch (object) {
    case JobView.JOB_VIEW_UNKNOWN:
      return "JOB_VIEW_UNKNOWN";
    case JobView.JOB_VIEW_SUMMARY:
      return "JOB_VIEW_SUMMARY";
    case JobView.JOB_VIEW_ALL:
      return "JOB_VIEW_ALL";
    case JobView.JOB_VIEW_DESCRIPTION:
      return "JOB_VIEW_DESCRIPTION";
    case JobView.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Defines a job to be run by the Cloud Dataflow service. */
export interface Job {
  /**
   * The unique ID of this job.
   *
   * This field is set by the Cloud Dataflow service when the Job is
   * created, and is immutable for the life of the job.
   */
  id: string;
  /** The ID of the Cloud Platform project that the job belongs to. */
  projectId: string;
  /**
   * The user-specified Cloud Dataflow job name.
   *
   * Only one Job with a given name may exist in a project at any
   * given time. If a caller attempts to create a Job with the same
   * name as an already-existing Job, the attempt returns the
   * existing Job.
   *
   * The name must match the regular expression
   * `[a-z]([-a-z0-9]{0,1022}[a-z0-9])?`
   */
  name: string;
  /** The type of Cloud Dataflow job. */
  type: JobType;
  /** The environment for the job. */
  environment:
    | Environment
    | undefined;
  /**
   * Exactly one of step or steps_location should be specified.
   *
   * The top-level steps that constitute the entire job. Only retrieved with
   * JOB_VIEW_ALL.
   */
  steps: Step[];
  /** The Cloud Storage location where the steps are stored. */
  stepsLocation: string;
  /**
   * The current state of the job.
   *
   * Jobs are created in the `JOB_STATE_STOPPED` state unless otherwise
   * specified.
   *
   * A job in the `JOB_STATE_RUNNING` state may asynchronously enter a
   * terminal state. After a job has reached a terminal state, no
   * further state updates may be made.
   *
   * This field may be mutated by the Cloud Dataflow service;
   * callers cannot mutate it.
   */
  currentState: JobState;
  /** The timestamp associated with the current state. */
  currentStateTime:
    | Date
    | undefined;
  /**
   * The job's requested state.
   *
   * `UpdateJob` may be used to switch between the `JOB_STATE_STOPPED` and
   * `JOB_STATE_RUNNING` states, by setting requested_state.  `UpdateJob` may
   * also be used to directly set a job's requested state to
   * `JOB_STATE_CANCELLED` or `JOB_STATE_DONE`, irrevocably terminating the
   * job if it has not already reached a terminal state.
   */
  requestedState: JobState;
  /** Deprecated. */
  executionInfo:
    | JobExecutionInfo
    | undefined;
  /**
   * The timestamp when the job was initially created. Immutable and set by the
   * Cloud Dataflow service.
   */
  createTime:
    | Date
    | undefined;
  /**
   * If this job is an update of an existing job, this field is the job ID
   * of the job it replaced.
   *
   * When sending a `CreateJobRequest`, you can update a job by specifying it
   * here. The job named here is stopped, and its intermediate state is
   * transferred to this job.
   */
  replaceJobId: string;
  /**
   * The map of transform name prefixes of the job to be replaced to the
   * corresponding name prefixes of the new job.
   */
  transformNameMapping: { [key: string]: string };
  /**
   * The client's unique identifier of the job, re-used across retried attempts.
   * If this field is set, the service will ensure its uniqueness.
   * The request to create a job will fail if the service has knowledge of a
   * previously submitted job with the same client's ID and job name.
   * The caller may use this field to ensure idempotence of job
   * creation across retried attempts to create a job.
   * By default, the field is empty and, in that case, the service ignores it.
   */
  clientRequestId: string;
  /**
   * If another job is an update of this job (and thus, this job is in
   * `JOB_STATE_UPDATED`), this field contains the ID of that job.
   */
  replacedByJobId: string;
  /**
   * A set of files the system should be aware of that are used
   * for temporary storage. These temporary files will be
   * removed on job completion.
   * No duplicates are allowed.
   * No file patterns are supported.
   *
   * The supported files are:
   *
   * Google Cloud Storage:
   *
   *    storage.googleapis.com/{bucket}/{object}
   *    bucket.storage.googleapis.com/{object}
   */
  tempFiles: string[];
  /**
   * User-defined labels for this job.
   *
   * The labels map can contain no more than 64 entries.  Entries of the labels
   * map are UTF8 strings that comply with the following restrictions:
   *
   * * Keys must conform to regexp:  [\p{Ll}\p{Lo}][\p{Ll}\p{Lo}\p{N}_-]{0,62}
   * * Values must conform to regexp:  [\p{Ll}\p{Lo}\p{N}_-]{0,63}
   * * Both keys and values are additionally constrained to be <= 128 bytes in
   * size.
   */
  labels: { [key: string]: string };
  /**
   * The [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) that
   * contains this job.
   */
  location: string;
  /**
   * Preliminary field: The format of this data may change at any time.
   * A description of the user pipeline and stages through which it is executed.
   * Created by Cloud Dataflow service.  Only retrieved with
   * JOB_VIEW_DESCRIPTION or JOB_VIEW_ALL.
   */
  pipelineDescription:
    | PipelineDescription
    | undefined;
  /**
   * This field may be mutated by the Cloud Dataflow service;
   * callers cannot mutate it.
   */
  stageStates: ExecutionStageState[];
  /**
   * This field is populated by the Dataflow service to support filtering jobs
   * by the metadata values provided here. Populated for ListJobs and all GetJob
   * views SUMMARY and higher.
   */
  jobMetadata:
    | JobMetadata
    | undefined;
  /**
   * The timestamp when the job was started (transitioned to JOB_STATE_PENDING).
   * Flexible resource scheduling jobs are started with some delay after job
   * creation, so start_time is unset before start and is updated when the
   * job is started by the Cloud Dataflow service. For other jobs, start_time
   * always equals to create_time and is immutable and set by the Cloud Dataflow
   * service.
   */
  startTime:
    | Date
    | undefined;
  /**
   * If this is specified, the job's initial state is populated from the given
   * snapshot.
   */
  createdFromSnapshotId: string;
  /**
   * Reserved for future use. This field is set only in responses from the
   * server; it is ignored if it is set in any requests.
   */
  satisfiesPzs: boolean;
}

export interface Job_TransformNameMappingEntry {
  key: string;
  value: string;
}

export interface Job_LabelsEntry {
  key: string;
  value: string;
}

/** Metadata for a Datastore connector used by the job. */
export interface DatastoreIODetails {
  /** Namespace used in the connection. */
  namespace: string;
  /** ProjectId accessed in the connection. */
  projectId: string;
}

/** Metadata for a Pub/Sub connector used by the job. */
export interface PubSubIODetails {
  /** Topic accessed in the connection. */
  topic: string;
  /** Subscription used in the connection. */
  subscription: string;
}

/** Metadata for a File connector used by the job. */
export interface FileIODetails {
  /** File Pattern used to access files by the connector. */
  filePattern: string;
}

/** Metadata for a Cloud Bigtable connector used by the job. */
export interface BigTableIODetails {
  /** ProjectId accessed in the connection. */
  projectId: string;
  /** InstanceId accessed in the connection. */
  instanceId: string;
  /** TableId accessed in the connection. */
  tableId: string;
}

/** Metadata for a BigQuery connector used by the job. */
export interface BigQueryIODetails {
  /** Table accessed in the connection. */
  table: string;
  /** Dataset accessed in the connection. */
  dataset: string;
  /** Project accessed in the connection. */
  projectId: string;
  /** Query used to access data in the connection. */
  query: string;
}

/** Metadata for a Spanner connector used by the job. */
export interface SpannerIODetails {
  /** ProjectId accessed in the connection. */
  projectId: string;
  /** InstanceId accessed in the connection. */
  instanceId: string;
  /** DatabaseId accessed in the connection. */
  databaseId: string;
}

/** The version of the SDK used to run the job. */
export interface SdkVersion {
  /** The version of the SDK used to run the job. */
  version: string;
  /** A readable string describing the version of the SDK. */
  versionDisplayName: string;
  /** The support status for this SDK version. */
  sdkSupportStatus: SdkVersion_SdkSupportStatus;
}

/** The support status of the SDK used to run the job. */
export enum SdkVersion_SdkSupportStatus {
  /** UNKNOWN - Cloud Dataflow is unaware of this version. */
  UNKNOWN = 0,
  /** SUPPORTED - This is a known version of an SDK, and is supported. */
  SUPPORTED = 1,
  /** STALE - A newer version of the SDK family exists, and an update is recommended. */
  STALE = 2,
  /**
   * DEPRECATED - This version of the SDK is deprecated and will eventually be
   * unsupported.
   */
  DEPRECATED = 3,
  /** UNSUPPORTED - Support for this SDK version has ended and it should no longer be used. */
  UNSUPPORTED = 4,
  UNRECOGNIZED = -1,
}

export function sdkVersion_SdkSupportStatusFromJSON(object: any): SdkVersion_SdkSupportStatus {
  switch (object) {
    case 0:
    case "UNKNOWN":
      return SdkVersion_SdkSupportStatus.UNKNOWN;
    case 1:
    case "SUPPORTED":
      return SdkVersion_SdkSupportStatus.SUPPORTED;
    case 2:
    case "STALE":
      return SdkVersion_SdkSupportStatus.STALE;
    case 3:
    case "DEPRECATED":
      return SdkVersion_SdkSupportStatus.DEPRECATED;
    case 4:
    case "UNSUPPORTED":
      return SdkVersion_SdkSupportStatus.UNSUPPORTED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SdkVersion_SdkSupportStatus.UNRECOGNIZED;
  }
}

export function sdkVersion_SdkSupportStatusToJSON(object: SdkVersion_SdkSupportStatus): string {
  switch (object) {
    case SdkVersion_SdkSupportStatus.UNKNOWN:
      return "UNKNOWN";
    case SdkVersion_SdkSupportStatus.SUPPORTED:
      return "SUPPORTED";
    case SdkVersion_SdkSupportStatus.STALE:
      return "STALE";
    case SdkVersion_SdkSupportStatus.DEPRECATED:
      return "DEPRECATED";
    case SdkVersion_SdkSupportStatus.UNSUPPORTED:
      return "UNSUPPORTED";
    case SdkVersion_SdkSupportStatus.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Metadata available primarily for filtering jobs. Will be included in the
 * ListJob response and Job SUMMARY view.
 */
export interface JobMetadata {
  /** The SDK version used to run the job. */
  sdkVersion:
    | SdkVersion
    | undefined;
  /** Identification of a Spanner source used in the Dataflow job. */
  spannerDetails: SpannerIODetails[];
  /** Identification of a BigQuery source used in the Dataflow job. */
  bigqueryDetails: BigQueryIODetails[];
  /** Identification of a Cloud Bigtable source used in the Dataflow job. */
  bigTableDetails: BigTableIODetails[];
  /** Identification of a Pub/Sub source used in the Dataflow job. */
  pubsubDetails: PubSubIODetails[];
  /** Identification of a File source used in the Dataflow job. */
  fileDetails: FileIODetails[];
  /** Identification of a Datastore source used in the Dataflow job. */
  datastoreDetails: DatastoreIODetails[];
}

/** A message describing the state of a particular execution stage. */
export interface ExecutionStageState {
  /** The name of the execution stage. */
  executionStageName: string;
  /** Executions stage states allow the same set of values as JobState. */
  executionStageState: JobState;
  /** The time at which the stage transitioned to this state. */
  currentStateTime: Date | undefined;
}

/**
 * A descriptive representation of submitted pipeline as well as the executed
 * form.  This data is provided by the Dataflow service for ease of visualizing
 * the pipeline and interpreting Dataflow provided metrics.
 */
export interface PipelineDescription {
  /** Description of each transform in the pipeline and collections between them. */
  originalPipelineTransform: TransformSummary[];
  /** Description of each stage of execution of the pipeline. */
  executionPipelineStage: ExecutionStageSummary[];
  /** Pipeline level display data. */
  displayData: DisplayData[];
}

/** Description of the type, names/ids, and input/outputs for a transform. */
export interface TransformSummary {
  /** Type of transform. */
  kind: KindType;
  /** SDK generated id of this transform instance. */
  id: string;
  /** User provided name for this transform instance. */
  name: string;
  /** Transform-specific display data. */
  displayData: DisplayData[];
  /** User  names for all collection outputs to this transform. */
  outputCollectionName: string[];
  /** User names for all collection inputs to this transform. */
  inputCollectionName: string[];
}

/**
 * Description of the composing transforms, names/ids, and input/outputs of a
 * stage of execution.  Some composing transforms and sources may have been
 * generated by the Dataflow service during execution planning.
 */
export interface ExecutionStageSummary {
  /** Dataflow service generated name for this stage. */
  name: string;
  /** Dataflow service generated id for this stage. */
  id: string;
  /** Type of transform this stage is executing. */
  kind: KindType;
  /** Input sources for this stage. */
  inputSource: ExecutionStageSummary_StageSource[];
  /** Output sources for this stage. */
  outputSource: ExecutionStageSummary_StageSource[];
  /** Other stages that must complete before this stage can run. */
  prerequisiteStage: string[];
  /** Transforms that comprise this execution stage. */
  componentTransform: ExecutionStageSummary_ComponentTransform[];
  /** Collections produced and consumed by component transforms of this stage. */
  componentSource: ExecutionStageSummary_ComponentSource[];
}

/** Description of an input or output of an execution stage. */
export interface ExecutionStageSummary_StageSource {
  /** Human-readable name for this source; may be user or system generated. */
  userName: string;
  /** Dataflow service generated name for this source. */
  name: string;
  /**
   * User name for the original user transform or collection with which this
   * source is most closely associated.
   */
  originalTransformOrCollection: string;
  /** Size of the source, if measurable. */
  sizeBytes: Long;
}

/** Description of a transform executed as part of an execution stage. */
export interface ExecutionStageSummary_ComponentTransform {
  /** Human-readable name for this transform; may be user or system generated. */
  userName: string;
  /** Dataflow service generated name for this source. */
  name: string;
  /**
   * User name for the original user transform with which this transform is
   * most closely associated.
   */
  originalTransform: string;
}

/**
 * Description of an interstitial value between transforms in an execution
 * stage.
 */
export interface ExecutionStageSummary_ComponentSource {
  /** Human-readable name for this transform; may be user or system generated. */
  userName: string;
  /** Dataflow service generated name for this source. */
  name: string;
  /**
   * User name for the original user transform or collection with which this
   * source is most closely associated.
   */
  originalTransformOrCollection: string;
}

/** Data provided with a pipeline or transform to provide descriptive info. */
export interface DisplayData {
  /**
   * The key identifying the display data.
   * This is intended to be used as a label for the display data
   * when viewed in a dax monitoring system.
   */
  key: string;
  /**
   * The namespace for the key. This is usually a class name or programming
   * language namespace (i.e. python module) which defines the display data.
   * This allows a dax monitoring system to specially handle the data
   * and perform custom rendering.
   */
  namespace: string;
  /** Contains value if the data is of string type. */
  strValue?:
    | string
    | undefined;
  /** Contains value if the data is of int64 type. */
  int64Value?:
    | Long
    | undefined;
  /** Contains value if the data is of float type. */
  floatValue?:
    | number
    | undefined;
  /** Contains value if the data is of java class type. */
  javaClassValue?:
    | string
    | undefined;
  /** Contains value if the data is of timestamp type. */
  timestampValue?:
    | Date
    | undefined;
  /** Contains value if the data is of duration type. */
  durationValue?:
    | Duration
    | undefined;
  /** Contains value if the data is of a boolean type. */
  boolValue?:
    | boolean
    | undefined;
  /**
   * A possible additional shorter value to display.
   * For example a java_class_name_value of com.mypackage.MyDoFn
   * will be stored with MyDoFn as the short_str_value and
   * com.mypackage.MyDoFn as the java_class_name value.
   * short_str_value can be displayed and java_class_name_value
   * will be displayed as a tooltip.
   */
  shortStrValue: string;
  /** An optional full URL. */
  url: string;
  /** An optional label to display in a dax UI for the element. */
  label: string;
}

/**
 * Defines a particular step within a Cloud Dataflow job.
 *
 * A job consists of multiple steps, each of which performs some
 * specific operation as part of the overall job.  Data is typically
 * passed from one step to another as part of the job.
 *
 * Here's an example of a sequence of steps which together implement a
 * Map-Reduce job:
 *
 *   * Read a collection of data from some source, parsing the
 *     collection's elements.
 *
 *   * Validate the elements.
 *
 *   * Apply a user-defined function to map each element to some value
 *     and extract an element-specific key value.
 *
 *   * Group elements with the same key into a single element with
 *     that key, transforming a multiply-keyed collection into a
 *     uniquely-keyed collection.
 *
 *   * Write the elements out to some data sink.
 *
 * Note that the Cloud Dataflow service may be used to run many different
 * types of jobs, not just Map-Reduce.
 */
export interface Step {
  /** The kind of step in the Cloud Dataflow job. */
  kind: string;
  /**
   * The name that identifies the step. This must be unique for each
   * step with respect to all other steps in the Cloud Dataflow job.
   */
  name: string;
  /**
   * Named properties associated with the step. Each kind of
   * predefined step has its own required set of properties.
   * Must be provided on Create.  Only retrieved with JOB_VIEW_ALL.
   */
  properties: { [key: string]: any } | undefined;
}

/**
 * Additional information about how a Cloud Dataflow job will be executed that
 * isn't contained in the submitted job.
 */
export interface JobExecutionInfo {
  /** A mapping from each stage to the information about that stage. */
  stages: { [key: string]: JobExecutionStageInfo };
}

export interface JobExecutionInfo_StagesEntry {
  key: string;
  value: JobExecutionStageInfo | undefined;
}

/**
 * Contains information about how a particular
 * [google.dataflow.v1beta3.Step][google.dataflow.v1beta3.Step] will be executed.
 */
export interface JobExecutionStageInfo {
  /**
   * The steps associated with the execution stage.
   * Note that stages may have several steps, and that a given step
   * might be run by more than one stage.
   */
  stepName: string[];
}

/** Request to create a Cloud Dataflow job. */
export interface CreateJobRequest {
  /** The ID of the Cloud Platform project that the job belongs to. */
  projectId: string;
  /** The job to create. */
  job:
    | Job
    | undefined;
  /** The level of information requested in response. */
  view: JobView;
  /** Deprecated. This field is now in the Job message. */
  replaceJobId: string;
  /**
   * The [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) that
   * contains this job.
   */
  location: string;
}

/** Request to get the state of a Cloud Dataflow job. */
export interface GetJobRequest {
  /** The ID of the Cloud Platform project that the job belongs to. */
  projectId: string;
  /** The job ID. */
  jobId: string;
  /** The level of information requested in response. */
  view: JobView;
  /**
   * The [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) that
   * contains this job.
   */
  location: string;
}

/** Request to update a Cloud Dataflow job. */
export interface UpdateJobRequest {
  /** The ID of the Cloud Platform project that the job belongs to. */
  projectId: string;
  /** The job ID. */
  jobId: string;
  /**
   * The updated job.
   * Only the job state is updatable; other fields will be ignored.
   */
  job:
    | Job
    | undefined;
  /**
   * The [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) that
   * contains this job.
   */
  location: string;
}

/** Request to list Cloud Dataflow jobs. */
export interface ListJobsRequest {
  /** The kind of filter to use. */
  filter: ListJobsRequest_Filter;
  /** The project which owns the jobs. */
  projectId: string;
  /**
   * Deprecated. ListJobs always returns summaries now.
   * Use GetJob for other JobViews.
   *
   * @deprecated
   */
  view: JobView;
  /**
   * If there are many jobs, limit response to at most this many.
   * The actual number of jobs returned will be the lesser of max_responses
   * and an unspecified server-defined limit.
   */
  pageSize: number;
  /**
   * Set this to the 'next_page_token' field of a previous response
   * to request additional results in a long list.
   */
  pageToken: string;
  /**
   * The [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) that
   * contains this job.
   */
  location: string;
}

/**
 * This field filters out and returns jobs in the specified job state. The
 * order of data returned is determined by the filter used, and is subject to
 * change.
 */
export enum ListJobsRequest_Filter {
  /**
   * UNKNOWN - The filter isn't specified, or is unknown. This returns all jobs ordered
   * on descending `JobUuid`.
   */
  UNKNOWN = 0,
  /**
   * ALL - Returns all running jobs first ordered on creation timestamp, then
   * returns all terminated jobs ordered on the termination timestamp.
   */
  ALL = 1,
  /**
   * TERMINATED - Filters the jobs that have a terminated state, ordered on the
   * termination timestamp. Example terminated states: `JOB_STATE_STOPPED`,
   * `JOB_STATE_UPDATED`, `JOB_STATE_DRAINED`, etc.
   */
  TERMINATED = 2,
  /** ACTIVE - Filters the jobs that are running ordered on the creation timestamp. */
  ACTIVE = 3,
  UNRECOGNIZED = -1,
}

export function listJobsRequest_FilterFromJSON(object: any): ListJobsRequest_Filter {
  switch (object) {
    case 0:
    case "UNKNOWN":
      return ListJobsRequest_Filter.UNKNOWN;
    case 1:
    case "ALL":
      return ListJobsRequest_Filter.ALL;
    case 2:
    case "TERMINATED":
      return ListJobsRequest_Filter.TERMINATED;
    case 3:
    case "ACTIVE":
      return ListJobsRequest_Filter.ACTIVE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ListJobsRequest_Filter.UNRECOGNIZED;
  }
}

export function listJobsRequest_FilterToJSON(object: ListJobsRequest_Filter): string {
  switch (object) {
    case ListJobsRequest_Filter.UNKNOWN:
      return "UNKNOWN";
    case ListJobsRequest_Filter.ALL:
      return "ALL";
    case ListJobsRequest_Filter.TERMINATED:
      return "TERMINATED";
    case ListJobsRequest_Filter.ACTIVE:
      return "ACTIVE";
    case ListJobsRequest_Filter.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Indicates which [regional endpoint]
 * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) failed
 * to respond to a request for data.
 */
export interface FailedLocation {
  /**
   * The name of the [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) that
   * failed to respond.
   */
  name: string;
}

/**
 * Response to a request to list Cloud Dataflow jobs in a project. This might
 * be a partial response, depending on the page size in the ListJobsRequest.
 * However, if the project does not have any jobs, an instance of
 * ListJobsResponse is not returned and the requests's response
 * body is empty {}.
 */
export interface ListJobsResponse {
  /** A subset of the requested job information. */
  jobs: Job[];
  /** Set if there may be more results than fit in this response. */
  nextPageToken: string;
  /**
   * Zero or more messages describing the [regional endpoints]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) that
   * failed to respond.
   */
  failedLocation: FailedLocation[];
}

/** Request to create a snapshot of a job. */
export interface SnapshotJobRequest {
  /** The project which owns the job to be snapshotted. */
  projectId: string;
  /** The job to be snapshotted. */
  jobId: string;
  /** TTL for the snapshot. */
  ttl:
    | Duration
    | undefined;
  /** The location that contains this job. */
  location: string;
  /** If true, perform snapshots for sources which support this. */
  snapshotSources: boolean;
  /** User specified description of the snapshot. Maybe empty. */
  description: string;
}

/** Request to check is active jobs exists for a project */
export interface CheckActiveJobsRequest {
  /** The project which owns the jobs. */
  projectId: string;
}

/** Response for CheckActiveJobsRequest. */
export interface CheckActiveJobsResponse {
  /** If True, active jobs exists for project. False otherwise. */
  activeJobsExist: boolean;
}

function createBaseJob(): Job {
  return {
    id: "",
    projectId: "",
    name: "",
    type: 0,
    environment: undefined,
    steps: [],
    stepsLocation: "",
    currentState: 0,
    currentStateTime: undefined,
    requestedState: 0,
    executionInfo: undefined,
    createTime: undefined,
    replaceJobId: "",
    transformNameMapping: {},
    clientRequestId: "",
    replacedByJobId: "",
    tempFiles: [],
    labels: {},
    location: "",
    pipelineDescription: undefined,
    stageStates: [],
    jobMetadata: undefined,
    startTime: undefined,
    createdFromSnapshotId: "",
    satisfiesPzs: false,
  };
}

export const Job: MessageFns<Job> = {
  encode(message: Job, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.id !== "") {
      writer.uint32(10).string(message.id);
    }
    if (message.projectId !== "") {
      writer.uint32(18).string(message.projectId);
    }
    if (message.name !== "") {
      writer.uint32(26).string(message.name);
    }
    if (message.type !== 0) {
      writer.uint32(32).int32(message.type);
    }
    if (message.environment !== undefined) {
      Environment.encode(message.environment, writer.uint32(42).fork()).join();
    }
    for (const v of message.steps) {
      Step.encode(v!, writer.uint32(50).fork()).join();
    }
    if (message.stepsLocation !== "") {
      writer.uint32(194).string(message.stepsLocation);
    }
    if (message.currentState !== 0) {
      writer.uint32(56).int32(message.currentState);
    }
    if (message.currentStateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.currentStateTime), writer.uint32(66).fork()).join();
    }
    if (message.requestedState !== 0) {
      writer.uint32(72).int32(message.requestedState);
    }
    if (message.executionInfo !== undefined) {
      JobExecutionInfo.encode(message.executionInfo, writer.uint32(82).fork()).join();
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(90).fork()).join();
    }
    if (message.replaceJobId !== "") {
      writer.uint32(98).string(message.replaceJobId);
    }
    Object.entries(message.transformNameMapping).forEach(([key, value]) => {
      Job_TransformNameMappingEntry.encode({ key: key as any, value }, writer.uint32(106).fork()).join();
    });
    if (message.clientRequestId !== "") {
      writer.uint32(114).string(message.clientRequestId);
    }
    if (message.replacedByJobId !== "") {
      writer.uint32(122).string(message.replacedByJobId);
    }
    for (const v of message.tempFiles) {
      writer.uint32(130).string(v!);
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      Job_LabelsEntry.encode({ key: key as any, value }, writer.uint32(138).fork()).join();
    });
    if (message.location !== "") {
      writer.uint32(146).string(message.location);
    }
    if (message.pipelineDescription !== undefined) {
      PipelineDescription.encode(message.pipelineDescription, writer.uint32(154).fork()).join();
    }
    for (const v of message.stageStates) {
      ExecutionStageState.encode(v!, writer.uint32(162).fork()).join();
    }
    if (message.jobMetadata !== undefined) {
      JobMetadata.encode(message.jobMetadata, writer.uint32(170).fork()).join();
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(178).fork()).join();
    }
    if (message.createdFromSnapshotId !== "") {
      writer.uint32(186).string(message.createdFromSnapshotId);
    }
    if (message.satisfiesPzs !== false) {
      writer.uint32(200).bool(message.satisfiesPzs);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Job {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.id = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.name = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.environment = Environment.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.steps.push(Step.decode(reader, reader.uint32()));
          continue;
        case 24:
          if (tag !== 194) {
            break;
          }

          message.stepsLocation = reader.string();
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.currentState = reader.int32() as any;
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.currentStateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.requestedState = reader.int32() as any;
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.executionInfo = JobExecutionInfo.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.replaceJobId = reader.string();
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          const entry13 = Job_TransformNameMappingEntry.decode(reader, reader.uint32());
          if (entry13.value !== undefined) {
            message.transformNameMapping[entry13.key] = entry13.value;
          }
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.clientRequestId = reader.string();
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.replacedByJobId = reader.string();
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.tempFiles.push(reader.string());
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          const entry17 = Job_LabelsEntry.decode(reader, reader.uint32());
          if (entry17.value !== undefined) {
            message.labels[entry17.key] = entry17.value;
          }
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.location = reader.string();
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.pipelineDescription = PipelineDescription.decode(reader, reader.uint32());
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.stageStates.push(ExecutionStageState.decode(reader, reader.uint32()));
          continue;
        case 21:
          if (tag !== 170) {
            break;
          }

          message.jobMetadata = JobMetadata.decode(reader, reader.uint32());
          continue;
        case 22:
          if (tag !== 178) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.createdFromSnapshotId = reader.string();
          continue;
        case 25:
          if (tag !== 200) {
            break;
          }

          message.satisfiesPzs = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Job {
    return {
      id: isSet(object.id) ? globalThis.String(object.id) : "",
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      type: isSet(object.type) ? jobTypeFromJSON(object.type) : 0,
      environment: isSet(object.environment) ? Environment.fromJSON(object.environment) : undefined,
      steps: globalThis.Array.isArray(object?.steps) ? object.steps.map((e: any) => Step.fromJSON(e)) : [],
      stepsLocation: isSet(object.stepsLocation) ? globalThis.String(object.stepsLocation) : "",
      currentState: isSet(object.currentState) ? jobStateFromJSON(object.currentState) : 0,
      currentStateTime: isSet(object.currentStateTime) ? fromJsonTimestamp(object.currentStateTime) : undefined,
      requestedState: isSet(object.requestedState) ? jobStateFromJSON(object.requestedState) : 0,
      executionInfo: isSet(object.executionInfo) ? JobExecutionInfo.fromJSON(object.executionInfo) : undefined,
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      replaceJobId: isSet(object.replaceJobId) ? globalThis.String(object.replaceJobId) : "",
      transformNameMapping: isObject(object.transformNameMapping)
        ? Object.entries(object.transformNameMapping).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      clientRequestId: isSet(object.clientRequestId) ? globalThis.String(object.clientRequestId) : "",
      replacedByJobId: isSet(object.replacedByJobId) ? globalThis.String(object.replacedByJobId) : "",
      tempFiles: globalThis.Array.isArray(object?.tempFiles)
        ? object.tempFiles.map((e: any) => globalThis.String(e))
        : [],
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      location: isSet(object.location) ? globalThis.String(object.location) : "",
      pipelineDescription: isSet(object.pipelineDescription)
        ? PipelineDescription.fromJSON(object.pipelineDescription)
        : undefined,
      stageStates: globalThis.Array.isArray(object?.stageStates)
        ? object.stageStates.map((e: any) => ExecutionStageState.fromJSON(e))
        : [],
      jobMetadata: isSet(object.jobMetadata) ? JobMetadata.fromJSON(object.jobMetadata) : undefined,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      createdFromSnapshotId: isSet(object.createdFromSnapshotId) ? globalThis.String(object.createdFromSnapshotId) : "",
      satisfiesPzs: isSet(object.satisfiesPzs) ? globalThis.Boolean(object.satisfiesPzs) : false,
    };
  },

  toJSON(message: Job): unknown {
    const obj: any = {};
    if (message.id !== "") {
      obj.id = message.id;
    }
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.type !== 0) {
      obj.type = jobTypeToJSON(message.type);
    }
    if (message.environment !== undefined) {
      obj.environment = Environment.toJSON(message.environment);
    }
    if (message.steps?.length) {
      obj.steps = message.steps.map((e) => Step.toJSON(e));
    }
    if (message.stepsLocation !== "") {
      obj.stepsLocation = message.stepsLocation;
    }
    if (message.currentState !== 0) {
      obj.currentState = jobStateToJSON(message.currentState);
    }
    if (message.currentStateTime !== undefined) {
      obj.currentStateTime = message.currentStateTime.toISOString();
    }
    if (message.requestedState !== 0) {
      obj.requestedState = jobStateToJSON(message.requestedState);
    }
    if (message.executionInfo !== undefined) {
      obj.executionInfo = JobExecutionInfo.toJSON(message.executionInfo);
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.replaceJobId !== "") {
      obj.replaceJobId = message.replaceJobId;
    }
    if (message.transformNameMapping) {
      const entries = Object.entries(message.transformNameMapping);
      if (entries.length > 0) {
        obj.transformNameMapping = {};
        entries.forEach(([k, v]) => {
          obj.transformNameMapping[k] = v;
        });
      }
    }
    if (message.clientRequestId !== "") {
      obj.clientRequestId = message.clientRequestId;
    }
    if (message.replacedByJobId !== "") {
      obj.replacedByJobId = message.replacedByJobId;
    }
    if (message.tempFiles?.length) {
      obj.tempFiles = message.tempFiles;
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    if (message.pipelineDescription !== undefined) {
      obj.pipelineDescription = PipelineDescription.toJSON(message.pipelineDescription);
    }
    if (message.stageStates?.length) {
      obj.stageStates = message.stageStates.map((e) => ExecutionStageState.toJSON(e));
    }
    if (message.jobMetadata !== undefined) {
      obj.jobMetadata = JobMetadata.toJSON(message.jobMetadata);
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.createdFromSnapshotId !== "") {
      obj.createdFromSnapshotId = message.createdFromSnapshotId;
    }
    if (message.satisfiesPzs !== false) {
      obj.satisfiesPzs = message.satisfiesPzs;
    }
    return obj;
  },

  create(base?: DeepPartial<Job>): Job {
    return Job.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Job>): Job {
    const message = createBaseJob();
    message.id = object.id ?? "";
    message.projectId = object.projectId ?? "";
    message.name = object.name ?? "";
    message.type = object.type ?? 0;
    message.environment = (object.environment !== undefined && object.environment !== null)
      ? Environment.fromPartial(object.environment)
      : undefined;
    message.steps = object.steps?.map((e) => Step.fromPartial(e)) || [];
    message.stepsLocation = object.stepsLocation ?? "";
    message.currentState = object.currentState ?? 0;
    message.currentStateTime = object.currentStateTime ?? undefined;
    message.requestedState = object.requestedState ?? 0;
    message.executionInfo = (object.executionInfo !== undefined && object.executionInfo !== null)
      ? JobExecutionInfo.fromPartial(object.executionInfo)
      : undefined;
    message.createTime = object.createTime ?? undefined;
    message.replaceJobId = object.replaceJobId ?? "";
    message.transformNameMapping = Object.entries(object.transformNameMapping ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.clientRequestId = object.clientRequestId ?? "";
    message.replacedByJobId = object.replacedByJobId ?? "";
    message.tempFiles = object.tempFiles?.map((e) => e) || [];
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.location = object.location ?? "";
    message.pipelineDescription = (object.pipelineDescription !== undefined && object.pipelineDescription !== null)
      ? PipelineDescription.fromPartial(object.pipelineDescription)
      : undefined;
    message.stageStates = object.stageStates?.map((e) => ExecutionStageState.fromPartial(e)) || [];
    message.jobMetadata = (object.jobMetadata !== undefined && object.jobMetadata !== null)
      ? JobMetadata.fromPartial(object.jobMetadata)
      : undefined;
    message.startTime = object.startTime ?? undefined;
    message.createdFromSnapshotId = object.createdFromSnapshotId ?? "";
    message.satisfiesPzs = object.satisfiesPzs ?? false;
    return message;
  },
};

function createBaseJob_TransformNameMappingEntry(): Job_TransformNameMappingEntry {
  return { key: "", value: "" };
}

export const Job_TransformNameMappingEntry: MessageFns<Job_TransformNameMappingEntry> = {
  encode(message: Job_TransformNameMappingEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Job_TransformNameMappingEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJob_TransformNameMappingEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Job_TransformNameMappingEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Job_TransformNameMappingEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Job_TransformNameMappingEntry>): Job_TransformNameMappingEntry {
    return Job_TransformNameMappingEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Job_TransformNameMappingEntry>): Job_TransformNameMappingEntry {
    const message = createBaseJob_TransformNameMappingEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseJob_LabelsEntry(): Job_LabelsEntry {
  return { key: "", value: "" };
}

export const Job_LabelsEntry: MessageFns<Job_LabelsEntry> = {
  encode(message: Job_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Job_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJob_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Job_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Job_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Job_LabelsEntry>): Job_LabelsEntry {
    return Job_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Job_LabelsEntry>): Job_LabelsEntry {
    const message = createBaseJob_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseDatastoreIODetails(): DatastoreIODetails {
  return { namespace: "", projectId: "" };
}

export const DatastoreIODetails: MessageFns<DatastoreIODetails> = {
  encode(message: DatastoreIODetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.namespace !== "") {
      writer.uint32(10).string(message.namespace);
    }
    if (message.projectId !== "") {
      writer.uint32(18).string(message.projectId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatastoreIODetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatastoreIODetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.namespace = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.projectId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatastoreIODetails {
    return {
      namespace: isSet(object.namespace) ? globalThis.String(object.namespace) : "",
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
    };
  },

  toJSON(message: DatastoreIODetails): unknown {
    const obj: any = {};
    if (message.namespace !== "") {
      obj.namespace = message.namespace;
    }
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    return obj;
  },

  create(base?: DeepPartial<DatastoreIODetails>): DatastoreIODetails {
    return DatastoreIODetails.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatastoreIODetails>): DatastoreIODetails {
    const message = createBaseDatastoreIODetails();
    message.namespace = object.namespace ?? "";
    message.projectId = object.projectId ?? "";
    return message;
  },
};

function createBasePubSubIODetails(): PubSubIODetails {
  return { topic: "", subscription: "" };
}

export const PubSubIODetails: MessageFns<PubSubIODetails> = {
  encode(message: PubSubIODetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.topic !== "") {
      writer.uint32(10).string(message.topic);
    }
    if (message.subscription !== "") {
      writer.uint32(18).string(message.subscription);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PubSubIODetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePubSubIODetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.topic = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.subscription = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PubSubIODetails {
    return {
      topic: isSet(object.topic) ? globalThis.String(object.topic) : "",
      subscription: isSet(object.subscription) ? globalThis.String(object.subscription) : "",
    };
  },

  toJSON(message: PubSubIODetails): unknown {
    const obj: any = {};
    if (message.topic !== "") {
      obj.topic = message.topic;
    }
    if (message.subscription !== "") {
      obj.subscription = message.subscription;
    }
    return obj;
  },

  create(base?: DeepPartial<PubSubIODetails>): PubSubIODetails {
    return PubSubIODetails.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PubSubIODetails>): PubSubIODetails {
    const message = createBasePubSubIODetails();
    message.topic = object.topic ?? "";
    message.subscription = object.subscription ?? "";
    return message;
  },
};

function createBaseFileIODetails(): FileIODetails {
  return { filePattern: "" };
}

export const FileIODetails: MessageFns<FileIODetails> = {
  encode(message: FileIODetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.filePattern !== "") {
      writer.uint32(10).string(message.filePattern);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FileIODetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFileIODetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.filePattern = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FileIODetails {
    return { filePattern: isSet(object.filePattern) ? globalThis.String(object.filePattern) : "" };
  },

  toJSON(message: FileIODetails): unknown {
    const obj: any = {};
    if (message.filePattern !== "") {
      obj.filePattern = message.filePattern;
    }
    return obj;
  },

  create(base?: DeepPartial<FileIODetails>): FileIODetails {
    return FileIODetails.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FileIODetails>): FileIODetails {
    const message = createBaseFileIODetails();
    message.filePattern = object.filePattern ?? "";
    return message;
  },
};

function createBaseBigTableIODetails(): BigTableIODetails {
  return { projectId: "", instanceId: "", tableId: "" };
}

export const BigTableIODetails: MessageFns<BigTableIODetails> = {
  encode(message: BigTableIODetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.instanceId !== "") {
      writer.uint32(18).string(message.instanceId);
    }
    if (message.tableId !== "") {
      writer.uint32(26).string(message.tableId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigTableIODetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigTableIODetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.instanceId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.tableId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigTableIODetails {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      instanceId: isSet(object.instanceId) ? globalThis.String(object.instanceId) : "",
      tableId: isSet(object.tableId) ? globalThis.String(object.tableId) : "",
    };
  },

  toJSON(message: BigTableIODetails): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.instanceId !== "") {
      obj.instanceId = message.instanceId;
    }
    if (message.tableId !== "") {
      obj.tableId = message.tableId;
    }
    return obj;
  },

  create(base?: DeepPartial<BigTableIODetails>): BigTableIODetails {
    return BigTableIODetails.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigTableIODetails>): BigTableIODetails {
    const message = createBaseBigTableIODetails();
    message.projectId = object.projectId ?? "";
    message.instanceId = object.instanceId ?? "";
    message.tableId = object.tableId ?? "";
    return message;
  },
};

function createBaseBigQueryIODetails(): BigQueryIODetails {
  return { table: "", dataset: "", projectId: "", query: "" };
}

export const BigQueryIODetails: MessageFns<BigQueryIODetails> = {
  encode(message: BigQueryIODetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.table !== "") {
      writer.uint32(10).string(message.table);
    }
    if (message.dataset !== "") {
      writer.uint32(18).string(message.dataset);
    }
    if (message.projectId !== "") {
      writer.uint32(26).string(message.projectId);
    }
    if (message.query !== "") {
      writer.uint32(34).string(message.query);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigQueryIODetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigQueryIODetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.table = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.dataset = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.query = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigQueryIODetails {
    return {
      table: isSet(object.table) ? globalThis.String(object.table) : "",
      dataset: isSet(object.dataset) ? globalThis.String(object.dataset) : "",
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      query: isSet(object.query) ? globalThis.String(object.query) : "",
    };
  },

  toJSON(message: BigQueryIODetails): unknown {
    const obj: any = {};
    if (message.table !== "") {
      obj.table = message.table;
    }
    if (message.dataset !== "") {
      obj.dataset = message.dataset;
    }
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.query !== "") {
      obj.query = message.query;
    }
    return obj;
  },

  create(base?: DeepPartial<BigQueryIODetails>): BigQueryIODetails {
    return BigQueryIODetails.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigQueryIODetails>): BigQueryIODetails {
    const message = createBaseBigQueryIODetails();
    message.table = object.table ?? "";
    message.dataset = object.dataset ?? "";
    message.projectId = object.projectId ?? "";
    message.query = object.query ?? "";
    return message;
  },
};

function createBaseSpannerIODetails(): SpannerIODetails {
  return { projectId: "", instanceId: "", databaseId: "" };
}

export const SpannerIODetails: MessageFns<SpannerIODetails> = {
  encode(message: SpannerIODetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.instanceId !== "") {
      writer.uint32(18).string(message.instanceId);
    }
    if (message.databaseId !== "") {
      writer.uint32(26).string(message.databaseId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpannerIODetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpannerIODetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.instanceId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.databaseId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpannerIODetails {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      instanceId: isSet(object.instanceId) ? globalThis.String(object.instanceId) : "",
      databaseId: isSet(object.databaseId) ? globalThis.String(object.databaseId) : "",
    };
  },

  toJSON(message: SpannerIODetails): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.instanceId !== "") {
      obj.instanceId = message.instanceId;
    }
    if (message.databaseId !== "") {
      obj.databaseId = message.databaseId;
    }
    return obj;
  },

  create(base?: DeepPartial<SpannerIODetails>): SpannerIODetails {
    return SpannerIODetails.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SpannerIODetails>): SpannerIODetails {
    const message = createBaseSpannerIODetails();
    message.projectId = object.projectId ?? "";
    message.instanceId = object.instanceId ?? "";
    message.databaseId = object.databaseId ?? "";
    return message;
  },
};

function createBaseSdkVersion(): SdkVersion {
  return { version: "", versionDisplayName: "", sdkSupportStatus: 0 };
}

export const SdkVersion: MessageFns<SdkVersion> = {
  encode(message: SdkVersion, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.version !== "") {
      writer.uint32(10).string(message.version);
    }
    if (message.versionDisplayName !== "") {
      writer.uint32(18).string(message.versionDisplayName);
    }
    if (message.sdkSupportStatus !== 0) {
      writer.uint32(24).int32(message.sdkSupportStatus);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SdkVersion {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSdkVersion();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.version = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.versionDisplayName = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.sdkSupportStatus = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SdkVersion {
    return {
      version: isSet(object.version) ? globalThis.String(object.version) : "",
      versionDisplayName: isSet(object.versionDisplayName) ? globalThis.String(object.versionDisplayName) : "",
      sdkSupportStatus: isSet(object.sdkSupportStatus)
        ? sdkVersion_SdkSupportStatusFromJSON(object.sdkSupportStatus)
        : 0,
    };
  },

  toJSON(message: SdkVersion): unknown {
    const obj: any = {};
    if (message.version !== "") {
      obj.version = message.version;
    }
    if (message.versionDisplayName !== "") {
      obj.versionDisplayName = message.versionDisplayName;
    }
    if (message.sdkSupportStatus !== 0) {
      obj.sdkSupportStatus = sdkVersion_SdkSupportStatusToJSON(message.sdkSupportStatus);
    }
    return obj;
  },

  create(base?: DeepPartial<SdkVersion>): SdkVersion {
    return SdkVersion.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SdkVersion>): SdkVersion {
    const message = createBaseSdkVersion();
    message.version = object.version ?? "";
    message.versionDisplayName = object.versionDisplayName ?? "";
    message.sdkSupportStatus = object.sdkSupportStatus ?? 0;
    return message;
  },
};

function createBaseJobMetadata(): JobMetadata {
  return {
    sdkVersion: undefined,
    spannerDetails: [],
    bigqueryDetails: [],
    bigTableDetails: [],
    pubsubDetails: [],
    fileDetails: [],
    datastoreDetails: [],
  };
}

export const JobMetadata: MessageFns<JobMetadata> = {
  encode(message: JobMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.sdkVersion !== undefined) {
      SdkVersion.encode(message.sdkVersion, writer.uint32(10).fork()).join();
    }
    for (const v of message.spannerDetails) {
      SpannerIODetails.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.bigqueryDetails) {
      BigQueryIODetails.encode(v!, writer.uint32(26).fork()).join();
    }
    for (const v of message.bigTableDetails) {
      BigTableIODetails.encode(v!, writer.uint32(34).fork()).join();
    }
    for (const v of message.pubsubDetails) {
      PubSubIODetails.encode(v!, writer.uint32(42).fork()).join();
    }
    for (const v of message.fileDetails) {
      FileIODetails.encode(v!, writer.uint32(50).fork()).join();
    }
    for (const v of message.datastoreDetails) {
      DatastoreIODetails.encode(v!, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.sdkVersion = SdkVersion.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.spannerDetails.push(SpannerIODetails.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.bigqueryDetails.push(BigQueryIODetails.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.bigTableDetails.push(BigTableIODetails.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.pubsubDetails.push(PubSubIODetails.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.fileDetails.push(FileIODetails.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.datastoreDetails.push(DatastoreIODetails.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobMetadata {
    return {
      sdkVersion: isSet(object.sdkVersion) ? SdkVersion.fromJSON(object.sdkVersion) : undefined,
      spannerDetails: globalThis.Array.isArray(object?.spannerDetails)
        ? object.spannerDetails.map((e: any) => SpannerIODetails.fromJSON(e))
        : [],
      bigqueryDetails: globalThis.Array.isArray(object?.bigqueryDetails)
        ? object.bigqueryDetails.map((e: any) => BigQueryIODetails.fromJSON(e))
        : [],
      bigTableDetails: globalThis.Array.isArray(object?.bigTableDetails)
        ? object.bigTableDetails.map((e: any) => BigTableIODetails.fromJSON(e))
        : [],
      pubsubDetails: globalThis.Array.isArray(object?.pubsubDetails)
        ? object.pubsubDetails.map((e: any) => PubSubIODetails.fromJSON(e))
        : [],
      fileDetails: globalThis.Array.isArray(object?.fileDetails)
        ? object.fileDetails.map((e: any) => FileIODetails.fromJSON(e))
        : [],
      datastoreDetails: globalThis.Array.isArray(object?.datastoreDetails)
        ? object.datastoreDetails.map((e: any) => DatastoreIODetails.fromJSON(e))
        : [],
    };
  },

  toJSON(message: JobMetadata): unknown {
    const obj: any = {};
    if (message.sdkVersion !== undefined) {
      obj.sdkVersion = SdkVersion.toJSON(message.sdkVersion);
    }
    if (message.spannerDetails?.length) {
      obj.spannerDetails = message.spannerDetails.map((e) => SpannerIODetails.toJSON(e));
    }
    if (message.bigqueryDetails?.length) {
      obj.bigqueryDetails = message.bigqueryDetails.map((e) => BigQueryIODetails.toJSON(e));
    }
    if (message.bigTableDetails?.length) {
      obj.bigTableDetails = message.bigTableDetails.map((e) => BigTableIODetails.toJSON(e));
    }
    if (message.pubsubDetails?.length) {
      obj.pubsubDetails = message.pubsubDetails.map((e) => PubSubIODetails.toJSON(e));
    }
    if (message.fileDetails?.length) {
      obj.fileDetails = message.fileDetails.map((e) => FileIODetails.toJSON(e));
    }
    if (message.datastoreDetails?.length) {
      obj.datastoreDetails = message.datastoreDetails.map((e) => DatastoreIODetails.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<JobMetadata>): JobMetadata {
    return JobMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobMetadata>): JobMetadata {
    const message = createBaseJobMetadata();
    message.sdkVersion = (object.sdkVersion !== undefined && object.sdkVersion !== null)
      ? SdkVersion.fromPartial(object.sdkVersion)
      : undefined;
    message.spannerDetails = object.spannerDetails?.map((e) => SpannerIODetails.fromPartial(e)) || [];
    message.bigqueryDetails = object.bigqueryDetails?.map((e) => BigQueryIODetails.fromPartial(e)) || [];
    message.bigTableDetails = object.bigTableDetails?.map((e) => BigTableIODetails.fromPartial(e)) || [];
    message.pubsubDetails = object.pubsubDetails?.map((e) => PubSubIODetails.fromPartial(e)) || [];
    message.fileDetails = object.fileDetails?.map((e) => FileIODetails.fromPartial(e)) || [];
    message.datastoreDetails = object.datastoreDetails?.map((e) => DatastoreIODetails.fromPartial(e)) || [];
    return message;
  },
};

function createBaseExecutionStageState(): ExecutionStageState {
  return { executionStageName: "", executionStageState: 0, currentStateTime: undefined };
}

export const ExecutionStageState: MessageFns<ExecutionStageState> = {
  encode(message: ExecutionStageState, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.executionStageName !== "") {
      writer.uint32(10).string(message.executionStageName);
    }
    if (message.executionStageState !== 0) {
      writer.uint32(16).int32(message.executionStageState);
    }
    if (message.currentStateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.currentStateTime), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecutionStageState {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecutionStageState();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.executionStageName = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.executionStageState = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.currentStateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecutionStageState {
    return {
      executionStageName: isSet(object.executionStageName) ? globalThis.String(object.executionStageName) : "",
      executionStageState: isSet(object.executionStageState) ? jobStateFromJSON(object.executionStageState) : 0,
      currentStateTime: isSet(object.currentStateTime) ? fromJsonTimestamp(object.currentStateTime) : undefined,
    };
  },

  toJSON(message: ExecutionStageState): unknown {
    const obj: any = {};
    if (message.executionStageName !== "") {
      obj.executionStageName = message.executionStageName;
    }
    if (message.executionStageState !== 0) {
      obj.executionStageState = jobStateToJSON(message.executionStageState);
    }
    if (message.currentStateTime !== undefined) {
      obj.currentStateTime = message.currentStateTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<ExecutionStageState>): ExecutionStageState {
    return ExecutionStageState.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecutionStageState>): ExecutionStageState {
    const message = createBaseExecutionStageState();
    message.executionStageName = object.executionStageName ?? "";
    message.executionStageState = object.executionStageState ?? 0;
    message.currentStateTime = object.currentStateTime ?? undefined;
    return message;
  },
};

function createBasePipelineDescription(): PipelineDescription {
  return { originalPipelineTransform: [], executionPipelineStage: [], displayData: [] };
}

export const PipelineDescription: MessageFns<PipelineDescription> = {
  encode(message: PipelineDescription, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.originalPipelineTransform) {
      TransformSummary.encode(v!, writer.uint32(10).fork()).join();
    }
    for (const v of message.executionPipelineStage) {
      ExecutionStageSummary.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.displayData) {
      DisplayData.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineDescription {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineDescription();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.originalPipelineTransform.push(TransformSummary.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.executionPipelineStage.push(ExecutionStageSummary.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.displayData.push(DisplayData.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineDescription {
    return {
      originalPipelineTransform: globalThis.Array.isArray(object?.originalPipelineTransform)
        ? object.originalPipelineTransform.map((e: any) => TransformSummary.fromJSON(e))
        : [],
      executionPipelineStage: globalThis.Array.isArray(object?.executionPipelineStage)
        ? object.executionPipelineStage.map((e: any) => ExecutionStageSummary.fromJSON(e))
        : [],
      displayData: globalThis.Array.isArray(object?.displayData)
        ? object.displayData.map((e: any) => DisplayData.fromJSON(e))
        : [],
    };
  },

  toJSON(message: PipelineDescription): unknown {
    const obj: any = {};
    if (message.originalPipelineTransform?.length) {
      obj.originalPipelineTransform = message.originalPipelineTransform.map((e) => TransformSummary.toJSON(e));
    }
    if (message.executionPipelineStage?.length) {
      obj.executionPipelineStage = message.executionPipelineStage.map((e) => ExecutionStageSummary.toJSON(e));
    }
    if (message.displayData?.length) {
      obj.displayData = message.displayData.map((e) => DisplayData.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineDescription>): PipelineDescription {
    return PipelineDescription.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineDescription>): PipelineDescription {
    const message = createBasePipelineDescription();
    message.originalPipelineTransform = object.originalPipelineTransform?.map((e) => TransformSummary.fromPartial(e)) ||
      [];
    message.executionPipelineStage = object.executionPipelineStage?.map((e) => ExecutionStageSummary.fromPartial(e)) ||
      [];
    message.displayData = object.displayData?.map((e) => DisplayData.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTransformSummary(): TransformSummary {
  return { kind: 0, id: "", name: "", displayData: [], outputCollectionName: [], inputCollectionName: [] };
}

export const TransformSummary: MessageFns<TransformSummary> = {
  encode(message: TransformSummary, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== 0) {
      writer.uint32(8).int32(message.kind);
    }
    if (message.id !== "") {
      writer.uint32(18).string(message.id);
    }
    if (message.name !== "") {
      writer.uint32(26).string(message.name);
    }
    for (const v of message.displayData) {
      DisplayData.encode(v!, writer.uint32(34).fork()).join();
    }
    for (const v of message.outputCollectionName) {
      writer.uint32(42).string(v!);
    }
    for (const v of message.inputCollectionName) {
      writer.uint32(50).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TransformSummary {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTransformSummary();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.kind = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.id = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.name = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.displayData.push(DisplayData.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.outputCollectionName.push(reader.string());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.inputCollectionName.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TransformSummary {
    return {
      kind: isSet(object.kind) ? kindTypeFromJSON(object.kind) : 0,
      id: isSet(object.id) ? globalThis.String(object.id) : "",
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      displayData: globalThis.Array.isArray(object?.displayData)
        ? object.displayData.map((e: any) => DisplayData.fromJSON(e))
        : [],
      outputCollectionName: globalThis.Array.isArray(object?.outputCollectionName)
        ? object.outputCollectionName.map((e: any) => globalThis.String(e))
        : [],
      inputCollectionName: globalThis.Array.isArray(object?.inputCollectionName)
        ? object.inputCollectionName.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: TransformSummary): unknown {
    const obj: any = {};
    if (message.kind !== 0) {
      obj.kind = kindTypeToJSON(message.kind);
    }
    if (message.id !== "") {
      obj.id = message.id;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.displayData?.length) {
      obj.displayData = message.displayData.map((e) => DisplayData.toJSON(e));
    }
    if (message.outputCollectionName?.length) {
      obj.outputCollectionName = message.outputCollectionName;
    }
    if (message.inputCollectionName?.length) {
      obj.inputCollectionName = message.inputCollectionName;
    }
    return obj;
  },

  create(base?: DeepPartial<TransformSummary>): TransformSummary {
    return TransformSummary.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TransformSummary>): TransformSummary {
    const message = createBaseTransformSummary();
    message.kind = object.kind ?? 0;
    message.id = object.id ?? "";
    message.name = object.name ?? "";
    message.displayData = object.displayData?.map((e) => DisplayData.fromPartial(e)) || [];
    message.outputCollectionName = object.outputCollectionName?.map((e) => e) || [];
    message.inputCollectionName = object.inputCollectionName?.map((e) => e) || [];
    return message;
  },
};

function createBaseExecutionStageSummary(): ExecutionStageSummary {
  return {
    name: "",
    id: "",
    kind: 0,
    inputSource: [],
    outputSource: [],
    prerequisiteStage: [],
    componentTransform: [],
    componentSource: [],
  };
}

export const ExecutionStageSummary: MessageFns<ExecutionStageSummary> = {
  encode(message: ExecutionStageSummary, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.id !== "") {
      writer.uint32(18).string(message.id);
    }
    if (message.kind !== 0) {
      writer.uint32(24).int32(message.kind);
    }
    for (const v of message.inputSource) {
      ExecutionStageSummary_StageSource.encode(v!, writer.uint32(34).fork()).join();
    }
    for (const v of message.outputSource) {
      ExecutionStageSummary_StageSource.encode(v!, writer.uint32(42).fork()).join();
    }
    for (const v of message.prerequisiteStage) {
      writer.uint32(66).string(v!);
    }
    for (const v of message.componentTransform) {
      ExecutionStageSummary_ComponentTransform.encode(v!, writer.uint32(50).fork()).join();
    }
    for (const v of message.componentSource) {
      ExecutionStageSummary_ComponentSource.encode(v!, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecutionStageSummary {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecutionStageSummary();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.id = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.kind = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.inputSource.push(ExecutionStageSummary_StageSource.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.outputSource.push(ExecutionStageSummary_StageSource.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.prerequisiteStage.push(reader.string());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.componentTransform.push(ExecutionStageSummary_ComponentTransform.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.componentSource.push(ExecutionStageSummary_ComponentSource.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecutionStageSummary {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      id: isSet(object.id) ? globalThis.String(object.id) : "",
      kind: isSet(object.kind) ? kindTypeFromJSON(object.kind) : 0,
      inputSource: globalThis.Array.isArray(object?.inputSource)
        ? object.inputSource.map((e: any) => ExecutionStageSummary_StageSource.fromJSON(e))
        : [],
      outputSource: globalThis.Array.isArray(object?.outputSource)
        ? object.outputSource.map((e: any) => ExecutionStageSummary_StageSource.fromJSON(e))
        : [],
      prerequisiteStage: globalThis.Array.isArray(object?.prerequisiteStage)
        ? object.prerequisiteStage.map((e: any) => globalThis.String(e))
        : [],
      componentTransform: globalThis.Array.isArray(object?.componentTransform)
        ? object.componentTransform.map((e: any) => ExecutionStageSummary_ComponentTransform.fromJSON(e))
        : [],
      componentSource: globalThis.Array.isArray(object?.componentSource)
        ? object.componentSource.map((e: any) => ExecutionStageSummary_ComponentSource.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ExecutionStageSummary): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.id !== "") {
      obj.id = message.id;
    }
    if (message.kind !== 0) {
      obj.kind = kindTypeToJSON(message.kind);
    }
    if (message.inputSource?.length) {
      obj.inputSource = message.inputSource.map((e) => ExecutionStageSummary_StageSource.toJSON(e));
    }
    if (message.outputSource?.length) {
      obj.outputSource = message.outputSource.map((e) => ExecutionStageSummary_StageSource.toJSON(e));
    }
    if (message.prerequisiteStage?.length) {
      obj.prerequisiteStage = message.prerequisiteStage;
    }
    if (message.componentTransform?.length) {
      obj.componentTransform = message.componentTransform.map((e) =>
        ExecutionStageSummary_ComponentTransform.toJSON(e)
      );
    }
    if (message.componentSource?.length) {
      obj.componentSource = message.componentSource.map((e) => ExecutionStageSummary_ComponentSource.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ExecutionStageSummary>): ExecutionStageSummary {
    return ExecutionStageSummary.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecutionStageSummary>): ExecutionStageSummary {
    const message = createBaseExecutionStageSummary();
    message.name = object.name ?? "";
    message.id = object.id ?? "";
    message.kind = object.kind ?? 0;
    message.inputSource = object.inputSource?.map((e) => ExecutionStageSummary_StageSource.fromPartial(e)) || [];
    message.outputSource = object.outputSource?.map((e) => ExecutionStageSummary_StageSource.fromPartial(e)) || [];
    message.prerequisiteStage = object.prerequisiteStage?.map((e) => e) || [];
    message.componentTransform =
      object.componentTransform?.map((e) => ExecutionStageSummary_ComponentTransform.fromPartial(e)) || [];
    message.componentSource =
      object.componentSource?.map((e) => ExecutionStageSummary_ComponentSource.fromPartial(e)) || [];
    return message;
  },
};

function createBaseExecutionStageSummary_StageSource(): ExecutionStageSummary_StageSource {
  return { userName: "", name: "", originalTransformOrCollection: "", sizeBytes: Long.ZERO };
}

export const ExecutionStageSummary_StageSource: MessageFns<ExecutionStageSummary_StageSource> = {
  encode(message: ExecutionStageSummary_StageSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.userName !== "") {
      writer.uint32(10).string(message.userName);
    }
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    if (message.originalTransformOrCollection !== "") {
      writer.uint32(26).string(message.originalTransformOrCollection);
    }
    if (!message.sizeBytes.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.sizeBytes.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecutionStageSummary_StageSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecutionStageSummary_StageSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.userName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.originalTransformOrCollection = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.sizeBytes = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecutionStageSummary_StageSource {
    return {
      userName: isSet(object.userName) ? globalThis.String(object.userName) : "",
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      originalTransformOrCollection: isSet(object.originalTransformOrCollection)
        ? globalThis.String(object.originalTransformOrCollection)
        : "",
      sizeBytes: isSet(object.sizeBytes) ? Long.fromValue(object.sizeBytes) : Long.ZERO,
    };
  },

  toJSON(message: ExecutionStageSummary_StageSource): unknown {
    const obj: any = {};
    if (message.userName !== "") {
      obj.userName = message.userName;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.originalTransformOrCollection !== "") {
      obj.originalTransformOrCollection = message.originalTransformOrCollection;
    }
    if (!message.sizeBytes.equals(Long.ZERO)) {
      obj.sizeBytes = (message.sizeBytes || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ExecutionStageSummary_StageSource>): ExecutionStageSummary_StageSource {
    return ExecutionStageSummary_StageSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecutionStageSummary_StageSource>): ExecutionStageSummary_StageSource {
    const message = createBaseExecutionStageSummary_StageSource();
    message.userName = object.userName ?? "";
    message.name = object.name ?? "";
    message.originalTransformOrCollection = object.originalTransformOrCollection ?? "";
    message.sizeBytes = (object.sizeBytes !== undefined && object.sizeBytes !== null)
      ? Long.fromValue(object.sizeBytes)
      : Long.ZERO;
    return message;
  },
};

function createBaseExecutionStageSummary_ComponentTransform(): ExecutionStageSummary_ComponentTransform {
  return { userName: "", name: "", originalTransform: "" };
}

export const ExecutionStageSummary_ComponentTransform: MessageFns<ExecutionStageSummary_ComponentTransform> = {
  encode(message: ExecutionStageSummary_ComponentTransform, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.userName !== "") {
      writer.uint32(10).string(message.userName);
    }
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    if (message.originalTransform !== "") {
      writer.uint32(26).string(message.originalTransform);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecutionStageSummary_ComponentTransform {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecutionStageSummary_ComponentTransform();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.userName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.originalTransform = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecutionStageSummary_ComponentTransform {
    return {
      userName: isSet(object.userName) ? globalThis.String(object.userName) : "",
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      originalTransform: isSet(object.originalTransform) ? globalThis.String(object.originalTransform) : "",
    };
  },

  toJSON(message: ExecutionStageSummary_ComponentTransform): unknown {
    const obj: any = {};
    if (message.userName !== "") {
      obj.userName = message.userName;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.originalTransform !== "") {
      obj.originalTransform = message.originalTransform;
    }
    return obj;
  },

  create(base?: DeepPartial<ExecutionStageSummary_ComponentTransform>): ExecutionStageSummary_ComponentTransform {
    return ExecutionStageSummary_ComponentTransform.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecutionStageSummary_ComponentTransform>): ExecutionStageSummary_ComponentTransform {
    const message = createBaseExecutionStageSummary_ComponentTransform();
    message.userName = object.userName ?? "";
    message.name = object.name ?? "";
    message.originalTransform = object.originalTransform ?? "";
    return message;
  },
};

function createBaseExecutionStageSummary_ComponentSource(): ExecutionStageSummary_ComponentSource {
  return { userName: "", name: "", originalTransformOrCollection: "" };
}

export const ExecutionStageSummary_ComponentSource: MessageFns<ExecutionStageSummary_ComponentSource> = {
  encode(message: ExecutionStageSummary_ComponentSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.userName !== "") {
      writer.uint32(10).string(message.userName);
    }
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    if (message.originalTransformOrCollection !== "") {
      writer.uint32(26).string(message.originalTransformOrCollection);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecutionStageSummary_ComponentSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecutionStageSummary_ComponentSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.userName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.originalTransformOrCollection = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecutionStageSummary_ComponentSource {
    return {
      userName: isSet(object.userName) ? globalThis.String(object.userName) : "",
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      originalTransformOrCollection: isSet(object.originalTransformOrCollection)
        ? globalThis.String(object.originalTransformOrCollection)
        : "",
    };
  },

  toJSON(message: ExecutionStageSummary_ComponentSource): unknown {
    const obj: any = {};
    if (message.userName !== "") {
      obj.userName = message.userName;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.originalTransformOrCollection !== "") {
      obj.originalTransformOrCollection = message.originalTransformOrCollection;
    }
    return obj;
  },

  create(base?: DeepPartial<ExecutionStageSummary_ComponentSource>): ExecutionStageSummary_ComponentSource {
    return ExecutionStageSummary_ComponentSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecutionStageSummary_ComponentSource>): ExecutionStageSummary_ComponentSource {
    const message = createBaseExecutionStageSummary_ComponentSource();
    message.userName = object.userName ?? "";
    message.name = object.name ?? "";
    message.originalTransformOrCollection = object.originalTransformOrCollection ?? "";
    return message;
  },
};

function createBaseDisplayData(): DisplayData {
  return {
    key: "",
    namespace: "",
    strValue: undefined,
    int64Value: undefined,
    floatValue: undefined,
    javaClassValue: undefined,
    timestampValue: undefined,
    durationValue: undefined,
    boolValue: undefined,
    shortStrValue: "",
    url: "",
    label: "",
  };
}

export const DisplayData: MessageFns<DisplayData> = {
  encode(message: DisplayData, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.namespace !== "") {
      writer.uint32(18).string(message.namespace);
    }
    if (message.strValue !== undefined) {
      writer.uint32(34).string(message.strValue);
    }
    if (message.int64Value !== undefined) {
      writer.uint32(40).int64(message.int64Value.toString());
    }
    if (message.floatValue !== undefined) {
      writer.uint32(53).float(message.floatValue);
    }
    if (message.javaClassValue !== undefined) {
      writer.uint32(58).string(message.javaClassValue);
    }
    if (message.timestampValue !== undefined) {
      Timestamp.encode(toTimestamp(message.timestampValue), writer.uint32(66).fork()).join();
    }
    if (message.durationValue !== undefined) {
      Duration.encode(message.durationValue, writer.uint32(74).fork()).join();
    }
    if (message.boolValue !== undefined) {
      writer.uint32(80).bool(message.boolValue);
    }
    if (message.shortStrValue !== "") {
      writer.uint32(90).string(message.shortStrValue);
    }
    if (message.url !== "") {
      writer.uint32(98).string(message.url);
    }
    if (message.label !== "") {
      writer.uint32(106).string(message.label);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DisplayData {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDisplayData();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.namespace = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.strValue = reader.string();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.int64Value = Long.fromString(reader.int64().toString());
          continue;
        case 6:
          if (tag !== 53) {
            break;
          }

          message.floatValue = reader.float();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.javaClassValue = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.timestampValue = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.durationValue = Duration.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.boolValue = reader.bool();
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.shortStrValue = reader.string();
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.url = reader.string();
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.label = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DisplayData {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      namespace: isSet(object.namespace) ? globalThis.String(object.namespace) : "",
      strValue: isSet(object.strValue) ? globalThis.String(object.strValue) : undefined,
      int64Value: isSet(object.int64Value) ? Long.fromValue(object.int64Value) : undefined,
      floatValue: isSet(object.floatValue) ? globalThis.Number(object.floatValue) : undefined,
      javaClassValue: isSet(object.javaClassValue) ? globalThis.String(object.javaClassValue) : undefined,
      timestampValue: isSet(object.timestampValue) ? fromJsonTimestamp(object.timestampValue) : undefined,
      durationValue: isSet(object.durationValue) ? Duration.fromJSON(object.durationValue) : undefined,
      boolValue: isSet(object.boolValue) ? globalThis.Boolean(object.boolValue) : undefined,
      shortStrValue: isSet(object.shortStrValue) ? globalThis.String(object.shortStrValue) : "",
      url: isSet(object.url) ? globalThis.String(object.url) : "",
      label: isSet(object.label) ? globalThis.String(object.label) : "",
    };
  },

  toJSON(message: DisplayData): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.namespace !== "") {
      obj.namespace = message.namespace;
    }
    if (message.strValue !== undefined) {
      obj.strValue = message.strValue;
    }
    if (message.int64Value !== undefined) {
      obj.int64Value = (message.int64Value || Long.ZERO).toString();
    }
    if (message.floatValue !== undefined) {
      obj.floatValue = message.floatValue;
    }
    if (message.javaClassValue !== undefined) {
      obj.javaClassValue = message.javaClassValue;
    }
    if (message.timestampValue !== undefined) {
      obj.timestampValue = message.timestampValue.toISOString();
    }
    if (message.durationValue !== undefined) {
      obj.durationValue = Duration.toJSON(message.durationValue);
    }
    if (message.boolValue !== undefined) {
      obj.boolValue = message.boolValue;
    }
    if (message.shortStrValue !== "") {
      obj.shortStrValue = message.shortStrValue;
    }
    if (message.url !== "") {
      obj.url = message.url;
    }
    if (message.label !== "") {
      obj.label = message.label;
    }
    return obj;
  },

  create(base?: DeepPartial<DisplayData>): DisplayData {
    return DisplayData.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DisplayData>): DisplayData {
    const message = createBaseDisplayData();
    message.key = object.key ?? "";
    message.namespace = object.namespace ?? "";
    message.strValue = object.strValue ?? undefined;
    message.int64Value = (object.int64Value !== undefined && object.int64Value !== null)
      ? Long.fromValue(object.int64Value)
      : undefined;
    message.floatValue = object.floatValue ?? undefined;
    message.javaClassValue = object.javaClassValue ?? undefined;
    message.timestampValue = object.timestampValue ?? undefined;
    message.durationValue = (object.durationValue !== undefined && object.durationValue !== null)
      ? Duration.fromPartial(object.durationValue)
      : undefined;
    message.boolValue = object.boolValue ?? undefined;
    message.shortStrValue = object.shortStrValue ?? "";
    message.url = object.url ?? "";
    message.label = object.label ?? "";
    return message;
  },
};

function createBaseStep(): Step {
  return { kind: "", name: "", properties: undefined };
}

export const Step: MessageFns<Step> = {
  encode(message: Step, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    if (message.properties !== undefined) {
      Struct.encode(Struct.wrap(message.properties), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Step {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStep();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.properties = Struct.unwrap(Struct.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Step {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      properties: isObject(object.properties) ? object.properties : undefined,
    };
  },

  toJSON(message: Step): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.properties !== undefined) {
      obj.properties = message.properties;
    }
    return obj;
  },

  create(base?: DeepPartial<Step>): Step {
    return Step.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Step>): Step {
    const message = createBaseStep();
    message.kind = object.kind ?? "";
    message.name = object.name ?? "";
    message.properties = object.properties ?? undefined;
    return message;
  },
};

function createBaseJobExecutionInfo(): JobExecutionInfo {
  return { stages: {} };
}

export const JobExecutionInfo: MessageFns<JobExecutionInfo> = {
  encode(message: JobExecutionInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.stages).forEach(([key, value]) => {
      JobExecutionInfo_StagesEntry.encode({ key: key as any, value }, writer.uint32(10).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobExecutionInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobExecutionInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          const entry1 = JobExecutionInfo_StagesEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.stages[entry1.key] = entry1.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobExecutionInfo {
    return {
      stages: isObject(object.stages)
        ? Object.entries(object.stages).reduce<{ [key: string]: JobExecutionStageInfo }>((acc, [key, value]) => {
          acc[key] = JobExecutionStageInfo.fromJSON(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: JobExecutionInfo): unknown {
    const obj: any = {};
    if (message.stages) {
      const entries = Object.entries(message.stages);
      if (entries.length > 0) {
        obj.stages = {};
        entries.forEach(([k, v]) => {
          obj.stages[k] = JobExecutionStageInfo.toJSON(v);
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<JobExecutionInfo>): JobExecutionInfo {
    return JobExecutionInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobExecutionInfo>): JobExecutionInfo {
    const message = createBaseJobExecutionInfo();
    message.stages = Object.entries(object.stages ?? {}).reduce<{ [key: string]: JobExecutionStageInfo }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = JobExecutionStageInfo.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseJobExecutionInfo_StagesEntry(): JobExecutionInfo_StagesEntry {
  return { key: "", value: undefined };
}

export const JobExecutionInfo_StagesEntry: MessageFns<JobExecutionInfo_StagesEntry> = {
  encode(message: JobExecutionInfo_StagesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      JobExecutionStageInfo.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobExecutionInfo_StagesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobExecutionInfo_StagesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = JobExecutionStageInfo.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobExecutionInfo_StagesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? JobExecutionStageInfo.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: JobExecutionInfo_StagesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = JobExecutionStageInfo.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<JobExecutionInfo_StagesEntry>): JobExecutionInfo_StagesEntry {
    return JobExecutionInfo_StagesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobExecutionInfo_StagesEntry>): JobExecutionInfo_StagesEntry {
    const message = createBaseJobExecutionInfo_StagesEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? JobExecutionStageInfo.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseJobExecutionStageInfo(): JobExecutionStageInfo {
  return { stepName: [] };
}

export const JobExecutionStageInfo: MessageFns<JobExecutionStageInfo> = {
  encode(message: JobExecutionStageInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.stepName) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobExecutionStageInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobExecutionStageInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.stepName.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobExecutionStageInfo {
    return {
      stepName: globalThis.Array.isArray(object?.stepName) ? object.stepName.map((e: any) => globalThis.String(e)) : [],
    };
  },

  toJSON(message: JobExecutionStageInfo): unknown {
    const obj: any = {};
    if (message.stepName?.length) {
      obj.stepName = message.stepName;
    }
    return obj;
  },

  create(base?: DeepPartial<JobExecutionStageInfo>): JobExecutionStageInfo {
    return JobExecutionStageInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobExecutionStageInfo>): JobExecutionStageInfo {
    const message = createBaseJobExecutionStageInfo();
    message.stepName = object.stepName?.map((e) => e) || [];
    return message;
  },
};

function createBaseCreateJobRequest(): CreateJobRequest {
  return { projectId: "", job: undefined, view: 0, replaceJobId: "", location: "" };
}

export const CreateJobRequest: MessageFns<CreateJobRequest> = {
  encode(message: CreateJobRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.job !== undefined) {
      Job.encode(message.job, writer.uint32(18).fork()).join();
    }
    if (message.view !== 0) {
      writer.uint32(24).int32(message.view);
    }
    if (message.replaceJobId !== "") {
      writer.uint32(34).string(message.replaceJobId);
    }
    if (message.location !== "") {
      writer.uint32(42).string(message.location);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateJobRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateJobRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.job = Job.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.view = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.replaceJobId = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.location = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateJobRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      job: isSet(object.job) ? Job.fromJSON(object.job) : undefined,
      view: isSet(object.view) ? jobViewFromJSON(object.view) : 0,
      replaceJobId: isSet(object.replaceJobId) ? globalThis.String(object.replaceJobId) : "",
      location: isSet(object.location) ? globalThis.String(object.location) : "",
    };
  },

  toJSON(message: CreateJobRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.job !== undefined) {
      obj.job = Job.toJSON(message.job);
    }
    if (message.view !== 0) {
      obj.view = jobViewToJSON(message.view);
    }
    if (message.replaceJobId !== "") {
      obj.replaceJobId = message.replaceJobId;
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    return obj;
  },

  create(base?: DeepPartial<CreateJobRequest>): CreateJobRequest {
    return CreateJobRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateJobRequest>): CreateJobRequest {
    const message = createBaseCreateJobRequest();
    message.projectId = object.projectId ?? "";
    message.job = (object.job !== undefined && object.job !== null) ? Job.fromPartial(object.job) : undefined;
    message.view = object.view ?? 0;
    message.replaceJobId = object.replaceJobId ?? "";
    message.location = object.location ?? "";
    return message;
  },
};

function createBaseGetJobRequest(): GetJobRequest {
  return { projectId: "", jobId: "", view: 0, location: "" };
}

export const GetJobRequest: MessageFns<GetJobRequest> = {
  encode(message: GetJobRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.jobId !== "") {
      writer.uint32(18).string(message.jobId);
    }
    if (message.view !== 0) {
      writer.uint32(24).int32(message.view);
    }
    if (message.location !== "") {
      writer.uint32(34).string(message.location);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetJobRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetJobRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.jobId = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.view = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.location = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetJobRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      jobId: isSet(object.jobId) ? globalThis.String(object.jobId) : "",
      view: isSet(object.view) ? jobViewFromJSON(object.view) : 0,
      location: isSet(object.location) ? globalThis.String(object.location) : "",
    };
  },

  toJSON(message: GetJobRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.jobId !== "") {
      obj.jobId = message.jobId;
    }
    if (message.view !== 0) {
      obj.view = jobViewToJSON(message.view);
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    return obj;
  },

  create(base?: DeepPartial<GetJobRequest>): GetJobRequest {
    return GetJobRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetJobRequest>): GetJobRequest {
    const message = createBaseGetJobRequest();
    message.projectId = object.projectId ?? "";
    message.jobId = object.jobId ?? "";
    message.view = object.view ?? 0;
    message.location = object.location ?? "";
    return message;
  },
};

function createBaseUpdateJobRequest(): UpdateJobRequest {
  return { projectId: "", jobId: "", job: undefined, location: "" };
}

export const UpdateJobRequest: MessageFns<UpdateJobRequest> = {
  encode(message: UpdateJobRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.jobId !== "") {
      writer.uint32(18).string(message.jobId);
    }
    if (message.job !== undefined) {
      Job.encode(message.job, writer.uint32(26).fork()).join();
    }
    if (message.location !== "") {
      writer.uint32(34).string(message.location);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateJobRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateJobRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.jobId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.job = Job.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.location = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateJobRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      jobId: isSet(object.jobId) ? globalThis.String(object.jobId) : "",
      job: isSet(object.job) ? Job.fromJSON(object.job) : undefined,
      location: isSet(object.location) ? globalThis.String(object.location) : "",
    };
  },

  toJSON(message: UpdateJobRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.jobId !== "") {
      obj.jobId = message.jobId;
    }
    if (message.job !== undefined) {
      obj.job = Job.toJSON(message.job);
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateJobRequest>): UpdateJobRequest {
    return UpdateJobRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateJobRequest>): UpdateJobRequest {
    const message = createBaseUpdateJobRequest();
    message.projectId = object.projectId ?? "";
    message.jobId = object.jobId ?? "";
    message.job = (object.job !== undefined && object.job !== null) ? Job.fromPartial(object.job) : undefined;
    message.location = object.location ?? "";
    return message;
  },
};

function createBaseListJobsRequest(): ListJobsRequest {
  return { filter: 0, projectId: "", view: 0, pageSize: 0, pageToken: "", location: "" };
}

export const ListJobsRequest: MessageFns<ListJobsRequest> = {
  encode(message: ListJobsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.filter !== 0) {
      writer.uint32(40).int32(message.filter);
    }
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.view !== 0) {
      writer.uint32(16).int32(message.view);
    }
    if (message.pageSize !== 0) {
      writer.uint32(24).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(34).string(message.pageToken);
    }
    if (message.location !== "") {
      writer.uint32(138).string(message.location);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListJobsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListJobsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 5:
          if (tag !== 40) {
            break;
          }

          message.filter = reader.int32() as any;
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.view = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.pageToken = reader.string();
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.location = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListJobsRequest {
    return {
      filter: isSet(object.filter) ? listJobsRequest_FilterFromJSON(object.filter) : 0,
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      view: isSet(object.view) ? jobViewFromJSON(object.view) : 0,
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
      location: isSet(object.location) ? globalThis.String(object.location) : "",
    };
  },

  toJSON(message: ListJobsRequest): unknown {
    const obj: any = {};
    if (message.filter !== 0) {
      obj.filter = listJobsRequest_FilterToJSON(message.filter);
    }
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.view !== 0) {
      obj.view = jobViewToJSON(message.view);
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    return obj;
  },

  create(base?: DeepPartial<ListJobsRequest>): ListJobsRequest {
    return ListJobsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListJobsRequest>): ListJobsRequest {
    const message = createBaseListJobsRequest();
    message.filter = object.filter ?? 0;
    message.projectId = object.projectId ?? "";
    message.view = object.view ?? 0;
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    message.location = object.location ?? "";
    return message;
  },
};

function createBaseFailedLocation(): FailedLocation {
  return { name: "" };
}

export const FailedLocation: MessageFns<FailedLocation> = {
  encode(message: FailedLocation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FailedLocation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFailedLocation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FailedLocation {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: FailedLocation): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<FailedLocation>): FailedLocation {
    return FailedLocation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FailedLocation>): FailedLocation {
    const message = createBaseFailedLocation();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseListJobsResponse(): ListJobsResponse {
  return { jobs: [], nextPageToken: "", failedLocation: [] };
}

export const ListJobsResponse: MessageFns<ListJobsResponse> = {
  encode(message: ListJobsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.jobs) {
      Job.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    for (const v of message.failedLocation) {
      FailedLocation.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListJobsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListJobsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.jobs.push(Job.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.failedLocation.push(FailedLocation.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListJobsResponse {
    return {
      jobs: globalThis.Array.isArray(object?.jobs) ? object.jobs.map((e: any) => Job.fromJSON(e)) : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
      failedLocation: globalThis.Array.isArray(object?.failedLocation)
        ? object.failedLocation.map((e: any) => FailedLocation.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ListJobsResponse): unknown {
    const obj: any = {};
    if (message.jobs?.length) {
      obj.jobs = message.jobs.map((e) => Job.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    if (message.failedLocation?.length) {
      obj.failedLocation = message.failedLocation.map((e) => FailedLocation.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ListJobsResponse>): ListJobsResponse {
    return ListJobsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListJobsResponse>): ListJobsResponse {
    const message = createBaseListJobsResponse();
    message.jobs = object.jobs?.map((e) => Job.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    message.failedLocation = object.failedLocation?.map((e) => FailedLocation.fromPartial(e)) || [];
    return message;
  },
};

function createBaseSnapshotJobRequest(): SnapshotJobRequest {
  return { projectId: "", jobId: "", ttl: undefined, location: "", snapshotSources: false, description: "" };
}

export const SnapshotJobRequest: MessageFns<SnapshotJobRequest> = {
  encode(message: SnapshotJobRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.jobId !== "") {
      writer.uint32(18).string(message.jobId);
    }
    if (message.ttl !== undefined) {
      Duration.encode(message.ttl, writer.uint32(26).fork()).join();
    }
    if (message.location !== "") {
      writer.uint32(34).string(message.location);
    }
    if (message.snapshotSources !== false) {
      writer.uint32(40).bool(message.snapshotSources);
    }
    if (message.description !== "") {
      writer.uint32(50).string(message.description);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SnapshotJobRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSnapshotJobRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.jobId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.ttl = Duration.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.location = reader.string();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.snapshotSources = reader.bool();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.description = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SnapshotJobRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      jobId: isSet(object.jobId) ? globalThis.String(object.jobId) : "",
      ttl: isSet(object.ttl) ? Duration.fromJSON(object.ttl) : undefined,
      location: isSet(object.location) ? globalThis.String(object.location) : "",
      snapshotSources: isSet(object.snapshotSources) ? globalThis.Boolean(object.snapshotSources) : false,
      description: isSet(object.description) ? globalThis.String(object.description) : "",
    };
  },

  toJSON(message: SnapshotJobRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.jobId !== "") {
      obj.jobId = message.jobId;
    }
    if (message.ttl !== undefined) {
      obj.ttl = Duration.toJSON(message.ttl);
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    if (message.snapshotSources !== false) {
      obj.snapshotSources = message.snapshotSources;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    return obj;
  },

  create(base?: DeepPartial<SnapshotJobRequest>): SnapshotJobRequest {
    return SnapshotJobRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SnapshotJobRequest>): SnapshotJobRequest {
    const message = createBaseSnapshotJobRequest();
    message.projectId = object.projectId ?? "";
    message.jobId = object.jobId ?? "";
    message.ttl = (object.ttl !== undefined && object.ttl !== null) ? Duration.fromPartial(object.ttl) : undefined;
    message.location = object.location ?? "";
    message.snapshotSources = object.snapshotSources ?? false;
    message.description = object.description ?? "";
    return message;
  },
};

function createBaseCheckActiveJobsRequest(): CheckActiveJobsRequest {
  return { projectId: "" };
}

export const CheckActiveJobsRequest: MessageFns<CheckActiveJobsRequest> = {
  encode(message: CheckActiveJobsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CheckActiveJobsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCheckActiveJobsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CheckActiveJobsRequest {
    return { projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "" };
  },

  toJSON(message: CheckActiveJobsRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    return obj;
  },

  create(base?: DeepPartial<CheckActiveJobsRequest>): CheckActiveJobsRequest {
    return CheckActiveJobsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CheckActiveJobsRequest>): CheckActiveJobsRequest {
    const message = createBaseCheckActiveJobsRequest();
    message.projectId = object.projectId ?? "";
    return message;
  },
};

function createBaseCheckActiveJobsResponse(): CheckActiveJobsResponse {
  return { activeJobsExist: false };
}

export const CheckActiveJobsResponse: MessageFns<CheckActiveJobsResponse> = {
  encode(message: CheckActiveJobsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.activeJobsExist !== false) {
      writer.uint32(8).bool(message.activeJobsExist);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CheckActiveJobsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCheckActiveJobsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.activeJobsExist = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CheckActiveJobsResponse {
    return { activeJobsExist: isSet(object.activeJobsExist) ? globalThis.Boolean(object.activeJobsExist) : false };
  },

  toJSON(message: CheckActiveJobsResponse): unknown {
    const obj: any = {};
    if (message.activeJobsExist !== false) {
      obj.activeJobsExist = message.activeJobsExist;
    }
    return obj;
  },

  create(base?: DeepPartial<CheckActiveJobsResponse>): CheckActiveJobsResponse {
    return CheckActiveJobsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CheckActiveJobsResponse>): CheckActiveJobsResponse {
    const message = createBaseCheckActiveJobsResponse();
    message.activeJobsExist = object.activeJobsExist ?? false;
    return message;
  },
};

/**
 * Provides a method to create and modify Google Cloud Dataflow jobs.
 * A Job is a multi-stage computation graph run by the Cloud Dataflow service.
 */
export type JobsV1Beta3Definition = typeof JobsV1Beta3Definition;
export const JobsV1Beta3Definition = {
  name: "JobsV1Beta3",
  fullName: "google.dataflow.v1beta3.JobsV1Beta3",
  methods: {
    /**
     * Creates a Cloud Dataflow job.
     *
     * To create a job, we recommend using `projects.locations.jobs.create` with a
     * [regional endpoint]
     * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints). Using
     * `projects.jobs.create` is not recommended, as your job will always start
     * in `us-central1`.
     */
    createJob: {
      name: "CreateJob",
      requestType: CreateJobRequest,
      requestStream: false,
      responseType: Job,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              101,
              58,
              3,
              106,
              111,
              98,
              90,
              39,
              58,
              3,
              106,
              111,
              98,
              34,
              32,
              47,
              118,
              49,
              98,
              51,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              106,
              111,
              98,
              115,
              34,
              53,
              47,
              118,
              49,
              98,
              51,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              123,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              125,
              47,
              106,
              111,
              98,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Gets the state of the specified Cloud Dataflow job.
     *
     * To get the state of a job, we recommend using `projects.locations.jobs.get`
     * with a [regional endpoint]
     * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints). Using
     * `projects.jobs.get` is not recommended, as you can only get the state of
     * jobs that are running in `us-central1`.
     */
    getJob: {
      name: "GetJob",
      requestType: GetJobRequest,
      requestStream: false,
      responseType: Job,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              109,
              90,
              43,
              18,
              41,
              47,
              118,
              49,
              98,
              51,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              106,
              111,
              98,
              115,
              47,
              123,
              106,
              111,
              98,
              95,
              105,
              100,
              125,
              18,
              62,
              47,
              118,
              49,
              98,
              51,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              123,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              125,
              47,
              106,
              111,
              98,
              115,
              47,
              123,
              106,
              111,
              98,
              95,
              105,
              100,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Updates the state of an existing Cloud Dataflow job.
     *
     * To update the state of an existing job, we recommend using
     * `projects.locations.jobs.update` with a [regional endpoint]
     * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints). Using
     * `projects.jobs.update` is not recommended, as you can only update the state
     * of jobs that are running in `us-central1`.
     */
    updateJob: {
      name: "UpdateJob",
      requestType: UpdateJobRequest,
      requestStream: false,
      responseType: Job,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              119,
              58,
              3,
              106,
              111,
              98,
              90,
              48,
              58,
              3,
              106,
              111,
              98,
              26,
              41,
              47,
              118,
              49,
              98,
              51,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              106,
              111,
              98,
              115,
              47,
              123,
              106,
              111,
              98,
              95,
              105,
              100,
              125,
              26,
              62,
              47,
              118,
              49,
              98,
              51,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              123,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              125,
              47,
              106,
              111,
              98,
              115,
              47,
              123,
              106,
              111,
              98,
              95,
              105,
              100,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * List the jobs of a project.
     *
     * To list the jobs of a project in a region, we recommend using
     * `projects.locations.jobs.list` with a [regional endpoint]
     * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints). To
     * list the all jobs across all regions, use `projects.jobs.aggregated`. Using
     * `projects.jobs.list` is not recommended, as you can only get the list of
     * jobs that are running in `us-central1`.
     */
    listJobs: {
      name: "ListJobs",
      requestType: ListJobsRequest,
      requestStream: false,
      responseType: ListJobsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              91,
              90,
              34,
              18,
              32,
              47,
              118,
              49,
              98,
              51,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              106,
              111,
              98,
              115,
              18,
              53,
              47,
              118,
              49,
              98,
              51,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              123,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              125,
              47,
              106,
              111,
              98,
              115,
            ]),
          ],
        },
      },
    },
    /** List the jobs of a project across all regions. */
    aggregatedListJobs: {
      name: "AggregatedListJobs",
      requestType: ListJobsRequest,
      requestStream: false,
      responseType: ListJobsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              45,
              18,
              43,
              47,
              118,
              49,
              98,
              51,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              106,
              111,
              98,
              115,
              58,
              97,
              103,
              103,
              114,
              101,
              103,
              97,
              116,
              101,
              100,
            ]),
          ],
        },
      },
    },
    /** Check for existence of active jobs in the given project across all regions. */
    checkActiveJobs: {
      name: "CheckActiveJobs",
      requestType: CheckActiveJobsRequest,
      requestStream: false,
      responseType: CheckActiveJobsResponse,
      responseStream: false,
      options: {},
    },
    /** Snapshot the state of a streaming job. */
    snapshotJob: {
      name: "SnapshotJob",
      requestType: SnapshotJobRequest,
      requestStream: false,
      responseType: Snapshot,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              133,
              1,
              58,
              1,
              42,
              90,
              55,
              58,
              1,
              42,
              34,
              50,
              47,
              118,
              49,
              98,
              51,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              106,
              111,
              98,
              115,
              47,
              123,
              106,
              111,
              98,
              95,
              105,
              100,
              125,
              58,
              115,
              110,
              97,
              112,
              115,
              104,
              111,
              116,
              34,
              71,
              47,
              118,
              49,
              98,
              51,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              123,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              125,
              47,
              106,
              111,
              98,
              115,
              47,
              123,
              106,
              111,
              98,
              95,
              105,
              100,
              125,
              58,
              115,
              110,
              97,
              112,
              115,
              104,
              111,
              116,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface JobsV1Beta3ServiceImplementation<CallContextExt = {}> {
  /**
   * Creates a Cloud Dataflow job.
   *
   * To create a job, we recommend using `projects.locations.jobs.create` with a
   * [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints). Using
   * `projects.jobs.create` is not recommended, as your job will always start
   * in `us-central1`.
   */
  createJob(request: CreateJobRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Job>>;
  /**
   * Gets the state of the specified Cloud Dataflow job.
   *
   * To get the state of a job, we recommend using `projects.locations.jobs.get`
   * with a [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints). Using
   * `projects.jobs.get` is not recommended, as you can only get the state of
   * jobs that are running in `us-central1`.
   */
  getJob(request: GetJobRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Job>>;
  /**
   * Updates the state of an existing Cloud Dataflow job.
   *
   * To update the state of an existing job, we recommend using
   * `projects.locations.jobs.update` with a [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints). Using
   * `projects.jobs.update` is not recommended, as you can only update the state
   * of jobs that are running in `us-central1`.
   */
  updateJob(request: UpdateJobRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Job>>;
  /**
   * List the jobs of a project.
   *
   * To list the jobs of a project in a region, we recommend using
   * `projects.locations.jobs.list` with a [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints). To
   * list the all jobs across all regions, use `projects.jobs.aggregated`. Using
   * `projects.jobs.list` is not recommended, as you can only get the list of
   * jobs that are running in `us-central1`.
   */
  listJobs(request: ListJobsRequest, context: CallContext & CallContextExt): Promise<DeepPartial<ListJobsResponse>>;
  /** List the jobs of a project across all regions. */
  aggregatedListJobs(
    request: ListJobsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ListJobsResponse>>;
  /** Check for existence of active jobs in the given project across all regions. */
  checkActiveJobs(
    request: CheckActiveJobsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<CheckActiveJobsResponse>>;
  /** Snapshot the state of a streaming job. */
  snapshotJob(request: SnapshotJobRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Snapshot>>;
}

export interface JobsV1Beta3Client<CallOptionsExt = {}> {
  /**
   * Creates a Cloud Dataflow job.
   *
   * To create a job, we recommend using `projects.locations.jobs.create` with a
   * [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints). Using
   * `projects.jobs.create` is not recommended, as your job will always start
   * in `us-central1`.
   */
  createJob(request: DeepPartial<CreateJobRequest>, options?: CallOptions & CallOptionsExt): Promise<Job>;
  /**
   * Gets the state of the specified Cloud Dataflow job.
   *
   * To get the state of a job, we recommend using `projects.locations.jobs.get`
   * with a [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints). Using
   * `projects.jobs.get` is not recommended, as you can only get the state of
   * jobs that are running in `us-central1`.
   */
  getJob(request: DeepPartial<GetJobRequest>, options?: CallOptions & CallOptionsExt): Promise<Job>;
  /**
   * Updates the state of an existing Cloud Dataflow job.
   *
   * To update the state of an existing job, we recommend using
   * `projects.locations.jobs.update` with a [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints). Using
   * `projects.jobs.update` is not recommended, as you can only update the state
   * of jobs that are running in `us-central1`.
   */
  updateJob(request: DeepPartial<UpdateJobRequest>, options?: CallOptions & CallOptionsExt): Promise<Job>;
  /**
   * List the jobs of a project.
   *
   * To list the jobs of a project in a region, we recommend using
   * `projects.locations.jobs.list` with a [regional endpoint]
   * (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints). To
   * list the all jobs across all regions, use `projects.jobs.aggregated`. Using
   * `projects.jobs.list` is not recommended, as you can only get the list of
   * jobs that are running in `us-central1`.
   */
  listJobs(request: DeepPartial<ListJobsRequest>, options?: CallOptions & CallOptionsExt): Promise<ListJobsResponse>;
  /** List the jobs of a project across all regions. */
  aggregatedListJobs(
    request: DeepPartial<ListJobsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ListJobsResponse>;
  /** Check for existence of active jobs in the given project across all regions. */
  checkActiveJobs(
    request: DeepPartial<CheckActiveJobsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<CheckActiveJobsResponse>;
  /** Snapshot the state of a streaming job. */
  snapshotJob(request: DeepPartial<SnapshotJobRequest>, options?: CallOptions & CallOptionsExt): Promise<Snapshot>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
