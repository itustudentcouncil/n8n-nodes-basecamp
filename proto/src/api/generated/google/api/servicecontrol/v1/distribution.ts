// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/api/servicecontrol/v1/distribution.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Distribution_Exemplar } from "../../distribution.js";

export const protobufPackage = "google.api.servicecontrol.v1";

/**
 * Distribution represents a frequency distribution of double-valued sample
 * points. It contains the size of the population of sample points plus
 * additional optional information:
 *
 * * the arithmetic mean of the samples
 * * the minimum and maximum of the samples
 * * the sum-squared-deviation of the samples, used to compute variance
 * * a histogram of the values of the sample points
 */
export interface Distribution {
  /** The total number of samples in the distribution. Must be >= 0. */
  count: Long;
  /**
   * The arithmetic mean of the samples in the distribution. If `count` is
   * zero then this field must be zero.
   */
  mean: number;
  /** The minimum of the population of values. Ignored if `count` is zero. */
  minimum: number;
  /** The maximum of the population of values. Ignored if `count` is zero. */
  maximum: number;
  /**
   * The sum of squared deviations from the mean:
   *   Sum[i=1..count]((x_i - mean)^2)
   * where each x_i is a sample values. If `count` is zero then this field
   * must be zero, otherwise validation of the request fails.
   */
  sumOfSquaredDeviation: number;
  /**
   * The number of samples in each histogram bucket. `bucket_counts` are
   * optional. If present, they must sum to the `count` value.
   *
   * The buckets are defined below in `bucket_option`. There are N buckets.
   * `bucket_counts[0]` is the number of samples in the underflow bucket.
   * `bucket_counts[1]` to `bucket_counts[N-1]` are the numbers of samples
   * in each of the finite buckets. And `bucket_counts[N] is the number
   * of samples in the overflow bucket. See the comments of `bucket_option`
   * below for more details.
   *
   * Any suffix of trailing zeros may be omitted.
   */
  bucketCounts: Long[];
  /** Buckets with constant width. */
  linearBuckets?:
    | Distribution_LinearBuckets
    | undefined;
  /** Buckets with exponentially growing width. */
  exponentialBuckets?:
    | Distribution_ExponentialBuckets
    | undefined;
  /** Buckets with arbitrary user-provided width. */
  explicitBuckets?:
    | Distribution_ExplicitBuckets
    | undefined;
  /** Example points. Must be in increasing order of `value` field. */
  exemplars: Distribution_Exemplar[];
}

/** Describing buckets with constant width. */
export interface Distribution_LinearBuckets {
  /**
   * The number of finite buckets. With the underflow and overflow buckets,
   * the total number of buckets is `num_finite_buckets` + 2.
   * See comments on `bucket_options` for details.
   */
  numFiniteBuckets: number;
  /**
   * The i'th linear bucket covers the interval
   *   [offset + (i-1) * width, offset + i * width)
   * where i ranges from 1 to num_finite_buckets, inclusive.
   * Must be strictly positive.
   */
  width: number;
  /**
   * The i'th linear bucket covers the interval
   *   [offset + (i-1) * width, offset + i * width)
   * where i ranges from 1 to num_finite_buckets, inclusive.
   */
  offset: number;
}

/** Describing buckets with exponentially growing width. */
export interface Distribution_ExponentialBuckets {
  /**
   * The number of finite buckets. With the underflow and overflow buckets,
   * the total number of buckets is `num_finite_buckets` + 2.
   * See comments on `bucket_options` for details.
   */
  numFiniteBuckets: number;
  /**
   * The i'th exponential bucket covers the interval
   *   [scale * growth_factor^(i-1), scale * growth_factor^i)
   * where i ranges from 1 to num_finite_buckets inclusive.
   * Must be larger than 1.0.
   */
  growthFactor: number;
  /**
   * The i'th exponential bucket covers the interval
   *   [scale * growth_factor^(i-1), scale * growth_factor^i)
   * where i ranges from 1 to num_finite_buckets inclusive.
   * Must be > 0.
   */
  scale: number;
}

/** Describing buckets with arbitrary user-provided width. */
export interface Distribution_ExplicitBuckets {
  /**
   * 'bound' is a list of strictly increasing boundaries between
   * buckets. Note that a list of length N-1 defines N buckets because
   * of fenceposting. See comments on `bucket_options` for details.
   *
   * The i'th finite bucket covers the interval
   *   [bound[i-1], bound[i])
   * where i ranges from 1 to bound_size() - 1. Note that there are no
   * finite buckets at all if 'bound' only contains a single element; in
   * that special case the single bound defines the boundary between the
   * underflow and overflow buckets.
   *
   * bucket number                   lower bound    upper bound
   *  i == 0 (underflow)              -inf           bound[i]
   *  0 < i < bound_size()            bound[i-1]     bound[i]
   *  i == bound_size() (overflow)    bound[i-1]     +inf
   */
  bounds: number[];
}

function createBaseDistribution(): Distribution {
  return {
    count: Long.ZERO,
    mean: 0,
    minimum: 0,
    maximum: 0,
    sumOfSquaredDeviation: 0,
    bucketCounts: [],
    linearBuckets: undefined,
    exponentialBuckets: undefined,
    explicitBuckets: undefined,
    exemplars: [],
  };
}

export const Distribution: MessageFns<Distribution> = {
  encode(message: Distribution, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.count.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.count.toString());
    }
    if (message.mean !== 0) {
      writer.uint32(17).double(message.mean);
    }
    if (message.minimum !== 0) {
      writer.uint32(25).double(message.minimum);
    }
    if (message.maximum !== 0) {
      writer.uint32(33).double(message.maximum);
    }
    if (message.sumOfSquaredDeviation !== 0) {
      writer.uint32(41).double(message.sumOfSquaredDeviation);
    }
    writer.uint32(50).fork();
    for (const v of message.bucketCounts) {
      writer.int64(v.toString());
    }
    writer.join();
    if (message.linearBuckets !== undefined) {
      Distribution_LinearBuckets.encode(message.linearBuckets, writer.uint32(58).fork()).join();
    }
    if (message.exponentialBuckets !== undefined) {
      Distribution_ExponentialBuckets.encode(message.exponentialBuckets, writer.uint32(66).fork()).join();
    }
    if (message.explicitBuckets !== undefined) {
      Distribution_ExplicitBuckets.encode(message.explicitBuckets, writer.uint32(74).fork()).join();
    }
    for (const v of message.exemplars) {
      Distribution_Exemplar.encode(v!, writer.uint32(82).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Distribution {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDistribution();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.count = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.mean = reader.double();
          continue;
        case 3:
          if (tag !== 25) {
            break;
          }

          message.minimum = reader.double();
          continue;
        case 4:
          if (tag !== 33) {
            break;
          }

          message.maximum = reader.double();
          continue;
        case 5:
          if (tag !== 41) {
            break;
          }

          message.sumOfSquaredDeviation = reader.double();
          continue;
        case 6:
          if (tag === 48) {
            message.bucketCounts.push(Long.fromString(reader.int64().toString()));

            continue;
          }

          if (tag === 50) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.bucketCounts.push(Long.fromString(reader.int64().toString()));
            }

            continue;
          }

          break;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.linearBuckets = Distribution_LinearBuckets.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.exponentialBuckets = Distribution_ExponentialBuckets.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.explicitBuckets = Distribution_ExplicitBuckets.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.exemplars.push(Distribution_Exemplar.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Distribution {
    return {
      count: isSet(object.count) ? Long.fromValue(object.count) : Long.ZERO,
      mean: isSet(object.mean) ? globalThis.Number(object.mean) : 0,
      minimum: isSet(object.minimum) ? globalThis.Number(object.minimum) : 0,
      maximum: isSet(object.maximum) ? globalThis.Number(object.maximum) : 0,
      sumOfSquaredDeviation: isSet(object.sumOfSquaredDeviation) ? globalThis.Number(object.sumOfSquaredDeviation) : 0,
      bucketCounts: globalThis.Array.isArray(object?.bucketCounts)
        ? object.bucketCounts.map((e: any) => Long.fromValue(e))
        : [],
      linearBuckets: isSet(object.linearBuckets)
        ? Distribution_LinearBuckets.fromJSON(object.linearBuckets)
        : undefined,
      exponentialBuckets: isSet(object.exponentialBuckets)
        ? Distribution_ExponentialBuckets.fromJSON(object.exponentialBuckets)
        : undefined,
      explicitBuckets: isSet(object.explicitBuckets)
        ? Distribution_ExplicitBuckets.fromJSON(object.explicitBuckets)
        : undefined,
      exemplars: globalThis.Array.isArray(object?.exemplars)
        ? object.exemplars.map((e: any) => Distribution_Exemplar.fromJSON(e))
        : [],
    };
  },

  toJSON(message: Distribution): unknown {
    const obj: any = {};
    if (!message.count.equals(Long.ZERO)) {
      obj.count = (message.count || Long.ZERO).toString();
    }
    if (message.mean !== 0) {
      obj.mean = message.mean;
    }
    if (message.minimum !== 0) {
      obj.minimum = message.minimum;
    }
    if (message.maximum !== 0) {
      obj.maximum = message.maximum;
    }
    if (message.sumOfSquaredDeviation !== 0) {
      obj.sumOfSquaredDeviation = message.sumOfSquaredDeviation;
    }
    if (message.bucketCounts?.length) {
      obj.bucketCounts = message.bucketCounts.map((e) => (e || Long.ZERO).toString());
    }
    if (message.linearBuckets !== undefined) {
      obj.linearBuckets = Distribution_LinearBuckets.toJSON(message.linearBuckets);
    }
    if (message.exponentialBuckets !== undefined) {
      obj.exponentialBuckets = Distribution_ExponentialBuckets.toJSON(message.exponentialBuckets);
    }
    if (message.explicitBuckets !== undefined) {
      obj.explicitBuckets = Distribution_ExplicitBuckets.toJSON(message.explicitBuckets);
    }
    if (message.exemplars?.length) {
      obj.exemplars = message.exemplars.map((e) => Distribution_Exemplar.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<Distribution>): Distribution {
    return Distribution.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Distribution>): Distribution {
    const message = createBaseDistribution();
    message.count = (object.count !== undefined && object.count !== null) ? Long.fromValue(object.count) : Long.ZERO;
    message.mean = object.mean ?? 0;
    message.minimum = object.minimum ?? 0;
    message.maximum = object.maximum ?? 0;
    message.sumOfSquaredDeviation = object.sumOfSquaredDeviation ?? 0;
    message.bucketCounts = object.bucketCounts?.map((e) => Long.fromValue(e)) || [];
    message.linearBuckets = (object.linearBuckets !== undefined && object.linearBuckets !== null)
      ? Distribution_LinearBuckets.fromPartial(object.linearBuckets)
      : undefined;
    message.exponentialBuckets = (object.exponentialBuckets !== undefined && object.exponentialBuckets !== null)
      ? Distribution_ExponentialBuckets.fromPartial(object.exponentialBuckets)
      : undefined;
    message.explicitBuckets = (object.explicitBuckets !== undefined && object.explicitBuckets !== null)
      ? Distribution_ExplicitBuckets.fromPartial(object.explicitBuckets)
      : undefined;
    message.exemplars = object.exemplars?.map((e) => Distribution_Exemplar.fromPartial(e)) || [];
    return message;
  },
};

function createBaseDistribution_LinearBuckets(): Distribution_LinearBuckets {
  return { numFiniteBuckets: 0, width: 0, offset: 0 };
}

export const Distribution_LinearBuckets: MessageFns<Distribution_LinearBuckets> = {
  encode(message: Distribution_LinearBuckets, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.numFiniteBuckets !== 0) {
      writer.uint32(8).int32(message.numFiniteBuckets);
    }
    if (message.width !== 0) {
      writer.uint32(17).double(message.width);
    }
    if (message.offset !== 0) {
      writer.uint32(25).double(message.offset);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Distribution_LinearBuckets {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDistribution_LinearBuckets();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.numFiniteBuckets = reader.int32();
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.width = reader.double();
          continue;
        case 3:
          if (tag !== 25) {
            break;
          }

          message.offset = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Distribution_LinearBuckets {
    return {
      numFiniteBuckets: isSet(object.numFiniteBuckets) ? globalThis.Number(object.numFiniteBuckets) : 0,
      width: isSet(object.width) ? globalThis.Number(object.width) : 0,
      offset: isSet(object.offset) ? globalThis.Number(object.offset) : 0,
    };
  },

  toJSON(message: Distribution_LinearBuckets): unknown {
    const obj: any = {};
    if (message.numFiniteBuckets !== 0) {
      obj.numFiniteBuckets = Math.round(message.numFiniteBuckets);
    }
    if (message.width !== 0) {
      obj.width = message.width;
    }
    if (message.offset !== 0) {
      obj.offset = message.offset;
    }
    return obj;
  },

  create(base?: DeepPartial<Distribution_LinearBuckets>): Distribution_LinearBuckets {
    return Distribution_LinearBuckets.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Distribution_LinearBuckets>): Distribution_LinearBuckets {
    const message = createBaseDistribution_LinearBuckets();
    message.numFiniteBuckets = object.numFiniteBuckets ?? 0;
    message.width = object.width ?? 0;
    message.offset = object.offset ?? 0;
    return message;
  },
};

function createBaseDistribution_ExponentialBuckets(): Distribution_ExponentialBuckets {
  return { numFiniteBuckets: 0, growthFactor: 0, scale: 0 };
}

export const Distribution_ExponentialBuckets: MessageFns<Distribution_ExponentialBuckets> = {
  encode(message: Distribution_ExponentialBuckets, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.numFiniteBuckets !== 0) {
      writer.uint32(8).int32(message.numFiniteBuckets);
    }
    if (message.growthFactor !== 0) {
      writer.uint32(17).double(message.growthFactor);
    }
    if (message.scale !== 0) {
      writer.uint32(25).double(message.scale);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Distribution_ExponentialBuckets {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDistribution_ExponentialBuckets();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.numFiniteBuckets = reader.int32();
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.growthFactor = reader.double();
          continue;
        case 3:
          if (tag !== 25) {
            break;
          }

          message.scale = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Distribution_ExponentialBuckets {
    return {
      numFiniteBuckets: isSet(object.numFiniteBuckets) ? globalThis.Number(object.numFiniteBuckets) : 0,
      growthFactor: isSet(object.growthFactor) ? globalThis.Number(object.growthFactor) : 0,
      scale: isSet(object.scale) ? globalThis.Number(object.scale) : 0,
    };
  },

  toJSON(message: Distribution_ExponentialBuckets): unknown {
    const obj: any = {};
    if (message.numFiniteBuckets !== 0) {
      obj.numFiniteBuckets = Math.round(message.numFiniteBuckets);
    }
    if (message.growthFactor !== 0) {
      obj.growthFactor = message.growthFactor;
    }
    if (message.scale !== 0) {
      obj.scale = message.scale;
    }
    return obj;
  },

  create(base?: DeepPartial<Distribution_ExponentialBuckets>): Distribution_ExponentialBuckets {
    return Distribution_ExponentialBuckets.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Distribution_ExponentialBuckets>): Distribution_ExponentialBuckets {
    const message = createBaseDistribution_ExponentialBuckets();
    message.numFiniteBuckets = object.numFiniteBuckets ?? 0;
    message.growthFactor = object.growthFactor ?? 0;
    message.scale = object.scale ?? 0;
    return message;
  },
};

function createBaseDistribution_ExplicitBuckets(): Distribution_ExplicitBuckets {
  return { bounds: [] };
}

export const Distribution_ExplicitBuckets: MessageFns<Distribution_ExplicitBuckets> = {
  encode(message: Distribution_ExplicitBuckets, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    writer.uint32(10).fork();
    for (const v of message.bounds) {
      writer.double(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Distribution_ExplicitBuckets {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDistribution_ExplicitBuckets();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag === 9) {
            message.bounds.push(reader.double());

            continue;
          }

          if (tag === 10) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.bounds.push(reader.double());
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Distribution_ExplicitBuckets {
    return {
      bounds: globalThis.Array.isArray(object?.bounds) ? object.bounds.map((e: any) => globalThis.Number(e)) : [],
    };
  },

  toJSON(message: Distribution_ExplicitBuckets): unknown {
    const obj: any = {};
    if (message.bounds?.length) {
      obj.bounds = message.bounds;
    }
    return obj;
  },

  create(base?: DeepPartial<Distribution_ExplicitBuckets>): Distribution_ExplicitBuckets {
    return Distribution_ExplicitBuckets.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Distribution_ExplicitBuckets>): Distribution_ExplicitBuckets {
    const message = createBaseDistribution_ExplicitBuckets();
    message.bounds = object.bounds?.map((e) => e) || [];
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
