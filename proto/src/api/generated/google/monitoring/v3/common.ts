// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/monitoring/v3/common.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Distribution } from "../../api/distribution.js";
import { Duration } from "../../protobuf/duration.js";
import { Timestamp } from "../../protobuf/timestamp.js";

export const protobufPackage = "google.monitoring.v3";

/**
 * Specifies an ordering relationship on two arguments, called `left` and
 * `right`.
 */
export enum ComparisonType {
  /** COMPARISON_UNSPECIFIED - No ordering relationship is specified. */
  COMPARISON_UNSPECIFIED = 0,
  /** COMPARISON_GT - True if the left argument is greater than the right argument. */
  COMPARISON_GT = 1,
  /** COMPARISON_GE - True if the left argument is greater than or equal to the right argument. */
  COMPARISON_GE = 2,
  /** COMPARISON_LT - True if the left argument is less than the right argument. */
  COMPARISON_LT = 3,
  /** COMPARISON_LE - True if the left argument is less than or equal to the right argument. */
  COMPARISON_LE = 4,
  /** COMPARISON_EQ - True if the left argument is equal to the right argument. */
  COMPARISON_EQ = 5,
  /** COMPARISON_NE - True if the left argument is not equal to the right argument. */
  COMPARISON_NE = 6,
  UNRECOGNIZED = -1,
}

export function comparisonTypeFromJSON(object: any): ComparisonType {
  switch (object) {
    case 0:
    case "COMPARISON_UNSPECIFIED":
      return ComparisonType.COMPARISON_UNSPECIFIED;
    case 1:
    case "COMPARISON_GT":
      return ComparisonType.COMPARISON_GT;
    case 2:
    case "COMPARISON_GE":
      return ComparisonType.COMPARISON_GE;
    case 3:
    case "COMPARISON_LT":
      return ComparisonType.COMPARISON_LT;
    case 4:
    case "COMPARISON_LE":
      return ComparisonType.COMPARISON_LE;
    case 5:
    case "COMPARISON_EQ":
      return ComparisonType.COMPARISON_EQ;
    case 6:
    case "COMPARISON_NE":
      return ComparisonType.COMPARISON_NE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ComparisonType.UNRECOGNIZED;
  }
}

export function comparisonTypeToJSON(object: ComparisonType): string {
  switch (object) {
    case ComparisonType.COMPARISON_UNSPECIFIED:
      return "COMPARISON_UNSPECIFIED";
    case ComparisonType.COMPARISON_GT:
      return "COMPARISON_GT";
    case ComparisonType.COMPARISON_GE:
      return "COMPARISON_GE";
    case ComparisonType.COMPARISON_LT:
      return "COMPARISON_LT";
    case ComparisonType.COMPARISON_LE:
      return "COMPARISON_LE";
    case ComparisonType.COMPARISON_EQ:
      return "COMPARISON_EQ";
    case ComparisonType.COMPARISON_NE:
      return "COMPARISON_NE";
    case ComparisonType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * The tier of service for a Metrics Scope. Please see the
 * [service tiers
 * documentation](https://cloud.google.com/monitoring/workspaces/tiers) for more
 * details.
 *
 * @deprecated
 */
export enum ServiceTier {
  /**
   * SERVICE_TIER_UNSPECIFIED - An invalid sentinel value, used to indicate that a tier has not
   * been provided explicitly.
   */
  SERVICE_TIER_UNSPECIFIED = 0,
  /**
   * SERVICE_TIER_BASIC - The Cloud Monitoring Basic tier, a free tier of service that provides basic
   * features, a moderate allotment of logs, and access to built-in metrics.
   * A number of features are not available in this tier. For more details,
   * see [the service tiers
   * documentation](https://cloud.google.com/monitoring/workspaces/tiers).
   */
  SERVICE_TIER_BASIC = 1,
  /**
   * SERVICE_TIER_PREMIUM - The Cloud Monitoring Premium tier, a higher, more expensive tier of service
   * that provides access to all Cloud Monitoring features, lets you use Cloud
   * Monitoring with AWS accounts, and has a larger allotments for logs and
   * metrics. For more details, see [the service tiers
   * documentation](https://cloud.google.com/monitoring/workspaces/tiers).
   */
  SERVICE_TIER_PREMIUM = 2,
  UNRECOGNIZED = -1,
}

export function serviceTierFromJSON(object: any): ServiceTier {
  switch (object) {
    case 0:
    case "SERVICE_TIER_UNSPECIFIED":
      return ServiceTier.SERVICE_TIER_UNSPECIFIED;
    case 1:
    case "SERVICE_TIER_BASIC":
      return ServiceTier.SERVICE_TIER_BASIC;
    case 2:
    case "SERVICE_TIER_PREMIUM":
      return ServiceTier.SERVICE_TIER_PREMIUM;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ServiceTier.UNRECOGNIZED;
  }
}

export function serviceTierToJSON(object: ServiceTier): string {
  switch (object) {
    case ServiceTier.SERVICE_TIER_UNSPECIFIED:
      return "SERVICE_TIER_UNSPECIFIED";
    case ServiceTier.SERVICE_TIER_BASIC:
      return "SERVICE_TIER_BASIC";
    case ServiceTier.SERVICE_TIER_PREMIUM:
      return "SERVICE_TIER_PREMIUM";
    case ServiceTier.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** A single strongly-typed value. */
export interface TypedValue {
  /** A Boolean value: `true` or `false`. */
  boolValue?:
    | boolean
    | undefined;
  /** A 64-bit integer. Its range is approximately &plusmn;9.2x10<sup>18</sup>. */
  int64Value?:
    | Long
    | undefined;
  /**
   * A 64-bit double-precision floating-point number. Its magnitude
   * is approximately &plusmn;10<sup>&plusmn;300</sup> and it has 16
   * significant digits of precision.
   */
  doubleValue?:
    | number
    | undefined;
  /** A variable-length string value. */
  stringValue?:
    | string
    | undefined;
  /** A distribution value. */
  distributionValue?: Distribution | undefined;
}

/**
 * Describes a time interval:
 *
 *   * Reads: A half-open time interval. It includes the end time but
 *     excludes the start time: `(startTime, endTime]`. The start time
 *     must be specified, must be earlier than the end time, and should be
 *     no older than the data retention period for the metric.
 *   * Writes: A closed time interval. It extends from the start time to the end
 *   time,
 *     and includes both: `[startTime, endTime]`. Valid time intervals
 *     depend on the
 *     [`MetricKind`](https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.metricDescriptors#MetricKind)
 *     of the metric value. The end time must not be earlier than the start
 *     time, and the end time must not be more than 25 hours in the past or more
 *     than five minutes in the future.
 *     * For `GAUGE` metrics, the `startTime` value is technically optional; if
 *       no value is specified, the start time defaults to the value of the
 *       end time, and the interval represents a single point in time. If both
 *       start and end times are specified, they must be identical. Such an
 *       interval is valid only for `GAUGE` metrics, which are point-in-time
 *       measurements. The end time of a new interval must be at least a
 *       millisecond after the end time of the previous interval.
 *     * For `DELTA` metrics, the start time and end time must specify a
 *       non-zero interval, with subsequent points specifying contiguous and
 *       non-overlapping intervals. For `DELTA` metrics, the start time of
 *       the next interval must be at least a millisecond after the end time
 *       of the previous interval.
 *     * For `CUMULATIVE` metrics, the start time and end time must specify a
 *       non-zero interval, with subsequent points specifying the same
 *       start time and increasing end times, until an event resets the
 *       cumulative value to zero and sets a new start time for the following
 *       points. The new start time must be at least a millisecond after the
 *       end time of the previous interval.
 *     * The start time of a new interval must be at least a millisecond after
 *     the
 *       end time of the previous interval because intervals are closed. If the
 *       start time of a new interval is the same as the end time of the
 *       previous interval, then data written at the new start time could
 *       overwrite data written at the previous end time.
 */
export interface TimeInterval {
  /** Required. The end of the time interval. */
  endTime:
    | Date
    | undefined;
  /**
   * Optional. The beginning of the time interval.  The default value
   * for the start time is the end time. The start time must not be
   * later than the end time.
   */
  startTime: Date | undefined;
}

/**
 * Describes how to combine multiple time series to provide a different view of
 * the data.  Aggregation of time series is done in two steps. First, each time
 * series in the set is _aligned_ to the same time interval boundaries, then the
 * set of time series is optionally _reduced_ in number.
 *
 * Alignment consists of applying the `per_series_aligner` operation
 * to each time series after its data has been divided into regular
 * `alignment_period` time intervals. This process takes _all_ of the data
 * points in an alignment period, applies a mathematical transformation such as
 * averaging, minimum, maximum, delta, etc., and converts them into a single
 * data point per period.
 *
 * Reduction is when the aligned and transformed time series can optionally be
 * combined, reducing the number of time series through similar mathematical
 * transformations. Reduction involves applying a `cross_series_reducer` to
 * all the time series, optionally sorting the time series into subsets with
 * `group_by_fields`, and applying the reducer to each subset.
 *
 * The raw time series data can contain a huge amount of information from
 * multiple sources. Alignment and reduction transforms this mass of data into
 * a more manageable and representative collection of data, for example "the
 * 95% latency across the average of all tasks in a cluster". This
 * representative data can be more easily graphed and comprehended, and the
 * individual time series data is still available for later drilldown. For more
 * details, see [Filtering and
 * aggregation](https://cloud.google.com/monitoring/api/v3/aggregation).
 */
export interface Aggregation {
  /**
   * The `alignment_period` specifies a time interval, in seconds, that is used
   * to divide the data in all the
   * [time series][google.monitoring.v3.TimeSeries] into consistent blocks of
   * time. This will be done before the per-series aligner can be applied to
   * the data.
   *
   * The value must be at least 60 seconds. If a per-series
   * aligner other than `ALIGN_NONE` is specified, this field is required or an
   * error is returned. If no per-series aligner is specified, or the aligner
   * `ALIGN_NONE` is specified, then this field is ignored.
   *
   * The maximum value of the `alignment_period` is 104 weeks (2 years) for
   * charts, and 90,000 seconds (25 hours) for alerting policies.
   */
  alignmentPeriod:
    | Duration
    | undefined;
  /**
   * An `Aligner` describes how to bring the data points in a single
   * time series into temporal alignment. Except for `ALIGN_NONE`, all
   * alignments cause all the data points in an `alignment_period` to be
   * mathematically grouped together, resulting in a single data point for
   * each `alignment_period` with end timestamp at the end of the period.
   *
   * Not all alignment operations may be applied to all time series. The valid
   * choices depend on the `metric_kind` and `value_type` of the original time
   * series. Alignment can change the `metric_kind` or the `value_type` of
   * the time series.
   *
   * Time series data must be aligned in order to perform cross-time
   * series reduction. If `cross_series_reducer` is specified, then
   * `per_series_aligner` must be specified and not equal to `ALIGN_NONE`
   * and `alignment_period` must be specified; otherwise, an error is
   * returned.
   */
  perSeriesAligner: Aggregation_Aligner;
  /**
   * The reduction operation to be used to combine time series into a single
   * time series, where the value of each data point in the resulting series is
   * a function of all the already aligned values in the input time series.
   *
   * Not all reducer operations can be applied to all time series. The valid
   * choices depend on the `metric_kind` and the `value_type` of the original
   * time series. Reduction can yield a time series with a different
   * `metric_kind` or `value_type` than the input time series.
   *
   * Time series data must first be aligned (see `per_series_aligner`) in order
   * to perform cross-time series reduction. If `cross_series_reducer` is
   * specified, then `per_series_aligner` must be specified, and must not be
   * `ALIGN_NONE`. An `alignment_period` must also be specified; otherwise, an
   * error is returned.
   */
  crossSeriesReducer: Aggregation_Reducer;
  /**
   * The set of fields to preserve when `cross_series_reducer` is
   * specified. The `group_by_fields` determine how the time series are
   * partitioned into subsets prior to applying the aggregation
   * operation. Each subset contains time series that have the same
   * value for each of the grouping fields. Each individual time
   * series is a member of exactly one subset. The
   * `cross_series_reducer` is applied to each subset of time series.
   * It is not possible to reduce across different resource types, so
   * this field implicitly contains `resource.type`.  Fields not
   * specified in `group_by_fields` are aggregated away.  If
   * `group_by_fields` is not specified and all the time series have
   * the same resource type, then the time series are aggregated into
   * a single output time series. If `cross_series_reducer` is not
   * defined, this field is ignored.
   */
  groupByFields: string[];
}

/**
 * The `Aligner` specifies the operation that will be applied to the data
 * points in each alignment period in a time series. Except for
 * `ALIGN_NONE`, which specifies that no operation be applied, each alignment
 * operation replaces the set of data values in each alignment period with
 * a single value: the result of applying the operation to the data values.
 * An aligned time series has a single data value at the end of each
 * `alignment_period`.
 *
 * An alignment operation can change the data type of the values, too. For
 * example, if you apply a counting operation to boolean values, the data
 * `value_type` in the original time series is `BOOLEAN`, but the `value_type`
 * in the aligned result is `INT64`.
 */
export enum Aggregation_Aligner {
  /**
   * ALIGN_NONE - No alignment. Raw data is returned. Not valid if cross-series reduction
   * is requested. The `value_type` of the result is the same as the
   * `value_type` of the input.
   */
  ALIGN_NONE = 0,
  /**
   * ALIGN_DELTA - Align and convert to
   * [DELTA][google.api.MetricDescriptor.MetricKind.DELTA].
   * The output is `delta = y1 - y0`.
   *
   * This alignment is valid for
   * [CUMULATIVE][google.api.MetricDescriptor.MetricKind.CUMULATIVE] and
   * `DELTA` metrics. If the selected alignment period results in periods
   * with no data, then the aligned value for such a period is created by
   * interpolation. The `value_type`  of the aligned result is the same as
   * the `value_type` of the input.
   */
  ALIGN_DELTA = 1,
  /**
   * ALIGN_RATE - Align and convert to a rate. The result is computed as
   * `rate = (y1 - y0)/(t1 - t0)`, or "delta over time".
   * Think of this aligner as providing the slope of the line that passes
   * through the value at the start and at the end of the `alignment_period`.
   *
   * This aligner is valid for `CUMULATIVE`
   * and `DELTA` metrics with numeric values. If the selected alignment
   * period results in periods with no data, then the aligned value for
   * such a period is created by interpolation. The output is a `GAUGE`
   * metric with `value_type` `DOUBLE`.
   *
   * If, by "rate", you mean "percentage change", see the
   * `ALIGN_PERCENT_CHANGE` aligner instead.
   */
  ALIGN_RATE = 2,
  /**
   * ALIGN_INTERPOLATE - Align by interpolating between adjacent points around the alignment
   * period boundary. This aligner is valid for `GAUGE` metrics with
   * numeric values. The `value_type` of the aligned result is the same as the
   * `value_type` of the input.
   */
  ALIGN_INTERPOLATE = 3,
  /**
   * ALIGN_NEXT_OLDER - Align by moving the most recent data point before the end of the
   * alignment period to the boundary at the end of the alignment
   * period. This aligner is valid for `GAUGE` metrics. The `value_type` of
   * the aligned result is the same as the `value_type` of the input.
   */
  ALIGN_NEXT_OLDER = 4,
  /**
   * ALIGN_MIN - Align the time series by returning the minimum value in each alignment
   * period. This aligner is valid for `GAUGE` and `DELTA` metrics with
   * numeric values. The `value_type` of the aligned result is the same as
   * the `value_type` of the input.
   */
  ALIGN_MIN = 10,
  /**
   * ALIGN_MAX - Align the time series by returning the maximum value in each alignment
   * period. This aligner is valid for `GAUGE` and `DELTA` metrics with
   * numeric values. The `value_type` of the aligned result is the same as
   * the `value_type` of the input.
   */
  ALIGN_MAX = 11,
  /**
   * ALIGN_MEAN - Align the time series by returning the mean value in each alignment
   * period. This aligner is valid for `GAUGE` and `DELTA` metrics with
   * numeric values. The `value_type` of the aligned result is `DOUBLE`.
   */
  ALIGN_MEAN = 12,
  /**
   * ALIGN_COUNT - Align the time series by returning the number of values in each alignment
   * period. This aligner is valid for `GAUGE` and `DELTA` metrics with
   * numeric or Boolean values. The `value_type` of the aligned result is
   * `INT64`.
   */
  ALIGN_COUNT = 13,
  /**
   * ALIGN_SUM - Align the time series by returning the sum of the values in each
   * alignment period. This aligner is valid for `GAUGE` and `DELTA`
   * metrics with numeric and distribution values. The `value_type` of the
   * aligned result is the same as the `value_type` of the input.
   */
  ALIGN_SUM = 14,
  /**
   * ALIGN_STDDEV - Align the time series by returning the standard deviation of the values
   * in each alignment period. This aligner is valid for `GAUGE` and
   * `DELTA` metrics with numeric values. The `value_type` of the output is
   * `DOUBLE`.
   */
  ALIGN_STDDEV = 15,
  /**
   * ALIGN_COUNT_TRUE - Align the time series by returning the number of `True` values in
   * each alignment period. This aligner is valid for `GAUGE` metrics with
   * Boolean values. The `value_type` of the output is `INT64`.
   */
  ALIGN_COUNT_TRUE = 16,
  /**
   * ALIGN_COUNT_FALSE - Align the time series by returning the number of `False` values in
   * each alignment period. This aligner is valid for `GAUGE` metrics with
   * Boolean values. The `value_type` of the output is `INT64`.
   */
  ALIGN_COUNT_FALSE = 24,
  /**
   * ALIGN_FRACTION_TRUE - Align the time series by returning the ratio of the number of `True`
   * values to the total number of values in each alignment period. This
   * aligner is valid for `GAUGE` metrics with Boolean values. The output
   * value is in the range [0.0, 1.0] and has `value_type` `DOUBLE`.
   */
  ALIGN_FRACTION_TRUE = 17,
  /**
   * ALIGN_PERCENTILE_99 - Align the time series by using [percentile
   * aggregation](https://en.wikipedia.org/wiki/Percentile). The resulting
   * data point in each alignment period is the 99th percentile of all data
   * points in the period. This aligner is valid for `GAUGE` and `DELTA`
   * metrics with distribution values. The output is a `GAUGE` metric with
   * `value_type` `DOUBLE`.
   */
  ALIGN_PERCENTILE_99 = 18,
  /**
   * ALIGN_PERCENTILE_95 - Align the time series by using [percentile
   * aggregation](https://en.wikipedia.org/wiki/Percentile). The resulting
   * data point in each alignment period is the 95th percentile of all data
   * points in the period. This aligner is valid for `GAUGE` and `DELTA`
   * metrics with distribution values. The output is a `GAUGE` metric with
   * `value_type` `DOUBLE`.
   */
  ALIGN_PERCENTILE_95 = 19,
  /**
   * ALIGN_PERCENTILE_50 - Align the time series by using [percentile
   * aggregation](https://en.wikipedia.org/wiki/Percentile). The resulting
   * data point in each alignment period is the 50th percentile of all data
   * points in the period. This aligner is valid for `GAUGE` and `DELTA`
   * metrics with distribution values. The output is a `GAUGE` metric with
   * `value_type` `DOUBLE`.
   */
  ALIGN_PERCENTILE_50 = 20,
  /**
   * ALIGN_PERCENTILE_05 - Align the time series by using [percentile
   * aggregation](https://en.wikipedia.org/wiki/Percentile). The resulting
   * data point in each alignment period is the 5th percentile of all data
   * points in the period. This aligner is valid for `GAUGE` and `DELTA`
   * metrics with distribution values. The output is a `GAUGE` metric with
   * `value_type` `DOUBLE`.
   */
  ALIGN_PERCENTILE_05 = 21,
  /**
   * ALIGN_PERCENT_CHANGE - Align and convert to a percentage change. This aligner is valid for
   * `GAUGE` and `DELTA` metrics with numeric values. This alignment returns
   * `((current - previous)/previous) * 100`, where the value of `previous` is
   * determined based on the `alignment_period`.
   *
   * If the values of `current` and `previous` are both 0, then the returned
   * value is 0. If only `previous` is 0, the returned value is infinity.
   *
   * A 10-minute moving mean is computed at each point of the alignment period
   * prior to the above calculation to smooth the metric and prevent false
   * positives from very short-lived spikes. The moving mean is only
   * applicable for data whose values are `>= 0`. Any values `< 0` are
   * treated as a missing datapoint, and are ignored. While `DELTA`
   * metrics are accepted by this alignment, special care should be taken that
   * the values for the metric will always be positive. The output is a
   * `GAUGE` metric with `value_type` `DOUBLE`.
   */
  ALIGN_PERCENT_CHANGE = 23,
  UNRECOGNIZED = -1,
}

export function aggregation_AlignerFromJSON(object: any): Aggregation_Aligner {
  switch (object) {
    case 0:
    case "ALIGN_NONE":
      return Aggregation_Aligner.ALIGN_NONE;
    case 1:
    case "ALIGN_DELTA":
      return Aggregation_Aligner.ALIGN_DELTA;
    case 2:
    case "ALIGN_RATE":
      return Aggregation_Aligner.ALIGN_RATE;
    case 3:
    case "ALIGN_INTERPOLATE":
      return Aggregation_Aligner.ALIGN_INTERPOLATE;
    case 4:
    case "ALIGN_NEXT_OLDER":
      return Aggregation_Aligner.ALIGN_NEXT_OLDER;
    case 10:
    case "ALIGN_MIN":
      return Aggregation_Aligner.ALIGN_MIN;
    case 11:
    case "ALIGN_MAX":
      return Aggregation_Aligner.ALIGN_MAX;
    case 12:
    case "ALIGN_MEAN":
      return Aggregation_Aligner.ALIGN_MEAN;
    case 13:
    case "ALIGN_COUNT":
      return Aggregation_Aligner.ALIGN_COUNT;
    case 14:
    case "ALIGN_SUM":
      return Aggregation_Aligner.ALIGN_SUM;
    case 15:
    case "ALIGN_STDDEV":
      return Aggregation_Aligner.ALIGN_STDDEV;
    case 16:
    case "ALIGN_COUNT_TRUE":
      return Aggregation_Aligner.ALIGN_COUNT_TRUE;
    case 24:
    case "ALIGN_COUNT_FALSE":
      return Aggregation_Aligner.ALIGN_COUNT_FALSE;
    case 17:
    case "ALIGN_FRACTION_TRUE":
      return Aggregation_Aligner.ALIGN_FRACTION_TRUE;
    case 18:
    case "ALIGN_PERCENTILE_99":
      return Aggregation_Aligner.ALIGN_PERCENTILE_99;
    case 19:
    case "ALIGN_PERCENTILE_95":
      return Aggregation_Aligner.ALIGN_PERCENTILE_95;
    case 20:
    case "ALIGN_PERCENTILE_50":
      return Aggregation_Aligner.ALIGN_PERCENTILE_50;
    case 21:
    case "ALIGN_PERCENTILE_05":
      return Aggregation_Aligner.ALIGN_PERCENTILE_05;
    case 23:
    case "ALIGN_PERCENT_CHANGE":
      return Aggregation_Aligner.ALIGN_PERCENT_CHANGE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Aggregation_Aligner.UNRECOGNIZED;
  }
}

export function aggregation_AlignerToJSON(object: Aggregation_Aligner): string {
  switch (object) {
    case Aggregation_Aligner.ALIGN_NONE:
      return "ALIGN_NONE";
    case Aggregation_Aligner.ALIGN_DELTA:
      return "ALIGN_DELTA";
    case Aggregation_Aligner.ALIGN_RATE:
      return "ALIGN_RATE";
    case Aggregation_Aligner.ALIGN_INTERPOLATE:
      return "ALIGN_INTERPOLATE";
    case Aggregation_Aligner.ALIGN_NEXT_OLDER:
      return "ALIGN_NEXT_OLDER";
    case Aggregation_Aligner.ALIGN_MIN:
      return "ALIGN_MIN";
    case Aggregation_Aligner.ALIGN_MAX:
      return "ALIGN_MAX";
    case Aggregation_Aligner.ALIGN_MEAN:
      return "ALIGN_MEAN";
    case Aggregation_Aligner.ALIGN_COUNT:
      return "ALIGN_COUNT";
    case Aggregation_Aligner.ALIGN_SUM:
      return "ALIGN_SUM";
    case Aggregation_Aligner.ALIGN_STDDEV:
      return "ALIGN_STDDEV";
    case Aggregation_Aligner.ALIGN_COUNT_TRUE:
      return "ALIGN_COUNT_TRUE";
    case Aggregation_Aligner.ALIGN_COUNT_FALSE:
      return "ALIGN_COUNT_FALSE";
    case Aggregation_Aligner.ALIGN_FRACTION_TRUE:
      return "ALIGN_FRACTION_TRUE";
    case Aggregation_Aligner.ALIGN_PERCENTILE_99:
      return "ALIGN_PERCENTILE_99";
    case Aggregation_Aligner.ALIGN_PERCENTILE_95:
      return "ALIGN_PERCENTILE_95";
    case Aggregation_Aligner.ALIGN_PERCENTILE_50:
      return "ALIGN_PERCENTILE_50";
    case Aggregation_Aligner.ALIGN_PERCENTILE_05:
      return "ALIGN_PERCENTILE_05";
    case Aggregation_Aligner.ALIGN_PERCENT_CHANGE:
      return "ALIGN_PERCENT_CHANGE";
    case Aggregation_Aligner.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * A Reducer operation describes how to aggregate data points from multiple
 * time series into a single time series, where the value of each data point
 * in the resulting series is a function of all the already aligned values in
 * the input time series.
 */
export enum Aggregation_Reducer {
  /**
   * REDUCE_NONE - No cross-time series reduction. The output of the `Aligner` is
   * returned.
   */
  REDUCE_NONE = 0,
  /**
   * REDUCE_MEAN - Reduce by computing the mean value across time series for each
   * alignment period. This reducer is valid for
   * [DELTA][google.api.MetricDescriptor.MetricKind.DELTA] and
   * [GAUGE][google.api.MetricDescriptor.MetricKind.GAUGE] metrics with
   * numeric or distribution values. The `value_type` of the output is
   * [DOUBLE][google.api.MetricDescriptor.ValueType.DOUBLE].
   */
  REDUCE_MEAN = 1,
  /**
   * REDUCE_MIN - Reduce by computing the minimum value across time series for each
   * alignment period. This reducer is valid for `DELTA` and `GAUGE` metrics
   * with numeric values. The `value_type` of the output is the same as the
   * `value_type` of the input.
   */
  REDUCE_MIN = 2,
  /**
   * REDUCE_MAX - Reduce by computing the maximum value across time series for each
   * alignment period. This reducer is valid for `DELTA` and `GAUGE` metrics
   * with numeric values. The `value_type` of the output is the same as the
   * `value_type` of the input.
   */
  REDUCE_MAX = 3,
  /**
   * REDUCE_SUM - Reduce by computing the sum across time series for each
   * alignment period. This reducer is valid for `DELTA` and `GAUGE` metrics
   * with numeric and distribution values. The `value_type` of the output is
   * the same as the `value_type` of the input.
   */
  REDUCE_SUM = 4,
  /**
   * REDUCE_STDDEV - Reduce by computing the standard deviation across time series
   * for each alignment period. This reducer is valid for `DELTA` and
   * `GAUGE` metrics with numeric or distribution values. The `value_type`
   * of the output is `DOUBLE`.
   */
  REDUCE_STDDEV = 5,
  /**
   * REDUCE_COUNT - Reduce by computing the number of data points across time series
   * for each alignment period. This reducer is valid for `DELTA` and
   * `GAUGE` metrics of numeric, Boolean, distribution, and string
   * `value_type`. The `value_type` of the output is `INT64`.
   */
  REDUCE_COUNT = 6,
  /**
   * REDUCE_COUNT_TRUE - Reduce by computing the number of `True`-valued data points across time
   * series for each alignment period. This reducer is valid for `DELTA` and
   * `GAUGE` metrics of Boolean `value_type`. The `value_type` of the output
   * is `INT64`.
   */
  REDUCE_COUNT_TRUE = 7,
  /**
   * REDUCE_COUNT_FALSE - Reduce by computing the number of `False`-valued data points across time
   * series for each alignment period. This reducer is valid for `DELTA` and
   * `GAUGE` metrics of Boolean `value_type`. The `value_type` of the output
   * is `INT64`.
   */
  REDUCE_COUNT_FALSE = 15,
  /**
   * REDUCE_FRACTION_TRUE - Reduce by computing the ratio of the number of `True`-valued data points
   * to the total number of data points for each alignment period. This
   * reducer is valid for `DELTA` and `GAUGE` metrics of Boolean `value_type`.
   * The output value is in the range [0.0, 1.0] and has `value_type`
   * `DOUBLE`.
   */
  REDUCE_FRACTION_TRUE = 8,
  /**
   * REDUCE_PERCENTILE_99 - Reduce by computing the [99th
   * percentile](https://en.wikipedia.org/wiki/Percentile) of data points
   * across time series for each alignment period. This reducer is valid for
   * `GAUGE` and `DELTA` metrics of numeric and distribution type. The value
   * of the output is `DOUBLE`.
   */
  REDUCE_PERCENTILE_99 = 9,
  /**
   * REDUCE_PERCENTILE_95 - Reduce by computing the [95th
   * percentile](https://en.wikipedia.org/wiki/Percentile) of data points
   * across time series for each alignment period. This reducer is valid for
   * `GAUGE` and `DELTA` metrics of numeric and distribution type. The value
   * of the output is `DOUBLE`.
   */
  REDUCE_PERCENTILE_95 = 10,
  /**
   * REDUCE_PERCENTILE_50 - Reduce by computing the [50th
   * percentile](https://en.wikipedia.org/wiki/Percentile) of data points
   * across time series for each alignment period. This reducer is valid for
   * `GAUGE` and `DELTA` metrics of numeric and distribution type. The value
   * of the output is `DOUBLE`.
   */
  REDUCE_PERCENTILE_50 = 11,
  /**
   * REDUCE_PERCENTILE_05 - Reduce by computing the [5th
   * percentile](https://en.wikipedia.org/wiki/Percentile) of data points
   * across time series for each alignment period. This reducer is valid for
   * `GAUGE` and `DELTA` metrics of numeric and distribution type. The value
   * of the output is `DOUBLE`.
   */
  REDUCE_PERCENTILE_05 = 12,
  UNRECOGNIZED = -1,
}

export function aggregation_ReducerFromJSON(object: any): Aggregation_Reducer {
  switch (object) {
    case 0:
    case "REDUCE_NONE":
      return Aggregation_Reducer.REDUCE_NONE;
    case 1:
    case "REDUCE_MEAN":
      return Aggregation_Reducer.REDUCE_MEAN;
    case 2:
    case "REDUCE_MIN":
      return Aggregation_Reducer.REDUCE_MIN;
    case 3:
    case "REDUCE_MAX":
      return Aggregation_Reducer.REDUCE_MAX;
    case 4:
    case "REDUCE_SUM":
      return Aggregation_Reducer.REDUCE_SUM;
    case 5:
    case "REDUCE_STDDEV":
      return Aggregation_Reducer.REDUCE_STDDEV;
    case 6:
    case "REDUCE_COUNT":
      return Aggregation_Reducer.REDUCE_COUNT;
    case 7:
    case "REDUCE_COUNT_TRUE":
      return Aggregation_Reducer.REDUCE_COUNT_TRUE;
    case 15:
    case "REDUCE_COUNT_FALSE":
      return Aggregation_Reducer.REDUCE_COUNT_FALSE;
    case 8:
    case "REDUCE_FRACTION_TRUE":
      return Aggregation_Reducer.REDUCE_FRACTION_TRUE;
    case 9:
    case "REDUCE_PERCENTILE_99":
      return Aggregation_Reducer.REDUCE_PERCENTILE_99;
    case 10:
    case "REDUCE_PERCENTILE_95":
      return Aggregation_Reducer.REDUCE_PERCENTILE_95;
    case 11:
    case "REDUCE_PERCENTILE_50":
      return Aggregation_Reducer.REDUCE_PERCENTILE_50;
    case 12:
    case "REDUCE_PERCENTILE_05":
      return Aggregation_Reducer.REDUCE_PERCENTILE_05;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Aggregation_Reducer.UNRECOGNIZED;
  }
}

export function aggregation_ReducerToJSON(object: Aggregation_Reducer): string {
  switch (object) {
    case Aggregation_Reducer.REDUCE_NONE:
      return "REDUCE_NONE";
    case Aggregation_Reducer.REDUCE_MEAN:
      return "REDUCE_MEAN";
    case Aggregation_Reducer.REDUCE_MIN:
      return "REDUCE_MIN";
    case Aggregation_Reducer.REDUCE_MAX:
      return "REDUCE_MAX";
    case Aggregation_Reducer.REDUCE_SUM:
      return "REDUCE_SUM";
    case Aggregation_Reducer.REDUCE_STDDEV:
      return "REDUCE_STDDEV";
    case Aggregation_Reducer.REDUCE_COUNT:
      return "REDUCE_COUNT";
    case Aggregation_Reducer.REDUCE_COUNT_TRUE:
      return "REDUCE_COUNT_TRUE";
    case Aggregation_Reducer.REDUCE_COUNT_FALSE:
      return "REDUCE_COUNT_FALSE";
    case Aggregation_Reducer.REDUCE_FRACTION_TRUE:
      return "REDUCE_FRACTION_TRUE";
    case Aggregation_Reducer.REDUCE_PERCENTILE_99:
      return "REDUCE_PERCENTILE_99";
    case Aggregation_Reducer.REDUCE_PERCENTILE_95:
      return "REDUCE_PERCENTILE_95";
    case Aggregation_Reducer.REDUCE_PERCENTILE_50:
      return "REDUCE_PERCENTILE_50";
    case Aggregation_Reducer.REDUCE_PERCENTILE_05:
      return "REDUCE_PERCENTILE_05";
    case Aggregation_Reducer.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseTypedValue(): TypedValue {
  return {
    boolValue: undefined,
    int64Value: undefined,
    doubleValue: undefined,
    stringValue: undefined,
    distributionValue: undefined,
  };
}

export const TypedValue: MessageFns<TypedValue> = {
  encode(message: TypedValue, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.boolValue !== undefined) {
      writer.uint32(8).bool(message.boolValue);
    }
    if (message.int64Value !== undefined) {
      writer.uint32(16).int64(message.int64Value.toString());
    }
    if (message.doubleValue !== undefined) {
      writer.uint32(25).double(message.doubleValue);
    }
    if (message.stringValue !== undefined) {
      writer.uint32(34).string(message.stringValue);
    }
    if (message.distributionValue !== undefined) {
      Distribution.encode(message.distributionValue, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TypedValue {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTypedValue();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.boolValue = reader.bool();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.int64Value = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 25) {
            break;
          }

          message.doubleValue = reader.double();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.stringValue = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.distributionValue = Distribution.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TypedValue {
    return {
      boolValue: isSet(object.boolValue) ? globalThis.Boolean(object.boolValue) : undefined,
      int64Value: isSet(object.int64Value) ? Long.fromValue(object.int64Value) : undefined,
      doubleValue: isSet(object.doubleValue) ? globalThis.Number(object.doubleValue) : undefined,
      stringValue: isSet(object.stringValue) ? globalThis.String(object.stringValue) : undefined,
      distributionValue: isSet(object.distributionValue) ? Distribution.fromJSON(object.distributionValue) : undefined,
    };
  },

  toJSON(message: TypedValue): unknown {
    const obj: any = {};
    if (message.boolValue !== undefined) {
      obj.boolValue = message.boolValue;
    }
    if (message.int64Value !== undefined) {
      obj.int64Value = (message.int64Value || Long.ZERO).toString();
    }
    if (message.doubleValue !== undefined) {
      obj.doubleValue = message.doubleValue;
    }
    if (message.stringValue !== undefined) {
      obj.stringValue = message.stringValue;
    }
    if (message.distributionValue !== undefined) {
      obj.distributionValue = Distribution.toJSON(message.distributionValue);
    }
    return obj;
  },

  create(base?: DeepPartial<TypedValue>): TypedValue {
    return TypedValue.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TypedValue>): TypedValue {
    const message = createBaseTypedValue();
    message.boolValue = object.boolValue ?? undefined;
    message.int64Value = (object.int64Value !== undefined && object.int64Value !== null)
      ? Long.fromValue(object.int64Value)
      : undefined;
    message.doubleValue = object.doubleValue ?? undefined;
    message.stringValue = object.stringValue ?? undefined;
    message.distributionValue = (object.distributionValue !== undefined && object.distributionValue !== null)
      ? Distribution.fromPartial(object.distributionValue)
      : undefined;
    return message;
  },
};

function createBaseTimeInterval(): TimeInterval {
  return { endTime: undefined, startTime: undefined };
}

export const TimeInterval: MessageFns<TimeInterval> = {
  encode(message: TimeInterval, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(18).fork()).join();
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TimeInterval {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTimeInterval();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TimeInterval {
    return {
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
    };
  },

  toJSON(message: TimeInterval): unknown {
    const obj: any = {};
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<TimeInterval>): TimeInterval {
    return TimeInterval.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TimeInterval>): TimeInterval {
    const message = createBaseTimeInterval();
    message.endTime = object.endTime ?? undefined;
    message.startTime = object.startTime ?? undefined;
    return message;
  },
};

function createBaseAggregation(): Aggregation {
  return { alignmentPeriod: undefined, perSeriesAligner: 0, crossSeriesReducer: 0, groupByFields: [] };
}

export const Aggregation: MessageFns<Aggregation> = {
  encode(message: Aggregation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.alignmentPeriod !== undefined) {
      Duration.encode(message.alignmentPeriod, writer.uint32(10).fork()).join();
    }
    if (message.perSeriesAligner !== 0) {
      writer.uint32(16).int32(message.perSeriesAligner);
    }
    if (message.crossSeriesReducer !== 0) {
      writer.uint32(32).int32(message.crossSeriesReducer);
    }
    for (const v of message.groupByFields) {
      writer.uint32(42).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Aggregation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAggregation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.alignmentPeriod = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.perSeriesAligner = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.crossSeriesReducer = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.groupByFields.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Aggregation {
    return {
      alignmentPeriod: isSet(object.alignmentPeriod) ? Duration.fromJSON(object.alignmentPeriod) : undefined,
      perSeriesAligner: isSet(object.perSeriesAligner) ? aggregation_AlignerFromJSON(object.perSeriesAligner) : 0,
      crossSeriesReducer: isSet(object.crossSeriesReducer) ? aggregation_ReducerFromJSON(object.crossSeriesReducer) : 0,
      groupByFields: globalThis.Array.isArray(object?.groupByFields)
        ? object.groupByFields.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: Aggregation): unknown {
    const obj: any = {};
    if (message.alignmentPeriod !== undefined) {
      obj.alignmentPeriod = Duration.toJSON(message.alignmentPeriod);
    }
    if (message.perSeriesAligner !== 0) {
      obj.perSeriesAligner = aggregation_AlignerToJSON(message.perSeriesAligner);
    }
    if (message.crossSeriesReducer !== 0) {
      obj.crossSeriesReducer = aggregation_ReducerToJSON(message.crossSeriesReducer);
    }
    if (message.groupByFields?.length) {
      obj.groupByFields = message.groupByFields;
    }
    return obj;
  },

  create(base?: DeepPartial<Aggregation>): Aggregation {
    return Aggregation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Aggregation>): Aggregation {
    const message = createBaseAggregation();
    message.alignmentPeriod = (object.alignmentPeriod !== undefined && object.alignmentPeriod !== null)
      ? Duration.fromPartial(object.alignmentPeriod)
      : undefined;
    message.perSeriesAligner = object.perSeriesAligner ?? 0;
    message.crossSeriesReducer = object.crossSeriesReducer ?? 0;
    message.groupByFields = object.groupByFields?.map((e) => e) || [];
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
