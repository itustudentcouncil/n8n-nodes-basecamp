// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/monitoring/dashboard/v1/common.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Duration } from "../../../protobuf/duration.js";
import { Interval } from "../../../type/interval.js";

export const protobufPackage = "google.monitoring.dashboard.v1";

/**
 * Describes how to combine multiple time series to provide a different view of
 * the data.  Aggregation of time series is done in two steps. First, each time
 * series in the set is _aligned_ to the same time interval boundaries, then the
 * set of time series is optionally _reduced_ in number.
 *
 * Alignment consists of applying the `per_series_aligner` operation
 * to each time series after its data has been divided into regular
 * `alignment_period` time intervals. This process takes _all_ of the data
 * points in an alignment period, applies a mathematical transformation such as
 * averaging, minimum, maximum, delta, etc., and converts them into a single
 * data point per period.
 *
 * Reduction is when the aligned and transformed time series can optionally be
 * combined, reducing the number of time series through similar mathematical
 * transformations. Reduction involves applying a `cross_series_reducer` to
 * all the time series, optionally sorting the time series into subsets with
 * `group_by_fields`, and applying the reducer to each subset.
 *
 * The raw time series data can contain a huge amount of information from
 * multiple sources. Alignment and reduction transforms this mass of data into
 * a more manageable and representative collection of data, for example "the
 * 95% latency across the average of all tasks in a cluster". This
 * representative data can be more easily graphed and comprehended, and the
 * individual time series data is still available for later drilldown. For more
 * details, see [Filtering and
 * aggregation](https://cloud.google.com/monitoring/api/v3/aggregation).
 */
export interface Aggregation {
  /**
   * The `alignment_period` specifies a time interval, in seconds, that is used
   * to divide the data in all the
   * [time series][google.monitoring.v3.TimeSeries] into consistent blocks of
   * time. This will be done before the per-series aligner can be applied to
   * the data.
   *
   * The value must be at least 60 seconds. If a per-series aligner other than
   * `ALIGN_NONE` is specified, this field is required or an error is returned.
   * If no per-series aligner is specified, or the aligner `ALIGN_NONE` is
   * specified, then this field is ignored.
   *
   * The maximum value of the `alignment_period` is 2 years, or 104 weeks.
   */
  alignmentPeriod:
    | Duration
    | undefined;
  /**
   * An `Aligner` describes how to bring the data points in a single
   * time series into temporal alignment. Except for `ALIGN_NONE`, all
   * alignments cause all the data points in an `alignment_period` to be
   * mathematically grouped together, resulting in a single data point for
   * each `alignment_period` with end timestamp at the end of the period.
   *
   * Not all alignment operations may be applied to all time series. The valid
   * choices depend on the `metric_kind` and `value_type` of the original time
   * series. Alignment can change the `metric_kind` or the `value_type` of
   * the time series.
   *
   * Time series data must be aligned in order to perform cross-time
   * series reduction. If `cross_series_reducer` is specified, then
   * `per_series_aligner` must be specified and not equal to `ALIGN_NONE`
   * and `alignment_period` must be specified; otherwise, an error is
   * returned.
   */
  perSeriesAligner: Aggregation_Aligner;
  /**
   * The reduction operation to be used to combine time series into a single
   * time series, where the value of each data point in the resulting series is
   * a function of all the already aligned values in the input time series.
   *
   * Not all reducer operations can be applied to all time series. The valid
   * choices depend on the `metric_kind` and the `value_type` of the original
   * time series. Reduction can yield a time series with a different
   * `metric_kind` or `value_type` than the input time series.
   *
   * Time series data must first be aligned (see `per_series_aligner`) in order
   * to perform cross-time series reduction. If `cross_series_reducer` is
   * specified, then `per_series_aligner` must be specified, and must not be
   * `ALIGN_NONE`. An `alignment_period` must also be specified; otherwise, an
   * error is returned.
   */
  crossSeriesReducer: Aggregation_Reducer;
  /**
   * The set of fields to preserve when `cross_series_reducer` is
   * specified. The `group_by_fields` determine how the time series are
   * partitioned into subsets prior to applying the aggregation
   * operation. Each subset contains time series that have the same
   * value for each of the grouping fields. Each individual time
   * series is a member of exactly one subset. The
   * `cross_series_reducer` is applied to each subset of time series.
   * It is not possible to reduce across different resource types, so
   * this field implicitly contains `resource.type`.  Fields not
   * specified in `group_by_fields` are aggregated away.  If
   * `group_by_fields` is not specified and all the time series have
   * the same resource type, then the time series are aggregated into
   * a single output time series. If `cross_series_reducer` is not
   * defined, this field is ignored.
   */
  groupByFields: string[];
}

/**
 * The `Aligner` specifies the operation that will be applied to the data
 * points in each alignment period in a time series. Except for
 * `ALIGN_NONE`, which specifies that no operation be applied, each alignment
 * operation replaces the set of data values in each alignment period with
 * a single value: the result of applying the operation to the data values.
 * An aligned time series has a single data value at the end of each
 * `alignment_period`.
 *
 * An alignment operation can change the data type of the values, too. For
 * example, if you apply a counting operation to boolean values, the data
 * `value_type` in the original time series is `BOOLEAN`, but the `value_type`
 * in the aligned result is `INT64`.
 */
export enum Aggregation_Aligner {
  /**
   * ALIGN_NONE - No alignment. Raw data is returned. Not valid if cross-series reduction
   * is requested. The `value_type` of the result is the same as the
   * `value_type` of the input.
   */
  ALIGN_NONE = 0,
  /**
   * ALIGN_DELTA - Align and convert to
   * [DELTA][google.api.MetricDescriptor.MetricKind.DELTA].
   * The output is `delta = y1 - y0`.
   *
   * This alignment is valid for
   * [CUMULATIVE][google.api.MetricDescriptor.MetricKind.CUMULATIVE] and
   * `DELTA` metrics. If the selected alignment period results in periods
   * with no data, then the aligned value for such a period is created by
   * interpolation. The `value_type`  of the aligned result is the same as
   * the `value_type` of the input.
   */
  ALIGN_DELTA = 1,
  /**
   * ALIGN_RATE - Align and convert to a rate. The result is computed as
   * `rate = (y1 - y0)/(t1 - t0)`, or "delta over time".
   * Think of this aligner as providing the slope of the line that passes
   * through the value at the start and at the end of the `alignment_period`.
   *
   * This aligner is valid for `CUMULATIVE`
   * and `DELTA` metrics with numeric values. If the selected alignment
   * period results in periods with no data, then the aligned value for
   * such a period is created by interpolation. The output is a `GAUGE`
   * metric with `value_type` `DOUBLE`.
   *
   * If, by "rate", you mean "percentage change", see the
   * `ALIGN_PERCENT_CHANGE` aligner instead.
   */
  ALIGN_RATE = 2,
  /**
   * ALIGN_INTERPOLATE - Align by interpolating between adjacent points around the alignment
   * period boundary. This aligner is valid for `GAUGE` metrics with
   * numeric values. The `value_type` of the aligned result is the same as the
   * `value_type` of the input.
   */
  ALIGN_INTERPOLATE = 3,
  /**
   * ALIGN_NEXT_OLDER - Align by moving the most recent data point before the end of the
   * alignment period to the boundary at the end of the alignment
   * period. This aligner is valid for `GAUGE` metrics. The `value_type` of
   * the aligned result is the same as the `value_type` of the input.
   */
  ALIGN_NEXT_OLDER = 4,
  /**
   * ALIGN_MIN - Align the time series by returning the minimum value in each alignment
   * period. This aligner is valid for `GAUGE` and `DELTA` metrics with
   * numeric values. The `value_type` of the aligned result is the same as
   * the `value_type` of the input.
   */
  ALIGN_MIN = 10,
  /**
   * ALIGN_MAX - Align the time series by returning the maximum value in each alignment
   * period. This aligner is valid for `GAUGE` and `DELTA` metrics with
   * numeric values. The `value_type` of the aligned result is the same as
   * the `value_type` of the input.
   */
  ALIGN_MAX = 11,
  /**
   * ALIGN_MEAN - Align the time series by returning the mean value in each alignment
   * period. This aligner is valid for `GAUGE` and `DELTA` metrics with
   * numeric values. The `value_type` of the aligned result is `DOUBLE`.
   */
  ALIGN_MEAN = 12,
  /**
   * ALIGN_COUNT - Align the time series by returning the number of values in each alignment
   * period. This aligner is valid for `GAUGE` and `DELTA` metrics with
   * numeric or Boolean values. The `value_type` of the aligned result is
   * `INT64`.
   */
  ALIGN_COUNT = 13,
  /**
   * ALIGN_SUM - Align the time series by returning the sum of the values in each
   * alignment period. This aligner is valid for `GAUGE` and `DELTA`
   * metrics with numeric and distribution values. The `value_type` of the
   * aligned result is the same as the `value_type` of the input.
   */
  ALIGN_SUM = 14,
  /**
   * ALIGN_STDDEV - Align the time series by returning the standard deviation of the values
   * in each alignment period. This aligner is valid for `GAUGE` and
   * `DELTA` metrics with numeric values. The `value_type` of the output is
   * `DOUBLE`.
   */
  ALIGN_STDDEV = 15,
  /**
   * ALIGN_COUNT_TRUE - Align the time series by returning the number of `True` values in
   * each alignment period. This aligner is valid for `GAUGE` metrics with
   * Boolean values. The `value_type` of the output is `INT64`.
   */
  ALIGN_COUNT_TRUE = 16,
  /**
   * ALIGN_COUNT_FALSE - Align the time series by returning the number of `False` values in
   * each alignment period. This aligner is valid for `GAUGE` metrics with
   * Boolean values. The `value_type` of the output is `INT64`.
   */
  ALIGN_COUNT_FALSE = 24,
  /**
   * ALIGN_FRACTION_TRUE - Align the time series by returning the ratio of the number of `True`
   * values to the total number of values in each alignment period. This
   * aligner is valid for `GAUGE` metrics with Boolean values. The output
   * value is in the range [0.0, 1.0] and has `value_type` `DOUBLE`.
   */
  ALIGN_FRACTION_TRUE = 17,
  /**
   * ALIGN_PERCENTILE_99 - Align the time series by using [percentile
   * aggregation](https://en.wikipedia.org/wiki/Percentile). The resulting
   * data point in each alignment period is the 99th percentile of all data
   * points in the period. This aligner is valid for `GAUGE` and `DELTA`
   * metrics with distribution values. The output is a `GAUGE` metric with
   * `value_type` `DOUBLE`.
   */
  ALIGN_PERCENTILE_99 = 18,
  /**
   * ALIGN_PERCENTILE_95 - Align the time series by using [percentile
   * aggregation](https://en.wikipedia.org/wiki/Percentile). The resulting
   * data point in each alignment period is the 95th percentile of all data
   * points in the period. This aligner is valid for `GAUGE` and `DELTA`
   * metrics with distribution values. The output is a `GAUGE` metric with
   * `value_type` `DOUBLE`.
   */
  ALIGN_PERCENTILE_95 = 19,
  /**
   * ALIGN_PERCENTILE_50 - Align the time series by using [percentile
   * aggregation](https://en.wikipedia.org/wiki/Percentile). The resulting
   * data point in each alignment period is the 50th percentile of all data
   * points in the period. This aligner is valid for `GAUGE` and `DELTA`
   * metrics with distribution values. The output is a `GAUGE` metric with
   * `value_type` `DOUBLE`.
   */
  ALIGN_PERCENTILE_50 = 20,
  /**
   * ALIGN_PERCENTILE_05 - Align the time series by using [percentile
   * aggregation](https://en.wikipedia.org/wiki/Percentile). The resulting
   * data point in each alignment period is the 5th percentile of all data
   * points in the period. This aligner is valid for `GAUGE` and `DELTA`
   * metrics with distribution values. The output is a `GAUGE` metric with
   * `value_type` `DOUBLE`.
   */
  ALIGN_PERCENTILE_05 = 21,
  /**
   * ALIGN_PERCENT_CHANGE - Align and convert to a percentage change. This aligner is valid for
   * `GAUGE` and `DELTA` metrics with numeric values. This alignment returns
   * `((current - previous)/previous) * 100`, where the value of `previous` is
   * determined based on the `alignment_period`.
   *
   * If the values of `current` and `previous` are both 0, then the returned
   * value is 0. If only `previous` is 0, the returned value is infinity.
   *
   * A 10-minute moving mean is computed at each point of the alignment period
   * prior to the above calculation to smooth the metric and prevent false
   * positives from very short-lived spikes. The moving mean is only
   * applicable for data whose values are `>= 0`. Any values `< 0` are
   * treated as a missing datapoint, and are ignored. While `DELTA`
   * metrics are accepted by this alignment, special care should be taken that
   * the values for the metric will always be positive. The output is a
   * `GAUGE` metric with `value_type` `DOUBLE`.
   */
  ALIGN_PERCENT_CHANGE = 23,
  UNRECOGNIZED = -1,
}

export function aggregation_AlignerFromJSON(object: any): Aggregation_Aligner {
  switch (object) {
    case 0:
    case "ALIGN_NONE":
      return Aggregation_Aligner.ALIGN_NONE;
    case 1:
    case "ALIGN_DELTA":
      return Aggregation_Aligner.ALIGN_DELTA;
    case 2:
    case "ALIGN_RATE":
      return Aggregation_Aligner.ALIGN_RATE;
    case 3:
    case "ALIGN_INTERPOLATE":
      return Aggregation_Aligner.ALIGN_INTERPOLATE;
    case 4:
    case "ALIGN_NEXT_OLDER":
      return Aggregation_Aligner.ALIGN_NEXT_OLDER;
    case 10:
    case "ALIGN_MIN":
      return Aggregation_Aligner.ALIGN_MIN;
    case 11:
    case "ALIGN_MAX":
      return Aggregation_Aligner.ALIGN_MAX;
    case 12:
    case "ALIGN_MEAN":
      return Aggregation_Aligner.ALIGN_MEAN;
    case 13:
    case "ALIGN_COUNT":
      return Aggregation_Aligner.ALIGN_COUNT;
    case 14:
    case "ALIGN_SUM":
      return Aggregation_Aligner.ALIGN_SUM;
    case 15:
    case "ALIGN_STDDEV":
      return Aggregation_Aligner.ALIGN_STDDEV;
    case 16:
    case "ALIGN_COUNT_TRUE":
      return Aggregation_Aligner.ALIGN_COUNT_TRUE;
    case 24:
    case "ALIGN_COUNT_FALSE":
      return Aggregation_Aligner.ALIGN_COUNT_FALSE;
    case 17:
    case "ALIGN_FRACTION_TRUE":
      return Aggregation_Aligner.ALIGN_FRACTION_TRUE;
    case 18:
    case "ALIGN_PERCENTILE_99":
      return Aggregation_Aligner.ALIGN_PERCENTILE_99;
    case 19:
    case "ALIGN_PERCENTILE_95":
      return Aggregation_Aligner.ALIGN_PERCENTILE_95;
    case 20:
    case "ALIGN_PERCENTILE_50":
      return Aggregation_Aligner.ALIGN_PERCENTILE_50;
    case 21:
    case "ALIGN_PERCENTILE_05":
      return Aggregation_Aligner.ALIGN_PERCENTILE_05;
    case 23:
    case "ALIGN_PERCENT_CHANGE":
      return Aggregation_Aligner.ALIGN_PERCENT_CHANGE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Aggregation_Aligner.UNRECOGNIZED;
  }
}

export function aggregation_AlignerToJSON(object: Aggregation_Aligner): string {
  switch (object) {
    case Aggregation_Aligner.ALIGN_NONE:
      return "ALIGN_NONE";
    case Aggregation_Aligner.ALIGN_DELTA:
      return "ALIGN_DELTA";
    case Aggregation_Aligner.ALIGN_RATE:
      return "ALIGN_RATE";
    case Aggregation_Aligner.ALIGN_INTERPOLATE:
      return "ALIGN_INTERPOLATE";
    case Aggregation_Aligner.ALIGN_NEXT_OLDER:
      return "ALIGN_NEXT_OLDER";
    case Aggregation_Aligner.ALIGN_MIN:
      return "ALIGN_MIN";
    case Aggregation_Aligner.ALIGN_MAX:
      return "ALIGN_MAX";
    case Aggregation_Aligner.ALIGN_MEAN:
      return "ALIGN_MEAN";
    case Aggregation_Aligner.ALIGN_COUNT:
      return "ALIGN_COUNT";
    case Aggregation_Aligner.ALIGN_SUM:
      return "ALIGN_SUM";
    case Aggregation_Aligner.ALIGN_STDDEV:
      return "ALIGN_STDDEV";
    case Aggregation_Aligner.ALIGN_COUNT_TRUE:
      return "ALIGN_COUNT_TRUE";
    case Aggregation_Aligner.ALIGN_COUNT_FALSE:
      return "ALIGN_COUNT_FALSE";
    case Aggregation_Aligner.ALIGN_FRACTION_TRUE:
      return "ALIGN_FRACTION_TRUE";
    case Aggregation_Aligner.ALIGN_PERCENTILE_99:
      return "ALIGN_PERCENTILE_99";
    case Aggregation_Aligner.ALIGN_PERCENTILE_95:
      return "ALIGN_PERCENTILE_95";
    case Aggregation_Aligner.ALIGN_PERCENTILE_50:
      return "ALIGN_PERCENTILE_50";
    case Aggregation_Aligner.ALIGN_PERCENTILE_05:
      return "ALIGN_PERCENTILE_05";
    case Aggregation_Aligner.ALIGN_PERCENT_CHANGE:
      return "ALIGN_PERCENT_CHANGE";
    case Aggregation_Aligner.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * A Reducer operation describes how to aggregate data points from multiple
 * time series into a single time series, where the value of each data point
 * in the resulting series is a function of all the already aligned values in
 * the input time series.
 */
export enum Aggregation_Reducer {
  /**
   * REDUCE_NONE - No cross-time series reduction. The output of the `Aligner` is
   * returned.
   */
  REDUCE_NONE = 0,
  /**
   * REDUCE_MEAN - Reduce by computing the mean value across time series for each
   * alignment period. This reducer is valid for
   * [DELTA][google.api.MetricDescriptor.MetricKind.DELTA] and
   * [GAUGE][google.api.MetricDescriptor.MetricKind.GAUGE] metrics with
   * numeric or distribution values. The `value_type` of the output is
   * [DOUBLE][google.api.MetricDescriptor.ValueType.DOUBLE].
   */
  REDUCE_MEAN = 1,
  /**
   * REDUCE_MIN - Reduce by computing the minimum value across time series for each
   * alignment period. This reducer is valid for `DELTA` and `GAUGE` metrics
   * with numeric values. The `value_type` of the output is the same as the
   * `value_type` of the input.
   */
  REDUCE_MIN = 2,
  /**
   * REDUCE_MAX - Reduce by computing the maximum value across time series for each
   * alignment period. This reducer is valid for `DELTA` and `GAUGE` metrics
   * with numeric values. The `value_type` of the output is the same as the
   * `value_type` of the input.
   */
  REDUCE_MAX = 3,
  /**
   * REDUCE_SUM - Reduce by computing the sum across time series for each
   * alignment period. This reducer is valid for `DELTA` and `GAUGE` metrics
   * with numeric and distribution values. The `value_type` of the output is
   * the same as the `value_type` of the input.
   */
  REDUCE_SUM = 4,
  /**
   * REDUCE_STDDEV - Reduce by computing the standard deviation across time series
   * for each alignment period. This reducer is valid for `DELTA` and
   * `GAUGE` metrics with numeric or distribution values. The `value_type`
   * of the output is `DOUBLE`.
   */
  REDUCE_STDDEV = 5,
  /**
   * REDUCE_COUNT - Reduce by computing the number of data points across time series
   * for each alignment period. This reducer is valid for `DELTA` and
   * `GAUGE` metrics of numeric, Boolean, distribution, and string
   * `value_type`. The `value_type` of the output is `INT64`.
   */
  REDUCE_COUNT = 6,
  /**
   * REDUCE_COUNT_TRUE - Reduce by computing the number of `True`-valued data points across time
   * series for each alignment period. This reducer is valid for `DELTA` and
   * `GAUGE` metrics of Boolean `value_type`. The `value_type` of the output
   * is `INT64`.
   */
  REDUCE_COUNT_TRUE = 7,
  /**
   * REDUCE_COUNT_FALSE - Reduce by computing the number of `False`-valued data points across time
   * series for each alignment period. This reducer is valid for `DELTA` and
   * `GAUGE` metrics of Boolean `value_type`. The `value_type` of the output
   * is `INT64`.
   */
  REDUCE_COUNT_FALSE = 15,
  /**
   * REDUCE_FRACTION_TRUE - Reduce by computing the ratio of the number of `True`-valued data points
   * to the total number of data points for each alignment period. This
   * reducer is valid for `DELTA` and `GAUGE` metrics of Boolean `value_type`.
   * The output value is in the range [0.0, 1.0] and has `value_type`
   * `DOUBLE`.
   */
  REDUCE_FRACTION_TRUE = 8,
  /**
   * REDUCE_PERCENTILE_99 - Reduce by computing the [99th
   * percentile](https://en.wikipedia.org/wiki/Percentile) of data points
   * across time series for each alignment period. This reducer is valid for
   * `GAUGE` and `DELTA` metrics of numeric and distribution type. The value
   * of the output is `DOUBLE`.
   */
  REDUCE_PERCENTILE_99 = 9,
  /**
   * REDUCE_PERCENTILE_95 - Reduce by computing the [95th
   * percentile](https://en.wikipedia.org/wiki/Percentile) of data points
   * across time series for each alignment period. This reducer is valid for
   * `GAUGE` and `DELTA` metrics of numeric and distribution type. The value
   * of the output is `DOUBLE`.
   */
  REDUCE_PERCENTILE_95 = 10,
  /**
   * REDUCE_PERCENTILE_50 - Reduce by computing the [50th
   * percentile](https://en.wikipedia.org/wiki/Percentile) of data points
   * across time series for each alignment period. This reducer is valid for
   * `GAUGE` and `DELTA` metrics of numeric and distribution type. The value
   * of the output is `DOUBLE`.
   */
  REDUCE_PERCENTILE_50 = 11,
  /**
   * REDUCE_PERCENTILE_05 - Reduce by computing the [5th
   * percentile](https://en.wikipedia.org/wiki/Percentile) of data points
   * across time series for each alignment period. This reducer is valid for
   * `GAUGE` and `DELTA` metrics of numeric and distribution type. The value
   * of the output is `DOUBLE`.
   */
  REDUCE_PERCENTILE_05 = 12,
  UNRECOGNIZED = -1,
}

export function aggregation_ReducerFromJSON(object: any): Aggregation_Reducer {
  switch (object) {
    case 0:
    case "REDUCE_NONE":
      return Aggregation_Reducer.REDUCE_NONE;
    case 1:
    case "REDUCE_MEAN":
      return Aggregation_Reducer.REDUCE_MEAN;
    case 2:
    case "REDUCE_MIN":
      return Aggregation_Reducer.REDUCE_MIN;
    case 3:
    case "REDUCE_MAX":
      return Aggregation_Reducer.REDUCE_MAX;
    case 4:
    case "REDUCE_SUM":
      return Aggregation_Reducer.REDUCE_SUM;
    case 5:
    case "REDUCE_STDDEV":
      return Aggregation_Reducer.REDUCE_STDDEV;
    case 6:
    case "REDUCE_COUNT":
      return Aggregation_Reducer.REDUCE_COUNT;
    case 7:
    case "REDUCE_COUNT_TRUE":
      return Aggregation_Reducer.REDUCE_COUNT_TRUE;
    case 15:
    case "REDUCE_COUNT_FALSE":
      return Aggregation_Reducer.REDUCE_COUNT_FALSE;
    case 8:
    case "REDUCE_FRACTION_TRUE":
      return Aggregation_Reducer.REDUCE_FRACTION_TRUE;
    case 9:
    case "REDUCE_PERCENTILE_99":
      return Aggregation_Reducer.REDUCE_PERCENTILE_99;
    case 10:
    case "REDUCE_PERCENTILE_95":
      return Aggregation_Reducer.REDUCE_PERCENTILE_95;
    case 11:
    case "REDUCE_PERCENTILE_50":
      return Aggregation_Reducer.REDUCE_PERCENTILE_50;
    case 12:
    case "REDUCE_PERCENTILE_05":
      return Aggregation_Reducer.REDUCE_PERCENTILE_05;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Aggregation_Reducer.UNRECOGNIZED;
  }
}

export function aggregation_ReducerToJSON(object: Aggregation_Reducer): string {
  switch (object) {
    case Aggregation_Reducer.REDUCE_NONE:
      return "REDUCE_NONE";
    case Aggregation_Reducer.REDUCE_MEAN:
      return "REDUCE_MEAN";
    case Aggregation_Reducer.REDUCE_MIN:
      return "REDUCE_MIN";
    case Aggregation_Reducer.REDUCE_MAX:
      return "REDUCE_MAX";
    case Aggregation_Reducer.REDUCE_SUM:
      return "REDUCE_SUM";
    case Aggregation_Reducer.REDUCE_STDDEV:
      return "REDUCE_STDDEV";
    case Aggregation_Reducer.REDUCE_COUNT:
      return "REDUCE_COUNT";
    case Aggregation_Reducer.REDUCE_COUNT_TRUE:
      return "REDUCE_COUNT_TRUE";
    case Aggregation_Reducer.REDUCE_COUNT_FALSE:
      return "REDUCE_COUNT_FALSE";
    case Aggregation_Reducer.REDUCE_FRACTION_TRUE:
      return "REDUCE_FRACTION_TRUE";
    case Aggregation_Reducer.REDUCE_PERCENTILE_99:
      return "REDUCE_PERCENTILE_99";
    case Aggregation_Reducer.REDUCE_PERCENTILE_95:
      return "REDUCE_PERCENTILE_95";
    case Aggregation_Reducer.REDUCE_PERCENTILE_50:
      return "REDUCE_PERCENTILE_50";
    case Aggregation_Reducer.REDUCE_PERCENTILE_05:
      return "REDUCE_PERCENTILE_05";
    case Aggregation_Reducer.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Describes a ranking-based time series filter. Each input time series is
 * ranked with an aligner. The filter will allow up to `num_time_series` time
 * series to pass through it, selecting them based on the relative ranking.
 *
 * For example, if `ranking_method` is `METHOD_MEAN`,`direction` is `BOTTOM`,
 * and `num_time_series` is 3, then the 3 times series with the lowest mean
 * values will pass through the filter.
 */
export interface PickTimeSeriesFilter {
  /**
   * `ranking_method` is applied to each time series independently to produce
   * the value which will be used to compare the time series to other time
   * series.
   */
  rankingMethod: PickTimeSeriesFilter_Method;
  /** How many time series to allow to pass through the filter. */
  numTimeSeries: number;
  /** How to use the ranking to select time series that pass through the filter. */
  direction: PickTimeSeriesFilter_Direction;
  /** Select the top N streams/time series within this time interval */
  interval: Interval | undefined;
}

/** The value reducers that can be applied to a `PickTimeSeriesFilter`. */
export enum PickTimeSeriesFilter_Method {
  /**
   * METHOD_UNSPECIFIED - Not allowed. You must specify a different `Method` if you specify a
   * `PickTimeSeriesFilter`.
   */
  METHOD_UNSPECIFIED = 0,
  /** METHOD_MEAN - Select the mean of all values. */
  METHOD_MEAN = 1,
  /** METHOD_MAX - Select the maximum value. */
  METHOD_MAX = 2,
  /** METHOD_MIN - Select the minimum value. */
  METHOD_MIN = 3,
  /** METHOD_SUM - Compute the sum of all values. */
  METHOD_SUM = 4,
  /** METHOD_LATEST - Select the most recent value. */
  METHOD_LATEST = 5,
  UNRECOGNIZED = -1,
}

export function pickTimeSeriesFilter_MethodFromJSON(object: any): PickTimeSeriesFilter_Method {
  switch (object) {
    case 0:
    case "METHOD_UNSPECIFIED":
      return PickTimeSeriesFilter_Method.METHOD_UNSPECIFIED;
    case 1:
    case "METHOD_MEAN":
      return PickTimeSeriesFilter_Method.METHOD_MEAN;
    case 2:
    case "METHOD_MAX":
      return PickTimeSeriesFilter_Method.METHOD_MAX;
    case 3:
    case "METHOD_MIN":
      return PickTimeSeriesFilter_Method.METHOD_MIN;
    case 4:
    case "METHOD_SUM":
      return PickTimeSeriesFilter_Method.METHOD_SUM;
    case 5:
    case "METHOD_LATEST":
      return PickTimeSeriesFilter_Method.METHOD_LATEST;
    case -1:
    case "UNRECOGNIZED":
    default:
      return PickTimeSeriesFilter_Method.UNRECOGNIZED;
  }
}

export function pickTimeSeriesFilter_MethodToJSON(object: PickTimeSeriesFilter_Method): string {
  switch (object) {
    case PickTimeSeriesFilter_Method.METHOD_UNSPECIFIED:
      return "METHOD_UNSPECIFIED";
    case PickTimeSeriesFilter_Method.METHOD_MEAN:
      return "METHOD_MEAN";
    case PickTimeSeriesFilter_Method.METHOD_MAX:
      return "METHOD_MAX";
    case PickTimeSeriesFilter_Method.METHOD_MIN:
      return "METHOD_MIN";
    case PickTimeSeriesFilter_Method.METHOD_SUM:
      return "METHOD_SUM";
    case PickTimeSeriesFilter_Method.METHOD_LATEST:
      return "METHOD_LATEST";
    case PickTimeSeriesFilter_Method.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Describes the ranking directions. */
export enum PickTimeSeriesFilter_Direction {
  /**
   * DIRECTION_UNSPECIFIED - Not allowed. You must specify a different `Direction` if you specify a
   * `PickTimeSeriesFilter`.
   */
  DIRECTION_UNSPECIFIED = 0,
  /** TOP - Pass the highest `num_time_series` ranking inputs. */
  TOP = 1,
  /** BOTTOM - Pass the lowest `num_time_series` ranking inputs. */
  BOTTOM = 2,
  UNRECOGNIZED = -1,
}

export function pickTimeSeriesFilter_DirectionFromJSON(object: any): PickTimeSeriesFilter_Direction {
  switch (object) {
    case 0:
    case "DIRECTION_UNSPECIFIED":
      return PickTimeSeriesFilter_Direction.DIRECTION_UNSPECIFIED;
    case 1:
    case "TOP":
      return PickTimeSeriesFilter_Direction.TOP;
    case 2:
    case "BOTTOM":
      return PickTimeSeriesFilter_Direction.BOTTOM;
    case -1:
    case "UNRECOGNIZED":
    default:
      return PickTimeSeriesFilter_Direction.UNRECOGNIZED;
  }
}

export function pickTimeSeriesFilter_DirectionToJSON(object: PickTimeSeriesFilter_Direction): string {
  switch (object) {
    case PickTimeSeriesFilter_Direction.DIRECTION_UNSPECIFIED:
      return "DIRECTION_UNSPECIFIED";
    case PickTimeSeriesFilter_Direction.TOP:
      return "TOP";
    case PickTimeSeriesFilter_Direction.BOTTOM:
      return "BOTTOM";
    case PickTimeSeriesFilter_Direction.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * A filter that ranks streams based on their statistical relation to other
 * streams in a request.
 * Note: This field is deprecated and completely ignored by the API.
 */
export interface StatisticalTimeSeriesFilter {
  /**
   * `rankingMethod` is applied to a set of time series, and then the produced
   * value for each individual time series is used to compare a given time
   * series to others.
   * These are methods that cannot be applied stream-by-stream, but rather
   * require the full context of a request to evaluate time series.
   */
  rankingMethod: StatisticalTimeSeriesFilter_Method;
  /** How many time series to output. */
  numTimeSeries: number;
}

/** The filter methods that can be applied to a stream. */
export enum StatisticalTimeSeriesFilter_Method {
  /** METHOD_UNSPECIFIED - Not allowed in well-formed requests. */
  METHOD_UNSPECIFIED = 0,
  /** METHOD_CLUSTER_OUTLIER - Compute the outlier score of each stream. */
  METHOD_CLUSTER_OUTLIER = 1,
  UNRECOGNIZED = -1,
}

export function statisticalTimeSeriesFilter_MethodFromJSON(object: any): StatisticalTimeSeriesFilter_Method {
  switch (object) {
    case 0:
    case "METHOD_UNSPECIFIED":
      return StatisticalTimeSeriesFilter_Method.METHOD_UNSPECIFIED;
    case 1:
    case "METHOD_CLUSTER_OUTLIER":
      return StatisticalTimeSeriesFilter_Method.METHOD_CLUSTER_OUTLIER;
    case -1:
    case "UNRECOGNIZED":
    default:
      return StatisticalTimeSeriesFilter_Method.UNRECOGNIZED;
  }
}

export function statisticalTimeSeriesFilter_MethodToJSON(object: StatisticalTimeSeriesFilter_Method): string {
  switch (object) {
    case StatisticalTimeSeriesFilter_Method.METHOD_UNSPECIFIED:
      return "METHOD_UNSPECIFIED";
    case StatisticalTimeSeriesFilter_Method.METHOD_CLUSTER_OUTLIER:
      return "METHOD_CLUSTER_OUTLIER";
    case StatisticalTimeSeriesFilter_Method.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseAggregation(): Aggregation {
  return { alignmentPeriod: undefined, perSeriesAligner: 0, crossSeriesReducer: 0, groupByFields: [] };
}

export const Aggregation: MessageFns<Aggregation> = {
  encode(message: Aggregation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.alignmentPeriod !== undefined) {
      Duration.encode(message.alignmentPeriod, writer.uint32(10).fork()).join();
    }
    if (message.perSeriesAligner !== 0) {
      writer.uint32(16).int32(message.perSeriesAligner);
    }
    if (message.crossSeriesReducer !== 0) {
      writer.uint32(32).int32(message.crossSeriesReducer);
    }
    for (const v of message.groupByFields) {
      writer.uint32(42).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Aggregation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAggregation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.alignmentPeriod = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.perSeriesAligner = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.crossSeriesReducer = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.groupByFields.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Aggregation {
    return {
      alignmentPeriod: isSet(object.alignmentPeriod) ? Duration.fromJSON(object.alignmentPeriod) : undefined,
      perSeriesAligner: isSet(object.perSeriesAligner) ? aggregation_AlignerFromJSON(object.perSeriesAligner) : 0,
      crossSeriesReducer: isSet(object.crossSeriesReducer) ? aggregation_ReducerFromJSON(object.crossSeriesReducer) : 0,
      groupByFields: globalThis.Array.isArray(object?.groupByFields)
        ? object.groupByFields.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: Aggregation): unknown {
    const obj: any = {};
    if (message.alignmentPeriod !== undefined) {
      obj.alignmentPeriod = Duration.toJSON(message.alignmentPeriod);
    }
    if (message.perSeriesAligner !== 0) {
      obj.perSeriesAligner = aggregation_AlignerToJSON(message.perSeriesAligner);
    }
    if (message.crossSeriesReducer !== 0) {
      obj.crossSeriesReducer = aggregation_ReducerToJSON(message.crossSeriesReducer);
    }
    if (message.groupByFields?.length) {
      obj.groupByFields = message.groupByFields;
    }
    return obj;
  },

  create(base?: DeepPartial<Aggregation>): Aggregation {
    return Aggregation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Aggregation>): Aggregation {
    const message = createBaseAggregation();
    message.alignmentPeriod = (object.alignmentPeriod !== undefined && object.alignmentPeriod !== null)
      ? Duration.fromPartial(object.alignmentPeriod)
      : undefined;
    message.perSeriesAligner = object.perSeriesAligner ?? 0;
    message.crossSeriesReducer = object.crossSeriesReducer ?? 0;
    message.groupByFields = object.groupByFields?.map((e) => e) || [];
    return message;
  },
};

function createBasePickTimeSeriesFilter(): PickTimeSeriesFilter {
  return { rankingMethod: 0, numTimeSeries: 0, direction: 0, interval: undefined };
}

export const PickTimeSeriesFilter: MessageFns<PickTimeSeriesFilter> = {
  encode(message: PickTimeSeriesFilter, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.rankingMethod !== 0) {
      writer.uint32(8).int32(message.rankingMethod);
    }
    if (message.numTimeSeries !== 0) {
      writer.uint32(16).int32(message.numTimeSeries);
    }
    if (message.direction !== 0) {
      writer.uint32(24).int32(message.direction);
    }
    if (message.interval !== undefined) {
      Interval.encode(message.interval, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PickTimeSeriesFilter {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePickTimeSeriesFilter();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.rankingMethod = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.numTimeSeries = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.direction = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.interval = Interval.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PickTimeSeriesFilter {
    return {
      rankingMethod: isSet(object.rankingMethod) ? pickTimeSeriesFilter_MethodFromJSON(object.rankingMethod) : 0,
      numTimeSeries: isSet(object.numTimeSeries) ? globalThis.Number(object.numTimeSeries) : 0,
      direction: isSet(object.direction) ? pickTimeSeriesFilter_DirectionFromJSON(object.direction) : 0,
      interval: isSet(object.interval) ? Interval.fromJSON(object.interval) : undefined,
    };
  },

  toJSON(message: PickTimeSeriesFilter): unknown {
    const obj: any = {};
    if (message.rankingMethod !== 0) {
      obj.rankingMethod = pickTimeSeriesFilter_MethodToJSON(message.rankingMethod);
    }
    if (message.numTimeSeries !== 0) {
      obj.numTimeSeries = Math.round(message.numTimeSeries);
    }
    if (message.direction !== 0) {
      obj.direction = pickTimeSeriesFilter_DirectionToJSON(message.direction);
    }
    if (message.interval !== undefined) {
      obj.interval = Interval.toJSON(message.interval);
    }
    return obj;
  },

  create(base?: DeepPartial<PickTimeSeriesFilter>): PickTimeSeriesFilter {
    return PickTimeSeriesFilter.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PickTimeSeriesFilter>): PickTimeSeriesFilter {
    const message = createBasePickTimeSeriesFilter();
    message.rankingMethod = object.rankingMethod ?? 0;
    message.numTimeSeries = object.numTimeSeries ?? 0;
    message.direction = object.direction ?? 0;
    message.interval = (object.interval !== undefined && object.interval !== null)
      ? Interval.fromPartial(object.interval)
      : undefined;
    return message;
  },
};

function createBaseStatisticalTimeSeriesFilter(): StatisticalTimeSeriesFilter {
  return { rankingMethod: 0, numTimeSeries: 0 };
}

export const StatisticalTimeSeriesFilter: MessageFns<StatisticalTimeSeriesFilter> = {
  encode(message: StatisticalTimeSeriesFilter, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.rankingMethod !== 0) {
      writer.uint32(8).int32(message.rankingMethod);
    }
    if (message.numTimeSeries !== 0) {
      writer.uint32(16).int32(message.numTimeSeries);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StatisticalTimeSeriesFilter {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStatisticalTimeSeriesFilter();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.rankingMethod = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.numTimeSeries = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StatisticalTimeSeriesFilter {
    return {
      rankingMethod: isSet(object.rankingMethod) ? statisticalTimeSeriesFilter_MethodFromJSON(object.rankingMethod) : 0,
      numTimeSeries: isSet(object.numTimeSeries) ? globalThis.Number(object.numTimeSeries) : 0,
    };
  },

  toJSON(message: StatisticalTimeSeriesFilter): unknown {
    const obj: any = {};
    if (message.rankingMethod !== 0) {
      obj.rankingMethod = statisticalTimeSeriesFilter_MethodToJSON(message.rankingMethod);
    }
    if (message.numTimeSeries !== 0) {
      obj.numTimeSeries = Math.round(message.numTimeSeries);
    }
    return obj;
  },

  create(base?: DeepPartial<StatisticalTimeSeriesFilter>): StatisticalTimeSeriesFilter {
    return StatisticalTimeSeriesFilter.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StatisticalTimeSeriesFilter>): StatisticalTimeSeriesFilter {
    const message = createBaseStatisticalTimeSeriesFilter();
    message.rankingMethod = object.rankingMethod ?? 0;
    message.numTimeSeries = object.numTimeSeries ?? 0;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
