// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/sql/v1beta4/cloud_sql_resources.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Duration } from "../../../protobuf/duration.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { BoolValue, Int32Value, Int64Value } from "../../../protobuf/wrappers.js";

export const protobufPackage = "google.cloud.sql.v1beta4";

export enum SqlFileType {
  /** SQL_FILE_TYPE_UNSPECIFIED - Unknown file type. */
  SQL_FILE_TYPE_UNSPECIFIED = 0,
  /** SQL - File containing SQL statements. */
  SQL = 1,
  /** CSV - File in CSV format. */
  CSV = 2,
  BAK = 4,
  UNRECOGNIZED = -1,
}

export function sqlFileTypeFromJSON(object: any): SqlFileType {
  switch (object) {
    case 0:
    case "SQL_FILE_TYPE_UNSPECIFIED":
      return SqlFileType.SQL_FILE_TYPE_UNSPECIFIED;
    case 1:
    case "SQL":
      return SqlFileType.SQL;
    case 2:
    case "CSV":
      return SqlFileType.CSV;
    case 4:
    case "BAK":
      return SqlFileType.BAK;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlFileType.UNRECOGNIZED;
  }
}

export function sqlFileTypeToJSON(object: SqlFileType): string {
  switch (object) {
    case SqlFileType.SQL_FILE_TYPE_UNSPECIFIED:
      return "SQL_FILE_TYPE_UNSPECIFIED";
    case SqlFileType.SQL:
      return "SQL";
    case SqlFileType.CSV:
      return "CSV";
    case SqlFileType.BAK:
      return "BAK";
    case SqlFileType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export enum BakType {
  /** BAK_TYPE_UNSPECIFIED - Default type. */
  BAK_TYPE_UNSPECIFIED = 0,
  /** FULL - Full backup. */
  FULL = 1,
  /** DIFF - Differential backup. */
  DIFF = 2,
  /** TLOG - SQL Server Transaction Log */
  TLOG = 3,
  UNRECOGNIZED = -1,
}

export function bakTypeFromJSON(object: any): BakType {
  switch (object) {
    case 0:
    case "BAK_TYPE_UNSPECIFIED":
      return BakType.BAK_TYPE_UNSPECIFIED;
    case 1:
    case "FULL":
      return BakType.FULL;
    case 2:
    case "DIFF":
      return BakType.DIFF;
    case 3:
    case "TLOG":
      return BakType.TLOG;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BakType.UNRECOGNIZED;
  }
}

export function bakTypeToJSON(object: BakType): string {
  switch (object) {
    case BakType.BAK_TYPE_UNSPECIFIED:
      return "BAK_TYPE_UNSPECIFIED";
    case BakType.FULL:
      return "FULL";
    case BakType.DIFF:
      return "DIFF";
    case BakType.TLOG:
      return "TLOG";
    case BakType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The status of a backup run. */
export enum SqlBackupRunStatus {
  /** SQL_BACKUP_RUN_STATUS_UNSPECIFIED - The status of the run is unknown. */
  SQL_BACKUP_RUN_STATUS_UNSPECIFIED = 0,
  /** ENQUEUED - The backup operation was enqueued. */
  ENQUEUED = 1,
  /**
   * OVERDUE - The backup is overdue across a given backup window. Indicates a
   * problem. Example: Long-running operation in progress during
   * the whole window.
   */
  OVERDUE = 2,
  /** RUNNING - The backup is in progress. */
  RUNNING = 3,
  /** FAILED - The backup failed. */
  FAILED = 4,
  /** SUCCESSFUL - The backup was successful. */
  SUCCESSFUL = 5,
  /**
   * SKIPPED - The backup was skipped (without problems) for a given backup
   * window. Example: Instance was idle.
   */
  SKIPPED = 6,
  /** DELETION_PENDING - The backup is about to be deleted. */
  DELETION_PENDING = 7,
  /** DELETION_FAILED - The backup deletion failed. */
  DELETION_FAILED = 8,
  /** DELETED - The backup has been deleted. */
  DELETED = 9,
  UNRECOGNIZED = -1,
}

export function sqlBackupRunStatusFromJSON(object: any): SqlBackupRunStatus {
  switch (object) {
    case 0:
    case "SQL_BACKUP_RUN_STATUS_UNSPECIFIED":
      return SqlBackupRunStatus.SQL_BACKUP_RUN_STATUS_UNSPECIFIED;
    case 1:
    case "ENQUEUED":
      return SqlBackupRunStatus.ENQUEUED;
    case 2:
    case "OVERDUE":
      return SqlBackupRunStatus.OVERDUE;
    case 3:
    case "RUNNING":
      return SqlBackupRunStatus.RUNNING;
    case 4:
    case "FAILED":
      return SqlBackupRunStatus.FAILED;
    case 5:
    case "SUCCESSFUL":
      return SqlBackupRunStatus.SUCCESSFUL;
    case 6:
    case "SKIPPED":
      return SqlBackupRunStatus.SKIPPED;
    case 7:
    case "DELETION_PENDING":
      return SqlBackupRunStatus.DELETION_PENDING;
    case 8:
    case "DELETION_FAILED":
      return SqlBackupRunStatus.DELETION_FAILED;
    case 9:
    case "DELETED":
      return SqlBackupRunStatus.DELETED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlBackupRunStatus.UNRECOGNIZED;
  }
}

export function sqlBackupRunStatusToJSON(object: SqlBackupRunStatus): string {
  switch (object) {
    case SqlBackupRunStatus.SQL_BACKUP_RUN_STATUS_UNSPECIFIED:
      return "SQL_BACKUP_RUN_STATUS_UNSPECIFIED";
    case SqlBackupRunStatus.ENQUEUED:
      return "ENQUEUED";
    case SqlBackupRunStatus.OVERDUE:
      return "OVERDUE";
    case SqlBackupRunStatus.RUNNING:
      return "RUNNING";
    case SqlBackupRunStatus.FAILED:
      return "FAILED";
    case SqlBackupRunStatus.SUCCESSFUL:
      return "SUCCESSFUL";
    case SqlBackupRunStatus.SKIPPED:
      return "SKIPPED";
    case SqlBackupRunStatus.DELETION_PENDING:
      return "DELETION_PENDING";
    case SqlBackupRunStatus.DELETION_FAILED:
      return "DELETION_FAILED";
    case SqlBackupRunStatus.DELETED:
      return "DELETED";
    case SqlBackupRunStatus.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export enum SqlBackupRunType {
  /** SQL_BACKUP_RUN_TYPE_UNSPECIFIED - This is an unknown BackupRun type. */
  SQL_BACKUP_RUN_TYPE_UNSPECIFIED = 0,
  /** AUTOMATED - The backup schedule automatically triggers a backup. */
  AUTOMATED = 1,
  /** ON_DEMAND - The user manually triggers a backup. */
  ON_DEMAND = 2,
  UNRECOGNIZED = -1,
}

export function sqlBackupRunTypeFromJSON(object: any): SqlBackupRunType {
  switch (object) {
    case 0:
    case "SQL_BACKUP_RUN_TYPE_UNSPECIFIED":
      return SqlBackupRunType.SQL_BACKUP_RUN_TYPE_UNSPECIFIED;
    case 1:
    case "AUTOMATED":
      return SqlBackupRunType.AUTOMATED;
    case 2:
    case "ON_DEMAND":
      return SqlBackupRunType.ON_DEMAND;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlBackupRunType.UNRECOGNIZED;
  }
}

export function sqlBackupRunTypeToJSON(object: SqlBackupRunType): string {
  switch (object) {
    case SqlBackupRunType.SQL_BACKUP_RUN_TYPE_UNSPECIFIED:
      return "SQL_BACKUP_RUN_TYPE_UNSPECIFIED";
    case SqlBackupRunType.AUTOMATED:
      return "AUTOMATED";
    case SqlBackupRunType.ON_DEMAND:
      return "ON_DEMAND";
    case SqlBackupRunType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Defines the supported backup kinds */
export enum SqlBackupKind {
  /** SQL_BACKUP_KIND_UNSPECIFIED - This is an unknown BackupKind. */
  SQL_BACKUP_KIND_UNSPECIFIED = 0,
  /** SNAPSHOT - The snapshot based backups */
  SNAPSHOT = 1,
  /** PHYSICAL - Physical backups */
  PHYSICAL = 2,
  UNRECOGNIZED = -1,
}

export function sqlBackupKindFromJSON(object: any): SqlBackupKind {
  switch (object) {
    case 0:
    case "SQL_BACKUP_KIND_UNSPECIFIED":
      return SqlBackupKind.SQL_BACKUP_KIND_UNSPECIFIED;
    case 1:
    case "SNAPSHOT":
      return SqlBackupKind.SNAPSHOT;
    case 2:
    case "PHYSICAL":
      return SqlBackupKind.PHYSICAL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlBackupKind.UNRECOGNIZED;
  }
}

export function sqlBackupKindToJSON(object: SqlBackupKind): string {
  switch (object) {
    case SqlBackupKind.SQL_BACKUP_KIND_UNSPECIFIED:
      return "SQL_BACKUP_KIND_UNSPECIFIED";
    case SqlBackupKind.SNAPSHOT:
      return "SNAPSHOT";
    case SqlBackupKind.PHYSICAL:
      return "PHYSICAL";
    case SqlBackupKind.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export enum SqlBackendType {
  /** SQL_BACKEND_TYPE_UNSPECIFIED - This is an unknown backend type for instance. */
  SQL_BACKEND_TYPE_UNSPECIFIED = 0,
  /**
   * FIRST_GEN - V1 speckle instance.
   *
   * @deprecated
   */
  FIRST_GEN = 1,
  /** SECOND_GEN - V2 speckle instance. */
  SECOND_GEN = 2,
  /** EXTERNAL - On premises instance. */
  EXTERNAL = 3,
  UNRECOGNIZED = -1,
}

export function sqlBackendTypeFromJSON(object: any): SqlBackendType {
  switch (object) {
    case 0:
    case "SQL_BACKEND_TYPE_UNSPECIFIED":
      return SqlBackendType.SQL_BACKEND_TYPE_UNSPECIFIED;
    case 1:
    case "FIRST_GEN":
      return SqlBackendType.FIRST_GEN;
    case 2:
    case "SECOND_GEN":
      return SqlBackendType.SECOND_GEN;
    case 3:
    case "EXTERNAL":
      return SqlBackendType.EXTERNAL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlBackendType.UNRECOGNIZED;
  }
}

export function sqlBackendTypeToJSON(object: SqlBackendType): string {
  switch (object) {
    case SqlBackendType.SQL_BACKEND_TYPE_UNSPECIFIED:
      return "SQL_BACKEND_TYPE_UNSPECIFIED";
    case SqlBackendType.FIRST_GEN:
      return "FIRST_GEN";
    case SqlBackendType.SECOND_GEN:
      return "SECOND_GEN";
    case SqlBackendType.EXTERNAL:
      return "EXTERNAL";
    case SqlBackendType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export enum SqlIpAddressType {
  /** SQL_IP_ADDRESS_TYPE_UNSPECIFIED - This is an unknown IP address type. */
  SQL_IP_ADDRESS_TYPE_UNSPECIFIED = 0,
  /**
   * PRIMARY - IP address the customer is supposed to connect to. Usually this is the
   * load balancer's IP address
   */
  PRIMARY = 1,
  /**
   * OUTGOING - Source IP address of the connection a read replica establishes to its
   * external primary instance. This IP address can be allowlisted by the
   * customer in case it has a firewall that filters incoming connection to its
   * on premises primary instance.
   */
  OUTGOING = 2,
  /** PRIVATE - Private IP used when using private IPs and network peering. */
  PRIVATE = 3,
  /**
   * MIGRATED_1ST_GEN - V1 IP of a migrated instance. We want the user to
   * decommission this IP as soon as the migration is complete.
   * Note: V1 instances with V1 ip addresses will be counted as PRIMARY.
   */
  MIGRATED_1ST_GEN = 4,
  UNRECOGNIZED = -1,
}

export function sqlIpAddressTypeFromJSON(object: any): SqlIpAddressType {
  switch (object) {
    case 0:
    case "SQL_IP_ADDRESS_TYPE_UNSPECIFIED":
      return SqlIpAddressType.SQL_IP_ADDRESS_TYPE_UNSPECIFIED;
    case 1:
    case "PRIMARY":
      return SqlIpAddressType.PRIMARY;
    case 2:
    case "OUTGOING":
      return SqlIpAddressType.OUTGOING;
    case 3:
    case "PRIVATE":
      return SqlIpAddressType.PRIVATE;
    case 4:
    case "MIGRATED_1ST_GEN":
      return SqlIpAddressType.MIGRATED_1ST_GEN;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlIpAddressType.UNRECOGNIZED;
  }
}

export function sqlIpAddressTypeToJSON(object: SqlIpAddressType): string {
  switch (object) {
    case SqlIpAddressType.SQL_IP_ADDRESS_TYPE_UNSPECIFIED:
      return "SQL_IP_ADDRESS_TYPE_UNSPECIFIED";
    case SqlIpAddressType.PRIMARY:
      return "PRIMARY";
    case SqlIpAddressType.OUTGOING:
      return "OUTGOING";
    case SqlIpAddressType.PRIVATE:
      return "PRIVATE";
    case SqlIpAddressType.MIGRATED_1ST_GEN:
      return "MIGRATED_1ST_GEN";
    case SqlIpAddressType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export enum SqlInstanceType {
  /** SQL_INSTANCE_TYPE_UNSPECIFIED - This is an unknown Cloud SQL instance type. */
  SQL_INSTANCE_TYPE_UNSPECIFIED = 0,
  /**
   * CLOUD_SQL_INSTANCE - A regular Cloud SQL instance that is not replicating from a primary
   * instance.
   */
  CLOUD_SQL_INSTANCE = 1,
  /**
   * ON_PREMISES_INSTANCE - An instance running on the customer's premises that is not managed by
   * Cloud SQL.
   */
  ON_PREMISES_INSTANCE = 2,
  /** READ_REPLICA_INSTANCE - A Cloud SQL instance acting as a read-replica. */
  READ_REPLICA_INSTANCE = 3,
  UNRECOGNIZED = -1,
}

export function sqlInstanceTypeFromJSON(object: any): SqlInstanceType {
  switch (object) {
    case 0:
    case "SQL_INSTANCE_TYPE_UNSPECIFIED":
      return SqlInstanceType.SQL_INSTANCE_TYPE_UNSPECIFIED;
    case 1:
    case "CLOUD_SQL_INSTANCE":
      return SqlInstanceType.CLOUD_SQL_INSTANCE;
    case 2:
    case "ON_PREMISES_INSTANCE":
      return SqlInstanceType.ON_PREMISES_INSTANCE;
    case 3:
    case "READ_REPLICA_INSTANCE":
      return SqlInstanceType.READ_REPLICA_INSTANCE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlInstanceType.UNRECOGNIZED;
  }
}

export function sqlInstanceTypeToJSON(object: SqlInstanceType): string {
  switch (object) {
    case SqlInstanceType.SQL_INSTANCE_TYPE_UNSPECIFIED:
      return "SQL_INSTANCE_TYPE_UNSPECIFIED";
    case SqlInstanceType.CLOUD_SQL_INSTANCE:
      return "CLOUD_SQL_INSTANCE";
    case SqlInstanceType.ON_PREMISES_INSTANCE:
      return "ON_PREMISES_INSTANCE";
    case SqlInstanceType.READ_REPLICA_INSTANCE:
      return "READ_REPLICA_INSTANCE";
    case SqlInstanceType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The database engine type and version. */
export enum SqlDatabaseVersion {
  /** SQL_DATABASE_VERSION_UNSPECIFIED - This is an unknown database version. */
  SQL_DATABASE_VERSION_UNSPECIFIED = 0,
  /**
   * MYSQL_5_1 - The database version is MySQL 5.1.
   *
   * @deprecated
   */
  MYSQL_5_1 = 2,
  /**
   * MYSQL_5_5 - The database version is MySQL 5.5.
   *
   * @deprecated
   */
  MYSQL_5_5 = 3,
  /** MYSQL_5_6 - The database version is MySQL 5.6. */
  MYSQL_5_6 = 5,
  /** MYSQL_5_7 - The database version is MySQL 5.7. */
  MYSQL_5_7 = 6,
  /** SQLSERVER_2017_STANDARD - The database version is SQL Server 2017 Standard. */
  SQLSERVER_2017_STANDARD = 11,
  /** SQLSERVER_2017_ENTERPRISE - The database version is SQL Server 2017 Enterprise. */
  SQLSERVER_2017_ENTERPRISE = 14,
  /** SQLSERVER_2017_EXPRESS - The database version is SQL Server 2017 Express. */
  SQLSERVER_2017_EXPRESS = 15,
  /** SQLSERVER_2017_WEB - The database version is SQL Server 2017 Web. */
  SQLSERVER_2017_WEB = 16,
  /** POSTGRES_9_6 - The database version is PostgreSQL 9.6. */
  POSTGRES_9_6 = 9,
  /** POSTGRES_10 - The database version is PostgreSQL 10. */
  POSTGRES_10 = 18,
  /** POSTGRES_11 - The database version is PostgreSQL 11. */
  POSTGRES_11 = 10,
  /** POSTGRES_12 - The database version is PostgreSQL 12. */
  POSTGRES_12 = 19,
  /** POSTGRES_13 - The database version is PostgreSQL 13. */
  POSTGRES_13 = 23,
  /** POSTGRES_14 - The database version is PostgreSQL 14. */
  POSTGRES_14 = 110,
  /** POSTGRES_15 - The database version is PostgreSQL 15. */
  POSTGRES_15 = 172,
  /** POSTGRES_16 - The database version is PostgreSQL 16. */
  POSTGRES_16 = 272,
  /** MYSQL_8_0 - The database version is MySQL 8. */
  MYSQL_8_0 = 20,
  /** MYSQL_8_0_18 - The database major version is MySQL 8.0 and the minor version is 18. */
  MYSQL_8_0_18 = 41,
  /** MYSQL_8_0_26 - The database major version is MySQL 8.0 and the minor version is 26. */
  MYSQL_8_0_26 = 85,
  /** MYSQL_8_0_27 - The database major version is MySQL 8.0 and the minor version is 27. */
  MYSQL_8_0_27 = 111,
  /** MYSQL_8_0_28 - The database major version is MySQL 8.0 and the minor version is 28. */
  MYSQL_8_0_28 = 132,
  /**
   * MYSQL_8_0_29 - The database major version is MySQL 8.0 and the minor version is 29.
   *
   * @deprecated
   */
  MYSQL_8_0_29 = 148,
  /** MYSQL_8_0_30 - The database major version is MySQL 8.0 and the minor version is 30. */
  MYSQL_8_0_30 = 174,
  /** MYSQL_8_0_31 - The database major version is MySQL 8.0 and the minor version is 31. */
  MYSQL_8_0_31 = 197,
  /** MYSQL_8_0_32 - The database major version is MySQL 8.0 and the minor version is 32. */
  MYSQL_8_0_32 = 213,
  /** MYSQL_8_0_33 - The database major version is MySQL 8.0 and the minor version is 33. */
  MYSQL_8_0_33 = 238,
  /** MYSQL_8_0_34 - The database major version is MySQL 8.0 and the minor version is 34. */
  MYSQL_8_0_34 = 239,
  /** MYSQL_8_0_35 - The database major version is MySQL 8.0 and the minor version is 35. */
  MYSQL_8_0_35 = 240,
  /** MYSQL_8_0_36 - The database major version is MySQL 8.0 and the minor version is 36. */
  MYSQL_8_0_36 = 241,
  /** MYSQL_8_0_37 - The database major version is MySQL 8.0 and the minor version is 37. */
  MYSQL_8_0_37 = 355,
  /** MYSQL_8_0_38 - The database major version is MySQL 8.0 and the minor version is 38. */
  MYSQL_8_0_38 = 356,
  /** MYSQL_8_0_39 - The database major version is MySQL 8.0 and the minor version is 39. */
  MYSQL_8_0_39 = 357,
  /** MYSQL_8_0_40 - The database major version is MySQL 8.0 and the minor version is 40. */
  MYSQL_8_0_40 = 358,
  /** MYSQL_8_4 - The database version is MySQL 8.4. */
  MYSQL_8_4 = 398,
  /** MYSQL_8_4_0 - The database version is MySQL 8.4 and the patch version is 0. */
  MYSQL_8_4_0 = 399,
  /** SQLSERVER_2019_STANDARD - The database version is SQL Server 2019 Standard. */
  SQLSERVER_2019_STANDARD = 26,
  /** SQLSERVER_2019_ENTERPRISE - The database version is SQL Server 2019 Enterprise. */
  SQLSERVER_2019_ENTERPRISE = 27,
  /** SQLSERVER_2019_EXPRESS - The database version is SQL Server 2019 Express. */
  SQLSERVER_2019_EXPRESS = 28,
  /** SQLSERVER_2019_WEB - The database version is SQL Server 2019 Web. */
  SQLSERVER_2019_WEB = 29,
  /** SQLSERVER_2022_STANDARD - The database version is SQL Server 2022 Standard. */
  SQLSERVER_2022_STANDARD = 199,
  /** SQLSERVER_2022_ENTERPRISE - The database version is SQL Server 2022 Enterprise. */
  SQLSERVER_2022_ENTERPRISE = 200,
  /** SQLSERVER_2022_EXPRESS - The database version is SQL Server 2022 Express. */
  SQLSERVER_2022_EXPRESS = 201,
  /** SQLSERVER_2022_WEB - The database version is SQL Server 2022 Web. */
  SQLSERVER_2022_WEB = 202,
  UNRECOGNIZED = -1,
}

export function sqlDatabaseVersionFromJSON(object: any): SqlDatabaseVersion {
  switch (object) {
    case 0:
    case "SQL_DATABASE_VERSION_UNSPECIFIED":
      return SqlDatabaseVersion.SQL_DATABASE_VERSION_UNSPECIFIED;
    case 2:
    case "MYSQL_5_1":
      return SqlDatabaseVersion.MYSQL_5_1;
    case 3:
    case "MYSQL_5_5":
      return SqlDatabaseVersion.MYSQL_5_5;
    case 5:
    case "MYSQL_5_6":
      return SqlDatabaseVersion.MYSQL_5_6;
    case 6:
    case "MYSQL_5_7":
      return SqlDatabaseVersion.MYSQL_5_7;
    case 11:
    case "SQLSERVER_2017_STANDARD":
      return SqlDatabaseVersion.SQLSERVER_2017_STANDARD;
    case 14:
    case "SQLSERVER_2017_ENTERPRISE":
      return SqlDatabaseVersion.SQLSERVER_2017_ENTERPRISE;
    case 15:
    case "SQLSERVER_2017_EXPRESS":
      return SqlDatabaseVersion.SQLSERVER_2017_EXPRESS;
    case 16:
    case "SQLSERVER_2017_WEB":
      return SqlDatabaseVersion.SQLSERVER_2017_WEB;
    case 9:
    case "POSTGRES_9_6":
      return SqlDatabaseVersion.POSTGRES_9_6;
    case 18:
    case "POSTGRES_10":
      return SqlDatabaseVersion.POSTGRES_10;
    case 10:
    case "POSTGRES_11":
      return SqlDatabaseVersion.POSTGRES_11;
    case 19:
    case "POSTGRES_12":
      return SqlDatabaseVersion.POSTGRES_12;
    case 23:
    case "POSTGRES_13":
      return SqlDatabaseVersion.POSTGRES_13;
    case 110:
    case "POSTGRES_14":
      return SqlDatabaseVersion.POSTGRES_14;
    case 172:
    case "POSTGRES_15":
      return SqlDatabaseVersion.POSTGRES_15;
    case 272:
    case "POSTGRES_16":
      return SqlDatabaseVersion.POSTGRES_16;
    case 20:
    case "MYSQL_8_0":
      return SqlDatabaseVersion.MYSQL_8_0;
    case 41:
    case "MYSQL_8_0_18":
      return SqlDatabaseVersion.MYSQL_8_0_18;
    case 85:
    case "MYSQL_8_0_26":
      return SqlDatabaseVersion.MYSQL_8_0_26;
    case 111:
    case "MYSQL_8_0_27":
      return SqlDatabaseVersion.MYSQL_8_0_27;
    case 132:
    case "MYSQL_8_0_28":
      return SqlDatabaseVersion.MYSQL_8_0_28;
    case 148:
    case "MYSQL_8_0_29":
      return SqlDatabaseVersion.MYSQL_8_0_29;
    case 174:
    case "MYSQL_8_0_30":
      return SqlDatabaseVersion.MYSQL_8_0_30;
    case 197:
    case "MYSQL_8_0_31":
      return SqlDatabaseVersion.MYSQL_8_0_31;
    case 213:
    case "MYSQL_8_0_32":
      return SqlDatabaseVersion.MYSQL_8_0_32;
    case 238:
    case "MYSQL_8_0_33":
      return SqlDatabaseVersion.MYSQL_8_0_33;
    case 239:
    case "MYSQL_8_0_34":
      return SqlDatabaseVersion.MYSQL_8_0_34;
    case 240:
    case "MYSQL_8_0_35":
      return SqlDatabaseVersion.MYSQL_8_0_35;
    case 241:
    case "MYSQL_8_0_36":
      return SqlDatabaseVersion.MYSQL_8_0_36;
    case 355:
    case "MYSQL_8_0_37":
      return SqlDatabaseVersion.MYSQL_8_0_37;
    case 356:
    case "MYSQL_8_0_38":
      return SqlDatabaseVersion.MYSQL_8_0_38;
    case 357:
    case "MYSQL_8_0_39":
      return SqlDatabaseVersion.MYSQL_8_0_39;
    case 358:
    case "MYSQL_8_0_40":
      return SqlDatabaseVersion.MYSQL_8_0_40;
    case 398:
    case "MYSQL_8_4":
      return SqlDatabaseVersion.MYSQL_8_4;
    case 399:
    case "MYSQL_8_4_0":
      return SqlDatabaseVersion.MYSQL_8_4_0;
    case 26:
    case "SQLSERVER_2019_STANDARD":
      return SqlDatabaseVersion.SQLSERVER_2019_STANDARD;
    case 27:
    case "SQLSERVER_2019_ENTERPRISE":
      return SqlDatabaseVersion.SQLSERVER_2019_ENTERPRISE;
    case 28:
    case "SQLSERVER_2019_EXPRESS":
      return SqlDatabaseVersion.SQLSERVER_2019_EXPRESS;
    case 29:
    case "SQLSERVER_2019_WEB":
      return SqlDatabaseVersion.SQLSERVER_2019_WEB;
    case 199:
    case "SQLSERVER_2022_STANDARD":
      return SqlDatabaseVersion.SQLSERVER_2022_STANDARD;
    case 200:
    case "SQLSERVER_2022_ENTERPRISE":
      return SqlDatabaseVersion.SQLSERVER_2022_ENTERPRISE;
    case 201:
    case "SQLSERVER_2022_EXPRESS":
      return SqlDatabaseVersion.SQLSERVER_2022_EXPRESS;
    case 202:
    case "SQLSERVER_2022_WEB":
      return SqlDatabaseVersion.SQLSERVER_2022_WEB;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlDatabaseVersion.UNRECOGNIZED;
  }
}

export function sqlDatabaseVersionToJSON(object: SqlDatabaseVersion): string {
  switch (object) {
    case SqlDatabaseVersion.SQL_DATABASE_VERSION_UNSPECIFIED:
      return "SQL_DATABASE_VERSION_UNSPECIFIED";
    case SqlDatabaseVersion.MYSQL_5_1:
      return "MYSQL_5_1";
    case SqlDatabaseVersion.MYSQL_5_5:
      return "MYSQL_5_5";
    case SqlDatabaseVersion.MYSQL_5_6:
      return "MYSQL_5_6";
    case SqlDatabaseVersion.MYSQL_5_7:
      return "MYSQL_5_7";
    case SqlDatabaseVersion.SQLSERVER_2017_STANDARD:
      return "SQLSERVER_2017_STANDARD";
    case SqlDatabaseVersion.SQLSERVER_2017_ENTERPRISE:
      return "SQLSERVER_2017_ENTERPRISE";
    case SqlDatabaseVersion.SQLSERVER_2017_EXPRESS:
      return "SQLSERVER_2017_EXPRESS";
    case SqlDatabaseVersion.SQLSERVER_2017_WEB:
      return "SQLSERVER_2017_WEB";
    case SqlDatabaseVersion.POSTGRES_9_6:
      return "POSTGRES_9_6";
    case SqlDatabaseVersion.POSTGRES_10:
      return "POSTGRES_10";
    case SqlDatabaseVersion.POSTGRES_11:
      return "POSTGRES_11";
    case SqlDatabaseVersion.POSTGRES_12:
      return "POSTGRES_12";
    case SqlDatabaseVersion.POSTGRES_13:
      return "POSTGRES_13";
    case SqlDatabaseVersion.POSTGRES_14:
      return "POSTGRES_14";
    case SqlDatabaseVersion.POSTGRES_15:
      return "POSTGRES_15";
    case SqlDatabaseVersion.POSTGRES_16:
      return "POSTGRES_16";
    case SqlDatabaseVersion.MYSQL_8_0:
      return "MYSQL_8_0";
    case SqlDatabaseVersion.MYSQL_8_0_18:
      return "MYSQL_8_0_18";
    case SqlDatabaseVersion.MYSQL_8_0_26:
      return "MYSQL_8_0_26";
    case SqlDatabaseVersion.MYSQL_8_0_27:
      return "MYSQL_8_0_27";
    case SqlDatabaseVersion.MYSQL_8_0_28:
      return "MYSQL_8_0_28";
    case SqlDatabaseVersion.MYSQL_8_0_29:
      return "MYSQL_8_0_29";
    case SqlDatabaseVersion.MYSQL_8_0_30:
      return "MYSQL_8_0_30";
    case SqlDatabaseVersion.MYSQL_8_0_31:
      return "MYSQL_8_0_31";
    case SqlDatabaseVersion.MYSQL_8_0_32:
      return "MYSQL_8_0_32";
    case SqlDatabaseVersion.MYSQL_8_0_33:
      return "MYSQL_8_0_33";
    case SqlDatabaseVersion.MYSQL_8_0_34:
      return "MYSQL_8_0_34";
    case SqlDatabaseVersion.MYSQL_8_0_35:
      return "MYSQL_8_0_35";
    case SqlDatabaseVersion.MYSQL_8_0_36:
      return "MYSQL_8_0_36";
    case SqlDatabaseVersion.MYSQL_8_0_37:
      return "MYSQL_8_0_37";
    case SqlDatabaseVersion.MYSQL_8_0_38:
      return "MYSQL_8_0_38";
    case SqlDatabaseVersion.MYSQL_8_0_39:
      return "MYSQL_8_0_39";
    case SqlDatabaseVersion.MYSQL_8_0_40:
      return "MYSQL_8_0_40";
    case SqlDatabaseVersion.MYSQL_8_4:
      return "MYSQL_8_4";
    case SqlDatabaseVersion.MYSQL_8_4_0:
      return "MYSQL_8_4_0";
    case SqlDatabaseVersion.SQLSERVER_2019_STANDARD:
      return "SQLSERVER_2019_STANDARD";
    case SqlDatabaseVersion.SQLSERVER_2019_ENTERPRISE:
      return "SQLSERVER_2019_ENTERPRISE";
    case SqlDatabaseVersion.SQLSERVER_2019_EXPRESS:
      return "SQLSERVER_2019_EXPRESS";
    case SqlDatabaseVersion.SQLSERVER_2019_WEB:
      return "SQLSERVER_2019_WEB";
    case SqlDatabaseVersion.SQLSERVER_2022_STANDARD:
      return "SQLSERVER_2022_STANDARD";
    case SqlDatabaseVersion.SQLSERVER_2022_ENTERPRISE:
      return "SQLSERVER_2022_ENTERPRISE";
    case SqlDatabaseVersion.SQLSERVER_2022_EXPRESS:
      return "SQLSERVER_2022_EXPRESS";
    case SqlDatabaseVersion.SQLSERVER_2022_WEB:
      return "SQLSERVER_2022_WEB";
    case SqlDatabaseVersion.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The suspension reason of the database instance if the state is SUSPENDED. */
export enum SqlSuspensionReason {
  /** SQL_SUSPENSION_REASON_UNSPECIFIED - This is an unknown suspension reason. */
  SQL_SUSPENSION_REASON_UNSPECIFIED = 0,
  /**
   * BILLING_ISSUE - The instance is suspended due to billing issues (for example:, GCP account
   * issue)
   */
  BILLING_ISSUE = 2,
  /**
   * LEGAL_ISSUE - The instance is suspended due to illegal content (for example:, child
   * pornography, copyrighted material, etc.).
   */
  LEGAL_ISSUE = 3,
  /**
   * OPERATIONAL_ISSUE - The instance is causing operational issues (for example:, causing the
   * database to crash).
   */
  OPERATIONAL_ISSUE = 4,
  /** KMS_KEY_ISSUE - The KMS key used by the instance is either revoked or denied access to */
  KMS_KEY_ISSUE = 5,
  UNRECOGNIZED = -1,
}

export function sqlSuspensionReasonFromJSON(object: any): SqlSuspensionReason {
  switch (object) {
    case 0:
    case "SQL_SUSPENSION_REASON_UNSPECIFIED":
      return SqlSuspensionReason.SQL_SUSPENSION_REASON_UNSPECIFIED;
    case 2:
    case "BILLING_ISSUE":
      return SqlSuspensionReason.BILLING_ISSUE;
    case 3:
    case "LEGAL_ISSUE":
      return SqlSuspensionReason.LEGAL_ISSUE;
    case 4:
    case "OPERATIONAL_ISSUE":
      return SqlSuspensionReason.OPERATIONAL_ISSUE;
    case 5:
    case "KMS_KEY_ISSUE":
      return SqlSuspensionReason.KMS_KEY_ISSUE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlSuspensionReason.UNRECOGNIZED;
  }
}

export function sqlSuspensionReasonToJSON(object: SqlSuspensionReason): string {
  switch (object) {
    case SqlSuspensionReason.SQL_SUSPENSION_REASON_UNSPECIFIED:
      return "SQL_SUSPENSION_REASON_UNSPECIFIED";
    case SqlSuspensionReason.BILLING_ISSUE:
      return "BILLING_ISSUE";
    case SqlSuspensionReason.LEGAL_ISSUE:
      return "LEGAL_ISSUE";
    case SqlSuspensionReason.OPERATIONAL_ISSUE:
      return "OPERATIONAL_ISSUE";
    case SqlSuspensionReason.KMS_KEY_ISSUE:
      return "KMS_KEY_ISSUE";
    case SqlSuspensionReason.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The pricing plan for this instance. */
export enum SqlPricingPlan {
  /** SQL_PRICING_PLAN_UNSPECIFIED - This is an unknown pricing plan for this instance. */
  SQL_PRICING_PLAN_UNSPECIFIED = 0,
  /** PACKAGE - The instance is billed at a monthly flat rate. */
  PACKAGE = 1,
  /** PER_USE - The instance is billed per usage. */
  PER_USE = 2,
  UNRECOGNIZED = -1,
}

export function sqlPricingPlanFromJSON(object: any): SqlPricingPlan {
  switch (object) {
    case 0:
    case "SQL_PRICING_PLAN_UNSPECIFIED":
      return SqlPricingPlan.SQL_PRICING_PLAN_UNSPECIFIED;
    case 1:
    case "PACKAGE":
      return SqlPricingPlan.PACKAGE;
    case 2:
    case "PER_USE":
      return SqlPricingPlan.PER_USE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlPricingPlan.UNRECOGNIZED;
  }
}

export function sqlPricingPlanToJSON(object: SqlPricingPlan): string {
  switch (object) {
    case SqlPricingPlan.SQL_PRICING_PLAN_UNSPECIFIED:
      return "SQL_PRICING_PLAN_UNSPECIFIED";
    case SqlPricingPlan.PACKAGE:
      return "PACKAGE";
    case SqlPricingPlan.PER_USE:
      return "PER_USE";
    case SqlPricingPlan.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export enum SqlReplicationType {
  /** SQL_REPLICATION_TYPE_UNSPECIFIED - This is an unknown replication type for a Cloud SQL instance. */
  SQL_REPLICATION_TYPE_UNSPECIFIED = 0,
  /**
   * SYNCHRONOUS - The synchronous replication mode for First Generation instances. It is the
   * default value.
   */
  SYNCHRONOUS = 1,
  /**
   * ASYNCHRONOUS - The asynchronous replication mode for First Generation instances. It
   * provides a slight performance gain, but if an outage occurs while this
   * option is set to asynchronous, you can lose up to a few seconds of updates
   * to your data.
   */
  ASYNCHRONOUS = 2,
  UNRECOGNIZED = -1,
}

export function sqlReplicationTypeFromJSON(object: any): SqlReplicationType {
  switch (object) {
    case 0:
    case "SQL_REPLICATION_TYPE_UNSPECIFIED":
      return SqlReplicationType.SQL_REPLICATION_TYPE_UNSPECIFIED;
    case 1:
    case "SYNCHRONOUS":
      return SqlReplicationType.SYNCHRONOUS;
    case 2:
    case "ASYNCHRONOUS":
      return SqlReplicationType.ASYNCHRONOUS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlReplicationType.UNRECOGNIZED;
  }
}

export function sqlReplicationTypeToJSON(object: SqlReplicationType): string {
  switch (object) {
    case SqlReplicationType.SQL_REPLICATION_TYPE_UNSPECIFIED:
      return "SQL_REPLICATION_TYPE_UNSPECIFIED";
    case SqlReplicationType.SYNCHRONOUS:
      return "SYNCHRONOUS";
    case SqlReplicationType.ASYNCHRONOUS:
      return "ASYNCHRONOUS";
    case SqlReplicationType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The type of disk that is used for a v2 instance to use. */
export enum SqlDataDiskType {
  /** SQL_DATA_DISK_TYPE_UNSPECIFIED - This is an unknown data disk type. */
  SQL_DATA_DISK_TYPE_UNSPECIFIED = 0,
  /** PD_SSD - An SSD data disk. */
  PD_SSD = 1,
  /** PD_HDD - An HDD data disk. */
  PD_HDD = 2,
  /**
   * OBSOLETE_LOCAL_SSD - This field is deprecated and will be removed from a future version of the
   * API.
   *
   * @deprecated
   */
  OBSOLETE_LOCAL_SSD = 3,
  UNRECOGNIZED = -1,
}

export function sqlDataDiskTypeFromJSON(object: any): SqlDataDiskType {
  switch (object) {
    case 0:
    case "SQL_DATA_DISK_TYPE_UNSPECIFIED":
      return SqlDataDiskType.SQL_DATA_DISK_TYPE_UNSPECIFIED;
    case 1:
    case "PD_SSD":
      return SqlDataDiskType.PD_SSD;
    case 2:
    case "PD_HDD":
      return SqlDataDiskType.PD_HDD;
    case 3:
    case "OBSOLETE_LOCAL_SSD":
      return SqlDataDiskType.OBSOLETE_LOCAL_SSD;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlDataDiskType.UNRECOGNIZED;
  }
}

export function sqlDataDiskTypeToJSON(object: SqlDataDiskType): string {
  switch (object) {
    case SqlDataDiskType.SQL_DATA_DISK_TYPE_UNSPECIFIED:
      return "SQL_DATA_DISK_TYPE_UNSPECIFIED";
    case SqlDataDiskType.PD_SSD:
      return "PD_SSD";
    case SqlDataDiskType.PD_HDD:
      return "PD_HDD";
    case SqlDataDiskType.OBSOLETE_LOCAL_SSD:
      return "OBSOLETE_LOCAL_SSD";
    case SqlDataDiskType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The availability type of the given Cloud SQL instance. */
export enum SqlAvailabilityType {
  /** SQL_AVAILABILITY_TYPE_UNSPECIFIED - This is an unknown Availability type. */
  SQL_AVAILABILITY_TYPE_UNSPECIFIED = 0,
  /** ZONAL - Zonal available instance. */
  ZONAL = 1,
  /** REGIONAL - Regional available instance. */
  REGIONAL = 2,
  UNRECOGNIZED = -1,
}

export function sqlAvailabilityTypeFromJSON(object: any): SqlAvailabilityType {
  switch (object) {
    case 0:
    case "SQL_AVAILABILITY_TYPE_UNSPECIFIED":
      return SqlAvailabilityType.SQL_AVAILABILITY_TYPE_UNSPECIFIED;
    case 1:
    case "ZONAL":
      return SqlAvailabilityType.ZONAL;
    case 2:
    case "REGIONAL":
      return SqlAvailabilityType.REGIONAL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlAvailabilityType.UNRECOGNIZED;
  }
}

export function sqlAvailabilityTypeToJSON(object: SqlAvailabilityType): string {
  switch (object) {
    case SqlAvailabilityType.SQL_AVAILABILITY_TYPE_UNSPECIFIED:
      return "SQL_AVAILABILITY_TYPE_UNSPECIFIED";
    case SqlAvailabilityType.ZONAL:
      return "ZONAL";
    case SqlAvailabilityType.REGIONAL:
      return "REGIONAL";
    case SqlAvailabilityType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export enum SqlUpdateTrack {
  /** SQL_UPDATE_TRACK_UNSPECIFIED - This is an unknown maintenance timing preference. */
  SQL_UPDATE_TRACK_UNSPECIFIED = 0,
  /**
   * canary - For instance update that requires a restart, this update track indicates
   * your instance prefer to restart for new version early in maintenance
   * window.
   */
  canary = 1,
  /**
   * stable - For instance update that requires a restart, this update track indicates
   * your instance prefer to let Cloud SQL choose the timing of restart (within
   * its Maintenance window, if applicable).
   */
  stable = 2,
  /**
   * week5 - For instance update that requires a restart, this update track indicates
   * your instance prefer to let Cloud SQL choose the timing of restart (within
   * its Maintenance window, if applicable) to be at least 5 weeks after the
   * notification.
   */
  week5 = 3,
  UNRECOGNIZED = -1,
}

export function sqlUpdateTrackFromJSON(object: any): SqlUpdateTrack {
  switch (object) {
    case 0:
    case "SQL_UPDATE_TRACK_UNSPECIFIED":
      return SqlUpdateTrack.SQL_UPDATE_TRACK_UNSPECIFIED;
    case 1:
    case "canary":
      return SqlUpdateTrack.canary;
    case 2:
    case "stable":
      return SqlUpdateTrack.stable;
    case 3:
    case "week5":
      return SqlUpdateTrack.week5;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlUpdateTrack.UNRECOGNIZED;
  }
}

export function sqlUpdateTrackToJSON(object: SqlUpdateTrack): string {
  switch (object) {
    case SqlUpdateTrack.SQL_UPDATE_TRACK_UNSPECIFIED:
      return "SQL_UPDATE_TRACK_UNSPECIFIED";
    case SqlUpdateTrack.canary:
      return "canary";
    case SqlUpdateTrack.stable:
      return "stable";
    case SqlUpdateTrack.week5:
      return "week5";
    case SqlUpdateTrack.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export enum SqlFlagType {
  /** SQL_FLAG_TYPE_UNSPECIFIED - This is an unknown flag type. */
  SQL_FLAG_TYPE_UNSPECIFIED = 0,
  /** BOOLEAN - Boolean type flag. */
  BOOLEAN = 1,
  /** STRING - String type flag. */
  STRING = 2,
  /** INTEGER - Integer type flag. */
  INTEGER = 3,
  /** NONE - Flag type used for a server startup option. */
  NONE = 4,
  /**
   * MYSQL_TIMEZONE_OFFSET - Type introduced specially for MySQL TimeZone offset. Accept a string value
   * with the format [-12:59, 13:00].
   */
  MYSQL_TIMEZONE_OFFSET = 5,
  /** FLOAT - Float type flag. */
  FLOAT = 6,
  /** REPEATED_STRING - Comma-separated list of the strings in a SqlFlagType enum. */
  REPEATED_STRING = 7,
  UNRECOGNIZED = -1,
}

export function sqlFlagTypeFromJSON(object: any): SqlFlagType {
  switch (object) {
    case 0:
    case "SQL_FLAG_TYPE_UNSPECIFIED":
      return SqlFlagType.SQL_FLAG_TYPE_UNSPECIFIED;
    case 1:
    case "BOOLEAN":
      return SqlFlagType.BOOLEAN;
    case 2:
    case "STRING":
      return SqlFlagType.STRING;
    case 3:
    case "INTEGER":
      return SqlFlagType.INTEGER;
    case 4:
    case "NONE":
      return SqlFlagType.NONE;
    case 5:
    case "MYSQL_TIMEZONE_OFFSET":
      return SqlFlagType.MYSQL_TIMEZONE_OFFSET;
    case 6:
    case "FLOAT":
      return SqlFlagType.FLOAT;
    case 7:
    case "REPEATED_STRING":
      return SqlFlagType.REPEATED_STRING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlFlagType.UNRECOGNIZED;
  }
}

export function sqlFlagTypeToJSON(object: SqlFlagType): string {
  switch (object) {
    case SqlFlagType.SQL_FLAG_TYPE_UNSPECIFIED:
      return "SQL_FLAG_TYPE_UNSPECIFIED";
    case SqlFlagType.BOOLEAN:
      return "BOOLEAN";
    case SqlFlagType.STRING:
      return "STRING";
    case SqlFlagType.INTEGER:
      return "INTEGER";
    case SqlFlagType.NONE:
      return "NONE";
    case SqlFlagType.MYSQL_TIMEZONE_OFFSET:
      return "MYSQL_TIMEZONE_OFFSET";
    case SqlFlagType.FLOAT:
      return "FLOAT";
    case SqlFlagType.REPEATED_STRING:
      return "REPEATED_STRING";
    case SqlFlagType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** An entry for an Access Control list. */
export interface AclEntry {
  /** The allowlisted value for the access control list. */
  value: string;
  /**
   * The time when this access control entry expires in
   * [RFC 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`.
   */
  expirationTime:
    | Date
    | undefined;
  /** Optional. A label to identify this entry. */
  name: string;
  /** This is always `sql#aclEntry`. */
  kind: string;
}

/** An Admin API warning message. */
export interface ApiWarning {
  /** Code to uniquely identify the warning type. */
  code: ApiWarning_SqlApiWarningCode;
  /** The warning message. */
  message: string;
  /** The region name for REGION_UNREACHABLE warning. */
  region: string;
}

export enum ApiWarning_SqlApiWarningCode {
  /** SQL_API_WARNING_CODE_UNSPECIFIED - An unknown or unset warning type from Cloud SQL API. */
  SQL_API_WARNING_CODE_UNSPECIFIED = 0,
  /**
   * REGION_UNREACHABLE - Warning when one or more regions are not reachable.  The returned result
   * set may be incomplete.
   */
  REGION_UNREACHABLE = 1,
  /**
   * MAX_RESULTS_EXCEEDS_LIMIT - Warning when user provided maxResults parameter exceeds the limit.  The
   * returned result set may be incomplete.
   */
  MAX_RESULTS_EXCEEDS_LIMIT = 2,
  /**
   * COMPROMISED_CREDENTIALS - Warning when user tries to create/update a user with credentials that
   * have previously been compromised by a public data breach.
   */
  COMPROMISED_CREDENTIALS = 3,
  /**
   * INTERNAL_STATE_FAILURE - Warning when the operation succeeds but some non-critical workflow state
   * failed.
   */
  INTERNAL_STATE_FAILURE = 4,
  UNRECOGNIZED = -1,
}

export function apiWarning_SqlApiWarningCodeFromJSON(object: any): ApiWarning_SqlApiWarningCode {
  switch (object) {
    case 0:
    case "SQL_API_WARNING_CODE_UNSPECIFIED":
      return ApiWarning_SqlApiWarningCode.SQL_API_WARNING_CODE_UNSPECIFIED;
    case 1:
    case "REGION_UNREACHABLE":
      return ApiWarning_SqlApiWarningCode.REGION_UNREACHABLE;
    case 2:
    case "MAX_RESULTS_EXCEEDS_LIMIT":
      return ApiWarning_SqlApiWarningCode.MAX_RESULTS_EXCEEDS_LIMIT;
    case 3:
    case "COMPROMISED_CREDENTIALS":
      return ApiWarning_SqlApiWarningCode.COMPROMISED_CREDENTIALS;
    case 4:
    case "INTERNAL_STATE_FAILURE":
      return ApiWarning_SqlApiWarningCode.INTERNAL_STATE_FAILURE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ApiWarning_SqlApiWarningCode.UNRECOGNIZED;
  }
}

export function apiWarning_SqlApiWarningCodeToJSON(object: ApiWarning_SqlApiWarningCode): string {
  switch (object) {
    case ApiWarning_SqlApiWarningCode.SQL_API_WARNING_CODE_UNSPECIFIED:
      return "SQL_API_WARNING_CODE_UNSPECIFIED";
    case ApiWarning_SqlApiWarningCode.REGION_UNREACHABLE:
      return "REGION_UNREACHABLE";
    case ApiWarning_SqlApiWarningCode.MAX_RESULTS_EXCEEDS_LIMIT:
      return "MAX_RESULTS_EXCEEDS_LIMIT";
    case ApiWarning_SqlApiWarningCode.COMPROMISED_CREDENTIALS:
      return "COMPROMISED_CREDENTIALS";
    case ApiWarning_SqlApiWarningCode.INTERNAL_STATE_FAILURE:
      return "INTERNAL_STATE_FAILURE";
    case ApiWarning_SqlApiWarningCode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * We currently only support backup retention by specifying the number
 * of backups we will retain.
 */
export interface BackupRetentionSettings {
  /** The unit that 'retained_backups' represents. */
  retentionUnit: BackupRetentionSettings_RetentionUnit;
  /**
   * Depending on the value of retention_unit, this is used to determine
   * if a backup needs to be deleted.  If retention_unit is 'COUNT', we will
   * retain this many backups.
   */
  retainedBackups: number | undefined;
}

/** The units that retained_backups specifies, we only support COUNT. */
export enum BackupRetentionSettings_RetentionUnit {
  /** RETENTION_UNIT_UNSPECIFIED - Backup retention unit is unspecified, will be treated as COUNT. */
  RETENTION_UNIT_UNSPECIFIED = 0,
  /** COUNT - Retention will be by count, eg. "retain the most recent 7 backups". */
  COUNT = 1,
  UNRECOGNIZED = -1,
}

export function backupRetentionSettings_RetentionUnitFromJSON(object: any): BackupRetentionSettings_RetentionUnit {
  switch (object) {
    case 0:
    case "RETENTION_UNIT_UNSPECIFIED":
      return BackupRetentionSettings_RetentionUnit.RETENTION_UNIT_UNSPECIFIED;
    case 1:
    case "COUNT":
      return BackupRetentionSettings_RetentionUnit.COUNT;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BackupRetentionSettings_RetentionUnit.UNRECOGNIZED;
  }
}

export function backupRetentionSettings_RetentionUnitToJSON(object: BackupRetentionSettings_RetentionUnit): string {
  switch (object) {
    case BackupRetentionSettings_RetentionUnit.RETENTION_UNIT_UNSPECIFIED:
      return "RETENTION_UNIT_UNSPECIFIED";
    case BackupRetentionSettings_RetentionUnit.COUNT:
      return "COUNT";
    case BackupRetentionSettings_RetentionUnit.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Database instance backup configuration. */
export interface BackupConfiguration {
  /**
   * Start time for the daily backup configuration in UTC timezone in the 24
   * hour format - `HH:MM`.
   */
  startTime: string;
  /** Whether this configuration is enabled. */
  enabled:
    | boolean
    | undefined;
  /** This is always `sql#backupConfiguration`. */
  kind: string;
  /**
   * (MySQL only) Whether binary log is enabled. If backup configuration is
   * disabled, binarylog must be disabled as well.
   */
  binaryLogEnabled:
    | boolean
    | undefined;
  /** Reserved for future use. */
  replicationLogArchivingEnabled:
    | boolean
    | undefined;
  /** Location of the backup */
  location: string;
  /** Whether point in time recovery is enabled. */
  pointInTimeRecoveryEnabled:
    | boolean
    | undefined;
  /**
   * The number of days of transaction logs we retain for point in time
   * restore, from 1-7.
   */
  transactionLogRetentionDays:
    | number
    | undefined;
  /** Backup retention settings. */
  backupRetentionSettings:
    | BackupRetentionSettings
    | undefined;
  /**
   * Output only. This value contains the storage location of transactional logs
   * for the database for point-in-time recovery.
   */
  transactionalLogStorageState?: BackupConfiguration_TransactionalLogStorageState | undefined;
}

/**
 * This value contains the storage location of the transactional logs
 * used to perform point-in-time recovery (PITR) for the database.
 */
export enum BackupConfiguration_TransactionalLogStorageState {
  /** TRANSACTIONAL_LOG_STORAGE_STATE_UNSPECIFIED - Unspecified. */
  TRANSACTIONAL_LOG_STORAGE_STATE_UNSPECIFIED = 0,
  /**
   * DISK - The transaction logs used for PITR for the instance are stored
   * on a data disk.
   */
  DISK = 1,
  /**
   * SWITCHING_TO_CLOUD_STORAGE - The transaction logs used for PITR for the instance are switching from
   * being stored on a data disk to being stored in Cloud Storage.
   * Only applicable to MySQL.
   */
  SWITCHING_TO_CLOUD_STORAGE = 2,
  /**
   * SWITCHED_TO_CLOUD_STORAGE - The transaction logs used for PITR for the instance are now stored
   * in Cloud Storage. Previously, they were stored on a data disk.
   * Only applicable to MySQL.
   */
  SWITCHED_TO_CLOUD_STORAGE = 3,
  /**
   * CLOUD_STORAGE - The transaction logs used for PITR for the instance are stored in
   * Cloud Storage. Only applicable to MySQL and PostgreSQL.
   */
  CLOUD_STORAGE = 4,
  UNRECOGNIZED = -1,
}

export function backupConfiguration_TransactionalLogStorageStateFromJSON(
  object: any,
): BackupConfiguration_TransactionalLogStorageState {
  switch (object) {
    case 0:
    case "TRANSACTIONAL_LOG_STORAGE_STATE_UNSPECIFIED":
      return BackupConfiguration_TransactionalLogStorageState.TRANSACTIONAL_LOG_STORAGE_STATE_UNSPECIFIED;
    case 1:
    case "DISK":
      return BackupConfiguration_TransactionalLogStorageState.DISK;
    case 2:
    case "SWITCHING_TO_CLOUD_STORAGE":
      return BackupConfiguration_TransactionalLogStorageState.SWITCHING_TO_CLOUD_STORAGE;
    case 3:
    case "SWITCHED_TO_CLOUD_STORAGE":
      return BackupConfiguration_TransactionalLogStorageState.SWITCHED_TO_CLOUD_STORAGE;
    case 4:
    case "CLOUD_STORAGE":
      return BackupConfiguration_TransactionalLogStorageState.CLOUD_STORAGE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BackupConfiguration_TransactionalLogStorageState.UNRECOGNIZED;
  }
}

export function backupConfiguration_TransactionalLogStorageStateToJSON(
  object: BackupConfiguration_TransactionalLogStorageState,
): string {
  switch (object) {
    case BackupConfiguration_TransactionalLogStorageState.TRANSACTIONAL_LOG_STORAGE_STATE_UNSPECIFIED:
      return "TRANSACTIONAL_LOG_STORAGE_STATE_UNSPECIFIED";
    case BackupConfiguration_TransactionalLogStorageState.DISK:
      return "DISK";
    case BackupConfiguration_TransactionalLogStorageState.SWITCHING_TO_CLOUD_STORAGE:
      return "SWITCHING_TO_CLOUD_STORAGE";
    case BackupConfiguration_TransactionalLogStorageState.SWITCHED_TO_CLOUD_STORAGE:
      return "SWITCHED_TO_CLOUD_STORAGE";
    case BackupConfiguration_TransactionalLogStorageState.CLOUD_STORAGE:
      return "CLOUD_STORAGE";
    case BackupConfiguration_TransactionalLogStorageState.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** A BackupRun resource. */
export interface BackupRun {
  /** This is always `sql#backupRun`. */
  kind: string;
  /** The status of this run. */
  status: SqlBackupRunStatus;
  /**
   * The time the run was enqueued in UTC timezone in
   * [RFC 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`.
   */
  enqueuedTime:
    | Date
    | undefined;
  /**
   * The identifier for this backup run. Unique only for a specific Cloud SQL
   * instance.
   */
  id: Long;
  /**
   * The time the backup operation actually started in UTC timezone in
   * [RFC 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`.
   */
  startTime:
    | Date
    | undefined;
  /**
   * The time the backup operation completed in UTC timezone in
   * [RFC 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`.
   */
  endTime:
    | Date
    | undefined;
  /**
   * Information about why the backup operation failed. This is only present if
   * the run has the FAILED status.
   */
  error:
    | OperationError
    | undefined;
  /**
   * The type of this run; can be either "AUTOMATED" or "ON_DEMAND" or "FINAL".
   * This field defaults to "ON_DEMAND" and is ignored, when specified for
   * insert requests.
   */
  type: SqlBackupRunType;
  /** The description of this run, only applicable to on-demand backups. */
  description: string;
  /**
   * The start time of the backup window during which this the backup was
   * attempted in [RFC 3339](https://tools.ietf.org/html/rfc3339) format, for
   * example `2012-11-15T16:19:00.094Z`.
   */
  windowStartTime:
    | Date
    | undefined;
  /** Name of the database instance. */
  instance: string;
  /** The URI of this resource. */
  selfLink: string;
  /** Location of the backups. */
  location: string;
  /** Encryption configuration specific to a backup. */
  diskEncryptionConfiguration:
    | DiskEncryptionConfiguration
    | undefined;
  /** Encryption status specific to a backup. */
  diskEncryptionStatus:
    | DiskEncryptionStatus
    | undefined;
  /** Specifies the kind of backup, PHYSICAL or DEFAULT_SNAPSHOT. */
  backupKind: SqlBackupKind;
  /**
   * Backup time zone to prevent restores to an instance with
   * a different time zone. Now relevant only for SQL Server.
   */
  timeZone: string;
}

/** Backup run list results. */
export interface BackupRunsListResponse {
  /** This is always `sql#backupRunsList`. */
  kind: string;
  /** A list of backup runs in reverse chronological order of the enqueued time. */
  items: BackupRun[];
  /**
   * The continuation token, used to page through large result sets. Provide
   * this value in a subsequent request to return the next page of results.
   */
  nextPageToken: string;
}

/** Binary log coordinates. */
export interface BinLogCoordinates {
  /** Name of the binary log file for a Cloud SQL instance. */
  binLogFileName: string;
  /** Position (offset) within the binary log file. */
  binLogPosition: Long;
  /** This is always `sql#binLogCoordinates`. */
  kind: string;
}

/** Backup context. */
export interface BackupContext {
  /** The identifier of the backup. */
  backupId: Long;
  /** This is always `sql#backupContext`. */
  kind: string;
}

/** Database instance clone context. */
export interface CloneContext {
  /** This is always `sql#cloneContext`. */
  kind: string;
  /** Reserved for future use. */
  pitrTimestampMs: Long;
  /** Name of the Cloud SQL instance to be created as a clone. */
  destinationInstanceName: string;
  /**
   * Binary log coordinates, if specified, identify the position up to which the
   * source instance is cloned. If not specified, the source instance is
   * cloned up to the most recent binary log coordinates.
   */
  binLogCoordinates:
    | BinLogCoordinates
    | undefined;
  /**
   * Timestamp, if specified, identifies the time to which the source instance
   * is cloned.
   */
  pointInTime:
    | Date
    | undefined;
  /**
   * The name of the allocated ip range for the private ip Cloud SQL instance.
   * For example: "google-managed-services-default". If set, the cloned instance
   * ip will be created in the allocated range. The range name must comply with
   * [RFC 1035](https://tools.ietf.org/html/rfc1035). Specifically, the name
   * must be 1-63 characters long and match the regular expression
   * [a-z]([-a-z0-9]*[a-z0-9])?.
   * Reserved for future use.
   */
  allocatedIpRange: string;
  /**
   * (SQL Server only) Clone only the specified databases from the source
   * instance. Clone all databases if empty.
   */
  databaseNames: string[];
  /**
   * Optional. Copy clone and point-in-time recovery clone of an instance to the
   * specified zone. If no zone is specified, clone to the same primary zone as
   * the source instance.
   */
  preferredZone?:
    | string
    | undefined;
  /**
   * Optional. Copy clone and point-in-time recovery clone of a regional
   * instance in the specified zones. If not specified, clone to the same
   * secondary zone as the source instance. This value cannot be the same as the
   * preferred_zone field.
   */
  preferredSecondaryZone?: string | undefined;
}

/** Represents a SQL database on the Cloud SQL instance. */
export interface Database {
  /** This is always `sql#database`. */
  kind: string;
  /** The Cloud SQL charset value. */
  charset: string;
  /** The Cloud SQL collation value. */
  collation: string;
  /**
   * This field is deprecated and will be removed from a future version of the
   * API.
   */
  etag: string;
  /**
   * The name of the database in the Cloud SQL instance. This does not include
   * the project ID or instance name.
   */
  name: string;
  /** The name of the Cloud SQL instance. This does not include the project ID. */
  instance: string;
  /** The URI of this resource. */
  selfLink: string;
  /**
   * The project ID of the project containing the Cloud SQL database. The Google
   * apps domain is prefixed if applicable.
   */
  project: string;
  sqlserverDatabaseDetails?: SqlServerDatabaseDetails | undefined;
}

/** Represents a Sql Server database on the Cloud SQL instance. */
export interface SqlServerDatabaseDetails {
  /** The version of SQL Server with which the database is to be made compatible */
  compatibilityLevel: number;
  /** The recovery model of a SQL Server database */
  recoveryModel: string;
}

/** Database flags for Cloud SQL instances. */
export interface DatabaseFlags {
  /**
   * The name of the flag. These flags are passed at instance startup, so
   * include both server options and system variables. Flags are
   * specified with underscores, not hyphens. For more information, see
   * [Configuring Database Flags](https://cloud.google.com/sql/docs/mysql/flags)
   * in the Cloud SQL documentation.
   */
  name: string;
  /**
   * The value of the flag. Boolean flags are set to `on` for true
   * and `off` for false. This field must be omitted if the flag
   * doesn't take a value.
   */
  value: string;
}

/**
 * Initial sync flags for certain Cloud SQL APIs.
 * Currently used for the MySQL external server initial dump.
 */
export interface SyncFlags {
  /** The name of the flag. */
  name: string;
  /**
   * The value of the flag. This field must be omitted if the flag
   * doesn't take a value.
   */
  value: string;
}

/** Reference to another Cloud SQL instance. */
export interface InstanceReference {
  /**
   * The name of the Cloud SQL instance being referenced.
   * This does not include the project ID.
   */
  name: string;
  /** The region of the Cloud SQL instance being referenced. */
  region: string;
  /**
   * The project ID of the Cloud SQL instance being referenced.
   * The default is the same project ID as the instance references it.
   */
  project: string;
}

/** A Cloud SQL instance resource. */
export interface DatabaseInstance {
  /** This is always `sql#instance`. */
  kind: string;
  /** The current serving state of the Cloud SQL instance. */
  state: DatabaseInstance_SqlInstanceState;
  /**
   * The database engine type and version. The `databaseVersion` field cannot
   * be changed after instance creation.
   */
  databaseVersion: SqlDatabaseVersion;
  /** The user settings. */
  settings:
    | Settings
    | undefined;
  /**
   * This field is deprecated and will be removed from a future version of the
   * API. Use the `settings.settingsVersion` field instead.
   */
  etag: string;
  /** The name and status of the failover replica. */
  failoverReplica:
    | DatabaseInstance_SqlFailoverReplica
    | undefined;
  /**
   * The name of the instance which will act as primary in the replication
   * setup.
   */
  masterInstanceName: string;
  /** The replicas of the instance. */
  replicaNames: string[];
  /**
   * The maximum disk size of the instance in bytes.
   *
   * @deprecated
   */
  maxDiskSize:
    | Long
    | undefined;
  /**
   * The current disk usage of the instance in bytes. This property has been
   * deprecated. Use the
   * "cloudsql.googleapis.com/database/disk/bytes_used" metric in Cloud
   * Monitoring API instead. Please see [this
   * announcement](https://groups.google.com/d/msg/google-cloud-sql-announce/I_7-F9EBhT0/BtvFtdFeAgAJ)
   * for details.
   *
   * @deprecated
   */
  currentDiskSize:
    | Long
    | undefined;
  /** The assigned IP addresses for the instance. */
  ipAddresses: IpMapping[];
  /** SSL configuration. */
  serverCaCert:
    | SslCert
    | undefined;
  /** The instance type. */
  instanceType: SqlInstanceType;
  /**
   * The project ID of the project containing the Cloud SQL instance. The Google
   * apps domain is prefixed if applicable.
   */
  project: string;
  /**
   * The IPv6 address assigned to the instance.
   * (Deprecated) This property was applicable only
   * to First Generation instances.
   *
   * @deprecated
   */
  ipv6Address: string;
  /**
   * The service account email address assigned to the instance. \This
   * property is read-only.
   */
  serviceAccountEmailAddress: string;
  /** Configuration specific to on-premises instances. */
  onPremisesConfiguration:
    | OnPremisesConfiguration
    | undefined;
  /** Configuration specific to failover replicas and read replicas. */
  replicaConfiguration:
    | ReplicaConfiguration
    | undefined;
  /**
   * The backend type.
   * `SECOND_GEN`: Cloud SQL database instance.
   * `EXTERNAL`: A database server that is not managed by Google.
   *
   * This property is read-only; use the `tier` property in the `settings`
   * object to determine the database type.
   */
  backendType: SqlBackendType;
  /** The URI of this resource. */
  selfLink: string;
  /** If the instance state is SUSPENDED, the reason for the suspension. */
  suspensionReason: SqlSuspensionReason[];
  /** Connection name of the Cloud SQL instance used in connection strings. */
  connectionName: string;
  /** Name of the Cloud SQL instance. This does not include the project ID. */
  name: string;
  /**
   * The geographical region of the Cloud SQL instance.
   *
   * It can be one of the
   * [regions](https://cloud.google.com/sql/docs/mysql/locations#location-r)
   * where Cloud SQL operates:
   *
   * For example,  `asia-east1`, `europe-west1`, and  `us-central1`.
   * The default value is `us-central1`.
   */
  region: string;
  /**
   * The Compute Engine zone that the instance is currently serving from. This
   * value could be different from the zone that was specified when the instance
   * was created if the instance has failed over to its secondary zone. WARNING:
   * Changing this might restart the instance.
   */
  gceZone: string;
  /**
   * The Compute Engine zone that the failover instance is currently serving
   * from for a regional instance. This value could be different
   * from the zone that was specified when the instance
   * was created if the instance has failed over to its secondary/failover zone.
   */
  secondaryGceZone: string;
  /** Disk encryption configuration specific to an instance. */
  diskEncryptionConfiguration:
    | DiskEncryptionConfiguration
    | undefined;
  /** Disk encryption status specific to an instance. */
  diskEncryptionStatus:
    | DiskEncryptionStatus
    | undefined;
  /**
   * Initial root password. Use only on creation. You must set root passwords
   * before you can connect to PostgreSQL instances.
   */
  rootPassword: string;
  /** The start time of any upcoming scheduled maintenance for this instance. */
  scheduledMaintenance:
    | DatabaseInstance_SqlScheduledMaintenance
    | undefined;
  /**
   * This status indicates whether the instance satisfies PZS.
   *
   * The status is reserved for future use.
   */
  satisfiesPzs:
    | boolean
    | undefined;
  /**
   * Output only. Stores the current database version running on the instance
   * including minor version such as `MYSQL_8_0_18`.
   */
  databaseInstalledVersion: string;
  /**
   * This field represents the report generated by the proactive database
   * wellness job for OutOfDisk issues.
   * *  Writers:
   *   *  the proactive database wellness job for OOD.
   * *  Readers:
   *   *  the proactive database wellness job
   */
  outOfDiskReport?:
    | DatabaseInstance_SqlOutOfDiskReport
    | undefined;
  /**
   * Output only. The time when the instance was created in
   * [RFC 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`.
   */
  createTime:
    | Date
    | undefined;
  /** Output only. List all maintenance versions applicable on the instance */
  availableMaintenanceVersions: string[];
  /** The current software version on the instance. */
  maintenanceVersion: string;
  /** Output only. All database versions that are available for upgrade. */
  upgradableDatabaseVersions: AvailableDatabaseVersion[];
  /** The SQL network architecture for the instance. */
  sqlNetworkArchitecture?:
    | DatabaseInstance_SqlNetworkArchitecture
    | undefined;
  /** Output only. The link to service attachment of PSC instance. */
  pscServiceAttachmentLink?:
    | string
    | undefined;
  /** Output only. The dns name of the instance. */
  dnsName?:
    | string
    | undefined;
  /**
   * Output only. DEPRECATED: please use write_endpoint instead.
   *
   * @deprecated
   */
  primaryDnsName?:
    | string
    | undefined;
  /** Output only. The dns name of the primary instance in a replication group. */
  writeEndpoint?:
    | string
    | undefined;
  /**
   * A primary instance and disaster recovery (DR) replica pair.
   * A DR replica is a cross-region replica that you designate
   * for failover in the event that the primary instance
   * experiences regional failure. Only applicable to MySQL.
   */
  replicationCluster?:
    | ReplicationCluster
    | undefined;
  /** Gemini instance configuration. */
  geminiConfig?: GeminiInstanceConfig | undefined;
}

/** The current serving state of the database instance. */
export enum DatabaseInstance_SqlInstanceState {
  /** SQL_INSTANCE_STATE_UNSPECIFIED - The state of the instance is unknown. */
  SQL_INSTANCE_STATE_UNSPECIFIED = 0,
  /** RUNNABLE - The instance is running, or has been stopped by owner. */
  RUNNABLE = 1,
  /** SUSPENDED - The instance is not available, for example due to problems with billing. */
  SUSPENDED = 2,
  /** PENDING_DELETE - The instance is being deleted. */
  PENDING_DELETE = 3,
  /** PENDING_CREATE - The instance is being created. */
  PENDING_CREATE = 4,
  /** MAINTENANCE - The instance is down for maintenance. */
  MAINTENANCE = 5,
  /**
   * FAILED - The creation of the instance failed or a fatal error occurred during
   * maintenance.
   */
  FAILED = 6,
  /**
   * ONLINE_MAINTENANCE - Deprecated
   *
   * @deprecated
   */
  ONLINE_MAINTENANCE = 7,
  UNRECOGNIZED = -1,
}

export function databaseInstance_SqlInstanceStateFromJSON(object: any): DatabaseInstance_SqlInstanceState {
  switch (object) {
    case 0:
    case "SQL_INSTANCE_STATE_UNSPECIFIED":
      return DatabaseInstance_SqlInstanceState.SQL_INSTANCE_STATE_UNSPECIFIED;
    case 1:
    case "RUNNABLE":
      return DatabaseInstance_SqlInstanceState.RUNNABLE;
    case 2:
    case "SUSPENDED":
      return DatabaseInstance_SqlInstanceState.SUSPENDED;
    case 3:
    case "PENDING_DELETE":
      return DatabaseInstance_SqlInstanceState.PENDING_DELETE;
    case 4:
    case "PENDING_CREATE":
      return DatabaseInstance_SqlInstanceState.PENDING_CREATE;
    case 5:
    case "MAINTENANCE":
      return DatabaseInstance_SqlInstanceState.MAINTENANCE;
    case 6:
    case "FAILED":
      return DatabaseInstance_SqlInstanceState.FAILED;
    case 7:
    case "ONLINE_MAINTENANCE":
      return DatabaseInstance_SqlInstanceState.ONLINE_MAINTENANCE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DatabaseInstance_SqlInstanceState.UNRECOGNIZED;
  }
}

export function databaseInstance_SqlInstanceStateToJSON(object: DatabaseInstance_SqlInstanceState): string {
  switch (object) {
    case DatabaseInstance_SqlInstanceState.SQL_INSTANCE_STATE_UNSPECIFIED:
      return "SQL_INSTANCE_STATE_UNSPECIFIED";
    case DatabaseInstance_SqlInstanceState.RUNNABLE:
      return "RUNNABLE";
    case DatabaseInstance_SqlInstanceState.SUSPENDED:
      return "SUSPENDED";
    case DatabaseInstance_SqlInstanceState.PENDING_DELETE:
      return "PENDING_DELETE";
    case DatabaseInstance_SqlInstanceState.PENDING_CREATE:
      return "PENDING_CREATE";
    case DatabaseInstance_SqlInstanceState.MAINTENANCE:
      return "MAINTENANCE";
    case DatabaseInstance_SqlInstanceState.FAILED:
      return "FAILED";
    case DatabaseInstance_SqlInstanceState.ONLINE_MAINTENANCE:
      return "ONLINE_MAINTENANCE";
    case DatabaseInstance_SqlInstanceState.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The SQL network architecture for the instance. */
export enum DatabaseInstance_SqlNetworkArchitecture {
  SQL_NETWORK_ARCHITECTURE_UNSPECIFIED = 0,
  /** NEW_NETWORK_ARCHITECTURE - The instance uses the new network architecture. */
  NEW_NETWORK_ARCHITECTURE = 1,
  /** OLD_NETWORK_ARCHITECTURE - The instance uses the old network architecture. */
  OLD_NETWORK_ARCHITECTURE = 2,
  UNRECOGNIZED = -1,
}

export function databaseInstance_SqlNetworkArchitectureFromJSON(object: any): DatabaseInstance_SqlNetworkArchitecture {
  switch (object) {
    case 0:
    case "SQL_NETWORK_ARCHITECTURE_UNSPECIFIED":
      return DatabaseInstance_SqlNetworkArchitecture.SQL_NETWORK_ARCHITECTURE_UNSPECIFIED;
    case 1:
    case "NEW_NETWORK_ARCHITECTURE":
      return DatabaseInstance_SqlNetworkArchitecture.NEW_NETWORK_ARCHITECTURE;
    case 2:
    case "OLD_NETWORK_ARCHITECTURE":
      return DatabaseInstance_SqlNetworkArchitecture.OLD_NETWORK_ARCHITECTURE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DatabaseInstance_SqlNetworkArchitecture.UNRECOGNIZED;
  }
}

export function databaseInstance_SqlNetworkArchitectureToJSON(object: DatabaseInstance_SqlNetworkArchitecture): string {
  switch (object) {
    case DatabaseInstance_SqlNetworkArchitecture.SQL_NETWORK_ARCHITECTURE_UNSPECIFIED:
      return "SQL_NETWORK_ARCHITECTURE_UNSPECIFIED";
    case DatabaseInstance_SqlNetworkArchitecture.NEW_NETWORK_ARCHITECTURE:
      return "NEW_NETWORK_ARCHITECTURE";
    case DatabaseInstance_SqlNetworkArchitecture.OLD_NETWORK_ARCHITECTURE:
      return "OLD_NETWORK_ARCHITECTURE";
    case DatabaseInstance_SqlNetworkArchitecture.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export interface DatabaseInstance_SqlFailoverReplica {
  /**
   * The name of the failover replica. If specified at instance creation, a
   * failover replica is created for the instance. The name
   * doesn't include the project ID.
   */
  name: string;
  /**
   * The availability status of the failover replica. A false status indicates
   * that the failover replica is out of sync. The primary instance can only
   * failover to the failover replica when the status is true.
   */
  available: boolean | undefined;
}

/** Any scheduled maintenance for this instance. */
export interface DatabaseInstance_SqlScheduledMaintenance {
  /** The start time of any upcoming scheduled maintenance for this instance. */
  startTime:
    | Date
    | undefined;
  /** @deprecated */
  canDefer: boolean;
  /** If the scheduled maintenance can be rescheduled. */
  canReschedule: boolean;
  /** Maintenance cannot be rescheduled to start beyond this deadline. */
  scheduleDeadlineTime?: Date | undefined;
}

/** This message wraps up the information written by out-of-disk detection job. */
export interface DatabaseInstance_SqlOutOfDiskReport {
  /**
   * This field represents the state generated by the proactive database
   * wellness job for OutOfDisk issues.
   * *  Writers:
   *   *  the proactive database wellness job for OOD.
   * *  Readers:
   *   *  the proactive database wellness job
   */
  sqlOutOfDiskState?:
    | DatabaseInstance_SqlOutOfDiskReport_SqlOutOfDiskState
    | undefined;
  /**
   * The minimum recommended increase size in GigaBytes
   * This field is consumed by the frontend
   * *  Writers:
   *   *  the proactive database wellness job for OOD.
   * *  Readers:
   */
  sqlMinRecommendedIncreaseSizeGb?: number | undefined;
}

/** This enum lists all possible states regarding out-of-disk issues. */
export enum DatabaseInstance_SqlOutOfDiskReport_SqlOutOfDiskState {
  /** SQL_OUT_OF_DISK_STATE_UNSPECIFIED - Unspecified state */
  SQL_OUT_OF_DISK_STATE_UNSPECIFIED = 0,
  /** NORMAL - The instance has plenty space on data disk */
  NORMAL = 1,
  /**
   * SOFT_SHUTDOWN - Data disk is almost used up. It is shutdown to prevent data
   * corruption.
   */
  SOFT_SHUTDOWN = 2,
  UNRECOGNIZED = -1,
}

export function databaseInstance_SqlOutOfDiskReport_SqlOutOfDiskStateFromJSON(
  object: any,
): DatabaseInstance_SqlOutOfDiskReport_SqlOutOfDiskState {
  switch (object) {
    case 0:
    case "SQL_OUT_OF_DISK_STATE_UNSPECIFIED":
      return DatabaseInstance_SqlOutOfDiskReport_SqlOutOfDiskState.SQL_OUT_OF_DISK_STATE_UNSPECIFIED;
    case 1:
    case "NORMAL":
      return DatabaseInstance_SqlOutOfDiskReport_SqlOutOfDiskState.NORMAL;
    case 2:
    case "SOFT_SHUTDOWN":
      return DatabaseInstance_SqlOutOfDiskReport_SqlOutOfDiskState.SOFT_SHUTDOWN;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DatabaseInstance_SqlOutOfDiskReport_SqlOutOfDiskState.UNRECOGNIZED;
  }
}

export function databaseInstance_SqlOutOfDiskReport_SqlOutOfDiskStateToJSON(
  object: DatabaseInstance_SqlOutOfDiskReport_SqlOutOfDiskState,
): string {
  switch (object) {
    case DatabaseInstance_SqlOutOfDiskReport_SqlOutOfDiskState.SQL_OUT_OF_DISK_STATE_UNSPECIFIED:
      return "SQL_OUT_OF_DISK_STATE_UNSPECIFIED";
    case DatabaseInstance_SqlOutOfDiskReport_SqlOutOfDiskState.NORMAL:
      return "NORMAL";
    case DatabaseInstance_SqlOutOfDiskReport_SqlOutOfDiskState.SOFT_SHUTDOWN:
      return "SOFT_SHUTDOWN";
    case DatabaseInstance_SqlOutOfDiskReport_SqlOutOfDiskState.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Gemini instance configuration. */
export interface GeminiInstanceConfig {
  /** Output only. Whether Gemini is enabled. */
  entitled?:
    | boolean
    | undefined;
  /** Output only. Whether the vacuum management is enabled. */
  googleVacuumMgmtEnabled?:
    | boolean
    | undefined;
  /** Output only. Whether canceling the out-of-memory (OOM) session is enabled. */
  oomSessionCancelEnabled?:
    | boolean
    | undefined;
  /** Output only. Whether the active query is enabled. */
  activeQueryEnabled?:
    | boolean
    | undefined;
  /** Output only. Whether the index advisor is enabled. */
  indexAdvisorEnabled?:
    | boolean
    | undefined;
  /** Output only. Whether the flag recommender is enabled. */
  flagRecommenderEnabled?: boolean | undefined;
}

/**
 * A primary instance and disaster recovery (DR) replica pair.
 * A DR replica is a cross-region replica that you designate for failover
 * in the event that the primary instance has regional failure.
 * Only applicable to MySQL.
 */
export interface ReplicationCluster {
  /**
   * Output only. If set, it indicates this instance has a private service
   * access (PSA) dns endpoint that is pointing to the primary instance of the
   * cluster. If this instance is the primary, the dns should be pointing to
   * this instance. After Switchover or Replica failover, this DNS endpoint
   * points to the promoted instance. This is a read-only field, returned to the
   * user as information. This field can exist even if a standalone instance
   * does not yet have a replica, or had a DR replica that was deleted.
   */
  psaWriteEndpoint?:
    | string
    | undefined;
  /**
   * Optional. If the instance is a primary instance, then this field identifies
   * the disaster recovery (DR) replica. A DR replica is an optional
   * configuration for Enterprise Plus edition instances. If the instance is a
   * read replica, then the field is not set. Set this field to a replica name
   * to designate a DR replica for a primary instance. Remove the replica name
   * to remove the DR replica designation.
   */
  failoverDrReplicaName?:
    | string
    | undefined;
  /**
   * Output only. Read-only field that indicates whether the replica is a DR
   * replica. This field is not set if the instance is a primary instance.
   */
  drReplica?: boolean | undefined;
}

/** An available database version. It can be a major or a minor version. */
export interface AvailableDatabaseVersion {
  /** The version's major version name. */
  majorVersion?:
    | string
    | undefined;
  /**
   * The database version name. For MySQL 8.0, this string provides the database
   * major and minor version.
   */
  name?:
    | string
    | undefined;
  /** The database version's display name. */
  displayName?: string | undefined;
}

/** Database list response. */
export interface DatabasesListResponse {
  /** This is always `sql#databasesList`. */
  kind: string;
  /** List of database resources in the instance. */
  items: Database[];
}

/**
 * Read-replica configuration for connecting to the on-premises primary
 * instance.
 */
export interface DemoteMasterConfiguration {
  /** This is always `sql#demoteMasterConfiguration`. */
  kind: string;
  /**
   * MySQL specific configuration when replicating from a MySQL on-premises
   * primary instance. Replication configuration information such as the
   * username, password, certificates, and keys are not stored in the instance
   * metadata. The configuration information is used only to set up the
   * replication connection and is stored by MySQL in a file named
   * `master.info` in the data directory.
   */
  mysqlReplicaConfiguration: DemoteMasterMySqlReplicaConfiguration | undefined;
}

/** Database instance demote primary instance context. */
export interface DemoteMasterContext {
  /** This is always `sql#demoteMasterContext`. */
  kind: string;
  /**
   * Verify the GTID consistency for demote operation. Default value:
   * `True`. Setting this flag to `false` enables you to bypass the GTID
   * consistency check between on-premises primary instance and Cloud SQL
   * instance during the demotion operation but also exposes you to the risk of
   * future replication failures. Change the value only if you know the reason
   * for the GTID divergence and are confident that doing so will not cause any
   * replication issues.
   */
  verifyGtidConsistency:
    | boolean
    | undefined;
  /**
   * The name of the instance which will act as on-premises primary instance
   * in the replication setup.
   */
  masterInstanceName: string;
  /**
   * Configuration specific to read-replicas replicating from the on-premises
   * primary instance.
   */
  replicaConfiguration:
    | DemoteMasterConfiguration
    | undefined;
  /** Flag to skip replication setup on the instance. */
  skipReplicationSetup: boolean;
}

/** Read-replica configuration specific to MySQL databases. */
export interface DemoteMasterMySqlReplicaConfiguration {
  /** This is always `sql#demoteMasterMysqlReplicaConfiguration`. */
  kind: string;
  /** The username for the replication connection. */
  username: string;
  /** The password for the replication connection. */
  password: string;
  /**
   * PEM representation of the replica's private key. The corresponsing public
   * key is encoded in the client's certificate. The format of the replica's
   * private key can be either PKCS #1 or PKCS #8.
   */
  clientKey: string;
  /** PEM representation of the replica's x509 certificate. */
  clientCertificate: string;
  /** PEM representation of the trusted CA's x509 certificate. */
  caCertificate: string;
}

/**
 * This context is used to demote an existing standalone instance to be
 * a Cloud SQL read replica for an external database server.
 */
export interface DemoteContext {
  /** This is always `sql#demoteContext`. */
  kind: string;
  /**
   * Required. The name of the instance which acts as an on-premises primary
   * instance in the replication setup.
   */
  sourceRepresentativeInstanceName: string;
}

/** Database instance export context. */
export interface ExportContext {
  /**
   * The path to the file in Google Cloud Storage where the export will be
   * stored. The URI is in the form `gs://bucketName/fileName`. If the file
   * already exists, the request succeeds, but the operation fails. If
   * `fileType` is `SQL` and the filename ends with .gz,
   * the contents are compressed.
   */
  uri: string;
  /**
   * Databases to be exported. <br /> `MySQL instances:` If
   * `fileType` is `SQL` and no database is specified, all
   * databases are exported, except for the `mysql` system database.
   * If `fileType` is `CSV`, you can specify one database,
   * either by using this property or by using the
   * `csvExportOptions.selectQuery` property, which takes precedence
   * over this property. <br /> `PostgreSQL instances:` You must specify
   * one database to be exported. If `fileType` is `CSV`,
   * this database must match the one specified in the
   * `csvExportOptions.selectQuery` property. <br /> `SQL Server
   * instances:` You must specify one database to be exported, and the
   * `fileType` must be `BAK`.
   */
  databases: string[];
  /** This is always `sql#exportContext`. */
  kind: string;
  /** Options for exporting data as SQL statements. */
  sqlExportOptions:
    | ExportContext_SqlExportOptions
    | undefined;
  /**
   * Options for exporting data as CSV. `MySQL` and `PostgreSQL`
   * instances only.
   */
  csvExportOptions:
    | ExportContext_SqlCsvExportOptions
    | undefined;
  /** The file type for the specified uri. */
  fileType: SqlFileType;
  /** Option for export offload. */
  offload:
    | boolean
    | undefined;
  /** Options for exporting data as BAK files. */
  bakExportOptions: ExportContext_SqlBakExportOptions | undefined;
}

export interface ExportContext_SqlCsvExportOptions {
  /** The select query used to extract the data. */
  selectQuery: string;
  /**
   * Specifies the character that should appear before a data character that
   * needs to be escaped.
   */
  escapeCharacter: string;
  /** Specifies the quoting character to be used when a data value is quoted. */
  quoteCharacter: string;
  /**
   * Specifies the character that separates columns within each row (line) of
   * the file.
   */
  fieldsTerminatedBy: string;
  /**
   * This is used to separate lines. If a line does not contain all fields,
   * the rest of the columns are set to their default values.
   */
  linesTerminatedBy: string;
}

export interface ExportContext_SqlExportOptions {
  /**
   * Tables to export, or that were exported, from the specified database. If
   * you specify tables, specify one and only one database. For PostgreSQL
   * instances, you can specify only one table.
   */
  tables: string[];
  /** Export only schemas. */
  schemaOnly: boolean | undefined;
  mysqlExportOptions:
    | ExportContext_SqlExportOptions_MysqlExportOptions
    | undefined;
  /** Optional. The number of threads to use for parallel export. */
  threads:
    | number
    | undefined;
  /** Optional. Whether or not the export should be parallel. */
  parallel: boolean | undefined;
}

/** Options for exporting from MySQL. */
export interface ExportContext_SqlExportOptions_MysqlExportOptions {
  /**
   * Option to include SQL statement required to set up replication. If set
   * to `1`, the dump file includes a CHANGE MASTER TO statement with the
   * binary log coordinates, and --set-gtid-purged is set to ON. If set to
   * `2`, the CHANGE MASTER TO statement is written as a SQL comment and
   * has no effect. If set to any value other than `1`, --set-gtid-purged
   * is set to OFF.
   */
  masterData: number | undefined;
}

/** Options for exporting BAK files (SQL Server-only) */
export interface ExportContext_SqlBakExportOptions {
  /** Whether or not the export should be striped. */
  striped:
    | boolean
    | undefined;
  /**
   * Option for specifying how many stripes to use for the export.
   * If blank, and the value of the striped field is true,
   * the number of stripes is automatically chosen.
   */
  stripeCount:
    | number
    | undefined;
  /** Type of this bak file will be export, FULL or DIFF, SQL Server only */
  bakType: BakType;
  /**
   * Deprecated: copy_only is deprecated. Use differential_base instead
   *
   * @deprecated
   */
  copyOnly:
    | boolean
    | undefined;
  /**
   * Whether or not the backup can be used as a differential base
   * copy_only backup can not be served as differential base
   */
  differentialBase: boolean | undefined;
}

/** Database instance failover context. */
export interface FailoverContext {
  /**
   * The current settings version of this instance. Request will be rejected if
   * this version doesn't match the current settings version.
   */
  settingsVersion: Long;
  /** This is always `sql#failoverContext`. */
  kind: string;
}

/** A flag resource. */
export interface Flag {
  /**
   * This is the name of the flag. Flag names always use underscores, not
   * hyphens, for example: `max_allowed_packet`
   */
  name: string;
  /**
   * The type of the flag. Flags are typed to being `BOOLEAN`, `STRING`,
   * `INTEGER` or `NONE`. `NONE` is used for flags which do not take a
   * value, such as `skip_grant_tables`.
   */
  type: SqlFlagType;
  /**
   * The database version this flag applies to. Can be
   * MySQL instances: `MYSQL_8_0`, `MYSQL_8_0_18`, `MYSQL_8_0_26`, `MYSQL_5_7`,
   * or `MYSQL_5_6`. PostgreSQL instances: `POSTGRES_9_6`, `POSTGRES_10`,
   * `POSTGRES_11` or `POSTGRES_12`. SQL Server instances:
   * `SQLSERVER_2017_STANDARD`, `SQLSERVER_2017_ENTERPRISE`,
   * `SQLSERVER_2017_EXPRESS`, `SQLSERVER_2017_WEB`, `SQLSERVER_2019_STANDARD`,
   * `SQLSERVER_2019_ENTERPRISE`, `SQLSERVER_2019_EXPRESS`, or
   * `SQLSERVER_2019_WEB`.
   * See [the complete
   * list](/sql/docs/mysql/admin-api/rest/v1/SqlDatabaseVersion).
   */
  appliesTo: SqlDatabaseVersion[];
  /** For `STRING` flags, a list of strings that the value can be set to. */
  allowedStringValues: string[];
  /** For `INTEGER` flags, the minimum allowed value. */
  minValue:
    | Long
    | undefined;
  /** For `INTEGER` flags, the maximum allowed value. */
  maxValue:
    | Long
    | undefined;
  /**
   * Indicates whether changing this flag will trigger a database restart. Only
   * applicable to Second Generation instances.
   */
  requiresRestart:
    | boolean
    | undefined;
  /** This is always `sql#flag`. */
  kind: string;
  /** Whether or not the flag is considered in beta. */
  inBeta:
    | boolean
    | undefined;
  /**
   * Use this field if only certain integers are accepted. Can be combined
   * with min_value and max_value to add additional values.
   */
  allowedIntValues: Long[];
}

/** Flags list response. */
export interface FlagsListResponse {
  /** This is always `sql#flagsList`. */
  kind: string;
  /** List of flags. */
  items: Flag[];
}

/** Database instance import context. */
export interface ImportContext {
  /**
   * Path to the import file in Cloud Storage, in the form
   * `gs://bucketName/fileName`. Compressed gzip files (.gz) are supported
   * when `fileType` is `SQL`. The instance must have
   * write permissions to the bucket and read access to the file.
   */
  uri: string;
  /**
   * The target database for the import. If `fileType` is `SQL`, this field
   * is required only if the import file does not specify a database, and is
   * overridden by any database specification in the import file. If
   * `fileType` is `CSV`, one database must be specified.
   */
  database: string;
  /** This is always `sql#importContext`. */
  kind: string;
  /**
   * The file type for the specified uri.
   * *  `SQL`: The file contains SQL statements.
   * *  `CSV`: The file contains CSV data.
   * *  `BAK`: The file contains backup data for a SQL Server instance.
   */
  fileType: SqlFileType;
  /** Options for importing data as CSV. */
  csvImportOptions:
    | ImportContext_SqlCsvImportOptions
    | undefined;
  /** The PostgreSQL user for this import operation. PostgreSQL instances only. */
  importUser: string;
  /** Import parameters specific to SQL Server .BAK files */
  bakImportOptions:
    | ImportContext_SqlBakImportOptions
    | undefined;
  /** Optional. Options for importing data from SQL statements. */
  sqlImportOptions: ImportContext_SqlImportOptions | undefined;
}

export interface ImportContext_SqlImportOptions {
  /** Optional. The number of threads to use for parallel import. */
  threads:
    | number
    | undefined;
  /** Optional. Whether or not the import should be parallel. */
  parallel: boolean | undefined;
}

export interface ImportContext_SqlCsvImportOptions {
  /** The table to which CSV data is imported. */
  table: string;
  /**
   * The columns to which CSV data is imported. If not specified, all columns
   * of the database table are loaded with CSV data.
   */
  columns: string[];
  /**
   * Specifies the character that should appear before a data character that
   * needs to be escaped.
   */
  escapeCharacter: string;
  /** Specifies the quoting character to be used when a data value is quoted. */
  quoteCharacter: string;
  /**
   * Specifies the character that separates columns within each row (line) of
   * the file.
   */
  fieldsTerminatedBy: string;
  /**
   * This is used to separate lines. If a line does not contain all fields,
   * the rest of the columns are set to their default values.
   */
  linesTerminatedBy: string;
}

export interface ImportContext_SqlBakImportOptions {
  encryptionOptions:
    | ImportContext_SqlBakImportOptions_EncryptionOptions
    | undefined;
  /**
   * Whether or not the backup set being restored is striped.
   * Applies only to Cloud SQL for SQL Server.
   */
  striped:
    | boolean
    | undefined;
  /**
   * Whether or not the backup importing will restore database
   * with NORECOVERY option
   * Applies only to Cloud SQL for SQL Server.
   */
  noRecovery:
    | boolean
    | undefined;
  /**
   * Whether or not the backup importing request will just bring database
   * online without downloading Bak content only one of "no_recovery" and
   * "recovery_only" can be true otherwise error will return. Applies only to
   * Cloud SQL for SQL Server.
   */
  recoveryOnly:
    | boolean
    | undefined;
  /** Type of the bak content, FULL or DIFF. */
  bakType: BakType;
  /**
   * Optional. The timestamp when the import should stop. This timestamp is in
   * the [RFC 3339](https://tools.ietf.org/html/rfc3339) format (for example,
   * `2023-10-01T16:19:00.094`). This field is equivalent to the STOPAT
   * keyword and applies to Cloud SQL for SQL Server only.
   */
  stopAt:
    | Date
    | undefined;
  /**
   * Optional. The marked transaction where the import should stop. This field
   * is equivalent to the STOPATMARK keyword and applies to Cloud SQL for SQL
   * Server only.
   */
  stopAtMark: string;
}

export interface ImportContext_SqlBakImportOptions_EncryptionOptions {
  /**
   * Path to the Certificate (.cer) in Cloud Storage, in the form
   * `gs://bucketName/fileName`. The instance must have write permissions
   * to the bucket and read access to the file.
   */
  certPath: string;
  /**
   * Path to the Certificate Private Key (.pvk)  in Cloud Storage, in the
   * form `gs://bucketName/fileName`. The instance must have write
   * permissions to the bucket and read access to the file.
   */
  pvkPath: string;
  /** Password that encrypts the private key */
  pvkPassword: string;
}

/** Database instance clone request. */
export interface InstancesCloneRequest {
  /** Contains details about the clone operation. */
  cloneContext: CloneContext | undefined;
}

/** Database demote primary instance request. */
export interface InstancesDemoteMasterRequest {
  /** Contains details about the demoteMaster operation. */
  demoteMasterContext: DemoteMasterContext | undefined;
}

/**
 * This request is used to demote an existing standalone instance to be a
 * Cloud SQL read replica for an external database server.
 */
export interface InstancesDemoteRequest {
  /**
   * Required. This context is used to demote an existing standalone instance to
   * be a Cloud SQL read replica for an external database server.
   */
  demoteContext: DemoteContext | undefined;
}

/** Database instance export request. */
export interface InstancesExportRequest {
  /** Contains details about the export operation. */
  exportContext: ExportContext | undefined;
}

/** Instance failover request. */
export interface InstancesFailoverRequest {
  /** Failover Context. */
  failoverContext: FailoverContext | undefined;
}

/** Database instance import request. */
export interface InstancesImportRequest {
  /** Contains details about the import operation. */
  importContext: ImportContext | undefined;
}

/** MySQL-specific external server sync settings. */
export interface MySqlSyncConfig {
  /** Flags to use for the initial dump. */
  initialSyncFlags: SyncFlags[];
}

/** Database instances list response. */
export interface InstancesListResponse {
  /** This is always `sql#instancesList`. */
  kind: string;
  /** List of warnings that occurred while handling the request. */
  warnings: ApiWarning[];
  /** List of database instance resources. */
  items: DatabaseInstance[];
  /**
   * The continuation token, used to page through large result sets. Provide
   * this value in a subsequent request to return the next page of results.
   */
  nextPageToken: string;
}

/** Instances ListServerCas response. */
export interface InstancesListServerCasResponse {
  /** List of server CA certificates for the instance. */
  certs: SslCert[];
  activeVersion: string;
  /** This is always `sql#instancesListServerCas`. */
  kind: string;
}

/** Database instance restore backup request. */
export interface InstancesRestoreBackupRequest {
  /** Parameters required to perform the restore backup operation. */
  restoreBackupContext: RestoreBackupContext | undefined;
}

/** Rotate Server CA request. */
export interface InstancesRotateServerCaRequest {
  /** Contains details about the rotate server CA operation. */
  rotateServerCaContext: RotateServerCaContext | undefined;
}

/** Instance truncate log request. */
export interface InstancesTruncateLogRequest {
  /** Contains details about the truncate log operation. */
  truncateLogContext: TruncateLogContext | undefined;
}

/** Request to acquire an SSRS lease for an instance. */
export interface InstancesAcquireSsrsLeaseRequest {
  /** Contains details about the acquire SSRS lease operation. */
  acquireSsrsLeaseContext: AcquireSsrsLeaseContext | undefined;
}

/** Perform disk shrink context. */
export interface PerformDiskShrinkContext {
  /** The target disk shrink size in GigaBytes. */
  targetSizeGb: Long;
}

/** Instance get disk shrink config response. */
export interface SqlInstancesGetDiskShrinkConfigResponse {
  /** This is always `sql#getDiskShrinkConfig`. */
  kind: string;
  /** The minimum size to which a disk can be shrunk in GigaBytes. */
  minimalTargetSizeGb: Long;
  /** Additional message to customers. */
  message: string;
}

/** Instance verify external sync settings response. */
export interface SqlInstancesVerifyExternalSyncSettingsResponse {
  /** This is always `sql#migrationSettingErrorList`. */
  kind: string;
  /** List of migration violations. */
  errors: SqlExternalSyncSettingError[];
  /** List of migration warnings. */
  warnings: SqlExternalSyncSettingError[];
}

/** External primary instance migration setting error/warning. */
export interface SqlExternalSyncSettingError {
  /**
   * Can be `sql#externalSyncSettingError` or
   * `sql#externalSyncSettingWarning`.
   */
  kind: string;
  /** Identifies the specific error that occurred. */
  type: SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType;
  /** Additional information about the error encountered. */
  detail: string;
}

export enum SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType {
  SQL_EXTERNAL_SYNC_SETTING_ERROR_TYPE_UNSPECIFIED = 0,
  CONNECTION_FAILURE = 1,
  BINLOG_NOT_ENABLED = 2,
  INCOMPATIBLE_DATABASE_VERSION = 3,
  REPLICA_ALREADY_SETUP = 4,
  /** INSUFFICIENT_PRIVILEGE - The replication user is missing privileges that are required. */
  INSUFFICIENT_PRIVILEGE = 5,
  /** UNSUPPORTED_MIGRATION_TYPE - Unsupported migration type. */
  UNSUPPORTED_MIGRATION_TYPE = 6,
  /** NO_PGLOGICAL_INSTALLED - No pglogical extension installed on databases, applicable for postgres. */
  NO_PGLOGICAL_INSTALLED = 7,
  /** PGLOGICAL_NODE_ALREADY_EXISTS - pglogical node already exists on databases, applicable for postgres. */
  PGLOGICAL_NODE_ALREADY_EXISTS = 8,
  /** INVALID_WAL_LEVEL - The value of parameter wal_level is not set to logical. */
  INVALID_WAL_LEVEL = 9,
  /**
   * INVALID_SHARED_PRELOAD_LIBRARY - The value of parameter shared_preload_libraries does not include
   * pglogical.
   */
  INVALID_SHARED_PRELOAD_LIBRARY = 10,
  /** INSUFFICIENT_MAX_REPLICATION_SLOTS - The value of parameter max_replication_slots is not sufficient. */
  INSUFFICIENT_MAX_REPLICATION_SLOTS = 11,
  /** INSUFFICIENT_MAX_WAL_SENDERS - The value of parameter max_wal_senders is not sufficient. */
  INSUFFICIENT_MAX_WAL_SENDERS = 12,
  /** INSUFFICIENT_MAX_WORKER_PROCESSES - The value of parameter max_worker_processes is not sufficient. */
  INSUFFICIENT_MAX_WORKER_PROCESSES = 13,
  /**
   * UNSUPPORTED_EXTENSIONS - Extensions installed are either not supported or having unsupported
   * versions
   */
  UNSUPPORTED_EXTENSIONS = 14,
  /** INVALID_RDS_LOGICAL_REPLICATION - The value of parameter rds.logical_replication is not set to 1. */
  INVALID_RDS_LOGICAL_REPLICATION = 15,
  /** INVALID_LOGGING_SETUP - The primary instance logging setup doesn't allow EM sync. */
  INVALID_LOGGING_SETUP = 16,
  /** INVALID_DB_PARAM - The primary instance database parameter setup doesn't allow EM sync. */
  INVALID_DB_PARAM = 17,
  /** UNSUPPORTED_GTID_MODE - The gtid_mode is not supported, applicable for MySQL. */
  UNSUPPORTED_GTID_MODE = 18,
  /** SQLSERVER_AGENT_NOT_RUNNING - SQL Server Agent is not running. */
  SQLSERVER_AGENT_NOT_RUNNING = 19,
  /**
   * UNSUPPORTED_TABLE_DEFINITION - The table definition is not support due to missing primary key or replica
   * identity, applicable for postgres.
   */
  UNSUPPORTED_TABLE_DEFINITION = 20,
  /** UNSUPPORTED_DEFINER - The customer has a definer that will break EM setup. */
  UNSUPPORTED_DEFINER = 21,
  /** SQLSERVER_SERVERNAME_MISMATCH - SQL Server @@SERVERNAME does not match actual host name. */
  SQLSERVER_SERVERNAME_MISMATCH = 22,
  /** PRIMARY_ALREADY_SETUP - The primary instance has been setup and will fail the setup. */
  PRIMARY_ALREADY_SETUP = 23,
  /** UNSUPPORTED_BINLOG_FORMAT - The primary instance has unsupported binary log format. */
  UNSUPPORTED_BINLOG_FORMAT = 24,
  /** BINLOG_RETENTION_SETTING - The primary instance's binary log retention setting. */
  BINLOG_RETENTION_SETTING = 25,
  /** UNSUPPORTED_STORAGE_ENGINE - The primary instance has tables with unsupported storage engine. */
  UNSUPPORTED_STORAGE_ENGINE = 26,
  /**
   * LIMITED_SUPPORT_TABLES - Source has tables with limited support
   * eg: PostgreSQL tables without primary keys.
   */
  LIMITED_SUPPORT_TABLES = 27,
  /** EXISTING_DATA_IN_REPLICA - The replica instance contains existing data. */
  EXISTING_DATA_IN_REPLICA = 28,
  /** MISSING_OPTIONAL_PRIVILEGES - The replication user is missing privileges that are optional. */
  MISSING_OPTIONAL_PRIVILEGES = 29,
  /**
   * RISKY_BACKUP_ADMIN_PRIVILEGE - Additional BACKUP_ADMIN privilege is granted to the replication user
   * which may lock source MySQL 8 instance for DDLs during initial sync.
   */
  RISKY_BACKUP_ADMIN_PRIVILEGE = 30,
  /** INSUFFICIENT_GCS_PERMISSIONS - The Cloud Storage bucket is missing necessary permissions. */
  INSUFFICIENT_GCS_PERMISSIONS = 31,
  /**
   * INVALID_FILE_INFO - The Cloud Storage bucket has an error in the file or contains invalid
   * file information.
   */
  INVALID_FILE_INFO = 32,
  /** UNSUPPORTED_DATABASE_SETTINGS - The source instance has unsupported database settings for migration. */
  UNSUPPORTED_DATABASE_SETTINGS = 33,
  /**
   * MYSQL_PARALLEL_IMPORT_INSUFFICIENT_PRIVILEGE - The replication user is missing parallel import specific privileges.
   * (e.g. LOCK TABLES) for MySQL.
   */
  MYSQL_PARALLEL_IMPORT_INSUFFICIENT_PRIVILEGE = 34,
  /** LOCAL_INFILE_OFF - The global variable local_infile is off on external server replica. */
  LOCAL_INFILE_OFF = 35,
  /**
   * TURN_ON_PITR_AFTER_PROMOTE - This code instructs customers to turn on point-in-time recovery manually
   * for the instance after promoting the Cloud SQL for PostgreSQL instance.
   */
  TURN_ON_PITR_AFTER_PROMOTE = 36,
  /** INCOMPATIBLE_DATABASE_MINOR_VERSION - The minor version of replica database is incompatible with the source. */
  INCOMPATIBLE_DATABASE_MINOR_VERSION = 37,
  /**
   * SOURCE_MAX_SUBSCRIPTIONS - This warning message indicates that Cloud SQL uses the maximum number of
   * subscriptions to migrate data from the source to the destination.
   */
  SOURCE_MAX_SUBSCRIPTIONS = 38,
  /** UNABLE_TO_VERIFY_DEFINERS - Unable to verify definers on the source for MySQL. */
  UNABLE_TO_VERIFY_DEFINERS = 39,
  /**
   * SUBSCRIPTION_CALCULATION_STATUS - If a time out occurs while the subscription counts are calculated, then
   * this value is set to 1. Otherwise, this value is set to 2.
   */
  SUBSCRIPTION_CALCULATION_STATUS = 40,
  /**
   * PG_SUBSCRIPTION_COUNT - Count of subscriptions needed to sync source data for PostgreSQL
   * database.
   */
  PG_SUBSCRIPTION_COUNT = 41,
  /** PG_SYNC_PARALLEL_LEVEL - Final parallel level that is used to do migration. */
  PG_SYNC_PARALLEL_LEVEL = 42,
  /**
   * INSUFFICIENT_DISK_SIZE - The disk size of the replica instance is smaller than the data size of
   * the source instance.
   */
  INSUFFICIENT_DISK_SIZE = 43,
  /**
   * INSUFFICIENT_MACHINE_TIER - The data size of the source instance is greater than 1 TB, the number of
   * cores of the replica instance is less than 8, and the memory of the
   * replica is less than 32 GB.
   */
  INSUFFICIENT_MACHINE_TIER = 44,
  /**
   * UNSUPPORTED_EXTENSIONS_NOT_MIGRATED - The warning message indicates the unsupported extensions will not be
   * migrated to the destination.
   */
  UNSUPPORTED_EXTENSIONS_NOT_MIGRATED = 45,
  /**
   * EXTENSIONS_NOT_MIGRATED - The warning message indicates the pg_cron extension and settings will not
   * be migrated to the destination.
   */
  EXTENSIONS_NOT_MIGRATED = 46,
  /**
   * PG_CRON_FLAG_ENABLED_IN_REPLICA - The error message indicates that pg_cron flags are enabled on the
   * destination which is not supported during the migration.
   */
  PG_CRON_FLAG_ENABLED_IN_REPLICA = 47,
  /**
   * EXTENSIONS_NOT_ENABLED_IN_REPLICA - This error message indicates that the specified extensions are not
   * enabled on destination instance. For example, before you can migrate
   * data to the destination instance, you must enable the PGAudit extension
   * on the instance.
   */
  EXTENSIONS_NOT_ENABLED_IN_REPLICA = 48,
  UNRECOGNIZED = -1,
}

export function sqlExternalSyncSettingError_SqlExternalSyncSettingErrorTypeFromJSON(
  object: any,
): SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType {
  switch (object) {
    case 0:
    case "SQL_EXTERNAL_SYNC_SETTING_ERROR_TYPE_UNSPECIFIED":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType
        .SQL_EXTERNAL_SYNC_SETTING_ERROR_TYPE_UNSPECIFIED;
    case 1:
    case "CONNECTION_FAILURE":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.CONNECTION_FAILURE;
    case 2:
    case "BINLOG_NOT_ENABLED":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.BINLOG_NOT_ENABLED;
    case 3:
    case "INCOMPATIBLE_DATABASE_VERSION":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INCOMPATIBLE_DATABASE_VERSION;
    case 4:
    case "REPLICA_ALREADY_SETUP":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.REPLICA_ALREADY_SETUP;
    case 5:
    case "INSUFFICIENT_PRIVILEGE":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INSUFFICIENT_PRIVILEGE;
    case 6:
    case "UNSUPPORTED_MIGRATION_TYPE":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_MIGRATION_TYPE;
    case 7:
    case "NO_PGLOGICAL_INSTALLED":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.NO_PGLOGICAL_INSTALLED;
    case 8:
    case "PGLOGICAL_NODE_ALREADY_EXISTS":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.PGLOGICAL_NODE_ALREADY_EXISTS;
    case 9:
    case "INVALID_WAL_LEVEL":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INVALID_WAL_LEVEL;
    case 10:
    case "INVALID_SHARED_PRELOAD_LIBRARY":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INVALID_SHARED_PRELOAD_LIBRARY;
    case 11:
    case "INSUFFICIENT_MAX_REPLICATION_SLOTS":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INSUFFICIENT_MAX_REPLICATION_SLOTS;
    case 12:
    case "INSUFFICIENT_MAX_WAL_SENDERS":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INSUFFICIENT_MAX_WAL_SENDERS;
    case 13:
    case "INSUFFICIENT_MAX_WORKER_PROCESSES":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INSUFFICIENT_MAX_WORKER_PROCESSES;
    case 14:
    case "UNSUPPORTED_EXTENSIONS":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_EXTENSIONS;
    case 15:
    case "INVALID_RDS_LOGICAL_REPLICATION":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INVALID_RDS_LOGICAL_REPLICATION;
    case 16:
    case "INVALID_LOGGING_SETUP":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INVALID_LOGGING_SETUP;
    case 17:
    case "INVALID_DB_PARAM":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INVALID_DB_PARAM;
    case 18:
    case "UNSUPPORTED_GTID_MODE":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_GTID_MODE;
    case 19:
    case "SQLSERVER_AGENT_NOT_RUNNING":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.SQLSERVER_AGENT_NOT_RUNNING;
    case 20:
    case "UNSUPPORTED_TABLE_DEFINITION":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_TABLE_DEFINITION;
    case 21:
    case "UNSUPPORTED_DEFINER":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_DEFINER;
    case 22:
    case "SQLSERVER_SERVERNAME_MISMATCH":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.SQLSERVER_SERVERNAME_MISMATCH;
    case 23:
    case "PRIMARY_ALREADY_SETUP":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.PRIMARY_ALREADY_SETUP;
    case 24:
    case "UNSUPPORTED_BINLOG_FORMAT":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_BINLOG_FORMAT;
    case 25:
    case "BINLOG_RETENTION_SETTING":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.BINLOG_RETENTION_SETTING;
    case 26:
    case "UNSUPPORTED_STORAGE_ENGINE":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_STORAGE_ENGINE;
    case 27:
    case "LIMITED_SUPPORT_TABLES":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.LIMITED_SUPPORT_TABLES;
    case 28:
    case "EXISTING_DATA_IN_REPLICA":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.EXISTING_DATA_IN_REPLICA;
    case 29:
    case "MISSING_OPTIONAL_PRIVILEGES":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.MISSING_OPTIONAL_PRIVILEGES;
    case 30:
    case "RISKY_BACKUP_ADMIN_PRIVILEGE":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.RISKY_BACKUP_ADMIN_PRIVILEGE;
    case 31:
    case "INSUFFICIENT_GCS_PERMISSIONS":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INSUFFICIENT_GCS_PERMISSIONS;
    case 32:
    case "INVALID_FILE_INFO":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INVALID_FILE_INFO;
    case 33:
    case "UNSUPPORTED_DATABASE_SETTINGS":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_DATABASE_SETTINGS;
    case 34:
    case "MYSQL_PARALLEL_IMPORT_INSUFFICIENT_PRIVILEGE":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.MYSQL_PARALLEL_IMPORT_INSUFFICIENT_PRIVILEGE;
    case 35:
    case "LOCAL_INFILE_OFF":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.LOCAL_INFILE_OFF;
    case 36:
    case "TURN_ON_PITR_AFTER_PROMOTE":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.TURN_ON_PITR_AFTER_PROMOTE;
    case 37:
    case "INCOMPATIBLE_DATABASE_MINOR_VERSION":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INCOMPATIBLE_DATABASE_MINOR_VERSION;
    case 38:
    case "SOURCE_MAX_SUBSCRIPTIONS":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.SOURCE_MAX_SUBSCRIPTIONS;
    case 39:
    case "UNABLE_TO_VERIFY_DEFINERS":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNABLE_TO_VERIFY_DEFINERS;
    case 40:
    case "SUBSCRIPTION_CALCULATION_STATUS":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.SUBSCRIPTION_CALCULATION_STATUS;
    case 41:
    case "PG_SUBSCRIPTION_COUNT":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.PG_SUBSCRIPTION_COUNT;
    case 42:
    case "PG_SYNC_PARALLEL_LEVEL":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.PG_SYNC_PARALLEL_LEVEL;
    case 43:
    case "INSUFFICIENT_DISK_SIZE":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INSUFFICIENT_DISK_SIZE;
    case 44:
    case "INSUFFICIENT_MACHINE_TIER":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INSUFFICIENT_MACHINE_TIER;
    case 45:
    case "UNSUPPORTED_EXTENSIONS_NOT_MIGRATED":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_EXTENSIONS_NOT_MIGRATED;
    case 46:
    case "EXTENSIONS_NOT_MIGRATED":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.EXTENSIONS_NOT_MIGRATED;
    case 47:
    case "PG_CRON_FLAG_ENABLED_IN_REPLICA":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.PG_CRON_FLAG_ENABLED_IN_REPLICA;
    case 48:
    case "EXTENSIONS_NOT_ENABLED_IN_REPLICA":
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.EXTENSIONS_NOT_ENABLED_IN_REPLICA;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNRECOGNIZED;
  }
}

export function sqlExternalSyncSettingError_SqlExternalSyncSettingErrorTypeToJSON(
  object: SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType,
): string {
  switch (object) {
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.SQL_EXTERNAL_SYNC_SETTING_ERROR_TYPE_UNSPECIFIED:
      return "SQL_EXTERNAL_SYNC_SETTING_ERROR_TYPE_UNSPECIFIED";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.CONNECTION_FAILURE:
      return "CONNECTION_FAILURE";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.BINLOG_NOT_ENABLED:
      return "BINLOG_NOT_ENABLED";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INCOMPATIBLE_DATABASE_VERSION:
      return "INCOMPATIBLE_DATABASE_VERSION";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.REPLICA_ALREADY_SETUP:
      return "REPLICA_ALREADY_SETUP";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INSUFFICIENT_PRIVILEGE:
      return "INSUFFICIENT_PRIVILEGE";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_MIGRATION_TYPE:
      return "UNSUPPORTED_MIGRATION_TYPE";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.NO_PGLOGICAL_INSTALLED:
      return "NO_PGLOGICAL_INSTALLED";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.PGLOGICAL_NODE_ALREADY_EXISTS:
      return "PGLOGICAL_NODE_ALREADY_EXISTS";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INVALID_WAL_LEVEL:
      return "INVALID_WAL_LEVEL";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INVALID_SHARED_PRELOAD_LIBRARY:
      return "INVALID_SHARED_PRELOAD_LIBRARY";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INSUFFICIENT_MAX_REPLICATION_SLOTS:
      return "INSUFFICIENT_MAX_REPLICATION_SLOTS";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INSUFFICIENT_MAX_WAL_SENDERS:
      return "INSUFFICIENT_MAX_WAL_SENDERS";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INSUFFICIENT_MAX_WORKER_PROCESSES:
      return "INSUFFICIENT_MAX_WORKER_PROCESSES";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_EXTENSIONS:
      return "UNSUPPORTED_EXTENSIONS";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INVALID_RDS_LOGICAL_REPLICATION:
      return "INVALID_RDS_LOGICAL_REPLICATION";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INVALID_LOGGING_SETUP:
      return "INVALID_LOGGING_SETUP";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INVALID_DB_PARAM:
      return "INVALID_DB_PARAM";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_GTID_MODE:
      return "UNSUPPORTED_GTID_MODE";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.SQLSERVER_AGENT_NOT_RUNNING:
      return "SQLSERVER_AGENT_NOT_RUNNING";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_TABLE_DEFINITION:
      return "UNSUPPORTED_TABLE_DEFINITION";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_DEFINER:
      return "UNSUPPORTED_DEFINER";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.SQLSERVER_SERVERNAME_MISMATCH:
      return "SQLSERVER_SERVERNAME_MISMATCH";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.PRIMARY_ALREADY_SETUP:
      return "PRIMARY_ALREADY_SETUP";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_BINLOG_FORMAT:
      return "UNSUPPORTED_BINLOG_FORMAT";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.BINLOG_RETENTION_SETTING:
      return "BINLOG_RETENTION_SETTING";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_STORAGE_ENGINE:
      return "UNSUPPORTED_STORAGE_ENGINE";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.LIMITED_SUPPORT_TABLES:
      return "LIMITED_SUPPORT_TABLES";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.EXISTING_DATA_IN_REPLICA:
      return "EXISTING_DATA_IN_REPLICA";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.MISSING_OPTIONAL_PRIVILEGES:
      return "MISSING_OPTIONAL_PRIVILEGES";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.RISKY_BACKUP_ADMIN_PRIVILEGE:
      return "RISKY_BACKUP_ADMIN_PRIVILEGE";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INSUFFICIENT_GCS_PERMISSIONS:
      return "INSUFFICIENT_GCS_PERMISSIONS";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INVALID_FILE_INFO:
      return "INVALID_FILE_INFO";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_DATABASE_SETTINGS:
      return "UNSUPPORTED_DATABASE_SETTINGS";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.MYSQL_PARALLEL_IMPORT_INSUFFICIENT_PRIVILEGE:
      return "MYSQL_PARALLEL_IMPORT_INSUFFICIENT_PRIVILEGE";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.LOCAL_INFILE_OFF:
      return "LOCAL_INFILE_OFF";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.TURN_ON_PITR_AFTER_PROMOTE:
      return "TURN_ON_PITR_AFTER_PROMOTE";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INCOMPATIBLE_DATABASE_MINOR_VERSION:
      return "INCOMPATIBLE_DATABASE_MINOR_VERSION";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.SOURCE_MAX_SUBSCRIPTIONS:
      return "SOURCE_MAX_SUBSCRIPTIONS";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNABLE_TO_VERIFY_DEFINERS:
      return "UNABLE_TO_VERIFY_DEFINERS";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.SUBSCRIPTION_CALCULATION_STATUS:
      return "SUBSCRIPTION_CALCULATION_STATUS";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.PG_SUBSCRIPTION_COUNT:
      return "PG_SUBSCRIPTION_COUNT";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.PG_SYNC_PARALLEL_LEVEL:
      return "PG_SYNC_PARALLEL_LEVEL";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INSUFFICIENT_DISK_SIZE:
      return "INSUFFICIENT_DISK_SIZE";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.INSUFFICIENT_MACHINE_TIER:
      return "INSUFFICIENT_MACHINE_TIER";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNSUPPORTED_EXTENSIONS_NOT_MIGRATED:
      return "UNSUPPORTED_EXTENSIONS_NOT_MIGRATED";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.EXTENSIONS_NOT_MIGRATED:
      return "EXTENSIONS_NOT_MIGRATED";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.PG_CRON_FLAG_ENABLED_IN_REPLICA:
      return "PG_CRON_FLAG_ENABLED_IN_REPLICA";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.EXTENSIONS_NOT_ENABLED_IN_REPLICA:
      return "EXTENSIONS_NOT_ENABLED_IN_REPLICA";
    case SqlExternalSyncSettingError_SqlExternalSyncSettingErrorType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** IP Management configuration. */
export interface IpConfiguration {
  /** Whether the instance is assigned a public IP address or not. */
  ipv4Enabled:
    | boolean
    | undefined;
  /**
   * The resource link for the VPC network from which the Cloud SQL instance is
   * accessible for private IP. For example,
   * `/projects/myProject/global/networks/default`. This setting can
   * be updated, but it cannot be removed after it is set.
   */
  privateNetwork: string;
  /**
   * Use `ssl_mode` instead.
   *
   * Whether SSL/TLS connections over IP are enforced.
   * If set to false, then allow both non-SSL/non-TLS and SSL/TLS connections.
   * For SSL/TLS connections, the client certificate won't be verified. If
   * set to true, then only allow connections encrypted with SSL/TLS and with
   * valid client certificates. If you want to enforce SSL/TLS without enforcing
   * the requirement for valid client certificates, then use the `ssl_mode` flag
   * instead of the legacy `require_ssl` flag.
   */
  requireSsl:
    | boolean
    | undefined;
  /**
   * The list of external networks that are allowed to connect to the instance
   * using the IP. In 'CIDR' notation, also known as 'slash' notation (for
   * example: `157.197.200.0/24`).
   */
  authorizedNetworks: AclEntry[];
  /**
   * The name of the allocated ip range for the private ip Cloud SQL instance.
   * For example: "google-managed-services-default". If set, the instance ip
   * will be created in the allocated range. The range name must comply with
   * [RFC 1035](https://tools.ietf.org/html/rfc1035). Specifically, the name
   * must be 1-63 characters long and match the regular expression
   * `[a-z]([-a-z0-9]*[a-z0-9])?.`
   */
  allocatedIpRange: string;
  /**
   * Controls connectivity to private IP instances from Google services,
   * such as BigQuery.
   */
  enablePrivatePathForGoogleCloudServices:
    | boolean
    | undefined;
  /**
   * Specify how SSL/TLS is enforced in database connections. If you must use
   * the `require_ssl` flag for backward compatibility, then only the following
   * value pairs are valid:
   *
   * For PostgreSQL and MySQL:
   *
   * * `ssl_mode=ALLOW_UNENCRYPTED_AND_ENCRYPTED` and `require_ssl=false`
   * * `ssl_mode=ENCRYPTED_ONLY` and `require_ssl=false`
   * * `ssl_mode=TRUSTED_CLIENT_CERTIFICATE_REQUIRED` and `require_ssl=true`
   *
   * For SQL Server:
   *
   * * `ssl_mode=ALLOW_UNENCRYPTED_AND_ENCRYPTED` and `require_ssl=false`
   * * `ssl_mode=ENCRYPTED_ONLY` and `require_ssl=true`
   *
   * The value of `ssl_mode` has priority over the value of `require_ssl`.
   *
   * For example, for the pair `ssl_mode=ENCRYPTED_ONLY` and
   * `require_ssl=false`, `ssl_mode=ENCRYPTED_ONLY` means accept only SSL
   * connections, while `require_ssl=false` means accept both non-SSL
   * and SSL connections. In this case, MySQL and PostgreSQL databases respect
   * `ssl_mode` and accepts only SSL connections.
   */
  sslMode: IpConfiguration_SslMode;
  /** PSC settings for this instance. */
  pscConfig?: PscConfig | undefined;
}

/** The SSL options for database connections. */
export enum IpConfiguration_SslMode {
  /** SSL_MODE_UNSPECIFIED - The SSL mode is unknown. */
  SSL_MODE_UNSPECIFIED = 0,
  /**
   * ALLOW_UNENCRYPTED_AND_ENCRYPTED - Allow non-SSL/non-TLS and SSL/TLS connections.
   * For SSL connections to MySQL and PostgreSQL, the client certificate
   * isn't verified.
   *
   * When this value is used, the legacy `require_ssl` flag must be false or
   * cleared to avoid a conflict between the values of the two flags.
   */
  ALLOW_UNENCRYPTED_AND_ENCRYPTED = 1,
  /**
   * ENCRYPTED_ONLY - Only allow connections encrypted with SSL/TLS.
   * For SSL connections to MySQL and PostgreSQL, the client certificate
   * isn't verified.
   *
   * When this value is used, the legacy `require_ssl` flag must be false or
   * cleared to avoid a conflict between the values of the two flags.
   */
  ENCRYPTED_ONLY = 2,
  /**
   * TRUSTED_CLIENT_CERTIFICATE_REQUIRED - Only allow connections encrypted with SSL/TLS and with valid
   * client certificates.
   *
   * When this value is used, the legacy `require_ssl` flag must be true or
   * cleared to avoid the conflict between values of two flags.
   * PostgreSQL clients or users that connect using IAM database
   * authentication must use either the
   * [Cloud SQL Auth
   * Proxy](https://cloud.google.com/sql/docs/postgres/connect-auth-proxy) or
   * [Cloud SQL
   * Connectors](https://cloud.google.com/sql/docs/postgres/connect-connectors)
   * to enforce client identity verification.
   *
   * Only applicable to MySQL and PostgreSQL. Not applicable to SQL Server.
   */
  TRUSTED_CLIENT_CERTIFICATE_REQUIRED = 3,
  UNRECOGNIZED = -1,
}

export function ipConfiguration_SslModeFromJSON(object: any): IpConfiguration_SslMode {
  switch (object) {
    case 0:
    case "SSL_MODE_UNSPECIFIED":
      return IpConfiguration_SslMode.SSL_MODE_UNSPECIFIED;
    case 1:
    case "ALLOW_UNENCRYPTED_AND_ENCRYPTED":
      return IpConfiguration_SslMode.ALLOW_UNENCRYPTED_AND_ENCRYPTED;
    case 2:
    case "ENCRYPTED_ONLY":
      return IpConfiguration_SslMode.ENCRYPTED_ONLY;
    case 3:
    case "TRUSTED_CLIENT_CERTIFICATE_REQUIRED":
      return IpConfiguration_SslMode.TRUSTED_CLIENT_CERTIFICATE_REQUIRED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return IpConfiguration_SslMode.UNRECOGNIZED;
  }
}

export function ipConfiguration_SslModeToJSON(object: IpConfiguration_SslMode): string {
  switch (object) {
    case IpConfiguration_SslMode.SSL_MODE_UNSPECIFIED:
      return "SSL_MODE_UNSPECIFIED";
    case IpConfiguration_SslMode.ALLOW_UNENCRYPTED_AND_ENCRYPTED:
      return "ALLOW_UNENCRYPTED_AND_ENCRYPTED";
    case IpConfiguration_SslMode.ENCRYPTED_ONLY:
      return "ENCRYPTED_ONLY";
    case IpConfiguration_SslMode.TRUSTED_CLIENT_CERTIFICATE_REQUIRED:
      return "TRUSTED_CLIENT_CERTIFICATE_REQUIRED";
    case IpConfiguration_SslMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** PSC settings for a Cloud SQL instance. */
export interface PscConfig {
  /** Whether PSC connectivity is enabled for this instance. */
  pscEnabled?:
    | boolean
    | undefined;
  /**
   * Optional. The list of consumer projects that are allow-listed for PSC
   * connections to this instance. This instance can be connected to with PSC
   * from any network in these projects.
   *
   * Each consumer project in this list may be represented by a project number
   * (numeric) or by a project id (alphanumeric).
   */
  allowedConsumerProjects: string[];
}

/** Database instance IP mapping */
export interface IpMapping {
  /**
   * The type of this IP address. A `PRIMARY` address is a public address that
   * can accept incoming connections. A `PRIVATE` address is a private address
   * that can accept incoming connections. An `OUTGOING` address is the source
   * address of connections originating from the instance, if supported.
   */
  type: SqlIpAddressType;
  /** The IP address assigned. */
  ipAddress: string;
  /**
   * The due time for this IP to be retired in
   * [RFC 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`. This field is only available when
   * the IP is scheduled to be retired.
   */
  timeToRetire: Date | undefined;
}

/**
 * Preferred location. This specifies where a Cloud SQL instance is located.
 * Note that if the preferred location is not available, the instance will be
 * located as close as possible within the region. Only one location may be
 * specified.
 */
export interface LocationPreference {
  /**
   * The App Engine application to follow, it must be in the same region as the
   * Cloud SQL instance. WARNING: Changing this might restart the instance.
   *
   * @deprecated
   */
  followGaeApplication: string;
  /**
   * The preferred Compute Engine zone (for example: us-central1-a,
   * us-central1-b, etc.). WARNING: Changing this might restart the instance.
   */
  zone: string;
  /**
   * The preferred Compute Engine zone for the secondary/failover
   * (for example: us-central1-a, us-central1-b, etc.).
   * To disable this field, set it to 'no_secondary_zone'.
   */
  secondaryZone: string;
  /** This is always `sql#locationPreference`. */
  kind: string;
}

/**
 * Maintenance window. This specifies when a Cloud SQL instance
 * is restarted for system maintenance purposes.
 */
export interface MaintenanceWindow {
  /** hour of day - 0 to 23. */
  hour:
    | number
    | undefined;
  /** day of week (1-7), starting on Monday. */
  day:
    | number
    | undefined;
  /**
   * Maintenance timing setting: `canary` (Earlier) or `stable` (Later).
   * [Learn
   * more](https://cloud.google.com/sql/docs/mysql/instance-settings#maintenance-timing-2ndgen).
   */
  updateTrack: SqlUpdateTrack;
  /** This is always `sql#maintenanceWindow`. */
  kind: string;
}

/**
 * Deny Maintenance Periods. This specifies a date range during when all CSA
 * rollout will be denied.
 */
export interface DenyMaintenancePeriod {
  /**
   * "deny maintenance period" start date. If the year of the start date is
   * empty, the year of the end date also must be empty. In this case, it means
   * the deny maintenance period recurs every year. The date is in format
   * yyyy-mm-dd i.e., 2020-11-01, or mm-dd, i.e., 11-01
   */
  startDate: string;
  /**
   * "deny maintenance period" end date. If the year of the end date is empty,
   * the year of the start date also must be empty. In this case, it means the
   * deny maintenance period recurs every year. The date is in format yyyy-mm-dd
   * i.e., 2020-11-01, or mm-dd, i.e., 11-01
   */
  endDate: string;
  /**
   * Time in UTC when the "deny maintenance period" starts on start_date and
   * ends on end_date. The time is in format: HH:mm:SS, i.e., 00:00:00
   */
  time: string;
}

/**
 * Insights configuration. This specifies when Cloud SQL Insights feature is
 * enabled and optional configuration.
 */
export interface InsightsConfig {
  /** Whether Query Insights feature is enabled. */
  queryInsightsEnabled: boolean;
  /** Whether Query Insights will record client address when enabled. */
  recordClientAddress: boolean;
  /**
   * Whether Query Insights will record application tags from query when
   * enabled.
   */
  recordApplicationTags: boolean;
  /**
   * Maximum query length stored in bytes. Default value: 1024 bytes.
   * Range: 256-4500 bytes. Query length more than this field value will be
   * truncated to this value. When unset, query length will be the default
   * value. Changing query length will restart the database.
   */
  queryStringLength:
    | number
    | undefined;
  /**
   * Number of query execution plans captured by Insights per minute
   * for all queries combined. Default is 5.
   */
  queryPlansPerMinute: number | undefined;
}

/** Read-replica configuration specific to MySQL databases. */
export interface MySqlReplicaConfiguration {
  /**
   * Path to a SQL dump file in Google Cloud Storage from which the replica
   * instance is to be created. The URI is in the form gs://bucketName/fileName.
   * Compressed gzip files (.gz) are also supported.
   * Dumps have the binlog co-ordinates from which replication
   * begins. This can be accomplished by setting --master-data to 1 when using
   * mysqldump.
   */
  dumpFilePath: string;
  /** The username for the replication connection. */
  username: string;
  /** The password for the replication connection. */
  password: string;
  /** Seconds to wait between connect retries. MySQL's default is 60 seconds. */
  connectRetryInterval:
    | number
    | undefined;
  /** Interval in milliseconds between replication heartbeats. */
  masterHeartbeatPeriod:
    | Long
    | undefined;
  /** PEM representation of the trusted CA's x509 certificate. */
  caCertificate: string;
  /** PEM representation of the replica's x509 certificate. */
  clientCertificate: string;
  /**
   * PEM representation of the replica's private key. The corresponsing public
   * key is encoded in the client's certificate.
   */
  clientKey: string;
  /** A list of permissible ciphers to use for SSL encryption. */
  sslCipher: string;
  /**
   * Whether or not to check the primary instance's Common Name value in the
   * certificate that it sends during the SSL handshake.
   */
  verifyServerCertificate:
    | boolean
    | undefined;
  /** This is always `sql#mysqlReplicaConfiguration`. */
  kind: string;
}

/** On-premises instance configuration. */
export interface OnPremisesConfiguration {
  /** The host and port of the on-premises instance in host:port format */
  hostPort: string;
  /** This is always `sql#onPremisesConfiguration`. */
  kind: string;
  /** The username for connecting to on-premises instance. */
  username: string;
  /** The password for connecting to on-premises instance. */
  password: string;
  /** PEM representation of the trusted CA's x509 certificate. */
  caCertificate: string;
  /** PEM representation of the replica's x509 certificate. */
  clientCertificate: string;
  /**
   * PEM representation of the replica's private key. The corresponsing public
   * key is encoded in the client's certificate.
   */
  clientKey: string;
  /** The dump file to create the Cloud SQL replica. */
  dumpFilePath: string;
  /** The reference to Cloud SQL instance if the source is Cloud SQL. */
  sourceInstance: InstanceReference | undefined;
}

/** Disk encryption configuration for an instance. */
export interface DiskEncryptionConfiguration {
  /** Resource name of KMS key for disk encryption */
  kmsKeyName: string;
  /** This is always `sql#diskEncryptionConfiguration`. */
  kind: string;
}

/** Disk encryption status for an instance. */
export interface DiskEncryptionStatus {
  /** KMS key version used to encrypt the Cloud SQL instance resource */
  kmsKeyVersionName: string;
  /** This is always `sql#diskEncryptionStatus`. */
  kind: string;
}

/**
 * An Operation resource.&nbsp;For successful operations that return an
 * Operation resource, only the fields relevant to the operation are populated
 * in the resource.
 */
export interface Operation {
  /** This is always `sql#operation`. */
  kind: string;
  targetLink: string;
  /** The status of an operation. */
  status: Operation_SqlOperationStatus;
  /** The email address of the user who initiated this operation. */
  user: string;
  /**
   * The time this operation was enqueued in UTC timezone in
   * [RFC 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`.
   */
  insertTime:
    | Date
    | undefined;
  /**
   * The time this operation actually started in UTC timezone in
   * [RFC 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`.
   */
  startTime:
    | Date
    | undefined;
  /**
   * The time this operation finished in UTC timezone in
   * [RFC 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`.
   */
  endTime:
    | Date
    | undefined;
  /**
   * If errors occurred during processing of this operation, this field will be
   * populated.
   */
  error:
    | OperationErrors
    | undefined;
  /** An Admin API warning message. */
  apiWarning:
    | ApiWarning
    | undefined;
  /**
   * The type of the operation. Valid values are:
   * *  `CREATE`
   * *  `DELETE`
   * *  `UPDATE`
   * *  `RESTART`
   * *  `IMPORT`
   * *  `EXPORT`
   * *  `BACKUP_VOLUME`
   * *  `RESTORE_VOLUME`
   * *  `CREATE_USER`
   * *  `DELETE_USER`
   * *  `CREATE_DATABASE`
   * *  `DELETE_DATABASE`
   */
  operationType: Operation_SqlOperationType;
  /** The context for import operation, if applicable. */
  importContext:
    | ImportContext
    | undefined;
  /** The context for export operation, if applicable. */
  exportContext:
    | ExportContext
    | undefined;
  /** The context for backup operation, if applicable. */
  backupContext:
    | BackupContext
    | undefined;
  /**
   * An identifier that uniquely identifies the operation. You can use this
   * identifier to retrieve the Operations resource that has information about
   * the operation.
   */
  name: string;
  /** Name of the database instance related to this operation. */
  targetId: string;
  /** The URI of this resource. */
  selfLink: string;
  /** The project ID of the target instance related to this operation. */
  targetProject: string;
  /** The context for acquire SSRS lease operation, if applicable. */
  acquireSsrsLeaseContext: AcquireSsrsLeaseContext | undefined;
}

/** The type of Cloud SQL operation. */
export enum Operation_SqlOperationType {
  /** SQL_OPERATION_TYPE_UNSPECIFIED - Unknown operation type. */
  SQL_OPERATION_TYPE_UNSPECIFIED = 0,
  /** IMPORT - Imports data into a Cloud SQL instance. */
  IMPORT = 1,
  /**
   * EXPORT - Exports data from a Cloud SQL instance to a Cloud Storage
   * bucket.
   */
  EXPORT = 2,
  /** CREATE - Creates a new Cloud SQL instance. */
  CREATE = 3,
  /** UPDATE - Updates the settings of a Cloud SQL instance. */
  UPDATE = 4,
  /** DELETE - Deletes a Cloud SQL instance. */
  DELETE = 5,
  /** RESTART - Restarts the Cloud SQL instance. */
  RESTART = 6,
  /** @deprecated */
  BACKUP = 7,
  /** @deprecated */
  SNAPSHOT = 8,
  /** BACKUP_VOLUME - Performs instance backup. */
  BACKUP_VOLUME = 9,
  /** DELETE_VOLUME - Deletes an instance backup. */
  DELETE_VOLUME = 10,
  /** RESTORE_VOLUME - Restores an instance backup. */
  RESTORE_VOLUME = 11,
  /** INJECT_USER - Injects a privileged user in mysql for MOB instances. */
  INJECT_USER = 12,
  /** CLONE - Clones a Cloud SQL instance. */
  CLONE = 14,
  /** STOP_REPLICA - Stops replication on a Cloud SQL read replica instance. */
  STOP_REPLICA = 15,
  /** START_REPLICA - Starts replication on a Cloud SQL read replica instance. */
  START_REPLICA = 16,
  /** PROMOTE_REPLICA - Promotes a Cloud SQL replica instance. */
  PROMOTE_REPLICA = 17,
  /** CREATE_REPLICA - Creates a Cloud SQL replica instance. */
  CREATE_REPLICA = 18,
  /** CREATE_USER - Creates a new user in a Cloud SQL instance. */
  CREATE_USER = 19,
  /** DELETE_USER - Deletes a user from a Cloud SQL instance. */
  DELETE_USER = 20,
  /** UPDATE_USER - Updates an existing user in a Cloud SQL instance. */
  UPDATE_USER = 21,
  /** CREATE_DATABASE - Creates a database in the Cloud SQL instance. */
  CREATE_DATABASE = 22,
  /** DELETE_DATABASE - Deletes a database in the Cloud SQL instance. */
  DELETE_DATABASE = 23,
  /** UPDATE_DATABASE - Updates a database in the Cloud SQL instance. */
  UPDATE_DATABASE = 24,
  /**
   * FAILOVER - Performs failover of an HA-enabled Cloud SQL
   * failover replica.
   */
  FAILOVER = 25,
  /** DELETE_BACKUP - Deletes the backup taken by a backup run. */
  DELETE_BACKUP = 26,
  RECREATE_REPLICA = 27,
  /** TRUNCATE_LOG - Truncates a general or slow log table in MySQL. */
  TRUNCATE_LOG = 28,
  /**
   * DEMOTE_MASTER - Demotes the stand-alone instance to be a Cloud SQL
   * read replica for an external database server.
   */
  DEMOTE_MASTER = 29,
  /**
   * MAINTENANCE - Indicates that the instance is currently in maintenance. Maintenance
   * typically causes the instance to be unavailable for 1-3 minutes.
   */
  MAINTENANCE = 30,
  /**
   * ENABLE_PRIVATE_IP - This field is deprecated, and will be removed in future version of API.
   *
   * @deprecated
   */
  ENABLE_PRIVATE_IP = 31,
  /** @deprecated */
  DEFER_MAINTENANCE = 32,
  /**
   * CREATE_CLONE - Creates clone instance.
   *
   * @deprecated
   */
  CREATE_CLONE = 33,
  /** RESCHEDULE_MAINTENANCE - Reschedule maintenance to another time. */
  RESCHEDULE_MAINTENANCE = 34,
  /**
   * START_EXTERNAL_SYNC - Starts external sync of a Cloud SQL EM replica to an external primary
   * instance.
   */
  START_EXTERNAL_SYNC = 35,
  /** LOG_CLEANUP - Recovers logs from an instance's old data disk. */
  LOG_CLEANUP = 36,
  /**
   * AUTO_RESTART - Performs auto-restart of an HA-enabled Cloud SQL database for auto
   * recovery.
   */
  AUTO_RESTART = 37,
  /** REENCRYPT - Re-encrypts CMEK instances with latest key version. */
  REENCRYPT = 38,
  /**
   * SWITCHOVER - Switches the roles of the primary and replica pair. The target instance
   * should be the replica.
   */
  SWITCHOVER = 39,
  /** ACQUIRE_SSRS_LEASE - Acquire a lease for the setup of SQL Server Reporting Services (SSRS). */
  ACQUIRE_SSRS_LEASE = 42,
  /** RELEASE_SSRS_LEASE - Release a lease for the setup of SQL Server Reporting Services (SSRS). */
  RELEASE_SSRS_LEASE = 43,
  /**
   * RECONFIGURE_OLD_PRIMARY - Reconfigures old primary after a promote replica operation. Effect of a
   * promote operation to the old primary is executed in this operation,
   * asynchronously from the promote replica operation executed to the
   * replica.
   */
  RECONFIGURE_OLD_PRIMARY = 44,
  /**
   * CLUSTER_MAINTENANCE - Indicates that the instance, its read replicas, and its cascading
   * replicas are in maintenance. Maintenance typically gets initiated on
   * groups of replicas first, followed by the primary instance. For each
   * instance, maintenance typically causes the instance to be unavailable for
   * 1-3 minutes.
   */
  CLUSTER_MAINTENANCE = 45,
  /**
   * SELF_SERVICE_MAINTENANCE - Indicates that the instance (and any of its replicas) are currently in
   * maintenance. This is initiated as a self-service request by using SSM.
   * Maintenance typically causes the instance to be unavailable for 1-3
   * minutes.
   */
  SELF_SERVICE_MAINTENANCE = 46,
  /**
   * SWITCHOVER_TO_REPLICA - Switches a primary instance to a replica. This operation runs as part of
   * a switchover operation to the original primary instance.
   */
  SWITCHOVER_TO_REPLICA = 47,
  UNRECOGNIZED = -1,
}

export function operation_SqlOperationTypeFromJSON(object: any): Operation_SqlOperationType {
  switch (object) {
    case 0:
    case "SQL_OPERATION_TYPE_UNSPECIFIED":
      return Operation_SqlOperationType.SQL_OPERATION_TYPE_UNSPECIFIED;
    case 1:
    case "IMPORT":
      return Operation_SqlOperationType.IMPORT;
    case 2:
    case "EXPORT":
      return Operation_SqlOperationType.EXPORT;
    case 3:
    case "CREATE":
      return Operation_SqlOperationType.CREATE;
    case 4:
    case "UPDATE":
      return Operation_SqlOperationType.UPDATE;
    case 5:
    case "DELETE":
      return Operation_SqlOperationType.DELETE;
    case 6:
    case "RESTART":
      return Operation_SqlOperationType.RESTART;
    case 7:
    case "BACKUP":
      return Operation_SqlOperationType.BACKUP;
    case 8:
    case "SNAPSHOT":
      return Operation_SqlOperationType.SNAPSHOT;
    case 9:
    case "BACKUP_VOLUME":
      return Operation_SqlOperationType.BACKUP_VOLUME;
    case 10:
    case "DELETE_VOLUME":
      return Operation_SqlOperationType.DELETE_VOLUME;
    case 11:
    case "RESTORE_VOLUME":
      return Operation_SqlOperationType.RESTORE_VOLUME;
    case 12:
    case "INJECT_USER":
      return Operation_SqlOperationType.INJECT_USER;
    case 14:
    case "CLONE":
      return Operation_SqlOperationType.CLONE;
    case 15:
    case "STOP_REPLICA":
      return Operation_SqlOperationType.STOP_REPLICA;
    case 16:
    case "START_REPLICA":
      return Operation_SqlOperationType.START_REPLICA;
    case 17:
    case "PROMOTE_REPLICA":
      return Operation_SqlOperationType.PROMOTE_REPLICA;
    case 18:
    case "CREATE_REPLICA":
      return Operation_SqlOperationType.CREATE_REPLICA;
    case 19:
    case "CREATE_USER":
      return Operation_SqlOperationType.CREATE_USER;
    case 20:
    case "DELETE_USER":
      return Operation_SqlOperationType.DELETE_USER;
    case 21:
    case "UPDATE_USER":
      return Operation_SqlOperationType.UPDATE_USER;
    case 22:
    case "CREATE_DATABASE":
      return Operation_SqlOperationType.CREATE_DATABASE;
    case 23:
    case "DELETE_DATABASE":
      return Operation_SqlOperationType.DELETE_DATABASE;
    case 24:
    case "UPDATE_DATABASE":
      return Operation_SqlOperationType.UPDATE_DATABASE;
    case 25:
    case "FAILOVER":
      return Operation_SqlOperationType.FAILOVER;
    case 26:
    case "DELETE_BACKUP":
      return Operation_SqlOperationType.DELETE_BACKUP;
    case 27:
    case "RECREATE_REPLICA":
      return Operation_SqlOperationType.RECREATE_REPLICA;
    case 28:
    case "TRUNCATE_LOG":
      return Operation_SqlOperationType.TRUNCATE_LOG;
    case 29:
    case "DEMOTE_MASTER":
      return Operation_SqlOperationType.DEMOTE_MASTER;
    case 30:
    case "MAINTENANCE":
      return Operation_SqlOperationType.MAINTENANCE;
    case 31:
    case "ENABLE_PRIVATE_IP":
      return Operation_SqlOperationType.ENABLE_PRIVATE_IP;
    case 32:
    case "DEFER_MAINTENANCE":
      return Operation_SqlOperationType.DEFER_MAINTENANCE;
    case 33:
    case "CREATE_CLONE":
      return Operation_SqlOperationType.CREATE_CLONE;
    case 34:
    case "RESCHEDULE_MAINTENANCE":
      return Operation_SqlOperationType.RESCHEDULE_MAINTENANCE;
    case 35:
    case "START_EXTERNAL_SYNC":
      return Operation_SqlOperationType.START_EXTERNAL_SYNC;
    case 36:
    case "LOG_CLEANUP":
      return Operation_SqlOperationType.LOG_CLEANUP;
    case 37:
    case "AUTO_RESTART":
      return Operation_SqlOperationType.AUTO_RESTART;
    case 38:
    case "REENCRYPT":
      return Operation_SqlOperationType.REENCRYPT;
    case 39:
    case "SWITCHOVER":
      return Operation_SqlOperationType.SWITCHOVER;
    case 42:
    case "ACQUIRE_SSRS_LEASE":
      return Operation_SqlOperationType.ACQUIRE_SSRS_LEASE;
    case 43:
    case "RELEASE_SSRS_LEASE":
      return Operation_SqlOperationType.RELEASE_SSRS_LEASE;
    case 44:
    case "RECONFIGURE_OLD_PRIMARY":
      return Operation_SqlOperationType.RECONFIGURE_OLD_PRIMARY;
    case 45:
    case "CLUSTER_MAINTENANCE":
      return Operation_SqlOperationType.CLUSTER_MAINTENANCE;
    case 46:
    case "SELF_SERVICE_MAINTENANCE":
      return Operation_SqlOperationType.SELF_SERVICE_MAINTENANCE;
    case 47:
    case "SWITCHOVER_TO_REPLICA":
      return Operation_SqlOperationType.SWITCHOVER_TO_REPLICA;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Operation_SqlOperationType.UNRECOGNIZED;
  }
}

export function operation_SqlOperationTypeToJSON(object: Operation_SqlOperationType): string {
  switch (object) {
    case Operation_SqlOperationType.SQL_OPERATION_TYPE_UNSPECIFIED:
      return "SQL_OPERATION_TYPE_UNSPECIFIED";
    case Operation_SqlOperationType.IMPORT:
      return "IMPORT";
    case Operation_SqlOperationType.EXPORT:
      return "EXPORT";
    case Operation_SqlOperationType.CREATE:
      return "CREATE";
    case Operation_SqlOperationType.UPDATE:
      return "UPDATE";
    case Operation_SqlOperationType.DELETE:
      return "DELETE";
    case Operation_SqlOperationType.RESTART:
      return "RESTART";
    case Operation_SqlOperationType.BACKUP:
      return "BACKUP";
    case Operation_SqlOperationType.SNAPSHOT:
      return "SNAPSHOT";
    case Operation_SqlOperationType.BACKUP_VOLUME:
      return "BACKUP_VOLUME";
    case Operation_SqlOperationType.DELETE_VOLUME:
      return "DELETE_VOLUME";
    case Operation_SqlOperationType.RESTORE_VOLUME:
      return "RESTORE_VOLUME";
    case Operation_SqlOperationType.INJECT_USER:
      return "INJECT_USER";
    case Operation_SqlOperationType.CLONE:
      return "CLONE";
    case Operation_SqlOperationType.STOP_REPLICA:
      return "STOP_REPLICA";
    case Operation_SqlOperationType.START_REPLICA:
      return "START_REPLICA";
    case Operation_SqlOperationType.PROMOTE_REPLICA:
      return "PROMOTE_REPLICA";
    case Operation_SqlOperationType.CREATE_REPLICA:
      return "CREATE_REPLICA";
    case Operation_SqlOperationType.CREATE_USER:
      return "CREATE_USER";
    case Operation_SqlOperationType.DELETE_USER:
      return "DELETE_USER";
    case Operation_SqlOperationType.UPDATE_USER:
      return "UPDATE_USER";
    case Operation_SqlOperationType.CREATE_DATABASE:
      return "CREATE_DATABASE";
    case Operation_SqlOperationType.DELETE_DATABASE:
      return "DELETE_DATABASE";
    case Operation_SqlOperationType.UPDATE_DATABASE:
      return "UPDATE_DATABASE";
    case Operation_SqlOperationType.FAILOVER:
      return "FAILOVER";
    case Operation_SqlOperationType.DELETE_BACKUP:
      return "DELETE_BACKUP";
    case Operation_SqlOperationType.RECREATE_REPLICA:
      return "RECREATE_REPLICA";
    case Operation_SqlOperationType.TRUNCATE_LOG:
      return "TRUNCATE_LOG";
    case Operation_SqlOperationType.DEMOTE_MASTER:
      return "DEMOTE_MASTER";
    case Operation_SqlOperationType.MAINTENANCE:
      return "MAINTENANCE";
    case Operation_SqlOperationType.ENABLE_PRIVATE_IP:
      return "ENABLE_PRIVATE_IP";
    case Operation_SqlOperationType.DEFER_MAINTENANCE:
      return "DEFER_MAINTENANCE";
    case Operation_SqlOperationType.CREATE_CLONE:
      return "CREATE_CLONE";
    case Operation_SqlOperationType.RESCHEDULE_MAINTENANCE:
      return "RESCHEDULE_MAINTENANCE";
    case Operation_SqlOperationType.START_EXTERNAL_SYNC:
      return "START_EXTERNAL_SYNC";
    case Operation_SqlOperationType.LOG_CLEANUP:
      return "LOG_CLEANUP";
    case Operation_SqlOperationType.AUTO_RESTART:
      return "AUTO_RESTART";
    case Operation_SqlOperationType.REENCRYPT:
      return "REENCRYPT";
    case Operation_SqlOperationType.SWITCHOVER:
      return "SWITCHOVER";
    case Operation_SqlOperationType.ACQUIRE_SSRS_LEASE:
      return "ACQUIRE_SSRS_LEASE";
    case Operation_SqlOperationType.RELEASE_SSRS_LEASE:
      return "RELEASE_SSRS_LEASE";
    case Operation_SqlOperationType.RECONFIGURE_OLD_PRIMARY:
      return "RECONFIGURE_OLD_PRIMARY";
    case Operation_SqlOperationType.CLUSTER_MAINTENANCE:
      return "CLUSTER_MAINTENANCE";
    case Operation_SqlOperationType.SELF_SERVICE_MAINTENANCE:
      return "SELF_SERVICE_MAINTENANCE";
    case Operation_SqlOperationType.SWITCHOVER_TO_REPLICA:
      return "SWITCHOVER_TO_REPLICA";
    case Operation_SqlOperationType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The status of an operation. */
export enum Operation_SqlOperationStatus {
  /** SQL_OPERATION_STATUS_UNSPECIFIED - The state of the operation is unknown. */
  SQL_OPERATION_STATUS_UNSPECIFIED = 0,
  /** PENDING - The operation has been queued, but has not started yet. */
  PENDING = 1,
  /** RUNNING - The operation is running. */
  RUNNING = 2,
  /** DONE - The operation completed. */
  DONE = 3,
  UNRECOGNIZED = -1,
}

export function operation_SqlOperationStatusFromJSON(object: any): Operation_SqlOperationStatus {
  switch (object) {
    case 0:
    case "SQL_OPERATION_STATUS_UNSPECIFIED":
      return Operation_SqlOperationStatus.SQL_OPERATION_STATUS_UNSPECIFIED;
    case 1:
    case "PENDING":
      return Operation_SqlOperationStatus.PENDING;
    case 2:
    case "RUNNING":
      return Operation_SqlOperationStatus.RUNNING;
    case 3:
    case "DONE":
      return Operation_SqlOperationStatus.DONE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Operation_SqlOperationStatus.UNRECOGNIZED;
  }
}

export function operation_SqlOperationStatusToJSON(object: Operation_SqlOperationStatus): string {
  switch (object) {
    case Operation_SqlOperationStatus.SQL_OPERATION_STATUS_UNSPECIFIED:
      return "SQL_OPERATION_STATUS_UNSPECIFIED";
    case Operation_SqlOperationStatus.PENDING:
      return "PENDING";
    case Operation_SqlOperationStatus.RUNNING:
      return "RUNNING";
    case Operation_SqlOperationStatus.DONE:
      return "DONE";
    case Operation_SqlOperationStatus.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Database instance operation error. */
export interface OperationError {
  /** This is always `sql#operationError`. */
  kind: string;
  /** Identifies the specific error that occurred. */
  code: string;
  /** Additional information about the error encountered. */
  message: string;
}

/** Database instance operation errors list wrapper. */
export interface OperationErrors {
  /** This is always `sql#operationErrors`. */
  kind: string;
  /** The list of errors encountered while processing this operation. */
  errors: OperationError[];
}

/** Database instance local user password validation policy */
export interface PasswordValidationPolicy {
  /** Minimum number of characters allowed. */
  minLength:
    | number
    | undefined;
  /** The complexity of the password. */
  complexity: PasswordValidationPolicy_Complexity;
  /** Number of previous passwords that cannot be reused. */
  reuseInterval:
    | number
    | undefined;
  /** Disallow username as a part of the password. */
  disallowUsernameSubstring:
    | boolean
    | undefined;
  /**
   * Minimum interval after which the password can be changed. This flag is only
   * supported for PostgreSQL.
   */
  passwordChangeInterval:
    | Duration
    | undefined;
  /** Whether the password policy is enabled or not. */
  enablePasswordPolicy:
    | boolean
    | undefined;
  /**
   * This field is deprecated and will be removed in a future version of the
   * API.
   *
   * @deprecated
   */
  disallowCompromisedCredentials: boolean | undefined;
}

/** The complexity choices of the password. */
export enum PasswordValidationPolicy_Complexity {
  /** COMPLEXITY_UNSPECIFIED - Complexity check is not specified. */
  COMPLEXITY_UNSPECIFIED = 0,
  /**
   * COMPLEXITY_DEFAULT - A combination of lowercase, uppercase, numeric, and non-alphanumeric
   * characters.
   */
  COMPLEXITY_DEFAULT = 1,
  UNRECOGNIZED = -1,
}

export function passwordValidationPolicy_ComplexityFromJSON(object: any): PasswordValidationPolicy_Complexity {
  switch (object) {
    case 0:
    case "COMPLEXITY_UNSPECIFIED":
      return PasswordValidationPolicy_Complexity.COMPLEXITY_UNSPECIFIED;
    case 1:
    case "COMPLEXITY_DEFAULT":
      return PasswordValidationPolicy_Complexity.COMPLEXITY_DEFAULT;
    case -1:
    case "UNRECOGNIZED":
    default:
      return PasswordValidationPolicy_Complexity.UNRECOGNIZED;
  }
}

export function passwordValidationPolicy_ComplexityToJSON(object: PasswordValidationPolicy_Complexity): string {
  switch (object) {
    case PasswordValidationPolicy_Complexity.COMPLEXITY_UNSPECIFIED:
      return "COMPLEXITY_UNSPECIFIED";
    case PasswordValidationPolicy_Complexity.COMPLEXITY_DEFAULT:
      return "COMPLEXITY_DEFAULT";
    case PasswordValidationPolicy_Complexity.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Operations list response. */
export interface OperationsListResponse {
  /** This is always `sql#operationsList`. */
  kind: string;
  /** List of operation resources. */
  items: Operation[];
  /**
   * The continuation token, used to page through large result sets. Provide
   * this value in a subsequent request to return the next page of results.
   */
  nextPageToken: string;
}

/** Read-replica configuration for connecting to the primary instance. */
export interface ReplicaConfiguration {
  /** This is always `sql#replicaConfiguration`. */
  kind: string;
  /**
   * MySQL specific configuration when replicating from a MySQL on-premises
   * primary instance. Replication configuration information such as the
   * username, password, certificates, and keys are not stored in the instance
   * metadata. The configuration information is used only to set up the
   * replication connection and is stored by MySQL in a file named
   * `master.info` in the data directory.
   */
  mysqlReplicaConfiguration:
    | MySqlReplicaConfiguration
    | undefined;
  /**
   * Specifies if the replica is the failover target. If the field is set to
   * `true` the replica will be designated as a failover replica. In case the
   * primary instance fails, the replica instance will be promoted as the new
   * primary instance. Only one replica can be specified as failover target, and
   * the replica has to be in different zone with the primary instance.
   */
  failoverTarget:
    | boolean
    | undefined;
  /**
   * Optional. Specifies if a SQL Server replica is a cascadable replica. A
   * cascadable replica is a SQL Server cross region replica that supports
   * replica(s) under it.
   */
  cascadableReplica: boolean | undefined;
}

/**
 * Database instance restore from backup context.
 * Backup context contains source instance id and project id.
 */
export interface RestoreBackupContext {
  /** This is always `sql#restoreBackupContext`. */
  kind: string;
  /** The ID of the backup run to restore from. */
  backupRunId: Long;
  /** The ID of the instance that the backup was taken from. */
  instanceId: string;
  /** The full project ID of the source instance. */
  project: string;
}

/** Instance rotate server CA context. */
export interface RotateServerCaContext {
  /** This is always `sql#rotateServerCaContext`. */
  kind: string;
  /**
   * The fingerprint of the next version to be rotated to. If left unspecified,
   * will be rotated to the most recently added server CA version.
   */
  nextVersion: string;
}

/** Data cache configurations. */
export interface DataCacheConfig {
  /** Whether data cache is enabled for the instance. */
  dataCacheEnabled: boolean;
}

/** Database instance settings. */
export interface Settings {
  /**
   * The version of instance settings. This is a required field for update
   * method to make sure concurrent updates are handled properly. During update,
   * use the most recent settingsVersion value for this instance and do not try
   * to update this value.
   */
  settingsVersion:
    | Long
    | undefined;
  /**
   * The App Engine app IDs that can access this instance.
   * (Deprecated) Applied to First Generation instances only.
   *
   * @deprecated
   */
  authorizedGaeApplications: string[];
  /**
   * The tier (or machine type) for this instance, for example
   * `db-custom-1-3840`. WARNING: Changing this restarts the instance.
   */
  tier: string;
  /** This is always `sql#settings`. */
  kind: string;
  /**
   * User-provided labels, represented as a dictionary where each label is a
   * single key value pair.
   */
  userLabels: { [key: string]: string };
  /**
   * Availability type. Potential values:
   * *  `ZONAL`: The instance serves data from only one zone. Outages in that
   * zone affect data accessibility.
   * *  `REGIONAL`: The instance can serve data from more than one zone in a
   * region (it is highly available)./
   *
   * For more information, see [Overview of the High Availability
   * Configuration](https://cloud.google.com/sql/docs/mysql/high-availability).
   */
  availabilityType: SqlAvailabilityType;
  /**
   * The pricing plan for this instance. This can be either `PER_USE` or
   * `PACKAGE`. Only `PER_USE` is supported for Second Generation instances.
   */
  pricingPlan: SqlPricingPlan;
  /**
   * The type of replication this instance uses. This can be either
   * `ASYNCHRONOUS` or `SYNCHRONOUS`. (Deprecated) This property was only
   * applicable to First Generation instances.
   *
   * @deprecated
   */
  replicationType: SqlReplicationType;
  /**
   * The maximum size to which storage capacity can be automatically increased.
   * The default value is 0, which specifies that there is no limit.
   */
  storageAutoResizeLimit:
    | Long
    | undefined;
  /**
   * The activation policy specifies when the instance is activated; it is
   * applicable only when the instance state is RUNNABLE. Valid values:
   * *  `ALWAYS`: The instance is on, and remains so even in the absence of
   * connection requests.
   * *  `NEVER`: The instance is off; it is not activated, even if a
   * connection request arrives.
   */
  activationPolicy: Settings_SqlActivationPolicy;
  /**
   * The settings for IP Management. This allows to enable or disable the
   * instance IP and manage which external networks can connect to the instance.
   * The IPv4 address cannot be disabled for Second Generation instances.
   */
  ipConfiguration:
    | IpConfiguration
    | undefined;
  /**
   * Configuration to increase storage size automatically. The default value is
   * true.
   */
  storageAutoResize:
    | boolean
    | undefined;
  /**
   * The location preference settings. This allows the instance to be located as
   * near as possible to either an App Engine app or Compute Engine zone for
   * better performance. App Engine co-location was only applicable to First
   * Generation instances.
   */
  locationPreference:
    | LocationPreference
    | undefined;
  /** The database flags passed to the instance at startup. */
  databaseFlags: DatabaseFlags[];
  /**
   * The type of data disk: `PD_SSD` (default) or `PD_HDD`. Not used for
   * First Generation instances.
   */
  dataDiskType: SqlDataDiskType;
  /**
   * The maintenance window for this instance. This specifies when the instance
   * can be restarted for maintenance purposes.
   */
  maintenanceWindow:
    | MaintenanceWindow
    | undefined;
  /** The daily backup configuration for the instance. */
  backupConfiguration:
    | BackupConfiguration
    | undefined;
  /**
   * Configuration specific to read replica instances. Indicates whether
   * replication is enabled or not. WARNING: Changing this restarts the
   * instance.
   */
  databaseReplicationEnabled:
    | boolean
    | undefined;
  /**
   * Configuration specific to read replica instances. Indicates whether
   * database flags for crash-safe replication are enabled. This property was
   * only applicable to First Generation instances.
   *
   * @deprecated
   */
  crashSafeReplicationEnabled:
    | boolean
    | undefined;
  /** The size of data disk, in GB. The data disk size minimum is 10GB. */
  dataDiskSizeGb:
    | Long
    | undefined;
  /** Active Directory configuration, relevant only for Cloud SQL for SQL Server. */
  activeDirectoryConfig:
    | SqlActiveDirectoryConfig
    | undefined;
  /** The name of server Instance collation. */
  collation: string;
  /** Deny maintenance periods */
  denyMaintenancePeriods: DenyMaintenancePeriod[];
  /** Insights configuration, for now relevant only for Postgres. */
  insightsConfig:
    | InsightsConfig
    | undefined;
  /** The local user password validation policy of the instance. */
  passwordValidationPolicy:
    | PasswordValidationPolicy
    | undefined;
  /** SQL Server specific audit configuration. */
  sqlServerAuditConfig:
    | SqlServerAuditConfig
    | undefined;
  /** Optional. The edition of the instance. */
  edition: Settings_Edition;
  /**
   * Specifies if connections must use Cloud SQL connectors.
   * Option values include the following: `NOT_REQUIRED` (Cloud SQL instances
   * can be connected without Cloud SQL
   * Connectors) and `REQUIRED` (Only allow connections that use Cloud SQL
   * Connectors)
   *
   * Note that using REQUIRED disables all existing authorized networks. If
   * this field is not specified when creating a new instance, NOT_REQUIRED is
   * used. If this field is not specified when patching or updating an existing
   * instance, it is left unchanged in the instance.
   */
  connectorEnforcement: Settings_ConnectorEnforcement;
  /** Configuration to protect against accidental instance deletion. */
  deletionProtectionEnabled:
    | boolean
    | undefined;
  /** Server timezone, relevant only for Cloud SQL for SQL Server. */
  timeZone: string;
  /**
   * Specifies advanced machine configuration for the instances relevant only
   * for SQL Server.
   */
  advancedMachineFeatures:
    | AdvancedMachineFeatures
    | undefined;
  /** Configuration for data cache. */
  dataCacheConfig:
    | DataCacheConfig
    | undefined;
  /**
   * Optional. When this parameter is set to true, Cloud SQL instances can
   * connect to Vertex AI to pass requests for real-time predictions and
   * insights to the AI. The default value is false. This applies only to Cloud
   * SQL for PostgreSQL instances.
   */
  enableGoogleMlIntegration:
    | boolean
    | undefined;
  /**
   * Optional. By default, Cloud SQL instances have schema extraction disabled
   * for Dataplex. When this parameter is set to true, schema extraction for
   * Dataplex on Cloud SQL instances is activated.
   */
  enableDataplexIntegration: boolean | undefined;
}

/** Specifies when the instance is activated. */
export enum Settings_SqlActivationPolicy {
  /** SQL_ACTIVATION_POLICY_UNSPECIFIED - Unknown activation plan. */
  SQL_ACTIVATION_POLICY_UNSPECIFIED = 0,
  /** ALWAYS - The instance is always up and running. */
  ALWAYS = 1,
  /** NEVER - The instance never starts. */
  NEVER = 2,
  /**
   * ON_DEMAND - The instance starts upon receiving requests.
   *
   * @deprecated
   */
  ON_DEMAND = 3,
  UNRECOGNIZED = -1,
}

export function settings_SqlActivationPolicyFromJSON(object: any): Settings_SqlActivationPolicy {
  switch (object) {
    case 0:
    case "SQL_ACTIVATION_POLICY_UNSPECIFIED":
      return Settings_SqlActivationPolicy.SQL_ACTIVATION_POLICY_UNSPECIFIED;
    case 1:
    case "ALWAYS":
      return Settings_SqlActivationPolicy.ALWAYS;
    case 2:
    case "NEVER":
      return Settings_SqlActivationPolicy.NEVER;
    case 3:
    case "ON_DEMAND":
      return Settings_SqlActivationPolicy.ON_DEMAND;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Settings_SqlActivationPolicy.UNRECOGNIZED;
  }
}

export function settings_SqlActivationPolicyToJSON(object: Settings_SqlActivationPolicy): string {
  switch (object) {
    case Settings_SqlActivationPolicy.SQL_ACTIVATION_POLICY_UNSPECIFIED:
      return "SQL_ACTIVATION_POLICY_UNSPECIFIED";
    case Settings_SqlActivationPolicy.ALWAYS:
      return "ALWAYS";
    case Settings_SqlActivationPolicy.NEVER:
      return "NEVER";
    case Settings_SqlActivationPolicy.ON_DEMAND:
      return "ON_DEMAND";
    case Settings_SqlActivationPolicy.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The edition of the instance, can be ENTERPRISE or ENTERPRISE_PLUS. */
export enum Settings_Edition {
  /** EDITION_UNSPECIFIED - The instance did not specify the edition. */
  EDITION_UNSPECIFIED = 0,
  /** ENTERPRISE - The instance is an enterprise edition. */
  ENTERPRISE = 2,
  /** ENTERPRISE_PLUS - The instance is an Enterprise Plus edition. */
  ENTERPRISE_PLUS = 3,
  UNRECOGNIZED = -1,
}

export function settings_EditionFromJSON(object: any): Settings_Edition {
  switch (object) {
    case 0:
    case "EDITION_UNSPECIFIED":
      return Settings_Edition.EDITION_UNSPECIFIED;
    case 2:
    case "ENTERPRISE":
      return Settings_Edition.ENTERPRISE;
    case 3:
    case "ENTERPRISE_PLUS":
      return Settings_Edition.ENTERPRISE_PLUS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Settings_Edition.UNRECOGNIZED;
  }
}

export function settings_EditionToJSON(object: Settings_Edition): string {
  switch (object) {
    case Settings_Edition.EDITION_UNSPECIFIED:
      return "EDITION_UNSPECIFIED";
    case Settings_Edition.ENTERPRISE:
      return "ENTERPRISE";
    case Settings_Edition.ENTERPRISE_PLUS:
      return "ENTERPRISE_PLUS";
    case Settings_Edition.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The options for enforcing Cloud SQL connectors in the instance. */
export enum Settings_ConnectorEnforcement {
  /** CONNECTOR_ENFORCEMENT_UNSPECIFIED - The requirement for Cloud SQL connectors is unknown. */
  CONNECTOR_ENFORCEMENT_UNSPECIFIED = 0,
  /** NOT_REQUIRED - Do not require Cloud SQL connectors. */
  NOT_REQUIRED = 1,
  /**
   * REQUIRED - Require all connections to use Cloud SQL connectors, including the
   * Cloud SQL Auth Proxy and Cloud SQL Java, Python, and Go connectors.
   * Note: This disables all existing authorized networks.
   */
  REQUIRED = 2,
  UNRECOGNIZED = -1,
}

export function settings_ConnectorEnforcementFromJSON(object: any): Settings_ConnectorEnforcement {
  switch (object) {
    case 0:
    case "CONNECTOR_ENFORCEMENT_UNSPECIFIED":
      return Settings_ConnectorEnforcement.CONNECTOR_ENFORCEMENT_UNSPECIFIED;
    case 1:
    case "NOT_REQUIRED":
      return Settings_ConnectorEnforcement.NOT_REQUIRED;
    case 2:
    case "REQUIRED":
      return Settings_ConnectorEnforcement.REQUIRED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Settings_ConnectorEnforcement.UNRECOGNIZED;
  }
}

export function settings_ConnectorEnforcementToJSON(object: Settings_ConnectorEnforcement): string {
  switch (object) {
    case Settings_ConnectorEnforcement.CONNECTOR_ENFORCEMENT_UNSPECIFIED:
      return "CONNECTOR_ENFORCEMENT_UNSPECIFIED";
    case Settings_ConnectorEnforcement.NOT_REQUIRED:
      return "NOT_REQUIRED";
    case Settings_ConnectorEnforcement.REQUIRED:
      return "REQUIRED";
    case Settings_ConnectorEnforcement.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export interface Settings_UserLabelsEntry {
  key: string;
  value: string;
}

/** Specifies options for controlling advanced machine features. */
export interface AdvancedMachineFeatures {
  /** The number of threads per physical core. */
  threadsPerCore: number;
}

/** SslCerts Resource */
export interface SslCert {
  /** This is always `sql#sslCert`. */
  kind: string;
  /** Serial number, as extracted from the certificate. */
  certSerialNumber: string;
  /** PEM representation. */
  cert: string;
  /**
   * The time when the certificate was created in
   * [RFC 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`.
   */
  createTime:
    | Date
    | undefined;
  /** User supplied name.  Constrained to [a-zA-Z.-_ ]+. */
  commonName: string;
  /**
   * The time when the certificate expires in
   * [RFC 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`.
   */
  expirationTime:
    | Date
    | undefined;
  /** Sha1 Fingerprint. */
  sha1Fingerprint: string;
  /** Name of the database instance. */
  instance: string;
  /** The URI of this resource. */
  selfLink: string;
}

/** SslCertDetail. */
export interface SslCertDetail {
  /** The public information about the cert. */
  certInfo:
    | SslCert
    | undefined;
  /**
   * The private key for the client cert, in pem format.  Keep private in order
   * to protect your security.
   */
  certPrivateKey: string;
}

/** SslCerts create ephemeral certificate request. */
export interface SslCertsCreateEphemeralRequest {
  /** PEM encoded public key to include in the signed certificate. */
  publicKey: string;
  /** Access token to include in the signed certificate. */
  accessToken: string;
}

/** SslCerts insert request. */
export interface SslCertsInsertRequest {
  /**
   * User supplied name.  Must be a distinct name from the other certificates
   * for this instance.
   */
  commonName: string;
}

/** Reschedule options for maintenance windows. */
export interface SqlInstancesRescheduleMaintenanceRequestBody {
  /** Required. The type of the reschedule the user wants. */
  reschedule: SqlInstancesRescheduleMaintenanceRequestBody_Reschedule | undefined;
}

export enum SqlInstancesRescheduleMaintenanceRequestBody_RescheduleType {
  RESCHEDULE_TYPE_UNSPECIFIED = 0,
  /** IMMEDIATE - Reschedules maintenance to happen now (within 5 minutes). */
  IMMEDIATE = 1,
  /**
   * NEXT_AVAILABLE_WINDOW - Reschedules maintenance to occur within one week from the originally
   * scheduled day and time.
   */
  NEXT_AVAILABLE_WINDOW = 2,
  /** SPECIFIC_TIME - Reschedules maintenance to a specific time and day. */
  SPECIFIC_TIME = 3,
  UNRECOGNIZED = -1,
}

export function sqlInstancesRescheduleMaintenanceRequestBody_RescheduleTypeFromJSON(
  object: any,
): SqlInstancesRescheduleMaintenanceRequestBody_RescheduleType {
  switch (object) {
    case 0:
    case "RESCHEDULE_TYPE_UNSPECIFIED":
      return SqlInstancesRescheduleMaintenanceRequestBody_RescheduleType.RESCHEDULE_TYPE_UNSPECIFIED;
    case 1:
    case "IMMEDIATE":
      return SqlInstancesRescheduleMaintenanceRequestBody_RescheduleType.IMMEDIATE;
    case 2:
    case "NEXT_AVAILABLE_WINDOW":
      return SqlInstancesRescheduleMaintenanceRequestBody_RescheduleType.NEXT_AVAILABLE_WINDOW;
    case 3:
    case "SPECIFIC_TIME":
      return SqlInstancesRescheduleMaintenanceRequestBody_RescheduleType.SPECIFIC_TIME;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SqlInstancesRescheduleMaintenanceRequestBody_RescheduleType.UNRECOGNIZED;
  }
}

export function sqlInstancesRescheduleMaintenanceRequestBody_RescheduleTypeToJSON(
  object: SqlInstancesRescheduleMaintenanceRequestBody_RescheduleType,
): string {
  switch (object) {
    case SqlInstancesRescheduleMaintenanceRequestBody_RescheduleType.RESCHEDULE_TYPE_UNSPECIFIED:
      return "RESCHEDULE_TYPE_UNSPECIFIED";
    case SqlInstancesRescheduleMaintenanceRequestBody_RescheduleType.IMMEDIATE:
      return "IMMEDIATE";
    case SqlInstancesRescheduleMaintenanceRequestBody_RescheduleType.NEXT_AVAILABLE_WINDOW:
      return "NEXT_AVAILABLE_WINDOW";
    case SqlInstancesRescheduleMaintenanceRequestBody_RescheduleType.SPECIFIC_TIME:
      return "SPECIFIC_TIME";
    case SqlInstancesRescheduleMaintenanceRequestBody_RescheduleType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export interface SqlInstancesRescheduleMaintenanceRequestBody_Reschedule {
  /** Required. The type of the reschedule. */
  rescheduleType: SqlInstancesRescheduleMaintenanceRequestBody_RescheduleType;
  /**
   * Optional. Timestamp when the maintenance shall be rescheduled to if
   * reschedule_type=SPECIFIC_TIME, in
   * [RFC 3339](https://tools.ietf.org/html/rfc3339) format, for example
   * `2012-11-15T16:19:00.094Z`.
   */
  scheduleTime: Date | undefined;
}

/** SslCert insert response. */
export interface SslCertsInsertResponse {
  /** This is always `sql#sslCertsInsert`. */
  kind: string;
  /** The operation to track the ssl certs insert request. */
  operation:
    | Operation
    | undefined;
  /**
   * The server Certificate Authority's certificate.  If this is missing you can
   * force a new one to be generated by calling resetSslConfig method on
   * instances resource.
   */
  serverCaCert:
    | SslCert
    | undefined;
  /** The new client certificate and private key. */
  clientCert: SslCertDetail | undefined;
}

/** SslCerts list response. */
export interface SslCertsListResponse {
  /** This is always `sql#sslCertsList`. */
  kind: string;
  /** List of client certificates for the instance. */
  items: SslCert[];
}

/** Database Instance truncate log context. */
export interface TruncateLogContext {
  /** This is always `sql#truncateLogContext`. */
  kind: string;
  /**
   * The type of log to truncate. Valid values are `MYSQL_GENERAL_TABLE` and
   * `MYSQL_SLOW_TABLE`.
   */
  logType: string;
}

/** Active Directory configuration, relevant only for Cloud SQL for SQL Server. */
export interface SqlActiveDirectoryConfig {
  /** This is always sql#activeDirectoryConfig. */
  kind: string;
  /** The name of the domain (e.g., mydomain.com). */
  domain: string;
}

/** SQL Server specific audit configuration. */
export interface SqlServerAuditConfig {
  /** This is always sql#sqlServerAuditConfig */
  kind: string;
  /** The name of the destination bucket (e.g., gs://mybucket). */
  bucket: string;
  /** How long to keep generated audit files. */
  retentionInterval:
    | Duration
    | undefined;
  /** How often to upload generated audit files. */
  uploadInterval: Duration | undefined;
}

/** Acquire SSRS lease context. */
export interface AcquireSsrsLeaseContext {
  /**
   * The username to be used as the setup login to connect to the database
   * server for SSRS setup.
   */
  setupLogin?:
    | string
    | undefined;
  /**
   * The username to be used as the service login to connect to the report
   * database for SSRS setup.
   */
  serviceLogin?:
    | string
    | undefined;
  /** The report database to be used for the SSRS setup. */
  reportDatabase?:
    | string
    | undefined;
  /** Lease duration needed for the SSRS setup. */
  duration?: Duration | undefined;
}

function createBaseAclEntry(): AclEntry {
  return { value: "", expirationTime: undefined, name: "", kind: "" };
}

export const AclEntry: MessageFns<AclEntry> = {
  encode(message: AclEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.value !== "") {
      writer.uint32(10).string(message.value);
    }
    if (message.expirationTime !== undefined) {
      Timestamp.encode(toTimestamp(message.expirationTime), writer.uint32(18).fork()).join();
    }
    if (message.name !== "") {
      writer.uint32(26).string(message.name);
    }
    if (message.kind !== "") {
      writer.uint32(34).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AclEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAclEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.value = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.expirationTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.name = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AclEntry {
    return {
      value: isSet(object.value) ? globalThis.String(object.value) : "",
      expirationTime: isSet(object.expirationTime) ? fromJsonTimestamp(object.expirationTime) : undefined,
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: AclEntry): unknown {
    const obj: any = {};
    if (message.value !== "") {
      obj.value = message.value;
    }
    if (message.expirationTime !== undefined) {
      obj.expirationTime = message.expirationTime.toISOString();
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<AclEntry>): AclEntry {
    return AclEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AclEntry>): AclEntry {
    const message = createBaseAclEntry();
    message.value = object.value ?? "";
    message.expirationTime = object.expirationTime ?? undefined;
    message.name = object.name ?? "";
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseApiWarning(): ApiWarning {
  return { code: 0, message: "", region: "" };
}

export const ApiWarning: MessageFns<ApiWarning> = {
  encode(message: ApiWarning, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.code !== 0) {
      writer.uint32(8).int32(message.code);
    }
    if (message.message !== "") {
      writer.uint32(18).string(message.message);
    }
    if (message.region !== "") {
      writer.uint32(26).string(message.region);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ApiWarning {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseApiWarning();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.code = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.message = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.region = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ApiWarning {
    return {
      code: isSet(object.code) ? apiWarning_SqlApiWarningCodeFromJSON(object.code) : 0,
      message: isSet(object.message) ? globalThis.String(object.message) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
    };
  },

  toJSON(message: ApiWarning): unknown {
    const obj: any = {};
    if (message.code !== 0) {
      obj.code = apiWarning_SqlApiWarningCodeToJSON(message.code);
    }
    if (message.message !== "") {
      obj.message = message.message;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    return obj;
  },

  create(base?: DeepPartial<ApiWarning>): ApiWarning {
    return ApiWarning.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ApiWarning>): ApiWarning {
    const message = createBaseApiWarning();
    message.code = object.code ?? 0;
    message.message = object.message ?? "";
    message.region = object.region ?? "";
    return message;
  },
};

function createBaseBackupRetentionSettings(): BackupRetentionSettings {
  return { retentionUnit: 0, retainedBackups: undefined };
}

export const BackupRetentionSettings: MessageFns<BackupRetentionSettings> = {
  encode(message: BackupRetentionSettings, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.retentionUnit !== 0) {
      writer.uint32(8).int32(message.retentionUnit);
    }
    if (message.retainedBackups !== undefined) {
      Int32Value.encode({ value: message.retainedBackups! }, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BackupRetentionSettings {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackupRetentionSettings();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.retentionUnit = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.retainedBackups = Int32Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BackupRetentionSettings {
    return {
      retentionUnit: isSet(object.retentionUnit)
        ? backupRetentionSettings_RetentionUnitFromJSON(object.retentionUnit)
        : 0,
      retainedBackups: isSet(object.retainedBackups) ? Number(object.retainedBackups) : undefined,
    };
  },

  toJSON(message: BackupRetentionSettings): unknown {
    const obj: any = {};
    if (message.retentionUnit !== 0) {
      obj.retentionUnit = backupRetentionSettings_RetentionUnitToJSON(message.retentionUnit);
    }
    if (message.retainedBackups !== undefined) {
      obj.retainedBackups = message.retainedBackups;
    }
    return obj;
  },

  create(base?: DeepPartial<BackupRetentionSettings>): BackupRetentionSettings {
    return BackupRetentionSettings.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BackupRetentionSettings>): BackupRetentionSettings {
    const message = createBaseBackupRetentionSettings();
    message.retentionUnit = object.retentionUnit ?? 0;
    message.retainedBackups = object.retainedBackups ?? undefined;
    return message;
  },
};

function createBaseBackupConfiguration(): BackupConfiguration {
  return {
    startTime: "",
    enabled: undefined,
    kind: "",
    binaryLogEnabled: undefined,
    replicationLogArchivingEnabled: undefined,
    location: "",
    pointInTimeRecoveryEnabled: undefined,
    transactionLogRetentionDays: undefined,
    backupRetentionSettings: undefined,
    transactionalLogStorageState: undefined,
  };
}

export const BackupConfiguration: MessageFns<BackupConfiguration> = {
  encode(message: BackupConfiguration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startTime !== "") {
      writer.uint32(10).string(message.startTime);
    }
    if (message.enabled !== undefined) {
      BoolValue.encode({ value: message.enabled! }, writer.uint32(18).fork()).join();
    }
    if (message.kind !== "") {
      writer.uint32(26).string(message.kind);
    }
    if (message.binaryLogEnabled !== undefined) {
      BoolValue.encode({ value: message.binaryLogEnabled! }, writer.uint32(34).fork()).join();
    }
    if (message.replicationLogArchivingEnabled !== undefined) {
      BoolValue.encode({ value: message.replicationLogArchivingEnabled! }, writer.uint32(42).fork()).join();
    }
    if (message.location !== "") {
      writer.uint32(50).string(message.location);
    }
    if (message.pointInTimeRecoveryEnabled !== undefined) {
      BoolValue.encode({ value: message.pointInTimeRecoveryEnabled! }, writer.uint32(58).fork()).join();
    }
    if (message.transactionLogRetentionDays !== undefined) {
      Int32Value.encode({ value: message.transactionLogRetentionDays! }, writer.uint32(74).fork()).join();
    }
    if (message.backupRetentionSettings !== undefined) {
      BackupRetentionSettings.encode(message.backupRetentionSettings, writer.uint32(82).fork()).join();
    }
    if (message.transactionalLogStorageState !== undefined) {
      writer.uint32(88).int32(message.transactionalLogStorageState);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BackupConfiguration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackupConfiguration();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startTime = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.enabled = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.binaryLogEnabled = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.replicationLogArchivingEnabled = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.location = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.pointInTimeRecoveryEnabled = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.transactionLogRetentionDays = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.backupRetentionSettings = BackupRetentionSettings.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.transactionalLogStorageState = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BackupConfiguration {
    return {
      startTime: isSet(object.startTime) ? globalThis.String(object.startTime) : "",
      enabled: isSet(object.enabled) ? Boolean(object.enabled) : undefined,
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      binaryLogEnabled: isSet(object.binaryLogEnabled) ? Boolean(object.binaryLogEnabled) : undefined,
      replicationLogArchivingEnabled: isSet(object.replicationLogArchivingEnabled)
        ? Boolean(object.replicationLogArchivingEnabled)
        : undefined,
      location: isSet(object.location) ? globalThis.String(object.location) : "",
      pointInTimeRecoveryEnabled: isSet(object.pointInTimeRecoveryEnabled)
        ? Boolean(object.pointInTimeRecoveryEnabled)
        : undefined,
      transactionLogRetentionDays: isSet(object.transactionLogRetentionDays)
        ? Number(object.transactionLogRetentionDays)
        : undefined,
      backupRetentionSettings: isSet(object.backupRetentionSettings)
        ? BackupRetentionSettings.fromJSON(object.backupRetentionSettings)
        : undefined,
      transactionalLogStorageState: isSet(object.transactionalLogStorageState)
        ? backupConfiguration_TransactionalLogStorageStateFromJSON(object.transactionalLogStorageState)
        : undefined,
    };
  },

  toJSON(message: BackupConfiguration): unknown {
    const obj: any = {};
    if (message.startTime !== "") {
      obj.startTime = message.startTime;
    }
    if (message.enabled !== undefined) {
      obj.enabled = message.enabled;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.binaryLogEnabled !== undefined) {
      obj.binaryLogEnabled = message.binaryLogEnabled;
    }
    if (message.replicationLogArchivingEnabled !== undefined) {
      obj.replicationLogArchivingEnabled = message.replicationLogArchivingEnabled;
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    if (message.pointInTimeRecoveryEnabled !== undefined) {
      obj.pointInTimeRecoveryEnabled = message.pointInTimeRecoveryEnabled;
    }
    if (message.transactionLogRetentionDays !== undefined) {
      obj.transactionLogRetentionDays = message.transactionLogRetentionDays;
    }
    if (message.backupRetentionSettings !== undefined) {
      obj.backupRetentionSettings = BackupRetentionSettings.toJSON(message.backupRetentionSettings);
    }
    if (message.transactionalLogStorageState !== undefined) {
      obj.transactionalLogStorageState = backupConfiguration_TransactionalLogStorageStateToJSON(
        message.transactionalLogStorageState,
      );
    }
    return obj;
  },

  create(base?: DeepPartial<BackupConfiguration>): BackupConfiguration {
    return BackupConfiguration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BackupConfiguration>): BackupConfiguration {
    const message = createBaseBackupConfiguration();
    message.startTime = object.startTime ?? "";
    message.enabled = object.enabled ?? undefined;
    message.kind = object.kind ?? "";
    message.binaryLogEnabled = object.binaryLogEnabled ?? undefined;
    message.replicationLogArchivingEnabled = object.replicationLogArchivingEnabled ?? undefined;
    message.location = object.location ?? "";
    message.pointInTimeRecoveryEnabled = object.pointInTimeRecoveryEnabled ?? undefined;
    message.transactionLogRetentionDays = object.transactionLogRetentionDays ?? undefined;
    message.backupRetentionSettings =
      (object.backupRetentionSettings !== undefined && object.backupRetentionSettings !== null)
        ? BackupRetentionSettings.fromPartial(object.backupRetentionSettings)
        : undefined;
    message.transactionalLogStorageState = object.transactionalLogStorageState ?? undefined;
    return message;
  },
};

function createBaseBackupRun(): BackupRun {
  return {
    kind: "",
    status: 0,
    enqueuedTime: undefined,
    id: Long.ZERO,
    startTime: undefined,
    endTime: undefined,
    error: undefined,
    type: 0,
    description: "",
    windowStartTime: undefined,
    instance: "",
    selfLink: "",
    location: "",
    diskEncryptionConfiguration: undefined,
    diskEncryptionStatus: undefined,
    backupKind: 0,
    timeZone: "",
  };
}

export const BackupRun: MessageFns<BackupRun> = {
  encode(message: BackupRun, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.status !== 0) {
      writer.uint32(16).int32(message.status);
    }
    if (message.enqueuedTime !== undefined) {
      Timestamp.encode(toTimestamp(message.enqueuedTime), writer.uint32(26).fork()).join();
    }
    if (!message.id.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.id.toString());
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(42).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(50).fork()).join();
    }
    if (message.error !== undefined) {
      OperationError.encode(message.error, writer.uint32(58).fork()).join();
    }
    if (message.type !== 0) {
      writer.uint32(64).int32(message.type);
    }
    if (message.description !== "") {
      writer.uint32(74).string(message.description);
    }
    if (message.windowStartTime !== undefined) {
      Timestamp.encode(toTimestamp(message.windowStartTime), writer.uint32(82).fork()).join();
    }
    if (message.instance !== "") {
      writer.uint32(90).string(message.instance);
    }
    if (message.selfLink !== "") {
      writer.uint32(98).string(message.selfLink);
    }
    if (message.location !== "") {
      writer.uint32(106).string(message.location);
    }
    if (message.diskEncryptionConfiguration !== undefined) {
      DiskEncryptionConfiguration.encode(message.diskEncryptionConfiguration, writer.uint32(130).fork()).join();
    }
    if (message.diskEncryptionStatus !== undefined) {
      DiskEncryptionStatus.encode(message.diskEncryptionStatus, writer.uint32(138).fork()).join();
    }
    if (message.backupKind !== 0) {
      writer.uint32(152).int32(message.backupKind);
    }
    if (message.timeZone !== "") {
      writer.uint32(186).string(message.timeZone);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BackupRun {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackupRun();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.status = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.enqueuedTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.id = Long.fromString(reader.int64().toString());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.error = OperationError.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.description = reader.string();
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.windowStartTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.instance = reader.string();
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.selfLink = reader.string();
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.location = reader.string();
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.diskEncryptionConfiguration = DiskEncryptionConfiguration.decode(reader, reader.uint32());
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.diskEncryptionStatus = DiskEncryptionStatus.decode(reader, reader.uint32());
          continue;
        case 19:
          if (tag !== 152) {
            break;
          }

          message.backupKind = reader.int32() as any;
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.timeZone = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BackupRun {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      status: isSet(object.status) ? sqlBackupRunStatusFromJSON(object.status) : 0,
      enqueuedTime: isSet(object.enqueuedTime) ? fromJsonTimestamp(object.enqueuedTime) : undefined,
      id: isSet(object.id) ? Long.fromValue(object.id) : Long.ZERO,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      error: isSet(object.error) ? OperationError.fromJSON(object.error) : undefined,
      type: isSet(object.type) ? sqlBackupRunTypeFromJSON(object.type) : 0,
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      windowStartTime: isSet(object.windowStartTime) ? fromJsonTimestamp(object.windowStartTime) : undefined,
      instance: isSet(object.instance) ? globalThis.String(object.instance) : "",
      selfLink: isSet(object.selfLink) ? globalThis.String(object.selfLink) : "",
      location: isSet(object.location) ? globalThis.String(object.location) : "",
      diskEncryptionConfiguration: isSet(object.diskEncryptionConfiguration)
        ? DiskEncryptionConfiguration.fromJSON(object.diskEncryptionConfiguration)
        : undefined,
      diskEncryptionStatus: isSet(object.diskEncryptionStatus)
        ? DiskEncryptionStatus.fromJSON(object.diskEncryptionStatus)
        : undefined,
      backupKind: isSet(object.backupKind) ? sqlBackupKindFromJSON(object.backupKind) : 0,
      timeZone: isSet(object.timeZone) ? globalThis.String(object.timeZone) : "",
    };
  },

  toJSON(message: BackupRun): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.status !== 0) {
      obj.status = sqlBackupRunStatusToJSON(message.status);
    }
    if (message.enqueuedTime !== undefined) {
      obj.enqueuedTime = message.enqueuedTime.toISOString();
    }
    if (!message.id.equals(Long.ZERO)) {
      obj.id = (message.id || Long.ZERO).toString();
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (message.error !== undefined) {
      obj.error = OperationError.toJSON(message.error);
    }
    if (message.type !== 0) {
      obj.type = sqlBackupRunTypeToJSON(message.type);
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.windowStartTime !== undefined) {
      obj.windowStartTime = message.windowStartTime.toISOString();
    }
    if (message.instance !== "") {
      obj.instance = message.instance;
    }
    if (message.selfLink !== "") {
      obj.selfLink = message.selfLink;
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    if (message.diskEncryptionConfiguration !== undefined) {
      obj.diskEncryptionConfiguration = DiskEncryptionConfiguration.toJSON(message.diskEncryptionConfiguration);
    }
    if (message.diskEncryptionStatus !== undefined) {
      obj.diskEncryptionStatus = DiskEncryptionStatus.toJSON(message.diskEncryptionStatus);
    }
    if (message.backupKind !== 0) {
      obj.backupKind = sqlBackupKindToJSON(message.backupKind);
    }
    if (message.timeZone !== "") {
      obj.timeZone = message.timeZone;
    }
    return obj;
  },

  create(base?: DeepPartial<BackupRun>): BackupRun {
    return BackupRun.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BackupRun>): BackupRun {
    const message = createBaseBackupRun();
    message.kind = object.kind ?? "";
    message.status = object.status ?? 0;
    message.enqueuedTime = object.enqueuedTime ?? undefined;
    message.id = (object.id !== undefined && object.id !== null) ? Long.fromValue(object.id) : Long.ZERO;
    message.startTime = object.startTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    message.error = (object.error !== undefined && object.error !== null)
      ? OperationError.fromPartial(object.error)
      : undefined;
    message.type = object.type ?? 0;
    message.description = object.description ?? "";
    message.windowStartTime = object.windowStartTime ?? undefined;
    message.instance = object.instance ?? "";
    message.selfLink = object.selfLink ?? "";
    message.location = object.location ?? "";
    message.diskEncryptionConfiguration =
      (object.diskEncryptionConfiguration !== undefined && object.diskEncryptionConfiguration !== null)
        ? DiskEncryptionConfiguration.fromPartial(object.diskEncryptionConfiguration)
        : undefined;
    message.diskEncryptionStatus = (object.diskEncryptionStatus !== undefined && object.diskEncryptionStatus !== null)
      ? DiskEncryptionStatus.fromPartial(object.diskEncryptionStatus)
      : undefined;
    message.backupKind = object.backupKind ?? 0;
    message.timeZone = object.timeZone ?? "";
    return message;
  },
};

function createBaseBackupRunsListResponse(): BackupRunsListResponse {
  return { kind: "", items: [], nextPageToken: "" };
}

export const BackupRunsListResponse: MessageFns<BackupRunsListResponse> = {
  encode(message: BackupRunsListResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    for (const v of message.items) {
      BackupRun.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(26).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BackupRunsListResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackupRunsListResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.items.push(BackupRun.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BackupRunsListResponse {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      items: globalThis.Array.isArray(object?.items) ? object.items.map((e: any) => BackupRun.fromJSON(e)) : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: BackupRunsListResponse): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.items?.length) {
      obj.items = message.items.map((e) => BackupRun.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<BackupRunsListResponse>): BackupRunsListResponse {
    return BackupRunsListResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BackupRunsListResponse>): BackupRunsListResponse {
    const message = createBaseBackupRunsListResponse();
    message.kind = object.kind ?? "";
    message.items = object.items?.map((e) => BackupRun.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBaseBinLogCoordinates(): BinLogCoordinates {
  return { binLogFileName: "", binLogPosition: Long.ZERO, kind: "" };
}

export const BinLogCoordinates: MessageFns<BinLogCoordinates> = {
  encode(message: BinLogCoordinates, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.binLogFileName !== "") {
      writer.uint32(10).string(message.binLogFileName);
    }
    if (!message.binLogPosition.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.binLogPosition.toString());
    }
    if (message.kind !== "") {
      writer.uint32(26).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BinLogCoordinates {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBinLogCoordinates();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.binLogFileName = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.binLogPosition = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BinLogCoordinates {
    return {
      binLogFileName: isSet(object.binLogFileName) ? globalThis.String(object.binLogFileName) : "",
      binLogPosition: isSet(object.binLogPosition) ? Long.fromValue(object.binLogPosition) : Long.ZERO,
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: BinLogCoordinates): unknown {
    const obj: any = {};
    if (message.binLogFileName !== "") {
      obj.binLogFileName = message.binLogFileName;
    }
    if (!message.binLogPosition.equals(Long.ZERO)) {
      obj.binLogPosition = (message.binLogPosition || Long.ZERO).toString();
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<BinLogCoordinates>): BinLogCoordinates {
    return BinLogCoordinates.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BinLogCoordinates>): BinLogCoordinates {
    const message = createBaseBinLogCoordinates();
    message.binLogFileName = object.binLogFileName ?? "";
    message.binLogPosition = (object.binLogPosition !== undefined && object.binLogPosition !== null)
      ? Long.fromValue(object.binLogPosition)
      : Long.ZERO;
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseBackupContext(): BackupContext {
  return { backupId: Long.ZERO, kind: "" };
}

export const BackupContext: MessageFns<BackupContext> = {
  encode(message: BackupContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.backupId.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.backupId.toString());
    }
    if (message.kind !== "") {
      writer.uint32(18).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BackupContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBackupContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.backupId = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BackupContext {
    return {
      backupId: isSet(object.backupId) ? Long.fromValue(object.backupId) : Long.ZERO,
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: BackupContext): unknown {
    const obj: any = {};
    if (!message.backupId.equals(Long.ZERO)) {
      obj.backupId = (message.backupId || Long.ZERO).toString();
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<BackupContext>): BackupContext {
    return BackupContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BackupContext>): BackupContext {
    const message = createBaseBackupContext();
    message.backupId = (object.backupId !== undefined && object.backupId !== null)
      ? Long.fromValue(object.backupId)
      : Long.ZERO;
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseCloneContext(): CloneContext {
  return {
    kind: "",
    pitrTimestampMs: Long.ZERO,
    destinationInstanceName: "",
    binLogCoordinates: undefined,
    pointInTime: undefined,
    allocatedIpRange: "",
    databaseNames: [],
    preferredZone: undefined,
    preferredSecondaryZone: undefined,
  };
}

export const CloneContext: MessageFns<CloneContext> = {
  encode(message: CloneContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (!message.pitrTimestampMs.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.pitrTimestampMs.toString());
    }
    if (message.destinationInstanceName !== "") {
      writer.uint32(26).string(message.destinationInstanceName);
    }
    if (message.binLogCoordinates !== undefined) {
      BinLogCoordinates.encode(message.binLogCoordinates, writer.uint32(34).fork()).join();
    }
    if (message.pointInTime !== undefined) {
      Timestamp.encode(toTimestamp(message.pointInTime), writer.uint32(42).fork()).join();
    }
    if (message.allocatedIpRange !== "") {
      writer.uint32(50).string(message.allocatedIpRange);
    }
    for (const v of message.databaseNames) {
      writer.uint32(74).string(v!);
    }
    if (message.preferredZone !== undefined) {
      writer.uint32(82).string(message.preferredZone);
    }
    if (message.preferredSecondaryZone !== undefined) {
      writer.uint32(90).string(message.preferredSecondaryZone);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CloneContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCloneContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pitrTimestampMs = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.destinationInstanceName = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.binLogCoordinates = BinLogCoordinates.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.pointInTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.allocatedIpRange = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.databaseNames.push(reader.string());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.preferredZone = reader.string();
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.preferredSecondaryZone = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CloneContext {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      pitrTimestampMs: isSet(object.pitrTimestampMs) ? Long.fromValue(object.pitrTimestampMs) : Long.ZERO,
      destinationInstanceName: isSet(object.destinationInstanceName)
        ? globalThis.String(object.destinationInstanceName)
        : "",
      binLogCoordinates: isSet(object.binLogCoordinates)
        ? BinLogCoordinates.fromJSON(object.binLogCoordinates)
        : undefined,
      pointInTime: isSet(object.pointInTime) ? fromJsonTimestamp(object.pointInTime) : undefined,
      allocatedIpRange: isSet(object.allocatedIpRange) ? globalThis.String(object.allocatedIpRange) : "",
      databaseNames: globalThis.Array.isArray(object?.databaseNames)
        ? object.databaseNames.map((e: any) => globalThis.String(e))
        : [],
      preferredZone: isSet(object.preferredZone) ? globalThis.String(object.preferredZone) : undefined,
      preferredSecondaryZone: isSet(object.preferredSecondaryZone)
        ? globalThis.String(object.preferredSecondaryZone)
        : undefined,
    };
  },

  toJSON(message: CloneContext): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (!message.pitrTimestampMs.equals(Long.ZERO)) {
      obj.pitrTimestampMs = (message.pitrTimestampMs || Long.ZERO).toString();
    }
    if (message.destinationInstanceName !== "") {
      obj.destinationInstanceName = message.destinationInstanceName;
    }
    if (message.binLogCoordinates !== undefined) {
      obj.binLogCoordinates = BinLogCoordinates.toJSON(message.binLogCoordinates);
    }
    if (message.pointInTime !== undefined) {
      obj.pointInTime = message.pointInTime.toISOString();
    }
    if (message.allocatedIpRange !== "") {
      obj.allocatedIpRange = message.allocatedIpRange;
    }
    if (message.databaseNames?.length) {
      obj.databaseNames = message.databaseNames;
    }
    if (message.preferredZone !== undefined) {
      obj.preferredZone = message.preferredZone;
    }
    if (message.preferredSecondaryZone !== undefined) {
      obj.preferredSecondaryZone = message.preferredSecondaryZone;
    }
    return obj;
  },

  create(base?: DeepPartial<CloneContext>): CloneContext {
    return CloneContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CloneContext>): CloneContext {
    const message = createBaseCloneContext();
    message.kind = object.kind ?? "";
    message.pitrTimestampMs = (object.pitrTimestampMs !== undefined && object.pitrTimestampMs !== null)
      ? Long.fromValue(object.pitrTimestampMs)
      : Long.ZERO;
    message.destinationInstanceName = object.destinationInstanceName ?? "";
    message.binLogCoordinates = (object.binLogCoordinates !== undefined && object.binLogCoordinates !== null)
      ? BinLogCoordinates.fromPartial(object.binLogCoordinates)
      : undefined;
    message.pointInTime = object.pointInTime ?? undefined;
    message.allocatedIpRange = object.allocatedIpRange ?? "";
    message.databaseNames = object.databaseNames?.map((e) => e) || [];
    message.preferredZone = object.preferredZone ?? undefined;
    message.preferredSecondaryZone = object.preferredSecondaryZone ?? undefined;
    return message;
  },
};

function createBaseDatabase(): Database {
  return {
    kind: "",
    charset: "",
    collation: "",
    etag: "",
    name: "",
    instance: "",
    selfLink: "",
    project: "",
    sqlserverDatabaseDetails: undefined,
  };
}

export const Database: MessageFns<Database> = {
  encode(message: Database, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.charset !== "") {
      writer.uint32(18).string(message.charset);
    }
    if (message.collation !== "") {
      writer.uint32(26).string(message.collation);
    }
    if (message.etag !== "") {
      writer.uint32(34).string(message.etag);
    }
    if (message.name !== "") {
      writer.uint32(42).string(message.name);
    }
    if (message.instance !== "") {
      writer.uint32(50).string(message.instance);
    }
    if (message.selfLink !== "") {
      writer.uint32(58).string(message.selfLink);
    }
    if (message.project !== "") {
      writer.uint32(66).string(message.project);
    }
    if (message.sqlserverDatabaseDetails !== undefined) {
      SqlServerDatabaseDetails.encode(message.sqlserverDatabaseDetails, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Database {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatabase();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.charset = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.collation = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.etag = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.name = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.instance = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.selfLink = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.project = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.sqlserverDatabaseDetails = SqlServerDatabaseDetails.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Database {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      charset: isSet(object.charset) ? globalThis.String(object.charset) : "",
      collation: isSet(object.collation) ? globalThis.String(object.collation) : "",
      etag: isSet(object.etag) ? globalThis.String(object.etag) : "",
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      instance: isSet(object.instance) ? globalThis.String(object.instance) : "",
      selfLink: isSet(object.selfLink) ? globalThis.String(object.selfLink) : "",
      project: isSet(object.project) ? globalThis.String(object.project) : "",
      sqlserverDatabaseDetails: isSet(object.sqlserverDatabaseDetails)
        ? SqlServerDatabaseDetails.fromJSON(object.sqlserverDatabaseDetails)
        : undefined,
    };
  },

  toJSON(message: Database): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.charset !== "") {
      obj.charset = message.charset;
    }
    if (message.collation !== "") {
      obj.collation = message.collation;
    }
    if (message.etag !== "") {
      obj.etag = message.etag;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.instance !== "") {
      obj.instance = message.instance;
    }
    if (message.selfLink !== "") {
      obj.selfLink = message.selfLink;
    }
    if (message.project !== "") {
      obj.project = message.project;
    }
    if (message.sqlserverDatabaseDetails !== undefined) {
      obj.sqlserverDatabaseDetails = SqlServerDatabaseDetails.toJSON(message.sqlserverDatabaseDetails);
    }
    return obj;
  },

  create(base?: DeepPartial<Database>): Database {
    return Database.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Database>): Database {
    const message = createBaseDatabase();
    message.kind = object.kind ?? "";
    message.charset = object.charset ?? "";
    message.collation = object.collation ?? "";
    message.etag = object.etag ?? "";
    message.name = object.name ?? "";
    message.instance = object.instance ?? "";
    message.selfLink = object.selfLink ?? "";
    message.project = object.project ?? "";
    message.sqlserverDatabaseDetails =
      (object.sqlserverDatabaseDetails !== undefined && object.sqlserverDatabaseDetails !== null)
        ? SqlServerDatabaseDetails.fromPartial(object.sqlserverDatabaseDetails)
        : undefined;
    return message;
  },
};

function createBaseSqlServerDatabaseDetails(): SqlServerDatabaseDetails {
  return { compatibilityLevel: 0, recoveryModel: "" };
}

export const SqlServerDatabaseDetails: MessageFns<SqlServerDatabaseDetails> = {
  encode(message: SqlServerDatabaseDetails, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.compatibilityLevel !== 0) {
      writer.uint32(8).int32(message.compatibilityLevel);
    }
    if (message.recoveryModel !== "") {
      writer.uint32(18).string(message.recoveryModel);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SqlServerDatabaseDetails {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSqlServerDatabaseDetails();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.compatibilityLevel = reader.int32();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.recoveryModel = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SqlServerDatabaseDetails {
    return {
      compatibilityLevel: isSet(object.compatibilityLevel) ? globalThis.Number(object.compatibilityLevel) : 0,
      recoveryModel: isSet(object.recoveryModel) ? globalThis.String(object.recoveryModel) : "",
    };
  },

  toJSON(message: SqlServerDatabaseDetails): unknown {
    const obj: any = {};
    if (message.compatibilityLevel !== 0) {
      obj.compatibilityLevel = Math.round(message.compatibilityLevel);
    }
    if (message.recoveryModel !== "") {
      obj.recoveryModel = message.recoveryModel;
    }
    return obj;
  },

  create(base?: DeepPartial<SqlServerDatabaseDetails>): SqlServerDatabaseDetails {
    return SqlServerDatabaseDetails.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SqlServerDatabaseDetails>): SqlServerDatabaseDetails {
    const message = createBaseSqlServerDatabaseDetails();
    message.compatibilityLevel = object.compatibilityLevel ?? 0;
    message.recoveryModel = object.recoveryModel ?? "";
    return message;
  },
};

function createBaseDatabaseFlags(): DatabaseFlags {
  return { name: "", value: "" };
}

export const DatabaseFlags: MessageFns<DatabaseFlags> = {
  encode(message: DatabaseFlags, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatabaseFlags {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatabaseFlags();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatabaseFlags {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: DatabaseFlags): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<DatabaseFlags>): DatabaseFlags {
    return DatabaseFlags.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatabaseFlags>): DatabaseFlags {
    const message = createBaseDatabaseFlags();
    message.name = object.name ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseSyncFlags(): SyncFlags {
  return { name: "", value: "" };
}

export const SyncFlags: MessageFns<SyncFlags> = {
  encode(message: SyncFlags, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SyncFlags {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSyncFlags();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SyncFlags {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: SyncFlags): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<SyncFlags>): SyncFlags {
    return SyncFlags.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SyncFlags>): SyncFlags {
    const message = createBaseSyncFlags();
    message.name = object.name ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseInstanceReference(): InstanceReference {
  return { name: "", region: "", project: "" };
}

export const InstanceReference: MessageFns<InstanceReference> = {
  encode(message: InstanceReference, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.region !== "") {
      writer.uint32(18).string(message.region);
    }
    if (message.project !== "") {
      writer.uint32(26).string(message.project);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstanceReference {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstanceReference();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.region = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.project = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstanceReference {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      project: isSet(object.project) ? globalThis.String(object.project) : "",
    };
  },

  toJSON(message: InstanceReference): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.project !== "") {
      obj.project = message.project;
    }
    return obj;
  },

  create(base?: DeepPartial<InstanceReference>): InstanceReference {
    return InstanceReference.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstanceReference>): InstanceReference {
    const message = createBaseInstanceReference();
    message.name = object.name ?? "";
    message.region = object.region ?? "";
    message.project = object.project ?? "";
    return message;
  },
};

function createBaseDatabaseInstance(): DatabaseInstance {
  return {
    kind: "",
    state: 0,
    databaseVersion: 0,
    settings: undefined,
    etag: "",
    failoverReplica: undefined,
    masterInstanceName: "",
    replicaNames: [],
    maxDiskSize: undefined,
    currentDiskSize: undefined,
    ipAddresses: [],
    serverCaCert: undefined,
    instanceType: 0,
    project: "",
    ipv6Address: "",
    serviceAccountEmailAddress: "",
    onPremisesConfiguration: undefined,
    replicaConfiguration: undefined,
    backendType: 0,
    selfLink: "",
    suspensionReason: [],
    connectionName: "",
    name: "",
    region: "",
    gceZone: "",
    secondaryGceZone: "",
    diskEncryptionConfiguration: undefined,
    diskEncryptionStatus: undefined,
    rootPassword: "",
    scheduledMaintenance: undefined,
    satisfiesPzs: undefined,
    databaseInstalledVersion: "",
    outOfDiskReport: undefined,
    createTime: undefined,
    availableMaintenanceVersions: [],
    maintenanceVersion: "",
    upgradableDatabaseVersions: [],
    sqlNetworkArchitecture: undefined,
    pscServiceAttachmentLink: undefined,
    dnsName: undefined,
    primaryDnsName: undefined,
    writeEndpoint: undefined,
    replicationCluster: undefined,
    geminiConfig: undefined,
  };
}

export const DatabaseInstance: MessageFns<DatabaseInstance> = {
  encode(message: DatabaseInstance, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.state !== 0) {
      writer.uint32(16).int32(message.state);
    }
    if (message.databaseVersion !== 0) {
      writer.uint32(24).int32(message.databaseVersion);
    }
    if (message.settings !== undefined) {
      Settings.encode(message.settings, writer.uint32(34).fork()).join();
    }
    if (message.etag !== "") {
      writer.uint32(42).string(message.etag);
    }
    if (message.failoverReplica !== undefined) {
      DatabaseInstance_SqlFailoverReplica.encode(message.failoverReplica, writer.uint32(50).fork()).join();
    }
    if (message.masterInstanceName !== "") {
      writer.uint32(58).string(message.masterInstanceName);
    }
    for (const v of message.replicaNames) {
      writer.uint32(66).string(v!);
    }
    if (message.maxDiskSize !== undefined) {
      Int64Value.encode({ value: message.maxDiskSize! }, writer.uint32(74).fork()).join();
    }
    if (message.currentDiskSize !== undefined) {
      Int64Value.encode({ value: message.currentDiskSize! }, writer.uint32(82).fork()).join();
    }
    for (const v of message.ipAddresses) {
      IpMapping.encode(v!, writer.uint32(90).fork()).join();
    }
    if (message.serverCaCert !== undefined) {
      SslCert.encode(message.serverCaCert, writer.uint32(98).fork()).join();
    }
    if (message.instanceType !== 0) {
      writer.uint32(104).int32(message.instanceType);
    }
    if (message.project !== "") {
      writer.uint32(114).string(message.project);
    }
    if (message.ipv6Address !== "") {
      writer.uint32(122).string(message.ipv6Address);
    }
    if (message.serviceAccountEmailAddress !== "") {
      writer.uint32(130).string(message.serviceAccountEmailAddress);
    }
    if (message.onPremisesConfiguration !== undefined) {
      OnPremisesConfiguration.encode(message.onPremisesConfiguration, writer.uint32(138).fork()).join();
    }
    if (message.replicaConfiguration !== undefined) {
      ReplicaConfiguration.encode(message.replicaConfiguration, writer.uint32(146).fork()).join();
    }
    if (message.backendType !== 0) {
      writer.uint32(152).int32(message.backendType);
    }
    if (message.selfLink !== "") {
      writer.uint32(162).string(message.selfLink);
    }
    writer.uint32(170).fork();
    for (const v of message.suspensionReason) {
      writer.int32(v);
    }
    writer.join();
    if (message.connectionName !== "") {
      writer.uint32(178).string(message.connectionName);
    }
    if (message.name !== "") {
      writer.uint32(186).string(message.name);
    }
    if (message.region !== "") {
      writer.uint32(194).string(message.region);
    }
    if (message.gceZone !== "") {
      writer.uint32(202).string(message.gceZone);
    }
    if (message.secondaryGceZone !== "") {
      writer.uint32(274).string(message.secondaryGceZone);
    }
    if (message.diskEncryptionConfiguration !== undefined) {
      DiskEncryptionConfiguration.encode(message.diskEncryptionConfiguration, writer.uint32(210).fork()).join();
    }
    if (message.diskEncryptionStatus !== undefined) {
      DiskEncryptionStatus.encode(message.diskEncryptionStatus, writer.uint32(218).fork()).join();
    }
    if (message.rootPassword !== "") {
      writer.uint32(234).string(message.rootPassword);
    }
    if (message.scheduledMaintenance !== undefined) {
      DatabaseInstance_SqlScheduledMaintenance.encode(message.scheduledMaintenance, writer.uint32(242).fork()).join();
    }
    if (message.satisfiesPzs !== undefined) {
      BoolValue.encode({ value: message.satisfiesPzs! }, writer.uint32(282).fork()).join();
    }
    if (message.databaseInstalledVersion !== "") {
      writer.uint32(322).string(message.databaseInstalledVersion);
    }
    if (message.outOfDiskReport !== undefined) {
      DatabaseInstance_SqlOutOfDiskReport.encode(message.outOfDiskReport, writer.uint32(306).fork()).join();
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(314).fork()).join();
    }
    for (const v of message.availableMaintenanceVersions) {
      writer.uint32(330).string(v!);
    }
    if (message.maintenanceVersion !== "") {
      writer.uint32(338).string(message.maintenanceVersion);
    }
    for (const v of message.upgradableDatabaseVersions) {
      AvailableDatabaseVersion.encode(v!, writer.uint32(362).fork()).join();
    }
    if (message.sqlNetworkArchitecture !== undefined) {
      writer.uint32(376).int32(message.sqlNetworkArchitecture);
    }
    if (message.pscServiceAttachmentLink !== undefined) {
      writer.uint32(386).string(message.pscServiceAttachmentLink);
    }
    if (message.dnsName !== undefined) {
      writer.uint32(394).string(message.dnsName);
    }
    if (message.primaryDnsName !== undefined) {
      writer.uint32(410).string(message.primaryDnsName);
    }
    if (message.writeEndpoint !== undefined) {
      writer.uint32(418).string(message.writeEndpoint);
    }
    if (message.replicationCluster !== undefined) {
      ReplicationCluster.encode(message.replicationCluster, writer.uint32(434).fork()).join();
    }
    if (message.geminiConfig !== undefined) {
      GeminiInstanceConfig.encode(message.geminiConfig, writer.uint32(442).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatabaseInstance {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatabaseInstance();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.databaseVersion = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.settings = Settings.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.etag = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.failoverReplica = DatabaseInstance_SqlFailoverReplica.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.masterInstanceName = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.replicaNames.push(reader.string());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.maxDiskSize = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.currentDiskSize = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.ipAddresses.push(IpMapping.decode(reader, reader.uint32()));
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.serverCaCert = SslCert.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 104) {
            break;
          }

          message.instanceType = reader.int32() as any;
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.project = reader.string();
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.ipv6Address = reader.string();
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.serviceAccountEmailAddress = reader.string();
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.onPremisesConfiguration = OnPremisesConfiguration.decode(reader, reader.uint32());
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.replicaConfiguration = ReplicaConfiguration.decode(reader, reader.uint32());
          continue;
        case 19:
          if (tag !== 152) {
            break;
          }

          message.backendType = reader.int32() as any;
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.selfLink = reader.string();
          continue;
        case 21:
          if (tag === 168) {
            message.suspensionReason.push(reader.int32() as any);

            continue;
          }

          if (tag === 170) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.suspensionReason.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 22:
          if (tag !== 178) {
            break;
          }

          message.connectionName = reader.string();
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.name = reader.string();
          continue;
        case 24:
          if (tag !== 194) {
            break;
          }

          message.region = reader.string();
          continue;
        case 25:
          if (tag !== 202) {
            break;
          }

          message.gceZone = reader.string();
          continue;
        case 34:
          if (tag !== 274) {
            break;
          }

          message.secondaryGceZone = reader.string();
          continue;
        case 26:
          if (tag !== 210) {
            break;
          }

          message.diskEncryptionConfiguration = DiskEncryptionConfiguration.decode(reader, reader.uint32());
          continue;
        case 27:
          if (tag !== 218) {
            break;
          }

          message.diskEncryptionStatus = DiskEncryptionStatus.decode(reader, reader.uint32());
          continue;
        case 29:
          if (tag !== 234) {
            break;
          }

          message.rootPassword = reader.string();
          continue;
        case 30:
          if (tag !== 242) {
            break;
          }

          message.scheduledMaintenance = DatabaseInstance_SqlScheduledMaintenance.decode(reader, reader.uint32());
          continue;
        case 35:
          if (tag !== 282) {
            break;
          }

          message.satisfiesPzs = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 40:
          if (tag !== 322) {
            break;
          }

          message.databaseInstalledVersion = reader.string();
          continue;
        case 38:
          if (tag !== 306) {
            break;
          }

          message.outOfDiskReport = DatabaseInstance_SqlOutOfDiskReport.decode(reader, reader.uint32());
          continue;
        case 39:
          if (tag !== 314) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 41:
          if (tag !== 330) {
            break;
          }

          message.availableMaintenanceVersions.push(reader.string());
          continue;
        case 42:
          if (tag !== 338) {
            break;
          }

          message.maintenanceVersion = reader.string();
          continue;
        case 45:
          if (tag !== 362) {
            break;
          }

          message.upgradableDatabaseVersions.push(AvailableDatabaseVersion.decode(reader, reader.uint32()));
          continue;
        case 47:
          if (tag !== 376) {
            break;
          }

          message.sqlNetworkArchitecture = reader.int32() as any;
          continue;
        case 48:
          if (tag !== 386) {
            break;
          }

          message.pscServiceAttachmentLink = reader.string();
          continue;
        case 49:
          if (tag !== 394) {
            break;
          }

          message.dnsName = reader.string();
          continue;
        case 51:
          if (tag !== 410) {
            break;
          }

          message.primaryDnsName = reader.string();
          continue;
        case 52:
          if (tag !== 418) {
            break;
          }

          message.writeEndpoint = reader.string();
          continue;
        case 54:
          if (tag !== 434) {
            break;
          }

          message.replicationCluster = ReplicationCluster.decode(reader, reader.uint32());
          continue;
        case 55:
          if (tag !== 442) {
            break;
          }

          message.geminiConfig = GeminiInstanceConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatabaseInstance {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      state: isSet(object.state) ? databaseInstance_SqlInstanceStateFromJSON(object.state) : 0,
      databaseVersion: isSet(object.databaseVersion) ? sqlDatabaseVersionFromJSON(object.databaseVersion) : 0,
      settings: isSet(object.settings) ? Settings.fromJSON(object.settings) : undefined,
      etag: isSet(object.etag) ? globalThis.String(object.etag) : "",
      failoverReplica: isSet(object.failoverReplica)
        ? DatabaseInstance_SqlFailoverReplica.fromJSON(object.failoverReplica)
        : undefined,
      masterInstanceName: isSet(object.masterInstanceName) ? globalThis.String(object.masterInstanceName) : "",
      replicaNames: globalThis.Array.isArray(object?.replicaNames)
        ? object.replicaNames.map((e: any) => globalThis.String(e))
        : [],
      maxDiskSize: isSet(object.maxDiskSize) ? Long.fromValue(object.maxDiskSize) : undefined,
      currentDiskSize: isSet(object.currentDiskSize) ? Long.fromValue(object.currentDiskSize) : undefined,
      ipAddresses: globalThis.Array.isArray(object?.ipAddresses)
        ? object.ipAddresses.map((e: any) => IpMapping.fromJSON(e))
        : [],
      serverCaCert: isSet(object.serverCaCert) ? SslCert.fromJSON(object.serverCaCert) : undefined,
      instanceType: isSet(object.instanceType) ? sqlInstanceTypeFromJSON(object.instanceType) : 0,
      project: isSet(object.project) ? globalThis.String(object.project) : "",
      ipv6Address: isSet(object.ipv6Address) ? globalThis.String(object.ipv6Address) : "",
      serviceAccountEmailAddress: isSet(object.serviceAccountEmailAddress)
        ? globalThis.String(object.serviceAccountEmailAddress)
        : "",
      onPremisesConfiguration: isSet(object.onPremisesConfiguration)
        ? OnPremisesConfiguration.fromJSON(object.onPremisesConfiguration)
        : undefined,
      replicaConfiguration: isSet(object.replicaConfiguration)
        ? ReplicaConfiguration.fromJSON(object.replicaConfiguration)
        : undefined,
      backendType: isSet(object.backendType) ? sqlBackendTypeFromJSON(object.backendType) : 0,
      selfLink: isSet(object.selfLink) ? globalThis.String(object.selfLink) : "",
      suspensionReason: globalThis.Array.isArray(object?.suspensionReason)
        ? object.suspensionReason.map((e: any) => sqlSuspensionReasonFromJSON(e))
        : [],
      connectionName: isSet(object.connectionName) ? globalThis.String(object.connectionName) : "",
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      gceZone: isSet(object.gceZone) ? globalThis.String(object.gceZone) : "",
      secondaryGceZone: isSet(object.secondaryGceZone) ? globalThis.String(object.secondaryGceZone) : "",
      diskEncryptionConfiguration: isSet(object.diskEncryptionConfiguration)
        ? DiskEncryptionConfiguration.fromJSON(object.diskEncryptionConfiguration)
        : undefined,
      diskEncryptionStatus: isSet(object.diskEncryptionStatus)
        ? DiskEncryptionStatus.fromJSON(object.diskEncryptionStatus)
        : undefined,
      rootPassword: isSet(object.rootPassword) ? globalThis.String(object.rootPassword) : "",
      scheduledMaintenance: isSet(object.scheduledMaintenance)
        ? DatabaseInstance_SqlScheduledMaintenance.fromJSON(object.scheduledMaintenance)
        : undefined,
      satisfiesPzs: isSet(object.satisfiesPzs) ? Boolean(object.satisfiesPzs) : undefined,
      databaseInstalledVersion: isSet(object.databaseInstalledVersion)
        ? globalThis.String(object.databaseInstalledVersion)
        : "",
      outOfDiskReport: isSet(object.outOfDiskReport)
        ? DatabaseInstance_SqlOutOfDiskReport.fromJSON(object.outOfDiskReport)
        : undefined,
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      availableMaintenanceVersions: globalThis.Array.isArray(object?.availableMaintenanceVersions)
        ? object.availableMaintenanceVersions.map((e: any) => globalThis.String(e))
        : [],
      maintenanceVersion: isSet(object.maintenanceVersion) ? globalThis.String(object.maintenanceVersion) : "",
      upgradableDatabaseVersions: globalThis.Array.isArray(object?.upgradableDatabaseVersions)
        ? object.upgradableDatabaseVersions.map((e: any) => AvailableDatabaseVersion.fromJSON(e))
        : [],
      sqlNetworkArchitecture: isSet(object.sqlNetworkArchitecture)
        ? databaseInstance_SqlNetworkArchitectureFromJSON(object.sqlNetworkArchitecture)
        : undefined,
      pscServiceAttachmentLink: isSet(object.pscServiceAttachmentLink)
        ? globalThis.String(object.pscServiceAttachmentLink)
        : undefined,
      dnsName: isSet(object.dnsName) ? globalThis.String(object.dnsName) : undefined,
      primaryDnsName: isSet(object.primaryDnsName) ? globalThis.String(object.primaryDnsName) : undefined,
      writeEndpoint: isSet(object.writeEndpoint) ? globalThis.String(object.writeEndpoint) : undefined,
      replicationCluster: isSet(object.replicationCluster)
        ? ReplicationCluster.fromJSON(object.replicationCluster)
        : undefined,
      geminiConfig: isSet(object.geminiConfig) ? GeminiInstanceConfig.fromJSON(object.geminiConfig) : undefined,
    };
  },

  toJSON(message: DatabaseInstance): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.state !== 0) {
      obj.state = databaseInstance_SqlInstanceStateToJSON(message.state);
    }
    if (message.databaseVersion !== 0) {
      obj.databaseVersion = sqlDatabaseVersionToJSON(message.databaseVersion);
    }
    if (message.settings !== undefined) {
      obj.settings = Settings.toJSON(message.settings);
    }
    if (message.etag !== "") {
      obj.etag = message.etag;
    }
    if (message.failoverReplica !== undefined) {
      obj.failoverReplica = DatabaseInstance_SqlFailoverReplica.toJSON(message.failoverReplica);
    }
    if (message.masterInstanceName !== "") {
      obj.masterInstanceName = message.masterInstanceName;
    }
    if (message.replicaNames?.length) {
      obj.replicaNames = message.replicaNames;
    }
    if (message.maxDiskSize !== undefined) {
      obj.maxDiskSize = message.maxDiskSize;
    }
    if (message.currentDiskSize !== undefined) {
      obj.currentDiskSize = message.currentDiskSize;
    }
    if (message.ipAddresses?.length) {
      obj.ipAddresses = message.ipAddresses.map((e) => IpMapping.toJSON(e));
    }
    if (message.serverCaCert !== undefined) {
      obj.serverCaCert = SslCert.toJSON(message.serverCaCert);
    }
    if (message.instanceType !== 0) {
      obj.instanceType = sqlInstanceTypeToJSON(message.instanceType);
    }
    if (message.project !== "") {
      obj.project = message.project;
    }
    if (message.ipv6Address !== "") {
      obj.ipv6Address = message.ipv6Address;
    }
    if (message.serviceAccountEmailAddress !== "") {
      obj.serviceAccountEmailAddress = message.serviceAccountEmailAddress;
    }
    if (message.onPremisesConfiguration !== undefined) {
      obj.onPremisesConfiguration = OnPremisesConfiguration.toJSON(message.onPremisesConfiguration);
    }
    if (message.replicaConfiguration !== undefined) {
      obj.replicaConfiguration = ReplicaConfiguration.toJSON(message.replicaConfiguration);
    }
    if (message.backendType !== 0) {
      obj.backendType = sqlBackendTypeToJSON(message.backendType);
    }
    if (message.selfLink !== "") {
      obj.selfLink = message.selfLink;
    }
    if (message.suspensionReason?.length) {
      obj.suspensionReason = message.suspensionReason.map((e) => sqlSuspensionReasonToJSON(e));
    }
    if (message.connectionName !== "") {
      obj.connectionName = message.connectionName;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.gceZone !== "") {
      obj.gceZone = message.gceZone;
    }
    if (message.secondaryGceZone !== "") {
      obj.secondaryGceZone = message.secondaryGceZone;
    }
    if (message.diskEncryptionConfiguration !== undefined) {
      obj.diskEncryptionConfiguration = DiskEncryptionConfiguration.toJSON(message.diskEncryptionConfiguration);
    }
    if (message.diskEncryptionStatus !== undefined) {
      obj.diskEncryptionStatus = DiskEncryptionStatus.toJSON(message.diskEncryptionStatus);
    }
    if (message.rootPassword !== "") {
      obj.rootPassword = message.rootPassword;
    }
    if (message.scheduledMaintenance !== undefined) {
      obj.scheduledMaintenance = DatabaseInstance_SqlScheduledMaintenance.toJSON(message.scheduledMaintenance);
    }
    if (message.satisfiesPzs !== undefined) {
      obj.satisfiesPzs = message.satisfiesPzs;
    }
    if (message.databaseInstalledVersion !== "") {
      obj.databaseInstalledVersion = message.databaseInstalledVersion;
    }
    if (message.outOfDiskReport !== undefined) {
      obj.outOfDiskReport = DatabaseInstance_SqlOutOfDiskReport.toJSON(message.outOfDiskReport);
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.availableMaintenanceVersions?.length) {
      obj.availableMaintenanceVersions = message.availableMaintenanceVersions;
    }
    if (message.maintenanceVersion !== "") {
      obj.maintenanceVersion = message.maintenanceVersion;
    }
    if (message.upgradableDatabaseVersions?.length) {
      obj.upgradableDatabaseVersions = message.upgradableDatabaseVersions.map((e) =>
        AvailableDatabaseVersion.toJSON(e)
      );
    }
    if (message.sqlNetworkArchitecture !== undefined) {
      obj.sqlNetworkArchitecture = databaseInstance_SqlNetworkArchitectureToJSON(message.sqlNetworkArchitecture);
    }
    if (message.pscServiceAttachmentLink !== undefined) {
      obj.pscServiceAttachmentLink = message.pscServiceAttachmentLink;
    }
    if (message.dnsName !== undefined) {
      obj.dnsName = message.dnsName;
    }
    if (message.primaryDnsName !== undefined) {
      obj.primaryDnsName = message.primaryDnsName;
    }
    if (message.writeEndpoint !== undefined) {
      obj.writeEndpoint = message.writeEndpoint;
    }
    if (message.replicationCluster !== undefined) {
      obj.replicationCluster = ReplicationCluster.toJSON(message.replicationCluster);
    }
    if (message.geminiConfig !== undefined) {
      obj.geminiConfig = GeminiInstanceConfig.toJSON(message.geminiConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<DatabaseInstance>): DatabaseInstance {
    return DatabaseInstance.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatabaseInstance>): DatabaseInstance {
    const message = createBaseDatabaseInstance();
    message.kind = object.kind ?? "";
    message.state = object.state ?? 0;
    message.databaseVersion = object.databaseVersion ?? 0;
    message.settings = (object.settings !== undefined && object.settings !== null)
      ? Settings.fromPartial(object.settings)
      : undefined;
    message.etag = object.etag ?? "";
    message.failoverReplica = (object.failoverReplica !== undefined && object.failoverReplica !== null)
      ? DatabaseInstance_SqlFailoverReplica.fromPartial(object.failoverReplica)
      : undefined;
    message.masterInstanceName = object.masterInstanceName ?? "";
    message.replicaNames = object.replicaNames?.map((e) => e) || [];
    message.maxDiskSize = (object.maxDiskSize !== undefined && object.maxDiskSize !== null)
      ? Long.fromValue(object.maxDiskSize)
      : undefined;
    message.currentDiskSize = (object.currentDiskSize !== undefined && object.currentDiskSize !== null)
      ? Long.fromValue(object.currentDiskSize)
      : undefined;
    message.ipAddresses = object.ipAddresses?.map((e) => IpMapping.fromPartial(e)) || [];
    message.serverCaCert = (object.serverCaCert !== undefined && object.serverCaCert !== null)
      ? SslCert.fromPartial(object.serverCaCert)
      : undefined;
    message.instanceType = object.instanceType ?? 0;
    message.project = object.project ?? "";
    message.ipv6Address = object.ipv6Address ?? "";
    message.serviceAccountEmailAddress = object.serviceAccountEmailAddress ?? "";
    message.onPremisesConfiguration =
      (object.onPremisesConfiguration !== undefined && object.onPremisesConfiguration !== null)
        ? OnPremisesConfiguration.fromPartial(object.onPremisesConfiguration)
        : undefined;
    message.replicaConfiguration = (object.replicaConfiguration !== undefined && object.replicaConfiguration !== null)
      ? ReplicaConfiguration.fromPartial(object.replicaConfiguration)
      : undefined;
    message.backendType = object.backendType ?? 0;
    message.selfLink = object.selfLink ?? "";
    message.suspensionReason = object.suspensionReason?.map((e) => e) || [];
    message.connectionName = object.connectionName ?? "";
    message.name = object.name ?? "";
    message.region = object.region ?? "";
    message.gceZone = object.gceZone ?? "";
    message.secondaryGceZone = object.secondaryGceZone ?? "";
    message.diskEncryptionConfiguration =
      (object.diskEncryptionConfiguration !== undefined && object.diskEncryptionConfiguration !== null)
        ? DiskEncryptionConfiguration.fromPartial(object.diskEncryptionConfiguration)
        : undefined;
    message.diskEncryptionStatus = (object.diskEncryptionStatus !== undefined && object.diskEncryptionStatus !== null)
      ? DiskEncryptionStatus.fromPartial(object.diskEncryptionStatus)
      : undefined;
    message.rootPassword = object.rootPassword ?? "";
    message.scheduledMaintenance = (object.scheduledMaintenance !== undefined && object.scheduledMaintenance !== null)
      ? DatabaseInstance_SqlScheduledMaintenance.fromPartial(object.scheduledMaintenance)
      : undefined;
    message.satisfiesPzs = object.satisfiesPzs ?? undefined;
    message.databaseInstalledVersion = object.databaseInstalledVersion ?? "";
    message.outOfDiskReport = (object.outOfDiskReport !== undefined && object.outOfDiskReport !== null)
      ? DatabaseInstance_SqlOutOfDiskReport.fromPartial(object.outOfDiskReport)
      : undefined;
    message.createTime = object.createTime ?? undefined;
    message.availableMaintenanceVersions = object.availableMaintenanceVersions?.map((e) => e) || [];
    message.maintenanceVersion = object.maintenanceVersion ?? "";
    message.upgradableDatabaseVersions =
      object.upgradableDatabaseVersions?.map((e) => AvailableDatabaseVersion.fromPartial(e)) || [];
    message.sqlNetworkArchitecture = object.sqlNetworkArchitecture ?? undefined;
    message.pscServiceAttachmentLink = object.pscServiceAttachmentLink ?? undefined;
    message.dnsName = object.dnsName ?? undefined;
    message.primaryDnsName = object.primaryDnsName ?? undefined;
    message.writeEndpoint = object.writeEndpoint ?? undefined;
    message.replicationCluster = (object.replicationCluster !== undefined && object.replicationCluster !== null)
      ? ReplicationCluster.fromPartial(object.replicationCluster)
      : undefined;
    message.geminiConfig = (object.geminiConfig !== undefined && object.geminiConfig !== null)
      ? GeminiInstanceConfig.fromPartial(object.geminiConfig)
      : undefined;
    return message;
  },
};

function createBaseDatabaseInstance_SqlFailoverReplica(): DatabaseInstance_SqlFailoverReplica {
  return { name: "", available: undefined };
}

export const DatabaseInstance_SqlFailoverReplica: MessageFns<DatabaseInstance_SqlFailoverReplica> = {
  encode(message: DatabaseInstance_SqlFailoverReplica, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.available !== undefined) {
      BoolValue.encode({ value: message.available! }, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatabaseInstance_SqlFailoverReplica {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatabaseInstance_SqlFailoverReplica();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.available = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatabaseInstance_SqlFailoverReplica {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      available: isSet(object.available) ? Boolean(object.available) : undefined,
    };
  },

  toJSON(message: DatabaseInstance_SqlFailoverReplica): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.available !== undefined) {
      obj.available = message.available;
    }
    return obj;
  },

  create(base?: DeepPartial<DatabaseInstance_SqlFailoverReplica>): DatabaseInstance_SqlFailoverReplica {
    return DatabaseInstance_SqlFailoverReplica.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatabaseInstance_SqlFailoverReplica>): DatabaseInstance_SqlFailoverReplica {
    const message = createBaseDatabaseInstance_SqlFailoverReplica();
    message.name = object.name ?? "";
    message.available = object.available ?? undefined;
    return message;
  },
};

function createBaseDatabaseInstance_SqlScheduledMaintenance(): DatabaseInstance_SqlScheduledMaintenance {
  return { startTime: undefined, canDefer: false, canReschedule: false, scheduleDeadlineTime: undefined };
}

export const DatabaseInstance_SqlScheduledMaintenance: MessageFns<DatabaseInstance_SqlScheduledMaintenance> = {
  encode(message: DatabaseInstance_SqlScheduledMaintenance, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(10).fork()).join();
    }
    if (message.canDefer !== false) {
      writer.uint32(16).bool(message.canDefer);
    }
    if (message.canReschedule !== false) {
      writer.uint32(24).bool(message.canReschedule);
    }
    if (message.scheduleDeadlineTime !== undefined) {
      Timestamp.encode(toTimestamp(message.scheduleDeadlineTime), writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatabaseInstance_SqlScheduledMaintenance {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatabaseInstance_SqlScheduledMaintenance();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.canDefer = reader.bool();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.canReschedule = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.scheduleDeadlineTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatabaseInstance_SqlScheduledMaintenance {
    return {
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      canDefer: isSet(object.canDefer) ? globalThis.Boolean(object.canDefer) : false,
      canReschedule: isSet(object.canReschedule) ? globalThis.Boolean(object.canReschedule) : false,
      scheduleDeadlineTime: isSet(object.scheduleDeadlineTime)
        ? fromJsonTimestamp(object.scheduleDeadlineTime)
        : undefined,
    };
  },

  toJSON(message: DatabaseInstance_SqlScheduledMaintenance): unknown {
    const obj: any = {};
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.canDefer !== false) {
      obj.canDefer = message.canDefer;
    }
    if (message.canReschedule !== false) {
      obj.canReschedule = message.canReschedule;
    }
    if (message.scheduleDeadlineTime !== undefined) {
      obj.scheduleDeadlineTime = message.scheduleDeadlineTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<DatabaseInstance_SqlScheduledMaintenance>): DatabaseInstance_SqlScheduledMaintenance {
    return DatabaseInstance_SqlScheduledMaintenance.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatabaseInstance_SqlScheduledMaintenance>): DatabaseInstance_SqlScheduledMaintenance {
    const message = createBaseDatabaseInstance_SqlScheduledMaintenance();
    message.startTime = object.startTime ?? undefined;
    message.canDefer = object.canDefer ?? false;
    message.canReschedule = object.canReschedule ?? false;
    message.scheduleDeadlineTime = object.scheduleDeadlineTime ?? undefined;
    return message;
  },
};

function createBaseDatabaseInstance_SqlOutOfDiskReport(): DatabaseInstance_SqlOutOfDiskReport {
  return { sqlOutOfDiskState: undefined, sqlMinRecommendedIncreaseSizeGb: undefined };
}

export const DatabaseInstance_SqlOutOfDiskReport: MessageFns<DatabaseInstance_SqlOutOfDiskReport> = {
  encode(message: DatabaseInstance_SqlOutOfDiskReport, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.sqlOutOfDiskState !== undefined) {
      writer.uint32(8).int32(message.sqlOutOfDiskState);
    }
    if (message.sqlMinRecommendedIncreaseSizeGb !== undefined) {
      writer.uint32(16).int32(message.sqlMinRecommendedIncreaseSizeGb);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatabaseInstance_SqlOutOfDiskReport {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatabaseInstance_SqlOutOfDiskReport();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.sqlOutOfDiskState = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.sqlMinRecommendedIncreaseSizeGb = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatabaseInstance_SqlOutOfDiskReport {
    return {
      sqlOutOfDiskState: isSet(object.sqlOutOfDiskState)
        ? databaseInstance_SqlOutOfDiskReport_SqlOutOfDiskStateFromJSON(object.sqlOutOfDiskState)
        : undefined,
      sqlMinRecommendedIncreaseSizeGb: isSet(object.sqlMinRecommendedIncreaseSizeGb)
        ? globalThis.Number(object.sqlMinRecommendedIncreaseSizeGb)
        : undefined,
    };
  },

  toJSON(message: DatabaseInstance_SqlOutOfDiskReport): unknown {
    const obj: any = {};
    if (message.sqlOutOfDiskState !== undefined) {
      obj.sqlOutOfDiskState = databaseInstance_SqlOutOfDiskReport_SqlOutOfDiskStateToJSON(message.sqlOutOfDiskState);
    }
    if (message.sqlMinRecommendedIncreaseSizeGb !== undefined) {
      obj.sqlMinRecommendedIncreaseSizeGb = Math.round(message.sqlMinRecommendedIncreaseSizeGb);
    }
    return obj;
  },

  create(base?: DeepPartial<DatabaseInstance_SqlOutOfDiskReport>): DatabaseInstance_SqlOutOfDiskReport {
    return DatabaseInstance_SqlOutOfDiskReport.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatabaseInstance_SqlOutOfDiskReport>): DatabaseInstance_SqlOutOfDiskReport {
    const message = createBaseDatabaseInstance_SqlOutOfDiskReport();
    message.sqlOutOfDiskState = object.sqlOutOfDiskState ?? undefined;
    message.sqlMinRecommendedIncreaseSizeGb = object.sqlMinRecommendedIncreaseSizeGb ?? undefined;
    return message;
  },
};

function createBaseGeminiInstanceConfig(): GeminiInstanceConfig {
  return {
    entitled: undefined,
    googleVacuumMgmtEnabled: undefined,
    oomSessionCancelEnabled: undefined,
    activeQueryEnabled: undefined,
    indexAdvisorEnabled: undefined,
    flagRecommenderEnabled: undefined,
  };
}

export const GeminiInstanceConfig: MessageFns<GeminiInstanceConfig> = {
  encode(message: GeminiInstanceConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.entitled !== undefined) {
      writer.uint32(8).bool(message.entitled);
    }
    if (message.googleVacuumMgmtEnabled !== undefined) {
      writer.uint32(16).bool(message.googleVacuumMgmtEnabled);
    }
    if (message.oomSessionCancelEnabled !== undefined) {
      writer.uint32(24).bool(message.oomSessionCancelEnabled);
    }
    if (message.activeQueryEnabled !== undefined) {
      writer.uint32(32).bool(message.activeQueryEnabled);
    }
    if (message.indexAdvisorEnabled !== undefined) {
      writer.uint32(40).bool(message.indexAdvisorEnabled);
    }
    if (message.flagRecommenderEnabled !== undefined) {
      writer.uint32(48).bool(message.flagRecommenderEnabled);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GeminiInstanceConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGeminiInstanceConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.entitled = reader.bool();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.googleVacuumMgmtEnabled = reader.bool();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.oomSessionCancelEnabled = reader.bool();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.activeQueryEnabled = reader.bool();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.indexAdvisorEnabled = reader.bool();
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.flagRecommenderEnabled = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GeminiInstanceConfig {
    return {
      entitled: isSet(object.entitled) ? globalThis.Boolean(object.entitled) : undefined,
      googleVacuumMgmtEnabled: isSet(object.googleVacuumMgmtEnabled)
        ? globalThis.Boolean(object.googleVacuumMgmtEnabled)
        : undefined,
      oomSessionCancelEnabled: isSet(object.oomSessionCancelEnabled)
        ? globalThis.Boolean(object.oomSessionCancelEnabled)
        : undefined,
      activeQueryEnabled: isSet(object.activeQueryEnabled) ? globalThis.Boolean(object.activeQueryEnabled) : undefined,
      indexAdvisorEnabled: isSet(object.indexAdvisorEnabled)
        ? globalThis.Boolean(object.indexAdvisorEnabled)
        : undefined,
      flagRecommenderEnabled: isSet(object.flagRecommenderEnabled)
        ? globalThis.Boolean(object.flagRecommenderEnabled)
        : undefined,
    };
  },

  toJSON(message: GeminiInstanceConfig): unknown {
    const obj: any = {};
    if (message.entitled !== undefined) {
      obj.entitled = message.entitled;
    }
    if (message.googleVacuumMgmtEnabled !== undefined) {
      obj.googleVacuumMgmtEnabled = message.googleVacuumMgmtEnabled;
    }
    if (message.oomSessionCancelEnabled !== undefined) {
      obj.oomSessionCancelEnabled = message.oomSessionCancelEnabled;
    }
    if (message.activeQueryEnabled !== undefined) {
      obj.activeQueryEnabled = message.activeQueryEnabled;
    }
    if (message.indexAdvisorEnabled !== undefined) {
      obj.indexAdvisorEnabled = message.indexAdvisorEnabled;
    }
    if (message.flagRecommenderEnabled !== undefined) {
      obj.flagRecommenderEnabled = message.flagRecommenderEnabled;
    }
    return obj;
  },

  create(base?: DeepPartial<GeminiInstanceConfig>): GeminiInstanceConfig {
    return GeminiInstanceConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GeminiInstanceConfig>): GeminiInstanceConfig {
    const message = createBaseGeminiInstanceConfig();
    message.entitled = object.entitled ?? undefined;
    message.googleVacuumMgmtEnabled = object.googleVacuumMgmtEnabled ?? undefined;
    message.oomSessionCancelEnabled = object.oomSessionCancelEnabled ?? undefined;
    message.activeQueryEnabled = object.activeQueryEnabled ?? undefined;
    message.indexAdvisorEnabled = object.indexAdvisorEnabled ?? undefined;
    message.flagRecommenderEnabled = object.flagRecommenderEnabled ?? undefined;
    return message;
  },
};

function createBaseReplicationCluster(): ReplicationCluster {
  return { psaWriteEndpoint: undefined, failoverDrReplicaName: undefined, drReplica: undefined };
}

export const ReplicationCluster: MessageFns<ReplicationCluster> = {
  encode(message: ReplicationCluster, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.psaWriteEndpoint !== undefined) {
      writer.uint32(10).string(message.psaWriteEndpoint);
    }
    if (message.failoverDrReplicaName !== undefined) {
      writer.uint32(18).string(message.failoverDrReplicaName);
    }
    if (message.drReplica !== undefined) {
      writer.uint32(32).bool(message.drReplica);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReplicationCluster {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReplicationCluster();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.psaWriteEndpoint = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.failoverDrReplicaName = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.drReplica = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReplicationCluster {
    return {
      psaWriteEndpoint: isSet(object.psaWriteEndpoint) ? globalThis.String(object.psaWriteEndpoint) : undefined,
      failoverDrReplicaName: isSet(object.failoverDrReplicaName)
        ? globalThis.String(object.failoverDrReplicaName)
        : undefined,
      drReplica: isSet(object.drReplica) ? globalThis.Boolean(object.drReplica) : undefined,
    };
  },

  toJSON(message: ReplicationCluster): unknown {
    const obj: any = {};
    if (message.psaWriteEndpoint !== undefined) {
      obj.psaWriteEndpoint = message.psaWriteEndpoint;
    }
    if (message.failoverDrReplicaName !== undefined) {
      obj.failoverDrReplicaName = message.failoverDrReplicaName;
    }
    if (message.drReplica !== undefined) {
      obj.drReplica = message.drReplica;
    }
    return obj;
  },

  create(base?: DeepPartial<ReplicationCluster>): ReplicationCluster {
    return ReplicationCluster.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReplicationCluster>): ReplicationCluster {
    const message = createBaseReplicationCluster();
    message.psaWriteEndpoint = object.psaWriteEndpoint ?? undefined;
    message.failoverDrReplicaName = object.failoverDrReplicaName ?? undefined;
    message.drReplica = object.drReplica ?? undefined;
    return message;
  },
};

function createBaseAvailableDatabaseVersion(): AvailableDatabaseVersion {
  return { majorVersion: undefined, name: undefined, displayName: undefined };
}

export const AvailableDatabaseVersion: MessageFns<AvailableDatabaseVersion> = {
  encode(message: AvailableDatabaseVersion, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.majorVersion !== undefined) {
      writer.uint32(26).string(message.majorVersion);
    }
    if (message.name !== undefined) {
      writer.uint32(66).string(message.name);
    }
    if (message.displayName !== undefined) {
      writer.uint32(74).string(message.displayName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AvailableDatabaseVersion {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAvailableDatabaseVersion();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 26) {
            break;
          }

          message.majorVersion = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.name = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.displayName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AvailableDatabaseVersion {
    return {
      majorVersion: isSet(object.majorVersion) ? globalThis.String(object.majorVersion) : undefined,
      name: isSet(object.name) ? globalThis.String(object.name) : undefined,
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : undefined,
    };
  },

  toJSON(message: AvailableDatabaseVersion): unknown {
    const obj: any = {};
    if (message.majorVersion !== undefined) {
      obj.majorVersion = message.majorVersion;
    }
    if (message.name !== undefined) {
      obj.name = message.name;
    }
    if (message.displayName !== undefined) {
      obj.displayName = message.displayName;
    }
    return obj;
  },

  create(base?: DeepPartial<AvailableDatabaseVersion>): AvailableDatabaseVersion {
    return AvailableDatabaseVersion.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AvailableDatabaseVersion>): AvailableDatabaseVersion {
    const message = createBaseAvailableDatabaseVersion();
    message.majorVersion = object.majorVersion ?? undefined;
    message.name = object.name ?? undefined;
    message.displayName = object.displayName ?? undefined;
    return message;
  },
};

function createBaseDatabasesListResponse(): DatabasesListResponse {
  return { kind: "", items: [] };
}

export const DatabasesListResponse: MessageFns<DatabasesListResponse> = {
  encode(message: DatabasesListResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    for (const v of message.items) {
      Database.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatabasesListResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatabasesListResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.items.push(Database.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatabasesListResponse {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      items: globalThis.Array.isArray(object?.items) ? object.items.map((e: any) => Database.fromJSON(e)) : [],
    };
  },

  toJSON(message: DatabasesListResponse): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.items?.length) {
      obj.items = message.items.map((e) => Database.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<DatabasesListResponse>): DatabasesListResponse {
    return DatabasesListResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatabasesListResponse>): DatabasesListResponse {
    const message = createBaseDatabasesListResponse();
    message.kind = object.kind ?? "";
    message.items = object.items?.map((e) => Database.fromPartial(e)) || [];
    return message;
  },
};

function createBaseDemoteMasterConfiguration(): DemoteMasterConfiguration {
  return { kind: "", mysqlReplicaConfiguration: undefined };
}

export const DemoteMasterConfiguration: MessageFns<DemoteMasterConfiguration> = {
  encode(message: DemoteMasterConfiguration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.mysqlReplicaConfiguration !== undefined) {
      DemoteMasterMySqlReplicaConfiguration.encode(message.mysqlReplicaConfiguration, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DemoteMasterConfiguration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDemoteMasterConfiguration();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.mysqlReplicaConfiguration = DemoteMasterMySqlReplicaConfiguration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DemoteMasterConfiguration {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      mysqlReplicaConfiguration: isSet(object.mysqlReplicaConfiguration)
        ? DemoteMasterMySqlReplicaConfiguration.fromJSON(object.mysqlReplicaConfiguration)
        : undefined,
    };
  },

  toJSON(message: DemoteMasterConfiguration): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.mysqlReplicaConfiguration !== undefined) {
      obj.mysqlReplicaConfiguration = DemoteMasterMySqlReplicaConfiguration.toJSON(message.mysqlReplicaConfiguration);
    }
    return obj;
  },

  create(base?: DeepPartial<DemoteMasterConfiguration>): DemoteMasterConfiguration {
    return DemoteMasterConfiguration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DemoteMasterConfiguration>): DemoteMasterConfiguration {
    const message = createBaseDemoteMasterConfiguration();
    message.kind = object.kind ?? "";
    message.mysqlReplicaConfiguration =
      (object.mysqlReplicaConfiguration !== undefined && object.mysqlReplicaConfiguration !== null)
        ? DemoteMasterMySqlReplicaConfiguration.fromPartial(object.mysqlReplicaConfiguration)
        : undefined;
    return message;
  },
};

function createBaseDemoteMasterContext(): DemoteMasterContext {
  return {
    kind: "",
    verifyGtidConsistency: undefined,
    masterInstanceName: "",
    replicaConfiguration: undefined,
    skipReplicationSetup: false,
  };
}

export const DemoteMasterContext: MessageFns<DemoteMasterContext> = {
  encode(message: DemoteMasterContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.verifyGtidConsistency !== undefined) {
      BoolValue.encode({ value: message.verifyGtidConsistency! }, writer.uint32(18).fork()).join();
    }
    if (message.masterInstanceName !== "") {
      writer.uint32(26).string(message.masterInstanceName);
    }
    if (message.replicaConfiguration !== undefined) {
      DemoteMasterConfiguration.encode(message.replicaConfiguration, writer.uint32(34).fork()).join();
    }
    if (message.skipReplicationSetup !== false) {
      writer.uint32(40).bool(message.skipReplicationSetup);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DemoteMasterContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDemoteMasterContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.verifyGtidConsistency = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.masterInstanceName = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.replicaConfiguration = DemoteMasterConfiguration.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.skipReplicationSetup = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DemoteMasterContext {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      verifyGtidConsistency: isSet(object.verifyGtidConsistency) ? Boolean(object.verifyGtidConsistency) : undefined,
      masterInstanceName: isSet(object.masterInstanceName) ? globalThis.String(object.masterInstanceName) : "",
      replicaConfiguration: isSet(object.replicaConfiguration)
        ? DemoteMasterConfiguration.fromJSON(object.replicaConfiguration)
        : undefined,
      skipReplicationSetup: isSet(object.skipReplicationSetup)
        ? globalThis.Boolean(object.skipReplicationSetup)
        : false,
    };
  },

  toJSON(message: DemoteMasterContext): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.verifyGtidConsistency !== undefined) {
      obj.verifyGtidConsistency = message.verifyGtidConsistency;
    }
    if (message.masterInstanceName !== "") {
      obj.masterInstanceName = message.masterInstanceName;
    }
    if (message.replicaConfiguration !== undefined) {
      obj.replicaConfiguration = DemoteMasterConfiguration.toJSON(message.replicaConfiguration);
    }
    if (message.skipReplicationSetup !== false) {
      obj.skipReplicationSetup = message.skipReplicationSetup;
    }
    return obj;
  },

  create(base?: DeepPartial<DemoteMasterContext>): DemoteMasterContext {
    return DemoteMasterContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DemoteMasterContext>): DemoteMasterContext {
    const message = createBaseDemoteMasterContext();
    message.kind = object.kind ?? "";
    message.verifyGtidConsistency = object.verifyGtidConsistency ?? undefined;
    message.masterInstanceName = object.masterInstanceName ?? "";
    message.replicaConfiguration = (object.replicaConfiguration !== undefined && object.replicaConfiguration !== null)
      ? DemoteMasterConfiguration.fromPartial(object.replicaConfiguration)
      : undefined;
    message.skipReplicationSetup = object.skipReplicationSetup ?? false;
    return message;
  },
};

function createBaseDemoteMasterMySqlReplicaConfiguration(): DemoteMasterMySqlReplicaConfiguration {
  return { kind: "", username: "", password: "", clientKey: "", clientCertificate: "", caCertificate: "" };
}

export const DemoteMasterMySqlReplicaConfiguration: MessageFns<DemoteMasterMySqlReplicaConfiguration> = {
  encode(message: DemoteMasterMySqlReplicaConfiguration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.username !== "") {
      writer.uint32(18).string(message.username);
    }
    if (message.password !== "") {
      writer.uint32(26).string(message.password);
    }
    if (message.clientKey !== "") {
      writer.uint32(34).string(message.clientKey);
    }
    if (message.clientCertificate !== "") {
      writer.uint32(42).string(message.clientCertificate);
    }
    if (message.caCertificate !== "") {
      writer.uint32(50).string(message.caCertificate);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DemoteMasterMySqlReplicaConfiguration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDemoteMasterMySqlReplicaConfiguration();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.username = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.password = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.clientKey = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.clientCertificate = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.caCertificate = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DemoteMasterMySqlReplicaConfiguration {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      username: isSet(object.username) ? globalThis.String(object.username) : "",
      password: isSet(object.password) ? globalThis.String(object.password) : "",
      clientKey: isSet(object.clientKey) ? globalThis.String(object.clientKey) : "",
      clientCertificate: isSet(object.clientCertificate) ? globalThis.String(object.clientCertificate) : "",
      caCertificate: isSet(object.caCertificate) ? globalThis.String(object.caCertificate) : "",
    };
  },

  toJSON(message: DemoteMasterMySqlReplicaConfiguration): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.username !== "") {
      obj.username = message.username;
    }
    if (message.password !== "") {
      obj.password = message.password;
    }
    if (message.clientKey !== "") {
      obj.clientKey = message.clientKey;
    }
    if (message.clientCertificate !== "") {
      obj.clientCertificate = message.clientCertificate;
    }
    if (message.caCertificate !== "") {
      obj.caCertificate = message.caCertificate;
    }
    return obj;
  },

  create(base?: DeepPartial<DemoteMasterMySqlReplicaConfiguration>): DemoteMasterMySqlReplicaConfiguration {
    return DemoteMasterMySqlReplicaConfiguration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DemoteMasterMySqlReplicaConfiguration>): DemoteMasterMySqlReplicaConfiguration {
    const message = createBaseDemoteMasterMySqlReplicaConfiguration();
    message.kind = object.kind ?? "";
    message.username = object.username ?? "";
    message.password = object.password ?? "";
    message.clientKey = object.clientKey ?? "";
    message.clientCertificate = object.clientCertificate ?? "";
    message.caCertificate = object.caCertificate ?? "";
    return message;
  },
};

function createBaseDemoteContext(): DemoteContext {
  return { kind: "", sourceRepresentativeInstanceName: "" };
}

export const DemoteContext: MessageFns<DemoteContext> = {
  encode(message: DemoteContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.sourceRepresentativeInstanceName !== "") {
      writer.uint32(18).string(message.sourceRepresentativeInstanceName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DemoteContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDemoteContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.sourceRepresentativeInstanceName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DemoteContext {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      sourceRepresentativeInstanceName: isSet(object.sourceRepresentativeInstanceName)
        ? globalThis.String(object.sourceRepresentativeInstanceName)
        : "",
    };
  },

  toJSON(message: DemoteContext): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.sourceRepresentativeInstanceName !== "") {
      obj.sourceRepresentativeInstanceName = message.sourceRepresentativeInstanceName;
    }
    return obj;
  },

  create(base?: DeepPartial<DemoteContext>): DemoteContext {
    return DemoteContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DemoteContext>): DemoteContext {
    const message = createBaseDemoteContext();
    message.kind = object.kind ?? "";
    message.sourceRepresentativeInstanceName = object.sourceRepresentativeInstanceName ?? "";
    return message;
  },
};

function createBaseExportContext(): ExportContext {
  return {
    uri: "",
    databases: [],
    kind: "",
    sqlExportOptions: undefined,
    csvExportOptions: undefined,
    fileType: 0,
    offload: undefined,
    bakExportOptions: undefined,
  };
}

export const ExportContext: MessageFns<ExportContext> = {
  encode(message: ExportContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.uri !== "") {
      writer.uint32(10).string(message.uri);
    }
    for (const v of message.databases) {
      writer.uint32(18).string(v!);
    }
    if (message.kind !== "") {
      writer.uint32(26).string(message.kind);
    }
    if (message.sqlExportOptions !== undefined) {
      ExportContext_SqlExportOptions.encode(message.sqlExportOptions, writer.uint32(34).fork()).join();
    }
    if (message.csvExportOptions !== undefined) {
      ExportContext_SqlCsvExportOptions.encode(message.csvExportOptions, writer.uint32(42).fork()).join();
    }
    if (message.fileType !== 0) {
      writer.uint32(48).int32(message.fileType);
    }
    if (message.offload !== undefined) {
      BoolValue.encode({ value: message.offload! }, writer.uint32(66).fork()).join();
    }
    if (message.bakExportOptions !== undefined) {
      ExportContext_SqlBakExportOptions.encode(message.bakExportOptions, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.uri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.databases.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.sqlExportOptions = ExportContext_SqlExportOptions.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.csvExportOptions = ExportContext_SqlCsvExportOptions.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.fileType = reader.int32() as any;
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.offload = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.bakExportOptions = ExportContext_SqlBakExportOptions.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportContext {
    return {
      uri: isSet(object.uri) ? globalThis.String(object.uri) : "",
      databases: globalThis.Array.isArray(object?.databases)
        ? object.databases.map((e: any) => globalThis.String(e))
        : [],
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      sqlExportOptions: isSet(object.sqlExportOptions)
        ? ExportContext_SqlExportOptions.fromJSON(object.sqlExportOptions)
        : undefined,
      csvExportOptions: isSet(object.csvExportOptions)
        ? ExportContext_SqlCsvExportOptions.fromJSON(object.csvExportOptions)
        : undefined,
      fileType: isSet(object.fileType) ? sqlFileTypeFromJSON(object.fileType) : 0,
      offload: isSet(object.offload) ? Boolean(object.offload) : undefined,
      bakExportOptions: isSet(object.bakExportOptions)
        ? ExportContext_SqlBakExportOptions.fromJSON(object.bakExportOptions)
        : undefined,
    };
  },

  toJSON(message: ExportContext): unknown {
    const obj: any = {};
    if (message.uri !== "") {
      obj.uri = message.uri;
    }
    if (message.databases?.length) {
      obj.databases = message.databases;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.sqlExportOptions !== undefined) {
      obj.sqlExportOptions = ExportContext_SqlExportOptions.toJSON(message.sqlExportOptions);
    }
    if (message.csvExportOptions !== undefined) {
      obj.csvExportOptions = ExportContext_SqlCsvExportOptions.toJSON(message.csvExportOptions);
    }
    if (message.fileType !== 0) {
      obj.fileType = sqlFileTypeToJSON(message.fileType);
    }
    if (message.offload !== undefined) {
      obj.offload = message.offload;
    }
    if (message.bakExportOptions !== undefined) {
      obj.bakExportOptions = ExportContext_SqlBakExportOptions.toJSON(message.bakExportOptions);
    }
    return obj;
  },

  create(base?: DeepPartial<ExportContext>): ExportContext {
    return ExportContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportContext>): ExportContext {
    const message = createBaseExportContext();
    message.uri = object.uri ?? "";
    message.databases = object.databases?.map((e) => e) || [];
    message.kind = object.kind ?? "";
    message.sqlExportOptions = (object.sqlExportOptions !== undefined && object.sqlExportOptions !== null)
      ? ExportContext_SqlExportOptions.fromPartial(object.sqlExportOptions)
      : undefined;
    message.csvExportOptions = (object.csvExportOptions !== undefined && object.csvExportOptions !== null)
      ? ExportContext_SqlCsvExportOptions.fromPartial(object.csvExportOptions)
      : undefined;
    message.fileType = object.fileType ?? 0;
    message.offload = object.offload ?? undefined;
    message.bakExportOptions = (object.bakExportOptions !== undefined && object.bakExportOptions !== null)
      ? ExportContext_SqlBakExportOptions.fromPartial(object.bakExportOptions)
      : undefined;
    return message;
  },
};

function createBaseExportContext_SqlCsvExportOptions(): ExportContext_SqlCsvExportOptions {
  return { selectQuery: "", escapeCharacter: "", quoteCharacter: "", fieldsTerminatedBy: "", linesTerminatedBy: "" };
}

export const ExportContext_SqlCsvExportOptions: MessageFns<ExportContext_SqlCsvExportOptions> = {
  encode(message: ExportContext_SqlCsvExportOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.selectQuery !== "") {
      writer.uint32(10).string(message.selectQuery);
    }
    if (message.escapeCharacter !== "") {
      writer.uint32(18).string(message.escapeCharacter);
    }
    if (message.quoteCharacter !== "") {
      writer.uint32(26).string(message.quoteCharacter);
    }
    if (message.fieldsTerminatedBy !== "") {
      writer.uint32(34).string(message.fieldsTerminatedBy);
    }
    if (message.linesTerminatedBy !== "") {
      writer.uint32(50).string(message.linesTerminatedBy);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportContext_SqlCsvExportOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportContext_SqlCsvExportOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.selectQuery = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.escapeCharacter = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.quoteCharacter = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.fieldsTerminatedBy = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.linesTerminatedBy = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportContext_SqlCsvExportOptions {
    return {
      selectQuery: isSet(object.selectQuery) ? globalThis.String(object.selectQuery) : "",
      escapeCharacter: isSet(object.escapeCharacter) ? globalThis.String(object.escapeCharacter) : "",
      quoteCharacter: isSet(object.quoteCharacter) ? globalThis.String(object.quoteCharacter) : "",
      fieldsTerminatedBy: isSet(object.fieldsTerminatedBy) ? globalThis.String(object.fieldsTerminatedBy) : "",
      linesTerminatedBy: isSet(object.linesTerminatedBy) ? globalThis.String(object.linesTerminatedBy) : "",
    };
  },

  toJSON(message: ExportContext_SqlCsvExportOptions): unknown {
    const obj: any = {};
    if (message.selectQuery !== "") {
      obj.selectQuery = message.selectQuery;
    }
    if (message.escapeCharacter !== "") {
      obj.escapeCharacter = message.escapeCharacter;
    }
    if (message.quoteCharacter !== "") {
      obj.quoteCharacter = message.quoteCharacter;
    }
    if (message.fieldsTerminatedBy !== "") {
      obj.fieldsTerminatedBy = message.fieldsTerminatedBy;
    }
    if (message.linesTerminatedBy !== "") {
      obj.linesTerminatedBy = message.linesTerminatedBy;
    }
    return obj;
  },

  create(base?: DeepPartial<ExportContext_SqlCsvExportOptions>): ExportContext_SqlCsvExportOptions {
    return ExportContext_SqlCsvExportOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportContext_SqlCsvExportOptions>): ExportContext_SqlCsvExportOptions {
    const message = createBaseExportContext_SqlCsvExportOptions();
    message.selectQuery = object.selectQuery ?? "";
    message.escapeCharacter = object.escapeCharacter ?? "";
    message.quoteCharacter = object.quoteCharacter ?? "";
    message.fieldsTerminatedBy = object.fieldsTerminatedBy ?? "";
    message.linesTerminatedBy = object.linesTerminatedBy ?? "";
    return message;
  },
};

function createBaseExportContext_SqlExportOptions(): ExportContext_SqlExportOptions {
  return { tables: [], schemaOnly: undefined, mysqlExportOptions: undefined, threads: undefined, parallel: undefined };
}

export const ExportContext_SqlExportOptions: MessageFns<ExportContext_SqlExportOptions> = {
  encode(message: ExportContext_SqlExportOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.tables) {
      writer.uint32(10).string(v!);
    }
    if (message.schemaOnly !== undefined) {
      BoolValue.encode({ value: message.schemaOnly! }, writer.uint32(18).fork()).join();
    }
    if (message.mysqlExportOptions !== undefined) {
      ExportContext_SqlExportOptions_MysqlExportOptions.encode(message.mysqlExportOptions, writer.uint32(26).fork())
        .join();
    }
    if (message.threads !== undefined) {
      Int32Value.encode({ value: message.threads! }, writer.uint32(34).fork()).join();
    }
    if (message.parallel !== undefined) {
      BoolValue.encode({ value: message.parallel! }, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportContext_SqlExportOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportContext_SqlExportOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.tables.push(reader.string());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.schemaOnly = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.mysqlExportOptions = ExportContext_SqlExportOptions_MysqlExportOptions.decode(
            reader,
            reader.uint32(),
          );
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.threads = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.parallel = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportContext_SqlExportOptions {
    return {
      tables: globalThis.Array.isArray(object?.tables) ? object.tables.map((e: any) => globalThis.String(e)) : [],
      schemaOnly: isSet(object.schemaOnly) ? Boolean(object.schemaOnly) : undefined,
      mysqlExportOptions: isSet(object.mysqlExportOptions)
        ? ExportContext_SqlExportOptions_MysqlExportOptions.fromJSON(object.mysqlExportOptions)
        : undefined,
      threads: isSet(object.threads) ? Number(object.threads) : undefined,
      parallel: isSet(object.parallel) ? Boolean(object.parallel) : undefined,
    };
  },

  toJSON(message: ExportContext_SqlExportOptions): unknown {
    const obj: any = {};
    if (message.tables?.length) {
      obj.tables = message.tables;
    }
    if (message.schemaOnly !== undefined) {
      obj.schemaOnly = message.schemaOnly;
    }
    if (message.mysqlExportOptions !== undefined) {
      obj.mysqlExportOptions = ExportContext_SqlExportOptions_MysqlExportOptions.toJSON(message.mysqlExportOptions);
    }
    if (message.threads !== undefined) {
      obj.threads = message.threads;
    }
    if (message.parallel !== undefined) {
      obj.parallel = message.parallel;
    }
    return obj;
  },

  create(base?: DeepPartial<ExportContext_SqlExportOptions>): ExportContext_SqlExportOptions {
    return ExportContext_SqlExportOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportContext_SqlExportOptions>): ExportContext_SqlExportOptions {
    const message = createBaseExportContext_SqlExportOptions();
    message.tables = object.tables?.map((e) => e) || [];
    message.schemaOnly = object.schemaOnly ?? undefined;
    message.mysqlExportOptions = (object.mysqlExportOptions !== undefined && object.mysqlExportOptions !== null)
      ? ExportContext_SqlExportOptions_MysqlExportOptions.fromPartial(object.mysqlExportOptions)
      : undefined;
    message.threads = object.threads ?? undefined;
    message.parallel = object.parallel ?? undefined;
    return message;
  },
};

function createBaseExportContext_SqlExportOptions_MysqlExportOptions(): ExportContext_SqlExportOptions_MysqlExportOptions {
  return { masterData: undefined };
}

export const ExportContext_SqlExportOptions_MysqlExportOptions: MessageFns<
  ExportContext_SqlExportOptions_MysqlExportOptions
> = {
  encode(
    message: ExportContext_SqlExportOptions_MysqlExportOptions,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.masterData !== undefined) {
      Int32Value.encode({ value: message.masterData! }, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportContext_SqlExportOptions_MysqlExportOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportContext_SqlExportOptions_MysqlExportOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.masterData = Int32Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportContext_SqlExportOptions_MysqlExportOptions {
    return { masterData: isSet(object.masterData) ? Number(object.masterData) : undefined };
  },

  toJSON(message: ExportContext_SqlExportOptions_MysqlExportOptions): unknown {
    const obj: any = {};
    if (message.masterData !== undefined) {
      obj.masterData = message.masterData;
    }
    return obj;
  },

  create(
    base?: DeepPartial<ExportContext_SqlExportOptions_MysqlExportOptions>,
  ): ExportContext_SqlExportOptions_MysqlExportOptions {
    return ExportContext_SqlExportOptions_MysqlExportOptions.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ExportContext_SqlExportOptions_MysqlExportOptions>,
  ): ExportContext_SqlExportOptions_MysqlExportOptions {
    const message = createBaseExportContext_SqlExportOptions_MysqlExportOptions();
    message.masterData = object.masterData ?? undefined;
    return message;
  },
};

function createBaseExportContext_SqlBakExportOptions(): ExportContext_SqlBakExportOptions {
  return { striped: undefined, stripeCount: undefined, bakType: 0, copyOnly: undefined, differentialBase: undefined };
}

export const ExportContext_SqlBakExportOptions: MessageFns<ExportContext_SqlBakExportOptions> = {
  encode(message: ExportContext_SqlBakExportOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.striped !== undefined) {
      BoolValue.encode({ value: message.striped! }, writer.uint32(10).fork()).join();
    }
    if (message.stripeCount !== undefined) {
      Int32Value.encode({ value: message.stripeCount! }, writer.uint32(18).fork()).join();
    }
    if (message.bakType !== 0) {
      writer.uint32(32).int32(message.bakType);
    }
    if (message.copyOnly !== undefined) {
      BoolValue.encode({ value: message.copyOnly! }, writer.uint32(42).fork()).join();
    }
    if (message.differentialBase !== undefined) {
      BoolValue.encode({ value: message.differentialBase! }, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportContext_SqlBakExportOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportContext_SqlBakExportOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.striped = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.stripeCount = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.bakType = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.copyOnly = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.differentialBase = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportContext_SqlBakExportOptions {
    return {
      striped: isSet(object.striped) ? Boolean(object.striped) : undefined,
      stripeCount: isSet(object.stripeCount) ? Number(object.stripeCount) : undefined,
      bakType: isSet(object.bakType) ? bakTypeFromJSON(object.bakType) : 0,
      copyOnly: isSet(object.copyOnly) ? Boolean(object.copyOnly) : undefined,
      differentialBase: isSet(object.differentialBase) ? Boolean(object.differentialBase) : undefined,
    };
  },

  toJSON(message: ExportContext_SqlBakExportOptions): unknown {
    const obj: any = {};
    if (message.striped !== undefined) {
      obj.striped = message.striped;
    }
    if (message.stripeCount !== undefined) {
      obj.stripeCount = message.stripeCount;
    }
    if (message.bakType !== 0) {
      obj.bakType = bakTypeToJSON(message.bakType);
    }
    if (message.copyOnly !== undefined) {
      obj.copyOnly = message.copyOnly;
    }
    if (message.differentialBase !== undefined) {
      obj.differentialBase = message.differentialBase;
    }
    return obj;
  },

  create(base?: DeepPartial<ExportContext_SqlBakExportOptions>): ExportContext_SqlBakExportOptions {
    return ExportContext_SqlBakExportOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportContext_SqlBakExportOptions>): ExportContext_SqlBakExportOptions {
    const message = createBaseExportContext_SqlBakExportOptions();
    message.striped = object.striped ?? undefined;
    message.stripeCount = object.stripeCount ?? undefined;
    message.bakType = object.bakType ?? 0;
    message.copyOnly = object.copyOnly ?? undefined;
    message.differentialBase = object.differentialBase ?? undefined;
    return message;
  },
};

function createBaseFailoverContext(): FailoverContext {
  return { settingsVersion: Long.ZERO, kind: "" };
}

export const FailoverContext: MessageFns<FailoverContext> = {
  encode(message: FailoverContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.settingsVersion.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.settingsVersion.toString());
    }
    if (message.kind !== "") {
      writer.uint32(18).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FailoverContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFailoverContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.settingsVersion = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FailoverContext {
    return {
      settingsVersion: isSet(object.settingsVersion) ? Long.fromValue(object.settingsVersion) : Long.ZERO,
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: FailoverContext): unknown {
    const obj: any = {};
    if (!message.settingsVersion.equals(Long.ZERO)) {
      obj.settingsVersion = (message.settingsVersion || Long.ZERO).toString();
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<FailoverContext>): FailoverContext {
    return FailoverContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FailoverContext>): FailoverContext {
    const message = createBaseFailoverContext();
    message.settingsVersion = (object.settingsVersion !== undefined && object.settingsVersion !== null)
      ? Long.fromValue(object.settingsVersion)
      : Long.ZERO;
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseFlag(): Flag {
  return {
    name: "",
    type: 0,
    appliesTo: [],
    allowedStringValues: [],
    minValue: undefined,
    maxValue: undefined,
    requiresRestart: undefined,
    kind: "",
    inBeta: undefined,
    allowedIntValues: [],
  };
}

export const Flag: MessageFns<Flag> = {
  encode(message: Flag, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.type !== 0) {
      writer.uint32(16).int32(message.type);
    }
    writer.uint32(26).fork();
    for (const v of message.appliesTo) {
      writer.int32(v);
    }
    writer.join();
    for (const v of message.allowedStringValues) {
      writer.uint32(34).string(v!);
    }
    if (message.minValue !== undefined) {
      Int64Value.encode({ value: message.minValue! }, writer.uint32(42).fork()).join();
    }
    if (message.maxValue !== undefined) {
      Int64Value.encode({ value: message.maxValue! }, writer.uint32(50).fork()).join();
    }
    if (message.requiresRestart !== undefined) {
      BoolValue.encode({ value: message.requiresRestart! }, writer.uint32(58).fork()).join();
    }
    if (message.kind !== "") {
      writer.uint32(66).string(message.kind);
    }
    if (message.inBeta !== undefined) {
      BoolValue.encode({ value: message.inBeta! }, writer.uint32(74).fork()).join();
    }
    writer.uint32(82).fork();
    for (const v of message.allowedIntValues) {
      writer.int64(v.toString());
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Flag {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFlag();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 3:
          if (tag === 24) {
            message.appliesTo.push(reader.int32() as any);

            continue;
          }

          if (tag === 26) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.appliesTo.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.allowedStringValues.push(reader.string());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.minValue = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.maxValue = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.requiresRestart = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.inBeta = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 10:
          if (tag === 80) {
            message.allowedIntValues.push(Long.fromString(reader.int64().toString()));

            continue;
          }

          if (tag === 82) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.allowedIntValues.push(Long.fromString(reader.int64().toString()));
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Flag {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      type: isSet(object.type) ? sqlFlagTypeFromJSON(object.type) : 0,
      appliesTo: globalThis.Array.isArray(object?.appliesTo)
        ? object.appliesTo.map((e: any) => sqlDatabaseVersionFromJSON(e))
        : [],
      allowedStringValues: globalThis.Array.isArray(object?.allowedStringValues)
        ? object.allowedStringValues.map((e: any) => globalThis.String(e))
        : [],
      minValue: isSet(object.minValue) ? Long.fromValue(object.minValue) : undefined,
      maxValue: isSet(object.maxValue) ? Long.fromValue(object.maxValue) : undefined,
      requiresRestart: isSet(object.requiresRestart) ? Boolean(object.requiresRestart) : undefined,
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      inBeta: isSet(object.inBeta) ? Boolean(object.inBeta) : undefined,
      allowedIntValues: globalThis.Array.isArray(object?.allowedIntValues)
        ? object.allowedIntValues.map((e: any) => Long.fromValue(e))
        : [],
    };
  },

  toJSON(message: Flag): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.type !== 0) {
      obj.type = sqlFlagTypeToJSON(message.type);
    }
    if (message.appliesTo?.length) {
      obj.appliesTo = message.appliesTo.map((e) => sqlDatabaseVersionToJSON(e));
    }
    if (message.allowedStringValues?.length) {
      obj.allowedStringValues = message.allowedStringValues;
    }
    if (message.minValue !== undefined) {
      obj.minValue = message.minValue;
    }
    if (message.maxValue !== undefined) {
      obj.maxValue = message.maxValue;
    }
    if (message.requiresRestart !== undefined) {
      obj.requiresRestart = message.requiresRestart;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.inBeta !== undefined) {
      obj.inBeta = message.inBeta;
    }
    if (message.allowedIntValues?.length) {
      obj.allowedIntValues = message.allowedIntValues.map((e) => (e || Long.ZERO).toString());
    }
    return obj;
  },

  create(base?: DeepPartial<Flag>): Flag {
    return Flag.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Flag>): Flag {
    const message = createBaseFlag();
    message.name = object.name ?? "";
    message.type = object.type ?? 0;
    message.appliesTo = object.appliesTo?.map((e) => e) || [];
    message.allowedStringValues = object.allowedStringValues?.map((e) => e) || [];
    message.minValue = (object.minValue !== undefined && object.minValue !== null)
      ? Long.fromValue(object.minValue)
      : undefined;
    message.maxValue = (object.maxValue !== undefined && object.maxValue !== null)
      ? Long.fromValue(object.maxValue)
      : undefined;
    message.requiresRestart = object.requiresRestart ?? undefined;
    message.kind = object.kind ?? "";
    message.inBeta = object.inBeta ?? undefined;
    message.allowedIntValues = object.allowedIntValues?.map((e) => Long.fromValue(e)) || [];
    return message;
  },
};

function createBaseFlagsListResponse(): FlagsListResponse {
  return { kind: "", items: [] };
}

export const FlagsListResponse: MessageFns<FlagsListResponse> = {
  encode(message: FlagsListResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    for (const v of message.items) {
      Flag.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FlagsListResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFlagsListResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.items.push(Flag.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FlagsListResponse {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      items: globalThis.Array.isArray(object?.items) ? object.items.map((e: any) => Flag.fromJSON(e)) : [],
    };
  },

  toJSON(message: FlagsListResponse): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.items?.length) {
      obj.items = message.items.map((e) => Flag.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<FlagsListResponse>): FlagsListResponse {
    return FlagsListResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FlagsListResponse>): FlagsListResponse {
    const message = createBaseFlagsListResponse();
    message.kind = object.kind ?? "";
    message.items = object.items?.map((e) => Flag.fromPartial(e)) || [];
    return message;
  },
};

function createBaseImportContext(): ImportContext {
  return {
    uri: "",
    database: "",
    kind: "",
    fileType: 0,
    csvImportOptions: undefined,
    importUser: "",
    bakImportOptions: undefined,
    sqlImportOptions: undefined,
  };
}

export const ImportContext: MessageFns<ImportContext> = {
  encode(message: ImportContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.uri !== "") {
      writer.uint32(10).string(message.uri);
    }
    if (message.database !== "") {
      writer.uint32(18).string(message.database);
    }
    if (message.kind !== "") {
      writer.uint32(26).string(message.kind);
    }
    if (message.fileType !== 0) {
      writer.uint32(32).int32(message.fileType);
    }
    if (message.csvImportOptions !== undefined) {
      ImportContext_SqlCsvImportOptions.encode(message.csvImportOptions, writer.uint32(42).fork()).join();
    }
    if (message.importUser !== "") {
      writer.uint32(50).string(message.importUser);
    }
    if (message.bakImportOptions !== undefined) {
      ImportContext_SqlBakImportOptions.encode(message.bakImportOptions, writer.uint32(58).fork()).join();
    }
    if (message.sqlImportOptions !== undefined) {
      ImportContext_SqlImportOptions.encode(message.sqlImportOptions, writer.uint32(66).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.uri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.database = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.fileType = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.csvImportOptions = ImportContext_SqlCsvImportOptions.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.importUser = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.bakImportOptions = ImportContext_SqlBakImportOptions.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.sqlImportOptions = ImportContext_SqlImportOptions.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportContext {
    return {
      uri: isSet(object.uri) ? globalThis.String(object.uri) : "",
      database: isSet(object.database) ? globalThis.String(object.database) : "",
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      fileType: isSet(object.fileType) ? sqlFileTypeFromJSON(object.fileType) : 0,
      csvImportOptions: isSet(object.csvImportOptions)
        ? ImportContext_SqlCsvImportOptions.fromJSON(object.csvImportOptions)
        : undefined,
      importUser: isSet(object.importUser) ? globalThis.String(object.importUser) : "",
      bakImportOptions: isSet(object.bakImportOptions)
        ? ImportContext_SqlBakImportOptions.fromJSON(object.bakImportOptions)
        : undefined,
      sqlImportOptions: isSet(object.sqlImportOptions)
        ? ImportContext_SqlImportOptions.fromJSON(object.sqlImportOptions)
        : undefined,
    };
  },

  toJSON(message: ImportContext): unknown {
    const obj: any = {};
    if (message.uri !== "") {
      obj.uri = message.uri;
    }
    if (message.database !== "") {
      obj.database = message.database;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.fileType !== 0) {
      obj.fileType = sqlFileTypeToJSON(message.fileType);
    }
    if (message.csvImportOptions !== undefined) {
      obj.csvImportOptions = ImportContext_SqlCsvImportOptions.toJSON(message.csvImportOptions);
    }
    if (message.importUser !== "") {
      obj.importUser = message.importUser;
    }
    if (message.bakImportOptions !== undefined) {
      obj.bakImportOptions = ImportContext_SqlBakImportOptions.toJSON(message.bakImportOptions);
    }
    if (message.sqlImportOptions !== undefined) {
      obj.sqlImportOptions = ImportContext_SqlImportOptions.toJSON(message.sqlImportOptions);
    }
    return obj;
  },

  create(base?: DeepPartial<ImportContext>): ImportContext {
    return ImportContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportContext>): ImportContext {
    const message = createBaseImportContext();
    message.uri = object.uri ?? "";
    message.database = object.database ?? "";
    message.kind = object.kind ?? "";
    message.fileType = object.fileType ?? 0;
    message.csvImportOptions = (object.csvImportOptions !== undefined && object.csvImportOptions !== null)
      ? ImportContext_SqlCsvImportOptions.fromPartial(object.csvImportOptions)
      : undefined;
    message.importUser = object.importUser ?? "";
    message.bakImportOptions = (object.bakImportOptions !== undefined && object.bakImportOptions !== null)
      ? ImportContext_SqlBakImportOptions.fromPartial(object.bakImportOptions)
      : undefined;
    message.sqlImportOptions = (object.sqlImportOptions !== undefined && object.sqlImportOptions !== null)
      ? ImportContext_SqlImportOptions.fromPartial(object.sqlImportOptions)
      : undefined;
    return message;
  },
};

function createBaseImportContext_SqlImportOptions(): ImportContext_SqlImportOptions {
  return { threads: undefined, parallel: undefined };
}

export const ImportContext_SqlImportOptions: MessageFns<ImportContext_SqlImportOptions> = {
  encode(message: ImportContext_SqlImportOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.threads !== undefined) {
      Int32Value.encode({ value: message.threads! }, writer.uint32(10).fork()).join();
    }
    if (message.parallel !== undefined) {
      BoolValue.encode({ value: message.parallel! }, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportContext_SqlImportOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportContext_SqlImportOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.threads = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.parallel = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportContext_SqlImportOptions {
    return {
      threads: isSet(object.threads) ? Number(object.threads) : undefined,
      parallel: isSet(object.parallel) ? Boolean(object.parallel) : undefined,
    };
  },

  toJSON(message: ImportContext_SqlImportOptions): unknown {
    const obj: any = {};
    if (message.threads !== undefined) {
      obj.threads = message.threads;
    }
    if (message.parallel !== undefined) {
      obj.parallel = message.parallel;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportContext_SqlImportOptions>): ImportContext_SqlImportOptions {
    return ImportContext_SqlImportOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportContext_SqlImportOptions>): ImportContext_SqlImportOptions {
    const message = createBaseImportContext_SqlImportOptions();
    message.threads = object.threads ?? undefined;
    message.parallel = object.parallel ?? undefined;
    return message;
  },
};

function createBaseImportContext_SqlCsvImportOptions(): ImportContext_SqlCsvImportOptions {
  return {
    table: "",
    columns: [],
    escapeCharacter: "",
    quoteCharacter: "",
    fieldsTerminatedBy: "",
    linesTerminatedBy: "",
  };
}

export const ImportContext_SqlCsvImportOptions: MessageFns<ImportContext_SqlCsvImportOptions> = {
  encode(message: ImportContext_SqlCsvImportOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.table !== "") {
      writer.uint32(10).string(message.table);
    }
    for (const v of message.columns) {
      writer.uint32(18).string(v!);
    }
    if (message.escapeCharacter !== "") {
      writer.uint32(34).string(message.escapeCharacter);
    }
    if (message.quoteCharacter !== "") {
      writer.uint32(42).string(message.quoteCharacter);
    }
    if (message.fieldsTerminatedBy !== "") {
      writer.uint32(50).string(message.fieldsTerminatedBy);
    }
    if (message.linesTerminatedBy !== "") {
      writer.uint32(66).string(message.linesTerminatedBy);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportContext_SqlCsvImportOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportContext_SqlCsvImportOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.table = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.columns.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.escapeCharacter = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.quoteCharacter = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.fieldsTerminatedBy = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.linesTerminatedBy = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportContext_SqlCsvImportOptions {
    return {
      table: isSet(object.table) ? globalThis.String(object.table) : "",
      columns: globalThis.Array.isArray(object?.columns) ? object.columns.map((e: any) => globalThis.String(e)) : [],
      escapeCharacter: isSet(object.escapeCharacter) ? globalThis.String(object.escapeCharacter) : "",
      quoteCharacter: isSet(object.quoteCharacter) ? globalThis.String(object.quoteCharacter) : "",
      fieldsTerminatedBy: isSet(object.fieldsTerminatedBy) ? globalThis.String(object.fieldsTerminatedBy) : "",
      linesTerminatedBy: isSet(object.linesTerminatedBy) ? globalThis.String(object.linesTerminatedBy) : "",
    };
  },

  toJSON(message: ImportContext_SqlCsvImportOptions): unknown {
    const obj: any = {};
    if (message.table !== "") {
      obj.table = message.table;
    }
    if (message.columns?.length) {
      obj.columns = message.columns;
    }
    if (message.escapeCharacter !== "") {
      obj.escapeCharacter = message.escapeCharacter;
    }
    if (message.quoteCharacter !== "") {
      obj.quoteCharacter = message.quoteCharacter;
    }
    if (message.fieldsTerminatedBy !== "") {
      obj.fieldsTerminatedBy = message.fieldsTerminatedBy;
    }
    if (message.linesTerminatedBy !== "") {
      obj.linesTerminatedBy = message.linesTerminatedBy;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportContext_SqlCsvImportOptions>): ImportContext_SqlCsvImportOptions {
    return ImportContext_SqlCsvImportOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportContext_SqlCsvImportOptions>): ImportContext_SqlCsvImportOptions {
    const message = createBaseImportContext_SqlCsvImportOptions();
    message.table = object.table ?? "";
    message.columns = object.columns?.map((e) => e) || [];
    message.escapeCharacter = object.escapeCharacter ?? "";
    message.quoteCharacter = object.quoteCharacter ?? "";
    message.fieldsTerminatedBy = object.fieldsTerminatedBy ?? "";
    message.linesTerminatedBy = object.linesTerminatedBy ?? "";
    return message;
  },
};

function createBaseImportContext_SqlBakImportOptions(): ImportContext_SqlBakImportOptions {
  return {
    encryptionOptions: undefined,
    striped: undefined,
    noRecovery: undefined,
    recoveryOnly: undefined,
    bakType: 0,
    stopAt: undefined,
    stopAtMark: "",
  };
}

export const ImportContext_SqlBakImportOptions: MessageFns<ImportContext_SqlBakImportOptions> = {
  encode(message: ImportContext_SqlBakImportOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.encryptionOptions !== undefined) {
      ImportContext_SqlBakImportOptions_EncryptionOptions.encode(message.encryptionOptions, writer.uint32(10).fork())
        .join();
    }
    if (message.striped !== undefined) {
      BoolValue.encode({ value: message.striped! }, writer.uint32(18).fork()).join();
    }
    if (message.noRecovery !== undefined) {
      BoolValue.encode({ value: message.noRecovery! }, writer.uint32(34).fork()).join();
    }
    if (message.recoveryOnly !== undefined) {
      BoolValue.encode({ value: message.recoveryOnly! }, writer.uint32(42).fork()).join();
    }
    if (message.bakType !== 0) {
      writer.uint32(48).int32(message.bakType);
    }
    if (message.stopAt !== undefined) {
      Timestamp.encode(toTimestamp(message.stopAt), writer.uint32(58).fork()).join();
    }
    if (message.stopAtMark !== "") {
      writer.uint32(66).string(message.stopAtMark);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportContext_SqlBakImportOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportContext_SqlBakImportOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.encryptionOptions = ImportContext_SqlBakImportOptions_EncryptionOptions.decode(
            reader,
            reader.uint32(),
          );
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.striped = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.noRecovery = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.recoveryOnly = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.bakType = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.stopAt = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.stopAtMark = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportContext_SqlBakImportOptions {
    return {
      encryptionOptions: isSet(object.encryptionOptions)
        ? ImportContext_SqlBakImportOptions_EncryptionOptions.fromJSON(object.encryptionOptions)
        : undefined,
      striped: isSet(object.striped) ? Boolean(object.striped) : undefined,
      noRecovery: isSet(object.noRecovery) ? Boolean(object.noRecovery) : undefined,
      recoveryOnly: isSet(object.recoveryOnly) ? Boolean(object.recoveryOnly) : undefined,
      bakType: isSet(object.bakType) ? bakTypeFromJSON(object.bakType) : 0,
      stopAt: isSet(object.stopAt) ? fromJsonTimestamp(object.stopAt) : undefined,
      stopAtMark: isSet(object.stopAtMark) ? globalThis.String(object.stopAtMark) : "",
    };
  },

  toJSON(message: ImportContext_SqlBakImportOptions): unknown {
    const obj: any = {};
    if (message.encryptionOptions !== undefined) {
      obj.encryptionOptions = ImportContext_SqlBakImportOptions_EncryptionOptions.toJSON(message.encryptionOptions);
    }
    if (message.striped !== undefined) {
      obj.striped = message.striped;
    }
    if (message.noRecovery !== undefined) {
      obj.noRecovery = message.noRecovery;
    }
    if (message.recoveryOnly !== undefined) {
      obj.recoveryOnly = message.recoveryOnly;
    }
    if (message.bakType !== 0) {
      obj.bakType = bakTypeToJSON(message.bakType);
    }
    if (message.stopAt !== undefined) {
      obj.stopAt = message.stopAt.toISOString();
    }
    if (message.stopAtMark !== "") {
      obj.stopAtMark = message.stopAtMark;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportContext_SqlBakImportOptions>): ImportContext_SqlBakImportOptions {
    return ImportContext_SqlBakImportOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportContext_SqlBakImportOptions>): ImportContext_SqlBakImportOptions {
    const message = createBaseImportContext_SqlBakImportOptions();
    message.encryptionOptions = (object.encryptionOptions !== undefined && object.encryptionOptions !== null)
      ? ImportContext_SqlBakImportOptions_EncryptionOptions.fromPartial(object.encryptionOptions)
      : undefined;
    message.striped = object.striped ?? undefined;
    message.noRecovery = object.noRecovery ?? undefined;
    message.recoveryOnly = object.recoveryOnly ?? undefined;
    message.bakType = object.bakType ?? 0;
    message.stopAt = object.stopAt ?? undefined;
    message.stopAtMark = object.stopAtMark ?? "";
    return message;
  },
};

function createBaseImportContext_SqlBakImportOptions_EncryptionOptions(): ImportContext_SqlBakImportOptions_EncryptionOptions {
  return { certPath: "", pvkPath: "", pvkPassword: "" };
}

export const ImportContext_SqlBakImportOptions_EncryptionOptions: MessageFns<
  ImportContext_SqlBakImportOptions_EncryptionOptions
> = {
  encode(
    message: ImportContext_SqlBakImportOptions_EncryptionOptions,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.certPath !== "") {
      writer.uint32(10).string(message.certPath);
    }
    if (message.pvkPath !== "") {
      writer.uint32(18).string(message.pvkPath);
    }
    if (message.pvkPassword !== "") {
      writer.uint32(26).string(message.pvkPassword);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportContext_SqlBakImportOptions_EncryptionOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportContext_SqlBakImportOptions_EncryptionOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.certPath = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.pvkPath = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pvkPassword = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportContext_SqlBakImportOptions_EncryptionOptions {
    return {
      certPath: isSet(object.certPath) ? globalThis.String(object.certPath) : "",
      pvkPath: isSet(object.pvkPath) ? globalThis.String(object.pvkPath) : "",
      pvkPassword: isSet(object.pvkPassword) ? globalThis.String(object.pvkPassword) : "",
    };
  },

  toJSON(message: ImportContext_SqlBakImportOptions_EncryptionOptions): unknown {
    const obj: any = {};
    if (message.certPath !== "") {
      obj.certPath = message.certPath;
    }
    if (message.pvkPath !== "") {
      obj.pvkPath = message.pvkPath;
    }
    if (message.pvkPassword !== "") {
      obj.pvkPassword = message.pvkPassword;
    }
    return obj;
  },

  create(
    base?: DeepPartial<ImportContext_SqlBakImportOptions_EncryptionOptions>,
  ): ImportContext_SqlBakImportOptions_EncryptionOptions {
    return ImportContext_SqlBakImportOptions_EncryptionOptions.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ImportContext_SqlBakImportOptions_EncryptionOptions>,
  ): ImportContext_SqlBakImportOptions_EncryptionOptions {
    const message = createBaseImportContext_SqlBakImportOptions_EncryptionOptions();
    message.certPath = object.certPath ?? "";
    message.pvkPath = object.pvkPath ?? "";
    message.pvkPassword = object.pvkPassword ?? "";
    return message;
  },
};

function createBaseInstancesCloneRequest(): InstancesCloneRequest {
  return { cloneContext: undefined };
}

export const InstancesCloneRequest: MessageFns<InstancesCloneRequest> = {
  encode(message: InstancesCloneRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.cloneContext !== undefined) {
      CloneContext.encode(message.cloneContext, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstancesCloneRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstancesCloneRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.cloneContext = CloneContext.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstancesCloneRequest {
    return { cloneContext: isSet(object.cloneContext) ? CloneContext.fromJSON(object.cloneContext) : undefined };
  },

  toJSON(message: InstancesCloneRequest): unknown {
    const obj: any = {};
    if (message.cloneContext !== undefined) {
      obj.cloneContext = CloneContext.toJSON(message.cloneContext);
    }
    return obj;
  },

  create(base?: DeepPartial<InstancesCloneRequest>): InstancesCloneRequest {
    return InstancesCloneRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstancesCloneRequest>): InstancesCloneRequest {
    const message = createBaseInstancesCloneRequest();
    message.cloneContext = (object.cloneContext !== undefined && object.cloneContext !== null)
      ? CloneContext.fromPartial(object.cloneContext)
      : undefined;
    return message;
  },
};

function createBaseInstancesDemoteMasterRequest(): InstancesDemoteMasterRequest {
  return { demoteMasterContext: undefined };
}

export const InstancesDemoteMasterRequest: MessageFns<InstancesDemoteMasterRequest> = {
  encode(message: InstancesDemoteMasterRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.demoteMasterContext !== undefined) {
      DemoteMasterContext.encode(message.demoteMasterContext, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstancesDemoteMasterRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstancesDemoteMasterRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.demoteMasterContext = DemoteMasterContext.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstancesDemoteMasterRequest {
    return {
      demoteMasterContext: isSet(object.demoteMasterContext)
        ? DemoteMasterContext.fromJSON(object.demoteMasterContext)
        : undefined,
    };
  },

  toJSON(message: InstancesDemoteMasterRequest): unknown {
    const obj: any = {};
    if (message.demoteMasterContext !== undefined) {
      obj.demoteMasterContext = DemoteMasterContext.toJSON(message.demoteMasterContext);
    }
    return obj;
  },

  create(base?: DeepPartial<InstancesDemoteMasterRequest>): InstancesDemoteMasterRequest {
    return InstancesDemoteMasterRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstancesDemoteMasterRequest>): InstancesDemoteMasterRequest {
    const message = createBaseInstancesDemoteMasterRequest();
    message.demoteMasterContext = (object.demoteMasterContext !== undefined && object.demoteMasterContext !== null)
      ? DemoteMasterContext.fromPartial(object.demoteMasterContext)
      : undefined;
    return message;
  },
};

function createBaseInstancesDemoteRequest(): InstancesDemoteRequest {
  return { demoteContext: undefined };
}

export const InstancesDemoteRequest: MessageFns<InstancesDemoteRequest> = {
  encode(message: InstancesDemoteRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.demoteContext !== undefined) {
      DemoteContext.encode(message.demoteContext, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstancesDemoteRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstancesDemoteRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.demoteContext = DemoteContext.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstancesDemoteRequest {
    return { demoteContext: isSet(object.demoteContext) ? DemoteContext.fromJSON(object.demoteContext) : undefined };
  },

  toJSON(message: InstancesDemoteRequest): unknown {
    const obj: any = {};
    if (message.demoteContext !== undefined) {
      obj.demoteContext = DemoteContext.toJSON(message.demoteContext);
    }
    return obj;
  },

  create(base?: DeepPartial<InstancesDemoteRequest>): InstancesDemoteRequest {
    return InstancesDemoteRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstancesDemoteRequest>): InstancesDemoteRequest {
    const message = createBaseInstancesDemoteRequest();
    message.demoteContext = (object.demoteContext !== undefined && object.demoteContext !== null)
      ? DemoteContext.fromPartial(object.demoteContext)
      : undefined;
    return message;
  },
};

function createBaseInstancesExportRequest(): InstancesExportRequest {
  return { exportContext: undefined };
}

export const InstancesExportRequest: MessageFns<InstancesExportRequest> = {
  encode(message: InstancesExportRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.exportContext !== undefined) {
      ExportContext.encode(message.exportContext, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstancesExportRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstancesExportRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.exportContext = ExportContext.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstancesExportRequest {
    return { exportContext: isSet(object.exportContext) ? ExportContext.fromJSON(object.exportContext) : undefined };
  },

  toJSON(message: InstancesExportRequest): unknown {
    const obj: any = {};
    if (message.exportContext !== undefined) {
      obj.exportContext = ExportContext.toJSON(message.exportContext);
    }
    return obj;
  },

  create(base?: DeepPartial<InstancesExportRequest>): InstancesExportRequest {
    return InstancesExportRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstancesExportRequest>): InstancesExportRequest {
    const message = createBaseInstancesExportRequest();
    message.exportContext = (object.exportContext !== undefined && object.exportContext !== null)
      ? ExportContext.fromPartial(object.exportContext)
      : undefined;
    return message;
  },
};

function createBaseInstancesFailoverRequest(): InstancesFailoverRequest {
  return { failoverContext: undefined };
}

export const InstancesFailoverRequest: MessageFns<InstancesFailoverRequest> = {
  encode(message: InstancesFailoverRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.failoverContext !== undefined) {
      FailoverContext.encode(message.failoverContext, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstancesFailoverRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstancesFailoverRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.failoverContext = FailoverContext.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstancesFailoverRequest {
    return {
      failoverContext: isSet(object.failoverContext) ? FailoverContext.fromJSON(object.failoverContext) : undefined,
    };
  },

  toJSON(message: InstancesFailoverRequest): unknown {
    const obj: any = {};
    if (message.failoverContext !== undefined) {
      obj.failoverContext = FailoverContext.toJSON(message.failoverContext);
    }
    return obj;
  },

  create(base?: DeepPartial<InstancesFailoverRequest>): InstancesFailoverRequest {
    return InstancesFailoverRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstancesFailoverRequest>): InstancesFailoverRequest {
    const message = createBaseInstancesFailoverRequest();
    message.failoverContext = (object.failoverContext !== undefined && object.failoverContext !== null)
      ? FailoverContext.fromPartial(object.failoverContext)
      : undefined;
    return message;
  },
};

function createBaseInstancesImportRequest(): InstancesImportRequest {
  return { importContext: undefined };
}

export const InstancesImportRequest: MessageFns<InstancesImportRequest> = {
  encode(message: InstancesImportRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.importContext !== undefined) {
      ImportContext.encode(message.importContext, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstancesImportRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstancesImportRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.importContext = ImportContext.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstancesImportRequest {
    return { importContext: isSet(object.importContext) ? ImportContext.fromJSON(object.importContext) : undefined };
  },

  toJSON(message: InstancesImportRequest): unknown {
    const obj: any = {};
    if (message.importContext !== undefined) {
      obj.importContext = ImportContext.toJSON(message.importContext);
    }
    return obj;
  },

  create(base?: DeepPartial<InstancesImportRequest>): InstancesImportRequest {
    return InstancesImportRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstancesImportRequest>): InstancesImportRequest {
    const message = createBaseInstancesImportRequest();
    message.importContext = (object.importContext !== undefined && object.importContext !== null)
      ? ImportContext.fromPartial(object.importContext)
      : undefined;
    return message;
  },
};

function createBaseMySqlSyncConfig(): MySqlSyncConfig {
  return { initialSyncFlags: [] };
}

export const MySqlSyncConfig: MessageFns<MySqlSyncConfig> = {
  encode(message: MySqlSyncConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.initialSyncFlags) {
      SyncFlags.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MySqlSyncConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMySqlSyncConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.initialSyncFlags.push(SyncFlags.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MySqlSyncConfig {
    return {
      initialSyncFlags: globalThis.Array.isArray(object?.initialSyncFlags)
        ? object.initialSyncFlags.map((e: any) => SyncFlags.fromJSON(e))
        : [],
    };
  },

  toJSON(message: MySqlSyncConfig): unknown {
    const obj: any = {};
    if (message.initialSyncFlags?.length) {
      obj.initialSyncFlags = message.initialSyncFlags.map((e) => SyncFlags.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<MySqlSyncConfig>): MySqlSyncConfig {
    return MySqlSyncConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MySqlSyncConfig>): MySqlSyncConfig {
    const message = createBaseMySqlSyncConfig();
    message.initialSyncFlags = object.initialSyncFlags?.map((e) => SyncFlags.fromPartial(e)) || [];
    return message;
  },
};

function createBaseInstancesListResponse(): InstancesListResponse {
  return { kind: "", warnings: [], items: [], nextPageToken: "" };
}

export const InstancesListResponse: MessageFns<InstancesListResponse> = {
  encode(message: InstancesListResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    for (const v of message.warnings) {
      ApiWarning.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.items) {
      DatabaseInstance.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(34).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstancesListResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstancesListResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.warnings.push(ApiWarning.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.items.push(DatabaseInstance.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstancesListResponse {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      warnings: globalThis.Array.isArray(object?.warnings)
        ? object.warnings.map((e: any) => ApiWarning.fromJSON(e))
        : [],
      items: globalThis.Array.isArray(object?.items) ? object.items.map((e: any) => DatabaseInstance.fromJSON(e)) : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: InstancesListResponse): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.warnings?.length) {
      obj.warnings = message.warnings.map((e) => ApiWarning.toJSON(e));
    }
    if (message.items?.length) {
      obj.items = message.items.map((e) => DatabaseInstance.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<InstancesListResponse>): InstancesListResponse {
    return InstancesListResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstancesListResponse>): InstancesListResponse {
    const message = createBaseInstancesListResponse();
    message.kind = object.kind ?? "";
    message.warnings = object.warnings?.map((e) => ApiWarning.fromPartial(e)) || [];
    message.items = object.items?.map((e) => DatabaseInstance.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBaseInstancesListServerCasResponse(): InstancesListServerCasResponse {
  return { certs: [], activeVersion: "", kind: "" };
}

export const InstancesListServerCasResponse: MessageFns<InstancesListServerCasResponse> = {
  encode(message: InstancesListServerCasResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.certs) {
      SslCert.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.activeVersion !== "") {
      writer.uint32(18).string(message.activeVersion);
    }
    if (message.kind !== "") {
      writer.uint32(26).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstancesListServerCasResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstancesListServerCasResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.certs.push(SslCert.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.activeVersion = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstancesListServerCasResponse {
    return {
      certs: globalThis.Array.isArray(object?.certs) ? object.certs.map((e: any) => SslCert.fromJSON(e)) : [],
      activeVersion: isSet(object.activeVersion) ? globalThis.String(object.activeVersion) : "",
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: InstancesListServerCasResponse): unknown {
    const obj: any = {};
    if (message.certs?.length) {
      obj.certs = message.certs.map((e) => SslCert.toJSON(e));
    }
    if (message.activeVersion !== "") {
      obj.activeVersion = message.activeVersion;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<InstancesListServerCasResponse>): InstancesListServerCasResponse {
    return InstancesListServerCasResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstancesListServerCasResponse>): InstancesListServerCasResponse {
    const message = createBaseInstancesListServerCasResponse();
    message.certs = object.certs?.map((e) => SslCert.fromPartial(e)) || [];
    message.activeVersion = object.activeVersion ?? "";
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseInstancesRestoreBackupRequest(): InstancesRestoreBackupRequest {
  return { restoreBackupContext: undefined };
}

export const InstancesRestoreBackupRequest: MessageFns<InstancesRestoreBackupRequest> = {
  encode(message: InstancesRestoreBackupRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.restoreBackupContext !== undefined) {
      RestoreBackupContext.encode(message.restoreBackupContext, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstancesRestoreBackupRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstancesRestoreBackupRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.restoreBackupContext = RestoreBackupContext.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstancesRestoreBackupRequest {
    return {
      restoreBackupContext: isSet(object.restoreBackupContext)
        ? RestoreBackupContext.fromJSON(object.restoreBackupContext)
        : undefined,
    };
  },

  toJSON(message: InstancesRestoreBackupRequest): unknown {
    const obj: any = {};
    if (message.restoreBackupContext !== undefined) {
      obj.restoreBackupContext = RestoreBackupContext.toJSON(message.restoreBackupContext);
    }
    return obj;
  },

  create(base?: DeepPartial<InstancesRestoreBackupRequest>): InstancesRestoreBackupRequest {
    return InstancesRestoreBackupRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstancesRestoreBackupRequest>): InstancesRestoreBackupRequest {
    const message = createBaseInstancesRestoreBackupRequest();
    message.restoreBackupContext = (object.restoreBackupContext !== undefined && object.restoreBackupContext !== null)
      ? RestoreBackupContext.fromPartial(object.restoreBackupContext)
      : undefined;
    return message;
  },
};

function createBaseInstancesRotateServerCaRequest(): InstancesRotateServerCaRequest {
  return { rotateServerCaContext: undefined };
}

export const InstancesRotateServerCaRequest: MessageFns<InstancesRotateServerCaRequest> = {
  encode(message: InstancesRotateServerCaRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.rotateServerCaContext !== undefined) {
      RotateServerCaContext.encode(message.rotateServerCaContext, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstancesRotateServerCaRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstancesRotateServerCaRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.rotateServerCaContext = RotateServerCaContext.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstancesRotateServerCaRequest {
    return {
      rotateServerCaContext: isSet(object.rotateServerCaContext)
        ? RotateServerCaContext.fromJSON(object.rotateServerCaContext)
        : undefined,
    };
  },

  toJSON(message: InstancesRotateServerCaRequest): unknown {
    const obj: any = {};
    if (message.rotateServerCaContext !== undefined) {
      obj.rotateServerCaContext = RotateServerCaContext.toJSON(message.rotateServerCaContext);
    }
    return obj;
  },

  create(base?: DeepPartial<InstancesRotateServerCaRequest>): InstancesRotateServerCaRequest {
    return InstancesRotateServerCaRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstancesRotateServerCaRequest>): InstancesRotateServerCaRequest {
    const message = createBaseInstancesRotateServerCaRequest();
    message.rotateServerCaContext =
      (object.rotateServerCaContext !== undefined && object.rotateServerCaContext !== null)
        ? RotateServerCaContext.fromPartial(object.rotateServerCaContext)
        : undefined;
    return message;
  },
};

function createBaseInstancesTruncateLogRequest(): InstancesTruncateLogRequest {
  return { truncateLogContext: undefined };
}

export const InstancesTruncateLogRequest: MessageFns<InstancesTruncateLogRequest> = {
  encode(message: InstancesTruncateLogRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.truncateLogContext !== undefined) {
      TruncateLogContext.encode(message.truncateLogContext, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstancesTruncateLogRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstancesTruncateLogRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.truncateLogContext = TruncateLogContext.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstancesTruncateLogRequest {
    return {
      truncateLogContext: isSet(object.truncateLogContext)
        ? TruncateLogContext.fromJSON(object.truncateLogContext)
        : undefined,
    };
  },

  toJSON(message: InstancesTruncateLogRequest): unknown {
    const obj: any = {};
    if (message.truncateLogContext !== undefined) {
      obj.truncateLogContext = TruncateLogContext.toJSON(message.truncateLogContext);
    }
    return obj;
  },

  create(base?: DeepPartial<InstancesTruncateLogRequest>): InstancesTruncateLogRequest {
    return InstancesTruncateLogRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstancesTruncateLogRequest>): InstancesTruncateLogRequest {
    const message = createBaseInstancesTruncateLogRequest();
    message.truncateLogContext = (object.truncateLogContext !== undefined && object.truncateLogContext !== null)
      ? TruncateLogContext.fromPartial(object.truncateLogContext)
      : undefined;
    return message;
  },
};

function createBaseInstancesAcquireSsrsLeaseRequest(): InstancesAcquireSsrsLeaseRequest {
  return { acquireSsrsLeaseContext: undefined };
}

export const InstancesAcquireSsrsLeaseRequest: MessageFns<InstancesAcquireSsrsLeaseRequest> = {
  encode(message: InstancesAcquireSsrsLeaseRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.acquireSsrsLeaseContext !== undefined) {
      AcquireSsrsLeaseContext.encode(message.acquireSsrsLeaseContext, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstancesAcquireSsrsLeaseRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstancesAcquireSsrsLeaseRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.acquireSsrsLeaseContext = AcquireSsrsLeaseContext.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstancesAcquireSsrsLeaseRequest {
    return {
      acquireSsrsLeaseContext: isSet(object.acquireSsrsLeaseContext)
        ? AcquireSsrsLeaseContext.fromJSON(object.acquireSsrsLeaseContext)
        : undefined,
    };
  },

  toJSON(message: InstancesAcquireSsrsLeaseRequest): unknown {
    const obj: any = {};
    if (message.acquireSsrsLeaseContext !== undefined) {
      obj.acquireSsrsLeaseContext = AcquireSsrsLeaseContext.toJSON(message.acquireSsrsLeaseContext);
    }
    return obj;
  },

  create(base?: DeepPartial<InstancesAcquireSsrsLeaseRequest>): InstancesAcquireSsrsLeaseRequest {
    return InstancesAcquireSsrsLeaseRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstancesAcquireSsrsLeaseRequest>): InstancesAcquireSsrsLeaseRequest {
    const message = createBaseInstancesAcquireSsrsLeaseRequest();
    message.acquireSsrsLeaseContext =
      (object.acquireSsrsLeaseContext !== undefined && object.acquireSsrsLeaseContext !== null)
        ? AcquireSsrsLeaseContext.fromPartial(object.acquireSsrsLeaseContext)
        : undefined;
    return message;
  },
};

function createBasePerformDiskShrinkContext(): PerformDiskShrinkContext {
  return { targetSizeGb: Long.ZERO };
}

export const PerformDiskShrinkContext: MessageFns<PerformDiskShrinkContext> = {
  encode(message: PerformDiskShrinkContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.targetSizeGb.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.targetSizeGb.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PerformDiskShrinkContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePerformDiskShrinkContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.targetSizeGb = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PerformDiskShrinkContext {
    return { targetSizeGb: isSet(object.targetSizeGb) ? Long.fromValue(object.targetSizeGb) : Long.ZERO };
  },

  toJSON(message: PerformDiskShrinkContext): unknown {
    const obj: any = {};
    if (!message.targetSizeGb.equals(Long.ZERO)) {
      obj.targetSizeGb = (message.targetSizeGb || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<PerformDiskShrinkContext>): PerformDiskShrinkContext {
    return PerformDiskShrinkContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PerformDiskShrinkContext>): PerformDiskShrinkContext {
    const message = createBasePerformDiskShrinkContext();
    message.targetSizeGb = (object.targetSizeGb !== undefined && object.targetSizeGb !== null)
      ? Long.fromValue(object.targetSizeGb)
      : Long.ZERO;
    return message;
  },
};

function createBaseSqlInstancesGetDiskShrinkConfigResponse(): SqlInstancesGetDiskShrinkConfigResponse {
  return { kind: "", minimalTargetSizeGb: Long.ZERO, message: "" };
}

export const SqlInstancesGetDiskShrinkConfigResponse: MessageFns<SqlInstancesGetDiskShrinkConfigResponse> = {
  encode(message: SqlInstancesGetDiskShrinkConfigResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (!message.minimalTargetSizeGb.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.minimalTargetSizeGb.toString());
    }
    if (message.message !== "") {
      writer.uint32(26).string(message.message);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SqlInstancesGetDiskShrinkConfigResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSqlInstancesGetDiskShrinkConfigResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.minimalTargetSizeGb = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.message = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SqlInstancesGetDiskShrinkConfigResponse {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      minimalTargetSizeGb: isSet(object.minimalTargetSizeGb) ? Long.fromValue(object.minimalTargetSizeGb) : Long.ZERO,
      message: isSet(object.message) ? globalThis.String(object.message) : "",
    };
  },

  toJSON(message: SqlInstancesGetDiskShrinkConfigResponse): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (!message.minimalTargetSizeGb.equals(Long.ZERO)) {
      obj.minimalTargetSizeGb = (message.minimalTargetSizeGb || Long.ZERO).toString();
    }
    if (message.message !== "") {
      obj.message = message.message;
    }
    return obj;
  },

  create(base?: DeepPartial<SqlInstancesGetDiskShrinkConfigResponse>): SqlInstancesGetDiskShrinkConfigResponse {
    return SqlInstancesGetDiskShrinkConfigResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SqlInstancesGetDiskShrinkConfigResponse>): SqlInstancesGetDiskShrinkConfigResponse {
    const message = createBaseSqlInstancesGetDiskShrinkConfigResponse();
    message.kind = object.kind ?? "";
    message.minimalTargetSizeGb = (object.minimalTargetSizeGb !== undefined && object.minimalTargetSizeGb !== null)
      ? Long.fromValue(object.minimalTargetSizeGb)
      : Long.ZERO;
    message.message = object.message ?? "";
    return message;
  },
};

function createBaseSqlInstancesVerifyExternalSyncSettingsResponse(): SqlInstancesVerifyExternalSyncSettingsResponse {
  return { kind: "", errors: [], warnings: [] };
}

export const SqlInstancesVerifyExternalSyncSettingsResponse: MessageFns<
  SqlInstancesVerifyExternalSyncSettingsResponse
> = {
  encode(
    message: SqlInstancesVerifyExternalSyncSettingsResponse,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    for (const v of message.errors) {
      SqlExternalSyncSettingError.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.warnings) {
      SqlExternalSyncSettingError.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SqlInstancesVerifyExternalSyncSettingsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSqlInstancesVerifyExternalSyncSettingsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.errors.push(SqlExternalSyncSettingError.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.warnings.push(SqlExternalSyncSettingError.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SqlInstancesVerifyExternalSyncSettingsResponse {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      errors: globalThis.Array.isArray(object?.errors)
        ? object.errors.map((e: any) => SqlExternalSyncSettingError.fromJSON(e))
        : [],
      warnings: globalThis.Array.isArray(object?.warnings)
        ? object.warnings.map((e: any) => SqlExternalSyncSettingError.fromJSON(e))
        : [],
    };
  },

  toJSON(message: SqlInstancesVerifyExternalSyncSettingsResponse): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.errors?.length) {
      obj.errors = message.errors.map((e) => SqlExternalSyncSettingError.toJSON(e));
    }
    if (message.warnings?.length) {
      obj.warnings = message.warnings.map((e) => SqlExternalSyncSettingError.toJSON(e));
    }
    return obj;
  },

  create(
    base?: DeepPartial<SqlInstancesVerifyExternalSyncSettingsResponse>,
  ): SqlInstancesVerifyExternalSyncSettingsResponse {
    return SqlInstancesVerifyExternalSyncSettingsResponse.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<SqlInstancesVerifyExternalSyncSettingsResponse>,
  ): SqlInstancesVerifyExternalSyncSettingsResponse {
    const message = createBaseSqlInstancesVerifyExternalSyncSettingsResponse();
    message.kind = object.kind ?? "";
    message.errors = object.errors?.map((e) => SqlExternalSyncSettingError.fromPartial(e)) || [];
    message.warnings = object.warnings?.map((e) => SqlExternalSyncSettingError.fromPartial(e)) || [];
    return message;
  },
};

function createBaseSqlExternalSyncSettingError(): SqlExternalSyncSettingError {
  return { kind: "", type: 0, detail: "" };
}

export const SqlExternalSyncSettingError: MessageFns<SqlExternalSyncSettingError> = {
  encode(message: SqlExternalSyncSettingError, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.type !== 0) {
      writer.uint32(16).int32(message.type);
    }
    if (message.detail !== "") {
      writer.uint32(26).string(message.detail);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SqlExternalSyncSettingError {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSqlExternalSyncSettingError();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.detail = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SqlExternalSyncSettingError {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      type: isSet(object.type) ? sqlExternalSyncSettingError_SqlExternalSyncSettingErrorTypeFromJSON(object.type) : 0,
      detail: isSet(object.detail) ? globalThis.String(object.detail) : "",
    };
  },

  toJSON(message: SqlExternalSyncSettingError): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.type !== 0) {
      obj.type = sqlExternalSyncSettingError_SqlExternalSyncSettingErrorTypeToJSON(message.type);
    }
    if (message.detail !== "") {
      obj.detail = message.detail;
    }
    return obj;
  },

  create(base?: DeepPartial<SqlExternalSyncSettingError>): SqlExternalSyncSettingError {
    return SqlExternalSyncSettingError.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SqlExternalSyncSettingError>): SqlExternalSyncSettingError {
    const message = createBaseSqlExternalSyncSettingError();
    message.kind = object.kind ?? "";
    message.type = object.type ?? 0;
    message.detail = object.detail ?? "";
    return message;
  },
};

function createBaseIpConfiguration(): IpConfiguration {
  return {
    ipv4Enabled: undefined,
    privateNetwork: "",
    requireSsl: undefined,
    authorizedNetworks: [],
    allocatedIpRange: "",
    enablePrivatePathForGoogleCloudServices: undefined,
    sslMode: 0,
    pscConfig: undefined,
  };
}

export const IpConfiguration: MessageFns<IpConfiguration> = {
  encode(message: IpConfiguration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.ipv4Enabled !== undefined) {
      BoolValue.encode({ value: message.ipv4Enabled! }, writer.uint32(10).fork()).join();
    }
    if (message.privateNetwork !== "") {
      writer.uint32(18).string(message.privateNetwork);
    }
    if (message.requireSsl !== undefined) {
      BoolValue.encode({ value: message.requireSsl! }, writer.uint32(26).fork()).join();
    }
    for (const v of message.authorizedNetworks) {
      AclEntry.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.allocatedIpRange !== "") {
      writer.uint32(50).string(message.allocatedIpRange);
    }
    if (message.enablePrivatePathForGoogleCloudServices !== undefined) {
      BoolValue.encode({ value: message.enablePrivatePathForGoogleCloudServices! }, writer.uint32(58).fork()).join();
    }
    if (message.sslMode !== 0) {
      writer.uint32(64).int32(message.sslMode);
    }
    if (message.pscConfig !== undefined) {
      PscConfig.encode(message.pscConfig, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): IpConfiguration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseIpConfiguration();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.ipv4Enabled = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.privateNetwork = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.requireSsl = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.authorizedNetworks.push(AclEntry.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.allocatedIpRange = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.enablePrivatePathForGoogleCloudServices = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.sslMode = reader.int32() as any;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.pscConfig = PscConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): IpConfiguration {
    return {
      ipv4Enabled: isSet(object.ipv4Enabled) ? Boolean(object.ipv4Enabled) : undefined,
      privateNetwork: isSet(object.privateNetwork) ? globalThis.String(object.privateNetwork) : "",
      requireSsl: isSet(object.requireSsl) ? Boolean(object.requireSsl) : undefined,
      authorizedNetworks: globalThis.Array.isArray(object?.authorizedNetworks)
        ? object.authorizedNetworks.map((e: any) => AclEntry.fromJSON(e))
        : [],
      allocatedIpRange: isSet(object.allocatedIpRange) ? globalThis.String(object.allocatedIpRange) : "",
      enablePrivatePathForGoogleCloudServices: isSet(object.enablePrivatePathForGoogleCloudServices)
        ? Boolean(object.enablePrivatePathForGoogleCloudServices)
        : undefined,
      sslMode: isSet(object.sslMode) ? ipConfiguration_SslModeFromJSON(object.sslMode) : 0,
      pscConfig: isSet(object.pscConfig) ? PscConfig.fromJSON(object.pscConfig) : undefined,
    };
  },

  toJSON(message: IpConfiguration): unknown {
    const obj: any = {};
    if (message.ipv4Enabled !== undefined) {
      obj.ipv4Enabled = message.ipv4Enabled;
    }
    if (message.privateNetwork !== "") {
      obj.privateNetwork = message.privateNetwork;
    }
    if (message.requireSsl !== undefined) {
      obj.requireSsl = message.requireSsl;
    }
    if (message.authorizedNetworks?.length) {
      obj.authorizedNetworks = message.authorizedNetworks.map((e) => AclEntry.toJSON(e));
    }
    if (message.allocatedIpRange !== "") {
      obj.allocatedIpRange = message.allocatedIpRange;
    }
    if (message.enablePrivatePathForGoogleCloudServices !== undefined) {
      obj.enablePrivatePathForGoogleCloudServices = message.enablePrivatePathForGoogleCloudServices;
    }
    if (message.sslMode !== 0) {
      obj.sslMode = ipConfiguration_SslModeToJSON(message.sslMode);
    }
    if (message.pscConfig !== undefined) {
      obj.pscConfig = PscConfig.toJSON(message.pscConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<IpConfiguration>): IpConfiguration {
    return IpConfiguration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<IpConfiguration>): IpConfiguration {
    const message = createBaseIpConfiguration();
    message.ipv4Enabled = object.ipv4Enabled ?? undefined;
    message.privateNetwork = object.privateNetwork ?? "";
    message.requireSsl = object.requireSsl ?? undefined;
    message.authorizedNetworks = object.authorizedNetworks?.map((e) => AclEntry.fromPartial(e)) || [];
    message.allocatedIpRange = object.allocatedIpRange ?? "";
    message.enablePrivatePathForGoogleCloudServices = object.enablePrivatePathForGoogleCloudServices ?? undefined;
    message.sslMode = object.sslMode ?? 0;
    message.pscConfig = (object.pscConfig !== undefined && object.pscConfig !== null)
      ? PscConfig.fromPartial(object.pscConfig)
      : undefined;
    return message;
  },
};

function createBasePscConfig(): PscConfig {
  return { pscEnabled: undefined, allowedConsumerProjects: [] };
}

export const PscConfig: MessageFns<PscConfig> = {
  encode(message: PscConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.pscEnabled !== undefined) {
      writer.uint32(8).bool(message.pscEnabled);
    }
    for (const v of message.allowedConsumerProjects) {
      writer.uint32(18).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PscConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePscConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.pscEnabled = reader.bool();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.allowedConsumerProjects.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PscConfig {
    return {
      pscEnabled: isSet(object.pscEnabled) ? globalThis.Boolean(object.pscEnabled) : undefined,
      allowedConsumerProjects: globalThis.Array.isArray(object?.allowedConsumerProjects)
        ? object.allowedConsumerProjects.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: PscConfig): unknown {
    const obj: any = {};
    if (message.pscEnabled !== undefined) {
      obj.pscEnabled = message.pscEnabled;
    }
    if (message.allowedConsumerProjects?.length) {
      obj.allowedConsumerProjects = message.allowedConsumerProjects;
    }
    return obj;
  },

  create(base?: DeepPartial<PscConfig>): PscConfig {
    return PscConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PscConfig>): PscConfig {
    const message = createBasePscConfig();
    message.pscEnabled = object.pscEnabled ?? undefined;
    message.allowedConsumerProjects = object.allowedConsumerProjects?.map((e) => e) || [];
    return message;
  },
};

function createBaseIpMapping(): IpMapping {
  return { type: 0, ipAddress: "", timeToRetire: undefined };
}

export const IpMapping: MessageFns<IpMapping> = {
  encode(message: IpMapping, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.type !== 0) {
      writer.uint32(8).int32(message.type);
    }
    if (message.ipAddress !== "") {
      writer.uint32(18).string(message.ipAddress);
    }
    if (message.timeToRetire !== undefined) {
      Timestamp.encode(toTimestamp(message.timeToRetire), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): IpMapping {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseIpMapping();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.ipAddress = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.timeToRetire = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): IpMapping {
    return {
      type: isSet(object.type) ? sqlIpAddressTypeFromJSON(object.type) : 0,
      ipAddress: isSet(object.ipAddress) ? globalThis.String(object.ipAddress) : "",
      timeToRetire: isSet(object.timeToRetire) ? fromJsonTimestamp(object.timeToRetire) : undefined,
    };
  },

  toJSON(message: IpMapping): unknown {
    const obj: any = {};
    if (message.type !== 0) {
      obj.type = sqlIpAddressTypeToJSON(message.type);
    }
    if (message.ipAddress !== "") {
      obj.ipAddress = message.ipAddress;
    }
    if (message.timeToRetire !== undefined) {
      obj.timeToRetire = message.timeToRetire.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<IpMapping>): IpMapping {
    return IpMapping.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<IpMapping>): IpMapping {
    const message = createBaseIpMapping();
    message.type = object.type ?? 0;
    message.ipAddress = object.ipAddress ?? "";
    message.timeToRetire = object.timeToRetire ?? undefined;
    return message;
  },
};

function createBaseLocationPreference(): LocationPreference {
  return { followGaeApplication: "", zone: "", secondaryZone: "", kind: "" };
}

export const LocationPreference: MessageFns<LocationPreference> = {
  encode(message: LocationPreference, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.followGaeApplication !== "") {
      writer.uint32(10).string(message.followGaeApplication);
    }
    if (message.zone !== "") {
      writer.uint32(18).string(message.zone);
    }
    if (message.secondaryZone !== "") {
      writer.uint32(34).string(message.secondaryZone);
    }
    if (message.kind !== "") {
      writer.uint32(26).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LocationPreference {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLocationPreference();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.followGaeApplication = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.zone = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.secondaryZone = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LocationPreference {
    return {
      followGaeApplication: isSet(object.followGaeApplication) ? globalThis.String(object.followGaeApplication) : "",
      zone: isSet(object.zone) ? globalThis.String(object.zone) : "",
      secondaryZone: isSet(object.secondaryZone) ? globalThis.String(object.secondaryZone) : "",
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: LocationPreference): unknown {
    const obj: any = {};
    if (message.followGaeApplication !== "") {
      obj.followGaeApplication = message.followGaeApplication;
    }
    if (message.zone !== "") {
      obj.zone = message.zone;
    }
    if (message.secondaryZone !== "") {
      obj.secondaryZone = message.secondaryZone;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<LocationPreference>): LocationPreference {
    return LocationPreference.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LocationPreference>): LocationPreference {
    const message = createBaseLocationPreference();
    message.followGaeApplication = object.followGaeApplication ?? "";
    message.zone = object.zone ?? "";
    message.secondaryZone = object.secondaryZone ?? "";
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseMaintenanceWindow(): MaintenanceWindow {
  return { hour: undefined, day: undefined, updateTrack: 0, kind: "" };
}

export const MaintenanceWindow: MessageFns<MaintenanceWindow> = {
  encode(message: MaintenanceWindow, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.hour !== undefined) {
      Int32Value.encode({ value: message.hour! }, writer.uint32(10).fork()).join();
    }
    if (message.day !== undefined) {
      Int32Value.encode({ value: message.day! }, writer.uint32(18).fork()).join();
    }
    if (message.updateTrack !== 0) {
      writer.uint32(24).int32(message.updateTrack);
    }
    if (message.kind !== "") {
      writer.uint32(34).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MaintenanceWindow {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMaintenanceWindow();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.hour = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.day = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.updateTrack = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MaintenanceWindow {
    return {
      hour: isSet(object.hour) ? Number(object.hour) : undefined,
      day: isSet(object.day) ? Number(object.day) : undefined,
      updateTrack: isSet(object.updateTrack) ? sqlUpdateTrackFromJSON(object.updateTrack) : 0,
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: MaintenanceWindow): unknown {
    const obj: any = {};
    if (message.hour !== undefined) {
      obj.hour = message.hour;
    }
    if (message.day !== undefined) {
      obj.day = message.day;
    }
    if (message.updateTrack !== 0) {
      obj.updateTrack = sqlUpdateTrackToJSON(message.updateTrack);
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<MaintenanceWindow>): MaintenanceWindow {
    return MaintenanceWindow.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MaintenanceWindow>): MaintenanceWindow {
    const message = createBaseMaintenanceWindow();
    message.hour = object.hour ?? undefined;
    message.day = object.day ?? undefined;
    message.updateTrack = object.updateTrack ?? 0;
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseDenyMaintenancePeriod(): DenyMaintenancePeriod {
  return { startDate: "", endDate: "", time: "" };
}

export const DenyMaintenancePeriod: MessageFns<DenyMaintenancePeriod> = {
  encode(message: DenyMaintenancePeriod, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startDate !== "") {
      writer.uint32(10).string(message.startDate);
    }
    if (message.endDate !== "") {
      writer.uint32(18).string(message.endDate);
    }
    if (message.time !== "") {
      writer.uint32(26).string(message.time);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DenyMaintenancePeriod {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDenyMaintenancePeriod();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startDate = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.endDate = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.time = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DenyMaintenancePeriod {
    return {
      startDate: isSet(object.startDate) ? globalThis.String(object.startDate) : "",
      endDate: isSet(object.endDate) ? globalThis.String(object.endDate) : "",
      time: isSet(object.time) ? globalThis.String(object.time) : "",
    };
  },

  toJSON(message: DenyMaintenancePeriod): unknown {
    const obj: any = {};
    if (message.startDate !== "") {
      obj.startDate = message.startDate;
    }
    if (message.endDate !== "") {
      obj.endDate = message.endDate;
    }
    if (message.time !== "") {
      obj.time = message.time;
    }
    return obj;
  },

  create(base?: DeepPartial<DenyMaintenancePeriod>): DenyMaintenancePeriod {
    return DenyMaintenancePeriod.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DenyMaintenancePeriod>): DenyMaintenancePeriod {
    const message = createBaseDenyMaintenancePeriod();
    message.startDate = object.startDate ?? "";
    message.endDate = object.endDate ?? "";
    message.time = object.time ?? "";
    return message;
  },
};

function createBaseInsightsConfig(): InsightsConfig {
  return {
    queryInsightsEnabled: false,
    recordClientAddress: false,
    recordApplicationTags: false,
    queryStringLength: undefined,
    queryPlansPerMinute: undefined,
  };
}

export const InsightsConfig: MessageFns<InsightsConfig> = {
  encode(message: InsightsConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.queryInsightsEnabled !== false) {
      writer.uint32(8).bool(message.queryInsightsEnabled);
    }
    if (message.recordClientAddress !== false) {
      writer.uint32(16).bool(message.recordClientAddress);
    }
    if (message.recordApplicationTags !== false) {
      writer.uint32(24).bool(message.recordApplicationTags);
    }
    if (message.queryStringLength !== undefined) {
      Int32Value.encode({ value: message.queryStringLength! }, writer.uint32(34).fork()).join();
    }
    if (message.queryPlansPerMinute !== undefined) {
      Int32Value.encode({ value: message.queryPlansPerMinute! }, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InsightsConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInsightsConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.queryInsightsEnabled = reader.bool();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.recordClientAddress = reader.bool();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.recordApplicationTags = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.queryStringLength = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.queryPlansPerMinute = Int32Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InsightsConfig {
    return {
      queryInsightsEnabled: isSet(object.queryInsightsEnabled)
        ? globalThis.Boolean(object.queryInsightsEnabled)
        : false,
      recordClientAddress: isSet(object.recordClientAddress) ? globalThis.Boolean(object.recordClientAddress) : false,
      recordApplicationTags: isSet(object.recordApplicationTags)
        ? globalThis.Boolean(object.recordApplicationTags)
        : false,
      queryStringLength: isSet(object.queryStringLength) ? Number(object.queryStringLength) : undefined,
      queryPlansPerMinute: isSet(object.queryPlansPerMinute) ? Number(object.queryPlansPerMinute) : undefined,
    };
  },

  toJSON(message: InsightsConfig): unknown {
    const obj: any = {};
    if (message.queryInsightsEnabled !== false) {
      obj.queryInsightsEnabled = message.queryInsightsEnabled;
    }
    if (message.recordClientAddress !== false) {
      obj.recordClientAddress = message.recordClientAddress;
    }
    if (message.recordApplicationTags !== false) {
      obj.recordApplicationTags = message.recordApplicationTags;
    }
    if (message.queryStringLength !== undefined) {
      obj.queryStringLength = message.queryStringLength;
    }
    if (message.queryPlansPerMinute !== undefined) {
      obj.queryPlansPerMinute = message.queryPlansPerMinute;
    }
    return obj;
  },

  create(base?: DeepPartial<InsightsConfig>): InsightsConfig {
    return InsightsConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InsightsConfig>): InsightsConfig {
    const message = createBaseInsightsConfig();
    message.queryInsightsEnabled = object.queryInsightsEnabled ?? false;
    message.recordClientAddress = object.recordClientAddress ?? false;
    message.recordApplicationTags = object.recordApplicationTags ?? false;
    message.queryStringLength = object.queryStringLength ?? undefined;
    message.queryPlansPerMinute = object.queryPlansPerMinute ?? undefined;
    return message;
  },
};

function createBaseMySqlReplicaConfiguration(): MySqlReplicaConfiguration {
  return {
    dumpFilePath: "",
    username: "",
    password: "",
    connectRetryInterval: undefined,
    masterHeartbeatPeriod: undefined,
    caCertificate: "",
    clientCertificate: "",
    clientKey: "",
    sslCipher: "",
    verifyServerCertificate: undefined,
    kind: "",
  };
}

export const MySqlReplicaConfiguration: MessageFns<MySqlReplicaConfiguration> = {
  encode(message: MySqlReplicaConfiguration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dumpFilePath !== "") {
      writer.uint32(10).string(message.dumpFilePath);
    }
    if (message.username !== "") {
      writer.uint32(18).string(message.username);
    }
    if (message.password !== "") {
      writer.uint32(26).string(message.password);
    }
    if (message.connectRetryInterval !== undefined) {
      Int32Value.encode({ value: message.connectRetryInterval! }, writer.uint32(34).fork()).join();
    }
    if (message.masterHeartbeatPeriod !== undefined) {
      Int64Value.encode({ value: message.masterHeartbeatPeriod! }, writer.uint32(42).fork()).join();
    }
    if (message.caCertificate !== "") {
      writer.uint32(50).string(message.caCertificate);
    }
    if (message.clientCertificate !== "") {
      writer.uint32(58).string(message.clientCertificate);
    }
    if (message.clientKey !== "") {
      writer.uint32(66).string(message.clientKey);
    }
    if (message.sslCipher !== "") {
      writer.uint32(74).string(message.sslCipher);
    }
    if (message.verifyServerCertificate !== undefined) {
      BoolValue.encode({ value: message.verifyServerCertificate! }, writer.uint32(82).fork()).join();
    }
    if (message.kind !== "") {
      writer.uint32(90).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MySqlReplicaConfiguration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMySqlReplicaConfiguration();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dumpFilePath = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.username = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.password = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.connectRetryInterval = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.masterHeartbeatPeriod = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.caCertificate = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.clientCertificate = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.clientKey = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.sslCipher = reader.string();
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.verifyServerCertificate = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MySqlReplicaConfiguration {
    return {
      dumpFilePath: isSet(object.dumpFilePath) ? globalThis.String(object.dumpFilePath) : "",
      username: isSet(object.username) ? globalThis.String(object.username) : "",
      password: isSet(object.password) ? globalThis.String(object.password) : "",
      connectRetryInterval: isSet(object.connectRetryInterval) ? Number(object.connectRetryInterval) : undefined,
      masterHeartbeatPeriod: isSet(object.masterHeartbeatPeriod)
        ? Long.fromValue(object.masterHeartbeatPeriod)
        : undefined,
      caCertificate: isSet(object.caCertificate) ? globalThis.String(object.caCertificate) : "",
      clientCertificate: isSet(object.clientCertificate) ? globalThis.String(object.clientCertificate) : "",
      clientKey: isSet(object.clientKey) ? globalThis.String(object.clientKey) : "",
      sslCipher: isSet(object.sslCipher) ? globalThis.String(object.sslCipher) : "",
      verifyServerCertificate: isSet(object.verifyServerCertificate)
        ? Boolean(object.verifyServerCertificate)
        : undefined,
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: MySqlReplicaConfiguration): unknown {
    const obj: any = {};
    if (message.dumpFilePath !== "") {
      obj.dumpFilePath = message.dumpFilePath;
    }
    if (message.username !== "") {
      obj.username = message.username;
    }
    if (message.password !== "") {
      obj.password = message.password;
    }
    if (message.connectRetryInterval !== undefined) {
      obj.connectRetryInterval = message.connectRetryInterval;
    }
    if (message.masterHeartbeatPeriod !== undefined) {
      obj.masterHeartbeatPeriod = message.masterHeartbeatPeriod;
    }
    if (message.caCertificate !== "") {
      obj.caCertificate = message.caCertificate;
    }
    if (message.clientCertificate !== "") {
      obj.clientCertificate = message.clientCertificate;
    }
    if (message.clientKey !== "") {
      obj.clientKey = message.clientKey;
    }
    if (message.sslCipher !== "") {
      obj.sslCipher = message.sslCipher;
    }
    if (message.verifyServerCertificate !== undefined) {
      obj.verifyServerCertificate = message.verifyServerCertificate;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<MySqlReplicaConfiguration>): MySqlReplicaConfiguration {
    return MySqlReplicaConfiguration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MySqlReplicaConfiguration>): MySqlReplicaConfiguration {
    const message = createBaseMySqlReplicaConfiguration();
    message.dumpFilePath = object.dumpFilePath ?? "";
    message.username = object.username ?? "";
    message.password = object.password ?? "";
    message.connectRetryInterval = object.connectRetryInterval ?? undefined;
    message.masterHeartbeatPeriod =
      (object.masterHeartbeatPeriod !== undefined && object.masterHeartbeatPeriod !== null)
        ? Long.fromValue(object.masterHeartbeatPeriod)
        : undefined;
    message.caCertificate = object.caCertificate ?? "";
    message.clientCertificate = object.clientCertificate ?? "";
    message.clientKey = object.clientKey ?? "";
    message.sslCipher = object.sslCipher ?? "";
    message.verifyServerCertificate = object.verifyServerCertificate ?? undefined;
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseOnPremisesConfiguration(): OnPremisesConfiguration {
  return {
    hostPort: "",
    kind: "",
    username: "",
    password: "",
    caCertificate: "",
    clientCertificate: "",
    clientKey: "",
    dumpFilePath: "",
    sourceInstance: undefined,
  };
}

export const OnPremisesConfiguration: MessageFns<OnPremisesConfiguration> = {
  encode(message: OnPremisesConfiguration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.hostPort !== "") {
      writer.uint32(10).string(message.hostPort);
    }
    if (message.kind !== "") {
      writer.uint32(18).string(message.kind);
    }
    if (message.username !== "") {
      writer.uint32(26).string(message.username);
    }
    if (message.password !== "") {
      writer.uint32(34).string(message.password);
    }
    if (message.caCertificate !== "") {
      writer.uint32(42).string(message.caCertificate);
    }
    if (message.clientCertificate !== "") {
      writer.uint32(50).string(message.clientCertificate);
    }
    if (message.clientKey !== "") {
      writer.uint32(58).string(message.clientKey);
    }
    if (message.dumpFilePath !== "") {
      writer.uint32(66).string(message.dumpFilePath);
    }
    if (message.sourceInstance !== undefined) {
      InstanceReference.encode(message.sourceInstance, writer.uint32(122).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OnPremisesConfiguration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOnPremisesConfiguration();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.hostPort = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.username = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.password = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.caCertificate = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.clientCertificate = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.clientKey = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.dumpFilePath = reader.string();
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.sourceInstance = InstanceReference.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OnPremisesConfiguration {
    return {
      hostPort: isSet(object.hostPort) ? globalThis.String(object.hostPort) : "",
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      username: isSet(object.username) ? globalThis.String(object.username) : "",
      password: isSet(object.password) ? globalThis.String(object.password) : "",
      caCertificate: isSet(object.caCertificate) ? globalThis.String(object.caCertificate) : "",
      clientCertificate: isSet(object.clientCertificate) ? globalThis.String(object.clientCertificate) : "",
      clientKey: isSet(object.clientKey) ? globalThis.String(object.clientKey) : "",
      dumpFilePath: isSet(object.dumpFilePath) ? globalThis.String(object.dumpFilePath) : "",
      sourceInstance: isSet(object.sourceInstance) ? InstanceReference.fromJSON(object.sourceInstance) : undefined,
    };
  },

  toJSON(message: OnPremisesConfiguration): unknown {
    const obj: any = {};
    if (message.hostPort !== "") {
      obj.hostPort = message.hostPort;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.username !== "") {
      obj.username = message.username;
    }
    if (message.password !== "") {
      obj.password = message.password;
    }
    if (message.caCertificate !== "") {
      obj.caCertificate = message.caCertificate;
    }
    if (message.clientCertificate !== "") {
      obj.clientCertificate = message.clientCertificate;
    }
    if (message.clientKey !== "") {
      obj.clientKey = message.clientKey;
    }
    if (message.dumpFilePath !== "") {
      obj.dumpFilePath = message.dumpFilePath;
    }
    if (message.sourceInstance !== undefined) {
      obj.sourceInstance = InstanceReference.toJSON(message.sourceInstance);
    }
    return obj;
  },

  create(base?: DeepPartial<OnPremisesConfiguration>): OnPremisesConfiguration {
    return OnPremisesConfiguration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OnPremisesConfiguration>): OnPremisesConfiguration {
    const message = createBaseOnPremisesConfiguration();
    message.hostPort = object.hostPort ?? "";
    message.kind = object.kind ?? "";
    message.username = object.username ?? "";
    message.password = object.password ?? "";
    message.caCertificate = object.caCertificate ?? "";
    message.clientCertificate = object.clientCertificate ?? "";
    message.clientKey = object.clientKey ?? "";
    message.dumpFilePath = object.dumpFilePath ?? "";
    message.sourceInstance = (object.sourceInstance !== undefined && object.sourceInstance !== null)
      ? InstanceReference.fromPartial(object.sourceInstance)
      : undefined;
    return message;
  },
};

function createBaseDiskEncryptionConfiguration(): DiskEncryptionConfiguration {
  return { kmsKeyName: "", kind: "" };
}

export const DiskEncryptionConfiguration: MessageFns<DiskEncryptionConfiguration> = {
  encode(message: DiskEncryptionConfiguration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kmsKeyName !== "") {
      writer.uint32(10).string(message.kmsKeyName);
    }
    if (message.kind !== "") {
      writer.uint32(18).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DiskEncryptionConfiguration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDiskEncryptionConfiguration();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kmsKeyName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DiskEncryptionConfiguration {
    return {
      kmsKeyName: isSet(object.kmsKeyName) ? globalThis.String(object.kmsKeyName) : "",
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: DiskEncryptionConfiguration): unknown {
    const obj: any = {};
    if (message.kmsKeyName !== "") {
      obj.kmsKeyName = message.kmsKeyName;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<DiskEncryptionConfiguration>): DiskEncryptionConfiguration {
    return DiskEncryptionConfiguration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DiskEncryptionConfiguration>): DiskEncryptionConfiguration {
    const message = createBaseDiskEncryptionConfiguration();
    message.kmsKeyName = object.kmsKeyName ?? "";
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseDiskEncryptionStatus(): DiskEncryptionStatus {
  return { kmsKeyVersionName: "", kind: "" };
}

export const DiskEncryptionStatus: MessageFns<DiskEncryptionStatus> = {
  encode(message: DiskEncryptionStatus, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kmsKeyVersionName !== "") {
      writer.uint32(10).string(message.kmsKeyVersionName);
    }
    if (message.kind !== "") {
      writer.uint32(18).string(message.kind);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DiskEncryptionStatus {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDiskEncryptionStatus();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kmsKeyVersionName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.kind = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DiskEncryptionStatus {
    return {
      kmsKeyVersionName: isSet(object.kmsKeyVersionName) ? globalThis.String(object.kmsKeyVersionName) : "",
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
    };
  },

  toJSON(message: DiskEncryptionStatus): unknown {
    const obj: any = {};
    if (message.kmsKeyVersionName !== "") {
      obj.kmsKeyVersionName = message.kmsKeyVersionName;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    return obj;
  },

  create(base?: DeepPartial<DiskEncryptionStatus>): DiskEncryptionStatus {
    return DiskEncryptionStatus.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DiskEncryptionStatus>): DiskEncryptionStatus {
    const message = createBaseDiskEncryptionStatus();
    message.kmsKeyVersionName = object.kmsKeyVersionName ?? "";
    message.kind = object.kind ?? "";
    return message;
  },
};

function createBaseOperation(): Operation {
  return {
    kind: "",
    targetLink: "",
    status: 0,
    user: "",
    insertTime: undefined,
    startTime: undefined,
    endTime: undefined,
    error: undefined,
    apiWarning: undefined,
    operationType: 0,
    importContext: undefined,
    exportContext: undefined,
    backupContext: undefined,
    name: "",
    targetId: "",
    selfLink: "",
    targetProject: "",
    acquireSsrsLeaseContext: undefined,
  };
}

export const Operation: MessageFns<Operation> = {
  encode(message: Operation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.targetLink !== "") {
      writer.uint32(18).string(message.targetLink);
    }
    if (message.status !== 0) {
      writer.uint32(24).int32(message.status);
    }
    if (message.user !== "") {
      writer.uint32(34).string(message.user);
    }
    if (message.insertTime !== undefined) {
      Timestamp.encode(toTimestamp(message.insertTime), writer.uint32(42).fork()).join();
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(50).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(58).fork()).join();
    }
    if (message.error !== undefined) {
      OperationErrors.encode(message.error, writer.uint32(66).fork()).join();
    }
    if (message.apiWarning !== undefined) {
      ApiWarning.encode(message.apiWarning, writer.uint32(154).fork()).join();
    }
    if (message.operationType !== 0) {
      writer.uint32(72).int32(message.operationType);
    }
    if (message.importContext !== undefined) {
      ImportContext.encode(message.importContext, writer.uint32(82).fork()).join();
    }
    if (message.exportContext !== undefined) {
      ExportContext.encode(message.exportContext, writer.uint32(90).fork()).join();
    }
    if (message.backupContext !== undefined) {
      BackupContext.encode(message.backupContext, writer.uint32(138).fork()).join();
    }
    if (message.name !== "") {
      writer.uint32(98).string(message.name);
    }
    if (message.targetId !== "") {
      writer.uint32(106).string(message.targetId);
    }
    if (message.selfLink !== "") {
      writer.uint32(114).string(message.selfLink);
    }
    if (message.targetProject !== "") {
      writer.uint32(122).string(message.targetProject);
    }
    if (message.acquireSsrsLeaseContext !== undefined) {
      AcquireSsrsLeaseContext.encode(message.acquireSsrsLeaseContext, writer.uint32(162).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Operation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOperation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.targetLink = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.status = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.user = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.insertTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.error = OperationErrors.decode(reader, reader.uint32());
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.apiWarning = ApiWarning.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.operationType = reader.int32() as any;
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.importContext = ImportContext.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.exportContext = ExportContext.decode(reader, reader.uint32());
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.backupContext = BackupContext.decode(reader, reader.uint32());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.name = reader.string();
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.targetId = reader.string();
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.selfLink = reader.string();
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.targetProject = reader.string();
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.acquireSsrsLeaseContext = AcquireSsrsLeaseContext.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Operation {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      targetLink: isSet(object.targetLink) ? globalThis.String(object.targetLink) : "",
      status: isSet(object.status) ? operation_SqlOperationStatusFromJSON(object.status) : 0,
      user: isSet(object.user) ? globalThis.String(object.user) : "",
      insertTime: isSet(object.insertTime) ? fromJsonTimestamp(object.insertTime) : undefined,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      error: isSet(object.error) ? OperationErrors.fromJSON(object.error) : undefined,
      apiWarning: isSet(object.apiWarning) ? ApiWarning.fromJSON(object.apiWarning) : undefined,
      operationType: isSet(object.operationType) ? operation_SqlOperationTypeFromJSON(object.operationType) : 0,
      importContext: isSet(object.importContext) ? ImportContext.fromJSON(object.importContext) : undefined,
      exportContext: isSet(object.exportContext) ? ExportContext.fromJSON(object.exportContext) : undefined,
      backupContext: isSet(object.backupContext) ? BackupContext.fromJSON(object.backupContext) : undefined,
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      targetId: isSet(object.targetId) ? globalThis.String(object.targetId) : "",
      selfLink: isSet(object.selfLink) ? globalThis.String(object.selfLink) : "",
      targetProject: isSet(object.targetProject) ? globalThis.String(object.targetProject) : "",
      acquireSsrsLeaseContext: isSet(object.acquireSsrsLeaseContext)
        ? AcquireSsrsLeaseContext.fromJSON(object.acquireSsrsLeaseContext)
        : undefined,
    };
  },

  toJSON(message: Operation): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.targetLink !== "") {
      obj.targetLink = message.targetLink;
    }
    if (message.status !== 0) {
      obj.status = operation_SqlOperationStatusToJSON(message.status);
    }
    if (message.user !== "") {
      obj.user = message.user;
    }
    if (message.insertTime !== undefined) {
      obj.insertTime = message.insertTime.toISOString();
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (message.error !== undefined) {
      obj.error = OperationErrors.toJSON(message.error);
    }
    if (message.apiWarning !== undefined) {
      obj.apiWarning = ApiWarning.toJSON(message.apiWarning);
    }
    if (message.operationType !== 0) {
      obj.operationType = operation_SqlOperationTypeToJSON(message.operationType);
    }
    if (message.importContext !== undefined) {
      obj.importContext = ImportContext.toJSON(message.importContext);
    }
    if (message.exportContext !== undefined) {
      obj.exportContext = ExportContext.toJSON(message.exportContext);
    }
    if (message.backupContext !== undefined) {
      obj.backupContext = BackupContext.toJSON(message.backupContext);
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.targetId !== "") {
      obj.targetId = message.targetId;
    }
    if (message.selfLink !== "") {
      obj.selfLink = message.selfLink;
    }
    if (message.targetProject !== "") {
      obj.targetProject = message.targetProject;
    }
    if (message.acquireSsrsLeaseContext !== undefined) {
      obj.acquireSsrsLeaseContext = AcquireSsrsLeaseContext.toJSON(message.acquireSsrsLeaseContext);
    }
    return obj;
  },

  create(base?: DeepPartial<Operation>): Operation {
    return Operation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Operation>): Operation {
    const message = createBaseOperation();
    message.kind = object.kind ?? "";
    message.targetLink = object.targetLink ?? "";
    message.status = object.status ?? 0;
    message.user = object.user ?? "";
    message.insertTime = object.insertTime ?? undefined;
    message.startTime = object.startTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    message.error = (object.error !== undefined && object.error !== null)
      ? OperationErrors.fromPartial(object.error)
      : undefined;
    message.apiWarning = (object.apiWarning !== undefined && object.apiWarning !== null)
      ? ApiWarning.fromPartial(object.apiWarning)
      : undefined;
    message.operationType = object.operationType ?? 0;
    message.importContext = (object.importContext !== undefined && object.importContext !== null)
      ? ImportContext.fromPartial(object.importContext)
      : undefined;
    message.exportContext = (object.exportContext !== undefined && object.exportContext !== null)
      ? ExportContext.fromPartial(object.exportContext)
      : undefined;
    message.backupContext = (object.backupContext !== undefined && object.backupContext !== null)
      ? BackupContext.fromPartial(object.backupContext)
      : undefined;
    message.name = object.name ?? "";
    message.targetId = object.targetId ?? "";
    message.selfLink = object.selfLink ?? "";
    message.targetProject = object.targetProject ?? "";
    message.acquireSsrsLeaseContext =
      (object.acquireSsrsLeaseContext !== undefined && object.acquireSsrsLeaseContext !== null)
        ? AcquireSsrsLeaseContext.fromPartial(object.acquireSsrsLeaseContext)
        : undefined;
    return message;
  },
};

function createBaseOperationError(): OperationError {
  return { kind: "", code: "", message: "" };
}

export const OperationError: MessageFns<OperationError> = {
  encode(message: OperationError, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.code !== "") {
      writer.uint32(18).string(message.code);
    }
    if (message.message !== "") {
      writer.uint32(26).string(message.message);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OperationError {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOperationError();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.code = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.message = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OperationError {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      code: isSet(object.code) ? globalThis.String(object.code) : "",
      message: isSet(object.message) ? globalThis.String(object.message) : "",
    };
  },

  toJSON(message: OperationError): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.code !== "") {
      obj.code = message.code;
    }
    if (message.message !== "") {
      obj.message = message.message;
    }
    return obj;
  },

  create(base?: DeepPartial<OperationError>): OperationError {
    return OperationError.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OperationError>): OperationError {
    const message = createBaseOperationError();
    message.kind = object.kind ?? "";
    message.code = object.code ?? "";
    message.message = object.message ?? "";
    return message;
  },
};

function createBaseOperationErrors(): OperationErrors {
  return { kind: "", errors: [] };
}

export const OperationErrors: MessageFns<OperationErrors> = {
  encode(message: OperationErrors, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    for (const v of message.errors) {
      OperationError.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OperationErrors {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOperationErrors();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.errors.push(OperationError.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OperationErrors {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      errors: globalThis.Array.isArray(object?.errors) ? object.errors.map((e: any) => OperationError.fromJSON(e)) : [],
    };
  },

  toJSON(message: OperationErrors): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.errors?.length) {
      obj.errors = message.errors.map((e) => OperationError.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<OperationErrors>): OperationErrors {
    return OperationErrors.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OperationErrors>): OperationErrors {
    const message = createBaseOperationErrors();
    message.kind = object.kind ?? "";
    message.errors = object.errors?.map((e) => OperationError.fromPartial(e)) || [];
    return message;
  },
};

function createBasePasswordValidationPolicy(): PasswordValidationPolicy {
  return {
    minLength: undefined,
    complexity: 0,
    reuseInterval: undefined,
    disallowUsernameSubstring: undefined,
    passwordChangeInterval: undefined,
    enablePasswordPolicy: undefined,
    disallowCompromisedCredentials: undefined,
  };
}

export const PasswordValidationPolicy: MessageFns<PasswordValidationPolicy> = {
  encode(message: PasswordValidationPolicy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.minLength !== undefined) {
      Int32Value.encode({ value: message.minLength! }, writer.uint32(10).fork()).join();
    }
    if (message.complexity !== 0) {
      writer.uint32(16).int32(message.complexity);
    }
    if (message.reuseInterval !== undefined) {
      Int32Value.encode({ value: message.reuseInterval! }, writer.uint32(26).fork()).join();
    }
    if (message.disallowUsernameSubstring !== undefined) {
      BoolValue.encode({ value: message.disallowUsernameSubstring! }, writer.uint32(34).fork()).join();
    }
    if (message.passwordChangeInterval !== undefined) {
      Duration.encode(message.passwordChangeInterval, writer.uint32(42).fork()).join();
    }
    if (message.enablePasswordPolicy !== undefined) {
      BoolValue.encode({ value: message.enablePasswordPolicy! }, writer.uint32(50).fork()).join();
    }
    if (message.disallowCompromisedCredentials !== undefined) {
      BoolValue.encode({ value: message.disallowCompromisedCredentials! }, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PasswordValidationPolicy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePasswordValidationPolicy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.minLength = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.complexity = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.reuseInterval = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.disallowUsernameSubstring = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.passwordChangeInterval = Duration.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.enablePasswordPolicy = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.disallowCompromisedCredentials = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PasswordValidationPolicy {
    return {
      minLength: isSet(object.minLength) ? Number(object.minLength) : undefined,
      complexity: isSet(object.complexity) ? passwordValidationPolicy_ComplexityFromJSON(object.complexity) : 0,
      reuseInterval: isSet(object.reuseInterval) ? Number(object.reuseInterval) : undefined,
      disallowUsernameSubstring: isSet(object.disallowUsernameSubstring)
        ? Boolean(object.disallowUsernameSubstring)
        : undefined,
      passwordChangeInterval: isSet(object.passwordChangeInterval)
        ? Duration.fromJSON(object.passwordChangeInterval)
        : undefined,
      enablePasswordPolicy: isSet(object.enablePasswordPolicy) ? Boolean(object.enablePasswordPolicy) : undefined,
      disallowCompromisedCredentials: isSet(object.disallowCompromisedCredentials)
        ? Boolean(object.disallowCompromisedCredentials)
        : undefined,
    };
  },

  toJSON(message: PasswordValidationPolicy): unknown {
    const obj: any = {};
    if (message.minLength !== undefined) {
      obj.minLength = message.minLength;
    }
    if (message.complexity !== 0) {
      obj.complexity = passwordValidationPolicy_ComplexityToJSON(message.complexity);
    }
    if (message.reuseInterval !== undefined) {
      obj.reuseInterval = message.reuseInterval;
    }
    if (message.disallowUsernameSubstring !== undefined) {
      obj.disallowUsernameSubstring = message.disallowUsernameSubstring;
    }
    if (message.passwordChangeInterval !== undefined) {
      obj.passwordChangeInterval = Duration.toJSON(message.passwordChangeInterval);
    }
    if (message.enablePasswordPolicy !== undefined) {
      obj.enablePasswordPolicy = message.enablePasswordPolicy;
    }
    if (message.disallowCompromisedCredentials !== undefined) {
      obj.disallowCompromisedCredentials = message.disallowCompromisedCredentials;
    }
    return obj;
  },

  create(base?: DeepPartial<PasswordValidationPolicy>): PasswordValidationPolicy {
    return PasswordValidationPolicy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PasswordValidationPolicy>): PasswordValidationPolicy {
    const message = createBasePasswordValidationPolicy();
    message.minLength = object.minLength ?? undefined;
    message.complexity = object.complexity ?? 0;
    message.reuseInterval = object.reuseInterval ?? undefined;
    message.disallowUsernameSubstring = object.disallowUsernameSubstring ?? undefined;
    message.passwordChangeInterval =
      (object.passwordChangeInterval !== undefined && object.passwordChangeInterval !== null)
        ? Duration.fromPartial(object.passwordChangeInterval)
        : undefined;
    message.enablePasswordPolicy = object.enablePasswordPolicy ?? undefined;
    message.disallowCompromisedCredentials = object.disallowCompromisedCredentials ?? undefined;
    return message;
  },
};

function createBaseOperationsListResponse(): OperationsListResponse {
  return { kind: "", items: [], nextPageToken: "" };
}

export const OperationsListResponse: MessageFns<OperationsListResponse> = {
  encode(message: OperationsListResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    for (const v of message.items) {
      Operation.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(26).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OperationsListResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOperationsListResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.items.push(Operation.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OperationsListResponse {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      items: globalThis.Array.isArray(object?.items) ? object.items.map((e: any) => Operation.fromJSON(e)) : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: OperationsListResponse): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.items?.length) {
      obj.items = message.items.map((e) => Operation.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<OperationsListResponse>): OperationsListResponse {
    return OperationsListResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OperationsListResponse>): OperationsListResponse {
    const message = createBaseOperationsListResponse();
    message.kind = object.kind ?? "";
    message.items = object.items?.map((e) => Operation.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBaseReplicaConfiguration(): ReplicaConfiguration {
  return { kind: "", mysqlReplicaConfiguration: undefined, failoverTarget: undefined, cascadableReplica: undefined };
}

export const ReplicaConfiguration: MessageFns<ReplicaConfiguration> = {
  encode(message: ReplicaConfiguration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.mysqlReplicaConfiguration !== undefined) {
      MySqlReplicaConfiguration.encode(message.mysqlReplicaConfiguration, writer.uint32(18).fork()).join();
    }
    if (message.failoverTarget !== undefined) {
      BoolValue.encode({ value: message.failoverTarget! }, writer.uint32(26).fork()).join();
    }
    if (message.cascadableReplica !== undefined) {
      BoolValue.encode({ value: message.cascadableReplica! }, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReplicaConfiguration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReplicaConfiguration();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.mysqlReplicaConfiguration = MySqlReplicaConfiguration.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.failoverTarget = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.cascadableReplica = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReplicaConfiguration {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      mysqlReplicaConfiguration: isSet(object.mysqlReplicaConfiguration)
        ? MySqlReplicaConfiguration.fromJSON(object.mysqlReplicaConfiguration)
        : undefined,
      failoverTarget: isSet(object.failoverTarget) ? Boolean(object.failoverTarget) : undefined,
      cascadableReplica: isSet(object.cascadableReplica) ? Boolean(object.cascadableReplica) : undefined,
    };
  },

  toJSON(message: ReplicaConfiguration): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.mysqlReplicaConfiguration !== undefined) {
      obj.mysqlReplicaConfiguration = MySqlReplicaConfiguration.toJSON(message.mysqlReplicaConfiguration);
    }
    if (message.failoverTarget !== undefined) {
      obj.failoverTarget = message.failoverTarget;
    }
    if (message.cascadableReplica !== undefined) {
      obj.cascadableReplica = message.cascadableReplica;
    }
    return obj;
  },

  create(base?: DeepPartial<ReplicaConfiguration>): ReplicaConfiguration {
    return ReplicaConfiguration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReplicaConfiguration>): ReplicaConfiguration {
    const message = createBaseReplicaConfiguration();
    message.kind = object.kind ?? "";
    message.mysqlReplicaConfiguration =
      (object.mysqlReplicaConfiguration !== undefined && object.mysqlReplicaConfiguration !== null)
        ? MySqlReplicaConfiguration.fromPartial(object.mysqlReplicaConfiguration)
        : undefined;
    message.failoverTarget = object.failoverTarget ?? undefined;
    message.cascadableReplica = object.cascadableReplica ?? undefined;
    return message;
  },
};

function createBaseRestoreBackupContext(): RestoreBackupContext {
  return { kind: "", backupRunId: Long.ZERO, instanceId: "", project: "" };
}

export const RestoreBackupContext: MessageFns<RestoreBackupContext> = {
  encode(message: RestoreBackupContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (!message.backupRunId.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.backupRunId.toString());
    }
    if (message.instanceId !== "") {
      writer.uint32(26).string(message.instanceId);
    }
    if (message.project !== "") {
      writer.uint32(34).string(message.project);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RestoreBackupContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRestoreBackupContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.backupRunId = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.instanceId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.project = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RestoreBackupContext {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      backupRunId: isSet(object.backupRunId) ? Long.fromValue(object.backupRunId) : Long.ZERO,
      instanceId: isSet(object.instanceId) ? globalThis.String(object.instanceId) : "",
      project: isSet(object.project) ? globalThis.String(object.project) : "",
    };
  },

  toJSON(message: RestoreBackupContext): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (!message.backupRunId.equals(Long.ZERO)) {
      obj.backupRunId = (message.backupRunId || Long.ZERO).toString();
    }
    if (message.instanceId !== "") {
      obj.instanceId = message.instanceId;
    }
    if (message.project !== "") {
      obj.project = message.project;
    }
    return obj;
  },

  create(base?: DeepPartial<RestoreBackupContext>): RestoreBackupContext {
    return RestoreBackupContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RestoreBackupContext>): RestoreBackupContext {
    const message = createBaseRestoreBackupContext();
    message.kind = object.kind ?? "";
    message.backupRunId = (object.backupRunId !== undefined && object.backupRunId !== null)
      ? Long.fromValue(object.backupRunId)
      : Long.ZERO;
    message.instanceId = object.instanceId ?? "";
    message.project = object.project ?? "";
    return message;
  },
};

function createBaseRotateServerCaContext(): RotateServerCaContext {
  return { kind: "", nextVersion: "" };
}

export const RotateServerCaContext: MessageFns<RotateServerCaContext> = {
  encode(message: RotateServerCaContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.nextVersion !== "") {
      writer.uint32(18).string(message.nextVersion);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RotateServerCaContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRotateServerCaContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextVersion = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RotateServerCaContext {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      nextVersion: isSet(object.nextVersion) ? globalThis.String(object.nextVersion) : "",
    };
  },

  toJSON(message: RotateServerCaContext): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.nextVersion !== "") {
      obj.nextVersion = message.nextVersion;
    }
    return obj;
  },

  create(base?: DeepPartial<RotateServerCaContext>): RotateServerCaContext {
    return RotateServerCaContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RotateServerCaContext>): RotateServerCaContext {
    const message = createBaseRotateServerCaContext();
    message.kind = object.kind ?? "";
    message.nextVersion = object.nextVersion ?? "";
    return message;
  },
};

function createBaseDataCacheConfig(): DataCacheConfig {
  return { dataCacheEnabled: false };
}

export const DataCacheConfig: MessageFns<DataCacheConfig> = {
  encode(message: DataCacheConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dataCacheEnabled !== false) {
      writer.uint32(8).bool(message.dataCacheEnabled);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DataCacheConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDataCacheConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.dataCacheEnabled = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DataCacheConfig {
    return { dataCacheEnabled: isSet(object.dataCacheEnabled) ? globalThis.Boolean(object.dataCacheEnabled) : false };
  },

  toJSON(message: DataCacheConfig): unknown {
    const obj: any = {};
    if (message.dataCacheEnabled !== false) {
      obj.dataCacheEnabled = message.dataCacheEnabled;
    }
    return obj;
  },

  create(base?: DeepPartial<DataCacheConfig>): DataCacheConfig {
    return DataCacheConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DataCacheConfig>): DataCacheConfig {
    const message = createBaseDataCacheConfig();
    message.dataCacheEnabled = object.dataCacheEnabled ?? false;
    return message;
  },
};

function createBaseSettings(): Settings {
  return {
    settingsVersion: undefined,
    authorizedGaeApplications: [],
    tier: "",
    kind: "",
    userLabels: {},
    availabilityType: 0,
    pricingPlan: 0,
    replicationType: 0,
    storageAutoResizeLimit: undefined,
    activationPolicy: 0,
    ipConfiguration: undefined,
    storageAutoResize: undefined,
    locationPreference: undefined,
    databaseFlags: [],
    dataDiskType: 0,
    maintenanceWindow: undefined,
    backupConfiguration: undefined,
    databaseReplicationEnabled: undefined,
    crashSafeReplicationEnabled: undefined,
    dataDiskSizeGb: undefined,
    activeDirectoryConfig: undefined,
    collation: "",
    denyMaintenancePeriods: [],
    insightsConfig: undefined,
    passwordValidationPolicy: undefined,
    sqlServerAuditConfig: undefined,
    edition: 0,
    connectorEnforcement: 0,
    deletionProtectionEnabled: undefined,
    timeZone: "",
    advancedMachineFeatures: undefined,
    dataCacheConfig: undefined,
    enableGoogleMlIntegration: undefined,
    enableDataplexIntegration: undefined,
  };
}

export const Settings: MessageFns<Settings> = {
  encode(message: Settings, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.settingsVersion !== undefined) {
      Int64Value.encode({ value: message.settingsVersion! }, writer.uint32(10).fork()).join();
    }
    for (const v of message.authorizedGaeApplications) {
      writer.uint32(18).string(v!);
    }
    if (message.tier !== "") {
      writer.uint32(26).string(message.tier);
    }
    if (message.kind !== "") {
      writer.uint32(34).string(message.kind);
    }
    Object.entries(message.userLabels).forEach(([key, value]) => {
      Settings_UserLabelsEntry.encode({ key: key as any, value }, writer.uint32(42).fork()).join();
    });
    if (message.availabilityType !== 0) {
      writer.uint32(48).int32(message.availabilityType);
    }
    if (message.pricingPlan !== 0) {
      writer.uint32(56).int32(message.pricingPlan);
    }
    if (message.replicationType !== 0) {
      writer.uint32(64).int32(message.replicationType);
    }
    if (message.storageAutoResizeLimit !== undefined) {
      Int64Value.encode({ value: message.storageAutoResizeLimit! }, writer.uint32(74).fork()).join();
    }
    if (message.activationPolicy !== 0) {
      writer.uint32(80).int32(message.activationPolicy);
    }
    if (message.ipConfiguration !== undefined) {
      IpConfiguration.encode(message.ipConfiguration, writer.uint32(90).fork()).join();
    }
    if (message.storageAutoResize !== undefined) {
      BoolValue.encode({ value: message.storageAutoResize! }, writer.uint32(98).fork()).join();
    }
    if (message.locationPreference !== undefined) {
      LocationPreference.encode(message.locationPreference, writer.uint32(106).fork()).join();
    }
    for (const v of message.databaseFlags) {
      DatabaseFlags.encode(v!, writer.uint32(114).fork()).join();
    }
    if (message.dataDiskType !== 0) {
      writer.uint32(120).int32(message.dataDiskType);
    }
    if (message.maintenanceWindow !== undefined) {
      MaintenanceWindow.encode(message.maintenanceWindow, writer.uint32(130).fork()).join();
    }
    if (message.backupConfiguration !== undefined) {
      BackupConfiguration.encode(message.backupConfiguration, writer.uint32(138).fork()).join();
    }
    if (message.databaseReplicationEnabled !== undefined) {
      BoolValue.encode({ value: message.databaseReplicationEnabled! }, writer.uint32(146).fork()).join();
    }
    if (message.crashSafeReplicationEnabled !== undefined) {
      BoolValue.encode({ value: message.crashSafeReplicationEnabled! }, writer.uint32(154).fork()).join();
    }
    if (message.dataDiskSizeGb !== undefined) {
      Int64Value.encode({ value: message.dataDiskSizeGb! }, writer.uint32(162).fork()).join();
    }
    if (message.activeDirectoryConfig !== undefined) {
      SqlActiveDirectoryConfig.encode(message.activeDirectoryConfig, writer.uint32(178).fork()).join();
    }
    if (message.collation !== "") {
      writer.uint32(186).string(message.collation);
    }
    for (const v of message.denyMaintenancePeriods) {
      DenyMaintenancePeriod.encode(v!, writer.uint32(194).fork()).join();
    }
    if (message.insightsConfig !== undefined) {
      InsightsConfig.encode(message.insightsConfig, writer.uint32(202).fork()).join();
    }
    if (message.passwordValidationPolicy !== undefined) {
      PasswordValidationPolicy.encode(message.passwordValidationPolicy, writer.uint32(218).fork()).join();
    }
    if (message.sqlServerAuditConfig !== undefined) {
      SqlServerAuditConfig.encode(message.sqlServerAuditConfig, writer.uint32(234).fork()).join();
    }
    if (message.edition !== 0) {
      writer.uint32(304).int32(message.edition);
    }
    if (message.connectorEnforcement !== 0) {
      writer.uint32(256).int32(message.connectorEnforcement);
    }
    if (message.deletionProtectionEnabled !== undefined) {
      BoolValue.encode({ value: message.deletionProtectionEnabled! }, writer.uint32(266).fork()).join();
    }
    if (message.timeZone !== "") {
      writer.uint32(274).string(message.timeZone);
    }
    if (message.advancedMachineFeatures !== undefined) {
      AdvancedMachineFeatures.encode(message.advancedMachineFeatures, writer.uint32(282).fork()).join();
    }
    if (message.dataCacheConfig !== undefined) {
      DataCacheConfig.encode(message.dataCacheConfig, writer.uint32(298).fork()).join();
    }
    if (message.enableGoogleMlIntegration !== undefined) {
      BoolValue.encode({ value: message.enableGoogleMlIntegration! }, writer.uint32(322).fork()).join();
    }
    if (message.enableDataplexIntegration !== undefined) {
      BoolValue.encode({ value: message.enableDataplexIntegration! }, writer.uint32(330).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Settings {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSettings();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.settingsVersion = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.authorizedGaeApplications.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.tier = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          const entry5 = Settings_UserLabelsEntry.decode(reader, reader.uint32());
          if (entry5.value !== undefined) {
            message.userLabels[entry5.key] = entry5.value;
          }
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.availabilityType = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.pricingPlan = reader.int32() as any;
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.replicationType = reader.int32() as any;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.storageAutoResizeLimit = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.activationPolicy = reader.int32() as any;
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.ipConfiguration = IpConfiguration.decode(reader, reader.uint32());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.storageAutoResize = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.locationPreference = LocationPreference.decode(reader, reader.uint32());
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.databaseFlags.push(DatabaseFlags.decode(reader, reader.uint32()));
          continue;
        case 15:
          if (tag !== 120) {
            break;
          }

          message.dataDiskType = reader.int32() as any;
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.maintenanceWindow = MaintenanceWindow.decode(reader, reader.uint32());
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.backupConfiguration = BackupConfiguration.decode(reader, reader.uint32());
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.databaseReplicationEnabled = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.crashSafeReplicationEnabled = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.dataDiskSizeGb = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 22:
          if (tag !== 178) {
            break;
          }

          message.activeDirectoryConfig = SqlActiveDirectoryConfig.decode(reader, reader.uint32());
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.collation = reader.string();
          continue;
        case 24:
          if (tag !== 194) {
            break;
          }

          message.denyMaintenancePeriods.push(DenyMaintenancePeriod.decode(reader, reader.uint32()));
          continue;
        case 25:
          if (tag !== 202) {
            break;
          }

          message.insightsConfig = InsightsConfig.decode(reader, reader.uint32());
          continue;
        case 27:
          if (tag !== 218) {
            break;
          }

          message.passwordValidationPolicy = PasswordValidationPolicy.decode(reader, reader.uint32());
          continue;
        case 29:
          if (tag !== 234) {
            break;
          }

          message.sqlServerAuditConfig = SqlServerAuditConfig.decode(reader, reader.uint32());
          continue;
        case 38:
          if (tag !== 304) {
            break;
          }

          message.edition = reader.int32() as any;
          continue;
        case 32:
          if (tag !== 256) {
            break;
          }

          message.connectorEnforcement = reader.int32() as any;
          continue;
        case 33:
          if (tag !== 266) {
            break;
          }

          message.deletionProtectionEnabled = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 34:
          if (tag !== 274) {
            break;
          }

          message.timeZone = reader.string();
          continue;
        case 35:
          if (tag !== 282) {
            break;
          }

          message.advancedMachineFeatures = AdvancedMachineFeatures.decode(reader, reader.uint32());
          continue;
        case 37:
          if (tag !== 298) {
            break;
          }

          message.dataCacheConfig = DataCacheConfig.decode(reader, reader.uint32());
          continue;
        case 40:
          if (tag !== 322) {
            break;
          }

          message.enableGoogleMlIntegration = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 41:
          if (tag !== 330) {
            break;
          }

          message.enableDataplexIntegration = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Settings {
    return {
      settingsVersion: isSet(object.settingsVersion) ? Long.fromValue(object.settingsVersion) : undefined,
      authorizedGaeApplications: globalThis.Array.isArray(object?.authorizedGaeApplications)
        ? object.authorizedGaeApplications.map((e: any) => globalThis.String(e))
        : [],
      tier: isSet(object.tier) ? globalThis.String(object.tier) : "",
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      userLabels: isObject(object.userLabels)
        ? Object.entries(object.userLabels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      availabilityType: isSet(object.availabilityType) ? sqlAvailabilityTypeFromJSON(object.availabilityType) : 0,
      pricingPlan: isSet(object.pricingPlan) ? sqlPricingPlanFromJSON(object.pricingPlan) : 0,
      replicationType: isSet(object.replicationType) ? sqlReplicationTypeFromJSON(object.replicationType) : 0,
      storageAutoResizeLimit: isSet(object.storageAutoResizeLimit)
        ? Long.fromValue(object.storageAutoResizeLimit)
        : undefined,
      activationPolicy: isSet(object.activationPolicy)
        ? settings_SqlActivationPolicyFromJSON(object.activationPolicy)
        : 0,
      ipConfiguration: isSet(object.ipConfiguration) ? IpConfiguration.fromJSON(object.ipConfiguration) : undefined,
      storageAutoResize: isSet(object.storageAutoResize) ? Boolean(object.storageAutoResize) : undefined,
      locationPreference: isSet(object.locationPreference)
        ? LocationPreference.fromJSON(object.locationPreference)
        : undefined,
      databaseFlags: globalThis.Array.isArray(object?.databaseFlags)
        ? object.databaseFlags.map((e: any) => DatabaseFlags.fromJSON(e))
        : [],
      dataDiskType: isSet(object.dataDiskType) ? sqlDataDiskTypeFromJSON(object.dataDiskType) : 0,
      maintenanceWindow: isSet(object.maintenanceWindow)
        ? MaintenanceWindow.fromJSON(object.maintenanceWindow)
        : undefined,
      backupConfiguration: isSet(object.backupConfiguration)
        ? BackupConfiguration.fromJSON(object.backupConfiguration)
        : undefined,
      databaseReplicationEnabled: isSet(object.databaseReplicationEnabled)
        ? Boolean(object.databaseReplicationEnabled)
        : undefined,
      crashSafeReplicationEnabled: isSet(object.crashSafeReplicationEnabled)
        ? Boolean(object.crashSafeReplicationEnabled)
        : undefined,
      dataDiskSizeGb: isSet(object.dataDiskSizeGb) ? Long.fromValue(object.dataDiskSizeGb) : undefined,
      activeDirectoryConfig: isSet(object.activeDirectoryConfig)
        ? SqlActiveDirectoryConfig.fromJSON(object.activeDirectoryConfig)
        : undefined,
      collation: isSet(object.collation) ? globalThis.String(object.collation) : "",
      denyMaintenancePeriods: globalThis.Array.isArray(object?.denyMaintenancePeriods)
        ? object.denyMaintenancePeriods.map((e: any) => DenyMaintenancePeriod.fromJSON(e))
        : [],
      insightsConfig: isSet(object.insightsConfig) ? InsightsConfig.fromJSON(object.insightsConfig) : undefined,
      passwordValidationPolicy: isSet(object.passwordValidationPolicy)
        ? PasswordValidationPolicy.fromJSON(object.passwordValidationPolicy)
        : undefined,
      sqlServerAuditConfig: isSet(object.sqlServerAuditConfig)
        ? SqlServerAuditConfig.fromJSON(object.sqlServerAuditConfig)
        : undefined,
      edition: isSet(object.edition) ? settings_EditionFromJSON(object.edition) : 0,
      connectorEnforcement: isSet(object.connectorEnforcement)
        ? settings_ConnectorEnforcementFromJSON(object.connectorEnforcement)
        : 0,
      deletionProtectionEnabled: isSet(object.deletionProtectionEnabled)
        ? Boolean(object.deletionProtectionEnabled)
        : undefined,
      timeZone: isSet(object.timeZone) ? globalThis.String(object.timeZone) : "",
      advancedMachineFeatures: isSet(object.advancedMachineFeatures)
        ? AdvancedMachineFeatures.fromJSON(object.advancedMachineFeatures)
        : undefined,
      dataCacheConfig: isSet(object.dataCacheConfig) ? DataCacheConfig.fromJSON(object.dataCacheConfig) : undefined,
      enableGoogleMlIntegration: isSet(object.enableGoogleMlIntegration)
        ? Boolean(object.enableGoogleMlIntegration)
        : undefined,
      enableDataplexIntegration: isSet(object.enableDataplexIntegration)
        ? Boolean(object.enableDataplexIntegration)
        : undefined,
    };
  },

  toJSON(message: Settings): unknown {
    const obj: any = {};
    if (message.settingsVersion !== undefined) {
      obj.settingsVersion = message.settingsVersion;
    }
    if (message.authorizedGaeApplications?.length) {
      obj.authorizedGaeApplications = message.authorizedGaeApplications;
    }
    if (message.tier !== "") {
      obj.tier = message.tier;
    }
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.userLabels) {
      const entries = Object.entries(message.userLabels);
      if (entries.length > 0) {
        obj.userLabels = {};
        entries.forEach(([k, v]) => {
          obj.userLabels[k] = v;
        });
      }
    }
    if (message.availabilityType !== 0) {
      obj.availabilityType = sqlAvailabilityTypeToJSON(message.availabilityType);
    }
    if (message.pricingPlan !== 0) {
      obj.pricingPlan = sqlPricingPlanToJSON(message.pricingPlan);
    }
    if (message.replicationType !== 0) {
      obj.replicationType = sqlReplicationTypeToJSON(message.replicationType);
    }
    if (message.storageAutoResizeLimit !== undefined) {
      obj.storageAutoResizeLimit = message.storageAutoResizeLimit;
    }
    if (message.activationPolicy !== 0) {
      obj.activationPolicy = settings_SqlActivationPolicyToJSON(message.activationPolicy);
    }
    if (message.ipConfiguration !== undefined) {
      obj.ipConfiguration = IpConfiguration.toJSON(message.ipConfiguration);
    }
    if (message.storageAutoResize !== undefined) {
      obj.storageAutoResize = message.storageAutoResize;
    }
    if (message.locationPreference !== undefined) {
      obj.locationPreference = LocationPreference.toJSON(message.locationPreference);
    }
    if (message.databaseFlags?.length) {
      obj.databaseFlags = message.databaseFlags.map((e) => DatabaseFlags.toJSON(e));
    }
    if (message.dataDiskType !== 0) {
      obj.dataDiskType = sqlDataDiskTypeToJSON(message.dataDiskType);
    }
    if (message.maintenanceWindow !== undefined) {
      obj.maintenanceWindow = MaintenanceWindow.toJSON(message.maintenanceWindow);
    }
    if (message.backupConfiguration !== undefined) {
      obj.backupConfiguration = BackupConfiguration.toJSON(message.backupConfiguration);
    }
    if (message.databaseReplicationEnabled !== undefined) {
      obj.databaseReplicationEnabled = message.databaseReplicationEnabled;
    }
    if (message.crashSafeReplicationEnabled !== undefined) {
      obj.crashSafeReplicationEnabled = message.crashSafeReplicationEnabled;
    }
    if (message.dataDiskSizeGb !== undefined) {
      obj.dataDiskSizeGb = message.dataDiskSizeGb;
    }
    if (message.activeDirectoryConfig !== undefined) {
      obj.activeDirectoryConfig = SqlActiveDirectoryConfig.toJSON(message.activeDirectoryConfig);
    }
    if (message.collation !== "") {
      obj.collation = message.collation;
    }
    if (message.denyMaintenancePeriods?.length) {
      obj.denyMaintenancePeriods = message.denyMaintenancePeriods.map((e) => DenyMaintenancePeriod.toJSON(e));
    }
    if (message.insightsConfig !== undefined) {
      obj.insightsConfig = InsightsConfig.toJSON(message.insightsConfig);
    }
    if (message.passwordValidationPolicy !== undefined) {
      obj.passwordValidationPolicy = PasswordValidationPolicy.toJSON(message.passwordValidationPolicy);
    }
    if (message.sqlServerAuditConfig !== undefined) {
      obj.sqlServerAuditConfig = SqlServerAuditConfig.toJSON(message.sqlServerAuditConfig);
    }
    if (message.edition !== 0) {
      obj.edition = settings_EditionToJSON(message.edition);
    }
    if (message.connectorEnforcement !== 0) {
      obj.connectorEnforcement = settings_ConnectorEnforcementToJSON(message.connectorEnforcement);
    }
    if (message.deletionProtectionEnabled !== undefined) {
      obj.deletionProtectionEnabled = message.deletionProtectionEnabled;
    }
    if (message.timeZone !== "") {
      obj.timeZone = message.timeZone;
    }
    if (message.advancedMachineFeatures !== undefined) {
      obj.advancedMachineFeatures = AdvancedMachineFeatures.toJSON(message.advancedMachineFeatures);
    }
    if (message.dataCacheConfig !== undefined) {
      obj.dataCacheConfig = DataCacheConfig.toJSON(message.dataCacheConfig);
    }
    if (message.enableGoogleMlIntegration !== undefined) {
      obj.enableGoogleMlIntegration = message.enableGoogleMlIntegration;
    }
    if (message.enableDataplexIntegration !== undefined) {
      obj.enableDataplexIntegration = message.enableDataplexIntegration;
    }
    return obj;
  },

  create(base?: DeepPartial<Settings>): Settings {
    return Settings.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Settings>): Settings {
    const message = createBaseSettings();
    message.settingsVersion = (object.settingsVersion !== undefined && object.settingsVersion !== null)
      ? Long.fromValue(object.settingsVersion)
      : undefined;
    message.authorizedGaeApplications = object.authorizedGaeApplications?.map((e) => e) || [];
    message.tier = object.tier ?? "";
    message.kind = object.kind ?? "";
    message.userLabels = Object.entries(object.userLabels ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.availabilityType = object.availabilityType ?? 0;
    message.pricingPlan = object.pricingPlan ?? 0;
    message.replicationType = object.replicationType ?? 0;
    message.storageAutoResizeLimit =
      (object.storageAutoResizeLimit !== undefined && object.storageAutoResizeLimit !== null)
        ? Long.fromValue(object.storageAutoResizeLimit)
        : undefined;
    message.activationPolicy = object.activationPolicy ?? 0;
    message.ipConfiguration = (object.ipConfiguration !== undefined && object.ipConfiguration !== null)
      ? IpConfiguration.fromPartial(object.ipConfiguration)
      : undefined;
    message.storageAutoResize = object.storageAutoResize ?? undefined;
    message.locationPreference = (object.locationPreference !== undefined && object.locationPreference !== null)
      ? LocationPreference.fromPartial(object.locationPreference)
      : undefined;
    message.databaseFlags = object.databaseFlags?.map((e) => DatabaseFlags.fromPartial(e)) || [];
    message.dataDiskType = object.dataDiskType ?? 0;
    message.maintenanceWindow = (object.maintenanceWindow !== undefined && object.maintenanceWindow !== null)
      ? MaintenanceWindow.fromPartial(object.maintenanceWindow)
      : undefined;
    message.backupConfiguration = (object.backupConfiguration !== undefined && object.backupConfiguration !== null)
      ? BackupConfiguration.fromPartial(object.backupConfiguration)
      : undefined;
    message.databaseReplicationEnabled = object.databaseReplicationEnabled ?? undefined;
    message.crashSafeReplicationEnabled = object.crashSafeReplicationEnabled ?? undefined;
    message.dataDiskSizeGb = (object.dataDiskSizeGb !== undefined && object.dataDiskSizeGb !== null)
      ? Long.fromValue(object.dataDiskSizeGb)
      : undefined;
    message.activeDirectoryConfig =
      (object.activeDirectoryConfig !== undefined && object.activeDirectoryConfig !== null)
        ? SqlActiveDirectoryConfig.fromPartial(object.activeDirectoryConfig)
        : undefined;
    message.collation = object.collation ?? "";
    message.denyMaintenancePeriods = object.denyMaintenancePeriods?.map((e) => DenyMaintenancePeriod.fromPartial(e)) ||
      [];
    message.insightsConfig = (object.insightsConfig !== undefined && object.insightsConfig !== null)
      ? InsightsConfig.fromPartial(object.insightsConfig)
      : undefined;
    message.passwordValidationPolicy =
      (object.passwordValidationPolicy !== undefined && object.passwordValidationPolicy !== null)
        ? PasswordValidationPolicy.fromPartial(object.passwordValidationPolicy)
        : undefined;
    message.sqlServerAuditConfig = (object.sqlServerAuditConfig !== undefined && object.sqlServerAuditConfig !== null)
      ? SqlServerAuditConfig.fromPartial(object.sqlServerAuditConfig)
      : undefined;
    message.edition = object.edition ?? 0;
    message.connectorEnforcement = object.connectorEnforcement ?? 0;
    message.deletionProtectionEnabled = object.deletionProtectionEnabled ?? undefined;
    message.timeZone = object.timeZone ?? "";
    message.advancedMachineFeatures =
      (object.advancedMachineFeatures !== undefined && object.advancedMachineFeatures !== null)
        ? AdvancedMachineFeatures.fromPartial(object.advancedMachineFeatures)
        : undefined;
    message.dataCacheConfig = (object.dataCacheConfig !== undefined && object.dataCacheConfig !== null)
      ? DataCacheConfig.fromPartial(object.dataCacheConfig)
      : undefined;
    message.enableGoogleMlIntegration = object.enableGoogleMlIntegration ?? undefined;
    message.enableDataplexIntegration = object.enableDataplexIntegration ?? undefined;
    return message;
  },
};

function createBaseSettings_UserLabelsEntry(): Settings_UserLabelsEntry {
  return { key: "", value: "" };
}

export const Settings_UserLabelsEntry: MessageFns<Settings_UserLabelsEntry> = {
  encode(message: Settings_UserLabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Settings_UserLabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSettings_UserLabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Settings_UserLabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Settings_UserLabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Settings_UserLabelsEntry>): Settings_UserLabelsEntry {
    return Settings_UserLabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Settings_UserLabelsEntry>): Settings_UserLabelsEntry {
    const message = createBaseSettings_UserLabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseAdvancedMachineFeatures(): AdvancedMachineFeatures {
  return { threadsPerCore: 0 };
}

export const AdvancedMachineFeatures: MessageFns<AdvancedMachineFeatures> = {
  encode(message: AdvancedMachineFeatures, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.threadsPerCore !== 0) {
      writer.uint32(8).int32(message.threadsPerCore);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AdvancedMachineFeatures {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAdvancedMachineFeatures();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.threadsPerCore = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AdvancedMachineFeatures {
    return { threadsPerCore: isSet(object.threadsPerCore) ? globalThis.Number(object.threadsPerCore) : 0 };
  },

  toJSON(message: AdvancedMachineFeatures): unknown {
    const obj: any = {};
    if (message.threadsPerCore !== 0) {
      obj.threadsPerCore = Math.round(message.threadsPerCore);
    }
    return obj;
  },

  create(base?: DeepPartial<AdvancedMachineFeatures>): AdvancedMachineFeatures {
    return AdvancedMachineFeatures.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AdvancedMachineFeatures>): AdvancedMachineFeatures {
    const message = createBaseAdvancedMachineFeatures();
    message.threadsPerCore = object.threadsPerCore ?? 0;
    return message;
  },
};

function createBaseSslCert(): SslCert {
  return {
    kind: "",
    certSerialNumber: "",
    cert: "",
    createTime: undefined,
    commonName: "",
    expirationTime: undefined,
    sha1Fingerprint: "",
    instance: "",
    selfLink: "",
  };
}

export const SslCert: MessageFns<SslCert> = {
  encode(message: SslCert, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.certSerialNumber !== "") {
      writer.uint32(18).string(message.certSerialNumber);
    }
    if (message.cert !== "") {
      writer.uint32(26).string(message.cert);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(34).fork()).join();
    }
    if (message.commonName !== "") {
      writer.uint32(42).string(message.commonName);
    }
    if (message.expirationTime !== undefined) {
      Timestamp.encode(toTimestamp(message.expirationTime), writer.uint32(50).fork()).join();
    }
    if (message.sha1Fingerprint !== "") {
      writer.uint32(58).string(message.sha1Fingerprint);
    }
    if (message.instance !== "") {
      writer.uint32(66).string(message.instance);
    }
    if (message.selfLink !== "") {
      writer.uint32(74).string(message.selfLink);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SslCert {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSslCert();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.certSerialNumber = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.cert = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.commonName = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.expirationTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.sha1Fingerprint = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.instance = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.selfLink = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SslCert {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      certSerialNumber: isSet(object.certSerialNumber) ? globalThis.String(object.certSerialNumber) : "",
      cert: isSet(object.cert) ? globalThis.String(object.cert) : "",
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      commonName: isSet(object.commonName) ? globalThis.String(object.commonName) : "",
      expirationTime: isSet(object.expirationTime) ? fromJsonTimestamp(object.expirationTime) : undefined,
      sha1Fingerprint: isSet(object.sha1Fingerprint) ? globalThis.String(object.sha1Fingerprint) : "",
      instance: isSet(object.instance) ? globalThis.String(object.instance) : "",
      selfLink: isSet(object.selfLink) ? globalThis.String(object.selfLink) : "",
    };
  },

  toJSON(message: SslCert): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.certSerialNumber !== "") {
      obj.certSerialNumber = message.certSerialNumber;
    }
    if (message.cert !== "") {
      obj.cert = message.cert;
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.commonName !== "") {
      obj.commonName = message.commonName;
    }
    if (message.expirationTime !== undefined) {
      obj.expirationTime = message.expirationTime.toISOString();
    }
    if (message.sha1Fingerprint !== "") {
      obj.sha1Fingerprint = message.sha1Fingerprint;
    }
    if (message.instance !== "") {
      obj.instance = message.instance;
    }
    if (message.selfLink !== "") {
      obj.selfLink = message.selfLink;
    }
    return obj;
  },

  create(base?: DeepPartial<SslCert>): SslCert {
    return SslCert.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SslCert>): SslCert {
    const message = createBaseSslCert();
    message.kind = object.kind ?? "";
    message.certSerialNumber = object.certSerialNumber ?? "";
    message.cert = object.cert ?? "";
    message.createTime = object.createTime ?? undefined;
    message.commonName = object.commonName ?? "";
    message.expirationTime = object.expirationTime ?? undefined;
    message.sha1Fingerprint = object.sha1Fingerprint ?? "";
    message.instance = object.instance ?? "";
    message.selfLink = object.selfLink ?? "";
    return message;
  },
};

function createBaseSslCertDetail(): SslCertDetail {
  return { certInfo: undefined, certPrivateKey: "" };
}

export const SslCertDetail: MessageFns<SslCertDetail> = {
  encode(message: SslCertDetail, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.certInfo !== undefined) {
      SslCert.encode(message.certInfo, writer.uint32(10).fork()).join();
    }
    if (message.certPrivateKey !== "") {
      writer.uint32(18).string(message.certPrivateKey);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SslCertDetail {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSslCertDetail();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.certInfo = SslCert.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.certPrivateKey = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SslCertDetail {
    return {
      certInfo: isSet(object.certInfo) ? SslCert.fromJSON(object.certInfo) : undefined,
      certPrivateKey: isSet(object.certPrivateKey) ? globalThis.String(object.certPrivateKey) : "",
    };
  },

  toJSON(message: SslCertDetail): unknown {
    const obj: any = {};
    if (message.certInfo !== undefined) {
      obj.certInfo = SslCert.toJSON(message.certInfo);
    }
    if (message.certPrivateKey !== "") {
      obj.certPrivateKey = message.certPrivateKey;
    }
    return obj;
  },

  create(base?: DeepPartial<SslCertDetail>): SslCertDetail {
    return SslCertDetail.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SslCertDetail>): SslCertDetail {
    const message = createBaseSslCertDetail();
    message.certInfo = (object.certInfo !== undefined && object.certInfo !== null)
      ? SslCert.fromPartial(object.certInfo)
      : undefined;
    message.certPrivateKey = object.certPrivateKey ?? "";
    return message;
  },
};

function createBaseSslCertsCreateEphemeralRequest(): SslCertsCreateEphemeralRequest {
  return { publicKey: "", accessToken: "" };
}

export const SslCertsCreateEphemeralRequest: MessageFns<SslCertsCreateEphemeralRequest> = {
  encode(message: SslCertsCreateEphemeralRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.publicKey !== "") {
      writer.uint32(10).string(message.publicKey);
    }
    if (message.accessToken !== "") {
      writer.uint32(18).string(message.accessToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SslCertsCreateEphemeralRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSslCertsCreateEphemeralRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.publicKey = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.accessToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SslCertsCreateEphemeralRequest {
    return {
      publicKey: isSet(object.publicKey) ? globalThis.String(object.publicKey) : "",
      accessToken: isSet(object.accessToken) ? globalThis.String(object.accessToken) : "",
    };
  },

  toJSON(message: SslCertsCreateEphemeralRequest): unknown {
    const obj: any = {};
    if (message.publicKey !== "") {
      obj.publicKey = message.publicKey;
    }
    if (message.accessToken !== "") {
      obj.accessToken = message.accessToken;
    }
    return obj;
  },

  create(base?: DeepPartial<SslCertsCreateEphemeralRequest>): SslCertsCreateEphemeralRequest {
    return SslCertsCreateEphemeralRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SslCertsCreateEphemeralRequest>): SslCertsCreateEphemeralRequest {
    const message = createBaseSslCertsCreateEphemeralRequest();
    message.publicKey = object.publicKey ?? "";
    message.accessToken = object.accessToken ?? "";
    return message;
  },
};

function createBaseSslCertsInsertRequest(): SslCertsInsertRequest {
  return { commonName: "" };
}

export const SslCertsInsertRequest: MessageFns<SslCertsInsertRequest> = {
  encode(message: SslCertsInsertRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.commonName !== "") {
      writer.uint32(10).string(message.commonName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SslCertsInsertRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSslCertsInsertRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.commonName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SslCertsInsertRequest {
    return { commonName: isSet(object.commonName) ? globalThis.String(object.commonName) : "" };
  },

  toJSON(message: SslCertsInsertRequest): unknown {
    const obj: any = {};
    if (message.commonName !== "") {
      obj.commonName = message.commonName;
    }
    return obj;
  },

  create(base?: DeepPartial<SslCertsInsertRequest>): SslCertsInsertRequest {
    return SslCertsInsertRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SslCertsInsertRequest>): SslCertsInsertRequest {
    const message = createBaseSslCertsInsertRequest();
    message.commonName = object.commonName ?? "";
    return message;
  },
};

function createBaseSqlInstancesRescheduleMaintenanceRequestBody(): SqlInstancesRescheduleMaintenanceRequestBody {
  return { reschedule: undefined };
}

export const SqlInstancesRescheduleMaintenanceRequestBody: MessageFns<SqlInstancesRescheduleMaintenanceRequestBody> = {
  encode(
    message: SqlInstancesRescheduleMaintenanceRequestBody,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.reschedule !== undefined) {
      SqlInstancesRescheduleMaintenanceRequestBody_Reschedule.encode(message.reschedule, writer.uint32(26).fork())
        .join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SqlInstancesRescheduleMaintenanceRequestBody {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSqlInstancesRescheduleMaintenanceRequestBody();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 26) {
            break;
          }

          message.reschedule = SqlInstancesRescheduleMaintenanceRequestBody_Reschedule.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SqlInstancesRescheduleMaintenanceRequestBody {
    return {
      reschedule: isSet(object.reschedule)
        ? SqlInstancesRescheduleMaintenanceRequestBody_Reschedule.fromJSON(object.reschedule)
        : undefined,
    };
  },

  toJSON(message: SqlInstancesRescheduleMaintenanceRequestBody): unknown {
    const obj: any = {};
    if (message.reschedule !== undefined) {
      obj.reschedule = SqlInstancesRescheduleMaintenanceRequestBody_Reschedule.toJSON(message.reschedule);
    }
    return obj;
  },

  create(
    base?: DeepPartial<SqlInstancesRescheduleMaintenanceRequestBody>,
  ): SqlInstancesRescheduleMaintenanceRequestBody {
    return SqlInstancesRescheduleMaintenanceRequestBody.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<SqlInstancesRescheduleMaintenanceRequestBody>,
  ): SqlInstancesRescheduleMaintenanceRequestBody {
    const message = createBaseSqlInstancesRescheduleMaintenanceRequestBody();
    message.reschedule = (object.reschedule !== undefined && object.reschedule !== null)
      ? SqlInstancesRescheduleMaintenanceRequestBody_Reschedule.fromPartial(object.reschedule)
      : undefined;
    return message;
  },
};

function createBaseSqlInstancesRescheduleMaintenanceRequestBody_Reschedule(): SqlInstancesRescheduleMaintenanceRequestBody_Reschedule {
  return { rescheduleType: 0, scheduleTime: undefined };
}

export const SqlInstancesRescheduleMaintenanceRequestBody_Reschedule: MessageFns<
  SqlInstancesRescheduleMaintenanceRequestBody_Reschedule
> = {
  encode(
    message: SqlInstancesRescheduleMaintenanceRequestBody_Reschedule,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.rescheduleType !== 0) {
      writer.uint32(8).int32(message.rescheduleType);
    }
    if (message.scheduleTime !== undefined) {
      Timestamp.encode(toTimestamp(message.scheduleTime), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SqlInstancesRescheduleMaintenanceRequestBody_Reschedule {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSqlInstancesRescheduleMaintenanceRequestBody_Reschedule();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.rescheduleType = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.scheduleTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SqlInstancesRescheduleMaintenanceRequestBody_Reschedule {
    return {
      rescheduleType: isSet(object.rescheduleType)
        ? sqlInstancesRescheduleMaintenanceRequestBody_RescheduleTypeFromJSON(object.rescheduleType)
        : 0,
      scheduleTime: isSet(object.scheduleTime) ? fromJsonTimestamp(object.scheduleTime) : undefined,
    };
  },

  toJSON(message: SqlInstancesRescheduleMaintenanceRequestBody_Reschedule): unknown {
    const obj: any = {};
    if (message.rescheduleType !== 0) {
      obj.rescheduleType = sqlInstancesRescheduleMaintenanceRequestBody_RescheduleTypeToJSON(message.rescheduleType);
    }
    if (message.scheduleTime !== undefined) {
      obj.scheduleTime = message.scheduleTime.toISOString();
    }
    return obj;
  },

  create(
    base?: DeepPartial<SqlInstancesRescheduleMaintenanceRequestBody_Reschedule>,
  ): SqlInstancesRescheduleMaintenanceRequestBody_Reschedule {
    return SqlInstancesRescheduleMaintenanceRequestBody_Reschedule.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<SqlInstancesRescheduleMaintenanceRequestBody_Reschedule>,
  ): SqlInstancesRescheduleMaintenanceRequestBody_Reschedule {
    const message = createBaseSqlInstancesRescheduleMaintenanceRequestBody_Reschedule();
    message.rescheduleType = object.rescheduleType ?? 0;
    message.scheduleTime = object.scheduleTime ?? undefined;
    return message;
  },
};

function createBaseSslCertsInsertResponse(): SslCertsInsertResponse {
  return { kind: "", operation: undefined, serverCaCert: undefined, clientCert: undefined };
}

export const SslCertsInsertResponse: MessageFns<SslCertsInsertResponse> = {
  encode(message: SslCertsInsertResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.operation !== undefined) {
      Operation.encode(message.operation, writer.uint32(18).fork()).join();
    }
    if (message.serverCaCert !== undefined) {
      SslCert.encode(message.serverCaCert, writer.uint32(26).fork()).join();
    }
    if (message.clientCert !== undefined) {
      SslCertDetail.encode(message.clientCert, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SslCertsInsertResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSslCertsInsertResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.operation = Operation.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.serverCaCert = SslCert.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.clientCert = SslCertDetail.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SslCertsInsertResponse {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      operation: isSet(object.operation) ? Operation.fromJSON(object.operation) : undefined,
      serverCaCert: isSet(object.serverCaCert) ? SslCert.fromJSON(object.serverCaCert) : undefined,
      clientCert: isSet(object.clientCert) ? SslCertDetail.fromJSON(object.clientCert) : undefined,
    };
  },

  toJSON(message: SslCertsInsertResponse): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.operation !== undefined) {
      obj.operation = Operation.toJSON(message.operation);
    }
    if (message.serverCaCert !== undefined) {
      obj.serverCaCert = SslCert.toJSON(message.serverCaCert);
    }
    if (message.clientCert !== undefined) {
      obj.clientCert = SslCertDetail.toJSON(message.clientCert);
    }
    return obj;
  },

  create(base?: DeepPartial<SslCertsInsertResponse>): SslCertsInsertResponse {
    return SslCertsInsertResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SslCertsInsertResponse>): SslCertsInsertResponse {
    const message = createBaseSslCertsInsertResponse();
    message.kind = object.kind ?? "";
    message.operation = (object.operation !== undefined && object.operation !== null)
      ? Operation.fromPartial(object.operation)
      : undefined;
    message.serverCaCert = (object.serverCaCert !== undefined && object.serverCaCert !== null)
      ? SslCert.fromPartial(object.serverCaCert)
      : undefined;
    message.clientCert = (object.clientCert !== undefined && object.clientCert !== null)
      ? SslCertDetail.fromPartial(object.clientCert)
      : undefined;
    return message;
  },
};

function createBaseSslCertsListResponse(): SslCertsListResponse {
  return { kind: "", items: [] };
}

export const SslCertsListResponse: MessageFns<SslCertsListResponse> = {
  encode(message: SslCertsListResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    for (const v of message.items) {
      SslCert.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SslCertsListResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSslCertsListResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.items.push(SslCert.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SslCertsListResponse {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      items: globalThis.Array.isArray(object?.items) ? object.items.map((e: any) => SslCert.fromJSON(e)) : [],
    };
  },

  toJSON(message: SslCertsListResponse): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.items?.length) {
      obj.items = message.items.map((e) => SslCert.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<SslCertsListResponse>): SslCertsListResponse {
    return SslCertsListResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SslCertsListResponse>): SslCertsListResponse {
    const message = createBaseSslCertsListResponse();
    message.kind = object.kind ?? "";
    message.items = object.items?.map((e) => SslCert.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTruncateLogContext(): TruncateLogContext {
  return { kind: "", logType: "" };
}

export const TruncateLogContext: MessageFns<TruncateLogContext> = {
  encode(message: TruncateLogContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.logType !== "") {
      writer.uint32(18).string(message.logType);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TruncateLogContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTruncateLogContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.logType = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TruncateLogContext {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      logType: isSet(object.logType) ? globalThis.String(object.logType) : "",
    };
  },

  toJSON(message: TruncateLogContext): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.logType !== "") {
      obj.logType = message.logType;
    }
    return obj;
  },

  create(base?: DeepPartial<TruncateLogContext>): TruncateLogContext {
    return TruncateLogContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TruncateLogContext>): TruncateLogContext {
    const message = createBaseTruncateLogContext();
    message.kind = object.kind ?? "";
    message.logType = object.logType ?? "";
    return message;
  },
};

function createBaseSqlActiveDirectoryConfig(): SqlActiveDirectoryConfig {
  return { kind: "", domain: "" };
}

export const SqlActiveDirectoryConfig: MessageFns<SqlActiveDirectoryConfig> = {
  encode(message: SqlActiveDirectoryConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.domain !== "") {
      writer.uint32(18).string(message.domain);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SqlActiveDirectoryConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSqlActiveDirectoryConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.domain = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SqlActiveDirectoryConfig {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      domain: isSet(object.domain) ? globalThis.String(object.domain) : "",
    };
  },

  toJSON(message: SqlActiveDirectoryConfig): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.domain !== "") {
      obj.domain = message.domain;
    }
    return obj;
  },

  create(base?: DeepPartial<SqlActiveDirectoryConfig>): SqlActiveDirectoryConfig {
    return SqlActiveDirectoryConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SqlActiveDirectoryConfig>): SqlActiveDirectoryConfig {
    const message = createBaseSqlActiveDirectoryConfig();
    message.kind = object.kind ?? "";
    message.domain = object.domain ?? "";
    return message;
  },
};

function createBaseSqlServerAuditConfig(): SqlServerAuditConfig {
  return { kind: "", bucket: "", retentionInterval: undefined, uploadInterval: undefined };
}

export const SqlServerAuditConfig: MessageFns<SqlServerAuditConfig> = {
  encode(message: SqlServerAuditConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    if (message.bucket !== "") {
      writer.uint32(18).string(message.bucket);
    }
    if (message.retentionInterval !== undefined) {
      Duration.encode(message.retentionInterval, writer.uint32(26).fork()).join();
    }
    if (message.uploadInterval !== undefined) {
      Duration.encode(message.uploadInterval, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SqlServerAuditConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSqlServerAuditConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.bucket = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.retentionInterval = Duration.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.uploadInterval = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SqlServerAuditConfig {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      bucket: isSet(object.bucket) ? globalThis.String(object.bucket) : "",
      retentionInterval: isSet(object.retentionInterval) ? Duration.fromJSON(object.retentionInterval) : undefined,
      uploadInterval: isSet(object.uploadInterval) ? Duration.fromJSON(object.uploadInterval) : undefined,
    };
  },

  toJSON(message: SqlServerAuditConfig): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.bucket !== "") {
      obj.bucket = message.bucket;
    }
    if (message.retentionInterval !== undefined) {
      obj.retentionInterval = Duration.toJSON(message.retentionInterval);
    }
    if (message.uploadInterval !== undefined) {
      obj.uploadInterval = Duration.toJSON(message.uploadInterval);
    }
    return obj;
  },

  create(base?: DeepPartial<SqlServerAuditConfig>): SqlServerAuditConfig {
    return SqlServerAuditConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SqlServerAuditConfig>): SqlServerAuditConfig {
    const message = createBaseSqlServerAuditConfig();
    message.kind = object.kind ?? "";
    message.bucket = object.bucket ?? "";
    message.retentionInterval = (object.retentionInterval !== undefined && object.retentionInterval !== null)
      ? Duration.fromPartial(object.retentionInterval)
      : undefined;
    message.uploadInterval = (object.uploadInterval !== undefined && object.uploadInterval !== null)
      ? Duration.fromPartial(object.uploadInterval)
      : undefined;
    return message;
  },
};

function createBaseAcquireSsrsLeaseContext(): AcquireSsrsLeaseContext {
  return { setupLogin: undefined, serviceLogin: undefined, reportDatabase: undefined, duration: undefined };
}

export const AcquireSsrsLeaseContext: MessageFns<AcquireSsrsLeaseContext> = {
  encode(message: AcquireSsrsLeaseContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.setupLogin !== undefined) {
      writer.uint32(10).string(message.setupLogin);
    }
    if (message.serviceLogin !== undefined) {
      writer.uint32(18).string(message.serviceLogin);
    }
    if (message.reportDatabase !== undefined) {
      writer.uint32(26).string(message.reportDatabase);
    }
    if (message.duration !== undefined) {
      Duration.encode(message.duration, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AcquireSsrsLeaseContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAcquireSsrsLeaseContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.setupLogin = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.serviceLogin = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.reportDatabase = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.duration = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AcquireSsrsLeaseContext {
    return {
      setupLogin: isSet(object.setupLogin) ? globalThis.String(object.setupLogin) : undefined,
      serviceLogin: isSet(object.serviceLogin) ? globalThis.String(object.serviceLogin) : undefined,
      reportDatabase: isSet(object.reportDatabase) ? globalThis.String(object.reportDatabase) : undefined,
      duration: isSet(object.duration) ? Duration.fromJSON(object.duration) : undefined,
    };
  },

  toJSON(message: AcquireSsrsLeaseContext): unknown {
    const obj: any = {};
    if (message.setupLogin !== undefined) {
      obj.setupLogin = message.setupLogin;
    }
    if (message.serviceLogin !== undefined) {
      obj.serviceLogin = message.serviceLogin;
    }
    if (message.reportDatabase !== undefined) {
      obj.reportDatabase = message.reportDatabase;
    }
    if (message.duration !== undefined) {
      obj.duration = Duration.toJSON(message.duration);
    }
    return obj;
  },

  create(base?: DeepPartial<AcquireSsrsLeaseContext>): AcquireSsrsLeaseContext {
    return AcquireSsrsLeaseContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AcquireSsrsLeaseContext>): AcquireSsrsLeaseContext {
    const message = createBaseAcquireSsrsLeaseContext();
    message.setupLogin = object.setupLogin ?? undefined;
    message.serviceLogin = object.serviceLogin ?? undefined;
    message.reportDatabase = object.reportDatabase ?? undefined;
    message.duration = (object.duration !== undefined && object.duration !== null)
      ? Duration.fromPartial(object.duration)
      : undefined;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
