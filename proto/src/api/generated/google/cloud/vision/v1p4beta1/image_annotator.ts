// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/vision/v1p4beta1/image_annotator.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Operation } from "../../../longrunning/operations.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { Status } from "../../../rpc/status.js";
import { Color } from "../../../type/color.js";
import { LatLng } from "../../../type/latlng.js";
import { FaceRecognitionParams, FaceRecognitionResult } from "./face.js";
import { BoundingPoly, Position } from "./geometry.js";
import { ProductSearchParams, ProductSearchResults } from "./product_search.js";
import { TextAnnotation } from "./text_annotation.js";
import { WebDetection } from "./web_detection.js";

export const protobufPackage = "google.cloud.vision.v1p4beta1";

/**
 * A bucketized representation of likelihood, which is intended to give clients
 * highly stable results across model upgrades.
 */
export enum Likelihood {
  /** UNKNOWN - Unknown likelihood. */
  UNKNOWN = 0,
  /** VERY_UNLIKELY - It is very unlikely. */
  VERY_UNLIKELY = 1,
  /** UNLIKELY - It is unlikely. */
  UNLIKELY = 2,
  /** POSSIBLE - It is possible. */
  POSSIBLE = 3,
  /** LIKELY - It is likely. */
  LIKELY = 4,
  /** VERY_LIKELY - It is very likely. */
  VERY_LIKELY = 5,
  UNRECOGNIZED = -1,
}

export function likelihoodFromJSON(object: any): Likelihood {
  switch (object) {
    case 0:
    case "UNKNOWN":
      return Likelihood.UNKNOWN;
    case 1:
    case "VERY_UNLIKELY":
      return Likelihood.VERY_UNLIKELY;
    case 2:
    case "UNLIKELY":
      return Likelihood.UNLIKELY;
    case 3:
    case "POSSIBLE":
      return Likelihood.POSSIBLE;
    case 4:
    case "LIKELY":
      return Likelihood.LIKELY;
    case 5:
    case "VERY_LIKELY":
      return Likelihood.VERY_LIKELY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Likelihood.UNRECOGNIZED;
  }
}

export function likelihoodToJSON(object: Likelihood): string {
  switch (object) {
    case Likelihood.UNKNOWN:
      return "UNKNOWN";
    case Likelihood.VERY_UNLIKELY:
      return "VERY_UNLIKELY";
    case Likelihood.UNLIKELY:
      return "UNLIKELY";
    case Likelihood.POSSIBLE:
      return "POSSIBLE";
    case Likelihood.LIKELY:
      return "LIKELY";
    case Likelihood.VERY_LIKELY:
      return "VERY_LIKELY";
    case Likelihood.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * The type of Google Cloud Vision API detection to perform, and the maximum
 * number of results to return for that type. Multiple `Feature` objects can
 * be specified in the `features` list.
 */
export interface Feature {
  /** The feature type. */
  type: Feature_Type;
  /**
   * Maximum number of results of this type. Does not apply to
   * `TEXT_DETECTION`, `DOCUMENT_TEXT_DETECTION`, or `CROP_HINTS`.
   */
  maxResults: number;
  /**
   * Model to use for the feature.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest". `DOCUMENT_TEXT_DETECTION` and `TEXT_DETECTION` also
   * support "builtin/weekly" for the bleeding edge release updated weekly.
   */
  model: string;
}

/** Type of Google Cloud Vision API feature to be extracted. */
export enum Feature_Type {
  /** TYPE_UNSPECIFIED - Unspecified feature type. */
  TYPE_UNSPECIFIED = 0,
  /** FACE_DETECTION - Run face detection. */
  FACE_DETECTION = 1,
  /** LANDMARK_DETECTION - Run landmark detection. */
  LANDMARK_DETECTION = 2,
  /** LOGO_DETECTION - Run logo detection. */
  LOGO_DETECTION = 3,
  /** LABEL_DETECTION - Run label detection. */
  LABEL_DETECTION = 4,
  /**
   * TEXT_DETECTION - Run text detection / optical character recognition (OCR). Text detection
   * is optimized for areas of text within a larger image; if the image is
   * a document, use `DOCUMENT_TEXT_DETECTION` instead.
   */
  TEXT_DETECTION = 5,
  /**
   * DOCUMENT_TEXT_DETECTION - Run dense text document OCR. Takes precedence when both
   * `DOCUMENT_TEXT_DETECTION` and `TEXT_DETECTION` are present.
   */
  DOCUMENT_TEXT_DETECTION = 11,
  /**
   * SAFE_SEARCH_DETECTION - Run Safe Search to detect potentially unsafe
   * or undesirable content.
   */
  SAFE_SEARCH_DETECTION = 6,
  /**
   * IMAGE_PROPERTIES - Compute a set of image properties, such as the
   * image's dominant colors.
   */
  IMAGE_PROPERTIES = 7,
  /** CROP_HINTS - Run crop hints. */
  CROP_HINTS = 9,
  /** WEB_DETECTION - Run web detection. */
  WEB_DETECTION = 10,
  /** PRODUCT_SEARCH - Run Product Search. */
  PRODUCT_SEARCH = 12,
  /** OBJECT_LOCALIZATION - Run localizer for object detection. */
  OBJECT_LOCALIZATION = 19,
  UNRECOGNIZED = -1,
}

export function feature_TypeFromJSON(object: any): Feature_Type {
  switch (object) {
    case 0:
    case "TYPE_UNSPECIFIED":
      return Feature_Type.TYPE_UNSPECIFIED;
    case 1:
    case "FACE_DETECTION":
      return Feature_Type.FACE_DETECTION;
    case 2:
    case "LANDMARK_DETECTION":
      return Feature_Type.LANDMARK_DETECTION;
    case 3:
    case "LOGO_DETECTION":
      return Feature_Type.LOGO_DETECTION;
    case 4:
    case "LABEL_DETECTION":
      return Feature_Type.LABEL_DETECTION;
    case 5:
    case "TEXT_DETECTION":
      return Feature_Type.TEXT_DETECTION;
    case 11:
    case "DOCUMENT_TEXT_DETECTION":
      return Feature_Type.DOCUMENT_TEXT_DETECTION;
    case 6:
    case "SAFE_SEARCH_DETECTION":
      return Feature_Type.SAFE_SEARCH_DETECTION;
    case 7:
    case "IMAGE_PROPERTIES":
      return Feature_Type.IMAGE_PROPERTIES;
    case 9:
    case "CROP_HINTS":
      return Feature_Type.CROP_HINTS;
    case 10:
    case "WEB_DETECTION":
      return Feature_Type.WEB_DETECTION;
    case 12:
    case "PRODUCT_SEARCH":
      return Feature_Type.PRODUCT_SEARCH;
    case 19:
    case "OBJECT_LOCALIZATION":
      return Feature_Type.OBJECT_LOCALIZATION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Feature_Type.UNRECOGNIZED;
  }
}

export function feature_TypeToJSON(object: Feature_Type): string {
  switch (object) {
    case Feature_Type.TYPE_UNSPECIFIED:
      return "TYPE_UNSPECIFIED";
    case Feature_Type.FACE_DETECTION:
      return "FACE_DETECTION";
    case Feature_Type.LANDMARK_DETECTION:
      return "LANDMARK_DETECTION";
    case Feature_Type.LOGO_DETECTION:
      return "LOGO_DETECTION";
    case Feature_Type.LABEL_DETECTION:
      return "LABEL_DETECTION";
    case Feature_Type.TEXT_DETECTION:
      return "TEXT_DETECTION";
    case Feature_Type.DOCUMENT_TEXT_DETECTION:
      return "DOCUMENT_TEXT_DETECTION";
    case Feature_Type.SAFE_SEARCH_DETECTION:
      return "SAFE_SEARCH_DETECTION";
    case Feature_Type.IMAGE_PROPERTIES:
      return "IMAGE_PROPERTIES";
    case Feature_Type.CROP_HINTS:
      return "CROP_HINTS";
    case Feature_Type.WEB_DETECTION:
      return "WEB_DETECTION";
    case Feature_Type.PRODUCT_SEARCH:
      return "PRODUCT_SEARCH";
    case Feature_Type.OBJECT_LOCALIZATION:
      return "OBJECT_LOCALIZATION";
    case Feature_Type.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** External image source (Google Cloud Storage or web URL image location). */
export interface ImageSource {
  /**
   * *Use `image_uri` instead.**
   *
   * The Google Cloud Storage  URI of the form
   * `gs://bucket_name/object_name`. Object versioning is not supported. See
   * [Google Cloud Storage Request
   * URIs](https://cloud.google.com/storage/docs/reference-uris) for more info.
   */
  gcsImageUri: string;
  /**
   * The URI of the source image. Can be either:
   *
   * 1. A Google Cloud Storage URI of the form
   *    `gs://bucket_name/object_name`. Object versioning is not supported. See
   *    [Google Cloud Storage Request
   *    URIs](https://cloud.google.com/storage/docs/reference-uris) for more
   *    info.
   *
   * 2. A publicly-accessible image HTTP/HTTPS URL. When fetching images from
   *    HTTP/HTTPS URLs, Google cannot guarantee that the request will be
   *    completed. Your request may fail if the specified host denies the
   *    request (e.g. due to request throttling or DOS prevention), or if Google
   *    throttles requests to the site for abuse prevention. You should not
   *    depend on externally-hosted images for production applications.
   *
   * When both `gcs_image_uri` and `image_uri` are specified, `image_uri` takes
   * precedence.
   */
  imageUri: string;
}

/** Client image to perform Google Cloud Vision API tasks over. */
export interface Image {
  /**
   * Image content, represented as a stream of bytes.
   * Note: As with all `bytes` fields, protobuffers use a pure binary
   * representation, whereas JSON representations use base64.
   */
  content: Buffer;
  /**
   * Google Cloud Storage image location, or publicly-accessible image
   * URL. If both `content` and `source` are provided for an image, `content`
   * takes precedence and is used to perform the image annotation request.
   */
  source: ImageSource | undefined;
}

/** A face annotation object contains the results of face detection. */
export interface FaceAnnotation {
  /**
   * The bounding polygon around the face. The coordinates of the bounding box
   * are in the original image's scale.
   * The bounding box is computed to "frame" the face in accordance with human
   * expectations. It is based on the landmarker results.
   * Note that one or more x and/or y coordinates may not be generated in the
   * `BoundingPoly` (the polygon will be unbounded) if only a partial face
   * appears in the image to be annotated.
   */
  boundingPoly:
    | BoundingPoly
    | undefined;
  /**
   * The `fd_bounding_poly` bounding polygon is tighter than the
   * `boundingPoly`, and encloses only the skin part of the face. Typically, it
   * is used to eliminate the face from any image analysis that detects the
   * "amount of skin" visible in an image. It is not based on the
   * landmarker results, only on the initial face detection, hence
   * the <code>fd</code> (face detection) prefix.
   */
  fdBoundingPoly:
    | BoundingPoly
    | undefined;
  /** Detected face landmarks. */
  landmarks: FaceAnnotation_Landmark[];
  /**
   * Roll angle, which indicates the amount of clockwise/anti-clockwise rotation
   * of the face relative to the image vertical about the axis perpendicular to
   * the face. Range [-180,180].
   */
  rollAngle: number;
  /**
   * Yaw angle, which indicates the leftward/rightward angle that the face is
   * pointing relative to the vertical plane perpendicular to the image. Range
   * [-180,180].
   */
  panAngle: number;
  /**
   * Pitch angle, which indicates the upwards/downwards angle that the face is
   * pointing relative to the image's horizontal plane. Range [-180,180].
   */
  tiltAngle: number;
  /** Detection confidence. Range [0, 1]. */
  detectionConfidence: number;
  /** Face landmarking confidence. Range [0, 1]. */
  landmarkingConfidence: number;
  /** Joy likelihood. */
  joyLikelihood: Likelihood;
  /** Sorrow likelihood. */
  sorrowLikelihood: Likelihood;
  /** Anger likelihood. */
  angerLikelihood: Likelihood;
  /** Surprise likelihood. */
  surpriseLikelihood: Likelihood;
  /** Under-exposed likelihood. */
  underExposedLikelihood: Likelihood;
  /** Blurred likelihood. */
  blurredLikelihood: Likelihood;
  /** Headwear likelihood. */
  headwearLikelihood: Likelihood;
  /**
   * Additional recognition information. Only computed if
   * image_context.face_recognition_params is provided, **and** a match is found
   * to a [Celebrity][google.cloud.vision.v1p4beta1.Celebrity] in the input
   * [CelebritySet][google.cloud.vision.v1p4beta1.CelebritySet]. This field is
   * sorted in order of decreasing confidence values.
   */
  recognitionResult: FaceRecognitionResult[];
}

/** A face-specific landmark (for example, a face feature). */
export interface FaceAnnotation_Landmark {
  /** Face landmark type. */
  type: FaceAnnotation_Landmark_Type;
  /** Face landmark position. */
  position: Position | undefined;
}

/**
 * Face landmark (feature) type.
 * Left and right are defined from the vantage of the viewer of the image
 * without considering mirror projections typical of photos. So, `LEFT_EYE`,
 * typically, is the person's right eye.
 */
export enum FaceAnnotation_Landmark_Type {
  /** UNKNOWN_LANDMARK - Unknown face landmark detected. Should not be filled. */
  UNKNOWN_LANDMARK = 0,
  /** LEFT_EYE - Left eye. */
  LEFT_EYE = 1,
  /** RIGHT_EYE - Right eye. */
  RIGHT_EYE = 2,
  /** LEFT_OF_LEFT_EYEBROW - Left of left eyebrow. */
  LEFT_OF_LEFT_EYEBROW = 3,
  /** RIGHT_OF_LEFT_EYEBROW - Right of left eyebrow. */
  RIGHT_OF_LEFT_EYEBROW = 4,
  /** LEFT_OF_RIGHT_EYEBROW - Left of right eyebrow. */
  LEFT_OF_RIGHT_EYEBROW = 5,
  /** RIGHT_OF_RIGHT_EYEBROW - Right of right eyebrow. */
  RIGHT_OF_RIGHT_EYEBROW = 6,
  /** MIDPOINT_BETWEEN_EYES - Midpoint between eyes. */
  MIDPOINT_BETWEEN_EYES = 7,
  /** NOSE_TIP - Nose tip. */
  NOSE_TIP = 8,
  /** UPPER_LIP - Upper lip. */
  UPPER_LIP = 9,
  /** LOWER_LIP - Lower lip. */
  LOWER_LIP = 10,
  /** MOUTH_LEFT - Mouth left. */
  MOUTH_LEFT = 11,
  /** MOUTH_RIGHT - Mouth right. */
  MOUTH_RIGHT = 12,
  /** MOUTH_CENTER - Mouth center. */
  MOUTH_CENTER = 13,
  /** NOSE_BOTTOM_RIGHT - Nose, bottom right. */
  NOSE_BOTTOM_RIGHT = 14,
  /** NOSE_BOTTOM_LEFT - Nose, bottom left. */
  NOSE_BOTTOM_LEFT = 15,
  /** NOSE_BOTTOM_CENTER - Nose, bottom center. */
  NOSE_BOTTOM_CENTER = 16,
  /** LEFT_EYE_TOP_BOUNDARY - Left eye, top boundary. */
  LEFT_EYE_TOP_BOUNDARY = 17,
  /** LEFT_EYE_RIGHT_CORNER - Left eye, right corner. */
  LEFT_EYE_RIGHT_CORNER = 18,
  /** LEFT_EYE_BOTTOM_BOUNDARY - Left eye, bottom boundary. */
  LEFT_EYE_BOTTOM_BOUNDARY = 19,
  /** LEFT_EYE_LEFT_CORNER - Left eye, left corner. */
  LEFT_EYE_LEFT_CORNER = 20,
  /** RIGHT_EYE_TOP_BOUNDARY - Right eye, top boundary. */
  RIGHT_EYE_TOP_BOUNDARY = 21,
  /** RIGHT_EYE_RIGHT_CORNER - Right eye, right corner. */
  RIGHT_EYE_RIGHT_CORNER = 22,
  /** RIGHT_EYE_BOTTOM_BOUNDARY - Right eye, bottom boundary. */
  RIGHT_EYE_BOTTOM_BOUNDARY = 23,
  /** RIGHT_EYE_LEFT_CORNER - Right eye, left corner. */
  RIGHT_EYE_LEFT_CORNER = 24,
  /** LEFT_EYEBROW_UPPER_MIDPOINT - Left eyebrow, upper midpoint. */
  LEFT_EYEBROW_UPPER_MIDPOINT = 25,
  /** RIGHT_EYEBROW_UPPER_MIDPOINT - Right eyebrow, upper midpoint. */
  RIGHT_EYEBROW_UPPER_MIDPOINT = 26,
  /** LEFT_EAR_TRAGION - Left ear tragion. */
  LEFT_EAR_TRAGION = 27,
  /** RIGHT_EAR_TRAGION - Right ear tragion. */
  RIGHT_EAR_TRAGION = 28,
  /** LEFT_EYE_PUPIL - Left eye pupil. */
  LEFT_EYE_PUPIL = 29,
  /** RIGHT_EYE_PUPIL - Right eye pupil. */
  RIGHT_EYE_PUPIL = 30,
  /** FOREHEAD_GLABELLA - Forehead glabella. */
  FOREHEAD_GLABELLA = 31,
  /** CHIN_GNATHION - Chin gnathion. */
  CHIN_GNATHION = 32,
  /** CHIN_LEFT_GONION - Chin left gonion. */
  CHIN_LEFT_GONION = 33,
  /** CHIN_RIGHT_GONION - Chin right gonion. */
  CHIN_RIGHT_GONION = 34,
  UNRECOGNIZED = -1,
}

export function faceAnnotation_Landmark_TypeFromJSON(object: any): FaceAnnotation_Landmark_Type {
  switch (object) {
    case 0:
    case "UNKNOWN_LANDMARK":
      return FaceAnnotation_Landmark_Type.UNKNOWN_LANDMARK;
    case 1:
    case "LEFT_EYE":
      return FaceAnnotation_Landmark_Type.LEFT_EYE;
    case 2:
    case "RIGHT_EYE":
      return FaceAnnotation_Landmark_Type.RIGHT_EYE;
    case 3:
    case "LEFT_OF_LEFT_EYEBROW":
      return FaceAnnotation_Landmark_Type.LEFT_OF_LEFT_EYEBROW;
    case 4:
    case "RIGHT_OF_LEFT_EYEBROW":
      return FaceAnnotation_Landmark_Type.RIGHT_OF_LEFT_EYEBROW;
    case 5:
    case "LEFT_OF_RIGHT_EYEBROW":
      return FaceAnnotation_Landmark_Type.LEFT_OF_RIGHT_EYEBROW;
    case 6:
    case "RIGHT_OF_RIGHT_EYEBROW":
      return FaceAnnotation_Landmark_Type.RIGHT_OF_RIGHT_EYEBROW;
    case 7:
    case "MIDPOINT_BETWEEN_EYES":
      return FaceAnnotation_Landmark_Type.MIDPOINT_BETWEEN_EYES;
    case 8:
    case "NOSE_TIP":
      return FaceAnnotation_Landmark_Type.NOSE_TIP;
    case 9:
    case "UPPER_LIP":
      return FaceAnnotation_Landmark_Type.UPPER_LIP;
    case 10:
    case "LOWER_LIP":
      return FaceAnnotation_Landmark_Type.LOWER_LIP;
    case 11:
    case "MOUTH_LEFT":
      return FaceAnnotation_Landmark_Type.MOUTH_LEFT;
    case 12:
    case "MOUTH_RIGHT":
      return FaceAnnotation_Landmark_Type.MOUTH_RIGHT;
    case 13:
    case "MOUTH_CENTER":
      return FaceAnnotation_Landmark_Type.MOUTH_CENTER;
    case 14:
    case "NOSE_BOTTOM_RIGHT":
      return FaceAnnotation_Landmark_Type.NOSE_BOTTOM_RIGHT;
    case 15:
    case "NOSE_BOTTOM_LEFT":
      return FaceAnnotation_Landmark_Type.NOSE_BOTTOM_LEFT;
    case 16:
    case "NOSE_BOTTOM_CENTER":
      return FaceAnnotation_Landmark_Type.NOSE_BOTTOM_CENTER;
    case 17:
    case "LEFT_EYE_TOP_BOUNDARY":
      return FaceAnnotation_Landmark_Type.LEFT_EYE_TOP_BOUNDARY;
    case 18:
    case "LEFT_EYE_RIGHT_CORNER":
      return FaceAnnotation_Landmark_Type.LEFT_EYE_RIGHT_CORNER;
    case 19:
    case "LEFT_EYE_BOTTOM_BOUNDARY":
      return FaceAnnotation_Landmark_Type.LEFT_EYE_BOTTOM_BOUNDARY;
    case 20:
    case "LEFT_EYE_LEFT_CORNER":
      return FaceAnnotation_Landmark_Type.LEFT_EYE_LEFT_CORNER;
    case 21:
    case "RIGHT_EYE_TOP_BOUNDARY":
      return FaceAnnotation_Landmark_Type.RIGHT_EYE_TOP_BOUNDARY;
    case 22:
    case "RIGHT_EYE_RIGHT_CORNER":
      return FaceAnnotation_Landmark_Type.RIGHT_EYE_RIGHT_CORNER;
    case 23:
    case "RIGHT_EYE_BOTTOM_BOUNDARY":
      return FaceAnnotation_Landmark_Type.RIGHT_EYE_BOTTOM_BOUNDARY;
    case 24:
    case "RIGHT_EYE_LEFT_CORNER":
      return FaceAnnotation_Landmark_Type.RIGHT_EYE_LEFT_CORNER;
    case 25:
    case "LEFT_EYEBROW_UPPER_MIDPOINT":
      return FaceAnnotation_Landmark_Type.LEFT_EYEBROW_UPPER_MIDPOINT;
    case 26:
    case "RIGHT_EYEBROW_UPPER_MIDPOINT":
      return FaceAnnotation_Landmark_Type.RIGHT_EYEBROW_UPPER_MIDPOINT;
    case 27:
    case "LEFT_EAR_TRAGION":
      return FaceAnnotation_Landmark_Type.LEFT_EAR_TRAGION;
    case 28:
    case "RIGHT_EAR_TRAGION":
      return FaceAnnotation_Landmark_Type.RIGHT_EAR_TRAGION;
    case 29:
    case "LEFT_EYE_PUPIL":
      return FaceAnnotation_Landmark_Type.LEFT_EYE_PUPIL;
    case 30:
    case "RIGHT_EYE_PUPIL":
      return FaceAnnotation_Landmark_Type.RIGHT_EYE_PUPIL;
    case 31:
    case "FOREHEAD_GLABELLA":
      return FaceAnnotation_Landmark_Type.FOREHEAD_GLABELLA;
    case 32:
    case "CHIN_GNATHION":
      return FaceAnnotation_Landmark_Type.CHIN_GNATHION;
    case 33:
    case "CHIN_LEFT_GONION":
      return FaceAnnotation_Landmark_Type.CHIN_LEFT_GONION;
    case 34:
    case "CHIN_RIGHT_GONION":
      return FaceAnnotation_Landmark_Type.CHIN_RIGHT_GONION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return FaceAnnotation_Landmark_Type.UNRECOGNIZED;
  }
}

export function faceAnnotation_Landmark_TypeToJSON(object: FaceAnnotation_Landmark_Type): string {
  switch (object) {
    case FaceAnnotation_Landmark_Type.UNKNOWN_LANDMARK:
      return "UNKNOWN_LANDMARK";
    case FaceAnnotation_Landmark_Type.LEFT_EYE:
      return "LEFT_EYE";
    case FaceAnnotation_Landmark_Type.RIGHT_EYE:
      return "RIGHT_EYE";
    case FaceAnnotation_Landmark_Type.LEFT_OF_LEFT_EYEBROW:
      return "LEFT_OF_LEFT_EYEBROW";
    case FaceAnnotation_Landmark_Type.RIGHT_OF_LEFT_EYEBROW:
      return "RIGHT_OF_LEFT_EYEBROW";
    case FaceAnnotation_Landmark_Type.LEFT_OF_RIGHT_EYEBROW:
      return "LEFT_OF_RIGHT_EYEBROW";
    case FaceAnnotation_Landmark_Type.RIGHT_OF_RIGHT_EYEBROW:
      return "RIGHT_OF_RIGHT_EYEBROW";
    case FaceAnnotation_Landmark_Type.MIDPOINT_BETWEEN_EYES:
      return "MIDPOINT_BETWEEN_EYES";
    case FaceAnnotation_Landmark_Type.NOSE_TIP:
      return "NOSE_TIP";
    case FaceAnnotation_Landmark_Type.UPPER_LIP:
      return "UPPER_LIP";
    case FaceAnnotation_Landmark_Type.LOWER_LIP:
      return "LOWER_LIP";
    case FaceAnnotation_Landmark_Type.MOUTH_LEFT:
      return "MOUTH_LEFT";
    case FaceAnnotation_Landmark_Type.MOUTH_RIGHT:
      return "MOUTH_RIGHT";
    case FaceAnnotation_Landmark_Type.MOUTH_CENTER:
      return "MOUTH_CENTER";
    case FaceAnnotation_Landmark_Type.NOSE_BOTTOM_RIGHT:
      return "NOSE_BOTTOM_RIGHT";
    case FaceAnnotation_Landmark_Type.NOSE_BOTTOM_LEFT:
      return "NOSE_BOTTOM_LEFT";
    case FaceAnnotation_Landmark_Type.NOSE_BOTTOM_CENTER:
      return "NOSE_BOTTOM_CENTER";
    case FaceAnnotation_Landmark_Type.LEFT_EYE_TOP_BOUNDARY:
      return "LEFT_EYE_TOP_BOUNDARY";
    case FaceAnnotation_Landmark_Type.LEFT_EYE_RIGHT_CORNER:
      return "LEFT_EYE_RIGHT_CORNER";
    case FaceAnnotation_Landmark_Type.LEFT_EYE_BOTTOM_BOUNDARY:
      return "LEFT_EYE_BOTTOM_BOUNDARY";
    case FaceAnnotation_Landmark_Type.LEFT_EYE_LEFT_CORNER:
      return "LEFT_EYE_LEFT_CORNER";
    case FaceAnnotation_Landmark_Type.RIGHT_EYE_TOP_BOUNDARY:
      return "RIGHT_EYE_TOP_BOUNDARY";
    case FaceAnnotation_Landmark_Type.RIGHT_EYE_RIGHT_CORNER:
      return "RIGHT_EYE_RIGHT_CORNER";
    case FaceAnnotation_Landmark_Type.RIGHT_EYE_BOTTOM_BOUNDARY:
      return "RIGHT_EYE_BOTTOM_BOUNDARY";
    case FaceAnnotation_Landmark_Type.RIGHT_EYE_LEFT_CORNER:
      return "RIGHT_EYE_LEFT_CORNER";
    case FaceAnnotation_Landmark_Type.LEFT_EYEBROW_UPPER_MIDPOINT:
      return "LEFT_EYEBROW_UPPER_MIDPOINT";
    case FaceAnnotation_Landmark_Type.RIGHT_EYEBROW_UPPER_MIDPOINT:
      return "RIGHT_EYEBROW_UPPER_MIDPOINT";
    case FaceAnnotation_Landmark_Type.LEFT_EAR_TRAGION:
      return "LEFT_EAR_TRAGION";
    case FaceAnnotation_Landmark_Type.RIGHT_EAR_TRAGION:
      return "RIGHT_EAR_TRAGION";
    case FaceAnnotation_Landmark_Type.LEFT_EYE_PUPIL:
      return "LEFT_EYE_PUPIL";
    case FaceAnnotation_Landmark_Type.RIGHT_EYE_PUPIL:
      return "RIGHT_EYE_PUPIL";
    case FaceAnnotation_Landmark_Type.FOREHEAD_GLABELLA:
      return "FOREHEAD_GLABELLA";
    case FaceAnnotation_Landmark_Type.CHIN_GNATHION:
      return "CHIN_GNATHION";
    case FaceAnnotation_Landmark_Type.CHIN_LEFT_GONION:
      return "CHIN_LEFT_GONION";
    case FaceAnnotation_Landmark_Type.CHIN_RIGHT_GONION:
      return "CHIN_RIGHT_GONION";
    case FaceAnnotation_Landmark_Type.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Detected entity location information. */
export interface LocationInfo {
  /** lat/long location coordinates. */
  latLng: LatLng | undefined;
}

/** A `Property` consists of a user-supplied name/value pair. */
export interface Property {
  /** Name of the property. */
  name: string;
  /** Value of the property. */
  value: string;
  /** Value of numeric properties. */
  uint64Value: Long;
}

/** Set of detected entity features. */
export interface EntityAnnotation {
  /**
   * Opaque entity ID. Some IDs may be available in
   * [Google Knowledge Graph Search
   * API](https://developers.google.com/knowledge-graph/).
   */
  mid: string;
  /**
   * The language code for the locale in which the entity textual
   * `description` is expressed.
   */
  locale: string;
  /** Entity textual description, expressed in its `locale` language. */
  description: string;
  /** Overall score of the result. Range [0, 1]. */
  score: number;
  /**
   * *Deprecated. Use `score` instead.**
   * The accuracy of the entity detection in an image.
   * For example, for an image in which the "Eiffel Tower" entity is detected,
   * this field represents the confidence that there is a tower in the query
   * image. Range [0, 1].
   *
   * @deprecated
   */
  confidence: number;
  /**
   * The relevancy of the ICA (Image Content Annotation) label to the
   * image. For example, the relevancy of "tower" is likely higher to an image
   * containing the detected "Eiffel Tower" than to an image containing a
   * detected distant towering building, even though the confidence that
   * there is a tower in each image may be the same. Range [0, 1].
   */
  topicality: number;
  /**
   * Image region to which this entity belongs. Not produced
   * for `LABEL_DETECTION` features.
   */
  boundingPoly:
    | BoundingPoly
    | undefined;
  /**
   * The location information for the detected entity. Multiple
   * `LocationInfo` elements can be present because one location may
   * indicate the location of the scene in the image, and another location
   * may indicate the location of the place where the image was taken.
   * Location information is usually present for landmarks.
   */
  locations: LocationInfo[];
  /**
   * Some entities may have optional user-supplied `Property` (name/value)
   * fields, such a score or string that qualifies the entity.
   */
  properties: Property[];
}

/** Set of detected objects with bounding boxes. */
export interface LocalizedObjectAnnotation {
  /** Object ID that should align with EntityAnnotation mid. */
  mid: string;
  /**
   * The BCP-47 language code, such as "en-US" or "sr-Latn". For more
   * information, see
   * http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
   */
  languageCode: string;
  /** Object name, expressed in its `language_code` language. */
  name: string;
  /** Score of the result. Range [0, 1]. */
  score: number;
  /** Image region to which this object belongs. This must be populated. */
  boundingPoly: BoundingPoly | undefined;
}

/**
 * Set of features pertaining to the image, computed by computer vision
 * methods over safe-search verticals (for example, adult, spoof, medical,
 * violence).
 */
export interface SafeSearchAnnotation {
  /**
   * Represents the adult content likelihood for the image. Adult content may
   * contain elements such as nudity, pornographic images or cartoons, or
   * sexual activities.
   */
  adult: Likelihood;
  /**
   * Spoof likelihood. The likelihood that an modification
   * was made to the image's canonical version to make it appear
   * funny or offensive.
   */
  spoof: Likelihood;
  /** Likelihood that this is a medical image. */
  medical: Likelihood;
  /** Likelihood that this image contains violent content. */
  violence: Likelihood;
  /**
   * Likelihood that the request image contains racy content. Racy content may
   * include (but is not limited to) skimpy or sheer clothing, strategically
   * covered nudity, lewd or provocative poses, or close-ups of sensitive
   * body areas.
   */
  racy: Likelihood;
}

/** Rectangle determined by min and max `LatLng` pairs. */
export interface LatLongRect {
  /** Min lat/long pair. */
  minLatLng:
    | LatLng
    | undefined;
  /** Max lat/long pair. */
  maxLatLng: LatLng | undefined;
}

/**
 * Color information consists of RGB channels, score, and the fraction of
 * the image that the color occupies in the image.
 */
export interface ColorInfo {
  /** RGB components of the color. */
  color:
    | Color
    | undefined;
  /** Image-specific score for this color. Value in range [0, 1]. */
  score: number;
  /**
   * The fraction of pixels the color occupies in the image.
   * Value in range [0, 1].
   */
  pixelFraction: number;
}

/** Set of dominant colors and their corresponding scores. */
export interface DominantColorsAnnotation {
  /** RGB color values with their score and pixel fraction. */
  colors: ColorInfo[];
}

/** Stores image properties, such as dominant colors. */
export interface ImageProperties {
  /** If present, dominant colors completed successfully. */
  dominantColors: DominantColorsAnnotation | undefined;
}

/** Single crop hint that is used to generate a new crop when serving an image. */
export interface CropHint {
  /**
   * The bounding polygon for the crop region. The coordinates of the bounding
   * box are in the original image's scale.
   */
  boundingPoly:
    | BoundingPoly
    | undefined;
  /** Confidence of this being a salient region.  Range [0, 1]. */
  confidence: number;
  /**
   * Fraction of importance of this salient region with respect to the original
   * image.
   */
  importanceFraction: number;
}

/** Set of crop hints that are used to generate new crops when serving images. */
export interface CropHintsAnnotation {
  /** Crop hint results. */
  cropHints: CropHint[];
}

/** Parameters for crop hints annotation request. */
export interface CropHintsParams {
  /**
   * Aspect ratios in floats, representing the ratio of the width to the height
   * of the image. For example, if the desired aspect ratio is 4/3, the
   * corresponding float value should be 1.33333.  If not specified, the
   * best possible crop is returned. The number of provided aspect ratios is
   * limited to a maximum of 16; any aspect ratios provided after the 16th are
   * ignored.
   */
  aspectRatios: number[];
}

/** Parameters for web detection request. */
export interface WebDetectionParams {
  /** Whether to include results derived from the geo information in the image. */
  includeGeoResults: boolean;
}

/**
 * Parameters for text detections. This is used to control TEXT_DETECTION and
 * DOCUMENT_TEXT_DETECTION features.
 */
export interface TextDetectionParams {
  /**
   * By default, Cloud Vision API only includes confidence score for
   * DOCUMENT_TEXT_DETECTION result. Set the flag to true to include confidence
   * score for TEXT_DETECTION as well.
   */
  enableTextDetectionConfidenceScore: boolean;
  /** A list of advanced OCR options to fine-tune OCR behavior. */
  advancedOcrOptions: string[];
}

/** Image context and/or feature-specific parameters. */
export interface ImageContext {
  /** Not used. */
  latLongRect:
    | LatLongRect
    | undefined;
  /**
   * List of languages to use for TEXT_DETECTION. In most cases, an empty value
   * yields the best results since it enables automatic language detection. For
   * languages based on the Latin alphabet, setting `language_hints` is not
   * needed. In rare cases, when the language of the text in the image is known,
   * setting a hint will help get better results (although it will be a
   * significant hindrance if the hint is wrong). Text detection returns an
   * error if one or more of the specified languages is not one of the
   * [supported languages](https://cloud.google.com/vision/docs/languages).
   */
  languageHints: string[];
  /** Parameters for crop hints annotation request. */
  cropHintsParams:
    | CropHintsParams
    | undefined;
  /** Parameters for face recognition. */
  faceRecognitionParams:
    | FaceRecognitionParams
    | undefined;
  /** Parameters for product search. */
  productSearchParams:
    | ProductSearchParams
    | undefined;
  /** Parameters for web detection. */
  webDetectionParams:
    | WebDetectionParams
    | undefined;
  /** Parameters for text detection and document text detection. */
  textDetectionParams: TextDetectionParams | undefined;
}

/**
 * Request for performing Google Cloud Vision API tasks over a user-provided
 * image, with user-requested features, and with context information.
 */
export interface AnnotateImageRequest {
  /** The image to be processed. */
  image:
    | Image
    | undefined;
  /** Requested features. */
  features: Feature[];
  /** Additional context that may accompany the image. */
  imageContext: ImageContext | undefined;
}

/**
 * If an image was produced from a file (e.g. a PDF), this message gives
 * information about the source of that image.
 */
export interface ImageAnnotationContext {
  /** The URI of the file used to produce the image. */
  uri: string;
  /**
   * If the file was a PDF or TIFF, this field gives the page number within
   * the file used to produce the image.
   */
  pageNumber: number;
}

/** Response to an image annotation request. */
export interface AnnotateImageResponse {
  /** If present, face detection has completed successfully. */
  faceAnnotations: FaceAnnotation[];
  /** If present, landmark detection has completed successfully. */
  landmarkAnnotations: EntityAnnotation[];
  /** If present, logo detection has completed successfully. */
  logoAnnotations: EntityAnnotation[];
  /** If present, label detection has completed successfully. */
  labelAnnotations: EntityAnnotation[];
  /**
   * If present, localized object detection has completed successfully.
   * This will be sorted descending by confidence score.
   */
  localizedObjectAnnotations: LocalizedObjectAnnotation[];
  /** If present, text (OCR) detection has completed successfully. */
  textAnnotations: EntityAnnotation[];
  /**
   * If present, text (OCR) detection or document (OCR) text detection has
   * completed successfully.
   * This annotation provides the structural hierarchy for the OCR detected
   * text.
   */
  fullTextAnnotation:
    | TextAnnotation
    | undefined;
  /** If present, safe-search annotation has completed successfully. */
  safeSearchAnnotation:
    | SafeSearchAnnotation
    | undefined;
  /** If present, image properties were extracted successfully. */
  imagePropertiesAnnotation:
    | ImageProperties
    | undefined;
  /** If present, crop hints have completed successfully. */
  cropHintsAnnotation:
    | CropHintsAnnotation
    | undefined;
  /** If present, web detection has completed successfully. */
  webDetection:
    | WebDetection
    | undefined;
  /** If present, product search has completed successfully. */
  productSearchResults:
    | ProductSearchResults
    | undefined;
  /**
   * If set, represents the error message for the operation.
   * Note that filled-in image annotations are guaranteed to be
   * correct, even when `error` is set.
   */
  error:
    | Status
    | undefined;
  /**
   * If present, contextual information is needed to understand where this image
   * comes from.
   */
  context: ImageAnnotationContext | undefined;
}

/** Multiple image annotation requests are batched into a single service call. */
export interface BatchAnnotateImagesRequest {
  /** Required. Individual image annotation requests for this batch. */
  requests: AnnotateImageRequest[];
}

/** Response to a batch image annotation request. */
export interface BatchAnnotateImagesResponse {
  /** Individual responses to image annotation requests within the batch. */
  responses: AnnotateImageResponse[];
}

/** A request to annotate one single file, e.g. a PDF, TIFF or GIF file. */
export interface AnnotateFileRequest {
  /** Required. Information about the input file. */
  inputConfig:
    | InputConfig
    | undefined;
  /** Required. Requested features. */
  features: Feature[];
  /** Additional context that may accompany the image(s) in the file. */
  imageContext:
    | ImageContext
    | undefined;
  /**
   * Pages of the file to perform image annotation.
   *
   * Pages starts from 1, we assume the first page of the file is page 1.
   * At most 5 pages are supported per request. Pages can be negative.
   *
   * Page 1 means the first page.
   * Page 2 means the second page.
   * Page -1 means the last page.
   * Page -2 means the second to the last page.
   *
   * If the file is GIF instead of PDF or TIFF, page refers to GIF frames.
   *
   * If this field is empty, by default the service performs image annotation
   * for the first 5 pages of the file.
   */
  pages: number[];
}

/**
 * Response to a single file annotation request. A file may contain one or more
 * images, which individually have their own responses.
 */
export interface AnnotateFileResponse {
  /** Information about the file for which this response is generated. */
  inputConfig:
    | InputConfig
    | undefined;
  /**
   * Individual responses to images found within the file. This field will be
   * empty if the `error` field is set.
   */
  responses: AnnotateImageResponse[];
  /** This field gives the total number of pages in the file. */
  totalPages: number;
  /**
   * If set, represents the error message for the failed request. The
   * `responses` field will not be set in this case.
   */
  error: Status | undefined;
}

/** A list of requests to annotate files using the BatchAnnotateFiles API. */
export interface BatchAnnotateFilesRequest {
  /**
   * Required. The list of file annotation requests. Right now we support only
   * one AnnotateFileRequest in BatchAnnotateFilesRequest.
   */
  requests: AnnotateFileRequest[];
}

/** A list of file annotation responses. */
export interface BatchAnnotateFilesResponse {
  /**
   * The list of file annotation responses, each response corresponding to each
   * AnnotateFileRequest in BatchAnnotateFilesRequest.
   */
  responses: AnnotateFileResponse[];
}

/** An offline file annotation request. */
export interface AsyncAnnotateFileRequest {
  /** Required. Information about the input file. */
  inputConfig:
    | InputConfig
    | undefined;
  /** Required. Requested features. */
  features: Feature[];
  /** Additional context that may accompany the image(s) in the file. */
  imageContext:
    | ImageContext
    | undefined;
  /** Required. The desired output location and metadata (e.g. format). */
  outputConfig: OutputConfig | undefined;
}

/** The response for a single offline file annotation request. */
export interface AsyncAnnotateFileResponse {
  /** The output location and metadata from AsyncAnnotateFileRequest. */
  outputConfig: OutputConfig | undefined;
}

/** Request for async image annotation for a list of images. */
export interface AsyncBatchAnnotateImagesRequest {
  /** Required. Individual image annotation requests for this batch. */
  requests: AnnotateImageRequest[];
  /** Required. The desired output location and metadata (e.g. format). */
  outputConfig: OutputConfig | undefined;
}

/** Response to an async batch image annotation request. */
export interface AsyncBatchAnnotateImagesResponse {
  /** The output location and metadata from AsyncBatchAnnotateImagesRequest. */
  outputConfig: OutputConfig | undefined;
}

/**
 * Multiple async file annotation requests are batched into a single service
 * call.
 */
export interface AsyncBatchAnnotateFilesRequest {
  /** Required. Individual async file annotation requests for this batch. */
  requests: AsyncAnnotateFileRequest[];
}

/** Response to an async batch file annotation request. */
export interface AsyncBatchAnnotateFilesResponse {
  /**
   * The list of file annotation responses, one for each request in
   * AsyncBatchAnnotateFilesRequest.
   */
  responses: AsyncAnnotateFileResponse[];
}

/** The desired input location and metadata. */
export interface InputConfig {
  /** The Google Cloud Storage location to read the input from. */
  gcsSource:
    | GcsSource
    | undefined;
  /**
   * File content, represented as a stream of bytes.
   * Note: As with all `bytes` fields, protobuffers use a pure binary
   * representation, whereas JSON representations use base64.
   *
   * Currently, this field only works for BatchAnnotateFiles requests. It does
   * not work for AsyncBatchAnnotateFiles requests.
   */
  content: Buffer;
  /**
   * The type of the file. Currently only "application/pdf", "image/tiff" and
   * "image/gif" are supported. Wildcards are not supported.
   */
  mimeType: string;
}

/** The desired output location and metadata. */
export interface OutputConfig {
  /** The Google Cloud Storage location to write the output(s) to. */
  gcsDestination:
    | GcsDestination
    | undefined;
  /**
   * The max number of response protos to put into each output JSON file on
   * Google Cloud Storage.
   * The valid range is [1, 100]. If not specified, the default value is 20.
   *
   * For example, for one pdf file with 100 pages, 100 response protos will
   * be generated. If `batch_size` = 20, then 5 json files each
   * containing 20 response protos will be written under the prefix
   * `gcs_destination`.`uri`.
   *
   * Currently, batch_size only applies to GcsDestination, with potential future
   * support for other output configurations.
   */
  batchSize: number;
}

/** The Google Cloud Storage location where the input will be read from. */
export interface GcsSource {
  /**
   * Google Cloud Storage URI for the input file. This must only be a
   * Google Cloud Storage object. Wildcards are not currently supported.
   */
  uri: string;
}

/** The Google Cloud Storage location where the output will be written to. */
export interface GcsDestination {
  /**
   * Google Cloud Storage URI prefix where the results will be stored. Results
   * will be in JSON format and preceded by its corresponding input URI prefix.
   * This field can either represent a gcs file prefix or gcs directory. In
   * either case, the uri should be unique because in order to get all of the
   * output files, you will need to do a wildcard gcs search on the uri prefix
   * you provide.
   *
   * Examples:
   *
   * *    File Prefix: gs://bucket-name/here/filenameprefix   The output files
   * will be created in gs://bucket-name/here/ and the names of the
   * output files will begin with "filenameprefix".
   *
   * *    Directory Prefix: gs://bucket-name/some/location/   The output files
   * will be created in gs://bucket-name/some/location/ and the names of the
   * output files could be anything because there was no filename prefix
   * specified.
   *
   * If multiple outputs, each response is still AnnotateFileResponse, each of
   * which contains some subset of the full list of AnnotateImageResponse.
   * Multiple outputs can happen if, for example, the output JSON is too large
   * and overflows into multiple sharded files.
   */
  uri: string;
}

/** Contains metadata for the BatchAnnotateImages operation. */
export interface OperationMetadata {
  /** Current state of the batch operation. */
  state: OperationMetadata_State;
  /** The time when the batch request was received. */
  createTime:
    | Date
    | undefined;
  /** The time when the operation result was last updated. */
  updateTime: Date | undefined;
}

/** Batch operation states. */
export enum OperationMetadata_State {
  /** STATE_UNSPECIFIED - Invalid. */
  STATE_UNSPECIFIED = 0,
  /** CREATED - Request is received. */
  CREATED = 1,
  /** RUNNING - Request is actively being processed. */
  RUNNING = 2,
  /** DONE - The batch processing is done. */
  DONE = 3,
  /** CANCELLED - The batch processing was cancelled. */
  CANCELLED = 4,
  UNRECOGNIZED = -1,
}

export function operationMetadata_StateFromJSON(object: any): OperationMetadata_State {
  switch (object) {
    case 0:
    case "STATE_UNSPECIFIED":
      return OperationMetadata_State.STATE_UNSPECIFIED;
    case 1:
    case "CREATED":
      return OperationMetadata_State.CREATED;
    case 2:
    case "RUNNING":
      return OperationMetadata_State.RUNNING;
    case 3:
    case "DONE":
      return OperationMetadata_State.DONE;
    case 4:
    case "CANCELLED":
      return OperationMetadata_State.CANCELLED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return OperationMetadata_State.UNRECOGNIZED;
  }
}

export function operationMetadata_StateToJSON(object: OperationMetadata_State): string {
  switch (object) {
    case OperationMetadata_State.STATE_UNSPECIFIED:
      return "STATE_UNSPECIFIED";
    case OperationMetadata_State.CREATED:
      return "CREATED";
    case OperationMetadata_State.RUNNING:
      return "RUNNING";
    case OperationMetadata_State.DONE:
      return "DONE";
    case OperationMetadata_State.CANCELLED:
      return "CANCELLED";
    case OperationMetadata_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseFeature(): Feature {
  return { type: 0, maxResults: 0, model: "" };
}

export const Feature: MessageFns<Feature> = {
  encode(message: Feature, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.type !== 0) {
      writer.uint32(8).int32(message.type);
    }
    if (message.maxResults !== 0) {
      writer.uint32(16).int32(message.maxResults);
    }
    if (message.model !== "") {
      writer.uint32(26).string(message.model);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Feature {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFeature();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.maxResults = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.model = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Feature {
    return {
      type: isSet(object.type) ? feature_TypeFromJSON(object.type) : 0,
      maxResults: isSet(object.maxResults) ? globalThis.Number(object.maxResults) : 0,
      model: isSet(object.model) ? globalThis.String(object.model) : "",
    };
  },

  toJSON(message: Feature): unknown {
    const obj: any = {};
    if (message.type !== 0) {
      obj.type = feature_TypeToJSON(message.type);
    }
    if (message.maxResults !== 0) {
      obj.maxResults = Math.round(message.maxResults);
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    return obj;
  },

  create(base?: DeepPartial<Feature>): Feature {
    return Feature.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Feature>): Feature {
    const message = createBaseFeature();
    message.type = object.type ?? 0;
    message.maxResults = object.maxResults ?? 0;
    message.model = object.model ?? "";
    return message;
  },
};

function createBaseImageSource(): ImageSource {
  return { gcsImageUri: "", imageUri: "" };
}

export const ImageSource: MessageFns<ImageSource> = {
  encode(message: ImageSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcsImageUri !== "") {
      writer.uint32(10).string(message.gcsImageUri);
    }
    if (message.imageUri !== "") {
      writer.uint32(18).string(message.imageUri);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImageSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImageSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.gcsImageUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.imageUri = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImageSource {
    return {
      gcsImageUri: isSet(object.gcsImageUri) ? globalThis.String(object.gcsImageUri) : "",
      imageUri: isSet(object.imageUri) ? globalThis.String(object.imageUri) : "",
    };
  },

  toJSON(message: ImageSource): unknown {
    const obj: any = {};
    if (message.gcsImageUri !== "") {
      obj.gcsImageUri = message.gcsImageUri;
    }
    if (message.imageUri !== "") {
      obj.imageUri = message.imageUri;
    }
    return obj;
  },

  create(base?: DeepPartial<ImageSource>): ImageSource {
    return ImageSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImageSource>): ImageSource {
    const message = createBaseImageSource();
    message.gcsImageUri = object.gcsImageUri ?? "";
    message.imageUri = object.imageUri ?? "";
    return message;
  },
};

function createBaseImage(): Image {
  return { content: Buffer.alloc(0), source: undefined };
}

export const Image: MessageFns<Image> = {
  encode(message: Image, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.content.length !== 0) {
      writer.uint32(10).bytes(message.content);
    }
    if (message.source !== undefined) {
      ImageSource.encode(message.source, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Image {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImage();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.content = Buffer.from(reader.bytes());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.source = ImageSource.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Image {
    return {
      content: isSet(object.content) ? Buffer.from(bytesFromBase64(object.content)) : Buffer.alloc(0),
      source: isSet(object.source) ? ImageSource.fromJSON(object.source) : undefined,
    };
  },

  toJSON(message: Image): unknown {
    const obj: any = {};
    if (message.content.length !== 0) {
      obj.content = base64FromBytes(message.content);
    }
    if (message.source !== undefined) {
      obj.source = ImageSource.toJSON(message.source);
    }
    return obj;
  },

  create(base?: DeepPartial<Image>): Image {
    return Image.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Image>): Image {
    const message = createBaseImage();
    message.content = object.content ?? Buffer.alloc(0);
    message.source = (object.source !== undefined && object.source !== null)
      ? ImageSource.fromPartial(object.source)
      : undefined;
    return message;
  },
};

function createBaseFaceAnnotation(): FaceAnnotation {
  return {
    boundingPoly: undefined,
    fdBoundingPoly: undefined,
    landmarks: [],
    rollAngle: 0,
    panAngle: 0,
    tiltAngle: 0,
    detectionConfidence: 0,
    landmarkingConfidence: 0,
    joyLikelihood: 0,
    sorrowLikelihood: 0,
    angerLikelihood: 0,
    surpriseLikelihood: 0,
    underExposedLikelihood: 0,
    blurredLikelihood: 0,
    headwearLikelihood: 0,
    recognitionResult: [],
  };
}

export const FaceAnnotation: MessageFns<FaceAnnotation> = {
  encode(message: FaceAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.boundingPoly !== undefined) {
      BoundingPoly.encode(message.boundingPoly, writer.uint32(10).fork()).join();
    }
    if (message.fdBoundingPoly !== undefined) {
      BoundingPoly.encode(message.fdBoundingPoly, writer.uint32(18).fork()).join();
    }
    for (const v of message.landmarks) {
      FaceAnnotation_Landmark.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.rollAngle !== 0) {
      writer.uint32(37).float(message.rollAngle);
    }
    if (message.panAngle !== 0) {
      writer.uint32(45).float(message.panAngle);
    }
    if (message.tiltAngle !== 0) {
      writer.uint32(53).float(message.tiltAngle);
    }
    if (message.detectionConfidence !== 0) {
      writer.uint32(61).float(message.detectionConfidence);
    }
    if (message.landmarkingConfidence !== 0) {
      writer.uint32(69).float(message.landmarkingConfidence);
    }
    if (message.joyLikelihood !== 0) {
      writer.uint32(72).int32(message.joyLikelihood);
    }
    if (message.sorrowLikelihood !== 0) {
      writer.uint32(80).int32(message.sorrowLikelihood);
    }
    if (message.angerLikelihood !== 0) {
      writer.uint32(88).int32(message.angerLikelihood);
    }
    if (message.surpriseLikelihood !== 0) {
      writer.uint32(96).int32(message.surpriseLikelihood);
    }
    if (message.underExposedLikelihood !== 0) {
      writer.uint32(104).int32(message.underExposedLikelihood);
    }
    if (message.blurredLikelihood !== 0) {
      writer.uint32(112).int32(message.blurredLikelihood);
    }
    if (message.headwearLikelihood !== 0) {
      writer.uint32(120).int32(message.headwearLikelihood);
    }
    for (const v of message.recognitionResult) {
      FaceRecognitionResult.encode(v!, writer.uint32(130).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FaceAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFaceAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.boundingPoly = BoundingPoly.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.fdBoundingPoly = BoundingPoly.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.landmarks.push(FaceAnnotation_Landmark.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.rollAngle = reader.float();
          continue;
        case 5:
          if (tag !== 45) {
            break;
          }

          message.panAngle = reader.float();
          continue;
        case 6:
          if (tag !== 53) {
            break;
          }

          message.tiltAngle = reader.float();
          continue;
        case 7:
          if (tag !== 61) {
            break;
          }

          message.detectionConfidence = reader.float();
          continue;
        case 8:
          if (tag !== 69) {
            break;
          }

          message.landmarkingConfidence = reader.float();
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.joyLikelihood = reader.int32() as any;
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.sorrowLikelihood = reader.int32() as any;
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.angerLikelihood = reader.int32() as any;
          continue;
        case 12:
          if (tag !== 96) {
            break;
          }

          message.surpriseLikelihood = reader.int32() as any;
          continue;
        case 13:
          if (tag !== 104) {
            break;
          }

          message.underExposedLikelihood = reader.int32() as any;
          continue;
        case 14:
          if (tag !== 112) {
            break;
          }

          message.blurredLikelihood = reader.int32() as any;
          continue;
        case 15:
          if (tag !== 120) {
            break;
          }

          message.headwearLikelihood = reader.int32() as any;
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.recognitionResult.push(FaceRecognitionResult.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FaceAnnotation {
    return {
      boundingPoly: isSet(object.boundingPoly) ? BoundingPoly.fromJSON(object.boundingPoly) : undefined,
      fdBoundingPoly: isSet(object.fdBoundingPoly) ? BoundingPoly.fromJSON(object.fdBoundingPoly) : undefined,
      landmarks: globalThis.Array.isArray(object?.landmarks)
        ? object.landmarks.map((e: any) => FaceAnnotation_Landmark.fromJSON(e))
        : [],
      rollAngle: isSet(object.rollAngle) ? globalThis.Number(object.rollAngle) : 0,
      panAngle: isSet(object.panAngle) ? globalThis.Number(object.panAngle) : 0,
      tiltAngle: isSet(object.tiltAngle) ? globalThis.Number(object.tiltAngle) : 0,
      detectionConfidence: isSet(object.detectionConfidence) ? globalThis.Number(object.detectionConfidence) : 0,
      landmarkingConfidence: isSet(object.landmarkingConfidence) ? globalThis.Number(object.landmarkingConfidence) : 0,
      joyLikelihood: isSet(object.joyLikelihood) ? likelihoodFromJSON(object.joyLikelihood) : 0,
      sorrowLikelihood: isSet(object.sorrowLikelihood) ? likelihoodFromJSON(object.sorrowLikelihood) : 0,
      angerLikelihood: isSet(object.angerLikelihood) ? likelihoodFromJSON(object.angerLikelihood) : 0,
      surpriseLikelihood: isSet(object.surpriseLikelihood) ? likelihoodFromJSON(object.surpriseLikelihood) : 0,
      underExposedLikelihood: isSet(object.underExposedLikelihood)
        ? likelihoodFromJSON(object.underExposedLikelihood)
        : 0,
      blurredLikelihood: isSet(object.blurredLikelihood) ? likelihoodFromJSON(object.blurredLikelihood) : 0,
      headwearLikelihood: isSet(object.headwearLikelihood) ? likelihoodFromJSON(object.headwearLikelihood) : 0,
      recognitionResult: globalThis.Array.isArray(object?.recognitionResult)
        ? object.recognitionResult.map((e: any) => FaceRecognitionResult.fromJSON(e))
        : [],
    };
  },

  toJSON(message: FaceAnnotation): unknown {
    const obj: any = {};
    if (message.boundingPoly !== undefined) {
      obj.boundingPoly = BoundingPoly.toJSON(message.boundingPoly);
    }
    if (message.fdBoundingPoly !== undefined) {
      obj.fdBoundingPoly = BoundingPoly.toJSON(message.fdBoundingPoly);
    }
    if (message.landmarks?.length) {
      obj.landmarks = message.landmarks.map((e) => FaceAnnotation_Landmark.toJSON(e));
    }
    if (message.rollAngle !== 0) {
      obj.rollAngle = message.rollAngle;
    }
    if (message.panAngle !== 0) {
      obj.panAngle = message.panAngle;
    }
    if (message.tiltAngle !== 0) {
      obj.tiltAngle = message.tiltAngle;
    }
    if (message.detectionConfidence !== 0) {
      obj.detectionConfidence = message.detectionConfidence;
    }
    if (message.landmarkingConfidence !== 0) {
      obj.landmarkingConfidence = message.landmarkingConfidence;
    }
    if (message.joyLikelihood !== 0) {
      obj.joyLikelihood = likelihoodToJSON(message.joyLikelihood);
    }
    if (message.sorrowLikelihood !== 0) {
      obj.sorrowLikelihood = likelihoodToJSON(message.sorrowLikelihood);
    }
    if (message.angerLikelihood !== 0) {
      obj.angerLikelihood = likelihoodToJSON(message.angerLikelihood);
    }
    if (message.surpriseLikelihood !== 0) {
      obj.surpriseLikelihood = likelihoodToJSON(message.surpriseLikelihood);
    }
    if (message.underExposedLikelihood !== 0) {
      obj.underExposedLikelihood = likelihoodToJSON(message.underExposedLikelihood);
    }
    if (message.blurredLikelihood !== 0) {
      obj.blurredLikelihood = likelihoodToJSON(message.blurredLikelihood);
    }
    if (message.headwearLikelihood !== 0) {
      obj.headwearLikelihood = likelihoodToJSON(message.headwearLikelihood);
    }
    if (message.recognitionResult?.length) {
      obj.recognitionResult = message.recognitionResult.map((e) => FaceRecognitionResult.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<FaceAnnotation>): FaceAnnotation {
    return FaceAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FaceAnnotation>): FaceAnnotation {
    const message = createBaseFaceAnnotation();
    message.boundingPoly = (object.boundingPoly !== undefined && object.boundingPoly !== null)
      ? BoundingPoly.fromPartial(object.boundingPoly)
      : undefined;
    message.fdBoundingPoly = (object.fdBoundingPoly !== undefined && object.fdBoundingPoly !== null)
      ? BoundingPoly.fromPartial(object.fdBoundingPoly)
      : undefined;
    message.landmarks = object.landmarks?.map((e) => FaceAnnotation_Landmark.fromPartial(e)) || [];
    message.rollAngle = object.rollAngle ?? 0;
    message.panAngle = object.panAngle ?? 0;
    message.tiltAngle = object.tiltAngle ?? 0;
    message.detectionConfidence = object.detectionConfidence ?? 0;
    message.landmarkingConfidence = object.landmarkingConfidence ?? 0;
    message.joyLikelihood = object.joyLikelihood ?? 0;
    message.sorrowLikelihood = object.sorrowLikelihood ?? 0;
    message.angerLikelihood = object.angerLikelihood ?? 0;
    message.surpriseLikelihood = object.surpriseLikelihood ?? 0;
    message.underExposedLikelihood = object.underExposedLikelihood ?? 0;
    message.blurredLikelihood = object.blurredLikelihood ?? 0;
    message.headwearLikelihood = object.headwearLikelihood ?? 0;
    message.recognitionResult = object.recognitionResult?.map((e) => FaceRecognitionResult.fromPartial(e)) || [];
    return message;
  },
};

function createBaseFaceAnnotation_Landmark(): FaceAnnotation_Landmark {
  return { type: 0, position: undefined };
}

export const FaceAnnotation_Landmark: MessageFns<FaceAnnotation_Landmark> = {
  encode(message: FaceAnnotation_Landmark, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.type !== 0) {
      writer.uint32(24).int32(message.type);
    }
    if (message.position !== undefined) {
      Position.encode(message.position, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FaceAnnotation_Landmark {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFaceAnnotation_Landmark();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 24) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.position = Position.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FaceAnnotation_Landmark {
    return {
      type: isSet(object.type) ? faceAnnotation_Landmark_TypeFromJSON(object.type) : 0,
      position: isSet(object.position) ? Position.fromJSON(object.position) : undefined,
    };
  },

  toJSON(message: FaceAnnotation_Landmark): unknown {
    const obj: any = {};
    if (message.type !== 0) {
      obj.type = faceAnnotation_Landmark_TypeToJSON(message.type);
    }
    if (message.position !== undefined) {
      obj.position = Position.toJSON(message.position);
    }
    return obj;
  },

  create(base?: DeepPartial<FaceAnnotation_Landmark>): FaceAnnotation_Landmark {
    return FaceAnnotation_Landmark.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FaceAnnotation_Landmark>): FaceAnnotation_Landmark {
    const message = createBaseFaceAnnotation_Landmark();
    message.type = object.type ?? 0;
    message.position = (object.position !== undefined && object.position !== null)
      ? Position.fromPartial(object.position)
      : undefined;
    return message;
  },
};

function createBaseLocationInfo(): LocationInfo {
  return { latLng: undefined };
}

export const LocationInfo: MessageFns<LocationInfo> = {
  encode(message: LocationInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.latLng !== undefined) {
      LatLng.encode(message.latLng, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LocationInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLocationInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.latLng = LatLng.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LocationInfo {
    return { latLng: isSet(object.latLng) ? LatLng.fromJSON(object.latLng) : undefined };
  },

  toJSON(message: LocationInfo): unknown {
    const obj: any = {};
    if (message.latLng !== undefined) {
      obj.latLng = LatLng.toJSON(message.latLng);
    }
    return obj;
  },

  create(base?: DeepPartial<LocationInfo>): LocationInfo {
    return LocationInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LocationInfo>): LocationInfo {
    const message = createBaseLocationInfo();
    message.latLng = (object.latLng !== undefined && object.latLng !== null)
      ? LatLng.fromPartial(object.latLng)
      : undefined;
    return message;
  },
};

function createBaseProperty(): Property {
  return { name: "", value: "", uint64Value: Long.UZERO };
}

export const Property: MessageFns<Property> = {
  encode(message: Property, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    if (!message.uint64Value.equals(Long.UZERO)) {
      writer.uint32(24).uint64(message.uint64Value.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Property {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProperty();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.uint64Value = Long.fromString(reader.uint64().toString(), true);
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Property {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
      uint64Value: isSet(object.uint64Value) ? Long.fromValue(object.uint64Value) : Long.UZERO,
    };
  },

  toJSON(message: Property): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    if (!message.uint64Value.equals(Long.UZERO)) {
      obj.uint64Value = (message.uint64Value || Long.UZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<Property>): Property {
    return Property.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Property>): Property {
    const message = createBaseProperty();
    message.name = object.name ?? "";
    message.value = object.value ?? "";
    message.uint64Value = (object.uint64Value !== undefined && object.uint64Value !== null)
      ? Long.fromValue(object.uint64Value)
      : Long.UZERO;
    return message;
  },
};

function createBaseEntityAnnotation(): EntityAnnotation {
  return {
    mid: "",
    locale: "",
    description: "",
    score: 0,
    confidence: 0,
    topicality: 0,
    boundingPoly: undefined,
    locations: [],
    properties: [],
  };
}

export const EntityAnnotation: MessageFns<EntityAnnotation> = {
  encode(message: EntityAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.mid !== "") {
      writer.uint32(10).string(message.mid);
    }
    if (message.locale !== "") {
      writer.uint32(18).string(message.locale);
    }
    if (message.description !== "") {
      writer.uint32(26).string(message.description);
    }
    if (message.score !== 0) {
      writer.uint32(37).float(message.score);
    }
    if (message.confidence !== 0) {
      writer.uint32(45).float(message.confidence);
    }
    if (message.topicality !== 0) {
      writer.uint32(53).float(message.topicality);
    }
    if (message.boundingPoly !== undefined) {
      BoundingPoly.encode(message.boundingPoly, writer.uint32(58).fork()).join();
    }
    for (const v of message.locations) {
      LocationInfo.encode(v!, writer.uint32(66).fork()).join();
    }
    for (const v of message.properties) {
      Property.encode(v!, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EntityAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEntityAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.mid = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.locale = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.description = reader.string();
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.score = reader.float();
          continue;
        case 5:
          if (tag !== 45) {
            break;
          }

          message.confidence = reader.float();
          continue;
        case 6:
          if (tag !== 53) {
            break;
          }

          message.topicality = reader.float();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.boundingPoly = BoundingPoly.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.locations.push(LocationInfo.decode(reader, reader.uint32()));
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.properties.push(Property.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EntityAnnotation {
    return {
      mid: isSet(object.mid) ? globalThis.String(object.mid) : "",
      locale: isSet(object.locale) ? globalThis.String(object.locale) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      score: isSet(object.score) ? globalThis.Number(object.score) : 0,
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
      topicality: isSet(object.topicality) ? globalThis.Number(object.topicality) : 0,
      boundingPoly: isSet(object.boundingPoly) ? BoundingPoly.fromJSON(object.boundingPoly) : undefined,
      locations: globalThis.Array.isArray(object?.locations)
        ? object.locations.map((e: any) => LocationInfo.fromJSON(e))
        : [],
      properties: globalThis.Array.isArray(object?.properties)
        ? object.properties.map((e: any) => Property.fromJSON(e))
        : [],
    };
  },

  toJSON(message: EntityAnnotation): unknown {
    const obj: any = {};
    if (message.mid !== "") {
      obj.mid = message.mid;
    }
    if (message.locale !== "") {
      obj.locale = message.locale;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.score !== 0) {
      obj.score = message.score;
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    if (message.topicality !== 0) {
      obj.topicality = message.topicality;
    }
    if (message.boundingPoly !== undefined) {
      obj.boundingPoly = BoundingPoly.toJSON(message.boundingPoly);
    }
    if (message.locations?.length) {
      obj.locations = message.locations.map((e) => LocationInfo.toJSON(e));
    }
    if (message.properties?.length) {
      obj.properties = message.properties.map((e) => Property.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<EntityAnnotation>): EntityAnnotation {
    return EntityAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EntityAnnotation>): EntityAnnotation {
    const message = createBaseEntityAnnotation();
    message.mid = object.mid ?? "";
    message.locale = object.locale ?? "";
    message.description = object.description ?? "";
    message.score = object.score ?? 0;
    message.confidence = object.confidence ?? 0;
    message.topicality = object.topicality ?? 0;
    message.boundingPoly = (object.boundingPoly !== undefined && object.boundingPoly !== null)
      ? BoundingPoly.fromPartial(object.boundingPoly)
      : undefined;
    message.locations = object.locations?.map((e) => LocationInfo.fromPartial(e)) || [];
    message.properties = object.properties?.map((e) => Property.fromPartial(e)) || [];
    return message;
  },
};

function createBaseLocalizedObjectAnnotation(): LocalizedObjectAnnotation {
  return { mid: "", languageCode: "", name: "", score: 0, boundingPoly: undefined };
}

export const LocalizedObjectAnnotation: MessageFns<LocalizedObjectAnnotation> = {
  encode(message: LocalizedObjectAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.mid !== "") {
      writer.uint32(10).string(message.mid);
    }
    if (message.languageCode !== "") {
      writer.uint32(18).string(message.languageCode);
    }
    if (message.name !== "") {
      writer.uint32(26).string(message.name);
    }
    if (message.score !== 0) {
      writer.uint32(37).float(message.score);
    }
    if (message.boundingPoly !== undefined) {
      BoundingPoly.encode(message.boundingPoly, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LocalizedObjectAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLocalizedObjectAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.mid = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.languageCode = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.name = reader.string();
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.score = reader.float();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.boundingPoly = BoundingPoly.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LocalizedObjectAnnotation {
    return {
      mid: isSet(object.mid) ? globalThis.String(object.mid) : "",
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      score: isSet(object.score) ? globalThis.Number(object.score) : 0,
      boundingPoly: isSet(object.boundingPoly) ? BoundingPoly.fromJSON(object.boundingPoly) : undefined,
    };
  },

  toJSON(message: LocalizedObjectAnnotation): unknown {
    const obj: any = {};
    if (message.mid !== "") {
      obj.mid = message.mid;
    }
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.score !== 0) {
      obj.score = message.score;
    }
    if (message.boundingPoly !== undefined) {
      obj.boundingPoly = BoundingPoly.toJSON(message.boundingPoly);
    }
    return obj;
  },

  create(base?: DeepPartial<LocalizedObjectAnnotation>): LocalizedObjectAnnotation {
    return LocalizedObjectAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LocalizedObjectAnnotation>): LocalizedObjectAnnotation {
    const message = createBaseLocalizedObjectAnnotation();
    message.mid = object.mid ?? "";
    message.languageCode = object.languageCode ?? "";
    message.name = object.name ?? "";
    message.score = object.score ?? 0;
    message.boundingPoly = (object.boundingPoly !== undefined && object.boundingPoly !== null)
      ? BoundingPoly.fromPartial(object.boundingPoly)
      : undefined;
    return message;
  },
};

function createBaseSafeSearchAnnotation(): SafeSearchAnnotation {
  return { adult: 0, spoof: 0, medical: 0, violence: 0, racy: 0 };
}

export const SafeSearchAnnotation: MessageFns<SafeSearchAnnotation> = {
  encode(message: SafeSearchAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.adult !== 0) {
      writer.uint32(8).int32(message.adult);
    }
    if (message.spoof !== 0) {
      writer.uint32(16).int32(message.spoof);
    }
    if (message.medical !== 0) {
      writer.uint32(24).int32(message.medical);
    }
    if (message.violence !== 0) {
      writer.uint32(32).int32(message.violence);
    }
    if (message.racy !== 0) {
      writer.uint32(72).int32(message.racy);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SafeSearchAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSafeSearchAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.adult = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.spoof = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.medical = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.violence = reader.int32() as any;
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.racy = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SafeSearchAnnotation {
    return {
      adult: isSet(object.adult) ? likelihoodFromJSON(object.adult) : 0,
      spoof: isSet(object.spoof) ? likelihoodFromJSON(object.spoof) : 0,
      medical: isSet(object.medical) ? likelihoodFromJSON(object.medical) : 0,
      violence: isSet(object.violence) ? likelihoodFromJSON(object.violence) : 0,
      racy: isSet(object.racy) ? likelihoodFromJSON(object.racy) : 0,
    };
  },

  toJSON(message: SafeSearchAnnotation): unknown {
    const obj: any = {};
    if (message.adult !== 0) {
      obj.adult = likelihoodToJSON(message.adult);
    }
    if (message.spoof !== 0) {
      obj.spoof = likelihoodToJSON(message.spoof);
    }
    if (message.medical !== 0) {
      obj.medical = likelihoodToJSON(message.medical);
    }
    if (message.violence !== 0) {
      obj.violence = likelihoodToJSON(message.violence);
    }
    if (message.racy !== 0) {
      obj.racy = likelihoodToJSON(message.racy);
    }
    return obj;
  },

  create(base?: DeepPartial<SafeSearchAnnotation>): SafeSearchAnnotation {
    return SafeSearchAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SafeSearchAnnotation>): SafeSearchAnnotation {
    const message = createBaseSafeSearchAnnotation();
    message.adult = object.adult ?? 0;
    message.spoof = object.spoof ?? 0;
    message.medical = object.medical ?? 0;
    message.violence = object.violence ?? 0;
    message.racy = object.racy ?? 0;
    return message;
  },
};

function createBaseLatLongRect(): LatLongRect {
  return { minLatLng: undefined, maxLatLng: undefined };
}

export const LatLongRect: MessageFns<LatLongRect> = {
  encode(message: LatLongRect, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.minLatLng !== undefined) {
      LatLng.encode(message.minLatLng, writer.uint32(10).fork()).join();
    }
    if (message.maxLatLng !== undefined) {
      LatLng.encode(message.maxLatLng, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LatLongRect {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLatLongRect();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.minLatLng = LatLng.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.maxLatLng = LatLng.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LatLongRect {
    return {
      minLatLng: isSet(object.minLatLng) ? LatLng.fromJSON(object.minLatLng) : undefined,
      maxLatLng: isSet(object.maxLatLng) ? LatLng.fromJSON(object.maxLatLng) : undefined,
    };
  },

  toJSON(message: LatLongRect): unknown {
    const obj: any = {};
    if (message.minLatLng !== undefined) {
      obj.minLatLng = LatLng.toJSON(message.minLatLng);
    }
    if (message.maxLatLng !== undefined) {
      obj.maxLatLng = LatLng.toJSON(message.maxLatLng);
    }
    return obj;
  },

  create(base?: DeepPartial<LatLongRect>): LatLongRect {
    return LatLongRect.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LatLongRect>): LatLongRect {
    const message = createBaseLatLongRect();
    message.minLatLng = (object.minLatLng !== undefined && object.minLatLng !== null)
      ? LatLng.fromPartial(object.minLatLng)
      : undefined;
    message.maxLatLng = (object.maxLatLng !== undefined && object.maxLatLng !== null)
      ? LatLng.fromPartial(object.maxLatLng)
      : undefined;
    return message;
  },
};

function createBaseColorInfo(): ColorInfo {
  return { color: undefined, score: 0, pixelFraction: 0 };
}

export const ColorInfo: MessageFns<ColorInfo> = {
  encode(message: ColorInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.color !== undefined) {
      Color.encode(message.color, writer.uint32(10).fork()).join();
    }
    if (message.score !== 0) {
      writer.uint32(21).float(message.score);
    }
    if (message.pixelFraction !== 0) {
      writer.uint32(29).float(message.pixelFraction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ColorInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseColorInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.color = Color.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.score = reader.float();
          continue;
        case 3:
          if (tag !== 29) {
            break;
          }

          message.pixelFraction = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ColorInfo {
    return {
      color: isSet(object.color) ? Color.fromJSON(object.color) : undefined,
      score: isSet(object.score) ? globalThis.Number(object.score) : 0,
      pixelFraction: isSet(object.pixelFraction) ? globalThis.Number(object.pixelFraction) : 0,
    };
  },

  toJSON(message: ColorInfo): unknown {
    const obj: any = {};
    if (message.color !== undefined) {
      obj.color = Color.toJSON(message.color);
    }
    if (message.score !== 0) {
      obj.score = message.score;
    }
    if (message.pixelFraction !== 0) {
      obj.pixelFraction = message.pixelFraction;
    }
    return obj;
  },

  create(base?: DeepPartial<ColorInfo>): ColorInfo {
    return ColorInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ColorInfo>): ColorInfo {
    const message = createBaseColorInfo();
    message.color = (object.color !== undefined && object.color !== null) ? Color.fromPartial(object.color) : undefined;
    message.score = object.score ?? 0;
    message.pixelFraction = object.pixelFraction ?? 0;
    return message;
  },
};

function createBaseDominantColorsAnnotation(): DominantColorsAnnotation {
  return { colors: [] };
}

export const DominantColorsAnnotation: MessageFns<DominantColorsAnnotation> = {
  encode(message: DominantColorsAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.colors) {
      ColorInfo.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DominantColorsAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDominantColorsAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.colors.push(ColorInfo.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DominantColorsAnnotation {
    return {
      colors: globalThis.Array.isArray(object?.colors) ? object.colors.map((e: any) => ColorInfo.fromJSON(e)) : [],
    };
  },

  toJSON(message: DominantColorsAnnotation): unknown {
    const obj: any = {};
    if (message.colors?.length) {
      obj.colors = message.colors.map((e) => ColorInfo.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<DominantColorsAnnotation>): DominantColorsAnnotation {
    return DominantColorsAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DominantColorsAnnotation>): DominantColorsAnnotation {
    const message = createBaseDominantColorsAnnotation();
    message.colors = object.colors?.map((e) => ColorInfo.fromPartial(e)) || [];
    return message;
  },
};

function createBaseImageProperties(): ImageProperties {
  return { dominantColors: undefined };
}

export const ImageProperties: MessageFns<ImageProperties> = {
  encode(message: ImageProperties, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dominantColors !== undefined) {
      DominantColorsAnnotation.encode(message.dominantColors, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImageProperties {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImageProperties();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dominantColors = DominantColorsAnnotation.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImageProperties {
    return {
      dominantColors: isSet(object.dominantColors)
        ? DominantColorsAnnotation.fromJSON(object.dominantColors)
        : undefined,
    };
  },

  toJSON(message: ImageProperties): unknown {
    const obj: any = {};
    if (message.dominantColors !== undefined) {
      obj.dominantColors = DominantColorsAnnotation.toJSON(message.dominantColors);
    }
    return obj;
  },

  create(base?: DeepPartial<ImageProperties>): ImageProperties {
    return ImageProperties.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImageProperties>): ImageProperties {
    const message = createBaseImageProperties();
    message.dominantColors = (object.dominantColors !== undefined && object.dominantColors !== null)
      ? DominantColorsAnnotation.fromPartial(object.dominantColors)
      : undefined;
    return message;
  },
};

function createBaseCropHint(): CropHint {
  return { boundingPoly: undefined, confidence: 0, importanceFraction: 0 };
}

export const CropHint: MessageFns<CropHint> = {
  encode(message: CropHint, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.boundingPoly !== undefined) {
      BoundingPoly.encode(message.boundingPoly, writer.uint32(10).fork()).join();
    }
    if (message.confidence !== 0) {
      writer.uint32(21).float(message.confidence);
    }
    if (message.importanceFraction !== 0) {
      writer.uint32(29).float(message.importanceFraction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CropHint {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCropHint();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.boundingPoly = BoundingPoly.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.confidence = reader.float();
          continue;
        case 3:
          if (tag !== 29) {
            break;
          }

          message.importanceFraction = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CropHint {
    return {
      boundingPoly: isSet(object.boundingPoly) ? BoundingPoly.fromJSON(object.boundingPoly) : undefined,
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
      importanceFraction: isSet(object.importanceFraction) ? globalThis.Number(object.importanceFraction) : 0,
    };
  },

  toJSON(message: CropHint): unknown {
    const obj: any = {};
    if (message.boundingPoly !== undefined) {
      obj.boundingPoly = BoundingPoly.toJSON(message.boundingPoly);
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    if (message.importanceFraction !== 0) {
      obj.importanceFraction = message.importanceFraction;
    }
    return obj;
  },

  create(base?: DeepPartial<CropHint>): CropHint {
    return CropHint.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CropHint>): CropHint {
    const message = createBaseCropHint();
    message.boundingPoly = (object.boundingPoly !== undefined && object.boundingPoly !== null)
      ? BoundingPoly.fromPartial(object.boundingPoly)
      : undefined;
    message.confidence = object.confidence ?? 0;
    message.importanceFraction = object.importanceFraction ?? 0;
    return message;
  },
};

function createBaseCropHintsAnnotation(): CropHintsAnnotation {
  return { cropHints: [] };
}

export const CropHintsAnnotation: MessageFns<CropHintsAnnotation> = {
  encode(message: CropHintsAnnotation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.cropHints) {
      CropHint.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CropHintsAnnotation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCropHintsAnnotation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.cropHints.push(CropHint.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CropHintsAnnotation {
    return {
      cropHints: globalThis.Array.isArray(object?.cropHints)
        ? object.cropHints.map((e: any) => CropHint.fromJSON(e))
        : [],
    };
  },

  toJSON(message: CropHintsAnnotation): unknown {
    const obj: any = {};
    if (message.cropHints?.length) {
      obj.cropHints = message.cropHints.map((e) => CropHint.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<CropHintsAnnotation>): CropHintsAnnotation {
    return CropHintsAnnotation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CropHintsAnnotation>): CropHintsAnnotation {
    const message = createBaseCropHintsAnnotation();
    message.cropHints = object.cropHints?.map((e) => CropHint.fromPartial(e)) || [];
    return message;
  },
};

function createBaseCropHintsParams(): CropHintsParams {
  return { aspectRatios: [] };
}

export const CropHintsParams: MessageFns<CropHintsParams> = {
  encode(message: CropHintsParams, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    writer.uint32(10).fork();
    for (const v of message.aspectRatios) {
      writer.float(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CropHintsParams {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCropHintsParams();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag === 13) {
            message.aspectRatios.push(reader.float());

            continue;
          }

          if (tag === 10) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.aspectRatios.push(reader.float());
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CropHintsParams {
    return {
      aspectRatios: globalThis.Array.isArray(object?.aspectRatios)
        ? object.aspectRatios.map((e: any) => globalThis.Number(e))
        : [],
    };
  },

  toJSON(message: CropHintsParams): unknown {
    const obj: any = {};
    if (message.aspectRatios?.length) {
      obj.aspectRatios = message.aspectRatios;
    }
    return obj;
  },

  create(base?: DeepPartial<CropHintsParams>): CropHintsParams {
    return CropHintsParams.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CropHintsParams>): CropHintsParams {
    const message = createBaseCropHintsParams();
    message.aspectRatios = object.aspectRatios?.map((e) => e) || [];
    return message;
  },
};

function createBaseWebDetectionParams(): WebDetectionParams {
  return { includeGeoResults: false };
}

export const WebDetectionParams: MessageFns<WebDetectionParams> = {
  encode(message: WebDetectionParams, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.includeGeoResults !== false) {
      writer.uint32(16).bool(message.includeGeoResults);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): WebDetectionParams {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseWebDetectionParams();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 16) {
            break;
          }

          message.includeGeoResults = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): WebDetectionParams {
    return {
      includeGeoResults: isSet(object.includeGeoResults) ? globalThis.Boolean(object.includeGeoResults) : false,
    };
  },

  toJSON(message: WebDetectionParams): unknown {
    const obj: any = {};
    if (message.includeGeoResults !== false) {
      obj.includeGeoResults = message.includeGeoResults;
    }
    return obj;
  },

  create(base?: DeepPartial<WebDetectionParams>): WebDetectionParams {
    return WebDetectionParams.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<WebDetectionParams>): WebDetectionParams {
    const message = createBaseWebDetectionParams();
    message.includeGeoResults = object.includeGeoResults ?? false;
    return message;
  },
};

function createBaseTextDetectionParams(): TextDetectionParams {
  return { enableTextDetectionConfidenceScore: false, advancedOcrOptions: [] };
}

export const TextDetectionParams: MessageFns<TextDetectionParams> = {
  encode(message: TextDetectionParams, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.enableTextDetectionConfidenceScore !== false) {
      writer.uint32(72).bool(message.enableTextDetectionConfidenceScore);
    }
    for (const v of message.advancedOcrOptions) {
      writer.uint32(90).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TextDetectionParams {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextDetectionParams();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 9:
          if (tag !== 72) {
            break;
          }

          message.enableTextDetectionConfidenceScore = reader.bool();
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.advancedOcrOptions.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextDetectionParams {
    return {
      enableTextDetectionConfidenceScore: isSet(object.enableTextDetectionConfidenceScore)
        ? globalThis.Boolean(object.enableTextDetectionConfidenceScore)
        : false,
      advancedOcrOptions: globalThis.Array.isArray(object?.advancedOcrOptions)
        ? object.advancedOcrOptions.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: TextDetectionParams): unknown {
    const obj: any = {};
    if (message.enableTextDetectionConfidenceScore !== false) {
      obj.enableTextDetectionConfidenceScore = message.enableTextDetectionConfidenceScore;
    }
    if (message.advancedOcrOptions?.length) {
      obj.advancedOcrOptions = message.advancedOcrOptions;
    }
    return obj;
  },

  create(base?: DeepPartial<TextDetectionParams>): TextDetectionParams {
    return TextDetectionParams.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TextDetectionParams>): TextDetectionParams {
    const message = createBaseTextDetectionParams();
    message.enableTextDetectionConfidenceScore = object.enableTextDetectionConfidenceScore ?? false;
    message.advancedOcrOptions = object.advancedOcrOptions?.map((e) => e) || [];
    return message;
  },
};

function createBaseImageContext(): ImageContext {
  return {
    latLongRect: undefined,
    languageHints: [],
    cropHintsParams: undefined,
    faceRecognitionParams: undefined,
    productSearchParams: undefined,
    webDetectionParams: undefined,
    textDetectionParams: undefined,
  };
}

export const ImageContext: MessageFns<ImageContext> = {
  encode(message: ImageContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.latLongRect !== undefined) {
      LatLongRect.encode(message.latLongRect, writer.uint32(10).fork()).join();
    }
    for (const v of message.languageHints) {
      writer.uint32(18).string(v!);
    }
    if (message.cropHintsParams !== undefined) {
      CropHintsParams.encode(message.cropHintsParams, writer.uint32(34).fork()).join();
    }
    if (message.faceRecognitionParams !== undefined) {
      FaceRecognitionParams.encode(message.faceRecognitionParams, writer.uint32(82).fork()).join();
    }
    if (message.productSearchParams !== undefined) {
      ProductSearchParams.encode(message.productSearchParams, writer.uint32(42).fork()).join();
    }
    if (message.webDetectionParams !== undefined) {
      WebDetectionParams.encode(message.webDetectionParams, writer.uint32(50).fork()).join();
    }
    if (message.textDetectionParams !== undefined) {
      TextDetectionParams.encode(message.textDetectionParams, writer.uint32(98).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImageContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImageContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.latLongRect = LatLongRect.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.languageHints.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.cropHintsParams = CropHintsParams.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.faceRecognitionParams = FaceRecognitionParams.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.productSearchParams = ProductSearchParams.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.webDetectionParams = WebDetectionParams.decode(reader, reader.uint32());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.textDetectionParams = TextDetectionParams.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImageContext {
    return {
      latLongRect: isSet(object.latLongRect) ? LatLongRect.fromJSON(object.latLongRect) : undefined,
      languageHints: globalThis.Array.isArray(object?.languageHints)
        ? object.languageHints.map((e: any) => globalThis.String(e))
        : [],
      cropHintsParams: isSet(object.cropHintsParams) ? CropHintsParams.fromJSON(object.cropHintsParams) : undefined,
      faceRecognitionParams: isSet(object.faceRecognitionParams)
        ? FaceRecognitionParams.fromJSON(object.faceRecognitionParams)
        : undefined,
      productSearchParams: isSet(object.productSearchParams)
        ? ProductSearchParams.fromJSON(object.productSearchParams)
        : undefined,
      webDetectionParams: isSet(object.webDetectionParams)
        ? WebDetectionParams.fromJSON(object.webDetectionParams)
        : undefined,
      textDetectionParams: isSet(object.textDetectionParams)
        ? TextDetectionParams.fromJSON(object.textDetectionParams)
        : undefined,
    };
  },

  toJSON(message: ImageContext): unknown {
    const obj: any = {};
    if (message.latLongRect !== undefined) {
      obj.latLongRect = LatLongRect.toJSON(message.latLongRect);
    }
    if (message.languageHints?.length) {
      obj.languageHints = message.languageHints;
    }
    if (message.cropHintsParams !== undefined) {
      obj.cropHintsParams = CropHintsParams.toJSON(message.cropHintsParams);
    }
    if (message.faceRecognitionParams !== undefined) {
      obj.faceRecognitionParams = FaceRecognitionParams.toJSON(message.faceRecognitionParams);
    }
    if (message.productSearchParams !== undefined) {
      obj.productSearchParams = ProductSearchParams.toJSON(message.productSearchParams);
    }
    if (message.webDetectionParams !== undefined) {
      obj.webDetectionParams = WebDetectionParams.toJSON(message.webDetectionParams);
    }
    if (message.textDetectionParams !== undefined) {
      obj.textDetectionParams = TextDetectionParams.toJSON(message.textDetectionParams);
    }
    return obj;
  },

  create(base?: DeepPartial<ImageContext>): ImageContext {
    return ImageContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImageContext>): ImageContext {
    const message = createBaseImageContext();
    message.latLongRect = (object.latLongRect !== undefined && object.latLongRect !== null)
      ? LatLongRect.fromPartial(object.latLongRect)
      : undefined;
    message.languageHints = object.languageHints?.map((e) => e) || [];
    message.cropHintsParams = (object.cropHintsParams !== undefined && object.cropHintsParams !== null)
      ? CropHintsParams.fromPartial(object.cropHintsParams)
      : undefined;
    message.faceRecognitionParams =
      (object.faceRecognitionParams !== undefined && object.faceRecognitionParams !== null)
        ? FaceRecognitionParams.fromPartial(object.faceRecognitionParams)
        : undefined;
    message.productSearchParams = (object.productSearchParams !== undefined && object.productSearchParams !== null)
      ? ProductSearchParams.fromPartial(object.productSearchParams)
      : undefined;
    message.webDetectionParams = (object.webDetectionParams !== undefined && object.webDetectionParams !== null)
      ? WebDetectionParams.fromPartial(object.webDetectionParams)
      : undefined;
    message.textDetectionParams = (object.textDetectionParams !== undefined && object.textDetectionParams !== null)
      ? TextDetectionParams.fromPartial(object.textDetectionParams)
      : undefined;
    return message;
  },
};

function createBaseAnnotateImageRequest(): AnnotateImageRequest {
  return { image: undefined, features: [], imageContext: undefined };
}

export const AnnotateImageRequest: MessageFns<AnnotateImageRequest> = {
  encode(message: AnnotateImageRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.image !== undefined) {
      Image.encode(message.image, writer.uint32(10).fork()).join();
    }
    for (const v of message.features) {
      Feature.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.imageContext !== undefined) {
      ImageContext.encode(message.imageContext, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnnotateImageRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnnotateImageRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.image = Image.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.features.push(Feature.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.imageContext = ImageContext.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnnotateImageRequest {
    return {
      image: isSet(object.image) ? Image.fromJSON(object.image) : undefined,
      features: globalThis.Array.isArray(object?.features) ? object.features.map((e: any) => Feature.fromJSON(e)) : [],
      imageContext: isSet(object.imageContext) ? ImageContext.fromJSON(object.imageContext) : undefined,
    };
  },

  toJSON(message: AnnotateImageRequest): unknown {
    const obj: any = {};
    if (message.image !== undefined) {
      obj.image = Image.toJSON(message.image);
    }
    if (message.features?.length) {
      obj.features = message.features.map((e) => Feature.toJSON(e));
    }
    if (message.imageContext !== undefined) {
      obj.imageContext = ImageContext.toJSON(message.imageContext);
    }
    return obj;
  },

  create(base?: DeepPartial<AnnotateImageRequest>): AnnotateImageRequest {
    return AnnotateImageRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AnnotateImageRequest>): AnnotateImageRequest {
    const message = createBaseAnnotateImageRequest();
    message.image = (object.image !== undefined && object.image !== null) ? Image.fromPartial(object.image) : undefined;
    message.features = object.features?.map((e) => Feature.fromPartial(e)) || [];
    message.imageContext = (object.imageContext !== undefined && object.imageContext !== null)
      ? ImageContext.fromPartial(object.imageContext)
      : undefined;
    return message;
  },
};

function createBaseImageAnnotationContext(): ImageAnnotationContext {
  return { uri: "", pageNumber: 0 };
}

export const ImageAnnotationContext: MessageFns<ImageAnnotationContext> = {
  encode(message: ImageAnnotationContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.uri !== "") {
      writer.uint32(10).string(message.uri);
    }
    if (message.pageNumber !== 0) {
      writer.uint32(16).int32(message.pageNumber);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImageAnnotationContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImageAnnotationContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.uri = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pageNumber = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImageAnnotationContext {
    return {
      uri: isSet(object.uri) ? globalThis.String(object.uri) : "",
      pageNumber: isSet(object.pageNumber) ? globalThis.Number(object.pageNumber) : 0,
    };
  },

  toJSON(message: ImageAnnotationContext): unknown {
    const obj: any = {};
    if (message.uri !== "") {
      obj.uri = message.uri;
    }
    if (message.pageNumber !== 0) {
      obj.pageNumber = Math.round(message.pageNumber);
    }
    return obj;
  },

  create(base?: DeepPartial<ImageAnnotationContext>): ImageAnnotationContext {
    return ImageAnnotationContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImageAnnotationContext>): ImageAnnotationContext {
    const message = createBaseImageAnnotationContext();
    message.uri = object.uri ?? "";
    message.pageNumber = object.pageNumber ?? 0;
    return message;
  },
};

function createBaseAnnotateImageResponse(): AnnotateImageResponse {
  return {
    faceAnnotations: [],
    landmarkAnnotations: [],
    logoAnnotations: [],
    labelAnnotations: [],
    localizedObjectAnnotations: [],
    textAnnotations: [],
    fullTextAnnotation: undefined,
    safeSearchAnnotation: undefined,
    imagePropertiesAnnotation: undefined,
    cropHintsAnnotation: undefined,
    webDetection: undefined,
    productSearchResults: undefined,
    error: undefined,
    context: undefined,
  };
}

export const AnnotateImageResponse: MessageFns<AnnotateImageResponse> = {
  encode(message: AnnotateImageResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.faceAnnotations) {
      FaceAnnotation.encode(v!, writer.uint32(10).fork()).join();
    }
    for (const v of message.landmarkAnnotations) {
      EntityAnnotation.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.logoAnnotations) {
      EntityAnnotation.encode(v!, writer.uint32(26).fork()).join();
    }
    for (const v of message.labelAnnotations) {
      EntityAnnotation.encode(v!, writer.uint32(34).fork()).join();
    }
    for (const v of message.localizedObjectAnnotations) {
      LocalizedObjectAnnotation.encode(v!, writer.uint32(178).fork()).join();
    }
    for (const v of message.textAnnotations) {
      EntityAnnotation.encode(v!, writer.uint32(42).fork()).join();
    }
    if (message.fullTextAnnotation !== undefined) {
      TextAnnotation.encode(message.fullTextAnnotation, writer.uint32(98).fork()).join();
    }
    if (message.safeSearchAnnotation !== undefined) {
      SafeSearchAnnotation.encode(message.safeSearchAnnotation, writer.uint32(50).fork()).join();
    }
    if (message.imagePropertiesAnnotation !== undefined) {
      ImageProperties.encode(message.imagePropertiesAnnotation, writer.uint32(66).fork()).join();
    }
    if (message.cropHintsAnnotation !== undefined) {
      CropHintsAnnotation.encode(message.cropHintsAnnotation, writer.uint32(90).fork()).join();
    }
    if (message.webDetection !== undefined) {
      WebDetection.encode(message.webDetection, writer.uint32(106).fork()).join();
    }
    if (message.productSearchResults !== undefined) {
      ProductSearchResults.encode(message.productSearchResults, writer.uint32(114).fork()).join();
    }
    if (message.error !== undefined) {
      Status.encode(message.error, writer.uint32(74).fork()).join();
    }
    if (message.context !== undefined) {
      ImageAnnotationContext.encode(message.context, writer.uint32(170).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnnotateImageResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnnotateImageResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.faceAnnotations.push(FaceAnnotation.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.landmarkAnnotations.push(EntityAnnotation.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.logoAnnotations.push(EntityAnnotation.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.labelAnnotations.push(EntityAnnotation.decode(reader, reader.uint32()));
          continue;
        case 22:
          if (tag !== 178) {
            break;
          }

          message.localizedObjectAnnotations.push(LocalizedObjectAnnotation.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.textAnnotations.push(EntityAnnotation.decode(reader, reader.uint32()));
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.fullTextAnnotation = TextAnnotation.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.safeSearchAnnotation = SafeSearchAnnotation.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.imagePropertiesAnnotation = ImageProperties.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.cropHintsAnnotation = CropHintsAnnotation.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.webDetection = WebDetection.decode(reader, reader.uint32());
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.productSearchResults = ProductSearchResults.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.error = Status.decode(reader, reader.uint32());
          continue;
        case 21:
          if (tag !== 170) {
            break;
          }

          message.context = ImageAnnotationContext.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnnotateImageResponse {
    return {
      faceAnnotations: globalThis.Array.isArray(object?.faceAnnotations)
        ? object.faceAnnotations.map((e: any) => FaceAnnotation.fromJSON(e))
        : [],
      landmarkAnnotations: globalThis.Array.isArray(object?.landmarkAnnotations)
        ? object.landmarkAnnotations.map((e: any) => EntityAnnotation.fromJSON(e))
        : [],
      logoAnnotations: globalThis.Array.isArray(object?.logoAnnotations)
        ? object.logoAnnotations.map((e: any) => EntityAnnotation.fromJSON(e))
        : [],
      labelAnnotations: globalThis.Array.isArray(object?.labelAnnotations)
        ? object.labelAnnotations.map((e: any) => EntityAnnotation.fromJSON(e))
        : [],
      localizedObjectAnnotations: globalThis.Array.isArray(object?.localizedObjectAnnotations)
        ? object.localizedObjectAnnotations.map((e: any) => LocalizedObjectAnnotation.fromJSON(e))
        : [],
      textAnnotations: globalThis.Array.isArray(object?.textAnnotations)
        ? object.textAnnotations.map((e: any) => EntityAnnotation.fromJSON(e))
        : [],
      fullTextAnnotation: isSet(object.fullTextAnnotation)
        ? TextAnnotation.fromJSON(object.fullTextAnnotation)
        : undefined,
      safeSearchAnnotation: isSet(object.safeSearchAnnotation)
        ? SafeSearchAnnotation.fromJSON(object.safeSearchAnnotation)
        : undefined,
      imagePropertiesAnnotation: isSet(object.imagePropertiesAnnotation)
        ? ImageProperties.fromJSON(object.imagePropertiesAnnotation)
        : undefined,
      cropHintsAnnotation: isSet(object.cropHintsAnnotation)
        ? CropHintsAnnotation.fromJSON(object.cropHintsAnnotation)
        : undefined,
      webDetection: isSet(object.webDetection) ? WebDetection.fromJSON(object.webDetection) : undefined,
      productSearchResults: isSet(object.productSearchResults)
        ? ProductSearchResults.fromJSON(object.productSearchResults)
        : undefined,
      error: isSet(object.error) ? Status.fromJSON(object.error) : undefined,
      context: isSet(object.context) ? ImageAnnotationContext.fromJSON(object.context) : undefined,
    };
  },

  toJSON(message: AnnotateImageResponse): unknown {
    const obj: any = {};
    if (message.faceAnnotations?.length) {
      obj.faceAnnotations = message.faceAnnotations.map((e) => FaceAnnotation.toJSON(e));
    }
    if (message.landmarkAnnotations?.length) {
      obj.landmarkAnnotations = message.landmarkAnnotations.map((e) => EntityAnnotation.toJSON(e));
    }
    if (message.logoAnnotations?.length) {
      obj.logoAnnotations = message.logoAnnotations.map((e) => EntityAnnotation.toJSON(e));
    }
    if (message.labelAnnotations?.length) {
      obj.labelAnnotations = message.labelAnnotations.map((e) => EntityAnnotation.toJSON(e));
    }
    if (message.localizedObjectAnnotations?.length) {
      obj.localizedObjectAnnotations = message.localizedObjectAnnotations.map((e) =>
        LocalizedObjectAnnotation.toJSON(e)
      );
    }
    if (message.textAnnotations?.length) {
      obj.textAnnotations = message.textAnnotations.map((e) => EntityAnnotation.toJSON(e));
    }
    if (message.fullTextAnnotation !== undefined) {
      obj.fullTextAnnotation = TextAnnotation.toJSON(message.fullTextAnnotation);
    }
    if (message.safeSearchAnnotation !== undefined) {
      obj.safeSearchAnnotation = SafeSearchAnnotation.toJSON(message.safeSearchAnnotation);
    }
    if (message.imagePropertiesAnnotation !== undefined) {
      obj.imagePropertiesAnnotation = ImageProperties.toJSON(message.imagePropertiesAnnotation);
    }
    if (message.cropHintsAnnotation !== undefined) {
      obj.cropHintsAnnotation = CropHintsAnnotation.toJSON(message.cropHintsAnnotation);
    }
    if (message.webDetection !== undefined) {
      obj.webDetection = WebDetection.toJSON(message.webDetection);
    }
    if (message.productSearchResults !== undefined) {
      obj.productSearchResults = ProductSearchResults.toJSON(message.productSearchResults);
    }
    if (message.error !== undefined) {
      obj.error = Status.toJSON(message.error);
    }
    if (message.context !== undefined) {
      obj.context = ImageAnnotationContext.toJSON(message.context);
    }
    return obj;
  },

  create(base?: DeepPartial<AnnotateImageResponse>): AnnotateImageResponse {
    return AnnotateImageResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AnnotateImageResponse>): AnnotateImageResponse {
    const message = createBaseAnnotateImageResponse();
    message.faceAnnotations = object.faceAnnotations?.map((e) => FaceAnnotation.fromPartial(e)) || [];
    message.landmarkAnnotations = object.landmarkAnnotations?.map((e) => EntityAnnotation.fromPartial(e)) || [];
    message.logoAnnotations = object.logoAnnotations?.map((e) => EntityAnnotation.fromPartial(e)) || [];
    message.labelAnnotations = object.labelAnnotations?.map((e) => EntityAnnotation.fromPartial(e)) || [];
    message.localizedObjectAnnotations =
      object.localizedObjectAnnotations?.map((e) => LocalizedObjectAnnotation.fromPartial(e)) || [];
    message.textAnnotations = object.textAnnotations?.map((e) => EntityAnnotation.fromPartial(e)) || [];
    message.fullTextAnnotation = (object.fullTextAnnotation !== undefined && object.fullTextAnnotation !== null)
      ? TextAnnotation.fromPartial(object.fullTextAnnotation)
      : undefined;
    message.safeSearchAnnotation = (object.safeSearchAnnotation !== undefined && object.safeSearchAnnotation !== null)
      ? SafeSearchAnnotation.fromPartial(object.safeSearchAnnotation)
      : undefined;
    message.imagePropertiesAnnotation =
      (object.imagePropertiesAnnotation !== undefined && object.imagePropertiesAnnotation !== null)
        ? ImageProperties.fromPartial(object.imagePropertiesAnnotation)
        : undefined;
    message.cropHintsAnnotation = (object.cropHintsAnnotation !== undefined && object.cropHintsAnnotation !== null)
      ? CropHintsAnnotation.fromPartial(object.cropHintsAnnotation)
      : undefined;
    message.webDetection = (object.webDetection !== undefined && object.webDetection !== null)
      ? WebDetection.fromPartial(object.webDetection)
      : undefined;
    message.productSearchResults = (object.productSearchResults !== undefined && object.productSearchResults !== null)
      ? ProductSearchResults.fromPartial(object.productSearchResults)
      : undefined;
    message.error = (object.error !== undefined && object.error !== null)
      ? Status.fromPartial(object.error)
      : undefined;
    message.context = (object.context !== undefined && object.context !== null)
      ? ImageAnnotationContext.fromPartial(object.context)
      : undefined;
    return message;
  },
};

function createBaseBatchAnnotateImagesRequest(): BatchAnnotateImagesRequest {
  return { requests: [] };
}

export const BatchAnnotateImagesRequest: MessageFns<BatchAnnotateImagesRequest> = {
  encode(message: BatchAnnotateImagesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.requests) {
      AnnotateImageRequest.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchAnnotateImagesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchAnnotateImagesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.requests.push(AnnotateImageRequest.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchAnnotateImagesRequest {
    return {
      requests: globalThis.Array.isArray(object?.requests)
        ? object.requests.map((e: any) => AnnotateImageRequest.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchAnnotateImagesRequest): unknown {
    const obj: any = {};
    if (message.requests?.length) {
      obj.requests = message.requests.map((e) => AnnotateImageRequest.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchAnnotateImagesRequest>): BatchAnnotateImagesRequest {
    return BatchAnnotateImagesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchAnnotateImagesRequest>): BatchAnnotateImagesRequest {
    const message = createBaseBatchAnnotateImagesRequest();
    message.requests = object.requests?.map((e) => AnnotateImageRequest.fromPartial(e)) || [];
    return message;
  },
};

function createBaseBatchAnnotateImagesResponse(): BatchAnnotateImagesResponse {
  return { responses: [] };
}

export const BatchAnnotateImagesResponse: MessageFns<BatchAnnotateImagesResponse> = {
  encode(message: BatchAnnotateImagesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.responses) {
      AnnotateImageResponse.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchAnnotateImagesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchAnnotateImagesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.responses.push(AnnotateImageResponse.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchAnnotateImagesResponse {
    return {
      responses: globalThis.Array.isArray(object?.responses)
        ? object.responses.map((e: any) => AnnotateImageResponse.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchAnnotateImagesResponse): unknown {
    const obj: any = {};
    if (message.responses?.length) {
      obj.responses = message.responses.map((e) => AnnotateImageResponse.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchAnnotateImagesResponse>): BatchAnnotateImagesResponse {
    return BatchAnnotateImagesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchAnnotateImagesResponse>): BatchAnnotateImagesResponse {
    const message = createBaseBatchAnnotateImagesResponse();
    message.responses = object.responses?.map((e) => AnnotateImageResponse.fromPartial(e)) || [];
    return message;
  },
};

function createBaseAnnotateFileRequest(): AnnotateFileRequest {
  return { inputConfig: undefined, features: [], imageContext: undefined, pages: [] };
}

export const AnnotateFileRequest: MessageFns<AnnotateFileRequest> = {
  encode(message: AnnotateFileRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputConfig !== undefined) {
      InputConfig.encode(message.inputConfig, writer.uint32(10).fork()).join();
    }
    for (const v of message.features) {
      Feature.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.imageContext !== undefined) {
      ImageContext.encode(message.imageContext, writer.uint32(26).fork()).join();
    }
    writer.uint32(34).fork();
    for (const v of message.pages) {
      writer.int32(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnnotateFileRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnnotateFileRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputConfig = InputConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.features.push(Feature.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.imageContext = ImageContext.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag === 32) {
            message.pages.push(reader.int32());

            continue;
          }

          if (tag === 34) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.pages.push(reader.int32());
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnnotateFileRequest {
    return {
      inputConfig: isSet(object.inputConfig) ? InputConfig.fromJSON(object.inputConfig) : undefined,
      features: globalThis.Array.isArray(object?.features) ? object.features.map((e: any) => Feature.fromJSON(e)) : [],
      imageContext: isSet(object.imageContext) ? ImageContext.fromJSON(object.imageContext) : undefined,
      pages: globalThis.Array.isArray(object?.pages) ? object.pages.map((e: any) => globalThis.Number(e)) : [],
    };
  },

  toJSON(message: AnnotateFileRequest): unknown {
    const obj: any = {};
    if (message.inputConfig !== undefined) {
      obj.inputConfig = InputConfig.toJSON(message.inputConfig);
    }
    if (message.features?.length) {
      obj.features = message.features.map((e) => Feature.toJSON(e));
    }
    if (message.imageContext !== undefined) {
      obj.imageContext = ImageContext.toJSON(message.imageContext);
    }
    if (message.pages?.length) {
      obj.pages = message.pages.map((e) => Math.round(e));
    }
    return obj;
  },

  create(base?: DeepPartial<AnnotateFileRequest>): AnnotateFileRequest {
    return AnnotateFileRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AnnotateFileRequest>): AnnotateFileRequest {
    const message = createBaseAnnotateFileRequest();
    message.inputConfig = (object.inputConfig !== undefined && object.inputConfig !== null)
      ? InputConfig.fromPartial(object.inputConfig)
      : undefined;
    message.features = object.features?.map((e) => Feature.fromPartial(e)) || [];
    message.imageContext = (object.imageContext !== undefined && object.imageContext !== null)
      ? ImageContext.fromPartial(object.imageContext)
      : undefined;
    message.pages = object.pages?.map((e) => e) || [];
    return message;
  },
};

function createBaseAnnotateFileResponse(): AnnotateFileResponse {
  return { inputConfig: undefined, responses: [], totalPages: 0, error: undefined };
}

export const AnnotateFileResponse: MessageFns<AnnotateFileResponse> = {
  encode(message: AnnotateFileResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputConfig !== undefined) {
      InputConfig.encode(message.inputConfig, writer.uint32(10).fork()).join();
    }
    for (const v of message.responses) {
      AnnotateImageResponse.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.totalPages !== 0) {
      writer.uint32(24).int32(message.totalPages);
    }
    if (message.error !== undefined) {
      Status.encode(message.error, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnnotateFileResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnnotateFileResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputConfig = InputConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.responses.push(AnnotateImageResponse.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.totalPages = reader.int32();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.error = Status.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnnotateFileResponse {
    return {
      inputConfig: isSet(object.inputConfig) ? InputConfig.fromJSON(object.inputConfig) : undefined,
      responses: globalThis.Array.isArray(object?.responses)
        ? object.responses.map((e: any) => AnnotateImageResponse.fromJSON(e))
        : [],
      totalPages: isSet(object.totalPages) ? globalThis.Number(object.totalPages) : 0,
      error: isSet(object.error) ? Status.fromJSON(object.error) : undefined,
    };
  },

  toJSON(message: AnnotateFileResponse): unknown {
    const obj: any = {};
    if (message.inputConfig !== undefined) {
      obj.inputConfig = InputConfig.toJSON(message.inputConfig);
    }
    if (message.responses?.length) {
      obj.responses = message.responses.map((e) => AnnotateImageResponse.toJSON(e));
    }
    if (message.totalPages !== 0) {
      obj.totalPages = Math.round(message.totalPages);
    }
    if (message.error !== undefined) {
      obj.error = Status.toJSON(message.error);
    }
    return obj;
  },

  create(base?: DeepPartial<AnnotateFileResponse>): AnnotateFileResponse {
    return AnnotateFileResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AnnotateFileResponse>): AnnotateFileResponse {
    const message = createBaseAnnotateFileResponse();
    message.inputConfig = (object.inputConfig !== undefined && object.inputConfig !== null)
      ? InputConfig.fromPartial(object.inputConfig)
      : undefined;
    message.responses = object.responses?.map((e) => AnnotateImageResponse.fromPartial(e)) || [];
    message.totalPages = object.totalPages ?? 0;
    message.error = (object.error !== undefined && object.error !== null)
      ? Status.fromPartial(object.error)
      : undefined;
    return message;
  },
};

function createBaseBatchAnnotateFilesRequest(): BatchAnnotateFilesRequest {
  return { requests: [] };
}

export const BatchAnnotateFilesRequest: MessageFns<BatchAnnotateFilesRequest> = {
  encode(message: BatchAnnotateFilesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.requests) {
      AnnotateFileRequest.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchAnnotateFilesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchAnnotateFilesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.requests.push(AnnotateFileRequest.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchAnnotateFilesRequest {
    return {
      requests: globalThis.Array.isArray(object?.requests)
        ? object.requests.map((e: any) => AnnotateFileRequest.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchAnnotateFilesRequest): unknown {
    const obj: any = {};
    if (message.requests?.length) {
      obj.requests = message.requests.map((e) => AnnotateFileRequest.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchAnnotateFilesRequest>): BatchAnnotateFilesRequest {
    return BatchAnnotateFilesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchAnnotateFilesRequest>): BatchAnnotateFilesRequest {
    const message = createBaseBatchAnnotateFilesRequest();
    message.requests = object.requests?.map((e) => AnnotateFileRequest.fromPartial(e)) || [];
    return message;
  },
};

function createBaseBatchAnnotateFilesResponse(): BatchAnnotateFilesResponse {
  return { responses: [] };
}

export const BatchAnnotateFilesResponse: MessageFns<BatchAnnotateFilesResponse> = {
  encode(message: BatchAnnotateFilesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.responses) {
      AnnotateFileResponse.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchAnnotateFilesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchAnnotateFilesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.responses.push(AnnotateFileResponse.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchAnnotateFilesResponse {
    return {
      responses: globalThis.Array.isArray(object?.responses)
        ? object.responses.map((e: any) => AnnotateFileResponse.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchAnnotateFilesResponse): unknown {
    const obj: any = {};
    if (message.responses?.length) {
      obj.responses = message.responses.map((e) => AnnotateFileResponse.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchAnnotateFilesResponse>): BatchAnnotateFilesResponse {
    return BatchAnnotateFilesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchAnnotateFilesResponse>): BatchAnnotateFilesResponse {
    const message = createBaseBatchAnnotateFilesResponse();
    message.responses = object.responses?.map((e) => AnnotateFileResponse.fromPartial(e)) || [];
    return message;
  },
};

function createBaseAsyncAnnotateFileRequest(): AsyncAnnotateFileRequest {
  return { inputConfig: undefined, features: [], imageContext: undefined, outputConfig: undefined };
}

export const AsyncAnnotateFileRequest: MessageFns<AsyncAnnotateFileRequest> = {
  encode(message: AsyncAnnotateFileRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputConfig !== undefined) {
      InputConfig.encode(message.inputConfig, writer.uint32(10).fork()).join();
    }
    for (const v of message.features) {
      Feature.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.imageContext !== undefined) {
      ImageContext.encode(message.imageContext, writer.uint32(26).fork()).join();
    }
    if (message.outputConfig !== undefined) {
      OutputConfig.encode(message.outputConfig, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AsyncAnnotateFileRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAsyncAnnotateFileRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputConfig = InputConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.features.push(Feature.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.imageContext = ImageContext.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.outputConfig = OutputConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AsyncAnnotateFileRequest {
    return {
      inputConfig: isSet(object.inputConfig) ? InputConfig.fromJSON(object.inputConfig) : undefined,
      features: globalThis.Array.isArray(object?.features) ? object.features.map((e: any) => Feature.fromJSON(e)) : [],
      imageContext: isSet(object.imageContext) ? ImageContext.fromJSON(object.imageContext) : undefined,
      outputConfig: isSet(object.outputConfig) ? OutputConfig.fromJSON(object.outputConfig) : undefined,
    };
  },

  toJSON(message: AsyncAnnotateFileRequest): unknown {
    const obj: any = {};
    if (message.inputConfig !== undefined) {
      obj.inputConfig = InputConfig.toJSON(message.inputConfig);
    }
    if (message.features?.length) {
      obj.features = message.features.map((e) => Feature.toJSON(e));
    }
    if (message.imageContext !== undefined) {
      obj.imageContext = ImageContext.toJSON(message.imageContext);
    }
    if (message.outputConfig !== undefined) {
      obj.outputConfig = OutputConfig.toJSON(message.outputConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<AsyncAnnotateFileRequest>): AsyncAnnotateFileRequest {
    return AsyncAnnotateFileRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AsyncAnnotateFileRequest>): AsyncAnnotateFileRequest {
    const message = createBaseAsyncAnnotateFileRequest();
    message.inputConfig = (object.inputConfig !== undefined && object.inputConfig !== null)
      ? InputConfig.fromPartial(object.inputConfig)
      : undefined;
    message.features = object.features?.map((e) => Feature.fromPartial(e)) || [];
    message.imageContext = (object.imageContext !== undefined && object.imageContext !== null)
      ? ImageContext.fromPartial(object.imageContext)
      : undefined;
    message.outputConfig = (object.outputConfig !== undefined && object.outputConfig !== null)
      ? OutputConfig.fromPartial(object.outputConfig)
      : undefined;
    return message;
  },
};

function createBaseAsyncAnnotateFileResponse(): AsyncAnnotateFileResponse {
  return { outputConfig: undefined };
}

export const AsyncAnnotateFileResponse: MessageFns<AsyncAnnotateFileResponse> = {
  encode(message: AsyncAnnotateFileResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.outputConfig !== undefined) {
      OutputConfig.encode(message.outputConfig, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AsyncAnnotateFileResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAsyncAnnotateFileResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.outputConfig = OutputConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AsyncAnnotateFileResponse {
    return { outputConfig: isSet(object.outputConfig) ? OutputConfig.fromJSON(object.outputConfig) : undefined };
  },

  toJSON(message: AsyncAnnotateFileResponse): unknown {
    const obj: any = {};
    if (message.outputConfig !== undefined) {
      obj.outputConfig = OutputConfig.toJSON(message.outputConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<AsyncAnnotateFileResponse>): AsyncAnnotateFileResponse {
    return AsyncAnnotateFileResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AsyncAnnotateFileResponse>): AsyncAnnotateFileResponse {
    const message = createBaseAsyncAnnotateFileResponse();
    message.outputConfig = (object.outputConfig !== undefined && object.outputConfig !== null)
      ? OutputConfig.fromPartial(object.outputConfig)
      : undefined;
    return message;
  },
};

function createBaseAsyncBatchAnnotateImagesRequest(): AsyncBatchAnnotateImagesRequest {
  return { requests: [], outputConfig: undefined };
}

export const AsyncBatchAnnotateImagesRequest: MessageFns<AsyncBatchAnnotateImagesRequest> = {
  encode(message: AsyncBatchAnnotateImagesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.requests) {
      AnnotateImageRequest.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.outputConfig !== undefined) {
      OutputConfig.encode(message.outputConfig, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AsyncBatchAnnotateImagesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAsyncBatchAnnotateImagesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.requests.push(AnnotateImageRequest.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.outputConfig = OutputConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AsyncBatchAnnotateImagesRequest {
    return {
      requests: globalThis.Array.isArray(object?.requests)
        ? object.requests.map((e: any) => AnnotateImageRequest.fromJSON(e))
        : [],
      outputConfig: isSet(object.outputConfig) ? OutputConfig.fromJSON(object.outputConfig) : undefined,
    };
  },

  toJSON(message: AsyncBatchAnnotateImagesRequest): unknown {
    const obj: any = {};
    if (message.requests?.length) {
      obj.requests = message.requests.map((e) => AnnotateImageRequest.toJSON(e));
    }
    if (message.outputConfig !== undefined) {
      obj.outputConfig = OutputConfig.toJSON(message.outputConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<AsyncBatchAnnotateImagesRequest>): AsyncBatchAnnotateImagesRequest {
    return AsyncBatchAnnotateImagesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AsyncBatchAnnotateImagesRequest>): AsyncBatchAnnotateImagesRequest {
    const message = createBaseAsyncBatchAnnotateImagesRequest();
    message.requests = object.requests?.map((e) => AnnotateImageRequest.fromPartial(e)) || [];
    message.outputConfig = (object.outputConfig !== undefined && object.outputConfig !== null)
      ? OutputConfig.fromPartial(object.outputConfig)
      : undefined;
    return message;
  },
};

function createBaseAsyncBatchAnnotateImagesResponse(): AsyncBatchAnnotateImagesResponse {
  return { outputConfig: undefined };
}

export const AsyncBatchAnnotateImagesResponse: MessageFns<AsyncBatchAnnotateImagesResponse> = {
  encode(message: AsyncBatchAnnotateImagesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.outputConfig !== undefined) {
      OutputConfig.encode(message.outputConfig, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AsyncBatchAnnotateImagesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAsyncBatchAnnotateImagesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.outputConfig = OutputConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AsyncBatchAnnotateImagesResponse {
    return { outputConfig: isSet(object.outputConfig) ? OutputConfig.fromJSON(object.outputConfig) : undefined };
  },

  toJSON(message: AsyncBatchAnnotateImagesResponse): unknown {
    const obj: any = {};
    if (message.outputConfig !== undefined) {
      obj.outputConfig = OutputConfig.toJSON(message.outputConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<AsyncBatchAnnotateImagesResponse>): AsyncBatchAnnotateImagesResponse {
    return AsyncBatchAnnotateImagesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AsyncBatchAnnotateImagesResponse>): AsyncBatchAnnotateImagesResponse {
    const message = createBaseAsyncBatchAnnotateImagesResponse();
    message.outputConfig = (object.outputConfig !== undefined && object.outputConfig !== null)
      ? OutputConfig.fromPartial(object.outputConfig)
      : undefined;
    return message;
  },
};

function createBaseAsyncBatchAnnotateFilesRequest(): AsyncBatchAnnotateFilesRequest {
  return { requests: [] };
}

export const AsyncBatchAnnotateFilesRequest: MessageFns<AsyncBatchAnnotateFilesRequest> = {
  encode(message: AsyncBatchAnnotateFilesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.requests) {
      AsyncAnnotateFileRequest.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AsyncBatchAnnotateFilesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAsyncBatchAnnotateFilesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.requests.push(AsyncAnnotateFileRequest.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AsyncBatchAnnotateFilesRequest {
    return {
      requests: globalThis.Array.isArray(object?.requests)
        ? object.requests.map((e: any) => AsyncAnnotateFileRequest.fromJSON(e))
        : [],
    };
  },

  toJSON(message: AsyncBatchAnnotateFilesRequest): unknown {
    const obj: any = {};
    if (message.requests?.length) {
      obj.requests = message.requests.map((e) => AsyncAnnotateFileRequest.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<AsyncBatchAnnotateFilesRequest>): AsyncBatchAnnotateFilesRequest {
    return AsyncBatchAnnotateFilesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AsyncBatchAnnotateFilesRequest>): AsyncBatchAnnotateFilesRequest {
    const message = createBaseAsyncBatchAnnotateFilesRequest();
    message.requests = object.requests?.map((e) => AsyncAnnotateFileRequest.fromPartial(e)) || [];
    return message;
  },
};

function createBaseAsyncBatchAnnotateFilesResponse(): AsyncBatchAnnotateFilesResponse {
  return { responses: [] };
}

export const AsyncBatchAnnotateFilesResponse: MessageFns<AsyncBatchAnnotateFilesResponse> = {
  encode(message: AsyncBatchAnnotateFilesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.responses) {
      AsyncAnnotateFileResponse.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AsyncBatchAnnotateFilesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAsyncBatchAnnotateFilesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.responses.push(AsyncAnnotateFileResponse.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AsyncBatchAnnotateFilesResponse {
    return {
      responses: globalThis.Array.isArray(object?.responses)
        ? object.responses.map((e: any) => AsyncAnnotateFileResponse.fromJSON(e))
        : [],
    };
  },

  toJSON(message: AsyncBatchAnnotateFilesResponse): unknown {
    const obj: any = {};
    if (message.responses?.length) {
      obj.responses = message.responses.map((e) => AsyncAnnotateFileResponse.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<AsyncBatchAnnotateFilesResponse>): AsyncBatchAnnotateFilesResponse {
    return AsyncBatchAnnotateFilesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AsyncBatchAnnotateFilesResponse>): AsyncBatchAnnotateFilesResponse {
    const message = createBaseAsyncBatchAnnotateFilesResponse();
    message.responses = object.responses?.map((e) => AsyncAnnotateFileResponse.fromPartial(e)) || [];
    return message;
  },
};

function createBaseInputConfig(): InputConfig {
  return { gcsSource: undefined, content: Buffer.alloc(0), mimeType: "" };
}

export const InputConfig: MessageFns<InputConfig> = {
  encode(message: InputConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcsSource !== undefined) {
      GcsSource.encode(message.gcsSource, writer.uint32(10).fork()).join();
    }
    if (message.content.length !== 0) {
      writer.uint32(26).bytes(message.content);
    }
    if (message.mimeType !== "") {
      writer.uint32(18).string(message.mimeType);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InputConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInputConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.gcsSource = GcsSource.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.content = Buffer.from(reader.bytes());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.mimeType = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InputConfig {
    return {
      gcsSource: isSet(object.gcsSource) ? GcsSource.fromJSON(object.gcsSource) : undefined,
      content: isSet(object.content) ? Buffer.from(bytesFromBase64(object.content)) : Buffer.alloc(0),
      mimeType: isSet(object.mimeType) ? globalThis.String(object.mimeType) : "",
    };
  },

  toJSON(message: InputConfig): unknown {
    const obj: any = {};
    if (message.gcsSource !== undefined) {
      obj.gcsSource = GcsSource.toJSON(message.gcsSource);
    }
    if (message.content.length !== 0) {
      obj.content = base64FromBytes(message.content);
    }
    if (message.mimeType !== "") {
      obj.mimeType = message.mimeType;
    }
    return obj;
  },

  create(base?: DeepPartial<InputConfig>): InputConfig {
    return InputConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InputConfig>): InputConfig {
    const message = createBaseInputConfig();
    message.gcsSource = (object.gcsSource !== undefined && object.gcsSource !== null)
      ? GcsSource.fromPartial(object.gcsSource)
      : undefined;
    message.content = object.content ?? Buffer.alloc(0);
    message.mimeType = object.mimeType ?? "";
    return message;
  },
};

function createBaseOutputConfig(): OutputConfig {
  return { gcsDestination: undefined, batchSize: 0 };
}

export const OutputConfig: MessageFns<OutputConfig> = {
  encode(message: OutputConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcsDestination !== undefined) {
      GcsDestination.encode(message.gcsDestination, writer.uint32(10).fork()).join();
    }
    if (message.batchSize !== 0) {
      writer.uint32(16).int32(message.batchSize);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OutputConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOutputConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.gcsDestination = GcsDestination.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.batchSize = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OutputConfig {
    return {
      gcsDestination: isSet(object.gcsDestination) ? GcsDestination.fromJSON(object.gcsDestination) : undefined,
      batchSize: isSet(object.batchSize) ? globalThis.Number(object.batchSize) : 0,
    };
  },

  toJSON(message: OutputConfig): unknown {
    const obj: any = {};
    if (message.gcsDestination !== undefined) {
      obj.gcsDestination = GcsDestination.toJSON(message.gcsDestination);
    }
    if (message.batchSize !== 0) {
      obj.batchSize = Math.round(message.batchSize);
    }
    return obj;
  },

  create(base?: DeepPartial<OutputConfig>): OutputConfig {
    return OutputConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OutputConfig>): OutputConfig {
    const message = createBaseOutputConfig();
    message.gcsDestination = (object.gcsDestination !== undefined && object.gcsDestination !== null)
      ? GcsDestination.fromPartial(object.gcsDestination)
      : undefined;
    message.batchSize = object.batchSize ?? 0;
    return message;
  },
};

function createBaseGcsSource(): GcsSource {
  return { uri: "" };
}

export const GcsSource: MessageFns<GcsSource> = {
  encode(message: GcsSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.uri !== "") {
      writer.uint32(10).string(message.uri);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GcsSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGcsSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.uri = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GcsSource {
    return { uri: isSet(object.uri) ? globalThis.String(object.uri) : "" };
  },

  toJSON(message: GcsSource): unknown {
    const obj: any = {};
    if (message.uri !== "") {
      obj.uri = message.uri;
    }
    return obj;
  },

  create(base?: DeepPartial<GcsSource>): GcsSource {
    return GcsSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GcsSource>): GcsSource {
    const message = createBaseGcsSource();
    message.uri = object.uri ?? "";
    return message;
  },
};

function createBaseGcsDestination(): GcsDestination {
  return { uri: "" };
}

export const GcsDestination: MessageFns<GcsDestination> = {
  encode(message: GcsDestination, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.uri !== "") {
      writer.uint32(10).string(message.uri);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GcsDestination {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGcsDestination();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.uri = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GcsDestination {
    return { uri: isSet(object.uri) ? globalThis.String(object.uri) : "" };
  },

  toJSON(message: GcsDestination): unknown {
    const obj: any = {};
    if (message.uri !== "") {
      obj.uri = message.uri;
    }
    return obj;
  },

  create(base?: DeepPartial<GcsDestination>): GcsDestination {
    return GcsDestination.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GcsDestination>): GcsDestination {
    const message = createBaseGcsDestination();
    message.uri = object.uri ?? "";
    return message;
  },
};

function createBaseOperationMetadata(): OperationMetadata {
  return { state: 0, createTime: undefined, updateTime: undefined };
}

export const OperationMetadata: MessageFns<OperationMetadata> = {
  encode(message: OperationMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.state !== 0) {
      writer.uint32(8).int32(message.state);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(42).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OperationMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOperationMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OperationMetadata {
    return {
      state: isSet(object.state) ? operationMetadata_StateFromJSON(object.state) : 0,
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
    };
  },

  toJSON(message: OperationMetadata): unknown {
    const obj: any = {};
    if (message.state !== 0) {
      obj.state = operationMetadata_StateToJSON(message.state);
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<OperationMetadata>): OperationMetadata {
    return OperationMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OperationMetadata>): OperationMetadata {
    const message = createBaseOperationMetadata();
    message.state = object.state ?? 0;
    message.createTime = object.createTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    return message;
  },
};

/**
 * Service that performs Google Cloud Vision API detection tasks over client
 * images, such as face, landmark, logo, label, and text detection. The
 * ImageAnnotator service returns detected entities from the images.
 */
export type ImageAnnotatorDefinition = typeof ImageAnnotatorDefinition;
export const ImageAnnotatorDefinition = {
  name: "ImageAnnotator",
  fullName: "google.cloud.vision.v1p4beta1.ImageAnnotator",
  methods: {
    /** Run image detection and annotation for a batch of images. */
    batchAnnotateImages: {
      name: "BatchAnnotateImages",
      requestType: BatchAnnotateImagesRequest,
      requestStream: false,
      responseType: BatchAnnotateImagesResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([8, 114, 101, 113, 117, 101, 115, 116, 115])],
          578365826: [
            Buffer.from([
              31,
              58,
              1,
              42,
              34,
              26,
              47,
              118,
              49,
              112,
              52,
              98,
              101,
              116,
              97,
              49,
              47,
              105,
              109,
              97,
              103,
              101,
              115,
              58,
              97,
              110,
              110,
              111,
              116,
              97,
              116,
              101,
            ]),
          ],
        },
      },
    },
    /**
     * Service that performs image detection and annotation for a batch of files.
     * Now only "application/pdf", "image/tiff" and "image/gif" are supported.
     *
     * This service will extract at most 5 (customers can specify which 5 in
     * AnnotateFileRequest.pages) frames (gif) or pages (pdf or tiff) from each
     * file provided and perform detection and annotation for each image
     * extracted.
     */
    batchAnnotateFiles: {
      name: "BatchAnnotateFiles",
      requestType: BatchAnnotateFilesRequest,
      requestStream: false,
      responseType: BatchAnnotateFilesResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([8, 114, 101, 113, 117, 101, 115, 116, 115])],
          578365826: [
            Buffer.from([
              30,
              58,
              1,
              42,
              34,
              25,
              47,
              118,
              49,
              112,
              52,
              98,
              101,
              116,
              97,
              49,
              47,
              102,
              105,
              108,
              101,
              115,
              58,
              97,
              110,
              110,
              111,
              116,
              97,
              116,
              101,
            ]),
          ],
        },
      },
    },
    /**
     * Run asynchronous image detection and annotation for a list of images.
     *
     * Progress and results can be retrieved through the
     * `google.longrunning.Operations` interface.
     * `Operation.metadata` contains `OperationMetadata` (metadata).
     * `Operation.response` contains `AsyncBatchAnnotateImagesResponse` (results).
     *
     * This service will write image annotation outputs to json files in customer
     * GCS bucket, each json file containing BatchAnnotateImagesResponse proto.
     */
    asyncBatchAnnotateImages: {
      name: "AsyncBatchAnnotateImages",
      requestType: AsyncBatchAnnotateImagesRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              53,
              10,
              32,
              65,
              115,
              121,
              110,
              99,
              66,
              97,
              116,
              99,
              104,
              65,
              110,
              110,
              111,
              116,
              97,
              116,
              101,
              73,
              109,
              97,
              103,
              101,
              115,
              82,
              101,
              115,
              112,
              111,
              110,
              115,
              101,
              18,
              17,
              79,
              112,
              101,
              114,
              97,
              116,
              105,
              111,
              110,
              77,
              101,
              116,
              97,
              100,
              97,
              116,
              97,
            ]),
          ],
          8410: [
            Buffer.from([
              22,
              114,
              101,
              113,
              117,
              101,
              115,
              116,
              115,
              44,
              111,
              117,
              116,
              112,
              117,
              116,
              95,
              99,
              111,
              110,
              102,
              105,
              103,
            ]),
          ],
          578365826: [
            Buffer.from([
              41,
              58,
              1,
              42,
              34,
              36,
              47,
              118,
              49,
              112,
              52,
              98,
              101,
              116,
              97,
              49,
              47,
              105,
              109,
              97,
              103,
              101,
              115,
              58,
              97,
              115,
              121,
              110,
              99,
              66,
              97,
              116,
              99,
              104,
              65,
              110,
              110,
              111,
              116,
              97,
              116,
              101,
            ]),
          ],
        },
      },
    },
    /**
     * Run asynchronous image detection and annotation for a list of generic
     * files, such as PDF files, which may contain multiple pages and multiple
     * images per page. Progress and results can be retrieved through the
     * `google.longrunning.Operations` interface.
     * `Operation.metadata` contains `OperationMetadata` (metadata).
     * `Operation.response` contains `AsyncBatchAnnotateFilesResponse` (results).
     */
    asyncBatchAnnotateFiles: {
      name: "AsyncBatchAnnotateFiles",
      requestType: AsyncBatchAnnotateFilesRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              52,
              10,
              31,
              65,
              115,
              121,
              110,
              99,
              66,
              97,
              116,
              99,
              104,
              65,
              110,
              110,
              111,
              116,
              97,
              116,
              101,
              70,
              105,
              108,
              101,
              115,
              82,
              101,
              115,
              112,
              111,
              110,
              115,
              101,
              18,
              17,
              79,
              112,
              101,
              114,
              97,
              116,
              105,
              111,
              110,
              77,
              101,
              116,
              97,
              100,
              97,
              116,
              97,
            ]),
          ],
          8410: [Buffer.from([8, 114, 101, 113, 117, 101, 115, 116, 115])],
          578365826: [
            Buffer.from([
              40,
              58,
              1,
              42,
              34,
              35,
              47,
              118,
              49,
              112,
              52,
              98,
              101,
              116,
              97,
              49,
              47,
              102,
              105,
              108,
              101,
              115,
              58,
              97,
              115,
              121,
              110,
              99,
              66,
              97,
              116,
              99,
              104,
              65,
              110,
              110,
              111,
              116,
              97,
              116,
              101,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface ImageAnnotatorServiceImplementation<CallContextExt = {}> {
  /** Run image detection and annotation for a batch of images. */
  batchAnnotateImages(
    request: BatchAnnotateImagesRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<BatchAnnotateImagesResponse>>;
  /**
   * Service that performs image detection and annotation for a batch of files.
   * Now only "application/pdf", "image/tiff" and "image/gif" are supported.
   *
   * This service will extract at most 5 (customers can specify which 5 in
   * AnnotateFileRequest.pages) frames (gif) or pages (pdf or tiff) from each
   * file provided and perform detection and annotation for each image
   * extracted.
   */
  batchAnnotateFiles(
    request: BatchAnnotateFilesRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<BatchAnnotateFilesResponse>>;
  /**
   * Run asynchronous image detection and annotation for a list of images.
   *
   * Progress and results can be retrieved through the
   * `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `OperationMetadata` (metadata).
   * `Operation.response` contains `AsyncBatchAnnotateImagesResponse` (results).
   *
   * This service will write image annotation outputs to json files in customer
   * GCS bucket, each json file containing BatchAnnotateImagesResponse proto.
   */
  asyncBatchAnnotateImages(
    request: AsyncBatchAnnotateImagesRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Operation>>;
  /**
   * Run asynchronous image detection and annotation for a list of generic
   * files, such as PDF files, which may contain multiple pages and multiple
   * images per page. Progress and results can be retrieved through the
   * `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `OperationMetadata` (metadata).
   * `Operation.response` contains `AsyncBatchAnnotateFilesResponse` (results).
   */
  asyncBatchAnnotateFiles(
    request: AsyncBatchAnnotateFilesRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Operation>>;
}

export interface ImageAnnotatorClient<CallOptionsExt = {}> {
  /** Run image detection and annotation for a batch of images. */
  batchAnnotateImages(
    request: DeepPartial<BatchAnnotateImagesRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<BatchAnnotateImagesResponse>;
  /**
   * Service that performs image detection and annotation for a batch of files.
   * Now only "application/pdf", "image/tiff" and "image/gif" are supported.
   *
   * This service will extract at most 5 (customers can specify which 5 in
   * AnnotateFileRequest.pages) frames (gif) or pages (pdf or tiff) from each
   * file provided and perform detection and annotation for each image
   * extracted.
   */
  batchAnnotateFiles(
    request: DeepPartial<BatchAnnotateFilesRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<BatchAnnotateFilesResponse>;
  /**
   * Run asynchronous image detection and annotation for a list of images.
   *
   * Progress and results can be retrieved through the
   * `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `OperationMetadata` (metadata).
   * `Operation.response` contains `AsyncBatchAnnotateImagesResponse` (results).
   *
   * This service will write image annotation outputs to json files in customer
   * GCS bucket, each json file containing BatchAnnotateImagesResponse proto.
   */
  asyncBatchAnnotateImages(
    request: DeepPartial<AsyncBatchAnnotateImagesRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Operation>;
  /**
   * Run asynchronous image detection and annotation for a list of generic
   * files, such as PDF files, which may contain multiple pages and multiple
   * images per page. Progress and results can be retrieved through the
   * `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `OperationMetadata` (metadata).
   * `Operation.response` contains `AsyncBatchAnnotateFilesResponse` (results).
   */
  asyncBatchAnnotateFiles(
    request: DeepPartial<AsyncBatchAnnotateFilesRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Operation>;
}

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
