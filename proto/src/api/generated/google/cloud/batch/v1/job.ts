// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/batch/v1/job.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Duration } from "../../../protobuf/duration.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import {
  Environment,
  StatusEvent,
  TaskSpec,
  TaskStatus_State,
  taskStatus_StateFromJSON,
  taskStatus_StateToJSON,
} from "./task.js";

export const protobufPackage = "google.cloud.batch.v1";

/** The Cloud Batch Job description. */
export interface Job {
  /**
   * Output only. Job name.
   * For example: "projects/123456/locations/us-central1/jobs/job01".
   */
  name: string;
  /** Output only. A system generated unique ID for the Job. */
  uid: string;
  /**
   * Priority of the Job.
   * The valid value range is [0, 100). Default value is 0.
   * Higher value indicates higher priority.
   * A job with higher priority value is more likely to run earlier if all other
   * requirements are satisfied.
   */
  priority: Long;
  /** Required. TaskGroups in the Job. Only one TaskGroup is supported now. */
  taskGroups: TaskGroup[];
  /** Compute resource allocation for all TaskGroups in the Job. */
  allocationPolicy:
    | AllocationPolicy
    | undefined;
  /**
   * Labels for the Job. Labels could be user provided or system generated.
   * For example,
   * "labels": {
   *    "department": "finance",
   *    "environment": "test"
   *  }
   * You can assign up to 64 labels.  [Google Compute Engine label
   * restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions)
   * apply.
   * Label names that start with "goog-" or "google-" are reserved.
   */
  labels: { [key: string]: string };
  /** Output only. Job status. It is read only for users. */
  status:
    | JobStatus
    | undefined;
  /** Output only. When the Job was created. */
  createTime:
    | Date
    | undefined;
  /** Output only. The last time the Job was updated. */
  updateTime:
    | Date
    | undefined;
  /** Log preservation policy for the Job. */
  logsPolicy:
    | LogsPolicy
    | undefined;
  /** Notification configurations. */
  notifications: JobNotification[];
}

export interface Job_LabelsEntry {
  key: string;
  value: string;
}

/**
 * LogsPolicy describes how outputs from a Job's Tasks (stdout/stderr) will be
 * preserved.
 */
export interface LogsPolicy {
  /** Where logs should be saved. */
  destination: LogsPolicy_Destination;
  /**
   * The path to which logs are saved when the destination = PATH. This can be a
   * local file path on the VM, or under the mount point of a Persistent Disk or
   * Filestore, or a Cloud Storage path.
   */
  logsPath: string;
  /**
   * Optional. Additional settings for Cloud Logging. It will only take effect
   * when the destination of `LogsPolicy` is set to `CLOUD_LOGGING`.
   */
  cloudLoggingOption: LogsPolicy_CloudLoggingOption | undefined;
}

/** The destination (if any) for logs. */
export enum LogsPolicy_Destination {
  /** DESTINATION_UNSPECIFIED - Logs are not preserved. */
  DESTINATION_UNSPECIFIED = 0,
  /** CLOUD_LOGGING - Logs are streamed to Cloud Logging. */
  CLOUD_LOGGING = 1,
  /** PATH - Logs are saved to a file path. */
  PATH = 2,
  UNRECOGNIZED = -1,
}

export function logsPolicy_DestinationFromJSON(object: any): LogsPolicy_Destination {
  switch (object) {
    case 0:
    case "DESTINATION_UNSPECIFIED":
      return LogsPolicy_Destination.DESTINATION_UNSPECIFIED;
    case 1:
    case "CLOUD_LOGGING":
      return LogsPolicy_Destination.CLOUD_LOGGING;
    case 2:
    case "PATH":
      return LogsPolicy_Destination.PATH;
    case -1:
    case "UNRECOGNIZED":
    default:
      return LogsPolicy_Destination.UNRECOGNIZED;
  }
}

export function logsPolicy_DestinationToJSON(object: LogsPolicy_Destination): string {
  switch (object) {
    case LogsPolicy_Destination.DESTINATION_UNSPECIFIED:
      return "DESTINATION_UNSPECIFIED";
    case LogsPolicy_Destination.CLOUD_LOGGING:
      return "CLOUD_LOGGING";
    case LogsPolicy_Destination.PATH:
      return "PATH";
    case LogsPolicy_Destination.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * `CloudLoggingOption` contains additional settings for Cloud Logging logs
 * generated by Batch job.
 */
export interface LogsPolicy_CloudLoggingOption {
  /**
   * Optional. Set this flag to true to change the [monitored resource
   * type](https://cloud.google.com/monitoring/api/resources) for
   * Cloud Logging logs generated by this Batch job from
   * the
   * [`batch.googleapis.com/Job`](https://cloud.google.com/monitoring/api/resources#tag_batch.googleapis.com/Job)
   * type to the formerly used
   * [`generic_task`](https://cloud.google.com/monitoring/api/resources#tag_generic_task)
   * type.
   */
  useGenericTaskMonitoredResource: boolean;
}

/** Job status. */
export interface JobStatus {
  /** Job state */
  state: JobStatus_State;
  /** Job status events */
  statusEvents: StatusEvent[];
  /**
   * Aggregated task status for each TaskGroup in the Job.
   * The map key is TaskGroup ID.
   */
  taskGroups: { [key: string]: JobStatus_TaskGroupStatus };
  /** The duration of time that the Job spent in status RUNNING. */
  runDuration: Duration | undefined;
}

/** Valid Job states. */
export enum JobStatus_State {
  /** STATE_UNSPECIFIED - Job state unspecified. */
  STATE_UNSPECIFIED = 0,
  /** QUEUED - Job is admitted (validated and persisted) and waiting for resources. */
  QUEUED = 1,
  /**
   * SCHEDULED - Job is scheduled to run as soon as resource allocation is ready.
   * The resource allocation may happen at a later time but with a high
   * chance to succeed.
   */
  SCHEDULED = 2,
  /**
   * RUNNING - Resource allocation has been successful. At least one Task in the Job is
   * RUNNING.
   */
  RUNNING = 3,
  /** SUCCEEDED - All Tasks in the Job have finished successfully. */
  SUCCEEDED = 4,
  /** FAILED - At least one Task in the Job has failed. */
  FAILED = 5,
  /**
   * DELETION_IN_PROGRESS - The Job will be deleted, but has not been deleted yet. Typically this is
   * because resources used by the Job are still being cleaned up.
   */
  DELETION_IN_PROGRESS = 6,
  UNRECOGNIZED = -1,
}

export function jobStatus_StateFromJSON(object: any): JobStatus_State {
  switch (object) {
    case 0:
    case "STATE_UNSPECIFIED":
      return JobStatus_State.STATE_UNSPECIFIED;
    case 1:
    case "QUEUED":
      return JobStatus_State.QUEUED;
    case 2:
    case "SCHEDULED":
      return JobStatus_State.SCHEDULED;
    case 3:
    case "RUNNING":
      return JobStatus_State.RUNNING;
    case 4:
    case "SUCCEEDED":
      return JobStatus_State.SUCCEEDED;
    case 5:
    case "FAILED":
      return JobStatus_State.FAILED;
    case 6:
    case "DELETION_IN_PROGRESS":
      return JobStatus_State.DELETION_IN_PROGRESS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return JobStatus_State.UNRECOGNIZED;
  }
}

export function jobStatus_StateToJSON(object: JobStatus_State): string {
  switch (object) {
    case JobStatus_State.STATE_UNSPECIFIED:
      return "STATE_UNSPECIFIED";
    case JobStatus_State.QUEUED:
      return "QUEUED";
    case JobStatus_State.SCHEDULED:
      return "SCHEDULED";
    case JobStatus_State.RUNNING:
      return "RUNNING";
    case JobStatus_State.SUCCEEDED:
      return "SUCCEEDED";
    case JobStatus_State.FAILED:
      return "FAILED";
    case JobStatus_State.DELETION_IN_PROGRESS:
      return "DELETION_IN_PROGRESS";
    case JobStatus_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** VM instance status. */
export interface JobStatus_InstanceStatus {
  /** The Compute Engine machine type. */
  machineType: string;
  /** The VM instance provisioning model. */
  provisioningModel: AllocationPolicy_ProvisioningModel;
  /** The max number of tasks can be assigned to this instance type. */
  taskPack: Long;
  /** The VM boot disk. */
  bootDisk: AllocationPolicy_Disk | undefined;
}

/** Aggregated task status for a TaskGroup. */
export interface JobStatus_TaskGroupStatus {
  /**
   * Count of task in each state in the TaskGroup.
   * The map key is task state name.
   */
  counts: { [key: string]: Long };
  /** Status of instances allocated for the TaskGroup. */
  instances: JobStatus_InstanceStatus[];
}

export interface JobStatus_TaskGroupStatus_CountsEntry {
  key: string;
  value: Long;
}

export interface JobStatus_TaskGroupsEntry {
  key: string;
  value: JobStatus_TaskGroupStatus | undefined;
}

/** Notification configurations. */
export interface JobNotification {
  /**
   * The Pub/Sub topic where notifications for the job, like state
   * changes, will be published. If undefined, no Pub/Sub notifications
   * are sent for this job.
   *
   * Specify the topic using the following format:
   * `projects/{project}/topics/{topic}`.
   * Notably, if you want to specify a Pub/Sub topic that is in a
   * different project than the job, your administrator must grant your
   * project's Batch service agent permission to publish to that topic.
   *
   * For more information about configuring Pub/Sub notifications for
   * a job, see
   * https://cloud.google.com/batch/docs/enable-notifications.
   */
  pubsubTopic: string;
  /**
   * The attribute requirements of messages to be sent to this Pub/Sub topic.
   * Without this field, no message will be sent.
   */
  message: JobNotification_Message | undefined;
}

/** The message type. */
export enum JobNotification_Type {
  /** TYPE_UNSPECIFIED - Unspecified. */
  TYPE_UNSPECIFIED = 0,
  /** JOB_STATE_CHANGED - Notify users that the job state has changed. */
  JOB_STATE_CHANGED = 1,
  /** TASK_STATE_CHANGED - Notify users that the task state has changed. */
  TASK_STATE_CHANGED = 2,
  UNRECOGNIZED = -1,
}

export function jobNotification_TypeFromJSON(object: any): JobNotification_Type {
  switch (object) {
    case 0:
    case "TYPE_UNSPECIFIED":
      return JobNotification_Type.TYPE_UNSPECIFIED;
    case 1:
    case "JOB_STATE_CHANGED":
      return JobNotification_Type.JOB_STATE_CHANGED;
    case 2:
    case "TASK_STATE_CHANGED":
      return JobNotification_Type.TASK_STATE_CHANGED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return JobNotification_Type.UNRECOGNIZED;
  }
}

export function jobNotification_TypeToJSON(object: JobNotification_Type): string {
  switch (object) {
    case JobNotification_Type.TYPE_UNSPECIFIED:
      return "TYPE_UNSPECIFIED";
    case JobNotification_Type.JOB_STATE_CHANGED:
      return "JOB_STATE_CHANGED";
    case JobNotification_Type.TASK_STATE_CHANGED:
      return "TASK_STATE_CHANGED";
    case JobNotification_Type.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Message details.
 * Describe the conditions under which messages will be sent.
 * If no attribute is defined, no message will be sent by default.
 * One message should specify either the job or the task level attributes,
 * but not both. For example,
 * job level: JOB_STATE_CHANGED and/or a specified new_job_state;
 * task level: TASK_STATE_CHANGED and/or a specified new_task_state.
 */
export interface JobNotification_Message {
  /** The message type. */
  type: JobNotification_Type;
  /** The new job state. */
  newJobState: JobStatus_State;
  /** The new task state. */
  newTaskState: TaskStatus_State;
}

/**
 * A Job's resource allocation policy describes when, where, and how compute
 * resources should be allocated for the Job.
 */
export interface AllocationPolicy {
  /** Location where compute resources should be allocated for the Job. */
  location:
    | AllocationPolicy_LocationPolicy
    | undefined;
  /**
   * Describe instances that can be created by this AllocationPolicy.
   * Only instances[0] is supported now.
   */
  instances: AllocationPolicy_InstancePolicyOrTemplate[];
  /**
   * Defines the service account for Batch-created VMs. If omitted, the [default
   * Compute Engine service
   * account](https://cloud.google.com/compute/docs/access/service-accounts#default_service_account)
   * is used. Must match the service account specified in any used instance
   * template configured in the Batch job.
   *
   * Includes the following fields:
   *  * email: The service account's email address. If not set, the default
   *  Compute Engine service account is used.
   *  * scopes: Additional OAuth scopes to grant the service account, beyond the
   *  default cloud-platform scope. (list of strings)
   */
  serviceAccount:
    | ServiceAccount
    | undefined;
  /**
   * Labels applied to all VM instances and other resources
   * created by AllocationPolicy.
   * Labels could be user provided or system generated.
   * You can assign up to 64 labels. [Google Compute Engine label
   * restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions)
   * apply.
   * Label names that start with "goog-" or "google-" are reserved.
   */
  labels: { [key: string]: string };
  /**
   * The network policy.
   *
   * If you define an instance template in the `InstancePolicyOrTemplate` field,
   * Batch will use the network settings in the instance template instead of
   * this field.
   */
  network:
    | AllocationPolicy_NetworkPolicy
    | undefined;
  /** The placement policy. */
  placement:
    | AllocationPolicy_PlacementPolicy
    | undefined;
  /**
   * Optional. Tags applied to the VM instances.
   *
   * The tags identify valid sources or targets for network firewalls.
   * Each tag must be 1-63 characters long, and comply with
   * [RFC1035](https://www.ietf.org/rfc/rfc1035.txt).
   */
  tags: string[];
}

/** Compute Engine VM instance provisioning model. */
export enum AllocationPolicy_ProvisioningModel {
  /** PROVISIONING_MODEL_UNSPECIFIED - Unspecified. */
  PROVISIONING_MODEL_UNSPECIFIED = 0,
  /** STANDARD - Standard VM. */
  STANDARD = 1,
  /** SPOT - SPOT VM. */
  SPOT = 2,
  /**
   * PREEMPTIBLE - Preemptible VM (PVM).
   *
   * Above SPOT VM is the preferable model for preemptible VM instances: the
   * old preemptible VM model (indicated by this field) is the older model,
   * and has been migrated to use the SPOT model as the underlying technology.
   * This old model will still be supported.
   */
  PREEMPTIBLE = 3,
  UNRECOGNIZED = -1,
}

export function allocationPolicy_ProvisioningModelFromJSON(object: any): AllocationPolicy_ProvisioningModel {
  switch (object) {
    case 0:
    case "PROVISIONING_MODEL_UNSPECIFIED":
      return AllocationPolicy_ProvisioningModel.PROVISIONING_MODEL_UNSPECIFIED;
    case 1:
    case "STANDARD":
      return AllocationPolicy_ProvisioningModel.STANDARD;
    case 2:
    case "SPOT":
      return AllocationPolicy_ProvisioningModel.SPOT;
    case 3:
    case "PREEMPTIBLE":
      return AllocationPolicy_ProvisioningModel.PREEMPTIBLE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AllocationPolicy_ProvisioningModel.UNRECOGNIZED;
  }
}

export function allocationPolicy_ProvisioningModelToJSON(object: AllocationPolicy_ProvisioningModel): string {
  switch (object) {
    case AllocationPolicy_ProvisioningModel.PROVISIONING_MODEL_UNSPECIFIED:
      return "PROVISIONING_MODEL_UNSPECIFIED";
    case AllocationPolicy_ProvisioningModel.STANDARD:
      return "STANDARD";
    case AllocationPolicy_ProvisioningModel.SPOT:
      return "SPOT";
    case AllocationPolicy_ProvisioningModel.PREEMPTIBLE:
      return "PREEMPTIBLE";
    case AllocationPolicy_ProvisioningModel.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export interface AllocationPolicy_LocationPolicy {
  /**
   * A list of allowed location names represented by internal URLs.
   *
   * Each location can be a region or a zone.
   * Only one region or multiple zones in one region is supported now.
   * For example,
   * ["regions/us-central1"] allow VMs in any zones in region us-central1.
   * ["zones/us-central1-a", "zones/us-central1-c"] only allow VMs
   * in zones us-central1-a and us-central1-c.
   *
   * Mixing locations from different regions would cause errors.
   * For example,
   * ["regions/us-central1", "zones/us-central1-a", "zones/us-central1-b",
   * "zones/us-west1-a"] contains locations from two distinct regions:
   * us-central1 and us-west1. This combination will trigger an error.
   */
  allowedLocations: string[];
}

/**
 * A new persistent disk or a local ssd.
 * A VM can only have one local SSD setting but multiple local SSD partitions.
 * See https://cloud.google.com/compute/docs/disks#pdspecs and
 * https://cloud.google.com/compute/docs/disks#localssds.
 */
export interface AllocationPolicy_Disk {
  /**
   * URL for a VM image to use as the data source for this disk.
   * For example, the following are all valid URLs:
   *
   * * Specify the image by its family name:
   * projects/{project}/global/images/family/{image_family}
   * * Specify the image version:
   * projects/{project}/global/images/{image_version}
   *
   * You can also use Batch customized image in short names.
   * The following image values are supported for a boot disk:
   *
   * * `batch-debian`: use Batch Debian images.
   * * `batch-cos`: use Batch Container-Optimized images.
   * * `batch-hpc-rocky`: use Batch HPC Rocky Linux images.
   */
  image?:
    | string
    | undefined;
  /**
   * Name of a snapshot used as the data source.
   * Snapshot is not supported as boot disk now.
   */
  snapshot?:
    | string
    | undefined;
  /**
   * Disk type as shown in `gcloud compute disk-types list`.
   * For example, local SSD uses type "local-ssd".
   * Persistent disks and boot disks use "pd-balanced", "pd-extreme", "pd-ssd"
   * or "pd-standard". If not specified, "pd-standard" will be used as the
   * default type for non-boot disks, "pd-balanced" will be used as the
   * default type for boot disks.
   */
  type: string;
  /**
   * Disk size in GB.
   *
   * **Non-Boot Disk**:
   * If the `type` specifies a persistent disk, this field
   * is ignored if `data_source` is set as `image` or `snapshot`.
   * If the `type` specifies a local SSD, this field should be a multiple of
   * 375 GB, otherwise, the final size will be the next greater multiple of
   * 375 GB.
   *
   * **Boot Disk**:
   * Batch will calculate the boot disk size based on source
   * image and task requirements if you do not speicify the size.
   * If both this field and the `boot_disk_mib` field in task spec's
   * `compute_resource` are defined, Batch will only honor this field.
   * Also, this field should be no smaller than the source disk's
   * size when the `data_source` is set as `snapshot` or `image`.
   * For example, if you set an image as the `data_source` field and the
   * image's default disk size 30 GB, you can only use this field to make the
   * disk larger or equal to 30 GB.
   */
  sizeGb: Long;
  /**
   * Local SSDs are available through both "SCSI" and "NVMe" interfaces.
   * If not indicated, "NVMe" will be the default one for local ssds.
   * This field is ignored for persistent disks as the interface is chosen
   * automatically. See
   * https://cloud.google.com/compute/docs/disks/persistent-disks#choose_an_interface.
   */
  diskInterface: string;
}

/**
 * A new or an existing persistent disk (PD) or a local ssd attached to a VM
 * instance.
 */
export interface AllocationPolicy_AttachedDisk {
  newDisk?:
    | AllocationPolicy_Disk
    | undefined;
  /** Name of an existing PD. */
  existingDisk?:
    | string
    | undefined;
  /**
   * Device name that the guest operating system will see.
   * It is used by Runnable.volumes field to mount disks. So please specify
   * the device_name if you want Batch to help mount the disk, and it should
   * match the device_name field in volumes.
   */
  deviceName: string;
}

/** Accelerator describes Compute Engine accelerators to be attached to the VM. */
export interface AllocationPolicy_Accelerator {
  /**
   * The accelerator type. For example, "nvidia-tesla-t4".
   * See `gcloud compute accelerator-types list`.
   */
  type: string;
  /** The number of accelerators of this type. */
  count: Long;
  /**
   * Deprecated: please use instances[0].install_gpu_drivers instead.
   *
   * @deprecated
   */
  installGpuDrivers: boolean;
  /**
   * Optional. The NVIDIA GPU driver version that should be installed for this
   * type.
   *
   * You can define the specific driver version such as "470.103.01",
   * following the driver version requirements in
   * https://cloud.google.com/compute/docs/gpus/install-drivers-gpu#minimum-driver.
   * Batch will install the specific accelerator driver if qualified.
   */
  driverVersion: string;
}

/**
 * InstancePolicy describes an instance type and resources attached to each VM
 * created by this InstancePolicy.
 */
export interface AllocationPolicy_InstancePolicy {
  /** The Compute Engine machine type. */
  machineType: string;
  /**
   * The minimum CPU platform.
   * See
   * https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform.
   */
  minCpuPlatform: string;
  /** The provisioning model. */
  provisioningModel: AllocationPolicy_ProvisioningModel;
  /** The accelerators attached to each VM instance. */
  accelerators: AllocationPolicy_Accelerator[];
  /**
   * Boot disk to be created and attached to each VM by this InstancePolicy.
   * Boot disk will be deleted when the VM is deleted.
   * Batch API now only supports booting from image.
   */
  bootDisk:
    | AllocationPolicy_Disk
    | undefined;
  /**
   * Non-boot disks to be attached for each VM created by this InstancePolicy.
   * New disks will be deleted when the VM is deleted.
   * A non-boot disk is a disk that can be of a device with a
   * file system or a raw storage drive that is not ready for data
   * storage and accessing.
   */
  disks: AllocationPolicy_AttachedDisk[];
  /**
   * Optional. If specified, VMs will consume only the specified reservation.
   * If not specified (default), VMs will consume any applicable reservation.
   */
  reservation: string;
}

/**
 * InstancePolicyOrTemplate lets you define the type of resources to use for
 * this job either with an InstancePolicy or an instance template.
 * If undefined, Batch picks the type of VM to use and doesn't include
 * optional VM resources such as GPUs and extra disks.
 */
export interface AllocationPolicy_InstancePolicyOrTemplate {
  /** InstancePolicy. */
  policy?:
    | AllocationPolicy_InstancePolicy
    | undefined;
  /**
   * Name of an instance template used to create VMs.
   * Named the field as 'instance_template' instead of 'template' to avoid
   * c++ keyword conflict.
   */
  instanceTemplate?:
    | string
    | undefined;
  /**
   * Set this field true if you want Batch to help fetch drivers from a third
   * party location and install them for GPUs specified in
   * `policy.accelerators` or `instance_template` on your behalf. Default is
   * false.
   *
   * For Container-Optimized Image cases, Batch will install the
   * accelerator driver following milestones of
   * https://cloud.google.com/container-optimized-os/docs/release-notes. For
   * non Container-Optimized Image cases, following
   * https://github.com/GoogleCloudPlatform/compute-gpu-installation/blob/main/linux/install_gpu_driver.py.
   */
  installGpuDrivers: boolean;
  /**
   * Optional. Set this field true if you want Batch to install Ops Agent on
   * your behalf. Default is false.
   */
  installOpsAgent: boolean;
  /**
   * Optional. Set this field to `true` if you want Batch to block
   * project-level SSH keys from accessing this job's VMs.  Alternatively, you
   * can configure the job to specify a VM instance template that blocks
   * project-level SSH keys. In either case, Batch blocks project-level SSH
   * keys while creating the VMs for this job.
   *
   * Batch allows project-level SSH keys for a job's VMs only if all
   * the following are true:
   *
   * + This field is undefined or set to `false`.
   * + The job's VM instance template (if any) doesn't block project-level
   *   SSH keys.
   *
   * Notably, you can override this behavior by manually updating a VM to
   * block or allow project-level SSH keys. For more information about
   * blocking project-level SSH keys, see the Compute Engine documentation:
   * https://cloud.google.com/compute/docs/connect/restrict-ssh-keys#block-keys
   */
  blockProjectSshKeys: boolean;
}

/** A network interface. */
export interface AllocationPolicy_NetworkInterface {
  /**
   * The URL of an existing network resource.
   * You can specify the network as a full or partial URL.
   *
   * For example, the following are all valid URLs:
   *
   * * https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}
   * * projects/{project}/global/networks/{network}
   * * global/networks/{network}
   */
  network: string;
  /**
   * The URL of an existing subnetwork resource in the network.
   * You can specify the subnetwork as a full or partial URL.
   *
   * For example, the following are all valid URLs:
   *
   * * https://www.googleapis.com/compute/v1/projects/{project}/regions/{region}/subnetworks/{subnetwork}
   * * projects/{project}/regions/{region}/subnetworks/{subnetwork}
   * * regions/{region}/subnetworks/{subnetwork}
   */
  subnetwork: string;
  /**
   * Default is false (with an external IP address). Required if
   * no external public IP address is attached to the VM. If no external
   * public IP address, additional configuration is required to allow the VM
   * to access Google Services. See
   * https://cloud.google.com/vpc/docs/configure-private-google-access and
   * https://cloud.google.com/nat/docs/gce-example#create-nat for more
   * information.
   */
  noExternalIpAddress: boolean;
}

/** NetworkPolicy describes VM instance network configurations. */
export interface AllocationPolicy_NetworkPolicy {
  /** Network configurations. */
  networkInterfaces: AllocationPolicy_NetworkInterface[];
}

/**
 * PlacementPolicy describes a group placement policy for the VMs controlled
 * by this AllocationPolicy.
 */
export interface AllocationPolicy_PlacementPolicy {
  /**
   * UNSPECIFIED vs. COLLOCATED (default UNSPECIFIED). Use COLLOCATED when you
   * want VMs to be located close to each other for low network latency
   * between the VMs. No placement policy will be generated when collocation
   * is UNSPECIFIED.
   */
  collocation: string;
  /**
   * When specified, causes the job to fail if more than max_distance logical
   * switches are required between VMs. Batch uses the most compact possible
   * placement of VMs even when max_distance is not specified. An explicit
   * max_distance makes that level of compactness a strict requirement.
   * Not yet implemented
   */
  maxDistance: Long;
}

export interface AllocationPolicy_LabelsEntry {
  key: string;
  value: string;
}

/** A TaskGroup defines one or more Tasks that all share the same TaskSpec. */
export interface TaskGroup {
  /**
   * Output only. TaskGroup name.
   * The system generates this field based on parent Job name.
   * For example:
   * "projects/123456/locations/us-west1/jobs/job01/taskGroups/group01".
   */
  name: string;
  /** Required. Tasks in the group share the same task spec. */
  taskSpec:
    | TaskSpec
    | undefined;
  /**
   * Number of Tasks in the TaskGroup.
   * Default is 1.
   */
  taskCount: Long;
  /**
   * Max number of tasks that can run in parallel.
   * Default to min(task_count, parallel tasks per job limit).
   * See: [Job Limits](https://cloud.google.com/batch/quotas#job_limits).
   * Field parallelism must be 1 if the scheduling_policy is IN_ORDER.
   */
  parallelism: Long;
  /**
   * Scheduling policy for Tasks in the TaskGroup.
   * The default value is AS_SOON_AS_POSSIBLE.
   */
  schedulingPolicy: TaskGroup_SchedulingPolicy;
  /**
   * An array of environment variable mappings, which are passed to Tasks with
   * matching indices. If task_environments is used then task_count should
   * not be specified in the request (and will be ignored). Task count will be
   * the length of task_environments.
   *
   * Tasks get a BATCH_TASK_INDEX and BATCH_TASK_COUNT environment variable, in
   * addition to any environment variables set in task_environments, specifying
   * the number of Tasks in the Task's parent TaskGroup, and the specific Task's
   * index in the TaskGroup (0 through BATCH_TASK_COUNT - 1).
   */
  taskEnvironments: Environment[];
  /**
   * Max number of tasks that can be run on a VM at the same time.
   * If not specified, the system will decide a value based on available
   * compute resources on a VM and task requirements.
   */
  taskCountPerNode: Long;
  /**
   * When true, Batch will populate a file with a list of all VMs assigned to
   * the TaskGroup and set the BATCH_HOSTS_FILE environment variable to the path
   * of that file. Defaults to false. The host file supports up to 1000 VMs.
   */
  requireHostsFile: boolean;
  /**
   * When true, Batch will configure SSH to allow passwordless login between
   * VMs running the Batch tasks in the same TaskGroup.
   */
  permissiveSsh: boolean;
  /**
   * Optional. If not set or set to false, Batch uses the root user to execute
   * runnables. If set to true, Batch runs the runnables using a non-root user.
   * Currently, the non-root user Batch used is generated by OS Login. For more
   * information, see [About OS
   * Login](https://cloud.google.com/compute/docs/oslogin).
   */
  runAsNonRoot: boolean;
}

/** How Tasks in the TaskGroup should be scheduled relative to each other. */
export enum TaskGroup_SchedulingPolicy {
  /** SCHEDULING_POLICY_UNSPECIFIED - Unspecified. */
  SCHEDULING_POLICY_UNSPECIFIED = 0,
  /**
   * AS_SOON_AS_POSSIBLE - Run Tasks as soon as resources are available.
   *
   * Tasks might be executed in parallel depending on parallelism and
   * task_count values.
   */
  AS_SOON_AS_POSSIBLE = 1,
  /** IN_ORDER - Run Tasks sequentially with increased task index. */
  IN_ORDER = 2,
  UNRECOGNIZED = -1,
}

export function taskGroup_SchedulingPolicyFromJSON(object: any): TaskGroup_SchedulingPolicy {
  switch (object) {
    case 0:
    case "SCHEDULING_POLICY_UNSPECIFIED":
      return TaskGroup_SchedulingPolicy.SCHEDULING_POLICY_UNSPECIFIED;
    case 1:
    case "AS_SOON_AS_POSSIBLE":
      return TaskGroup_SchedulingPolicy.AS_SOON_AS_POSSIBLE;
    case 2:
    case "IN_ORDER":
      return TaskGroup_SchedulingPolicy.IN_ORDER;
    case -1:
    case "UNRECOGNIZED":
    default:
      return TaskGroup_SchedulingPolicy.UNRECOGNIZED;
  }
}

export function taskGroup_SchedulingPolicyToJSON(object: TaskGroup_SchedulingPolicy): string {
  switch (object) {
    case TaskGroup_SchedulingPolicy.SCHEDULING_POLICY_UNSPECIFIED:
      return "SCHEDULING_POLICY_UNSPECIFIED";
    case TaskGroup_SchedulingPolicy.AS_SOON_AS_POSSIBLE:
      return "AS_SOON_AS_POSSIBLE";
    case TaskGroup_SchedulingPolicy.IN_ORDER:
      return "IN_ORDER";
    case TaskGroup_SchedulingPolicy.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Carries information about a Google Cloud service account. */
export interface ServiceAccount {
  /** Email address of the service account. */
  email: string;
  /** List of scopes to be enabled for this service account. */
  scopes: string[];
}

function createBaseJob(): Job {
  return {
    name: "",
    uid: "",
    priority: Long.ZERO,
    taskGroups: [],
    allocationPolicy: undefined,
    labels: {},
    status: undefined,
    createTime: undefined,
    updateTime: undefined,
    logsPolicy: undefined,
    notifications: [],
  };
}

export const Job: MessageFns<Job> = {
  encode(message: Job, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.uid !== "") {
      writer.uint32(18).string(message.uid);
    }
    if (!message.priority.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.priority.toString());
    }
    for (const v of message.taskGroups) {
      TaskGroup.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.allocationPolicy !== undefined) {
      AllocationPolicy.encode(message.allocationPolicy, writer.uint32(58).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      Job_LabelsEntry.encode({ key: key as any, value }, writer.uint32(66).fork()).join();
    });
    if (message.status !== undefined) {
      JobStatus.encode(message.status, writer.uint32(74).fork()).join();
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(90).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(98).fork()).join();
    }
    if (message.logsPolicy !== undefined) {
      LogsPolicy.encode(message.logsPolicy, writer.uint32(106).fork()).join();
    }
    for (const v of message.notifications) {
      JobNotification.encode(v!, writer.uint32(114).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Job {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.uid = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.priority = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.taskGroups.push(TaskGroup.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.allocationPolicy = AllocationPolicy.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          const entry8 = Job_LabelsEntry.decode(reader, reader.uint32());
          if (entry8.value !== undefined) {
            message.labels[entry8.key] = entry8.value;
          }
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.status = JobStatus.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.logsPolicy = LogsPolicy.decode(reader, reader.uint32());
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.notifications.push(JobNotification.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Job {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      uid: isSet(object.uid) ? globalThis.String(object.uid) : "",
      priority: isSet(object.priority) ? Long.fromValue(object.priority) : Long.ZERO,
      taskGroups: globalThis.Array.isArray(object?.taskGroups)
        ? object.taskGroups.map((e: any) => TaskGroup.fromJSON(e))
        : [],
      allocationPolicy: isSet(object.allocationPolicy) ? AllocationPolicy.fromJSON(object.allocationPolicy) : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      status: isSet(object.status) ? JobStatus.fromJSON(object.status) : undefined,
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      logsPolicy: isSet(object.logsPolicy) ? LogsPolicy.fromJSON(object.logsPolicy) : undefined,
      notifications: globalThis.Array.isArray(object?.notifications)
        ? object.notifications.map((e: any) => JobNotification.fromJSON(e))
        : [],
    };
  },

  toJSON(message: Job): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.uid !== "") {
      obj.uid = message.uid;
    }
    if (!message.priority.equals(Long.ZERO)) {
      obj.priority = (message.priority || Long.ZERO).toString();
    }
    if (message.taskGroups?.length) {
      obj.taskGroups = message.taskGroups.map((e) => TaskGroup.toJSON(e));
    }
    if (message.allocationPolicy !== undefined) {
      obj.allocationPolicy = AllocationPolicy.toJSON(message.allocationPolicy);
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.status !== undefined) {
      obj.status = JobStatus.toJSON(message.status);
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (message.logsPolicy !== undefined) {
      obj.logsPolicy = LogsPolicy.toJSON(message.logsPolicy);
    }
    if (message.notifications?.length) {
      obj.notifications = message.notifications.map((e) => JobNotification.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<Job>): Job {
    return Job.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Job>): Job {
    const message = createBaseJob();
    message.name = object.name ?? "";
    message.uid = object.uid ?? "";
    message.priority = (object.priority !== undefined && object.priority !== null)
      ? Long.fromValue(object.priority)
      : Long.ZERO;
    message.taskGroups = object.taskGroups?.map((e) => TaskGroup.fromPartial(e)) || [];
    message.allocationPolicy = (object.allocationPolicy !== undefined && object.allocationPolicy !== null)
      ? AllocationPolicy.fromPartial(object.allocationPolicy)
      : undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.status = (object.status !== undefined && object.status !== null)
      ? JobStatus.fromPartial(object.status)
      : undefined;
    message.createTime = object.createTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.logsPolicy = (object.logsPolicy !== undefined && object.logsPolicy !== null)
      ? LogsPolicy.fromPartial(object.logsPolicy)
      : undefined;
    message.notifications = object.notifications?.map((e) => JobNotification.fromPartial(e)) || [];
    return message;
  },
};

function createBaseJob_LabelsEntry(): Job_LabelsEntry {
  return { key: "", value: "" };
}

export const Job_LabelsEntry: MessageFns<Job_LabelsEntry> = {
  encode(message: Job_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Job_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJob_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Job_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Job_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Job_LabelsEntry>): Job_LabelsEntry {
    return Job_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Job_LabelsEntry>): Job_LabelsEntry {
    const message = createBaseJob_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseLogsPolicy(): LogsPolicy {
  return { destination: 0, logsPath: "", cloudLoggingOption: undefined };
}

export const LogsPolicy: MessageFns<LogsPolicy> = {
  encode(message: LogsPolicy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.destination !== 0) {
      writer.uint32(8).int32(message.destination);
    }
    if (message.logsPath !== "") {
      writer.uint32(18).string(message.logsPath);
    }
    if (message.cloudLoggingOption !== undefined) {
      LogsPolicy_CloudLoggingOption.encode(message.cloudLoggingOption, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LogsPolicy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLogsPolicy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.destination = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.logsPath = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.cloudLoggingOption = LogsPolicy_CloudLoggingOption.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LogsPolicy {
    return {
      destination: isSet(object.destination) ? logsPolicy_DestinationFromJSON(object.destination) : 0,
      logsPath: isSet(object.logsPath) ? globalThis.String(object.logsPath) : "",
      cloudLoggingOption: isSet(object.cloudLoggingOption)
        ? LogsPolicy_CloudLoggingOption.fromJSON(object.cloudLoggingOption)
        : undefined,
    };
  },

  toJSON(message: LogsPolicy): unknown {
    const obj: any = {};
    if (message.destination !== 0) {
      obj.destination = logsPolicy_DestinationToJSON(message.destination);
    }
    if (message.logsPath !== "") {
      obj.logsPath = message.logsPath;
    }
    if (message.cloudLoggingOption !== undefined) {
      obj.cloudLoggingOption = LogsPolicy_CloudLoggingOption.toJSON(message.cloudLoggingOption);
    }
    return obj;
  },

  create(base?: DeepPartial<LogsPolicy>): LogsPolicy {
    return LogsPolicy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LogsPolicy>): LogsPolicy {
    const message = createBaseLogsPolicy();
    message.destination = object.destination ?? 0;
    message.logsPath = object.logsPath ?? "";
    message.cloudLoggingOption = (object.cloudLoggingOption !== undefined && object.cloudLoggingOption !== null)
      ? LogsPolicy_CloudLoggingOption.fromPartial(object.cloudLoggingOption)
      : undefined;
    return message;
  },
};

function createBaseLogsPolicy_CloudLoggingOption(): LogsPolicy_CloudLoggingOption {
  return { useGenericTaskMonitoredResource: false };
}

export const LogsPolicy_CloudLoggingOption: MessageFns<LogsPolicy_CloudLoggingOption> = {
  encode(message: LogsPolicy_CloudLoggingOption, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.useGenericTaskMonitoredResource !== false) {
      writer.uint32(8).bool(message.useGenericTaskMonitoredResource);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LogsPolicy_CloudLoggingOption {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLogsPolicy_CloudLoggingOption();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.useGenericTaskMonitoredResource = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LogsPolicy_CloudLoggingOption {
    return {
      useGenericTaskMonitoredResource: isSet(object.useGenericTaskMonitoredResource)
        ? globalThis.Boolean(object.useGenericTaskMonitoredResource)
        : false,
    };
  },

  toJSON(message: LogsPolicy_CloudLoggingOption): unknown {
    const obj: any = {};
    if (message.useGenericTaskMonitoredResource !== false) {
      obj.useGenericTaskMonitoredResource = message.useGenericTaskMonitoredResource;
    }
    return obj;
  },

  create(base?: DeepPartial<LogsPolicy_CloudLoggingOption>): LogsPolicy_CloudLoggingOption {
    return LogsPolicy_CloudLoggingOption.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LogsPolicy_CloudLoggingOption>): LogsPolicy_CloudLoggingOption {
    const message = createBaseLogsPolicy_CloudLoggingOption();
    message.useGenericTaskMonitoredResource = object.useGenericTaskMonitoredResource ?? false;
    return message;
  },
};

function createBaseJobStatus(): JobStatus {
  return { state: 0, statusEvents: [], taskGroups: {}, runDuration: undefined };
}

export const JobStatus: MessageFns<JobStatus> = {
  encode(message: JobStatus, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.state !== 0) {
      writer.uint32(8).int32(message.state);
    }
    for (const v of message.statusEvents) {
      StatusEvent.encode(v!, writer.uint32(18).fork()).join();
    }
    Object.entries(message.taskGroups).forEach(([key, value]) => {
      JobStatus_TaskGroupsEntry.encode({ key: key as any, value }, writer.uint32(34).fork()).join();
    });
    if (message.runDuration !== undefined) {
      Duration.encode(message.runDuration, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobStatus {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobStatus();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.statusEvents.push(StatusEvent.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          const entry4 = JobStatus_TaskGroupsEntry.decode(reader, reader.uint32());
          if (entry4.value !== undefined) {
            message.taskGroups[entry4.key] = entry4.value;
          }
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.runDuration = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobStatus {
    return {
      state: isSet(object.state) ? jobStatus_StateFromJSON(object.state) : 0,
      statusEvents: globalThis.Array.isArray(object?.statusEvents)
        ? object.statusEvents.map((e: any) => StatusEvent.fromJSON(e))
        : [],
      taskGroups: isObject(object.taskGroups)
        ? Object.entries(object.taskGroups).reduce<{ [key: string]: JobStatus_TaskGroupStatus }>(
          (acc, [key, value]) => {
            acc[key] = JobStatus_TaskGroupStatus.fromJSON(value);
            return acc;
          },
          {},
        )
        : {},
      runDuration: isSet(object.runDuration) ? Duration.fromJSON(object.runDuration) : undefined,
    };
  },

  toJSON(message: JobStatus): unknown {
    const obj: any = {};
    if (message.state !== 0) {
      obj.state = jobStatus_StateToJSON(message.state);
    }
    if (message.statusEvents?.length) {
      obj.statusEvents = message.statusEvents.map((e) => StatusEvent.toJSON(e));
    }
    if (message.taskGroups) {
      const entries = Object.entries(message.taskGroups);
      if (entries.length > 0) {
        obj.taskGroups = {};
        entries.forEach(([k, v]) => {
          obj.taskGroups[k] = JobStatus_TaskGroupStatus.toJSON(v);
        });
      }
    }
    if (message.runDuration !== undefined) {
      obj.runDuration = Duration.toJSON(message.runDuration);
    }
    return obj;
  },

  create(base?: DeepPartial<JobStatus>): JobStatus {
    return JobStatus.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobStatus>): JobStatus {
    const message = createBaseJobStatus();
    message.state = object.state ?? 0;
    message.statusEvents = object.statusEvents?.map((e) => StatusEvent.fromPartial(e)) || [];
    message.taskGroups = Object.entries(object.taskGroups ?? {}).reduce<{ [key: string]: JobStatus_TaskGroupStatus }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = JobStatus_TaskGroupStatus.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    message.runDuration = (object.runDuration !== undefined && object.runDuration !== null)
      ? Duration.fromPartial(object.runDuration)
      : undefined;
    return message;
  },
};

function createBaseJobStatus_InstanceStatus(): JobStatus_InstanceStatus {
  return { machineType: "", provisioningModel: 0, taskPack: Long.ZERO, bootDisk: undefined };
}

export const JobStatus_InstanceStatus: MessageFns<JobStatus_InstanceStatus> = {
  encode(message: JobStatus_InstanceStatus, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.machineType !== "") {
      writer.uint32(10).string(message.machineType);
    }
    if (message.provisioningModel !== 0) {
      writer.uint32(16).int32(message.provisioningModel);
    }
    if (!message.taskPack.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.taskPack.toString());
    }
    if (message.bootDisk !== undefined) {
      AllocationPolicy_Disk.encode(message.bootDisk, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobStatus_InstanceStatus {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobStatus_InstanceStatus();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.machineType = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.provisioningModel = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.taskPack = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.bootDisk = AllocationPolicy_Disk.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobStatus_InstanceStatus {
    return {
      machineType: isSet(object.machineType) ? globalThis.String(object.machineType) : "",
      provisioningModel: isSet(object.provisioningModel)
        ? allocationPolicy_ProvisioningModelFromJSON(object.provisioningModel)
        : 0,
      taskPack: isSet(object.taskPack) ? Long.fromValue(object.taskPack) : Long.ZERO,
      bootDisk: isSet(object.bootDisk) ? AllocationPolicy_Disk.fromJSON(object.bootDisk) : undefined,
    };
  },

  toJSON(message: JobStatus_InstanceStatus): unknown {
    const obj: any = {};
    if (message.machineType !== "") {
      obj.machineType = message.machineType;
    }
    if (message.provisioningModel !== 0) {
      obj.provisioningModel = allocationPolicy_ProvisioningModelToJSON(message.provisioningModel);
    }
    if (!message.taskPack.equals(Long.ZERO)) {
      obj.taskPack = (message.taskPack || Long.ZERO).toString();
    }
    if (message.bootDisk !== undefined) {
      obj.bootDisk = AllocationPolicy_Disk.toJSON(message.bootDisk);
    }
    return obj;
  },

  create(base?: DeepPartial<JobStatus_InstanceStatus>): JobStatus_InstanceStatus {
    return JobStatus_InstanceStatus.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobStatus_InstanceStatus>): JobStatus_InstanceStatus {
    const message = createBaseJobStatus_InstanceStatus();
    message.machineType = object.machineType ?? "";
    message.provisioningModel = object.provisioningModel ?? 0;
    message.taskPack = (object.taskPack !== undefined && object.taskPack !== null)
      ? Long.fromValue(object.taskPack)
      : Long.ZERO;
    message.bootDisk = (object.bootDisk !== undefined && object.bootDisk !== null)
      ? AllocationPolicy_Disk.fromPartial(object.bootDisk)
      : undefined;
    return message;
  },
};

function createBaseJobStatus_TaskGroupStatus(): JobStatus_TaskGroupStatus {
  return { counts: {}, instances: [] };
}

export const JobStatus_TaskGroupStatus: MessageFns<JobStatus_TaskGroupStatus> = {
  encode(message: JobStatus_TaskGroupStatus, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.counts).forEach(([key, value]) => {
      JobStatus_TaskGroupStatus_CountsEntry.encode({ key: key as any, value }, writer.uint32(10).fork()).join();
    });
    for (const v of message.instances) {
      JobStatus_InstanceStatus.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobStatus_TaskGroupStatus {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobStatus_TaskGroupStatus();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          const entry1 = JobStatus_TaskGroupStatus_CountsEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.counts[entry1.key] = entry1.value;
          }
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.instances.push(JobStatus_InstanceStatus.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobStatus_TaskGroupStatus {
    return {
      counts: isObject(object.counts)
        ? Object.entries(object.counts).reduce<{ [key: string]: Long }>((acc, [key, value]) => {
          acc[key] = Long.fromValue(value as Long | string);
          return acc;
        }, {})
        : {},
      instances: globalThis.Array.isArray(object?.instances)
        ? object.instances.map((e: any) => JobStatus_InstanceStatus.fromJSON(e))
        : [],
    };
  },

  toJSON(message: JobStatus_TaskGroupStatus): unknown {
    const obj: any = {};
    if (message.counts) {
      const entries = Object.entries(message.counts);
      if (entries.length > 0) {
        obj.counts = {};
        entries.forEach(([k, v]) => {
          obj.counts[k] = v.toString();
        });
      }
    }
    if (message.instances?.length) {
      obj.instances = message.instances.map((e) => JobStatus_InstanceStatus.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<JobStatus_TaskGroupStatus>): JobStatus_TaskGroupStatus {
    return JobStatus_TaskGroupStatus.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobStatus_TaskGroupStatus>): JobStatus_TaskGroupStatus {
    const message = createBaseJobStatus_TaskGroupStatus();
    message.counts = Object.entries(object.counts ?? {}).reduce<{ [key: string]: Long }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = Long.fromValue(value);
      }
      return acc;
    }, {});
    message.instances = object.instances?.map((e) => JobStatus_InstanceStatus.fromPartial(e)) || [];
    return message;
  },
};

function createBaseJobStatus_TaskGroupStatus_CountsEntry(): JobStatus_TaskGroupStatus_CountsEntry {
  return { key: "", value: Long.ZERO };
}

export const JobStatus_TaskGroupStatus_CountsEntry: MessageFns<JobStatus_TaskGroupStatus_CountsEntry> = {
  encode(message: JobStatus_TaskGroupStatus_CountsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (!message.value.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.value.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobStatus_TaskGroupStatus_CountsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobStatus_TaskGroupStatus_CountsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.value = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobStatus_TaskGroupStatus_CountsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? Long.fromValue(object.value) : Long.ZERO,
    };
  },

  toJSON(message: JobStatus_TaskGroupStatus_CountsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (!message.value.equals(Long.ZERO)) {
      obj.value = (message.value || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<JobStatus_TaskGroupStatus_CountsEntry>): JobStatus_TaskGroupStatus_CountsEntry {
    return JobStatus_TaskGroupStatus_CountsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobStatus_TaskGroupStatus_CountsEntry>): JobStatus_TaskGroupStatus_CountsEntry {
    const message = createBaseJobStatus_TaskGroupStatus_CountsEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null) ? Long.fromValue(object.value) : Long.ZERO;
    return message;
  },
};

function createBaseJobStatus_TaskGroupsEntry(): JobStatus_TaskGroupsEntry {
  return { key: "", value: undefined };
}

export const JobStatus_TaskGroupsEntry: MessageFns<JobStatus_TaskGroupsEntry> = {
  encode(message: JobStatus_TaskGroupsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      JobStatus_TaskGroupStatus.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobStatus_TaskGroupsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobStatus_TaskGroupsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = JobStatus_TaskGroupStatus.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobStatus_TaskGroupsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? JobStatus_TaskGroupStatus.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: JobStatus_TaskGroupsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = JobStatus_TaskGroupStatus.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<JobStatus_TaskGroupsEntry>): JobStatus_TaskGroupsEntry {
    return JobStatus_TaskGroupsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobStatus_TaskGroupsEntry>): JobStatus_TaskGroupsEntry {
    const message = createBaseJobStatus_TaskGroupsEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? JobStatus_TaskGroupStatus.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseJobNotification(): JobNotification {
  return { pubsubTopic: "", message: undefined };
}

export const JobNotification: MessageFns<JobNotification> = {
  encode(message: JobNotification, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.pubsubTopic !== "") {
      writer.uint32(10).string(message.pubsubTopic);
    }
    if (message.message !== undefined) {
      JobNotification_Message.encode(message.message, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobNotification {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobNotification();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.pubsubTopic = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.message = JobNotification_Message.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobNotification {
    return {
      pubsubTopic: isSet(object.pubsubTopic) ? globalThis.String(object.pubsubTopic) : "",
      message: isSet(object.message) ? JobNotification_Message.fromJSON(object.message) : undefined,
    };
  },

  toJSON(message: JobNotification): unknown {
    const obj: any = {};
    if (message.pubsubTopic !== "") {
      obj.pubsubTopic = message.pubsubTopic;
    }
    if (message.message !== undefined) {
      obj.message = JobNotification_Message.toJSON(message.message);
    }
    return obj;
  },

  create(base?: DeepPartial<JobNotification>): JobNotification {
    return JobNotification.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobNotification>): JobNotification {
    const message = createBaseJobNotification();
    message.pubsubTopic = object.pubsubTopic ?? "";
    message.message = (object.message !== undefined && object.message !== null)
      ? JobNotification_Message.fromPartial(object.message)
      : undefined;
    return message;
  },
};

function createBaseJobNotification_Message(): JobNotification_Message {
  return { type: 0, newJobState: 0, newTaskState: 0 };
}

export const JobNotification_Message: MessageFns<JobNotification_Message> = {
  encode(message: JobNotification_Message, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.type !== 0) {
      writer.uint32(8).int32(message.type);
    }
    if (message.newJobState !== 0) {
      writer.uint32(16).int32(message.newJobState);
    }
    if (message.newTaskState !== 0) {
      writer.uint32(24).int32(message.newTaskState);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobNotification_Message {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobNotification_Message();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.newJobState = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.newTaskState = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobNotification_Message {
    return {
      type: isSet(object.type) ? jobNotification_TypeFromJSON(object.type) : 0,
      newJobState: isSet(object.newJobState) ? jobStatus_StateFromJSON(object.newJobState) : 0,
      newTaskState: isSet(object.newTaskState) ? taskStatus_StateFromJSON(object.newTaskState) : 0,
    };
  },

  toJSON(message: JobNotification_Message): unknown {
    const obj: any = {};
    if (message.type !== 0) {
      obj.type = jobNotification_TypeToJSON(message.type);
    }
    if (message.newJobState !== 0) {
      obj.newJobState = jobStatus_StateToJSON(message.newJobState);
    }
    if (message.newTaskState !== 0) {
      obj.newTaskState = taskStatus_StateToJSON(message.newTaskState);
    }
    return obj;
  },

  create(base?: DeepPartial<JobNotification_Message>): JobNotification_Message {
    return JobNotification_Message.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobNotification_Message>): JobNotification_Message {
    const message = createBaseJobNotification_Message();
    message.type = object.type ?? 0;
    message.newJobState = object.newJobState ?? 0;
    message.newTaskState = object.newTaskState ?? 0;
    return message;
  },
};

function createBaseAllocationPolicy(): AllocationPolicy {
  return {
    location: undefined,
    instances: [],
    serviceAccount: undefined,
    labels: {},
    network: undefined,
    placement: undefined,
    tags: [],
  };
}

export const AllocationPolicy: MessageFns<AllocationPolicy> = {
  encode(message: AllocationPolicy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.location !== undefined) {
      AllocationPolicy_LocationPolicy.encode(message.location, writer.uint32(10).fork()).join();
    }
    for (const v of message.instances) {
      AllocationPolicy_InstancePolicyOrTemplate.encode(v!, writer.uint32(66).fork()).join();
    }
    if (message.serviceAccount !== undefined) {
      ServiceAccount.encode(message.serviceAccount, writer.uint32(74).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      AllocationPolicy_LabelsEntry.encode({ key: key as any, value }, writer.uint32(50).fork()).join();
    });
    if (message.network !== undefined) {
      AllocationPolicy_NetworkPolicy.encode(message.network, writer.uint32(58).fork()).join();
    }
    if (message.placement !== undefined) {
      AllocationPolicy_PlacementPolicy.encode(message.placement, writer.uint32(82).fork()).join();
    }
    for (const v of message.tags) {
      writer.uint32(90).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AllocationPolicy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAllocationPolicy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.location = AllocationPolicy_LocationPolicy.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.instances.push(AllocationPolicy_InstancePolicyOrTemplate.decode(reader, reader.uint32()));
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.serviceAccount = ServiceAccount.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          const entry6 = AllocationPolicy_LabelsEntry.decode(reader, reader.uint32());
          if (entry6.value !== undefined) {
            message.labels[entry6.key] = entry6.value;
          }
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.network = AllocationPolicy_NetworkPolicy.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.placement = AllocationPolicy_PlacementPolicy.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.tags.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AllocationPolicy {
    return {
      location: isSet(object.location) ? AllocationPolicy_LocationPolicy.fromJSON(object.location) : undefined,
      instances: globalThis.Array.isArray(object?.instances)
        ? object.instances.map((e: any) => AllocationPolicy_InstancePolicyOrTemplate.fromJSON(e))
        : [],
      serviceAccount: isSet(object.serviceAccount) ? ServiceAccount.fromJSON(object.serviceAccount) : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      network: isSet(object.network) ? AllocationPolicy_NetworkPolicy.fromJSON(object.network) : undefined,
      placement: isSet(object.placement) ? AllocationPolicy_PlacementPolicy.fromJSON(object.placement) : undefined,
      tags: globalThis.Array.isArray(object?.tags) ? object.tags.map((e: any) => globalThis.String(e)) : [],
    };
  },

  toJSON(message: AllocationPolicy): unknown {
    const obj: any = {};
    if (message.location !== undefined) {
      obj.location = AllocationPolicy_LocationPolicy.toJSON(message.location);
    }
    if (message.instances?.length) {
      obj.instances = message.instances.map((e) => AllocationPolicy_InstancePolicyOrTemplate.toJSON(e));
    }
    if (message.serviceAccount !== undefined) {
      obj.serviceAccount = ServiceAccount.toJSON(message.serviceAccount);
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.network !== undefined) {
      obj.network = AllocationPolicy_NetworkPolicy.toJSON(message.network);
    }
    if (message.placement !== undefined) {
      obj.placement = AllocationPolicy_PlacementPolicy.toJSON(message.placement);
    }
    if (message.tags?.length) {
      obj.tags = message.tags;
    }
    return obj;
  },

  create(base?: DeepPartial<AllocationPolicy>): AllocationPolicy {
    return AllocationPolicy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AllocationPolicy>): AllocationPolicy {
    const message = createBaseAllocationPolicy();
    message.location = (object.location !== undefined && object.location !== null)
      ? AllocationPolicy_LocationPolicy.fromPartial(object.location)
      : undefined;
    message.instances = object.instances?.map((e) => AllocationPolicy_InstancePolicyOrTemplate.fromPartial(e)) || [];
    message.serviceAccount = (object.serviceAccount !== undefined && object.serviceAccount !== null)
      ? ServiceAccount.fromPartial(object.serviceAccount)
      : undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.network = (object.network !== undefined && object.network !== null)
      ? AllocationPolicy_NetworkPolicy.fromPartial(object.network)
      : undefined;
    message.placement = (object.placement !== undefined && object.placement !== null)
      ? AllocationPolicy_PlacementPolicy.fromPartial(object.placement)
      : undefined;
    message.tags = object.tags?.map((e) => e) || [];
    return message;
  },
};

function createBaseAllocationPolicy_LocationPolicy(): AllocationPolicy_LocationPolicy {
  return { allowedLocations: [] };
}

export const AllocationPolicy_LocationPolicy: MessageFns<AllocationPolicy_LocationPolicy> = {
  encode(message: AllocationPolicy_LocationPolicy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.allowedLocations) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AllocationPolicy_LocationPolicy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAllocationPolicy_LocationPolicy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.allowedLocations.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AllocationPolicy_LocationPolicy {
    return {
      allowedLocations: globalThis.Array.isArray(object?.allowedLocations)
        ? object.allowedLocations.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: AllocationPolicy_LocationPolicy): unknown {
    const obj: any = {};
    if (message.allowedLocations?.length) {
      obj.allowedLocations = message.allowedLocations;
    }
    return obj;
  },

  create(base?: DeepPartial<AllocationPolicy_LocationPolicy>): AllocationPolicy_LocationPolicy {
    return AllocationPolicy_LocationPolicy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AllocationPolicy_LocationPolicy>): AllocationPolicy_LocationPolicy {
    const message = createBaseAllocationPolicy_LocationPolicy();
    message.allowedLocations = object.allowedLocations?.map((e) => e) || [];
    return message;
  },
};

function createBaseAllocationPolicy_Disk(): AllocationPolicy_Disk {
  return { image: undefined, snapshot: undefined, type: "", sizeGb: Long.ZERO, diskInterface: "" };
}

export const AllocationPolicy_Disk: MessageFns<AllocationPolicy_Disk> = {
  encode(message: AllocationPolicy_Disk, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.image !== undefined) {
      writer.uint32(34).string(message.image);
    }
    if (message.snapshot !== undefined) {
      writer.uint32(42).string(message.snapshot);
    }
    if (message.type !== "") {
      writer.uint32(10).string(message.type);
    }
    if (!message.sizeGb.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.sizeGb.toString());
    }
    if (message.diskInterface !== "") {
      writer.uint32(50).string(message.diskInterface);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AllocationPolicy_Disk {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAllocationPolicy_Disk();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 4:
          if (tag !== 34) {
            break;
          }

          message.image = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.snapshot = reader.string();
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.type = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.sizeGb = Long.fromString(reader.int64().toString());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.diskInterface = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AllocationPolicy_Disk {
    return {
      image: isSet(object.image) ? globalThis.String(object.image) : undefined,
      snapshot: isSet(object.snapshot) ? globalThis.String(object.snapshot) : undefined,
      type: isSet(object.type) ? globalThis.String(object.type) : "",
      sizeGb: isSet(object.sizeGb) ? Long.fromValue(object.sizeGb) : Long.ZERO,
      diskInterface: isSet(object.diskInterface) ? globalThis.String(object.diskInterface) : "",
    };
  },

  toJSON(message: AllocationPolicy_Disk): unknown {
    const obj: any = {};
    if (message.image !== undefined) {
      obj.image = message.image;
    }
    if (message.snapshot !== undefined) {
      obj.snapshot = message.snapshot;
    }
    if (message.type !== "") {
      obj.type = message.type;
    }
    if (!message.sizeGb.equals(Long.ZERO)) {
      obj.sizeGb = (message.sizeGb || Long.ZERO).toString();
    }
    if (message.diskInterface !== "") {
      obj.diskInterface = message.diskInterface;
    }
    return obj;
  },

  create(base?: DeepPartial<AllocationPolicy_Disk>): AllocationPolicy_Disk {
    return AllocationPolicy_Disk.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AllocationPolicy_Disk>): AllocationPolicy_Disk {
    const message = createBaseAllocationPolicy_Disk();
    message.image = object.image ?? undefined;
    message.snapshot = object.snapshot ?? undefined;
    message.type = object.type ?? "";
    message.sizeGb = (object.sizeGb !== undefined && object.sizeGb !== null)
      ? Long.fromValue(object.sizeGb)
      : Long.ZERO;
    message.diskInterface = object.diskInterface ?? "";
    return message;
  },
};

function createBaseAllocationPolicy_AttachedDisk(): AllocationPolicy_AttachedDisk {
  return { newDisk: undefined, existingDisk: undefined, deviceName: "" };
}

export const AllocationPolicy_AttachedDisk: MessageFns<AllocationPolicy_AttachedDisk> = {
  encode(message: AllocationPolicy_AttachedDisk, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.newDisk !== undefined) {
      AllocationPolicy_Disk.encode(message.newDisk, writer.uint32(10).fork()).join();
    }
    if (message.existingDisk !== undefined) {
      writer.uint32(18).string(message.existingDisk);
    }
    if (message.deviceName !== "") {
      writer.uint32(26).string(message.deviceName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AllocationPolicy_AttachedDisk {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAllocationPolicy_AttachedDisk();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.newDisk = AllocationPolicy_Disk.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.existingDisk = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.deviceName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AllocationPolicy_AttachedDisk {
    return {
      newDisk: isSet(object.newDisk) ? AllocationPolicy_Disk.fromJSON(object.newDisk) : undefined,
      existingDisk: isSet(object.existingDisk) ? globalThis.String(object.existingDisk) : undefined,
      deviceName: isSet(object.deviceName) ? globalThis.String(object.deviceName) : "",
    };
  },

  toJSON(message: AllocationPolicy_AttachedDisk): unknown {
    const obj: any = {};
    if (message.newDisk !== undefined) {
      obj.newDisk = AllocationPolicy_Disk.toJSON(message.newDisk);
    }
    if (message.existingDisk !== undefined) {
      obj.existingDisk = message.existingDisk;
    }
    if (message.deviceName !== "") {
      obj.deviceName = message.deviceName;
    }
    return obj;
  },

  create(base?: DeepPartial<AllocationPolicy_AttachedDisk>): AllocationPolicy_AttachedDisk {
    return AllocationPolicy_AttachedDisk.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AllocationPolicy_AttachedDisk>): AllocationPolicy_AttachedDisk {
    const message = createBaseAllocationPolicy_AttachedDisk();
    message.newDisk = (object.newDisk !== undefined && object.newDisk !== null)
      ? AllocationPolicy_Disk.fromPartial(object.newDisk)
      : undefined;
    message.existingDisk = object.existingDisk ?? undefined;
    message.deviceName = object.deviceName ?? "";
    return message;
  },
};

function createBaseAllocationPolicy_Accelerator(): AllocationPolicy_Accelerator {
  return { type: "", count: Long.ZERO, installGpuDrivers: false, driverVersion: "" };
}

export const AllocationPolicy_Accelerator: MessageFns<AllocationPolicy_Accelerator> = {
  encode(message: AllocationPolicy_Accelerator, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.type !== "") {
      writer.uint32(10).string(message.type);
    }
    if (!message.count.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.count.toString());
    }
    if (message.installGpuDrivers !== false) {
      writer.uint32(24).bool(message.installGpuDrivers);
    }
    if (message.driverVersion !== "") {
      writer.uint32(34).string(message.driverVersion);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AllocationPolicy_Accelerator {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAllocationPolicy_Accelerator();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.type = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.count = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.installGpuDrivers = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.driverVersion = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AllocationPolicy_Accelerator {
    return {
      type: isSet(object.type) ? globalThis.String(object.type) : "",
      count: isSet(object.count) ? Long.fromValue(object.count) : Long.ZERO,
      installGpuDrivers: isSet(object.installGpuDrivers) ? globalThis.Boolean(object.installGpuDrivers) : false,
      driverVersion: isSet(object.driverVersion) ? globalThis.String(object.driverVersion) : "",
    };
  },

  toJSON(message: AllocationPolicy_Accelerator): unknown {
    const obj: any = {};
    if (message.type !== "") {
      obj.type = message.type;
    }
    if (!message.count.equals(Long.ZERO)) {
      obj.count = (message.count || Long.ZERO).toString();
    }
    if (message.installGpuDrivers !== false) {
      obj.installGpuDrivers = message.installGpuDrivers;
    }
    if (message.driverVersion !== "") {
      obj.driverVersion = message.driverVersion;
    }
    return obj;
  },

  create(base?: DeepPartial<AllocationPolicy_Accelerator>): AllocationPolicy_Accelerator {
    return AllocationPolicy_Accelerator.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AllocationPolicy_Accelerator>): AllocationPolicy_Accelerator {
    const message = createBaseAllocationPolicy_Accelerator();
    message.type = object.type ?? "";
    message.count = (object.count !== undefined && object.count !== null) ? Long.fromValue(object.count) : Long.ZERO;
    message.installGpuDrivers = object.installGpuDrivers ?? false;
    message.driverVersion = object.driverVersion ?? "";
    return message;
  },
};

function createBaseAllocationPolicy_InstancePolicy(): AllocationPolicy_InstancePolicy {
  return {
    machineType: "",
    minCpuPlatform: "",
    provisioningModel: 0,
    accelerators: [],
    bootDisk: undefined,
    disks: [],
    reservation: "",
  };
}

export const AllocationPolicy_InstancePolicy: MessageFns<AllocationPolicy_InstancePolicy> = {
  encode(message: AllocationPolicy_InstancePolicy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.machineType !== "") {
      writer.uint32(18).string(message.machineType);
    }
    if (message.minCpuPlatform !== "") {
      writer.uint32(26).string(message.minCpuPlatform);
    }
    if (message.provisioningModel !== 0) {
      writer.uint32(32).int32(message.provisioningModel);
    }
    for (const v of message.accelerators) {
      AllocationPolicy_Accelerator.encode(v!, writer.uint32(42).fork()).join();
    }
    if (message.bootDisk !== undefined) {
      AllocationPolicy_Disk.encode(message.bootDisk, writer.uint32(66).fork()).join();
    }
    for (const v of message.disks) {
      AllocationPolicy_AttachedDisk.encode(v!, writer.uint32(50).fork()).join();
    }
    if (message.reservation !== "") {
      writer.uint32(58).string(message.reservation);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AllocationPolicy_InstancePolicy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAllocationPolicy_InstancePolicy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.machineType = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.minCpuPlatform = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.provisioningModel = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.accelerators.push(AllocationPolicy_Accelerator.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.bootDisk = AllocationPolicy_Disk.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.disks.push(AllocationPolicy_AttachedDisk.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.reservation = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AllocationPolicy_InstancePolicy {
    return {
      machineType: isSet(object.machineType) ? globalThis.String(object.machineType) : "",
      minCpuPlatform: isSet(object.minCpuPlatform) ? globalThis.String(object.minCpuPlatform) : "",
      provisioningModel: isSet(object.provisioningModel)
        ? allocationPolicy_ProvisioningModelFromJSON(object.provisioningModel)
        : 0,
      accelerators: globalThis.Array.isArray(object?.accelerators)
        ? object.accelerators.map((e: any) => AllocationPolicy_Accelerator.fromJSON(e))
        : [],
      bootDisk: isSet(object.bootDisk) ? AllocationPolicy_Disk.fromJSON(object.bootDisk) : undefined,
      disks: globalThis.Array.isArray(object?.disks)
        ? object.disks.map((e: any) => AllocationPolicy_AttachedDisk.fromJSON(e))
        : [],
      reservation: isSet(object.reservation) ? globalThis.String(object.reservation) : "",
    };
  },

  toJSON(message: AllocationPolicy_InstancePolicy): unknown {
    const obj: any = {};
    if (message.machineType !== "") {
      obj.machineType = message.machineType;
    }
    if (message.minCpuPlatform !== "") {
      obj.minCpuPlatform = message.minCpuPlatform;
    }
    if (message.provisioningModel !== 0) {
      obj.provisioningModel = allocationPolicy_ProvisioningModelToJSON(message.provisioningModel);
    }
    if (message.accelerators?.length) {
      obj.accelerators = message.accelerators.map((e) => AllocationPolicy_Accelerator.toJSON(e));
    }
    if (message.bootDisk !== undefined) {
      obj.bootDisk = AllocationPolicy_Disk.toJSON(message.bootDisk);
    }
    if (message.disks?.length) {
      obj.disks = message.disks.map((e) => AllocationPolicy_AttachedDisk.toJSON(e));
    }
    if (message.reservation !== "") {
      obj.reservation = message.reservation;
    }
    return obj;
  },

  create(base?: DeepPartial<AllocationPolicy_InstancePolicy>): AllocationPolicy_InstancePolicy {
    return AllocationPolicy_InstancePolicy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AllocationPolicy_InstancePolicy>): AllocationPolicy_InstancePolicy {
    const message = createBaseAllocationPolicy_InstancePolicy();
    message.machineType = object.machineType ?? "";
    message.minCpuPlatform = object.minCpuPlatform ?? "";
    message.provisioningModel = object.provisioningModel ?? 0;
    message.accelerators = object.accelerators?.map((e) => AllocationPolicy_Accelerator.fromPartial(e)) || [];
    message.bootDisk = (object.bootDisk !== undefined && object.bootDisk !== null)
      ? AllocationPolicy_Disk.fromPartial(object.bootDisk)
      : undefined;
    message.disks = object.disks?.map((e) => AllocationPolicy_AttachedDisk.fromPartial(e)) || [];
    message.reservation = object.reservation ?? "";
    return message;
  },
};

function createBaseAllocationPolicy_InstancePolicyOrTemplate(): AllocationPolicy_InstancePolicyOrTemplate {
  return {
    policy: undefined,
    instanceTemplate: undefined,
    installGpuDrivers: false,
    installOpsAgent: false,
    blockProjectSshKeys: false,
  };
}

export const AllocationPolicy_InstancePolicyOrTemplate: MessageFns<AllocationPolicy_InstancePolicyOrTemplate> = {
  encode(message: AllocationPolicy_InstancePolicyOrTemplate, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.policy !== undefined) {
      AllocationPolicy_InstancePolicy.encode(message.policy, writer.uint32(10).fork()).join();
    }
    if (message.instanceTemplate !== undefined) {
      writer.uint32(18).string(message.instanceTemplate);
    }
    if (message.installGpuDrivers !== false) {
      writer.uint32(24).bool(message.installGpuDrivers);
    }
    if (message.installOpsAgent !== false) {
      writer.uint32(32).bool(message.installOpsAgent);
    }
    if (message.blockProjectSshKeys !== false) {
      writer.uint32(40).bool(message.blockProjectSshKeys);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AllocationPolicy_InstancePolicyOrTemplate {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAllocationPolicy_InstancePolicyOrTemplate();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.policy = AllocationPolicy_InstancePolicy.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.instanceTemplate = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.installGpuDrivers = reader.bool();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.installOpsAgent = reader.bool();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.blockProjectSshKeys = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AllocationPolicy_InstancePolicyOrTemplate {
    return {
      policy: isSet(object.policy) ? AllocationPolicy_InstancePolicy.fromJSON(object.policy) : undefined,
      instanceTemplate: isSet(object.instanceTemplate) ? globalThis.String(object.instanceTemplate) : undefined,
      installGpuDrivers: isSet(object.installGpuDrivers) ? globalThis.Boolean(object.installGpuDrivers) : false,
      installOpsAgent: isSet(object.installOpsAgent) ? globalThis.Boolean(object.installOpsAgent) : false,
      blockProjectSshKeys: isSet(object.blockProjectSshKeys) ? globalThis.Boolean(object.blockProjectSshKeys) : false,
    };
  },

  toJSON(message: AllocationPolicy_InstancePolicyOrTemplate): unknown {
    const obj: any = {};
    if (message.policy !== undefined) {
      obj.policy = AllocationPolicy_InstancePolicy.toJSON(message.policy);
    }
    if (message.instanceTemplate !== undefined) {
      obj.instanceTemplate = message.instanceTemplate;
    }
    if (message.installGpuDrivers !== false) {
      obj.installGpuDrivers = message.installGpuDrivers;
    }
    if (message.installOpsAgent !== false) {
      obj.installOpsAgent = message.installOpsAgent;
    }
    if (message.blockProjectSshKeys !== false) {
      obj.blockProjectSshKeys = message.blockProjectSshKeys;
    }
    return obj;
  },

  create(base?: DeepPartial<AllocationPolicy_InstancePolicyOrTemplate>): AllocationPolicy_InstancePolicyOrTemplate {
    return AllocationPolicy_InstancePolicyOrTemplate.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<AllocationPolicy_InstancePolicyOrTemplate>,
  ): AllocationPolicy_InstancePolicyOrTemplate {
    const message = createBaseAllocationPolicy_InstancePolicyOrTemplate();
    message.policy = (object.policy !== undefined && object.policy !== null)
      ? AllocationPolicy_InstancePolicy.fromPartial(object.policy)
      : undefined;
    message.instanceTemplate = object.instanceTemplate ?? undefined;
    message.installGpuDrivers = object.installGpuDrivers ?? false;
    message.installOpsAgent = object.installOpsAgent ?? false;
    message.blockProjectSshKeys = object.blockProjectSshKeys ?? false;
    return message;
  },
};

function createBaseAllocationPolicy_NetworkInterface(): AllocationPolicy_NetworkInterface {
  return { network: "", subnetwork: "", noExternalIpAddress: false };
}

export const AllocationPolicy_NetworkInterface: MessageFns<AllocationPolicy_NetworkInterface> = {
  encode(message: AllocationPolicy_NetworkInterface, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.network !== "") {
      writer.uint32(10).string(message.network);
    }
    if (message.subnetwork !== "") {
      writer.uint32(18).string(message.subnetwork);
    }
    if (message.noExternalIpAddress !== false) {
      writer.uint32(24).bool(message.noExternalIpAddress);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AllocationPolicy_NetworkInterface {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAllocationPolicy_NetworkInterface();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.network = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.subnetwork = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.noExternalIpAddress = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AllocationPolicy_NetworkInterface {
    return {
      network: isSet(object.network) ? globalThis.String(object.network) : "",
      subnetwork: isSet(object.subnetwork) ? globalThis.String(object.subnetwork) : "",
      noExternalIpAddress: isSet(object.noExternalIpAddress) ? globalThis.Boolean(object.noExternalIpAddress) : false,
    };
  },

  toJSON(message: AllocationPolicy_NetworkInterface): unknown {
    const obj: any = {};
    if (message.network !== "") {
      obj.network = message.network;
    }
    if (message.subnetwork !== "") {
      obj.subnetwork = message.subnetwork;
    }
    if (message.noExternalIpAddress !== false) {
      obj.noExternalIpAddress = message.noExternalIpAddress;
    }
    return obj;
  },

  create(base?: DeepPartial<AllocationPolicy_NetworkInterface>): AllocationPolicy_NetworkInterface {
    return AllocationPolicy_NetworkInterface.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AllocationPolicy_NetworkInterface>): AllocationPolicy_NetworkInterface {
    const message = createBaseAllocationPolicy_NetworkInterface();
    message.network = object.network ?? "";
    message.subnetwork = object.subnetwork ?? "";
    message.noExternalIpAddress = object.noExternalIpAddress ?? false;
    return message;
  },
};

function createBaseAllocationPolicy_NetworkPolicy(): AllocationPolicy_NetworkPolicy {
  return { networkInterfaces: [] };
}

export const AllocationPolicy_NetworkPolicy: MessageFns<AllocationPolicy_NetworkPolicy> = {
  encode(message: AllocationPolicy_NetworkPolicy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.networkInterfaces) {
      AllocationPolicy_NetworkInterface.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AllocationPolicy_NetworkPolicy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAllocationPolicy_NetworkPolicy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.networkInterfaces.push(AllocationPolicy_NetworkInterface.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AllocationPolicy_NetworkPolicy {
    return {
      networkInterfaces: globalThis.Array.isArray(object?.networkInterfaces)
        ? object.networkInterfaces.map((e: any) => AllocationPolicy_NetworkInterface.fromJSON(e))
        : [],
    };
  },

  toJSON(message: AllocationPolicy_NetworkPolicy): unknown {
    const obj: any = {};
    if (message.networkInterfaces?.length) {
      obj.networkInterfaces = message.networkInterfaces.map((e) => AllocationPolicy_NetworkInterface.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<AllocationPolicy_NetworkPolicy>): AllocationPolicy_NetworkPolicy {
    return AllocationPolicy_NetworkPolicy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AllocationPolicy_NetworkPolicy>): AllocationPolicy_NetworkPolicy {
    const message = createBaseAllocationPolicy_NetworkPolicy();
    message.networkInterfaces =
      object.networkInterfaces?.map((e) => AllocationPolicy_NetworkInterface.fromPartial(e)) || [];
    return message;
  },
};

function createBaseAllocationPolicy_PlacementPolicy(): AllocationPolicy_PlacementPolicy {
  return { collocation: "", maxDistance: Long.ZERO };
}

export const AllocationPolicy_PlacementPolicy: MessageFns<AllocationPolicy_PlacementPolicy> = {
  encode(message: AllocationPolicy_PlacementPolicy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.collocation !== "") {
      writer.uint32(10).string(message.collocation);
    }
    if (!message.maxDistance.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.maxDistance.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AllocationPolicy_PlacementPolicy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAllocationPolicy_PlacementPolicy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.collocation = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.maxDistance = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AllocationPolicy_PlacementPolicy {
    return {
      collocation: isSet(object.collocation) ? globalThis.String(object.collocation) : "",
      maxDistance: isSet(object.maxDistance) ? Long.fromValue(object.maxDistance) : Long.ZERO,
    };
  },

  toJSON(message: AllocationPolicy_PlacementPolicy): unknown {
    const obj: any = {};
    if (message.collocation !== "") {
      obj.collocation = message.collocation;
    }
    if (!message.maxDistance.equals(Long.ZERO)) {
      obj.maxDistance = (message.maxDistance || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<AllocationPolicy_PlacementPolicy>): AllocationPolicy_PlacementPolicy {
    return AllocationPolicy_PlacementPolicy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AllocationPolicy_PlacementPolicy>): AllocationPolicy_PlacementPolicy {
    const message = createBaseAllocationPolicy_PlacementPolicy();
    message.collocation = object.collocation ?? "";
    message.maxDistance = (object.maxDistance !== undefined && object.maxDistance !== null)
      ? Long.fromValue(object.maxDistance)
      : Long.ZERO;
    return message;
  },
};

function createBaseAllocationPolicy_LabelsEntry(): AllocationPolicy_LabelsEntry {
  return { key: "", value: "" };
}

export const AllocationPolicy_LabelsEntry: MessageFns<AllocationPolicy_LabelsEntry> = {
  encode(message: AllocationPolicy_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AllocationPolicy_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAllocationPolicy_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AllocationPolicy_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: AllocationPolicy_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<AllocationPolicy_LabelsEntry>): AllocationPolicy_LabelsEntry {
    return AllocationPolicy_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AllocationPolicy_LabelsEntry>): AllocationPolicy_LabelsEntry {
    const message = createBaseAllocationPolicy_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseTaskGroup(): TaskGroup {
  return {
    name: "",
    taskSpec: undefined,
    taskCount: Long.ZERO,
    parallelism: Long.ZERO,
    schedulingPolicy: 0,
    taskEnvironments: [],
    taskCountPerNode: Long.ZERO,
    requireHostsFile: false,
    permissiveSsh: false,
    runAsNonRoot: false,
  };
}

export const TaskGroup: MessageFns<TaskGroup> = {
  encode(message: TaskGroup, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.taskSpec !== undefined) {
      TaskSpec.encode(message.taskSpec, writer.uint32(26).fork()).join();
    }
    if (!message.taskCount.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.taskCount.toString());
    }
    if (!message.parallelism.equals(Long.ZERO)) {
      writer.uint32(40).int64(message.parallelism.toString());
    }
    if (message.schedulingPolicy !== 0) {
      writer.uint32(48).int32(message.schedulingPolicy);
    }
    for (const v of message.taskEnvironments) {
      Environment.encode(v!, writer.uint32(74).fork()).join();
    }
    if (!message.taskCountPerNode.equals(Long.ZERO)) {
      writer.uint32(80).int64(message.taskCountPerNode.toString());
    }
    if (message.requireHostsFile !== false) {
      writer.uint32(88).bool(message.requireHostsFile);
    }
    if (message.permissiveSsh !== false) {
      writer.uint32(96).bool(message.permissiveSsh);
    }
    if (message.runAsNonRoot !== false) {
      writer.uint32(112).bool(message.runAsNonRoot);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TaskGroup {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTaskGroup();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.taskSpec = TaskSpec.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.taskCount = Long.fromString(reader.int64().toString());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.parallelism = Long.fromString(reader.int64().toString());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.schedulingPolicy = reader.int32() as any;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.taskEnvironments.push(Environment.decode(reader, reader.uint32()));
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.taskCountPerNode = Long.fromString(reader.int64().toString());
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.requireHostsFile = reader.bool();
          continue;
        case 12:
          if (tag !== 96) {
            break;
          }

          message.permissiveSsh = reader.bool();
          continue;
        case 14:
          if (tag !== 112) {
            break;
          }

          message.runAsNonRoot = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TaskGroup {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      taskSpec: isSet(object.taskSpec) ? TaskSpec.fromJSON(object.taskSpec) : undefined,
      taskCount: isSet(object.taskCount) ? Long.fromValue(object.taskCount) : Long.ZERO,
      parallelism: isSet(object.parallelism) ? Long.fromValue(object.parallelism) : Long.ZERO,
      schedulingPolicy: isSet(object.schedulingPolicy)
        ? taskGroup_SchedulingPolicyFromJSON(object.schedulingPolicy)
        : 0,
      taskEnvironments: globalThis.Array.isArray(object?.taskEnvironments)
        ? object.taskEnvironments.map((e: any) => Environment.fromJSON(e))
        : [],
      taskCountPerNode: isSet(object.taskCountPerNode) ? Long.fromValue(object.taskCountPerNode) : Long.ZERO,
      requireHostsFile: isSet(object.requireHostsFile) ? globalThis.Boolean(object.requireHostsFile) : false,
      permissiveSsh: isSet(object.permissiveSsh) ? globalThis.Boolean(object.permissiveSsh) : false,
      runAsNonRoot: isSet(object.runAsNonRoot) ? globalThis.Boolean(object.runAsNonRoot) : false,
    };
  },

  toJSON(message: TaskGroup): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.taskSpec !== undefined) {
      obj.taskSpec = TaskSpec.toJSON(message.taskSpec);
    }
    if (!message.taskCount.equals(Long.ZERO)) {
      obj.taskCount = (message.taskCount || Long.ZERO).toString();
    }
    if (!message.parallelism.equals(Long.ZERO)) {
      obj.parallelism = (message.parallelism || Long.ZERO).toString();
    }
    if (message.schedulingPolicy !== 0) {
      obj.schedulingPolicy = taskGroup_SchedulingPolicyToJSON(message.schedulingPolicy);
    }
    if (message.taskEnvironments?.length) {
      obj.taskEnvironments = message.taskEnvironments.map((e) => Environment.toJSON(e));
    }
    if (!message.taskCountPerNode.equals(Long.ZERO)) {
      obj.taskCountPerNode = (message.taskCountPerNode || Long.ZERO).toString();
    }
    if (message.requireHostsFile !== false) {
      obj.requireHostsFile = message.requireHostsFile;
    }
    if (message.permissiveSsh !== false) {
      obj.permissiveSsh = message.permissiveSsh;
    }
    if (message.runAsNonRoot !== false) {
      obj.runAsNonRoot = message.runAsNonRoot;
    }
    return obj;
  },

  create(base?: DeepPartial<TaskGroup>): TaskGroup {
    return TaskGroup.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TaskGroup>): TaskGroup {
    const message = createBaseTaskGroup();
    message.name = object.name ?? "";
    message.taskSpec = (object.taskSpec !== undefined && object.taskSpec !== null)
      ? TaskSpec.fromPartial(object.taskSpec)
      : undefined;
    message.taskCount = (object.taskCount !== undefined && object.taskCount !== null)
      ? Long.fromValue(object.taskCount)
      : Long.ZERO;
    message.parallelism = (object.parallelism !== undefined && object.parallelism !== null)
      ? Long.fromValue(object.parallelism)
      : Long.ZERO;
    message.schedulingPolicy = object.schedulingPolicy ?? 0;
    message.taskEnvironments = object.taskEnvironments?.map((e) => Environment.fromPartial(e)) || [];
    message.taskCountPerNode = (object.taskCountPerNode !== undefined && object.taskCountPerNode !== null)
      ? Long.fromValue(object.taskCountPerNode)
      : Long.ZERO;
    message.requireHostsFile = object.requireHostsFile ?? false;
    message.permissiveSsh = object.permissiveSsh ?? false;
    message.runAsNonRoot = object.runAsNonRoot ?? false;
    return message;
  },
};

function createBaseServiceAccount(): ServiceAccount {
  return { email: "", scopes: [] };
}

export const ServiceAccount: MessageFns<ServiceAccount> = {
  encode(message: ServiceAccount, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.email !== "") {
      writer.uint32(10).string(message.email);
    }
    for (const v of message.scopes) {
      writer.uint32(18).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ServiceAccount {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseServiceAccount();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.email = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.scopes.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ServiceAccount {
    return {
      email: isSet(object.email) ? globalThis.String(object.email) : "",
      scopes: globalThis.Array.isArray(object?.scopes) ? object.scopes.map((e: any) => globalThis.String(e)) : [],
    };
  },

  toJSON(message: ServiceAccount): unknown {
    const obj: any = {};
    if (message.email !== "") {
      obj.email = message.email;
    }
    if (message.scopes?.length) {
      obj.scopes = message.scopes;
    }
    return obj;
  },

  create(base?: DeepPartial<ServiceAccount>): ServiceAccount {
    return ServiceAccount.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ServiceAccount>): ServiceAccount {
    const message = createBaseServiceAccount();
    message.email = object.email ?? "";
    message.scopes = object.scopes?.map((e) => e) || [];
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
