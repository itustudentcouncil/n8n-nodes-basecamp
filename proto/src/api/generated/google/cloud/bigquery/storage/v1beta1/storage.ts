// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/bigquery/storage/v1beta1/storage.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Empty } from "../../../../protobuf/empty.js";
import { Timestamp } from "../../../../protobuf/timestamp.js";
import { ArrowRecordBatch, ArrowSchema } from "./arrow.js";
import { AvroRows, AvroSchema } from "./avro.js";
import { TableReadOptions } from "./read_options.js";
import { TableModifiers, TableReference } from "./table_reference.js";

export const protobufPackage = "google.cloud.bigquery.storage.v1beta1";

/** Data format for input or output data. */
export enum DataFormat {
  /** DATA_FORMAT_UNSPECIFIED - Data format is unspecified. */
  DATA_FORMAT_UNSPECIFIED = 0,
  /**
   * AVRO - Avro is a standard open source row based file format.
   * See https://avro.apache.org/ for more details.
   */
  AVRO = 1,
  /**
   * ARROW - Arrow is a standard open source column-based message format.
   * See https://arrow.apache.org/ for more details.
   */
  ARROW = 3,
  UNRECOGNIZED = -1,
}

export function dataFormatFromJSON(object: any): DataFormat {
  switch (object) {
    case 0:
    case "DATA_FORMAT_UNSPECIFIED":
      return DataFormat.DATA_FORMAT_UNSPECIFIED;
    case 1:
    case "AVRO":
      return DataFormat.AVRO;
    case 3:
    case "ARROW":
      return DataFormat.ARROW;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DataFormat.UNRECOGNIZED;
  }
}

export function dataFormatToJSON(object: DataFormat): string {
  switch (object) {
    case DataFormat.DATA_FORMAT_UNSPECIFIED:
      return "DATA_FORMAT_UNSPECIFIED";
    case DataFormat.AVRO:
      return "AVRO";
    case DataFormat.ARROW:
      return "ARROW";
    case DataFormat.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Strategy for distributing data among multiple streams in a read session. */
export enum ShardingStrategy {
  /** SHARDING_STRATEGY_UNSPECIFIED - Same as LIQUID. */
  SHARDING_STRATEGY_UNSPECIFIED = 0,
  /**
   * LIQUID - Assigns data to each stream based on the client's read rate. The faster the
   * client reads from a stream, the more data is assigned to the stream. In
   * this strategy, it's possible to read all data from a single stream even if
   * there are other streams present.
   */
  LIQUID = 1,
  /**
   * BALANCED - Assigns data to each stream such that roughly the same number of rows can
   * be read from each stream. Because the server-side unit for assigning data
   * is collections of rows, the API does not guarantee that each stream will
   * return the same number or rows. Additionally, the limits are enforced based
   * on the number of pre-filtering rows, so some filters can lead to lopsided
   * assignments.
   */
  BALANCED = 2,
  UNRECOGNIZED = -1,
}

export function shardingStrategyFromJSON(object: any): ShardingStrategy {
  switch (object) {
    case 0:
    case "SHARDING_STRATEGY_UNSPECIFIED":
      return ShardingStrategy.SHARDING_STRATEGY_UNSPECIFIED;
    case 1:
    case "LIQUID":
      return ShardingStrategy.LIQUID;
    case 2:
    case "BALANCED":
      return ShardingStrategy.BALANCED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ShardingStrategy.UNRECOGNIZED;
  }
}

export function shardingStrategyToJSON(object: ShardingStrategy): string {
  switch (object) {
    case ShardingStrategy.SHARDING_STRATEGY_UNSPECIFIED:
      return "SHARDING_STRATEGY_UNSPECIFIED";
    case ShardingStrategy.LIQUID:
      return "LIQUID";
    case ShardingStrategy.BALANCED:
      return "BALANCED";
    case ShardingStrategy.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Information about a single data stream within a read session. */
export interface Stream {
  /**
   * Name of the stream, in the form
   * `projects/{project_id}/locations/{location}/streams/{stream_id}`.
   */
  name: string;
}

/** Expresses a point within a given stream using an offset position. */
export interface StreamPosition {
  /** Identifier for a given Stream. */
  stream:
    | Stream
    | undefined;
  /** Position in the stream. */
  offset: Long;
}

/** Information returned from a `CreateReadSession` request. */
export interface ReadSession {
  /**
   * Unique identifier for the session, in the form
   * `projects/{project_id}/locations/{location}/sessions/{session_id}`.
   */
  name: string;
  /**
   * Time at which the session becomes invalid. After this time, subsequent
   * requests to read this Session will return errors.
   */
  expireTime:
    | Date
    | undefined;
  /** Avro schema. */
  avroSchema?:
    | AvroSchema
    | undefined;
  /** Arrow schema. */
  arrowSchema?:
    | ArrowSchema
    | undefined;
  /** Streams associated with this session. */
  streams: Stream[];
  /** Table that this ReadSession is reading from. */
  tableReference:
    | TableReference
    | undefined;
  /** Any modifiers which are applied when reading from the specified table. */
  tableModifiers:
    | TableModifiers
    | undefined;
  /** The strategy to use for distributing data among the streams. */
  shardingStrategy: ShardingStrategy;
}

/**
 * Creates a new read session, which may include additional options such as
 * requested parallelism, projection filters and constraints.
 */
export interface CreateReadSessionRequest {
  /** Required. Reference to the table to read. */
  tableReference:
    | TableReference
    | undefined;
  /**
   * Required. String of the form `projects/{project_id}` indicating the
   * project this ReadSession is associated with. This is the project that will
   * be billed for usage.
   */
  parent: string;
  /** Any modifiers to the Table (e.g. snapshot timestamp). */
  tableModifiers:
    | TableModifiers
    | undefined;
  /**
   * Initial number of streams. If unset or 0, we will
   * provide a value of streams so as to produce reasonable throughput. Must be
   * non-negative. The number of streams may be lower than the requested number,
   * depending on the amount parallelism that is reasonable for the table and
   * the maximum amount of parallelism allowed by the system.
   *
   * Streams must be read starting from offset 0.
   */
  requestedStreams: number;
  /** Read options for this session (e.g. column selection, filters). */
  readOptions:
    | TableReadOptions
    | undefined;
  /**
   * Data output format. Currently default to Avro.
   * DATA_FORMAT_UNSPECIFIED not supported.
   */
  format: DataFormat;
  /**
   * The strategy to use for distributing data among multiple streams. Currently
   * defaults to liquid sharding.
   */
  shardingStrategy: ShardingStrategy;
}

/** Requesting row data via `ReadRows` must provide Stream position information. */
export interface ReadRowsRequest {
  /**
   * Required. Identifier of the position in the stream to start reading from.
   * The offset requested must be less than the last row read from ReadRows.
   * Requesting a larger offset is undefined.
   */
  readPosition: StreamPosition | undefined;
}

/** Progress information for a given Stream. */
export interface StreamStatus {
  /**
   * Number of estimated rows in the current stream. May change over time as
   * different readers in the stream progress at rates which are relatively fast
   * or slow.
   */
  estimatedRowCount: Long;
  /**
   * A value in the range [0.0, 1.0] that represents the fraction of rows
   * assigned to this stream that have been processed by the server. In the
   * presence of read filters, the server may process more rows than it returns,
   * so this value reflects progress through the pre-filtering rows.
   *
   * This value is only populated for sessions created through the BALANCED
   * sharding strategy.
   */
  fractionConsumed: number;
  /** Represents the progress of the current stream. */
  progress:
    | Progress
    | undefined;
  /**
   * Whether this stream can be split. For sessions that use the LIQUID sharding
   * strategy, this value is always false. For BALANCED sessions, this value is
   * false when enough data have been read such that no more splits are possible
   * at that point or beyond. For small tables or streams that are the result of
   * a chain of splits, this value may never be true.
   */
  isSplittable: boolean;
}

export interface Progress {
  /**
   * The fraction of rows assigned to the stream that have been processed by the
   * server so far, not including the rows in the current response message.
   *
   * This value, along with `at_response_end`, can be used to interpolate the
   * progress made as the rows in the message are being processed using the
   * following formula: `at_response_start + (at_response_end -
   * at_response_start) * rows_processed_from_response / rows_in_response`.
   *
   * Note that if a filter is provided, the `at_response_end` value of the
   * previous response may not necessarily be equal to the `at_response_start`
   * value of the current response.
   */
  atResponseStart: number;
  /**
   * Similar to `at_response_start`, except that this value includes the rows in
   * the current response.
   */
  atResponseEnd: number;
}

/** Information on if the current connection is being throttled. */
export interface ThrottleStatus {
  /**
   * How much this connection is being throttled.
   * 0 is no throttling, 100 is completely throttled.
   */
  throttlePercent: number;
}

/**
 * Response from calling `ReadRows` may include row data, progress and
 * throttling information.
 */
export interface ReadRowsResponse {
  /** Serialized row data in AVRO format. */
  avroRows?:
    | AvroRows
    | undefined;
  /** Serialized row data in Arrow RecordBatch format. */
  arrowRecordBatch?:
    | ArrowRecordBatch
    | undefined;
  /**
   * Number of serialized rows in the rows block. This value is recorded here,
   * in addition to the row_count values in the output-specific messages in
   * `rows`, so that code which needs to record progress through the stream can
   * do so in an output format-independent way.
   */
  rowCount: Long;
  /** Estimated stream statistics. */
  status:
    | StreamStatus
    | undefined;
  /**
   * Throttling status. If unset, the latest response still describes
   * the current throttling status.
   */
  throttleStatus:
    | ThrottleStatus
    | undefined;
  /** Output only. Avro schema. */
  avroSchema?:
    | AvroSchema
    | undefined;
  /** Output only. Arrow schema. */
  arrowSchema?: ArrowSchema | undefined;
}

/**
 * Information needed to request additional streams for an established read
 * session.
 */
export interface BatchCreateReadSessionStreamsRequest {
  /**
   * Required. Must be a non-expired session obtained from a call to
   * CreateReadSession. Only the name field needs to be set.
   */
  session:
    | ReadSession
    | undefined;
  /**
   * Required. Number of new streams requested. Must be positive.
   * Number of added streams may be less than this, see CreateReadSessionRequest
   * for more information.
   */
  requestedStreams: number;
}

/**
 * The response from `BatchCreateReadSessionStreams` returns the stream
 * identifiers for the newly created streams.
 */
export interface BatchCreateReadSessionStreamsResponse {
  /** Newly added streams. */
  streams: Stream[];
}

/** Request information for invoking `FinalizeStream`. */
export interface FinalizeStreamRequest {
  /** Required. Stream to finalize. */
  stream: Stream | undefined;
}

/** Request information for `SplitReadStream`. */
export interface SplitReadStreamRequest {
  /** Required. Stream to split. */
  originalStream:
    | Stream
    | undefined;
  /**
   * A value in the range (0.0, 1.0) that specifies the fractional point at
   * which the original stream should be split. The actual split point is
   * evaluated on pre-filtered rows, so if a filter is provided, then there is
   * no guarantee that the division of the rows between the new child streams
   * will be proportional to this fractional value. Additionally, because the
   * server-side unit for assigning data is collections of rows, this fraction
   * will always map to to a data storage boundary on the server side.
   */
  fraction: number;
}

/** Response from `SplitReadStream`. */
export interface SplitReadStreamResponse {
  /**
   * Primary stream, which contains the beginning portion of
   * |original_stream|. An empty value indicates that the original stream can no
   * longer be split.
   */
  primaryStream:
    | Stream
    | undefined;
  /**
   * Remainder stream, which contains the tail of |original_stream|. An empty
   * value indicates that the original stream can no longer be split.
   */
  remainderStream: Stream | undefined;
}

function createBaseStream(): Stream {
  return { name: "" };
}

export const Stream: MessageFns<Stream> = {
  encode(message: Stream, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Stream {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStream();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Stream {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: Stream): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<Stream>): Stream {
    return Stream.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Stream>): Stream {
    const message = createBaseStream();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseStreamPosition(): StreamPosition {
  return { stream: undefined, offset: Long.ZERO };
}

export const StreamPosition: MessageFns<StreamPosition> = {
  encode(message: StreamPosition, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.stream !== undefined) {
      Stream.encode(message.stream, writer.uint32(10).fork()).join();
    }
    if (!message.offset.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.offset.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamPosition {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamPosition();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.stream = Stream.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.offset = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamPosition {
    return {
      stream: isSet(object.stream) ? Stream.fromJSON(object.stream) : undefined,
      offset: isSet(object.offset) ? Long.fromValue(object.offset) : Long.ZERO,
    };
  },

  toJSON(message: StreamPosition): unknown {
    const obj: any = {};
    if (message.stream !== undefined) {
      obj.stream = Stream.toJSON(message.stream);
    }
    if (!message.offset.equals(Long.ZERO)) {
      obj.offset = (message.offset || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<StreamPosition>): StreamPosition {
    return StreamPosition.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamPosition>): StreamPosition {
    const message = createBaseStreamPosition();
    message.stream = (object.stream !== undefined && object.stream !== null)
      ? Stream.fromPartial(object.stream)
      : undefined;
    message.offset = (object.offset !== undefined && object.offset !== null)
      ? Long.fromValue(object.offset)
      : Long.ZERO;
    return message;
  },
};

function createBaseReadSession(): ReadSession {
  return {
    name: "",
    expireTime: undefined,
    avroSchema: undefined,
    arrowSchema: undefined,
    streams: [],
    tableReference: undefined,
    tableModifiers: undefined,
    shardingStrategy: 0,
  };
}

export const ReadSession: MessageFns<ReadSession> = {
  encode(message: ReadSession, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.expireTime !== undefined) {
      Timestamp.encode(toTimestamp(message.expireTime), writer.uint32(18).fork()).join();
    }
    if (message.avroSchema !== undefined) {
      AvroSchema.encode(message.avroSchema, writer.uint32(42).fork()).join();
    }
    if (message.arrowSchema !== undefined) {
      ArrowSchema.encode(message.arrowSchema, writer.uint32(50).fork()).join();
    }
    for (const v of message.streams) {
      Stream.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.tableReference !== undefined) {
      TableReference.encode(message.tableReference, writer.uint32(58).fork()).join();
    }
    if (message.tableModifiers !== undefined) {
      TableModifiers.encode(message.tableModifiers, writer.uint32(66).fork()).join();
    }
    if (message.shardingStrategy !== 0) {
      writer.uint32(72).int32(message.shardingStrategy);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReadSession {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReadSession();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.expireTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.avroSchema = AvroSchema.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.arrowSchema = ArrowSchema.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.streams.push(Stream.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.tableReference = TableReference.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.tableModifiers = TableModifiers.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.shardingStrategy = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReadSession {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      expireTime: isSet(object.expireTime) ? fromJsonTimestamp(object.expireTime) : undefined,
      avroSchema: isSet(object.avroSchema) ? AvroSchema.fromJSON(object.avroSchema) : undefined,
      arrowSchema: isSet(object.arrowSchema) ? ArrowSchema.fromJSON(object.arrowSchema) : undefined,
      streams: globalThis.Array.isArray(object?.streams) ? object.streams.map((e: any) => Stream.fromJSON(e)) : [],
      tableReference: isSet(object.tableReference) ? TableReference.fromJSON(object.tableReference) : undefined,
      tableModifiers: isSet(object.tableModifiers) ? TableModifiers.fromJSON(object.tableModifiers) : undefined,
      shardingStrategy: isSet(object.shardingStrategy) ? shardingStrategyFromJSON(object.shardingStrategy) : 0,
    };
  },

  toJSON(message: ReadSession): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.expireTime !== undefined) {
      obj.expireTime = message.expireTime.toISOString();
    }
    if (message.avroSchema !== undefined) {
      obj.avroSchema = AvroSchema.toJSON(message.avroSchema);
    }
    if (message.arrowSchema !== undefined) {
      obj.arrowSchema = ArrowSchema.toJSON(message.arrowSchema);
    }
    if (message.streams?.length) {
      obj.streams = message.streams.map((e) => Stream.toJSON(e));
    }
    if (message.tableReference !== undefined) {
      obj.tableReference = TableReference.toJSON(message.tableReference);
    }
    if (message.tableModifiers !== undefined) {
      obj.tableModifiers = TableModifiers.toJSON(message.tableModifiers);
    }
    if (message.shardingStrategy !== 0) {
      obj.shardingStrategy = shardingStrategyToJSON(message.shardingStrategy);
    }
    return obj;
  },

  create(base?: DeepPartial<ReadSession>): ReadSession {
    return ReadSession.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReadSession>): ReadSession {
    const message = createBaseReadSession();
    message.name = object.name ?? "";
    message.expireTime = object.expireTime ?? undefined;
    message.avroSchema = (object.avroSchema !== undefined && object.avroSchema !== null)
      ? AvroSchema.fromPartial(object.avroSchema)
      : undefined;
    message.arrowSchema = (object.arrowSchema !== undefined && object.arrowSchema !== null)
      ? ArrowSchema.fromPartial(object.arrowSchema)
      : undefined;
    message.streams = object.streams?.map((e) => Stream.fromPartial(e)) || [];
    message.tableReference = (object.tableReference !== undefined && object.tableReference !== null)
      ? TableReference.fromPartial(object.tableReference)
      : undefined;
    message.tableModifiers = (object.tableModifiers !== undefined && object.tableModifiers !== null)
      ? TableModifiers.fromPartial(object.tableModifiers)
      : undefined;
    message.shardingStrategy = object.shardingStrategy ?? 0;
    return message;
  },
};

function createBaseCreateReadSessionRequest(): CreateReadSessionRequest {
  return {
    tableReference: undefined,
    parent: "",
    tableModifiers: undefined,
    requestedStreams: 0,
    readOptions: undefined,
    format: 0,
    shardingStrategy: 0,
  };
}

export const CreateReadSessionRequest: MessageFns<CreateReadSessionRequest> = {
  encode(message: CreateReadSessionRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.tableReference !== undefined) {
      TableReference.encode(message.tableReference, writer.uint32(10).fork()).join();
    }
    if (message.parent !== "") {
      writer.uint32(50).string(message.parent);
    }
    if (message.tableModifiers !== undefined) {
      TableModifiers.encode(message.tableModifiers, writer.uint32(18).fork()).join();
    }
    if (message.requestedStreams !== 0) {
      writer.uint32(24).int32(message.requestedStreams);
    }
    if (message.readOptions !== undefined) {
      TableReadOptions.encode(message.readOptions, writer.uint32(34).fork()).join();
    }
    if (message.format !== 0) {
      writer.uint32(40).int32(message.format);
    }
    if (message.shardingStrategy !== 0) {
      writer.uint32(56).int32(message.shardingStrategy);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateReadSessionRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateReadSessionRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.tableReference = TableReference.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.tableModifiers = TableModifiers.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.requestedStreams = reader.int32();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.readOptions = TableReadOptions.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.format = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.shardingStrategy = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateReadSessionRequest {
    return {
      tableReference: isSet(object.tableReference) ? TableReference.fromJSON(object.tableReference) : undefined,
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      tableModifiers: isSet(object.tableModifiers) ? TableModifiers.fromJSON(object.tableModifiers) : undefined,
      requestedStreams: isSet(object.requestedStreams) ? globalThis.Number(object.requestedStreams) : 0,
      readOptions: isSet(object.readOptions) ? TableReadOptions.fromJSON(object.readOptions) : undefined,
      format: isSet(object.format) ? dataFormatFromJSON(object.format) : 0,
      shardingStrategy: isSet(object.shardingStrategy) ? shardingStrategyFromJSON(object.shardingStrategy) : 0,
    };
  },

  toJSON(message: CreateReadSessionRequest): unknown {
    const obj: any = {};
    if (message.tableReference !== undefined) {
      obj.tableReference = TableReference.toJSON(message.tableReference);
    }
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.tableModifiers !== undefined) {
      obj.tableModifiers = TableModifiers.toJSON(message.tableModifiers);
    }
    if (message.requestedStreams !== 0) {
      obj.requestedStreams = Math.round(message.requestedStreams);
    }
    if (message.readOptions !== undefined) {
      obj.readOptions = TableReadOptions.toJSON(message.readOptions);
    }
    if (message.format !== 0) {
      obj.format = dataFormatToJSON(message.format);
    }
    if (message.shardingStrategy !== 0) {
      obj.shardingStrategy = shardingStrategyToJSON(message.shardingStrategy);
    }
    return obj;
  },

  create(base?: DeepPartial<CreateReadSessionRequest>): CreateReadSessionRequest {
    return CreateReadSessionRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateReadSessionRequest>): CreateReadSessionRequest {
    const message = createBaseCreateReadSessionRequest();
    message.tableReference = (object.tableReference !== undefined && object.tableReference !== null)
      ? TableReference.fromPartial(object.tableReference)
      : undefined;
    message.parent = object.parent ?? "";
    message.tableModifiers = (object.tableModifiers !== undefined && object.tableModifiers !== null)
      ? TableModifiers.fromPartial(object.tableModifiers)
      : undefined;
    message.requestedStreams = object.requestedStreams ?? 0;
    message.readOptions = (object.readOptions !== undefined && object.readOptions !== null)
      ? TableReadOptions.fromPartial(object.readOptions)
      : undefined;
    message.format = object.format ?? 0;
    message.shardingStrategy = object.shardingStrategy ?? 0;
    return message;
  },
};

function createBaseReadRowsRequest(): ReadRowsRequest {
  return { readPosition: undefined };
}

export const ReadRowsRequest: MessageFns<ReadRowsRequest> = {
  encode(message: ReadRowsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.readPosition !== undefined) {
      StreamPosition.encode(message.readPosition, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReadRowsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReadRowsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.readPosition = StreamPosition.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReadRowsRequest {
    return { readPosition: isSet(object.readPosition) ? StreamPosition.fromJSON(object.readPosition) : undefined };
  },

  toJSON(message: ReadRowsRequest): unknown {
    const obj: any = {};
    if (message.readPosition !== undefined) {
      obj.readPosition = StreamPosition.toJSON(message.readPosition);
    }
    return obj;
  },

  create(base?: DeepPartial<ReadRowsRequest>): ReadRowsRequest {
    return ReadRowsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReadRowsRequest>): ReadRowsRequest {
    const message = createBaseReadRowsRequest();
    message.readPosition = (object.readPosition !== undefined && object.readPosition !== null)
      ? StreamPosition.fromPartial(object.readPosition)
      : undefined;
    return message;
  },
};

function createBaseStreamStatus(): StreamStatus {
  return { estimatedRowCount: Long.ZERO, fractionConsumed: 0, progress: undefined, isSplittable: false };
}

export const StreamStatus: MessageFns<StreamStatus> = {
  encode(message: StreamStatus, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.estimatedRowCount.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.estimatedRowCount.toString());
    }
    if (message.fractionConsumed !== 0) {
      writer.uint32(21).float(message.fractionConsumed);
    }
    if (message.progress !== undefined) {
      Progress.encode(message.progress, writer.uint32(34).fork()).join();
    }
    if (message.isSplittable !== false) {
      writer.uint32(24).bool(message.isSplittable);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamStatus {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamStatus();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.estimatedRowCount = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.fractionConsumed = reader.float();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.progress = Progress.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.isSplittable = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamStatus {
    return {
      estimatedRowCount: isSet(object.estimatedRowCount) ? Long.fromValue(object.estimatedRowCount) : Long.ZERO,
      fractionConsumed: isSet(object.fractionConsumed) ? globalThis.Number(object.fractionConsumed) : 0,
      progress: isSet(object.progress) ? Progress.fromJSON(object.progress) : undefined,
      isSplittable: isSet(object.isSplittable) ? globalThis.Boolean(object.isSplittable) : false,
    };
  },

  toJSON(message: StreamStatus): unknown {
    const obj: any = {};
    if (!message.estimatedRowCount.equals(Long.ZERO)) {
      obj.estimatedRowCount = (message.estimatedRowCount || Long.ZERO).toString();
    }
    if (message.fractionConsumed !== 0) {
      obj.fractionConsumed = message.fractionConsumed;
    }
    if (message.progress !== undefined) {
      obj.progress = Progress.toJSON(message.progress);
    }
    if (message.isSplittable !== false) {
      obj.isSplittable = message.isSplittable;
    }
    return obj;
  },

  create(base?: DeepPartial<StreamStatus>): StreamStatus {
    return StreamStatus.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamStatus>): StreamStatus {
    const message = createBaseStreamStatus();
    message.estimatedRowCount = (object.estimatedRowCount !== undefined && object.estimatedRowCount !== null)
      ? Long.fromValue(object.estimatedRowCount)
      : Long.ZERO;
    message.fractionConsumed = object.fractionConsumed ?? 0;
    message.progress = (object.progress !== undefined && object.progress !== null)
      ? Progress.fromPartial(object.progress)
      : undefined;
    message.isSplittable = object.isSplittable ?? false;
    return message;
  },
};

function createBaseProgress(): Progress {
  return { atResponseStart: 0, atResponseEnd: 0 };
}

export const Progress: MessageFns<Progress> = {
  encode(message: Progress, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.atResponseStart !== 0) {
      writer.uint32(13).float(message.atResponseStart);
    }
    if (message.atResponseEnd !== 0) {
      writer.uint32(21).float(message.atResponseEnd);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Progress {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProgress();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 13) {
            break;
          }

          message.atResponseStart = reader.float();
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.atResponseEnd = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Progress {
    return {
      atResponseStart: isSet(object.atResponseStart) ? globalThis.Number(object.atResponseStart) : 0,
      atResponseEnd: isSet(object.atResponseEnd) ? globalThis.Number(object.atResponseEnd) : 0,
    };
  },

  toJSON(message: Progress): unknown {
    const obj: any = {};
    if (message.atResponseStart !== 0) {
      obj.atResponseStart = message.atResponseStart;
    }
    if (message.atResponseEnd !== 0) {
      obj.atResponseEnd = message.atResponseEnd;
    }
    return obj;
  },

  create(base?: DeepPartial<Progress>): Progress {
    return Progress.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Progress>): Progress {
    const message = createBaseProgress();
    message.atResponseStart = object.atResponseStart ?? 0;
    message.atResponseEnd = object.atResponseEnd ?? 0;
    return message;
  },
};

function createBaseThrottleStatus(): ThrottleStatus {
  return { throttlePercent: 0 };
}

export const ThrottleStatus: MessageFns<ThrottleStatus> = {
  encode(message: ThrottleStatus, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.throttlePercent !== 0) {
      writer.uint32(8).int32(message.throttlePercent);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ThrottleStatus {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseThrottleStatus();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.throttlePercent = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ThrottleStatus {
    return { throttlePercent: isSet(object.throttlePercent) ? globalThis.Number(object.throttlePercent) : 0 };
  },

  toJSON(message: ThrottleStatus): unknown {
    const obj: any = {};
    if (message.throttlePercent !== 0) {
      obj.throttlePercent = Math.round(message.throttlePercent);
    }
    return obj;
  },

  create(base?: DeepPartial<ThrottleStatus>): ThrottleStatus {
    return ThrottleStatus.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ThrottleStatus>): ThrottleStatus {
    const message = createBaseThrottleStatus();
    message.throttlePercent = object.throttlePercent ?? 0;
    return message;
  },
};

function createBaseReadRowsResponse(): ReadRowsResponse {
  return {
    avroRows: undefined,
    arrowRecordBatch: undefined,
    rowCount: Long.ZERO,
    status: undefined,
    throttleStatus: undefined,
    avroSchema: undefined,
    arrowSchema: undefined,
  };
}

export const ReadRowsResponse: MessageFns<ReadRowsResponse> = {
  encode(message: ReadRowsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.avroRows !== undefined) {
      AvroRows.encode(message.avroRows, writer.uint32(26).fork()).join();
    }
    if (message.arrowRecordBatch !== undefined) {
      ArrowRecordBatch.encode(message.arrowRecordBatch, writer.uint32(34).fork()).join();
    }
    if (!message.rowCount.equals(Long.ZERO)) {
      writer.uint32(48).int64(message.rowCount.toString());
    }
    if (message.status !== undefined) {
      StreamStatus.encode(message.status, writer.uint32(18).fork()).join();
    }
    if (message.throttleStatus !== undefined) {
      ThrottleStatus.encode(message.throttleStatus, writer.uint32(42).fork()).join();
    }
    if (message.avroSchema !== undefined) {
      AvroSchema.encode(message.avroSchema, writer.uint32(58).fork()).join();
    }
    if (message.arrowSchema !== undefined) {
      ArrowSchema.encode(message.arrowSchema, writer.uint32(66).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReadRowsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReadRowsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 26) {
            break;
          }

          message.avroRows = AvroRows.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.arrowRecordBatch = ArrowRecordBatch.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.rowCount = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.status = StreamStatus.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.throttleStatus = ThrottleStatus.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.avroSchema = AvroSchema.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.arrowSchema = ArrowSchema.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReadRowsResponse {
    return {
      avroRows: isSet(object.avroRows) ? AvroRows.fromJSON(object.avroRows) : undefined,
      arrowRecordBatch: isSet(object.arrowRecordBatch) ? ArrowRecordBatch.fromJSON(object.arrowRecordBatch) : undefined,
      rowCount: isSet(object.rowCount) ? Long.fromValue(object.rowCount) : Long.ZERO,
      status: isSet(object.status) ? StreamStatus.fromJSON(object.status) : undefined,
      throttleStatus: isSet(object.throttleStatus) ? ThrottleStatus.fromJSON(object.throttleStatus) : undefined,
      avroSchema: isSet(object.avroSchema) ? AvroSchema.fromJSON(object.avroSchema) : undefined,
      arrowSchema: isSet(object.arrowSchema) ? ArrowSchema.fromJSON(object.arrowSchema) : undefined,
    };
  },

  toJSON(message: ReadRowsResponse): unknown {
    const obj: any = {};
    if (message.avroRows !== undefined) {
      obj.avroRows = AvroRows.toJSON(message.avroRows);
    }
    if (message.arrowRecordBatch !== undefined) {
      obj.arrowRecordBatch = ArrowRecordBatch.toJSON(message.arrowRecordBatch);
    }
    if (!message.rowCount.equals(Long.ZERO)) {
      obj.rowCount = (message.rowCount || Long.ZERO).toString();
    }
    if (message.status !== undefined) {
      obj.status = StreamStatus.toJSON(message.status);
    }
    if (message.throttleStatus !== undefined) {
      obj.throttleStatus = ThrottleStatus.toJSON(message.throttleStatus);
    }
    if (message.avroSchema !== undefined) {
      obj.avroSchema = AvroSchema.toJSON(message.avroSchema);
    }
    if (message.arrowSchema !== undefined) {
      obj.arrowSchema = ArrowSchema.toJSON(message.arrowSchema);
    }
    return obj;
  },

  create(base?: DeepPartial<ReadRowsResponse>): ReadRowsResponse {
    return ReadRowsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReadRowsResponse>): ReadRowsResponse {
    const message = createBaseReadRowsResponse();
    message.avroRows = (object.avroRows !== undefined && object.avroRows !== null)
      ? AvroRows.fromPartial(object.avroRows)
      : undefined;
    message.arrowRecordBatch = (object.arrowRecordBatch !== undefined && object.arrowRecordBatch !== null)
      ? ArrowRecordBatch.fromPartial(object.arrowRecordBatch)
      : undefined;
    message.rowCount = (object.rowCount !== undefined && object.rowCount !== null)
      ? Long.fromValue(object.rowCount)
      : Long.ZERO;
    message.status = (object.status !== undefined && object.status !== null)
      ? StreamStatus.fromPartial(object.status)
      : undefined;
    message.throttleStatus = (object.throttleStatus !== undefined && object.throttleStatus !== null)
      ? ThrottleStatus.fromPartial(object.throttleStatus)
      : undefined;
    message.avroSchema = (object.avroSchema !== undefined && object.avroSchema !== null)
      ? AvroSchema.fromPartial(object.avroSchema)
      : undefined;
    message.arrowSchema = (object.arrowSchema !== undefined && object.arrowSchema !== null)
      ? ArrowSchema.fromPartial(object.arrowSchema)
      : undefined;
    return message;
  },
};

function createBaseBatchCreateReadSessionStreamsRequest(): BatchCreateReadSessionStreamsRequest {
  return { session: undefined, requestedStreams: 0 };
}

export const BatchCreateReadSessionStreamsRequest: MessageFns<BatchCreateReadSessionStreamsRequest> = {
  encode(message: BatchCreateReadSessionStreamsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.session !== undefined) {
      ReadSession.encode(message.session, writer.uint32(10).fork()).join();
    }
    if (message.requestedStreams !== 0) {
      writer.uint32(16).int32(message.requestedStreams);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchCreateReadSessionStreamsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchCreateReadSessionStreamsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.session = ReadSession.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.requestedStreams = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchCreateReadSessionStreamsRequest {
    return {
      session: isSet(object.session) ? ReadSession.fromJSON(object.session) : undefined,
      requestedStreams: isSet(object.requestedStreams) ? globalThis.Number(object.requestedStreams) : 0,
    };
  },

  toJSON(message: BatchCreateReadSessionStreamsRequest): unknown {
    const obj: any = {};
    if (message.session !== undefined) {
      obj.session = ReadSession.toJSON(message.session);
    }
    if (message.requestedStreams !== 0) {
      obj.requestedStreams = Math.round(message.requestedStreams);
    }
    return obj;
  },

  create(base?: DeepPartial<BatchCreateReadSessionStreamsRequest>): BatchCreateReadSessionStreamsRequest {
    return BatchCreateReadSessionStreamsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchCreateReadSessionStreamsRequest>): BatchCreateReadSessionStreamsRequest {
    const message = createBaseBatchCreateReadSessionStreamsRequest();
    message.session = (object.session !== undefined && object.session !== null)
      ? ReadSession.fromPartial(object.session)
      : undefined;
    message.requestedStreams = object.requestedStreams ?? 0;
    return message;
  },
};

function createBaseBatchCreateReadSessionStreamsResponse(): BatchCreateReadSessionStreamsResponse {
  return { streams: [] };
}

export const BatchCreateReadSessionStreamsResponse: MessageFns<BatchCreateReadSessionStreamsResponse> = {
  encode(message: BatchCreateReadSessionStreamsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.streams) {
      Stream.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchCreateReadSessionStreamsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchCreateReadSessionStreamsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.streams.push(Stream.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchCreateReadSessionStreamsResponse {
    return {
      streams: globalThis.Array.isArray(object?.streams) ? object.streams.map((e: any) => Stream.fromJSON(e)) : [],
    };
  },

  toJSON(message: BatchCreateReadSessionStreamsResponse): unknown {
    const obj: any = {};
    if (message.streams?.length) {
      obj.streams = message.streams.map((e) => Stream.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchCreateReadSessionStreamsResponse>): BatchCreateReadSessionStreamsResponse {
    return BatchCreateReadSessionStreamsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchCreateReadSessionStreamsResponse>): BatchCreateReadSessionStreamsResponse {
    const message = createBaseBatchCreateReadSessionStreamsResponse();
    message.streams = object.streams?.map((e) => Stream.fromPartial(e)) || [];
    return message;
  },
};

function createBaseFinalizeStreamRequest(): FinalizeStreamRequest {
  return { stream: undefined };
}

export const FinalizeStreamRequest: MessageFns<FinalizeStreamRequest> = {
  encode(message: FinalizeStreamRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.stream !== undefined) {
      Stream.encode(message.stream, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FinalizeStreamRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFinalizeStreamRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.stream = Stream.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FinalizeStreamRequest {
    return { stream: isSet(object.stream) ? Stream.fromJSON(object.stream) : undefined };
  },

  toJSON(message: FinalizeStreamRequest): unknown {
    const obj: any = {};
    if (message.stream !== undefined) {
      obj.stream = Stream.toJSON(message.stream);
    }
    return obj;
  },

  create(base?: DeepPartial<FinalizeStreamRequest>): FinalizeStreamRequest {
    return FinalizeStreamRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FinalizeStreamRequest>): FinalizeStreamRequest {
    const message = createBaseFinalizeStreamRequest();
    message.stream = (object.stream !== undefined && object.stream !== null)
      ? Stream.fromPartial(object.stream)
      : undefined;
    return message;
  },
};

function createBaseSplitReadStreamRequest(): SplitReadStreamRequest {
  return { originalStream: undefined, fraction: 0 };
}

export const SplitReadStreamRequest: MessageFns<SplitReadStreamRequest> = {
  encode(message: SplitReadStreamRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.originalStream !== undefined) {
      Stream.encode(message.originalStream, writer.uint32(10).fork()).join();
    }
    if (message.fraction !== 0) {
      writer.uint32(21).float(message.fraction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SplitReadStreamRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSplitReadStreamRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.originalStream = Stream.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.fraction = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SplitReadStreamRequest {
    return {
      originalStream: isSet(object.originalStream) ? Stream.fromJSON(object.originalStream) : undefined,
      fraction: isSet(object.fraction) ? globalThis.Number(object.fraction) : 0,
    };
  },

  toJSON(message: SplitReadStreamRequest): unknown {
    const obj: any = {};
    if (message.originalStream !== undefined) {
      obj.originalStream = Stream.toJSON(message.originalStream);
    }
    if (message.fraction !== 0) {
      obj.fraction = message.fraction;
    }
    return obj;
  },

  create(base?: DeepPartial<SplitReadStreamRequest>): SplitReadStreamRequest {
    return SplitReadStreamRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SplitReadStreamRequest>): SplitReadStreamRequest {
    const message = createBaseSplitReadStreamRequest();
    message.originalStream = (object.originalStream !== undefined && object.originalStream !== null)
      ? Stream.fromPartial(object.originalStream)
      : undefined;
    message.fraction = object.fraction ?? 0;
    return message;
  },
};

function createBaseSplitReadStreamResponse(): SplitReadStreamResponse {
  return { primaryStream: undefined, remainderStream: undefined };
}

export const SplitReadStreamResponse: MessageFns<SplitReadStreamResponse> = {
  encode(message: SplitReadStreamResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.primaryStream !== undefined) {
      Stream.encode(message.primaryStream, writer.uint32(10).fork()).join();
    }
    if (message.remainderStream !== undefined) {
      Stream.encode(message.remainderStream, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SplitReadStreamResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSplitReadStreamResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.primaryStream = Stream.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.remainderStream = Stream.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SplitReadStreamResponse {
    return {
      primaryStream: isSet(object.primaryStream) ? Stream.fromJSON(object.primaryStream) : undefined,
      remainderStream: isSet(object.remainderStream) ? Stream.fromJSON(object.remainderStream) : undefined,
    };
  },

  toJSON(message: SplitReadStreamResponse): unknown {
    const obj: any = {};
    if (message.primaryStream !== undefined) {
      obj.primaryStream = Stream.toJSON(message.primaryStream);
    }
    if (message.remainderStream !== undefined) {
      obj.remainderStream = Stream.toJSON(message.remainderStream);
    }
    return obj;
  },

  create(base?: DeepPartial<SplitReadStreamResponse>): SplitReadStreamResponse {
    return SplitReadStreamResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SplitReadStreamResponse>): SplitReadStreamResponse {
    const message = createBaseSplitReadStreamResponse();
    message.primaryStream = (object.primaryStream !== undefined && object.primaryStream !== null)
      ? Stream.fromPartial(object.primaryStream)
      : undefined;
    message.remainderStream = (object.remainderStream !== undefined && object.remainderStream !== null)
      ? Stream.fromPartial(object.remainderStream)
      : undefined;
    return message;
  },
};

/**
 * BigQuery storage API.
 *
 * The BigQuery storage API can be used to read data stored in BigQuery.
 *
 * The v1beta1 API is not yet officially deprecated, and will go through a full
 * deprecation cycle (https://cloud.google.com/products#product-launch-stages)
 * before the service is turned down. However, new code should use the v1 API
 * going forward.
 */
export type BigQueryStorageDefinition = typeof BigQueryStorageDefinition;
export const BigQueryStorageDefinition = {
  name: "BigQueryStorage",
  fullName: "google.cloud.bigquery.storage.v1beta1.BigQueryStorage",
  methods: {
    /**
     * Creates a new read session. A read session divides the contents of a
     * BigQuery table into one or more streams, which can then be used to read
     * data from the table. The read session also specifies properties of the
     * data to be read, such as a list of columns or a push-down filter describing
     * the rows to be returned.
     *
     * A particular row can be read by at most one stream. When the caller has
     * reached the end of each stream in the session, then all the data in the
     * table has been read.
     *
     * Read sessions automatically expire 6 hours after they are created and do
     * not require manual clean-up by the caller.
     */
    createReadSession: {
      name: "CreateReadSession",
      requestType: CreateReadSessionRequest,
      requestStream: false,
      responseType: ReadSession,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              40,
              116,
              97,
              98,
              108,
              101,
              95,
              114,
              101,
              102,
              101,
              114,
              101,
              110,
              99,
              101,
              44,
              112,
              97,
              114,
              101,
              110,
              116,
              44,
              114,
              101,
              113,
              117,
              101,
              115,
              116,
              101,
              100,
              95,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
            ]),
          ],
          578365826: [
            Buffer.from([
              119,
              58,
              1,
              42,
              90,
              64,
              58,
              1,
              42,
              34,
              59,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              116,
              97,
              98,
              108,
              101,
              95,
              114,
              101,
              102,
              101,
              114,
              101,
              110,
              99,
              101,
              46,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              95,
              105,
              100,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              125,
              34,
              48,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              116,
              97,
              98,
              108,
              101,
              95,
              114,
              101,
              102,
              101,
              114,
              101,
              110,
              99,
              101,
              46,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Reads rows from the table in the format prescribed by the read session.
     * Each response contains one or more table rows, up to a maximum of 10 MiB
     * per response; read requests which attempt to read individual rows larger
     * than this will fail.
     *
     * Each request also returns a set of stream statistics reflecting the
     * estimated total number of rows in the read stream. This number is computed
     * based on the total table size and the number of active streams in the read
     * session, and may change as other streams continue to read data.
     */
    readRows: {
      name: "ReadRows",
      requestType: ReadRowsRequest,
      requestStream: false,
      responseType: ReadRowsResponse,
      responseStream: true,
      options: {
        _unknownFields: {
          8410: [Buffer.from([13, 114, 101, 97, 100, 95, 112, 111, 115, 105, 116, 105, 111, 110])],
          578365826: [
            Buffer.from([
              59,
              18,
              57,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              114,
              101,
              97,
              100,
              95,
              112,
              111,
              115,
              105,
              116,
              105,
              111,
              110,
              46,
              115,
              116,
              114,
              101,
              97,
              109,
              46,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Creates additional streams for a ReadSession. This API can be used to
     * dynamically adjust the parallelism of a batch processing task upwards by
     * adding additional workers.
     */
    batchCreateReadSessionStreams: {
      name: "BatchCreateReadSessionStreams",
      requestType: BatchCreateReadSessionStreamsRequest,
      requestStream: false,
      responseType: BatchCreateReadSessionStreamsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              25,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              44,
              114,
              101,
              113,
              117,
              101,
              115,
              116,
              101,
              100,
              95,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
            ]),
          ],
          578365826: [
            Buffer.from([
              50,
              58,
              1,
              42,
              34,
              45,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              46,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Causes a single stream in a ReadSession to gracefully stop. This
     * API can be used to dynamically adjust the parallelism of a batch processing
     * task downwards without losing data.
     *
     * This API does not delete the stream -- it remains visible in the
     * ReadSession, and any data processed by the stream is not released to other
     * streams. However, no additional data will be assigned to the stream once
     * this call completes. Callers must continue reading data on the stream until
     * the end of the stream is reached so that data which has already been
     * assigned to the stream will be processed.
     *
     * This method will return an error if there are no other live streams
     * in the Session, or if SplitReadStream() has been called on the given
     * Stream.
     */
    finalizeStream: {
      name: "FinalizeStream",
      requestType: FinalizeStreamRequest,
      requestStream: false,
      responseType: Empty,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([6, 115, 116, 114, 101, 97, 109])],
          578365826: [
            Buffer.from([
              48,
              58,
              1,
              42,
              34,
              43,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              115,
              116,
              114,
              101,
              97,
              109,
              46,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Splits a given read stream into two Streams. These streams are referred to
     * as the primary and the residual of the split. The original stream can still
     * be read from in the same manner as before. Both of the returned streams can
     * also be read from, and the total rows return by both child streams will be
     * the same as the rows read from the original stream.
     *
     * Moreover, the two child streams will be allocated back to back in the
     * original Stream. Concretely, it is guaranteed that for streams Original,
     * Primary, and Residual, that Original[0-j] = Primary[0-j] and
     * Original[j-n] = Residual[0-m] once the streams have been read to
     * completion.
     *
     * This method is guaranteed to be idempotent.
     */
    splitReadStream: {
      name: "SplitReadStream",
      requestType: SplitReadStreamRequest,
      requestStream: false,
      responseType: SplitReadStreamResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([15, 111, 114, 105, 103, 105, 110, 97, 108, 95, 115, 116, 114, 101, 97, 109])],
          578365826: [
            Buffer.from([
              54,
              18,
              52,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              111,
              114,
              105,
              103,
              105,
              110,
              97,
              108,
              95,
              115,
              116,
              114,
              101,
              97,
              109,
              46,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface BigQueryStorageServiceImplementation<CallContextExt = {}> {
  /**
   * Creates a new read session. A read session divides the contents of a
   * BigQuery table into one or more streams, which can then be used to read
   * data from the table. The read session also specifies properties of the
   * data to be read, such as a list of columns or a push-down filter describing
   * the rows to be returned.
   *
   * A particular row can be read by at most one stream. When the caller has
   * reached the end of each stream in the session, then all the data in the
   * table has been read.
   *
   * Read sessions automatically expire 6 hours after they are created and do
   * not require manual clean-up by the caller.
   */
  createReadSession(
    request: CreateReadSessionRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ReadSession>>;
  /**
   * Reads rows from the table in the format prescribed by the read session.
   * Each response contains one or more table rows, up to a maximum of 10 MiB
   * per response; read requests which attempt to read individual rows larger
   * than this will fail.
   *
   * Each request also returns a set of stream statistics reflecting the
   * estimated total number of rows in the read stream. This number is computed
   * based on the total table size and the number of active streams in the read
   * session, and may change as other streams continue to read data.
   */
  readRows(
    request: ReadRowsRequest,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<ReadRowsResponse>>;
  /**
   * Creates additional streams for a ReadSession. This API can be used to
   * dynamically adjust the parallelism of a batch processing task upwards by
   * adding additional workers.
   */
  batchCreateReadSessionStreams(
    request: BatchCreateReadSessionStreamsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<BatchCreateReadSessionStreamsResponse>>;
  /**
   * Causes a single stream in a ReadSession to gracefully stop. This
   * API can be used to dynamically adjust the parallelism of a batch processing
   * task downwards without losing data.
   *
   * This API does not delete the stream -- it remains visible in the
   * ReadSession, and any data processed by the stream is not released to other
   * streams. However, no additional data will be assigned to the stream once
   * this call completes. Callers must continue reading data on the stream until
   * the end of the stream is reached so that data which has already been
   * assigned to the stream will be processed.
   *
   * This method will return an error if there are no other live streams
   * in the Session, or if SplitReadStream() has been called on the given
   * Stream.
   */
  finalizeStream(request: FinalizeStreamRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Empty>>;
  /**
   * Splits a given read stream into two Streams. These streams are referred to
   * as the primary and the residual of the split. The original stream can still
   * be read from in the same manner as before. Both of the returned streams can
   * also be read from, and the total rows return by both child streams will be
   * the same as the rows read from the original stream.
   *
   * Moreover, the two child streams will be allocated back to back in the
   * original Stream. Concretely, it is guaranteed that for streams Original,
   * Primary, and Residual, that Original[0-j] = Primary[0-j] and
   * Original[j-n] = Residual[0-m] once the streams have been read to
   * completion.
   *
   * This method is guaranteed to be idempotent.
   */
  splitReadStream(
    request: SplitReadStreamRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<SplitReadStreamResponse>>;
}

export interface BigQueryStorageClient<CallOptionsExt = {}> {
  /**
   * Creates a new read session. A read session divides the contents of a
   * BigQuery table into one or more streams, which can then be used to read
   * data from the table. The read session also specifies properties of the
   * data to be read, such as a list of columns or a push-down filter describing
   * the rows to be returned.
   *
   * A particular row can be read by at most one stream. When the caller has
   * reached the end of each stream in the session, then all the data in the
   * table has been read.
   *
   * Read sessions automatically expire 6 hours after they are created and do
   * not require manual clean-up by the caller.
   */
  createReadSession(
    request: DeepPartial<CreateReadSessionRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ReadSession>;
  /**
   * Reads rows from the table in the format prescribed by the read session.
   * Each response contains one or more table rows, up to a maximum of 10 MiB
   * per response; read requests which attempt to read individual rows larger
   * than this will fail.
   *
   * Each request also returns a set of stream statistics reflecting the
   * estimated total number of rows in the read stream. This number is computed
   * based on the total table size and the number of active streams in the read
   * session, and may change as other streams continue to read data.
   */
  readRows(
    request: DeepPartial<ReadRowsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<ReadRowsResponse>;
  /**
   * Creates additional streams for a ReadSession. This API can be used to
   * dynamically adjust the parallelism of a batch processing task upwards by
   * adding additional workers.
   */
  batchCreateReadSessionStreams(
    request: DeepPartial<BatchCreateReadSessionStreamsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<BatchCreateReadSessionStreamsResponse>;
  /**
   * Causes a single stream in a ReadSession to gracefully stop. This
   * API can be used to dynamically adjust the parallelism of a batch processing
   * task downwards without losing data.
   *
   * This API does not delete the stream -- it remains visible in the
   * ReadSession, and any data processed by the stream is not released to other
   * streams. However, no additional data will be assigned to the stream once
   * this call completes. Callers must continue reading data on the stream until
   * the end of the stream is reached so that data which has already been
   * assigned to the stream will be processed.
   *
   * This method will return an error if there are no other live streams
   * in the Session, or if SplitReadStream() has been called on the given
   * Stream.
   */
  finalizeStream(request: DeepPartial<FinalizeStreamRequest>, options?: CallOptions & CallOptionsExt): Promise<Empty>;
  /**
   * Splits a given read stream into two Streams. These streams are referred to
   * as the primary and the residual of the split. The original stream can still
   * be read from in the same manner as before. Both of the returned streams can
   * also be read from, and the total rows return by both child streams will be
   * the same as the rows read from the original stream.
   *
   * Moreover, the two child streams will be allocated back to back in the
   * original Stream. Concretely, it is guaranteed that for streams Original,
   * Primary, and Residual, that Original[0-j] = Primary[0-j] and
   * Original[j-n] = Residual[0-m] once the streams have been read to
   * completion.
   *
   * This method is guaranteed to be idempotent.
   */
  splitReadStream(
    request: DeepPartial<SplitReadStreamRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<SplitReadStreamResponse>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export type ServerStreamingMethodResult<Response> = { [Symbol.asyncIterator](): AsyncIterator<Response, void> };

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
