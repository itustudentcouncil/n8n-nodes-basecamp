// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/bigquery/storage/v1beta2/storage.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Timestamp } from "../../../../protobuf/timestamp.js";
import { Int64Value } from "../../../../protobuf/wrappers.js";
import { Status } from "../../../../rpc/status.js";
import { ArrowRecordBatch, ArrowSchema } from "./arrow.js";
import { AvroRows, AvroSchema } from "./avro.js";
import { ProtoRows, ProtoSchema } from "./protobuf.js";
import { ReadSession, ReadStream, WriteStream } from "./stream.js";
import { TableSchema } from "./table.js";

export const protobufPackage = "google.cloud.bigquery.storage.v1beta2";

/** Request message for `CreateReadSession`. */
export interface CreateReadSessionRequest {
  /**
   * Required. The request project that owns the session, in the form of
   * `projects/{project_id}`.
   */
  parent: string;
  /** Required. Session to be created. */
  readSession:
    | ReadSession
    | undefined;
  /**
   * Max initial number of streams. If unset or zero, the server will
   * provide a value of streams so as to produce reasonable throughput. Must be
   * non-negative. The number of streams may be lower than the requested number,
   * depending on the amount parallelism that is reasonable for the table. Error
   * will be returned if the max count is greater than the current system
   * max limit of 1,000.
   *
   * Streams must be read starting from offset 0.
   */
  maxStreamCount: number;
}

/** Request message for `ReadRows`. */
export interface ReadRowsRequest {
  /** Required. Stream to read rows from. */
  readStream: string;
  /**
   * The offset requested must be less than the last row read from Read.
   * Requesting a larger offset is undefined. If not specified, start reading
   * from offset zero.
   */
  offset: Long;
}

/** Information on if the current connection is being throttled. */
export interface ThrottleState {
  /**
   * How much this connection is being throttled. Zero means no throttling,
   * 100 means fully throttled.
   */
  throttlePercent: number;
}

/** Estimated stream statistics for a given Stream. */
export interface StreamStats {
  /** Represents the progress of the current stream. */
  progress: StreamStats_Progress | undefined;
}

export interface StreamStats_Progress {
  /**
   * The fraction of rows assigned to the stream that have been processed by
   * the server so far, not including the rows in the current response
   * message.
   *
   * This value, along with `at_response_end`, can be used to interpolate
   * the progress made as the rows in the message are being processed using
   * the following formula: `at_response_start + (at_response_end -
   * at_response_start) * rows_processed_from_response / rows_in_response`.
   *
   * Note that if a filter is provided, the `at_response_end` value of the
   * previous response may not necessarily be equal to the
   * `at_response_start` value of the current response.
   */
  atResponseStart: number;
  /**
   * Similar to `at_response_start`, except that this value includes the
   * rows in the current response.
   */
  atResponseEnd: number;
}

/**
 * Response from calling `ReadRows` may include row data, progress and
 * throttling information.
 */
export interface ReadRowsResponse {
  /** Serialized row data in AVRO format. */
  avroRows?:
    | AvroRows
    | undefined;
  /** Serialized row data in Arrow RecordBatch format. */
  arrowRecordBatch?:
    | ArrowRecordBatch
    | undefined;
  /** Number of serialized rows in the rows block. */
  rowCount: Long;
  /** Statistics for the stream. */
  stats:
    | StreamStats
    | undefined;
  /**
   * Throttling state. If unset, the latest response still describes
   * the current throttling status.
   */
  throttleState:
    | ThrottleState
    | undefined;
  /** Output only. Avro schema. */
  avroSchema?:
    | AvroSchema
    | undefined;
  /** Output only. Arrow schema. */
  arrowSchema?: ArrowSchema | undefined;
}

/** Request message for `SplitReadStream`. */
export interface SplitReadStreamRequest {
  /** Required. Name of the stream to split. */
  name: string;
  /**
   * A value in the range (0.0, 1.0) that specifies the fractional point at
   * which the original stream should be split. The actual split point is
   * evaluated on pre-filtered rows, so if a filter is provided, then there is
   * no guarantee that the division of the rows between the new child streams
   * will be proportional to this fractional value. Additionally, because the
   * server-side unit for assigning data is collections of rows, this fraction
   * will always map to a data storage boundary on the server side.
   */
  fraction: number;
}

export interface SplitReadStreamResponse {
  /**
   * Primary stream, which contains the beginning portion of
   * |original_stream|. An empty value indicates that the original stream can no
   * longer be split.
   */
  primaryStream:
    | ReadStream
    | undefined;
  /**
   * Remainder stream, which contains the tail of |original_stream|. An empty
   * value indicates that the original stream can no longer be split.
   */
  remainderStream: ReadStream | undefined;
}

/** Request message for `CreateWriteStream`. */
export interface CreateWriteStreamRequest {
  /**
   * Required. Reference to the table to which the stream belongs, in the format
   * of `projects/{project}/datasets/{dataset}/tables/{table}`.
   */
  parent: string;
  /** Required. Stream to be created. */
  writeStream: WriteStream | undefined;
}

/** Request message for `AppendRows`. */
export interface AppendRowsRequest {
  /**
   * Required. The stream that is the target of the append operation. This value
   * must be specified for the initial request. If subsequent requests specify
   * the stream name, it must equal to the value provided in the first request.
   * To write to the _default stream, populate this field with a string in the
   * format `projects/{project}/datasets/{dataset}/tables/{table}/_default`.
   */
  writeStream: string;
  /**
   * If present, the write is only performed if the next append offset is same
   * as the provided value. If not present, the write is performed at the
   * current end of stream. Specifying a value for this field is not allowed
   * when calling AppendRows for the '_default' stream.
   */
  offset:
    | Long
    | undefined;
  /** Rows in proto format. */
  protoRows?:
    | AppendRowsRequest_ProtoData
    | undefined;
  /**
   * Id set by client to annotate its identity. Only initial request setting is
   * respected.
   */
  traceId: string;
}

/** Proto schema and data. */
export interface AppendRowsRequest_ProtoData {
  /** Proto schema used to serialize the data. */
  writerSchema:
    | ProtoSchema
    | undefined;
  /** Serialized row data in protobuf message format. */
  rows: ProtoRows | undefined;
}

/** Response message for `AppendRows`. */
export interface AppendRowsResponse {
  /** Result if the append is successful. */
  appendResult?:
    | AppendRowsResponse_AppendResult
    | undefined;
  /**
   * Error returned when problems were encountered.  If present,
   * it indicates rows were not accepted into the system.
   * Users can retry or continue with other append requests within the
   * same connection.
   *
   * Additional information about error signalling:
   *
   * ALREADY_EXISTS: Happens when an append specified an offset, and the
   * backend already has received data at this offset.  Typically encountered
   * in retry scenarios, and can be ignored.
   *
   * OUT_OF_RANGE: Returned when the specified offset in the stream is beyond
   * the current end of the stream.
   *
   * INVALID_ARGUMENT: Indicates a malformed request or data.
   *
   * ABORTED: Request processing is aborted because of prior failures.  The
   * request can be retried if previous failure is addressed.
   *
   * INTERNAL: Indicates server side error(s) that can be retried.
   */
  error?:
    | Status
    | undefined;
  /**
   * If backend detects a schema update, pass it to user so that user can
   * use it to input new type of message. It will be empty when no schema
   * updates have occurred.
   */
  updatedSchema: TableSchema | undefined;
}

/** AppendResult is returned for successful append requests. */
export interface AppendRowsResponse_AppendResult {
  /**
   * The row offset at which the last append occurred. The offset will not be
   * set if appending using default streams.
   */
  offset: Long | undefined;
}

/** Request message for `GetWriteStreamRequest`. */
export interface GetWriteStreamRequest {
  /**
   * Required. Name of the stream to get, in the form of
   * `projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}`.
   */
  name: string;
}

/** Request message for `BatchCommitWriteStreams`. */
export interface BatchCommitWriteStreamsRequest {
  /**
   * Required. Parent table that all the streams should belong to, in the form
   * of `projects/{project}/datasets/{dataset}/tables/{table}`.
   */
  parent: string;
  /** Required. The group of streams that will be committed atomically. */
  writeStreams: string[];
}

/** Response message for `BatchCommitWriteStreams`. */
export interface BatchCommitWriteStreamsResponse {
  /**
   * The time at which streams were committed in microseconds granularity.
   * This field will only exist when there are no stream errors.
   * **Note** if this field is not set, it means the commit was not successful.
   */
  commitTime:
    | Date
    | undefined;
  /**
   * Stream level error if commit failed. Only streams with error will be in
   * the list.
   * If empty, there is no error and all streams are committed successfully.
   * If non empty, certain streams have errors and ZERO stream is committed due
   * to atomicity guarantee.
   */
  streamErrors: StorageError[];
}

/** Request message for invoking `FinalizeWriteStream`. */
export interface FinalizeWriteStreamRequest {
  /**
   * Required. Name of the stream to finalize, in the form of
   * `projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}`.
   */
  name: string;
}

/** Response message for `FinalizeWriteStream`. */
export interface FinalizeWriteStreamResponse {
  /** Number of rows in the finalized stream. */
  rowCount: Long;
}

/** Request message for `FlushRows`. */
export interface FlushRowsRequest {
  /** Required. The stream that is the target of the flush operation. */
  writeStream: string;
  /**
   * Ending offset of the flush operation. Rows before this offset(including
   * this offset) will be flushed.
   */
  offset: Long | undefined;
}

/** Respond message for `FlushRows`. */
export interface FlushRowsResponse {
  /** The rows before this offset (including this offset) are flushed. */
  offset: Long;
}

/**
 * Structured custom BigQuery Storage error message. The error can be attached
 * as error details in the returned rpc Status. In particular, the use of error
 * codes allows more structured error handling, and reduces the need to evaluate
 * unstructured error text strings.
 */
export interface StorageError {
  /** BigQuery Storage specific error code. */
  code: StorageError_StorageErrorCode;
  /** Name of the failed entity. */
  entity: string;
  /** Message that describes the error. */
  errorMessage: string;
}

/** Error code for `StorageError`. */
export enum StorageError_StorageErrorCode {
  /** STORAGE_ERROR_CODE_UNSPECIFIED - Default error. */
  STORAGE_ERROR_CODE_UNSPECIFIED = 0,
  /** TABLE_NOT_FOUND - Table is not found in the system. */
  TABLE_NOT_FOUND = 1,
  /** STREAM_ALREADY_COMMITTED - Stream is already committed. */
  STREAM_ALREADY_COMMITTED = 2,
  /** STREAM_NOT_FOUND - Stream is not found. */
  STREAM_NOT_FOUND = 3,
  /**
   * INVALID_STREAM_TYPE - Invalid Stream type.
   * For example, you try to commit a stream that is not pending.
   */
  INVALID_STREAM_TYPE = 4,
  /**
   * INVALID_STREAM_STATE - Invalid Stream state.
   * For example, you try to commit a stream that is not finalized or is
   * garbaged.
   */
  INVALID_STREAM_STATE = 5,
  /** STREAM_FINALIZED - Stream is finalized. */
  STREAM_FINALIZED = 6,
  UNRECOGNIZED = -1,
}

export function storageError_StorageErrorCodeFromJSON(object: any): StorageError_StorageErrorCode {
  switch (object) {
    case 0:
    case "STORAGE_ERROR_CODE_UNSPECIFIED":
      return StorageError_StorageErrorCode.STORAGE_ERROR_CODE_UNSPECIFIED;
    case 1:
    case "TABLE_NOT_FOUND":
      return StorageError_StorageErrorCode.TABLE_NOT_FOUND;
    case 2:
    case "STREAM_ALREADY_COMMITTED":
      return StorageError_StorageErrorCode.STREAM_ALREADY_COMMITTED;
    case 3:
    case "STREAM_NOT_FOUND":
      return StorageError_StorageErrorCode.STREAM_NOT_FOUND;
    case 4:
    case "INVALID_STREAM_TYPE":
      return StorageError_StorageErrorCode.INVALID_STREAM_TYPE;
    case 5:
    case "INVALID_STREAM_STATE":
      return StorageError_StorageErrorCode.INVALID_STREAM_STATE;
    case 6:
    case "STREAM_FINALIZED":
      return StorageError_StorageErrorCode.STREAM_FINALIZED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return StorageError_StorageErrorCode.UNRECOGNIZED;
  }
}

export function storageError_StorageErrorCodeToJSON(object: StorageError_StorageErrorCode): string {
  switch (object) {
    case StorageError_StorageErrorCode.STORAGE_ERROR_CODE_UNSPECIFIED:
      return "STORAGE_ERROR_CODE_UNSPECIFIED";
    case StorageError_StorageErrorCode.TABLE_NOT_FOUND:
      return "TABLE_NOT_FOUND";
    case StorageError_StorageErrorCode.STREAM_ALREADY_COMMITTED:
      return "STREAM_ALREADY_COMMITTED";
    case StorageError_StorageErrorCode.STREAM_NOT_FOUND:
      return "STREAM_NOT_FOUND";
    case StorageError_StorageErrorCode.INVALID_STREAM_TYPE:
      return "INVALID_STREAM_TYPE";
    case StorageError_StorageErrorCode.INVALID_STREAM_STATE:
      return "INVALID_STREAM_STATE";
    case StorageError_StorageErrorCode.STREAM_FINALIZED:
      return "STREAM_FINALIZED";
    case StorageError_StorageErrorCode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseCreateReadSessionRequest(): CreateReadSessionRequest {
  return { parent: "", readSession: undefined, maxStreamCount: 0 };
}

export const CreateReadSessionRequest: MessageFns<CreateReadSessionRequest> = {
  encode(message: CreateReadSessionRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.readSession !== undefined) {
      ReadSession.encode(message.readSession, writer.uint32(18).fork()).join();
    }
    if (message.maxStreamCount !== 0) {
      writer.uint32(24).int32(message.maxStreamCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateReadSessionRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateReadSessionRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.readSession = ReadSession.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.maxStreamCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateReadSessionRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      readSession: isSet(object.readSession) ? ReadSession.fromJSON(object.readSession) : undefined,
      maxStreamCount: isSet(object.maxStreamCount) ? globalThis.Number(object.maxStreamCount) : 0,
    };
  },

  toJSON(message: CreateReadSessionRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.readSession !== undefined) {
      obj.readSession = ReadSession.toJSON(message.readSession);
    }
    if (message.maxStreamCount !== 0) {
      obj.maxStreamCount = Math.round(message.maxStreamCount);
    }
    return obj;
  },

  create(base?: DeepPartial<CreateReadSessionRequest>): CreateReadSessionRequest {
    return CreateReadSessionRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateReadSessionRequest>): CreateReadSessionRequest {
    const message = createBaseCreateReadSessionRequest();
    message.parent = object.parent ?? "";
    message.readSession = (object.readSession !== undefined && object.readSession !== null)
      ? ReadSession.fromPartial(object.readSession)
      : undefined;
    message.maxStreamCount = object.maxStreamCount ?? 0;
    return message;
  },
};

function createBaseReadRowsRequest(): ReadRowsRequest {
  return { readStream: "", offset: Long.ZERO };
}

export const ReadRowsRequest: MessageFns<ReadRowsRequest> = {
  encode(message: ReadRowsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.readStream !== "") {
      writer.uint32(10).string(message.readStream);
    }
    if (!message.offset.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.offset.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReadRowsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReadRowsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.readStream = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.offset = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReadRowsRequest {
    return {
      readStream: isSet(object.readStream) ? globalThis.String(object.readStream) : "",
      offset: isSet(object.offset) ? Long.fromValue(object.offset) : Long.ZERO,
    };
  },

  toJSON(message: ReadRowsRequest): unknown {
    const obj: any = {};
    if (message.readStream !== "") {
      obj.readStream = message.readStream;
    }
    if (!message.offset.equals(Long.ZERO)) {
      obj.offset = (message.offset || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ReadRowsRequest>): ReadRowsRequest {
    return ReadRowsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReadRowsRequest>): ReadRowsRequest {
    const message = createBaseReadRowsRequest();
    message.readStream = object.readStream ?? "";
    message.offset = (object.offset !== undefined && object.offset !== null)
      ? Long.fromValue(object.offset)
      : Long.ZERO;
    return message;
  },
};

function createBaseThrottleState(): ThrottleState {
  return { throttlePercent: 0 };
}

export const ThrottleState: MessageFns<ThrottleState> = {
  encode(message: ThrottleState, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.throttlePercent !== 0) {
      writer.uint32(8).int32(message.throttlePercent);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ThrottleState {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseThrottleState();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.throttlePercent = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ThrottleState {
    return { throttlePercent: isSet(object.throttlePercent) ? globalThis.Number(object.throttlePercent) : 0 };
  },

  toJSON(message: ThrottleState): unknown {
    const obj: any = {};
    if (message.throttlePercent !== 0) {
      obj.throttlePercent = Math.round(message.throttlePercent);
    }
    return obj;
  },

  create(base?: DeepPartial<ThrottleState>): ThrottleState {
    return ThrottleState.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ThrottleState>): ThrottleState {
    const message = createBaseThrottleState();
    message.throttlePercent = object.throttlePercent ?? 0;
    return message;
  },
};

function createBaseStreamStats(): StreamStats {
  return { progress: undefined };
}

export const StreamStats: MessageFns<StreamStats> = {
  encode(message: StreamStats, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.progress !== undefined) {
      StreamStats_Progress.encode(message.progress, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamStats {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamStats();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.progress = StreamStats_Progress.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamStats {
    return { progress: isSet(object.progress) ? StreamStats_Progress.fromJSON(object.progress) : undefined };
  },

  toJSON(message: StreamStats): unknown {
    const obj: any = {};
    if (message.progress !== undefined) {
      obj.progress = StreamStats_Progress.toJSON(message.progress);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamStats>): StreamStats {
    return StreamStats.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamStats>): StreamStats {
    const message = createBaseStreamStats();
    message.progress = (object.progress !== undefined && object.progress !== null)
      ? StreamStats_Progress.fromPartial(object.progress)
      : undefined;
    return message;
  },
};

function createBaseStreamStats_Progress(): StreamStats_Progress {
  return { atResponseStart: 0, atResponseEnd: 0 };
}

export const StreamStats_Progress: MessageFns<StreamStats_Progress> = {
  encode(message: StreamStats_Progress, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.atResponseStart !== 0) {
      writer.uint32(9).double(message.atResponseStart);
    }
    if (message.atResponseEnd !== 0) {
      writer.uint32(17).double(message.atResponseEnd);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamStats_Progress {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamStats_Progress();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 9) {
            break;
          }

          message.atResponseStart = reader.double();
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.atResponseEnd = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamStats_Progress {
    return {
      atResponseStart: isSet(object.atResponseStart) ? globalThis.Number(object.atResponseStart) : 0,
      atResponseEnd: isSet(object.atResponseEnd) ? globalThis.Number(object.atResponseEnd) : 0,
    };
  },

  toJSON(message: StreamStats_Progress): unknown {
    const obj: any = {};
    if (message.atResponseStart !== 0) {
      obj.atResponseStart = message.atResponseStart;
    }
    if (message.atResponseEnd !== 0) {
      obj.atResponseEnd = message.atResponseEnd;
    }
    return obj;
  },

  create(base?: DeepPartial<StreamStats_Progress>): StreamStats_Progress {
    return StreamStats_Progress.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamStats_Progress>): StreamStats_Progress {
    const message = createBaseStreamStats_Progress();
    message.atResponseStart = object.atResponseStart ?? 0;
    message.atResponseEnd = object.atResponseEnd ?? 0;
    return message;
  },
};

function createBaseReadRowsResponse(): ReadRowsResponse {
  return {
    avroRows: undefined,
    arrowRecordBatch: undefined,
    rowCount: Long.ZERO,
    stats: undefined,
    throttleState: undefined,
    avroSchema: undefined,
    arrowSchema: undefined,
  };
}

export const ReadRowsResponse: MessageFns<ReadRowsResponse> = {
  encode(message: ReadRowsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.avroRows !== undefined) {
      AvroRows.encode(message.avroRows, writer.uint32(26).fork()).join();
    }
    if (message.arrowRecordBatch !== undefined) {
      ArrowRecordBatch.encode(message.arrowRecordBatch, writer.uint32(34).fork()).join();
    }
    if (!message.rowCount.equals(Long.ZERO)) {
      writer.uint32(48).int64(message.rowCount.toString());
    }
    if (message.stats !== undefined) {
      StreamStats.encode(message.stats, writer.uint32(18).fork()).join();
    }
    if (message.throttleState !== undefined) {
      ThrottleState.encode(message.throttleState, writer.uint32(42).fork()).join();
    }
    if (message.avroSchema !== undefined) {
      AvroSchema.encode(message.avroSchema, writer.uint32(58).fork()).join();
    }
    if (message.arrowSchema !== undefined) {
      ArrowSchema.encode(message.arrowSchema, writer.uint32(66).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReadRowsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReadRowsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 26) {
            break;
          }

          message.avroRows = AvroRows.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.arrowRecordBatch = ArrowRecordBatch.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.rowCount = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.stats = StreamStats.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.throttleState = ThrottleState.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.avroSchema = AvroSchema.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.arrowSchema = ArrowSchema.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReadRowsResponse {
    return {
      avroRows: isSet(object.avroRows) ? AvroRows.fromJSON(object.avroRows) : undefined,
      arrowRecordBatch: isSet(object.arrowRecordBatch) ? ArrowRecordBatch.fromJSON(object.arrowRecordBatch) : undefined,
      rowCount: isSet(object.rowCount) ? Long.fromValue(object.rowCount) : Long.ZERO,
      stats: isSet(object.stats) ? StreamStats.fromJSON(object.stats) : undefined,
      throttleState: isSet(object.throttleState) ? ThrottleState.fromJSON(object.throttleState) : undefined,
      avroSchema: isSet(object.avroSchema) ? AvroSchema.fromJSON(object.avroSchema) : undefined,
      arrowSchema: isSet(object.arrowSchema) ? ArrowSchema.fromJSON(object.arrowSchema) : undefined,
    };
  },

  toJSON(message: ReadRowsResponse): unknown {
    const obj: any = {};
    if (message.avroRows !== undefined) {
      obj.avroRows = AvroRows.toJSON(message.avroRows);
    }
    if (message.arrowRecordBatch !== undefined) {
      obj.arrowRecordBatch = ArrowRecordBatch.toJSON(message.arrowRecordBatch);
    }
    if (!message.rowCount.equals(Long.ZERO)) {
      obj.rowCount = (message.rowCount || Long.ZERO).toString();
    }
    if (message.stats !== undefined) {
      obj.stats = StreamStats.toJSON(message.stats);
    }
    if (message.throttleState !== undefined) {
      obj.throttleState = ThrottleState.toJSON(message.throttleState);
    }
    if (message.avroSchema !== undefined) {
      obj.avroSchema = AvroSchema.toJSON(message.avroSchema);
    }
    if (message.arrowSchema !== undefined) {
      obj.arrowSchema = ArrowSchema.toJSON(message.arrowSchema);
    }
    return obj;
  },

  create(base?: DeepPartial<ReadRowsResponse>): ReadRowsResponse {
    return ReadRowsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReadRowsResponse>): ReadRowsResponse {
    const message = createBaseReadRowsResponse();
    message.avroRows = (object.avroRows !== undefined && object.avroRows !== null)
      ? AvroRows.fromPartial(object.avroRows)
      : undefined;
    message.arrowRecordBatch = (object.arrowRecordBatch !== undefined && object.arrowRecordBatch !== null)
      ? ArrowRecordBatch.fromPartial(object.arrowRecordBatch)
      : undefined;
    message.rowCount = (object.rowCount !== undefined && object.rowCount !== null)
      ? Long.fromValue(object.rowCount)
      : Long.ZERO;
    message.stats = (object.stats !== undefined && object.stats !== null)
      ? StreamStats.fromPartial(object.stats)
      : undefined;
    message.throttleState = (object.throttleState !== undefined && object.throttleState !== null)
      ? ThrottleState.fromPartial(object.throttleState)
      : undefined;
    message.avroSchema = (object.avroSchema !== undefined && object.avroSchema !== null)
      ? AvroSchema.fromPartial(object.avroSchema)
      : undefined;
    message.arrowSchema = (object.arrowSchema !== undefined && object.arrowSchema !== null)
      ? ArrowSchema.fromPartial(object.arrowSchema)
      : undefined;
    return message;
  },
};

function createBaseSplitReadStreamRequest(): SplitReadStreamRequest {
  return { name: "", fraction: 0 };
}

export const SplitReadStreamRequest: MessageFns<SplitReadStreamRequest> = {
  encode(message: SplitReadStreamRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.fraction !== 0) {
      writer.uint32(17).double(message.fraction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SplitReadStreamRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSplitReadStreamRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.fraction = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SplitReadStreamRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      fraction: isSet(object.fraction) ? globalThis.Number(object.fraction) : 0,
    };
  },

  toJSON(message: SplitReadStreamRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.fraction !== 0) {
      obj.fraction = message.fraction;
    }
    return obj;
  },

  create(base?: DeepPartial<SplitReadStreamRequest>): SplitReadStreamRequest {
    return SplitReadStreamRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SplitReadStreamRequest>): SplitReadStreamRequest {
    const message = createBaseSplitReadStreamRequest();
    message.name = object.name ?? "";
    message.fraction = object.fraction ?? 0;
    return message;
  },
};

function createBaseSplitReadStreamResponse(): SplitReadStreamResponse {
  return { primaryStream: undefined, remainderStream: undefined };
}

export const SplitReadStreamResponse: MessageFns<SplitReadStreamResponse> = {
  encode(message: SplitReadStreamResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.primaryStream !== undefined) {
      ReadStream.encode(message.primaryStream, writer.uint32(10).fork()).join();
    }
    if (message.remainderStream !== undefined) {
      ReadStream.encode(message.remainderStream, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SplitReadStreamResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSplitReadStreamResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.primaryStream = ReadStream.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.remainderStream = ReadStream.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SplitReadStreamResponse {
    return {
      primaryStream: isSet(object.primaryStream) ? ReadStream.fromJSON(object.primaryStream) : undefined,
      remainderStream: isSet(object.remainderStream) ? ReadStream.fromJSON(object.remainderStream) : undefined,
    };
  },

  toJSON(message: SplitReadStreamResponse): unknown {
    const obj: any = {};
    if (message.primaryStream !== undefined) {
      obj.primaryStream = ReadStream.toJSON(message.primaryStream);
    }
    if (message.remainderStream !== undefined) {
      obj.remainderStream = ReadStream.toJSON(message.remainderStream);
    }
    return obj;
  },

  create(base?: DeepPartial<SplitReadStreamResponse>): SplitReadStreamResponse {
    return SplitReadStreamResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SplitReadStreamResponse>): SplitReadStreamResponse {
    const message = createBaseSplitReadStreamResponse();
    message.primaryStream = (object.primaryStream !== undefined && object.primaryStream !== null)
      ? ReadStream.fromPartial(object.primaryStream)
      : undefined;
    message.remainderStream = (object.remainderStream !== undefined && object.remainderStream !== null)
      ? ReadStream.fromPartial(object.remainderStream)
      : undefined;
    return message;
  },
};

function createBaseCreateWriteStreamRequest(): CreateWriteStreamRequest {
  return { parent: "", writeStream: undefined };
}

export const CreateWriteStreamRequest: MessageFns<CreateWriteStreamRequest> = {
  encode(message: CreateWriteStreamRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.writeStream !== undefined) {
      WriteStream.encode(message.writeStream, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateWriteStreamRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateWriteStreamRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.writeStream = WriteStream.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateWriteStreamRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      writeStream: isSet(object.writeStream) ? WriteStream.fromJSON(object.writeStream) : undefined,
    };
  },

  toJSON(message: CreateWriteStreamRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.writeStream !== undefined) {
      obj.writeStream = WriteStream.toJSON(message.writeStream);
    }
    return obj;
  },

  create(base?: DeepPartial<CreateWriteStreamRequest>): CreateWriteStreamRequest {
    return CreateWriteStreamRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateWriteStreamRequest>): CreateWriteStreamRequest {
    const message = createBaseCreateWriteStreamRequest();
    message.parent = object.parent ?? "";
    message.writeStream = (object.writeStream !== undefined && object.writeStream !== null)
      ? WriteStream.fromPartial(object.writeStream)
      : undefined;
    return message;
  },
};

function createBaseAppendRowsRequest(): AppendRowsRequest {
  return { writeStream: "", offset: undefined, protoRows: undefined, traceId: "" };
}

export const AppendRowsRequest: MessageFns<AppendRowsRequest> = {
  encode(message: AppendRowsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.writeStream !== "") {
      writer.uint32(10).string(message.writeStream);
    }
    if (message.offset !== undefined) {
      Int64Value.encode({ value: message.offset! }, writer.uint32(18).fork()).join();
    }
    if (message.protoRows !== undefined) {
      AppendRowsRequest_ProtoData.encode(message.protoRows, writer.uint32(34).fork()).join();
    }
    if (message.traceId !== "") {
      writer.uint32(50).string(message.traceId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppendRowsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppendRowsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.writeStream = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.offset = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.protoRows = AppendRowsRequest_ProtoData.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.traceId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AppendRowsRequest {
    return {
      writeStream: isSet(object.writeStream) ? globalThis.String(object.writeStream) : "",
      offset: isSet(object.offset) ? Long.fromValue(object.offset) : undefined,
      protoRows: isSet(object.protoRows) ? AppendRowsRequest_ProtoData.fromJSON(object.protoRows) : undefined,
      traceId: isSet(object.traceId) ? globalThis.String(object.traceId) : "",
    };
  },

  toJSON(message: AppendRowsRequest): unknown {
    const obj: any = {};
    if (message.writeStream !== "") {
      obj.writeStream = message.writeStream;
    }
    if (message.offset !== undefined) {
      obj.offset = message.offset;
    }
    if (message.protoRows !== undefined) {
      obj.protoRows = AppendRowsRequest_ProtoData.toJSON(message.protoRows);
    }
    if (message.traceId !== "") {
      obj.traceId = message.traceId;
    }
    return obj;
  },

  create(base?: DeepPartial<AppendRowsRequest>): AppendRowsRequest {
    return AppendRowsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AppendRowsRequest>): AppendRowsRequest {
    const message = createBaseAppendRowsRequest();
    message.writeStream = object.writeStream ?? "";
    message.offset = (object.offset !== undefined && object.offset !== null)
      ? Long.fromValue(object.offset)
      : undefined;
    message.protoRows = (object.protoRows !== undefined && object.protoRows !== null)
      ? AppendRowsRequest_ProtoData.fromPartial(object.protoRows)
      : undefined;
    message.traceId = object.traceId ?? "";
    return message;
  },
};

function createBaseAppendRowsRequest_ProtoData(): AppendRowsRequest_ProtoData {
  return { writerSchema: undefined, rows: undefined };
}

export const AppendRowsRequest_ProtoData: MessageFns<AppendRowsRequest_ProtoData> = {
  encode(message: AppendRowsRequest_ProtoData, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.writerSchema !== undefined) {
      ProtoSchema.encode(message.writerSchema, writer.uint32(10).fork()).join();
    }
    if (message.rows !== undefined) {
      ProtoRows.encode(message.rows, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppendRowsRequest_ProtoData {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppendRowsRequest_ProtoData();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.writerSchema = ProtoSchema.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.rows = ProtoRows.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AppendRowsRequest_ProtoData {
    return {
      writerSchema: isSet(object.writerSchema) ? ProtoSchema.fromJSON(object.writerSchema) : undefined,
      rows: isSet(object.rows) ? ProtoRows.fromJSON(object.rows) : undefined,
    };
  },

  toJSON(message: AppendRowsRequest_ProtoData): unknown {
    const obj: any = {};
    if (message.writerSchema !== undefined) {
      obj.writerSchema = ProtoSchema.toJSON(message.writerSchema);
    }
    if (message.rows !== undefined) {
      obj.rows = ProtoRows.toJSON(message.rows);
    }
    return obj;
  },

  create(base?: DeepPartial<AppendRowsRequest_ProtoData>): AppendRowsRequest_ProtoData {
    return AppendRowsRequest_ProtoData.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AppendRowsRequest_ProtoData>): AppendRowsRequest_ProtoData {
    const message = createBaseAppendRowsRequest_ProtoData();
    message.writerSchema = (object.writerSchema !== undefined && object.writerSchema !== null)
      ? ProtoSchema.fromPartial(object.writerSchema)
      : undefined;
    message.rows = (object.rows !== undefined && object.rows !== null) ? ProtoRows.fromPartial(object.rows) : undefined;
    return message;
  },
};

function createBaseAppendRowsResponse(): AppendRowsResponse {
  return { appendResult: undefined, error: undefined, updatedSchema: undefined };
}

export const AppendRowsResponse: MessageFns<AppendRowsResponse> = {
  encode(message: AppendRowsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.appendResult !== undefined) {
      AppendRowsResponse_AppendResult.encode(message.appendResult, writer.uint32(10).fork()).join();
    }
    if (message.error !== undefined) {
      Status.encode(message.error, writer.uint32(18).fork()).join();
    }
    if (message.updatedSchema !== undefined) {
      TableSchema.encode(message.updatedSchema, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppendRowsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppendRowsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.appendResult = AppendRowsResponse_AppendResult.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.error = Status.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.updatedSchema = TableSchema.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AppendRowsResponse {
    return {
      appendResult: isSet(object.appendResult)
        ? AppendRowsResponse_AppendResult.fromJSON(object.appendResult)
        : undefined,
      error: isSet(object.error) ? Status.fromJSON(object.error) : undefined,
      updatedSchema: isSet(object.updatedSchema) ? TableSchema.fromJSON(object.updatedSchema) : undefined,
    };
  },

  toJSON(message: AppendRowsResponse): unknown {
    const obj: any = {};
    if (message.appendResult !== undefined) {
      obj.appendResult = AppendRowsResponse_AppendResult.toJSON(message.appendResult);
    }
    if (message.error !== undefined) {
      obj.error = Status.toJSON(message.error);
    }
    if (message.updatedSchema !== undefined) {
      obj.updatedSchema = TableSchema.toJSON(message.updatedSchema);
    }
    return obj;
  },

  create(base?: DeepPartial<AppendRowsResponse>): AppendRowsResponse {
    return AppendRowsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AppendRowsResponse>): AppendRowsResponse {
    const message = createBaseAppendRowsResponse();
    message.appendResult = (object.appendResult !== undefined && object.appendResult !== null)
      ? AppendRowsResponse_AppendResult.fromPartial(object.appendResult)
      : undefined;
    message.error = (object.error !== undefined && object.error !== null)
      ? Status.fromPartial(object.error)
      : undefined;
    message.updatedSchema = (object.updatedSchema !== undefined && object.updatedSchema !== null)
      ? TableSchema.fromPartial(object.updatedSchema)
      : undefined;
    return message;
  },
};

function createBaseAppendRowsResponse_AppendResult(): AppendRowsResponse_AppendResult {
  return { offset: undefined };
}

export const AppendRowsResponse_AppendResult: MessageFns<AppendRowsResponse_AppendResult> = {
  encode(message: AppendRowsResponse_AppendResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.offset !== undefined) {
      Int64Value.encode({ value: message.offset! }, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AppendRowsResponse_AppendResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAppendRowsResponse_AppendResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.offset = Int64Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AppendRowsResponse_AppendResult {
    return { offset: isSet(object.offset) ? Long.fromValue(object.offset) : undefined };
  },

  toJSON(message: AppendRowsResponse_AppendResult): unknown {
    const obj: any = {};
    if (message.offset !== undefined) {
      obj.offset = message.offset;
    }
    return obj;
  },

  create(base?: DeepPartial<AppendRowsResponse_AppendResult>): AppendRowsResponse_AppendResult {
    return AppendRowsResponse_AppendResult.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AppendRowsResponse_AppendResult>): AppendRowsResponse_AppendResult {
    const message = createBaseAppendRowsResponse_AppendResult();
    message.offset = (object.offset !== undefined && object.offset !== null)
      ? Long.fromValue(object.offset)
      : undefined;
    return message;
  },
};

function createBaseGetWriteStreamRequest(): GetWriteStreamRequest {
  return { name: "" };
}

export const GetWriteStreamRequest: MessageFns<GetWriteStreamRequest> = {
  encode(message: GetWriteStreamRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetWriteStreamRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetWriteStreamRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetWriteStreamRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: GetWriteStreamRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<GetWriteStreamRequest>): GetWriteStreamRequest {
    return GetWriteStreamRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetWriteStreamRequest>): GetWriteStreamRequest {
    const message = createBaseGetWriteStreamRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseBatchCommitWriteStreamsRequest(): BatchCommitWriteStreamsRequest {
  return { parent: "", writeStreams: [] };
}

export const BatchCommitWriteStreamsRequest: MessageFns<BatchCommitWriteStreamsRequest> = {
  encode(message: BatchCommitWriteStreamsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    for (const v of message.writeStreams) {
      writer.uint32(18).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchCommitWriteStreamsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchCommitWriteStreamsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.writeStreams.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchCommitWriteStreamsRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      writeStreams: globalThis.Array.isArray(object?.writeStreams)
        ? object.writeStreams.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: BatchCommitWriteStreamsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.writeStreams?.length) {
      obj.writeStreams = message.writeStreams;
    }
    return obj;
  },

  create(base?: DeepPartial<BatchCommitWriteStreamsRequest>): BatchCommitWriteStreamsRequest {
    return BatchCommitWriteStreamsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchCommitWriteStreamsRequest>): BatchCommitWriteStreamsRequest {
    const message = createBaseBatchCommitWriteStreamsRequest();
    message.parent = object.parent ?? "";
    message.writeStreams = object.writeStreams?.map((e) => e) || [];
    return message;
  },
};

function createBaseBatchCommitWriteStreamsResponse(): BatchCommitWriteStreamsResponse {
  return { commitTime: undefined, streamErrors: [] };
}

export const BatchCommitWriteStreamsResponse: MessageFns<BatchCommitWriteStreamsResponse> = {
  encode(message: BatchCommitWriteStreamsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.commitTime !== undefined) {
      Timestamp.encode(toTimestamp(message.commitTime), writer.uint32(10).fork()).join();
    }
    for (const v of message.streamErrors) {
      StorageError.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchCommitWriteStreamsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchCommitWriteStreamsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.commitTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.streamErrors.push(StorageError.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchCommitWriteStreamsResponse {
    return {
      commitTime: isSet(object.commitTime) ? fromJsonTimestamp(object.commitTime) : undefined,
      streamErrors: globalThis.Array.isArray(object?.streamErrors)
        ? object.streamErrors.map((e: any) => StorageError.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchCommitWriteStreamsResponse): unknown {
    const obj: any = {};
    if (message.commitTime !== undefined) {
      obj.commitTime = message.commitTime.toISOString();
    }
    if (message.streamErrors?.length) {
      obj.streamErrors = message.streamErrors.map((e) => StorageError.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchCommitWriteStreamsResponse>): BatchCommitWriteStreamsResponse {
    return BatchCommitWriteStreamsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchCommitWriteStreamsResponse>): BatchCommitWriteStreamsResponse {
    const message = createBaseBatchCommitWriteStreamsResponse();
    message.commitTime = object.commitTime ?? undefined;
    message.streamErrors = object.streamErrors?.map((e) => StorageError.fromPartial(e)) || [];
    return message;
  },
};

function createBaseFinalizeWriteStreamRequest(): FinalizeWriteStreamRequest {
  return { name: "" };
}

export const FinalizeWriteStreamRequest: MessageFns<FinalizeWriteStreamRequest> = {
  encode(message: FinalizeWriteStreamRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FinalizeWriteStreamRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFinalizeWriteStreamRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FinalizeWriteStreamRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: FinalizeWriteStreamRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<FinalizeWriteStreamRequest>): FinalizeWriteStreamRequest {
    return FinalizeWriteStreamRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FinalizeWriteStreamRequest>): FinalizeWriteStreamRequest {
    const message = createBaseFinalizeWriteStreamRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseFinalizeWriteStreamResponse(): FinalizeWriteStreamResponse {
  return { rowCount: Long.ZERO };
}

export const FinalizeWriteStreamResponse: MessageFns<FinalizeWriteStreamResponse> = {
  encode(message: FinalizeWriteStreamResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.rowCount.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.rowCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FinalizeWriteStreamResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFinalizeWriteStreamResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.rowCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FinalizeWriteStreamResponse {
    return { rowCount: isSet(object.rowCount) ? Long.fromValue(object.rowCount) : Long.ZERO };
  },

  toJSON(message: FinalizeWriteStreamResponse): unknown {
    const obj: any = {};
    if (!message.rowCount.equals(Long.ZERO)) {
      obj.rowCount = (message.rowCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<FinalizeWriteStreamResponse>): FinalizeWriteStreamResponse {
    return FinalizeWriteStreamResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FinalizeWriteStreamResponse>): FinalizeWriteStreamResponse {
    const message = createBaseFinalizeWriteStreamResponse();
    message.rowCount = (object.rowCount !== undefined && object.rowCount !== null)
      ? Long.fromValue(object.rowCount)
      : Long.ZERO;
    return message;
  },
};

function createBaseFlushRowsRequest(): FlushRowsRequest {
  return { writeStream: "", offset: undefined };
}

export const FlushRowsRequest: MessageFns<FlushRowsRequest> = {
  encode(message: FlushRowsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.writeStream !== "") {
      writer.uint32(10).string(message.writeStream);
    }
    if (message.offset !== undefined) {
      Int64Value.encode({ value: message.offset! }, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FlushRowsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFlushRowsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.writeStream = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.offset = Int64Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FlushRowsRequest {
    return {
      writeStream: isSet(object.writeStream) ? globalThis.String(object.writeStream) : "",
      offset: isSet(object.offset) ? Long.fromValue(object.offset) : undefined,
    };
  },

  toJSON(message: FlushRowsRequest): unknown {
    const obj: any = {};
    if (message.writeStream !== "") {
      obj.writeStream = message.writeStream;
    }
    if (message.offset !== undefined) {
      obj.offset = message.offset;
    }
    return obj;
  },

  create(base?: DeepPartial<FlushRowsRequest>): FlushRowsRequest {
    return FlushRowsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FlushRowsRequest>): FlushRowsRequest {
    const message = createBaseFlushRowsRequest();
    message.writeStream = object.writeStream ?? "";
    message.offset = (object.offset !== undefined && object.offset !== null)
      ? Long.fromValue(object.offset)
      : undefined;
    return message;
  },
};

function createBaseFlushRowsResponse(): FlushRowsResponse {
  return { offset: Long.ZERO };
}

export const FlushRowsResponse: MessageFns<FlushRowsResponse> = {
  encode(message: FlushRowsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.offset.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.offset.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FlushRowsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFlushRowsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.offset = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FlushRowsResponse {
    return { offset: isSet(object.offset) ? Long.fromValue(object.offset) : Long.ZERO };
  },

  toJSON(message: FlushRowsResponse): unknown {
    const obj: any = {};
    if (!message.offset.equals(Long.ZERO)) {
      obj.offset = (message.offset || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<FlushRowsResponse>): FlushRowsResponse {
    return FlushRowsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FlushRowsResponse>): FlushRowsResponse {
    const message = createBaseFlushRowsResponse();
    message.offset = (object.offset !== undefined && object.offset !== null)
      ? Long.fromValue(object.offset)
      : Long.ZERO;
    return message;
  },
};

function createBaseStorageError(): StorageError {
  return { code: 0, entity: "", errorMessage: "" };
}

export const StorageError: MessageFns<StorageError> = {
  encode(message: StorageError, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.code !== 0) {
      writer.uint32(8).int32(message.code);
    }
    if (message.entity !== "") {
      writer.uint32(18).string(message.entity);
    }
    if (message.errorMessage !== "") {
      writer.uint32(26).string(message.errorMessage);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StorageError {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStorageError();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.code = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.entity = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.errorMessage = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StorageError {
    return {
      code: isSet(object.code) ? storageError_StorageErrorCodeFromJSON(object.code) : 0,
      entity: isSet(object.entity) ? globalThis.String(object.entity) : "",
      errorMessage: isSet(object.errorMessage) ? globalThis.String(object.errorMessage) : "",
    };
  },

  toJSON(message: StorageError): unknown {
    const obj: any = {};
    if (message.code !== 0) {
      obj.code = storageError_StorageErrorCodeToJSON(message.code);
    }
    if (message.entity !== "") {
      obj.entity = message.entity;
    }
    if (message.errorMessage !== "") {
      obj.errorMessage = message.errorMessage;
    }
    return obj;
  },

  create(base?: DeepPartial<StorageError>): StorageError {
    return StorageError.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StorageError>): StorageError {
    const message = createBaseStorageError();
    message.code = object.code ?? 0;
    message.entity = object.entity ?? "";
    message.errorMessage = object.errorMessage ?? "";
    return message;
  },
};

/**
 * BigQuery Read API.
 *
 * The Read API can be used to read data from BigQuery.
 *
 * New code should use the v1 Read API going forward, if they don't use Write
 * API at the same time.
 */
export type BigQueryReadDefinition = typeof BigQueryReadDefinition;
export const BigQueryReadDefinition = {
  name: "BigQueryRead",
  fullName: "google.cloud.bigquery.storage.v1beta2.BigQueryRead",
  methods: {
    /**
     * Creates a new read session. A read session divides the contents of a
     * BigQuery table into one or more streams, which can then be used to read
     * data from the table. The read session also specifies properties of the
     * data to be read, such as a list of columns or a push-down filter describing
     * the rows to be returned.
     *
     * A particular row can be read by at most one stream. When the caller has
     * reached the end of each stream in the session, then all the data in the
     * table has been read.
     *
     * Data is assigned to each stream such that roughly the same number of
     * rows can be read from each stream. Because the server-side unit for
     * assigning data is collections of rows, the API does not guarantee that
     * each stream will return the same number or rows. Additionally, the
     * limits are enforced based on the number of pre-filtered rows, so some
     * filters can lead to lopsided assignments.
     *
     * Read sessions automatically expire 6 hours after they are created and do
     * not require manual clean-up by the caller.
     */
    createReadSession: {
      name: "CreateReadSession",
      requestType: CreateReadSessionRequest,
      requestStream: false,
      responseType: ReadSession,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              36,
              112,
              97,
              114,
              101,
              110,
              116,
              44,
              114,
              101,
              97,
              100,
              95,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              44,
              109,
              97,
              120,
              95,
              115,
              116,
              114,
              101,
              97,
              109,
              95,
              99,
              111,
              117,
              110,
              116,
            ]),
          ],
          578365826: [
            Buffer.from([
              65,
              58,
              1,
              42,
              34,
              60,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              50,
              47,
              123,
              114,
              101,
              97,
              100,
              95,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              46,
              116,
              97,
              98,
              108,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Reads rows from the stream in the format prescribed by the ReadSession.
     * Each response contains one or more table rows, up to a maximum of 100 MiB
     * per response; read requests which attempt to read individual rows larger
     * than 100 MiB will fail.
     *
     * Each request also returns a set of stream statistics reflecting the current
     * state of the stream.
     */
    readRows: {
      name: "ReadRows",
      requestType: ReadRowsRequest,
      requestStream: false,
      responseType: ReadRowsResponse,
      responseStream: true,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([18, 114, 101, 97, 100, 95, 115, 116, 114, 101, 97, 109, 44, 111, 102, 102, 115, 101, 116]),
          ],
          578365826: [
            Buffer.from([
              68,
              18,
              66,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              50,
              47,
              123,
              114,
              101,
              97,
              100,
              95,
              115,
              116,
              114,
              101,
              97,
              109,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Splits a given `ReadStream` into two `ReadStream` objects. These
     * `ReadStream` objects are referred to as the primary and the residual
     * streams of the split. The original `ReadStream` can still be read from in
     * the same manner as before. Both of the returned `ReadStream` objects can
     * also be read from, and the rows returned by both child streams will be
     * the same as the rows read from the original stream.
     *
     * Moreover, the two child streams will be allocated back-to-back in the
     * original `ReadStream`. Concretely, it is guaranteed that for streams
     * original, primary, and residual, that original[0-j] = primary[0-j] and
     * original[j-n] = residual[0-m] once the streams have been read to
     * completion.
     */
    splitReadStream: {
      name: "SplitReadStream",
      requestType: SplitReadStreamRequest,
      requestStream: false,
      responseType: SplitReadStreamResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              61,
              18,
              59,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              50,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface BigQueryReadServiceImplementation<CallContextExt = {}> {
  /**
   * Creates a new read session. A read session divides the contents of a
   * BigQuery table into one or more streams, which can then be used to read
   * data from the table. The read session also specifies properties of the
   * data to be read, such as a list of columns or a push-down filter describing
   * the rows to be returned.
   *
   * A particular row can be read by at most one stream. When the caller has
   * reached the end of each stream in the session, then all the data in the
   * table has been read.
   *
   * Data is assigned to each stream such that roughly the same number of
   * rows can be read from each stream. Because the server-side unit for
   * assigning data is collections of rows, the API does not guarantee that
   * each stream will return the same number or rows. Additionally, the
   * limits are enforced based on the number of pre-filtered rows, so some
   * filters can lead to lopsided assignments.
   *
   * Read sessions automatically expire 6 hours after they are created and do
   * not require manual clean-up by the caller.
   */
  createReadSession(
    request: CreateReadSessionRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ReadSession>>;
  /**
   * Reads rows from the stream in the format prescribed by the ReadSession.
   * Each response contains one or more table rows, up to a maximum of 100 MiB
   * per response; read requests which attempt to read individual rows larger
   * than 100 MiB will fail.
   *
   * Each request also returns a set of stream statistics reflecting the current
   * state of the stream.
   */
  readRows(
    request: ReadRowsRequest,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<ReadRowsResponse>>;
  /**
   * Splits a given `ReadStream` into two `ReadStream` objects. These
   * `ReadStream` objects are referred to as the primary and the residual
   * streams of the split. The original `ReadStream` can still be read from in
   * the same manner as before. Both of the returned `ReadStream` objects can
   * also be read from, and the rows returned by both child streams will be
   * the same as the rows read from the original stream.
   *
   * Moreover, the two child streams will be allocated back-to-back in the
   * original `ReadStream`. Concretely, it is guaranteed that for streams
   * original, primary, and residual, that original[0-j] = primary[0-j] and
   * original[j-n] = residual[0-m] once the streams have been read to
   * completion.
   */
  splitReadStream(
    request: SplitReadStreamRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<SplitReadStreamResponse>>;
}

export interface BigQueryReadClient<CallOptionsExt = {}> {
  /**
   * Creates a new read session. A read session divides the contents of a
   * BigQuery table into one or more streams, which can then be used to read
   * data from the table. The read session also specifies properties of the
   * data to be read, such as a list of columns or a push-down filter describing
   * the rows to be returned.
   *
   * A particular row can be read by at most one stream. When the caller has
   * reached the end of each stream in the session, then all the data in the
   * table has been read.
   *
   * Data is assigned to each stream such that roughly the same number of
   * rows can be read from each stream. Because the server-side unit for
   * assigning data is collections of rows, the API does not guarantee that
   * each stream will return the same number or rows. Additionally, the
   * limits are enforced based on the number of pre-filtered rows, so some
   * filters can lead to lopsided assignments.
   *
   * Read sessions automatically expire 6 hours after they are created and do
   * not require manual clean-up by the caller.
   */
  createReadSession(
    request: DeepPartial<CreateReadSessionRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ReadSession>;
  /**
   * Reads rows from the stream in the format prescribed by the ReadSession.
   * Each response contains one or more table rows, up to a maximum of 100 MiB
   * per response; read requests which attempt to read individual rows larger
   * than 100 MiB will fail.
   *
   * Each request also returns a set of stream statistics reflecting the current
   * state of the stream.
   */
  readRows(
    request: DeepPartial<ReadRowsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<ReadRowsResponse>;
  /**
   * Splits a given `ReadStream` into two `ReadStream` objects. These
   * `ReadStream` objects are referred to as the primary and the residual
   * streams of the split. The original `ReadStream` can still be read from in
   * the same manner as before. Both of the returned `ReadStream` objects can
   * also be read from, and the rows returned by both child streams will be
   * the same as the rows read from the original stream.
   *
   * Moreover, the two child streams will be allocated back-to-back in the
   * original `ReadStream`. Concretely, it is guaranteed that for streams
   * original, primary, and residual, that original[0-j] = primary[0-j] and
   * original[j-n] = residual[0-m] once the streams have been read to
   * completion.
   */
  splitReadStream(
    request: DeepPartial<SplitReadStreamRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<SplitReadStreamResponse>;
}

/**
 * BigQuery Write API.
 *
 * The Write API can be used to write data to BigQuery.
 *
 * The [google.cloud.bigquery.storage.v1
 *   API](/bigquery/docs/reference/storage/rpc/google.cloud.bigquery.storage.v1)
 *   should be used instead of the v1beta2 API for BigQueryWrite operations.
 *
 * @deprecated
 */
export type BigQueryWriteDefinition = typeof BigQueryWriteDefinition;
export const BigQueryWriteDefinition = {
  name: "BigQueryWrite",
  fullName: "google.cloud.bigquery.storage.v1beta2.BigQueryWrite",
  methods: {
    /**
     * Creates a write stream to the given table.
     * Additionally, every table has a special COMMITTED stream named '_default'
     * to which data can be written. This stream doesn't need to be created using
     * CreateWriteStream. It is a stream that can be used simultaneously by any
     * number of clients. Data written to this stream is considered committed as
     * soon as an acknowledgement is received.
     *
     * @deprecated
     */
    createWriteStream: {
      name: "CreateWriteStream",
      requestType: CreateWriteStreamRequest,
      requestStream: false,
      responseType: WriteStream,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              19,
              112,
              97,
              114,
              101,
              110,
              116,
              44,
              119,
              114,
              105,
              116,
              101,
              95,
              115,
              116,
              114,
              101,
              97,
              109,
            ]),
          ],
          578365826: [
            Buffer.from([
              64,
              58,
              12,
              119,
              114,
              105,
              116,
              101,
              95,
              115,
              116,
              114,
              101,
              97,
              109,
              34,
              48,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              50,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Appends data to the given stream.
     *
     * If `offset` is specified, the `offset` is checked against the end of
     * stream. The server returns `OUT_OF_RANGE` in `AppendRowsResponse` if an
     * attempt is made to append to an offset beyond the current end of the stream
     * or `ALREADY_EXISTS` if user provids an `offset` that has already been
     * written to. User can retry with adjusted offset within the same RPC
     * stream. If `offset` is not specified, append happens at the end of the
     * stream.
     *
     * The response contains the offset at which the append happened. Responses
     * are received in the same order in which requests are sent. There will be
     * one response for each successful request. If the `offset` is not set in
     * response, it means append didn't happen due to some errors. If one request
     * fails, all the subsequent requests will also fail until a success request
     * is made again.
     *
     * If the stream is of `PENDING` type, data will only be available for read
     * operations after the stream is committed.
     *
     * @deprecated
     */
    appendRows: {
      name: "AppendRows",
      requestType: AppendRowsRequest,
      requestStream: true,
      responseType: AppendRowsResponse,
      responseStream: true,
      options: {
        _unknownFields: {
          8410: [Buffer.from([12, 119, 114, 105, 116, 101, 95, 115, 116, 114, 101, 97, 109])],
          578365826: [
            Buffer.from([
              69,
              58,
              1,
              42,
              34,
              64,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              50,
              47,
              123,
              119,
              114,
              105,
              116,
              101,
              95,
              115,
              116,
              114,
              101,
              97,
              109,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              47,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Gets a write stream.
     *
     * @deprecated
     */
    getWriteStream: {
      name: "GetWriteStream",
      requestType: GetWriteStreamRequest,
      requestStream: false,
      responseType: WriteStream,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              61,
              58,
              1,
              42,
              34,
              56,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              50,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              47,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Finalize a write stream so that no new data can be appended to the
     * stream. Finalize is not supported on the '_default' stream.
     *
     * @deprecated
     */
    finalizeWriteStream: {
      name: "FinalizeWriteStream",
      requestType: FinalizeWriteStreamRequest,
      requestStream: false,
      responseType: FinalizeWriteStreamResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              61,
              58,
              1,
              42,
              34,
              56,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              50,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              47,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Atomically commits a group of `PENDING` streams that belong to the same
     * `parent` table.
     * Streams must be finalized before commit and cannot be committed multiple
     * times. Once a stream is committed, data in the stream becomes available
     * for read operations.
     *
     * @deprecated
     */
    batchCommitWriteStreams: {
      name: "BatchCommitWriteStreams",
      requestType: BatchCommitWriteStreamsRequest,
      requestStream: false,
      responseType: BatchCommitWriteStreamsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([6, 112, 97, 114, 101, 110, 116])],
          578365826: [
            Buffer.from([
              50,
              18,
              48,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              50,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Flushes rows to a BUFFERED stream.
     * If users are appending rows to BUFFERED stream, flush operation is
     * required in order for the rows to become available for reading. A
     * Flush operation flushes up to any previously flushed offset in a BUFFERED
     * stream, to the offset specified in the request.
     * Flush is not supported on the _default stream, since it is not BUFFERED.
     *
     * @deprecated
     */
    flushRows: {
      name: "FlushRows",
      requestType: FlushRowsRequest,
      requestStream: false,
      responseType: FlushRowsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([12, 119, 114, 105, 116, 101, 95, 115, 116, 114, 101, 97, 109])],
          578365826: [
            Buffer.from([
              69,
              58,
              1,
              42,
              34,
              64,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              50,
              47,
              123,
              119,
              114,
              105,
              116,
              101,
              95,
              115,
              116,
              114,
              101,
              97,
              109,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              115,
              47,
              42,
              47,
              116,
              97,
              98,
              108,
              101,
              115,
              47,
              42,
              47,
              115,
              116,
              114,
              101,
              97,
              109,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface BigQueryWriteServiceImplementation<CallContextExt = {}> {
  /**
   * Creates a write stream to the given table.
   * Additionally, every table has a special COMMITTED stream named '_default'
   * to which data can be written. This stream doesn't need to be created using
   * CreateWriteStream. It is a stream that can be used simultaneously by any
   * number of clients. Data written to this stream is considered committed as
   * soon as an acknowledgement is received.
   *
   * @deprecated
   */
  createWriteStream(
    request: CreateWriteStreamRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<WriteStream>>;
  /**
   * Appends data to the given stream.
   *
   * If `offset` is specified, the `offset` is checked against the end of
   * stream. The server returns `OUT_OF_RANGE` in `AppendRowsResponse` if an
   * attempt is made to append to an offset beyond the current end of the stream
   * or `ALREADY_EXISTS` if user provids an `offset` that has already been
   * written to. User can retry with adjusted offset within the same RPC
   * stream. If `offset` is not specified, append happens at the end of the
   * stream.
   *
   * The response contains the offset at which the append happened. Responses
   * are received in the same order in which requests are sent. There will be
   * one response for each successful request. If the `offset` is not set in
   * response, it means append didn't happen due to some errors. If one request
   * fails, all the subsequent requests will also fail until a success request
   * is made again.
   *
   * If the stream is of `PENDING` type, data will only be available for read
   * operations after the stream is committed.
   *
   * @deprecated
   */
  appendRows(
    request: AsyncIterable<AppendRowsRequest>,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<AppendRowsResponse>>;
  /**
   * Gets a write stream.
   *
   * @deprecated
   */
  getWriteStream(
    request: GetWriteStreamRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<WriteStream>>;
  /**
   * Finalize a write stream so that no new data can be appended to the
   * stream. Finalize is not supported on the '_default' stream.
   *
   * @deprecated
   */
  finalizeWriteStream(
    request: FinalizeWriteStreamRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<FinalizeWriteStreamResponse>>;
  /**
   * Atomically commits a group of `PENDING` streams that belong to the same
   * `parent` table.
   * Streams must be finalized before commit and cannot be committed multiple
   * times. Once a stream is committed, data in the stream becomes available
   * for read operations.
   *
   * @deprecated
   */
  batchCommitWriteStreams(
    request: BatchCommitWriteStreamsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<BatchCommitWriteStreamsResponse>>;
  /**
   * Flushes rows to a BUFFERED stream.
   * If users are appending rows to BUFFERED stream, flush operation is
   * required in order for the rows to become available for reading. A
   * Flush operation flushes up to any previously flushed offset in a BUFFERED
   * stream, to the offset specified in the request.
   * Flush is not supported on the _default stream, since it is not BUFFERED.
   *
   * @deprecated
   */
  flushRows(request: FlushRowsRequest, context: CallContext & CallContextExt): Promise<DeepPartial<FlushRowsResponse>>;
}

export interface BigQueryWriteClient<CallOptionsExt = {}> {
  /**
   * Creates a write stream to the given table.
   * Additionally, every table has a special COMMITTED stream named '_default'
   * to which data can be written. This stream doesn't need to be created using
   * CreateWriteStream. It is a stream that can be used simultaneously by any
   * number of clients. Data written to this stream is considered committed as
   * soon as an acknowledgement is received.
   *
   * @deprecated
   */
  createWriteStream(
    request: DeepPartial<CreateWriteStreamRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<WriteStream>;
  /**
   * Appends data to the given stream.
   *
   * If `offset` is specified, the `offset` is checked against the end of
   * stream. The server returns `OUT_OF_RANGE` in `AppendRowsResponse` if an
   * attempt is made to append to an offset beyond the current end of the stream
   * or `ALREADY_EXISTS` if user provids an `offset` that has already been
   * written to. User can retry with adjusted offset within the same RPC
   * stream. If `offset` is not specified, append happens at the end of the
   * stream.
   *
   * The response contains the offset at which the append happened. Responses
   * are received in the same order in which requests are sent. There will be
   * one response for each successful request. If the `offset` is not set in
   * response, it means append didn't happen due to some errors. If one request
   * fails, all the subsequent requests will also fail until a success request
   * is made again.
   *
   * If the stream is of `PENDING` type, data will only be available for read
   * operations after the stream is committed.
   *
   * @deprecated
   */
  appendRows(
    request: AsyncIterable<DeepPartial<AppendRowsRequest>>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<AppendRowsResponse>;
  /**
   * Gets a write stream.
   *
   * @deprecated
   */
  getWriteStream(
    request: DeepPartial<GetWriteStreamRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<WriteStream>;
  /**
   * Finalize a write stream so that no new data can be appended to the
   * stream. Finalize is not supported on the '_default' stream.
   *
   * @deprecated
   */
  finalizeWriteStream(
    request: DeepPartial<FinalizeWriteStreamRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<FinalizeWriteStreamResponse>;
  /**
   * Atomically commits a group of `PENDING` streams that belong to the same
   * `parent` table.
   * Streams must be finalized before commit and cannot be committed multiple
   * times. Once a stream is committed, data in the stream becomes available
   * for read operations.
   *
   * @deprecated
   */
  batchCommitWriteStreams(
    request: DeepPartial<BatchCommitWriteStreamsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<BatchCommitWriteStreamsResponse>;
  /**
   * Flushes rows to a BUFFERED stream.
   * If users are appending rows to BUFFERED stream, flush operation is
   * required in order for the rows to become available for reading. A
   * Flush operation flushes up to any previously flushed offset in a BUFFERED
   * stream, to the offset specified in the request.
   * Flush is not supported on the _default stream, since it is not BUFFERED.
   *
   * @deprecated
   */
  flushRows(request: DeepPartial<FlushRowsRequest>, options?: CallOptions & CallOptionsExt): Promise<FlushRowsResponse>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export type ServerStreamingMethodResult<Response> = { [Symbol.asyncIterator](): AsyncIterator<Response, void> };

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
