// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/bigquery/v2/external_data_config.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { BoolValue, BytesValue, Int32Value, Int64Value, StringValue } from "../../../protobuf/wrappers.js";
import { DecimalTargetType, decimalTargetTypeFromJSON, decimalTargetTypeToJSON } from "./decimal_target_types.js";
import { FileSetSpecType, fileSetSpecTypeFromJSON, fileSetSpecTypeToJSON } from "./file_set_specification_type.js";
import { HivePartitioningOptions } from "./hive_partitioning.js";
import { JsonExtension, jsonExtensionFromJSON, jsonExtensionToJSON } from "./json_extension.js";
import { MapTargetType, mapTargetTypeFromJSON, mapTargetTypeToJSON } from "./map_target_type.js";
import { TableSchema } from "./table_schema.js";

export const protobufPackage = "google.cloud.bigquery.v2";

/** Options for external data sources. */
export interface AvroOptions {
  /**
   * Optional. If sourceFormat is set to "AVRO", indicates whether to interpret
   * logical types as the corresponding BigQuery data type (for example,
   * TIMESTAMP), instead of using the raw type (for example, INTEGER).
   */
  useAvroLogicalTypes: boolean | undefined;
}

/** Parquet Options for load and make external tables. */
export interface ParquetOptions {
  /**
   * Optional. Indicates whether to infer Parquet ENUM logical type as STRING
   * instead of BYTES by default.
   */
  enumAsString:
    | boolean
    | undefined;
  /**
   * Optional. Indicates whether to use schema inference specifically for
   * Parquet LIST logical type.
   */
  enableListInference:
    | boolean
    | undefined;
  /** Optional. Indicates how to represent a Parquet map if present. */
  mapTargetType: MapTargetType;
}

/** Information related to a CSV data source. */
export interface CsvOptions {
  /**
   * Optional. The separator character for fields in a CSV file. The separator
   * is interpreted as a single byte. For files encoded in ISO-8859-1, any
   * single character can be used as a separator. For files encoded in UTF-8,
   * characters represented in decimal range 1-127 (U+0001-U+007F) can be used
   * without any modification. UTF-8 characters encoded with multiple bytes
   * (i.e. U+0080 and above) will have only the first byte used for separating
   * fields. The remaining bytes will be treated as a part of the field.
   * BigQuery also supports the escape sequence "\t" (U+0009) to specify a tab
   * separator. The default value is comma (",", U+002C).
   */
  fieldDelimiter: string;
  /**
   * Optional. The number of rows at the top of a CSV file that BigQuery will
   * skip when reading the data. The default value is 0. This property is
   * useful if you have header rows in the file that should be skipped.
   * When autodetect is on, the behavior is the following:
   *
   * * skipLeadingRows unspecified - Autodetect tries to detect headers in the
   *   first row. If they are not detected, the row is read as data. Otherwise
   *   data is read starting from the second row.
   * * skipLeadingRows is 0 - Instructs autodetect that there are no headers and
   *   data should be read starting from the first row.
   * * skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect
   *   headers in row N. If headers are not detected, row N is just skipped.
   *   Otherwise row N is used to extract column names for the detected schema.
   */
  skipLeadingRows:
    | Long
    | undefined;
  /**
   * Optional. The value that is used to quote data sections in a CSV file.
   * BigQuery converts the string to ISO-8859-1 encoding, and then uses the
   * first byte of the encoded string to split the data in its raw, binary
   * state.
   * The default value is a double-quote (").
   * If your data does not contain quoted sections,
   * set the property value to an empty string.
   * If your data contains quoted newline characters, you must also set the
   * allowQuotedNewlines property to true.
   * To include the specific quote character within a quoted value, precede it
   * with an additional matching quote character. For example, if you want to
   * escape the default character  ' " ', use ' "" '.
   */
  quote:
    | string
    | undefined;
  /**
   * Optional. Indicates if BigQuery should allow quoted data sections that
   * contain newline characters in a CSV file. The default value is false.
   */
  allowQuotedNewlines:
    | boolean
    | undefined;
  /**
   * Optional. Indicates if BigQuery should accept rows that are missing
   * trailing optional columns. If true, BigQuery treats missing trailing
   * columns as null values.
   * If false, records with missing trailing columns are treated as bad records,
   * and if there are too many bad records, an invalid error is returned in the
   * job result. The default value is false.
   */
  allowJaggedRows:
    | boolean
    | undefined;
  /**
   * Optional. The character encoding of the data.
   * The supported values are UTF-8, ISO-8859-1, UTF-16BE, UTF-16LE, UTF-32BE,
   * and UTF-32LE.  The default value is UTF-8.
   * BigQuery decodes the data after the raw, binary data has been split using
   * the values of the quote and fieldDelimiter properties.
   */
  encoding: string;
  /**
   * Optional. Indicates if the embedded ASCII control characters (the first 32
   * characters in the ASCII-table, from '\x00' to '\x1F') are preserved.
   */
  preserveAsciiControlCharacters:
    | boolean
    | undefined;
  /**
   * Optional. Specifies a string that represents a null value in a CSV file.
   * For example, if you specify "\N", BigQuery interprets "\N" as a null value
   * when querying a CSV file.
   * The default value is the empty string. If you set this property to a custom
   * value, BigQuery throws an error if an empty string is present for all data
   * types except for STRING and BYTE. For STRING and BYTE columns, BigQuery
   * interprets the empty string as an empty value.
   */
  nullMarker: string | undefined;
}

/** Json Options for load and make external tables. */
export interface JsonOptions {
  /**
   * Optional. The character encoding of the data.
   * The supported values are UTF-8, UTF-16BE, UTF-16LE, UTF-32BE,
   * and UTF-32LE.  The default value is UTF-8.
   */
  encoding: string;
}

/** Information related to a Bigtable column. */
export interface BigtableColumn {
  /**
   * [Required] Qualifier of the column.
   * Columns in the parent column family that has this exact qualifier are
   * exposed as `<family field name>.<column field name>` field.
   * If the qualifier is valid UTF-8 string, it can be specified in the
   * qualifier_string field.  Otherwise, a base-64 encoded value must be set to
   * qualifier_encoded.
   * The column field name is the same as the column qualifier. However, if the
   * qualifier is not a valid BigQuery field identifier i.e. does not match
   * [a-zA-Z][a-zA-Z0-9_]*, a valid identifier must be provided as field_name.
   */
  qualifierEncoded:
    | Buffer
    | undefined;
  /** Qualifier string. */
  qualifierString:
    | string
    | undefined;
  /**
   * Optional. If the qualifier is not a valid BigQuery field identifier i.e.
   * does not match [a-zA-Z][a-zA-Z0-9_]*,  a valid identifier must be provided
   * as the column field name and is used as field name in queries.
   */
  fieldName: string;
  /**
   * Optional. The type to convert the value in cells of this column.
   * The values are expected to be encoded using HBase Bytes.toBytes function
   * when using the BINARY encoding value.
   * Following BigQuery types are allowed (case-sensitive):
   *
   * * BYTES
   * * STRING
   * * INTEGER
   * * FLOAT
   * * BOOLEAN
   * * JSON
   *
   * Default type is BYTES.
   * 'type' can also be set at the column family level. However, the setting at
   * this level takes precedence if 'type' is set at both levels.
   */
  type: string;
  /**
   * Optional. The encoding of the values when the type is not STRING.
   * Acceptable encoding values are:
   *   TEXT - indicates values are alphanumeric text strings.
   *   BINARY - indicates values are encoded using HBase Bytes.toBytes family of
   *            functions.
   * 'encoding' can also be set at the column family level. However, the setting
   * at this level takes precedence if 'encoding' is set at both levels.
   */
  encoding: string;
  /**
   * Optional. If this is set, only the latest version of value in this column
   *             are exposed.
   * 'onlyReadLatest' can also be set at the column family level. However, the
   * setting at this level takes precedence if 'onlyReadLatest' is set at both
   * levels.
   */
  onlyReadLatest: boolean | undefined;
}

/** Information related to a Bigtable column family. */
export interface BigtableColumnFamily {
  /** Identifier of the column family. */
  familyId: string;
  /**
   * Optional. The type to convert the value in cells of this column family.
   * The values are expected to be encoded using HBase Bytes.toBytes function
   * when using the BINARY encoding value.
   * Following BigQuery types are allowed (case-sensitive):
   *
   * * BYTES
   * * STRING
   * * INTEGER
   * * FLOAT
   * * BOOLEAN
   * * JSON
   *
   * Default type is BYTES.
   * This can be overridden for a specific column by listing that column in
   * 'columns' and specifying a type for it.
   */
  type: string;
  /**
   * Optional. The encoding of the values when the type is not STRING.
   * Acceptable encoding values are:
   *   TEXT - indicates values are alphanumeric text strings.
   *   BINARY - indicates values are encoded using HBase Bytes.toBytes family of
   *            functions.
   * This can be overridden for a specific column by listing that column in
   * 'columns' and specifying an encoding for it.
   */
  encoding: string;
  /**
   * Optional. Lists of columns that should be exposed as individual fields as
   * opposed to a list of (column name, value) pairs.
   * All columns whose qualifier matches a qualifier in this list can be
   * accessed as `<family field name>.<column field name>`.
   * Other columns can be accessed as a list through
   * the `<family field name>.Column` field.
   */
  columns: BigtableColumn[];
  /**
   * Optional. If this is set only the latest version of value are exposed for
   * all columns in this column family.
   * This can be overridden for a specific column by listing that column in
   * 'columns' and specifying a different setting
   * for that column.
   */
  onlyReadLatest: boolean | undefined;
}

/** Options specific to Google Cloud Bigtable data sources. */
export interface BigtableOptions {
  /**
   * Optional. List of column families to expose in the table schema along with
   * their types.
   * This list restricts the column families that can be referenced in queries
   * and specifies their value types.
   * You can use this list to do type conversions - see the 'type' field for
   * more details.
   * If you leave this list empty, all column families are present in the table
   * schema and their values are read as BYTES.
   * During a query only the column families referenced in that query are read
   * from Bigtable.
   */
  columnFamilies: BigtableColumnFamily[];
  /**
   * Optional. If field is true, then the column families that are not
   * specified in columnFamilies list are not exposed in the table schema.
   * Otherwise, they are read with BYTES type values.
   * The default value is false.
   */
  ignoreUnspecifiedColumnFamilies:
    | boolean
    | undefined;
  /**
   * Optional. If field is true, then the rowkey column families will be read
   * and converted to string. Otherwise they are read with BYTES type values and
   * users need to manually cast them with CAST if necessary.
   * The default value is false.
   */
  readRowkeyAsString:
    | boolean
    | undefined;
  /**
   * Optional. If field is true, then each column family will be read as a
   * single JSON column. Otherwise they are read as a repeated cell structure
   * containing timestamp/value tuples. The default value is false.
   */
  outputColumnFamiliesAsJson: boolean | undefined;
}

/** Options specific to Google Sheets data sources. */
export interface GoogleSheetsOptions {
  /**
   * Optional. The number of rows at the top of a sheet that BigQuery will skip
   * when reading the data. The default value is 0. This property is useful if
   * you have header rows that should be skipped. When autodetect is on,
   * the behavior is the following:
   * * skipLeadingRows unspecified - Autodetect tries to detect headers in the
   *   first row. If they are not detected, the row is read as data. Otherwise
   *   data is read starting from the second row.
   * * skipLeadingRows is 0 - Instructs autodetect that there are no headers and
   *   data should be read starting from the first row.
   * * skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect
   *   headers in row N. If headers are not detected, row N is just skipped.
   *   Otherwise row N is used to extract column names for the detected schema.
   */
  skipLeadingRows:
    | Long
    | undefined;
  /**
   * Optional. Range of a sheet to query from. Only used when non-empty.
   * Typical format: sheet_name!top_left_cell_id:bottom_right_cell_id
   * For example: sheet1!A1:B20
   */
  range: string;
}

export interface ExternalDataConfiguration {
  /**
   * [Required] The fully-qualified URIs that point to your data in Google
   * Cloud. For Google Cloud Storage URIs:
   *   Each URI can contain one '*' wildcard character and it must come after
   *   the 'bucket' name.
   *   Size limits related to load jobs apply to external data sources.
   * For Google Cloud Bigtable URIs:
   *   Exactly one URI can be specified and it has be a fully specified and
   *   valid HTTPS URL for a Google Cloud Bigtable table.
   * For Google Cloud Datastore backups, exactly one URI can be specified. Also,
   * the '*' wildcard character is not allowed.
   */
  sourceUris: string[];
  /**
   * Optional. Specifies how source URIs are interpreted for constructing the
   * file set to load.  By default source URIs are expanded against the
   * underlying storage.  Other options include specifying manifest files. Only
   * applicable to object storage systems.
   */
  fileSetSpecType: FileSetSpecType;
  /**
   * Optional. The schema for the data.
   * Schema is required for CSV and JSON formats if autodetect is not on.
   * Schema is disallowed for Google Cloud Bigtable, Cloud Datastore backups,
   * Avro, ORC and Parquet formats.
   */
  schema:
    | TableSchema
    | undefined;
  /**
   * [Required] The data format.
   * For CSV files, specify "CSV".
   * For Google sheets, specify "GOOGLE_SHEETS".
   * For newline-delimited JSON, specify "NEWLINE_DELIMITED_JSON".
   * For Avro files, specify "AVRO".
   * For Google Cloud Datastore backups, specify "DATASTORE_BACKUP".
   * For Apache Iceberg tables, specify "ICEBERG".
   * For ORC files, specify "ORC".
   * For Parquet files, specify "PARQUET".
   * [Beta] For Google Cloud Bigtable, specify "BIGTABLE".
   */
  sourceFormat: string;
  /**
   * Optional. The maximum number of bad records that BigQuery can ignore when
   * reading data. If the number of bad records exceeds this value, an invalid
   * error is returned in the job result. The default value is 0, which requires
   * that all records are valid. This setting is ignored for Google Cloud
   * Bigtable, Google Cloud Datastore backups, Avro, ORC and Parquet formats.
   */
  maxBadRecords:
    | number
    | undefined;
  /**
   * Try to detect schema and format options automatically.
   * Any option specified explicitly will be honored.
   */
  autodetect:
    | boolean
    | undefined;
  /**
   * Optional. Indicates if BigQuery should allow extra values that are not
   * represented in the table schema.
   * If true, the extra values are ignored.
   * If false, records with extra columns are treated as bad records, and if
   * there are too many bad records, an invalid error is returned in the job
   * result.
   * The default value is false.
   * The sourceFormat property determines what BigQuery treats as an extra
   * value:
   *   CSV: Trailing columns
   *   JSON: Named values that don't match any column names
   *   Google Cloud Bigtable: This setting is ignored.
   *   Google Cloud Datastore backups: This setting is ignored.
   *   Avro: This setting is ignored.
   *   ORC: This setting is ignored.
   *   Parquet: This setting is ignored.
   */
  ignoreUnknownValues:
    | boolean
    | undefined;
  /**
   * Optional. The compression type of the data source.
   * Possible values include GZIP and NONE. The default value is NONE.
   * This setting is ignored for Google Cloud Bigtable, Google Cloud Datastore
   * backups, Avro, ORC and Parquet
   * formats. An empty string is an invalid value.
   */
  compression: string;
  /** Optional. Additional properties to set if sourceFormat is set to CSV. */
  csvOptions:
    | CsvOptions
    | undefined;
  /** Optional. Additional properties to set if sourceFormat is set to JSON. */
  jsonOptions:
    | JsonOptions
    | undefined;
  /** Optional. Additional options if sourceFormat is set to BIGTABLE. */
  bigtableOptions:
    | BigtableOptions
    | undefined;
  /** Optional. Additional options if sourceFormat is set to GOOGLE_SHEETS. */
  googleSheetsOptions:
    | GoogleSheetsOptions
    | undefined;
  /**
   * Optional. When set, configures hive partitioning support. Not all storage
   * formats support hive partitioning -- requesting hive partitioning on an
   * unsupported format will lead to an error, as will providing an invalid
   * specification.
   */
  hivePartitioningOptions:
    | HivePartitioningOptions
    | undefined;
  /**
   * Optional. The connection specifying the credentials to be used to read
   * external storage, such as Azure Blob, Cloud Storage, or S3. The
   * connection_id can have the form
   * `{project_id}.{location_id};{connection_id}` or
   * `projects/{project_id}/locations/{location_id}/connections/{connection_id}`.
   */
  connectionId: string;
  /**
   * Defines the list of possible SQL data types to which the source decimal
   * values are converted. This list and the precision and the scale parameters
   * of the decimal field determine the target type. In the order of NUMERIC,
   * BIGNUMERIC, and STRING, a
   * type is picked if it is in the specified list and if it supports the
   * precision and the scale. STRING supports all precision and scale values.
   * If none of the listed types supports the precision and the scale, the type
   * supporting the widest range in the specified list is picked, and if a value
   * exceeds the supported range when reading the data, an error will be thrown.
   *
   * Example: Suppose the value of this field is ["NUMERIC", "BIGNUMERIC"].
   * If (precision,scale) is:
   *
   * * (38,9) -> NUMERIC;
   * * (39,9) -> BIGNUMERIC (NUMERIC cannot hold 30 integer digits);
   * * (38,10) -> BIGNUMERIC (NUMERIC cannot hold 10 fractional digits);
   * * (76,38) -> BIGNUMERIC;
   * * (77,38) -> BIGNUMERIC (error if value exeeds supported range).
   *
   * This field cannot contain duplicate types. The order of the types in this
   * field is ignored. For example, ["BIGNUMERIC", "NUMERIC"] is the same as
   * ["NUMERIC", "BIGNUMERIC"] and NUMERIC always takes precedence over
   * BIGNUMERIC.
   *
   * Defaults to ["NUMERIC", "STRING"] for ORC and ["NUMERIC"] for the other
   * file formats.
   */
  decimalTargetTypes: DecimalTargetType[];
  /** Optional. Additional properties to set if sourceFormat is set to AVRO. */
  avroOptions:
    | AvroOptions
    | undefined;
  /**
   * Optional. Load option to be used together with source_format
   * newline-delimited JSON to indicate that a variant of JSON is being loaded.
   * To load newline-delimited GeoJSON, specify GEOJSON (and source_format must
   * be set to NEWLINE_DELIMITED_JSON).
   */
  jsonExtension: JsonExtension;
  /** Optional. Additional properties to set if sourceFormat is set to PARQUET. */
  parquetOptions:
    | ParquetOptions
    | undefined;
  /**
   * Optional. ObjectMetadata is used to create Object Tables. Object Tables
   * contain a listing of objects (with their metadata) found at the
   * source_uris. If ObjectMetadata is set, source_format should be omitted.
   *
   * Currently SIMPLE is the only supported Object Metadata type.
   */
  objectMetadata?:
    | ExternalDataConfiguration_ObjectMetadata
    | undefined;
  /**
   * Optional. When creating an external table, the user can provide a reference
   * file with the table schema. This is enabled for the following formats:
   * AVRO, PARQUET, ORC.
   */
  referenceFileSchemaUri:
    | string
    | undefined;
  /**
   * Optional. Metadata Cache Mode for the table. Set this to enable caching of
   * metadata from external data source.
   */
  metadataCacheMode: ExternalDataConfiguration_MetadataCacheMode;
}

/** Supported Object Metadata Types. */
export enum ExternalDataConfiguration_ObjectMetadata {
  /** OBJECT_METADATA_UNSPECIFIED - Unspecified by default. */
  OBJECT_METADATA_UNSPECIFIED = 0,
  /** DIRECTORY - A synonym for `SIMPLE`. */
  DIRECTORY = 1,
  /** SIMPLE - Directory listing of objects. */
  SIMPLE = 2,
  UNRECOGNIZED = -1,
}

export function externalDataConfiguration_ObjectMetadataFromJSON(
  object: any,
): ExternalDataConfiguration_ObjectMetadata {
  switch (object) {
    case 0:
    case "OBJECT_METADATA_UNSPECIFIED":
      return ExternalDataConfiguration_ObjectMetadata.OBJECT_METADATA_UNSPECIFIED;
    case 1:
    case "DIRECTORY":
      return ExternalDataConfiguration_ObjectMetadata.DIRECTORY;
    case 2:
    case "SIMPLE":
      return ExternalDataConfiguration_ObjectMetadata.SIMPLE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ExternalDataConfiguration_ObjectMetadata.UNRECOGNIZED;
  }
}

export function externalDataConfiguration_ObjectMetadataToJSON(
  object: ExternalDataConfiguration_ObjectMetadata,
): string {
  switch (object) {
    case ExternalDataConfiguration_ObjectMetadata.OBJECT_METADATA_UNSPECIFIED:
      return "OBJECT_METADATA_UNSPECIFIED";
    case ExternalDataConfiguration_ObjectMetadata.DIRECTORY:
      return "DIRECTORY";
    case ExternalDataConfiguration_ObjectMetadata.SIMPLE:
      return "SIMPLE";
    case ExternalDataConfiguration_ObjectMetadata.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * MetadataCacheMode identifies if the table should use metadata caching for
 * files from external source (eg Google Cloud Storage).
 */
export enum ExternalDataConfiguration_MetadataCacheMode {
  /** METADATA_CACHE_MODE_UNSPECIFIED - Unspecified metadata cache mode. */
  METADATA_CACHE_MODE_UNSPECIFIED = 0,
  /**
   * AUTOMATIC - Set this mode to trigger automatic background refresh of metadata cache
   * from the external source. Queries will use the latest available cache
   * version within the table's maxStaleness interval.
   */
  AUTOMATIC = 1,
  /**
   * MANUAL - Set this mode to enable triggering manual refresh of the metadata cache
   * from external source. Queries will use the latest manually triggered
   * cache version within the table's maxStaleness interval.
   */
  MANUAL = 2,
  UNRECOGNIZED = -1,
}

export function externalDataConfiguration_MetadataCacheModeFromJSON(
  object: any,
): ExternalDataConfiguration_MetadataCacheMode {
  switch (object) {
    case 0:
    case "METADATA_CACHE_MODE_UNSPECIFIED":
      return ExternalDataConfiguration_MetadataCacheMode.METADATA_CACHE_MODE_UNSPECIFIED;
    case 1:
    case "AUTOMATIC":
      return ExternalDataConfiguration_MetadataCacheMode.AUTOMATIC;
    case 2:
    case "MANUAL":
      return ExternalDataConfiguration_MetadataCacheMode.MANUAL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ExternalDataConfiguration_MetadataCacheMode.UNRECOGNIZED;
  }
}

export function externalDataConfiguration_MetadataCacheModeToJSON(
  object: ExternalDataConfiguration_MetadataCacheMode,
): string {
  switch (object) {
    case ExternalDataConfiguration_MetadataCacheMode.METADATA_CACHE_MODE_UNSPECIFIED:
      return "METADATA_CACHE_MODE_UNSPECIFIED";
    case ExternalDataConfiguration_MetadataCacheMode.AUTOMATIC:
      return "AUTOMATIC";
    case ExternalDataConfiguration_MetadataCacheMode.MANUAL:
      return "MANUAL";
    case ExternalDataConfiguration_MetadataCacheMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseAvroOptions(): AvroOptions {
  return { useAvroLogicalTypes: undefined };
}

export const AvroOptions: MessageFns<AvroOptions> = {
  encode(message: AvroOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.useAvroLogicalTypes !== undefined) {
      BoolValue.encode({ value: message.useAvroLogicalTypes! }, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AvroOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAvroOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.useAvroLogicalTypes = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AvroOptions {
    return { useAvroLogicalTypes: isSet(object.useAvroLogicalTypes) ? Boolean(object.useAvroLogicalTypes) : undefined };
  },

  toJSON(message: AvroOptions): unknown {
    const obj: any = {};
    if (message.useAvroLogicalTypes !== undefined) {
      obj.useAvroLogicalTypes = message.useAvroLogicalTypes;
    }
    return obj;
  },

  create(base?: DeepPartial<AvroOptions>): AvroOptions {
    return AvroOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AvroOptions>): AvroOptions {
    const message = createBaseAvroOptions();
    message.useAvroLogicalTypes = object.useAvroLogicalTypes ?? undefined;
    return message;
  },
};

function createBaseParquetOptions(): ParquetOptions {
  return { enumAsString: undefined, enableListInference: undefined, mapTargetType: 0 };
}

export const ParquetOptions: MessageFns<ParquetOptions> = {
  encode(message: ParquetOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.enumAsString !== undefined) {
      BoolValue.encode({ value: message.enumAsString! }, writer.uint32(10).fork()).join();
    }
    if (message.enableListInference !== undefined) {
      BoolValue.encode({ value: message.enableListInference! }, writer.uint32(18).fork()).join();
    }
    if (message.mapTargetType !== 0) {
      writer.uint32(24).int32(message.mapTargetType);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ParquetOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseParquetOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.enumAsString = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.enableListInference = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.mapTargetType = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ParquetOptions {
    return {
      enumAsString: isSet(object.enumAsString) ? Boolean(object.enumAsString) : undefined,
      enableListInference: isSet(object.enableListInference) ? Boolean(object.enableListInference) : undefined,
      mapTargetType: isSet(object.mapTargetType) ? mapTargetTypeFromJSON(object.mapTargetType) : 0,
    };
  },

  toJSON(message: ParquetOptions): unknown {
    const obj: any = {};
    if (message.enumAsString !== undefined) {
      obj.enumAsString = message.enumAsString;
    }
    if (message.enableListInference !== undefined) {
      obj.enableListInference = message.enableListInference;
    }
    if (message.mapTargetType !== 0) {
      obj.mapTargetType = mapTargetTypeToJSON(message.mapTargetType);
    }
    return obj;
  },

  create(base?: DeepPartial<ParquetOptions>): ParquetOptions {
    return ParquetOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ParquetOptions>): ParquetOptions {
    const message = createBaseParquetOptions();
    message.enumAsString = object.enumAsString ?? undefined;
    message.enableListInference = object.enableListInference ?? undefined;
    message.mapTargetType = object.mapTargetType ?? 0;
    return message;
  },
};

function createBaseCsvOptions(): CsvOptions {
  return {
    fieldDelimiter: "",
    skipLeadingRows: undefined,
    quote: undefined,
    allowQuotedNewlines: undefined,
    allowJaggedRows: undefined,
    encoding: "",
    preserveAsciiControlCharacters: undefined,
    nullMarker: undefined,
  };
}

export const CsvOptions: MessageFns<CsvOptions> = {
  encode(message: CsvOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.fieldDelimiter !== "") {
      writer.uint32(10).string(message.fieldDelimiter);
    }
    if (message.skipLeadingRows !== undefined) {
      Int64Value.encode({ value: message.skipLeadingRows! }, writer.uint32(18).fork()).join();
    }
    if (message.quote !== undefined) {
      StringValue.encode({ value: message.quote! }, writer.uint32(26).fork()).join();
    }
    if (message.allowQuotedNewlines !== undefined) {
      BoolValue.encode({ value: message.allowQuotedNewlines! }, writer.uint32(34).fork()).join();
    }
    if (message.allowJaggedRows !== undefined) {
      BoolValue.encode({ value: message.allowJaggedRows! }, writer.uint32(42).fork()).join();
    }
    if (message.encoding !== "") {
      writer.uint32(50).string(message.encoding);
    }
    if (message.preserveAsciiControlCharacters !== undefined) {
      BoolValue.encode({ value: message.preserveAsciiControlCharacters! }, writer.uint32(58).fork()).join();
    }
    if (message.nullMarker !== undefined) {
      StringValue.encode({ value: message.nullMarker! }, writer.uint32(66).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CsvOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCsvOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.fieldDelimiter = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.skipLeadingRows = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.quote = StringValue.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.allowQuotedNewlines = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.allowJaggedRows = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.encoding = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.preserveAsciiControlCharacters = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.nullMarker = StringValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CsvOptions {
    return {
      fieldDelimiter: isSet(object.fieldDelimiter) ? globalThis.String(object.fieldDelimiter) : "",
      skipLeadingRows: isSet(object.skipLeadingRows) ? Long.fromValue(object.skipLeadingRows) : undefined,
      quote: isSet(object.quote) ? String(object.quote) : undefined,
      allowQuotedNewlines: isSet(object.allowQuotedNewlines) ? Boolean(object.allowQuotedNewlines) : undefined,
      allowJaggedRows: isSet(object.allowJaggedRows) ? Boolean(object.allowJaggedRows) : undefined,
      encoding: isSet(object.encoding) ? globalThis.String(object.encoding) : "",
      preserveAsciiControlCharacters: isSet(object.preserveAsciiControlCharacters)
        ? Boolean(object.preserveAsciiControlCharacters)
        : undefined,
      nullMarker: isSet(object.nullMarker) ? String(object.nullMarker) : undefined,
    };
  },

  toJSON(message: CsvOptions): unknown {
    const obj: any = {};
    if (message.fieldDelimiter !== "") {
      obj.fieldDelimiter = message.fieldDelimiter;
    }
    if (message.skipLeadingRows !== undefined) {
      obj.skipLeadingRows = message.skipLeadingRows;
    }
    if (message.quote !== undefined) {
      obj.quote = message.quote;
    }
    if (message.allowQuotedNewlines !== undefined) {
      obj.allowQuotedNewlines = message.allowQuotedNewlines;
    }
    if (message.allowJaggedRows !== undefined) {
      obj.allowJaggedRows = message.allowJaggedRows;
    }
    if (message.encoding !== "") {
      obj.encoding = message.encoding;
    }
    if (message.preserveAsciiControlCharacters !== undefined) {
      obj.preserveAsciiControlCharacters = message.preserveAsciiControlCharacters;
    }
    if (message.nullMarker !== undefined) {
      obj.nullMarker = message.nullMarker;
    }
    return obj;
  },

  create(base?: DeepPartial<CsvOptions>): CsvOptions {
    return CsvOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CsvOptions>): CsvOptions {
    const message = createBaseCsvOptions();
    message.fieldDelimiter = object.fieldDelimiter ?? "";
    message.skipLeadingRows = (object.skipLeadingRows !== undefined && object.skipLeadingRows !== null)
      ? Long.fromValue(object.skipLeadingRows)
      : undefined;
    message.quote = object.quote ?? undefined;
    message.allowQuotedNewlines = object.allowQuotedNewlines ?? undefined;
    message.allowJaggedRows = object.allowJaggedRows ?? undefined;
    message.encoding = object.encoding ?? "";
    message.preserveAsciiControlCharacters = object.preserveAsciiControlCharacters ?? undefined;
    message.nullMarker = object.nullMarker ?? undefined;
    return message;
  },
};

function createBaseJsonOptions(): JsonOptions {
  return { encoding: "" };
}

export const JsonOptions: MessageFns<JsonOptions> = {
  encode(message: JsonOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.encoding !== "") {
      writer.uint32(10).string(message.encoding);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JsonOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJsonOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.encoding = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JsonOptions {
    return { encoding: isSet(object.encoding) ? globalThis.String(object.encoding) : "" };
  },

  toJSON(message: JsonOptions): unknown {
    const obj: any = {};
    if (message.encoding !== "") {
      obj.encoding = message.encoding;
    }
    return obj;
  },

  create(base?: DeepPartial<JsonOptions>): JsonOptions {
    return JsonOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JsonOptions>): JsonOptions {
    const message = createBaseJsonOptions();
    message.encoding = object.encoding ?? "";
    return message;
  },
};

function createBaseBigtableColumn(): BigtableColumn {
  return {
    qualifierEncoded: undefined,
    qualifierString: undefined,
    fieldName: "",
    type: "",
    encoding: "",
    onlyReadLatest: undefined,
  };
}

export const BigtableColumn: MessageFns<BigtableColumn> = {
  encode(message: BigtableColumn, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.qualifierEncoded !== undefined) {
      BytesValue.encode({ value: message.qualifierEncoded! }, writer.uint32(10).fork()).join();
    }
    if (message.qualifierString !== undefined) {
      StringValue.encode({ value: message.qualifierString! }, writer.uint32(18).fork()).join();
    }
    if (message.fieldName !== "") {
      writer.uint32(26).string(message.fieldName);
    }
    if (message.type !== "") {
      writer.uint32(34).string(message.type);
    }
    if (message.encoding !== "") {
      writer.uint32(42).string(message.encoding);
    }
    if (message.onlyReadLatest !== undefined) {
      BoolValue.encode({ value: message.onlyReadLatest! }, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigtableColumn {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigtableColumn();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.qualifierEncoded = BytesValue.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.qualifierString = StringValue.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.fieldName = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.type = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.encoding = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.onlyReadLatest = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigtableColumn {
    return {
      qualifierEncoded: isSet(object.qualifierEncoded) ? new Buffer(object.qualifierEncoded) : undefined,
      qualifierString: isSet(object.qualifierString) ? String(object.qualifierString) : undefined,
      fieldName: isSet(object.fieldName) ? globalThis.String(object.fieldName) : "",
      type: isSet(object.type) ? globalThis.String(object.type) : "",
      encoding: isSet(object.encoding) ? globalThis.String(object.encoding) : "",
      onlyReadLatest: isSet(object.onlyReadLatest) ? Boolean(object.onlyReadLatest) : undefined,
    };
  },

  toJSON(message: BigtableColumn): unknown {
    const obj: any = {};
    if (message.qualifierEncoded !== undefined) {
      obj.qualifierEncoded = message.qualifierEncoded;
    }
    if (message.qualifierString !== undefined) {
      obj.qualifierString = message.qualifierString;
    }
    if (message.fieldName !== "") {
      obj.fieldName = message.fieldName;
    }
    if (message.type !== "") {
      obj.type = message.type;
    }
    if (message.encoding !== "") {
      obj.encoding = message.encoding;
    }
    if (message.onlyReadLatest !== undefined) {
      obj.onlyReadLatest = message.onlyReadLatest;
    }
    return obj;
  },

  create(base?: DeepPartial<BigtableColumn>): BigtableColumn {
    return BigtableColumn.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigtableColumn>): BigtableColumn {
    const message = createBaseBigtableColumn();
    message.qualifierEncoded = object.qualifierEncoded ?? undefined;
    message.qualifierString = object.qualifierString ?? undefined;
    message.fieldName = object.fieldName ?? "";
    message.type = object.type ?? "";
    message.encoding = object.encoding ?? "";
    message.onlyReadLatest = object.onlyReadLatest ?? undefined;
    return message;
  },
};

function createBaseBigtableColumnFamily(): BigtableColumnFamily {
  return { familyId: "", type: "", encoding: "", columns: [], onlyReadLatest: undefined };
}

export const BigtableColumnFamily: MessageFns<BigtableColumnFamily> = {
  encode(message: BigtableColumnFamily, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.familyId !== "") {
      writer.uint32(10).string(message.familyId);
    }
    if (message.type !== "") {
      writer.uint32(18).string(message.type);
    }
    if (message.encoding !== "") {
      writer.uint32(26).string(message.encoding);
    }
    for (const v of message.columns) {
      BigtableColumn.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.onlyReadLatest !== undefined) {
      BoolValue.encode({ value: message.onlyReadLatest! }, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigtableColumnFamily {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigtableColumnFamily();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.familyId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.type = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.encoding = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.columns.push(BigtableColumn.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.onlyReadLatest = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigtableColumnFamily {
    return {
      familyId: isSet(object.familyId) ? globalThis.String(object.familyId) : "",
      type: isSet(object.type) ? globalThis.String(object.type) : "",
      encoding: isSet(object.encoding) ? globalThis.String(object.encoding) : "",
      columns: globalThis.Array.isArray(object?.columns)
        ? object.columns.map((e: any) => BigtableColumn.fromJSON(e))
        : [],
      onlyReadLatest: isSet(object.onlyReadLatest) ? Boolean(object.onlyReadLatest) : undefined,
    };
  },

  toJSON(message: BigtableColumnFamily): unknown {
    const obj: any = {};
    if (message.familyId !== "") {
      obj.familyId = message.familyId;
    }
    if (message.type !== "") {
      obj.type = message.type;
    }
    if (message.encoding !== "") {
      obj.encoding = message.encoding;
    }
    if (message.columns?.length) {
      obj.columns = message.columns.map((e) => BigtableColumn.toJSON(e));
    }
    if (message.onlyReadLatest !== undefined) {
      obj.onlyReadLatest = message.onlyReadLatest;
    }
    return obj;
  },

  create(base?: DeepPartial<BigtableColumnFamily>): BigtableColumnFamily {
    return BigtableColumnFamily.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigtableColumnFamily>): BigtableColumnFamily {
    const message = createBaseBigtableColumnFamily();
    message.familyId = object.familyId ?? "";
    message.type = object.type ?? "";
    message.encoding = object.encoding ?? "";
    message.columns = object.columns?.map((e) => BigtableColumn.fromPartial(e)) || [];
    message.onlyReadLatest = object.onlyReadLatest ?? undefined;
    return message;
  },
};

function createBaseBigtableOptions(): BigtableOptions {
  return {
    columnFamilies: [],
    ignoreUnspecifiedColumnFamilies: undefined,
    readRowkeyAsString: undefined,
    outputColumnFamiliesAsJson: undefined,
  };
}

export const BigtableOptions: MessageFns<BigtableOptions> = {
  encode(message: BigtableOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.columnFamilies) {
      BigtableColumnFamily.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.ignoreUnspecifiedColumnFamilies !== undefined) {
      BoolValue.encode({ value: message.ignoreUnspecifiedColumnFamilies! }, writer.uint32(18).fork()).join();
    }
    if (message.readRowkeyAsString !== undefined) {
      BoolValue.encode({ value: message.readRowkeyAsString! }, writer.uint32(26).fork()).join();
    }
    if (message.outputColumnFamiliesAsJson !== undefined) {
      BoolValue.encode({ value: message.outputColumnFamiliesAsJson! }, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigtableOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigtableOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.columnFamilies.push(BigtableColumnFamily.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.ignoreUnspecifiedColumnFamilies = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.readRowkeyAsString = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.outputColumnFamiliesAsJson = BoolValue.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigtableOptions {
    return {
      columnFamilies: globalThis.Array.isArray(object?.columnFamilies)
        ? object.columnFamilies.map((e: any) => BigtableColumnFamily.fromJSON(e))
        : [],
      ignoreUnspecifiedColumnFamilies: isSet(object.ignoreUnspecifiedColumnFamilies)
        ? Boolean(object.ignoreUnspecifiedColumnFamilies)
        : undefined,
      readRowkeyAsString: isSet(object.readRowkeyAsString) ? Boolean(object.readRowkeyAsString) : undefined,
      outputColumnFamiliesAsJson: isSet(object.outputColumnFamiliesAsJson)
        ? Boolean(object.outputColumnFamiliesAsJson)
        : undefined,
    };
  },

  toJSON(message: BigtableOptions): unknown {
    const obj: any = {};
    if (message.columnFamilies?.length) {
      obj.columnFamilies = message.columnFamilies.map((e) => BigtableColumnFamily.toJSON(e));
    }
    if (message.ignoreUnspecifiedColumnFamilies !== undefined) {
      obj.ignoreUnspecifiedColumnFamilies = message.ignoreUnspecifiedColumnFamilies;
    }
    if (message.readRowkeyAsString !== undefined) {
      obj.readRowkeyAsString = message.readRowkeyAsString;
    }
    if (message.outputColumnFamiliesAsJson !== undefined) {
      obj.outputColumnFamiliesAsJson = message.outputColumnFamiliesAsJson;
    }
    return obj;
  },

  create(base?: DeepPartial<BigtableOptions>): BigtableOptions {
    return BigtableOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigtableOptions>): BigtableOptions {
    const message = createBaseBigtableOptions();
    message.columnFamilies = object.columnFamilies?.map((e) => BigtableColumnFamily.fromPartial(e)) || [];
    message.ignoreUnspecifiedColumnFamilies = object.ignoreUnspecifiedColumnFamilies ?? undefined;
    message.readRowkeyAsString = object.readRowkeyAsString ?? undefined;
    message.outputColumnFamiliesAsJson = object.outputColumnFamiliesAsJson ?? undefined;
    return message;
  },
};

function createBaseGoogleSheetsOptions(): GoogleSheetsOptions {
  return { skipLeadingRows: undefined, range: "" };
}

export const GoogleSheetsOptions: MessageFns<GoogleSheetsOptions> = {
  encode(message: GoogleSheetsOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.skipLeadingRows !== undefined) {
      Int64Value.encode({ value: message.skipLeadingRows! }, writer.uint32(10).fork()).join();
    }
    if (message.range !== "") {
      writer.uint32(18).string(message.range);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GoogleSheetsOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGoogleSheetsOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.skipLeadingRows = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.range = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GoogleSheetsOptions {
    return {
      skipLeadingRows: isSet(object.skipLeadingRows) ? Long.fromValue(object.skipLeadingRows) : undefined,
      range: isSet(object.range) ? globalThis.String(object.range) : "",
    };
  },

  toJSON(message: GoogleSheetsOptions): unknown {
    const obj: any = {};
    if (message.skipLeadingRows !== undefined) {
      obj.skipLeadingRows = message.skipLeadingRows;
    }
    if (message.range !== "") {
      obj.range = message.range;
    }
    return obj;
  },

  create(base?: DeepPartial<GoogleSheetsOptions>): GoogleSheetsOptions {
    return GoogleSheetsOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GoogleSheetsOptions>): GoogleSheetsOptions {
    const message = createBaseGoogleSheetsOptions();
    message.skipLeadingRows = (object.skipLeadingRows !== undefined && object.skipLeadingRows !== null)
      ? Long.fromValue(object.skipLeadingRows)
      : undefined;
    message.range = object.range ?? "";
    return message;
  },
};

function createBaseExternalDataConfiguration(): ExternalDataConfiguration {
  return {
    sourceUris: [],
    fileSetSpecType: 0,
    schema: undefined,
    sourceFormat: "",
    maxBadRecords: undefined,
    autodetect: undefined,
    ignoreUnknownValues: undefined,
    compression: "",
    csvOptions: undefined,
    jsonOptions: undefined,
    bigtableOptions: undefined,
    googleSheetsOptions: undefined,
    hivePartitioningOptions: undefined,
    connectionId: "",
    decimalTargetTypes: [],
    avroOptions: undefined,
    jsonExtension: 0,
    parquetOptions: undefined,
    objectMetadata: undefined,
    referenceFileSchemaUri: undefined,
    metadataCacheMode: 0,
  };
}

export const ExternalDataConfiguration: MessageFns<ExternalDataConfiguration> = {
  encode(message: ExternalDataConfiguration, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.sourceUris) {
      writer.uint32(10).string(v!);
    }
    if (message.fileSetSpecType !== 0) {
      writer.uint32(200).int32(message.fileSetSpecType);
    }
    if (message.schema !== undefined) {
      TableSchema.encode(message.schema, writer.uint32(18).fork()).join();
    }
    if (message.sourceFormat !== "") {
      writer.uint32(26).string(message.sourceFormat);
    }
    if (message.maxBadRecords !== undefined) {
      Int32Value.encode({ value: message.maxBadRecords! }, writer.uint32(34).fork()).join();
    }
    if (message.autodetect !== undefined) {
      BoolValue.encode({ value: message.autodetect! }, writer.uint32(42).fork()).join();
    }
    if (message.ignoreUnknownValues !== undefined) {
      BoolValue.encode({ value: message.ignoreUnknownValues! }, writer.uint32(50).fork()).join();
    }
    if (message.compression !== "") {
      writer.uint32(58).string(message.compression);
    }
    if (message.csvOptions !== undefined) {
      CsvOptions.encode(message.csvOptions, writer.uint32(66).fork()).join();
    }
    if (message.jsonOptions !== undefined) {
      JsonOptions.encode(message.jsonOptions, writer.uint32(210).fork()).join();
    }
    if (message.bigtableOptions !== undefined) {
      BigtableOptions.encode(message.bigtableOptions, writer.uint32(74).fork()).join();
    }
    if (message.googleSheetsOptions !== undefined) {
      GoogleSheetsOptions.encode(message.googleSheetsOptions, writer.uint32(82).fork()).join();
    }
    if (message.hivePartitioningOptions !== undefined) {
      HivePartitioningOptions.encode(message.hivePartitioningOptions, writer.uint32(106).fork()).join();
    }
    if (message.connectionId !== "") {
      writer.uint32(114).string(message.connectionId);
    }
    writer.uint32(130).fork();
    for (const v of message.decimalTargetTypes) {
      writer.int32(v);
    }
    writer.join();
    if (message.avroOptions !== undefined) {
      AvroOptions.encode(message.avroOptions, writer.uint32(138).fork()).join();
    }
    if (message.jsonExtension !== 0) {
      writer.uint32(144).int32(message.jsonExtension);
    }
    if (message.parquetOptions !== undefined) {
      ParquetOptions.encode(message.parquetOptions, writer.uint32(154).fork()).join();
    }
    if (message.objectMetadata !== undefined) {
      writer.uint32(176).int32(message.objectMetadata);
    }
    if (message.referenceFileSchemaUri !== undefined) {
      StringValue.encode({ value: message.referenceFileSchemaUri! }, writer.uint32(186).fork()).join();
    }
    if (message.metadataCacheMode !== 0) {
      writer.uint32(192).int32(message.metadataCacheMode);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExternalDataConfiguration {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExternalDataConfiguration();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.sourceUris.push(reader.string());
          continue;
        case 25:
          if (tag !== 200) {
            break;
          }

          message.fileSetSpecType = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.schema = TableSchema.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.sourceFormat = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.maxBadRecords = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.autodetect = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.ignoreUnknownValues = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.compression = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.csvOptions = CsvOptions.decode(reader, reader.uint32());
          continue;
        case 26:
          if (tag !== 210) {
            break;
          }

          message.jsonOptions = JsonOptions.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.bigtableOptions = BigtableOptions.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.googleSheetsOptions = GoogleSheetsOptions.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.hivePartitioningOptions = HivePartitioningOptions.decode(reader, reader.uint32());
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.connectionId = reader.string();
          continue;
        case 16:
          if (tag === 128) {
            message.decimalTargetTypes.push(reader.int32() as any);

            continue;
          }

          if (tag === 130) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.decimalTargetTypes.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.avroOptions = AvroOptions.decode(reader, reader.uint32());
          continue;
        case 18:
          if (tag !== 144) {
            break;
          }

          message.jsonExtension = reader.int32() as any;
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.parquetOptions = ParquetOptions.decode(reader, reader.uint32());
          continue;
        case 22:
          if (tag !== 176) {
            break;
          }

          message.objectMetadata = reader.int32() as any;
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.referenceFileSchemaUri = StringValue.decode(reader, reader.uint32()).value;
          continue;
        case 24:
          if (tag !== 192) {
            break;
          }

          message.metadataCacheMode = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExternalDataConfiguration {
    return {
      sourceUris: globalThis.Array.isArray(object?.sourceUris)
        ? object.sourceUris.map((e: any) => globalThis.String(e))
        : [],
      fileSetSpecType: isSet(object.fileSetSpecType) ? fileSetSpecTypeFromJSON(object.fileSetSpecType) : 0,
      schema: isSet(object.schema) ? TableSchema.fromJSON(object.schema) : undefined,
      sourceFormat: isSet(object.sourceFormat) ? globalThis.String(object.sourceFormat) : "",
      maxBadRecords: isSet(object.maxBadRecords) ? Number(object.maxBadRecords) : undefined,
      autodetect: isSet(object.autodetect) ? Boolean(object.autodetect) : undefined,
      ignoreUnknownValues: isSet(object.ignoreUnknownValues) ? Boolean(object.ignoreUnknownValues) : undefined,
      compression: isSet(object.compression) ? globalThis.String(object.compression) : "",
      csvOptions: isSet(object.csvOptions) ? CsvOptions.fromJSON(object.csvOptions) : undefined,
      jsonOptions: isSet(object.jsonOptions) ? JsonOptions.fromJSON(object.jsonOptions) : undefined,
      bigtableOptions: isSet(object.bigtableOptions) ? BigtableOptions.fromJSON(object.bigtableOptions) : undefined,
      googleSheetsOptions: isSet(object.googleSheetsOptions)
        ? GoogleSheetsOptions.fromJSON(object.googleSheetsOptions)
        : undefined,
      hivePartitioningOptions: isSet(object.hivePartitioningOptions)
        ? HivePartitioningOptions.fromJSON(object.hivePartitioningOptions)
        : undefined,
      connectionId: isSet(object.connectionId) ? globalThis.String(object.connectionId) : "",
      decimalTargetTypes: globalThis.Array.isArray(object?.decimalTargetTypes)
        ? object.decimalTargetTypes.map((e: any) => decimalTargetTypeFromJSON(e))
        : [],
      avroOptions: isSet(object.avroOptions) ? AvroOptions.fromJSON(object.avroOptions) : undefined,
      jsonExtension: isSet(object.jsonExtension) ? jsonExtensionFromJSON(object.jsonExtension) : 0,
      parquetOptions: isSet(object.parquetOptions) ? ParquetOptions.fromJSON(object.parquetOptions) : undefined,
      objectMetadata: isSet(object.objectMetadata)
        ? externalDataConfiguration_ObjectMetadataFromJSON(object.objectMetadata)
        : undefined,
      referenceFileSchemaUri: isSet(object.referenceFileSchemaUri) ? String(object.referenceFileSchemaUri) : undefined,
      metadataCacheMode: isSet(object.metadataCacheMode)
        ? externalDataConfiguration_MetadataCacheModeFromJSON(object.metadataCacheMode)
        : 0,
    };
  },

  toJSON(message: ExternalDataConfiguration): unknown {
    const obj: any = {};
    if (message.sourceUris?.length) {
      obj.sourceUris = message.sourceUris;
    }
    if (message.fileSetSpecType !== 0) {
      obj.fileSetSpecType = fileSetSpecTypeToJSON(message.fileSetSpecType);
    }
    if (message.schema !== undefined) {
      obj.schema = TableSchema.toJSON(message.schema);
    }
    if (message.sourceFormat !== "") {
      obj.sourceFormat = message.sourceFormat;
    }
    if (message.maxBadRecords !== undefined) {
      obj.maxBadRecords = message.maxBadRecords;
    }
    if (message.autodetect !== undefined) {
      obj.autodetect = message.autodetect;
    }
    if (message.ignoreUnknownValues !== undefined) {
      obj.ignoreUnknownValues = message.ignoreUnknownValues;
    }
    if (message.compression !== "") {
      obj.compression = message.compression;
    }
    if (message.csvOptions !== undefined) {
      obj.csvOptions = CsvOptions.toJSON(message.csvOptions);
    }
    if (message.jsonOptions !== undefined) {
      obj.jsonOptions = JsonOptions.toJSON(message.jsonOptions);
    }
    if (message.bigtableOptions !== undefined) {
      obj.bigtableOptions = BigtableOptions.toJSON(message.bigtableOptions);
    }
    if (message.googleSheetsOptions !== undefined) {
      obj.googleSheetsOptions = GoogleSheetsOptions.toJSON(message.googleSheetsOptions);
    }
    if (message.hivePartitioningOptions !== undefined) {
      obj.hivePartitioningOptions = HivePartitioningOptions.toJSON(message.hivePartitioningOptions);
    }
    if (message.connectionId !== "") {
      obj.connectionId = message.connectionId;
    }
    if (message.decimalTargetTypes?.length) {
      obj.decimalTargetTypes = message.decimalTargetTypes.map((e) => decimalTargetTypeToJSON(e));
    }
    if (message.avroOptions !== undefined) {
      obj.avroOptions = AvroOptions.toJSON(message.avroOptions);
    }
    if (message.jsonExtension !== 0) {
      obj.jsonExtension = jsonExtensionToJSON(message.jsonExtension);
    }
    if (message.parquetOptions !== undefined) {
      obj.parquetOptions = ParquetOptions.toJSON(message.parquetOptions);
    }
    if (message.objectMetadata !== undefined) {
      obj.objectMetadata = externalDataConfiguration_ObjectMetadataToJSON(message.objectMetadata);
    }
    if (message.referenceFileSchemaUri !== undefined) {
      obj.referenceFileSchemaUri = message.referenceFileSchemaUri;
    }
    if (message.metadataCacheMode !== 0) {
      obj.metadataCacheMode = externalDataConfiguration_MetadataCacheModeToJSON(message.metadataCacheMode);
    }
    return obj;
  },

  create(base?: DeepPartial<ExternalDataConfiguration>): ExternalDataConfiguration {
    return ExternalDataConfiguration.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExternalDataConfiguration>): ExternalDataConfiguration {
    const message = createBaseExternalDataConfiguration();
    message.sourceUris = object.sourceUris?.map((e) => e) || [];
    message.fileSetSpecType = object.fileSetSpecType ?? 0;
    message.schema = (object.schema !== undefined && object.schema !== null)
      ? TableSchema.fromPartial(object.schema)
      : undefined;
    message.sourceFormat = object.sourceFormat ?? "";
    message.maxBadRecords = object.maxBadRecords ?? undefined;
    message.autodetect = object.autodetect ?? undefined;
    message.ignoreUnknownValues = object.ignoreUnknownValues ?? undefined;
    message.compression = object.compression ?? "";
    message.csvOptions = (object.csvOptions !== undefined && object.csvOptions !== null)
      ? CsvOptions.fromPartial(object.csvOptions)
      : undefined;
    message.jsonOptions = (object.jsonOptions !== undefined && object.jsonOptions !== null)
      ? JsonOptions.fromPartial(object.jsonOptions)
      : undefined;
    message.bigtableOptions = (object.bigtableOptions !== undefined && object.bigtableOptions !== null)
      ? BigtableOptions.fromPartial(object.bigtableOptions)
      : undefined;
    message.googleSheetsOptions = (object.googleSheetsOptions !== undefined && object.googleSheetsOptions !== null)
      ? GoogleSheetsOptions.fromPartial(object.googleSheetsOptions)
      : undefined;
    message.hivePartitioningOptions =
      (object.hivePartitioningOptions !== undefined && object.hivePartitioningOptions !== null)
        ? HivePartitioningOptions.fromPartial(object.hivePartitioningOptions)
        : undefined;
    message.connectionId = object.connectionId ?? "";
    message.decimalTargetTypes = object.decimalTargetTypes?.map((e) => e) || [];
    message.avroOptions = (object.avroOptions !== undefined && object.avroOptions !== null)
      ? AvroOptions.fromPartial(object.avroOptions)
      : undefined;
    message.jsonExtension = object.jsonExtension ?? 0;
    message.parquetOptions = (object.parquetOptions !== undefined && object.parquetOptions !== null)
      ? ParquetOptions.fromPartial(object.parquetOptions)
      : undefined;
    message.objectMetadata = object.objectMetadata ?? undefined;
    message.referenceFileSchemaUri = object.referenceFileSchemaUri ?? undefined;
    message.metadataCacheMode = object.metadataCacheMode ?? 0;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
