// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/bigquery/v2/job_stats.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Duration } from "../../../protobuf/duration.js";
import { Struct } from "../../../protobuf/struct.js";
import { BoolValue, DoubleValue, Int32Value, Int64Value } from "../../../protobuf/wrappers.js";
import { DatasetReference } from "./dataset_reference.js";
import {
  Model_HparamTuningTrial,
  Model_ModelType,
  model_ModelTypeFromJSON,
  model_ModelTypeToJSON,
  Model_TrainingRun_IterationResult,
} from "./model.js";
import { QueryParameter } from "./query_parameter.js";
import { RoutineReference } from "./routine_reference.js";
import { RowAccessPolicyReference } from "./row_access_policy_reference.js";
import { SessionInfo } from "./session_info.js";
import { TableReference } from "./table_reference.js";
import { TableSchema } from "./table_schema.js";

export const protobufPackage = "google.cloud.bigquery.v2";

/**
 * The type of editions.
 * Different features and behaviors are provided to different editions
 * Capacity commitments and reservations are linked to editions.
 */
export enum ReservationEdition {
  /** RESERVATION_EDITION_UNSPECIFIED - Default value, which will be treated as ENTERPRISE. */
  RESERVATION_EDITION_UNSPECIFIED = 0,
  /** STANDARD - Standard edition. */
  STANDARD = 1,
  /** ENTERPRISE - Enterprise edition. */
  ENTERPRISE = 2,
  /** ENTERPRISE_PLUS - Enterprise plus edition. */
  ENTERPRISE_PLUS = 3,
  UNRECOGNIZED = -1,
}

export function reservationEditionFromJSON(object: any): ReservationEdition {
  switch (object) {
    case 0:
    case "RESERVATION_EDITION_UNSPECIFIED":
      return ReservationEdition.RESERVATION_EDITION_UNSPECIFIED;
    case 1:
    case "STANDARD":
      return ReservationEdition.STANDARD;
    case 2:
    case "ENTERPRISE":
      return ReservationEdition.ENTERPRISE;
    case 3:
    case "ENTERPRISE_PLUS":
      return ReservationEdition.ENTERPRISE_PLUS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ReservationEdition.UNRECOGNIZED;
  }
}

export function reservationEditionToJSON(object: ReservationEdition): string {
  switch (object) {
    case ReservationEdition.RESERVATION_EDITION_UNSPECIFIED:
      return "RESERVATION_EDITION_UNSPECIFIED";
    case ReservationEdition.STANDARD:
      return "STANDARD";
    case ReservationEdition.ENTERPRISE:
      return "ENTERPRISE";
    case ReservationEdition.ENTERPRISE_PLUS:
      return "ENTERPRISE_PLUS";
    case ReservationEdition.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** An operation within a stage. */
export interface ExplainQueryStep {
  /** Machine-readable operation type. */
  kind: string;
  /** Human-readable description of the step(s). */
  substeps: string[];
}

/** A single stage of query execution. */
export interface ExplainQueryStage {
  /** Human-readable name for the stage. */
  name: string;
  /** Unique ID for the stage within the plan. */
  id:
    | Long
    | undefined;
  /** Stage start time represented as milliseconds since the epoch. */
  startMs: Long;
  /** Stage end time represented as milliseconds since the epoch. */
  endMs: Long;
  /** IDs for stages that are inputs to this stage. */
  inputStages: Long[];
  /**
   * Relative amount of time the average shard spent waiting to be
   * scheduled.
   */
  waitRatioAvg:
    | number
    | undefined;
  /** Milliseconds the average shard spent waiting to be scheduled. */
  waitMsAvg:
    | Long
    | undefined;
  /**
   * Relative amount of time the slowest shard spent waiting to be
   * scheduled.
   */
  waitRatioMax:
    | number
    | undefined;
  /** Milliseconds the slowest shard spent waiting to be scheduled. */
  waitMsMax:
    | Long
    | undefined;
  /** Relative amount of time the average shard spent reading input. */
  readRatioAvg:
    | number
    | undefined;
  /** Milliseconds the average shard spent reading input. */
  readMsAvg:
    | Long
    | undefined;
  /** Relative amount of time the slowest shard spent reading input. */
  readRatioMax:
    | number
    | undefined;
  /** Milliseconds the slowest shard spent reading input. */
  readMsMax:
    | Long
    | undefined;
  /** Relative amount of time the average shard spent on CPU-bound tasks. */
  computeRatioAvg:
    | number
    | undefined;
  /** Milliseconds the average shard spent on CPU-bound tasks. */
  computeMsAvg:
    | Long
    | undefined;
  /** Relative amount of time the slowest shard spent on CPU-bound tasks. */
  computeRatioMax:
    | number
    | undefined;
  /** Milliseconds the slowest shard spent on CPU-bound tasks. */
  computeMsMax:
    | Long
    | undefined;
  /** Relative amount of time the average shard spent on writing output. */
  writeRatioAvg:
    | number
    | undefined;
  /** Milliseconds the average shard spent on writing output. */
  writeMsAvg:
    | Long
    | undefined;
  /** Relative amount of time the slowest shard spent on writing output. */
  writeRatioMax:
    | number
    | undefined;
  /** Milliseconds the slowest shard spent on writing output. */
  writeMsMax:
    | Long
    | undefined;
  /** Total number of bytes written to shuffle. */
  shuffleOutputBytes:
    | Long
    | undefined;
  /** Total number of bytes written to shuffle and spilled to disk. */
  shuffleOutputBytesSpilled:
    | Long
    | undefined;
  /** Number of records read into the stage. */
  recordsRead:
    | Long
    | undefined;
  /** Number of records written by the stage. */
  recordsWritten:
    | Long
    | undefined;
  /** Number of parallel input segments to be processed */
  parallelInputs:
    | Long
    | undefined;
  /** Number of parallel input segments completed. */
  completedParallelInputs:
    | Long
    | undefined;
  /** Current status for this stage. */
  status: string;
  /**
   * List of operations within the stage in dependency order (approximately
   * chronological).
   */
  steps: ExplainQueryStep[];
  /** Slot-milliseconds used by the stage. */
  slotMs:
    | Long
    | undefined;
  /** Output only. Compute mode for this stage. */
  computeMode: ExplainQueryStage_ComputeMode;
}

/** Indicates the type of compute mode. */
export enum ExplainQueryStage_ComputeMode {
  /** COMPUTE_MODE_UNSPECIFIED - ComputeMode type not specified. */
  COMPUTE_MODE_UNSPECIFIED = 0,
  /** BIGQUERY - This stage was processed using BigQuery slots. */
  BIGQUERY = 1,
  /** BI_ENGINE - This stage was processed using BI Engine compute. */
  BI_ENGINE = 2,
  UNRECOGNIZED = -1,
}

export function explainQueryStage_ComputeModeFromJSON(object: any): ExplainQueryStage_ComputeMode {
  switch (object) {
    case 0:
    case "COMPUTE_MODE_UNSPECIFIED":
      return ExplainQueryStage_ComputeMode.COMPUTE_MODE_UNSPECIFIED;
    case 1:
    case "BIGQUERY":
      return ExplainQueryStage_ComputeMode.BIGQUERY;
    case 2:
    case "BI_ENGINE":
      return ExplainQueryStage_ComputeMode.BI_ENGINE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ExplainQueryStage_ComputeMode.UNRECOGNIZED;
  }
}

export function explainQueryStage_ComputeModeToJSON(object: ExplainQueryStage_ComputeMode): string {
  switch (object) {
    case ExplainQueryStage_ComputeMode.COMPUTE_MODE_UNSPECIFIED:
      return "COMPUTE_MODE_UNSPECIFIED";
    case ExplainQueryStage_ComputeMode.BIGQUERY:
      return "BIGQUERY";
    case ExplainQueryStage_ComputeMode.BI_ENGINE:
      return "BI_ENGINE";
    case ExplainQueryStage_ComputeMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Summary of the state of query execution at a given time. */
export interface QueryTimelineSample {
  /** Milliseconds elapsed since the start of query execution. */
  elapsedMs:
    | Long
    | undefined;
  /** Cumulative slot-ms consumed by the query. */
  totalSlotMs:
    | Long
    | undefined;
  /**
   * Total units of work remaining for the query. This number can be revised
   * (increased or decreased) while the query is running.
   */
  pendingUnits:
    | Long
    | undefined;
  /** Total parallel units of work completed by this query. */
  completedUnits:
    | Long
    | undefined;
  /**
   * Total number of active workers. This does not correspond directly to
   * slot usage. This is the largest value observed since the last sample.
   */
  activeUnits:
    | Long
    | undefined;
  /**
   * Units of work that can be scheduled immediately. Providing additional slots
   * for these units of work will accelerate the query, if no other query in
   * the reservation needs additional slots.
   */
  estimatedRunnableUnits: Long | undefined;
}

/**
 * The external service cost is a portion of the total cost, these costs are not
 * additive with total_bytes_billed. Moreover, this field only track external
 * service costs that will show up as BigQuery costs (e.g. training BigQuery
 * ML job with google cloud CAIP or Automl Tables services), not other costs
 * which may be accrued by running the query (e.g. reading from Bigtable or
 * Cloud Storage). The external service costs with different billing sku (e.g.
 * CAIP job is charged based on VM usage) are converted to BigQuery
 * billed_bytes and slot_ms with equivalent amount of US dollars. Services may
 * not directly correlate to these metrics, but these are the equivalents for
 * billing purposes.
 * Output only.
 */
export interface ExternalServiceCost {
  /** External service name. */
  externalService: string;
  /** External service cost in terms of bigquery bytes processed. */
  bytesProcessed:
    | Long
    | undefined;
  /** External service cost in terms of bigquery bytes billed. */
  bytesBilled:
    | Long
    | undefined;
  /** External service cost in terms of bigquery slot milliseconds. */
  slotMs:
    | Long
    | undefined;
  /**
   * Non-preemptable reserved slots used for external job.
   * For example, reserved slots for Cloua AI Platform job are the VM usages
   * converted to BigQuery slot with equivalent mount of price.
   */
  reservedSlotCount: Long;
}

/**
 * Statistics for the EXPORT DATA statement as part of Query Job. EXTRACT
 * JOB statistics are populated in JobStatistics4.
 */
export interface ExportDataStatistics {
  /**
   * Number of destination files generated in case of EXPORT DATA
   * statement only.
   */
  fileCount:
    | Long
    | undefined;
  /**
   * [Alpha] Number of destination rows generated in case of EXPORT DATA
   * statement only.
   */
  rowCount: Long | undefined;
}

/** Reason why BI Engine didn't accelerate the query (or sub-query). */
export interface BiEngineReason {
  /**
   * Output only. High-level BI Engine reason for partial or disabled
   * acceleration
   */
  code: BiEngineReason_Code;
  /**
   * Output only. Free form human-readable reason for partial or disabled
   * acceleration.
   */
  message: string;
}

/** Indicates the high-level reason for no/partial acceleration */
export enum BiEngineReason_Code {
  /** CODE_UNSPECIFIED - BiEngineReason not specified. */
  CODE_UNSPECIFIED = 0,
  /** NO_RESERVATION - No reservation available for BI Engine acceleration. */
  NO_RESERVATION = 1,
  /** INSUFFICIENT_RESERVATION - Not enough memory available for BI Engine acceleration. */
  INSUFFICIENT_RESERVATION = 2,
  /** UNSUPPORTED_SQL_TEXT - This particular SQL text is not supported for acceleration by BI Engine. */
  UNSUPPORTED_SQL_TEXT = 4,
  /** INPUT_TOO_LARGE - Input too large for acceleration by BI Engine. */
  INPUT_TOO_LARGE = 5,
  /** OTHER_REASON - Catch-all code for all other cases for partial or disabled acceleration. */
  OTHER_REASON = 6,
  /** TABLE_EXCLUDED - One or more tables were not eligible for BI Engine acceleration. */
  TABLE_EXCLUDED = 7,
  UNRECOGNIZED = -1,
}

export function biEngineReason_CodeFromJSON(object: any): BiEngineReason_Code {
  switch (object) {
    case 0:
    case "CODE_UNSPECIFIED":
      return BiEngineReason_Code.CODE_UNSPECIFIED;
    case 1:
    case "NO_RESERVATION":
      return BiEngineReason_Code.NO_RESERVATION;
    case 2:
    case "INSUFFICIENT_RESERVATION":
      return BiEngineReason_Code.INSUFFICIENT_RESERVATION;
    case 4:
    case "UNSUPPORTED_SQL_TEXT":
      return BiEngineReason_Code.UNSUPPORTED_SQL_TEXT;
    case 5:
    case "INPUT_TOO_LARGE":
      return BiEngineReason_Code.INPUT_TOO_LARGE;
    case 6:
    case "OTHER_REASON":
      return BiEngineReason_Code.OTHER_REASON;
    case 7:
    case "TABLE_EXCLUDED":
      return BiEngineReason_Code.TABLE_EXCLUDED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BiEngineReason_Code.UNRECOGNIZED;
  }
}

export function biEngineReason_CodeToJSON(object: BiEngineReason_Code): string {
  switch (object) {
    case BiEngineReason_Code.CODE_UNSPECIFIED:
      return "CODE_UNSPECIFIED";
    case BiEngineReason_Code.NO_RESERVATION:
      return "NO_RESERVATION";
    case BiEngineReason_Code.INSUFFICIENT_RESERVATION:
      return "INSUFFICIENT_RESERVATION";
    case BiEngineReason_Code.UNSUPPORTED_SQL_TEXT:
      return "UNSUPPORTED_SQL_TEXT";
    case BiEngineReason_Code.INPUT_TOO_LARGE:
      return "INPUT_TOO_LARGE";
    case BiEngineReason_Code.OTHER_REASON:
      return "OTHER_REASON";
    case BiEngineReason_Code.TABLE_EXCLUDED:
      return "TABLE_EXCLUDED";
    case BiEngineReason_Code.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Statistics for a BI Engine specific query.
 * Populated as part of JobStatistics2
 */
export interface BiEngineStatistics {
  /**
   * Output only. Specifies which mode of BI Engine acceleration was performed
   * (if any).
   */
  biEngineMode: BiEngineStatistics_BiEngineMode;
  /**
   * Output only. Specifies which mode of BI Engine acceleration was performed
   * (if any).
   */
  accelerationMode: BiEngineStatistics_BiEngineAccelerationMode;
  /**
   * In case of DISABLED or PARTIAL bi_engine_mode, these contain the
   * explanatory reasons as to why BI Engine could not accelerate.
   * In case the full query was accelerated, this field is not populated.
   */
  biEngineReasons: BiEngineReason[];
}

/** Indicates the type of BI Engine acceleration. */
export enum BiEngineStatistics_BiEngineMode {
  /** ACCELERATION_MODE_UNSPECIFIED - BiEngineMode type not specified. */
  ACCELERATION_MODE_UNSPECIFIED = 0,
  /**
   * DISABLED - BI Engine disabled the acceleration. bi_engine_reasons
   * specifies a more detailed reason.
   */
  DISABLED = 1,
  /**
   * PARTIAL - Part of the query was accelerated using BI Engine.
   * See bi_engine_reasons for why parts of the query were not
   * accelerated.
   */
  PARTIAL = 2,
  /** FULL - All of the query was accelerated using BI Engine. */
  FULL = 3,
  UNRECOGNIZED = -1,
}

export function biEngineStatistics_BiEngineModeFromJSON(object: any): BiEngineStatistics_BiEngineMode {
  switch (object) {
    case 0:
    case "ACCELERATION_MODE_UNSPECIFIED":
      return BiEngineStatistics_BiEngineMode.ACCELERATION_MODE_UNSPECIFIED;
    case 1:
    case "DISABLED":
      return BiEngineStatistics_BiEngineMode.DISABLED;
    case 2:
    case "PARTIAL":
      return BiEngineStatistics_BiEngineMode.PARTIAL;
    case 3:
    case "FULL":
      return BiEngineStatistics_BiEngineMode.FULL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BiEngineStatistics_BiEngineMode.UNRECOGNIZED;
  }
}

export function biEngineStatistics_BiEngineModeToJSON(object: BiEngineStatistics_BiEngineMode): string {
  switch (object) {
    case BiEngineStatistics_BiEngineMode.ACCELERATION_MODE_UNSPECIFIED:
      return "ACCELERATION_MODE_UNSPECIFIED";
    case BiEngineStatistics_BiEngineMode.DISABLED:
      return "DISABLED";
    case BiEngineStatistics_BiEngineMode.PARTIAL:
      return "PARTIAL";
    case BiEngineStatistics_BiEngineMode.FULL:
      return "FULL";
    case BiEngineStatistics_BiEngineMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Indicates the type of BI Engine acceleration. */
export enum BiEngineStatistics_BiEngineAccelerationMode {
  /** BI_ENGINE_ACCELERATION_MODE_UNSPECIFIED - BiEngineMode type not specified. */
  BI_ENGINE_ACCELERATION_MODE_UNSPECIFIED = 0,
  /**
   * BI_ENGINE_DISABLED - BI Engine acceleration was attempted but disabled. bi_engine_reasons
   * specifies a more detailed reason.
   */
  BI_ENGINE_DISABLED = 1,
  /**
   * PARTIAL_INPUT - Some inputs were accelerated using BI Engine.
   * See bi_engine_reasons for why parts of the query were not
   * accelerated.
   */
  PARTIAL_INPUT = 2,
  /** FULL_INPUT - All of the query inputs were accelerated using BI Engine. */
  FULL_INPUT = 3,
  /** FULL_QUERY - All of the query was accelerated using BI Engine. */
  FULL_QUERY = 4,
  UNRECOGNIZED = -1,
}

export function biEngineStatistics_BiEngineAccelerationModeFromJSON(
  object: any,
): BiEngineStatistics_BiEngineAccelerationMode {
  switch (object) {
    case 0:
    case "BI_ENGINE_ACCELERATION_MODE_UNSPECIFIED":
      return BiEngineStatistics_BiEngineAccelerationMode.BI_ENGINE_ACCELERATION_MODE_UNSPECIFIED;
    case 1:
    case "BI_ENGINE_DISABLED":
      return BiEngineStatistics_BiEngineAccelerationMode.BI_ENGINE_DISABLED;
    case 2:
    case "PARTIAL_INPUT":
      return BiEngineStatistics_BiEngineAccelerationMode.PARTIAL_INPUT;
    case 3:
    case "FULL_INPUT":
      return BiEngineStatistics_BiEngineAccelerationMode.FULL_INPUT;
    case 4:
    case "FULL_QUERY":
      return BiEngineStatistics_BiEngineAccelerationMode.FULL_QUERY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BiEngineStatistics_BiEngineAccelerationMode.UNRECOGNIZED;
  }
}

export function biEngineStatistics_BiEngineAccelerationModeToJSON(
  object: BiEngineStatistics_BiEngineAccelerationMode,
): string {
  switch (object) {
    case BiEngineStatistics_BiEngineAccelerationMode.BI_ENGINE_ACCELERATION_MODE_UNSPECIFIED:
      return "BI_ENGINE_ACCELERATION_MODE_UNSPECIFIED";
    case BiEngineStatistics_BiEngineAccelerationMode.BI_ENGINE_DISABLED:
      return "BI_ENGINE_DISABLED";
    case BiEngineStatistics_BiEngineAccelerationMode.PARTIAL_INPUT:
      return "PARTIAL_INPUT";
    case BiEngineStatistics_BiEngineAccelerationMode.FULL_INPUT:
      return "FULL_INPUT";
    case BiEngineStatistics_BiEngineAccelerationMode.FULL_QUERY:
      return "FULL_QUERY";
    case BiEngineStatistics_BiEngineAccelerationMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Reason about why no search index was used in the search query (or
 * sub-query).
 */
export interface IndexUnusedReason {
  /**
   * Specifies the high-level reason for the scenario when no search index was
   * used.
   */
  code?:
    | IndexUnusedReason_Code
    | undefined;
  /**
   * Free form human-readable reason for the scenario when no search index was
   * used.
   */
  message?:
    | string
    | undefined;
  /**
   * Specifies the base table involved in the reason that no search index was
   * used.
   */
  baseTable?:
    | TableReference
    | undefined;
  /** Specifies the name of the unused search index, if available. */
  indexName?: string | undefined;
}

/**
 * Indicates the high-level reason for the scenario when no search index was
 * used.
 */
export enum IndexUnusedReason_Code {
  /** CODE_UNSPECIFIED - Code not specified. */
  CODE_UNSPECIFIED = 0,
  /** INDEX_CONFIG_NOT_AVAILABLE - Indicates the search index configuration has not been created. */
  INDEX_CONFIG_NOT_AVAILABLE = 1,
  /** PENDING_INDEX_CREATION - Indicates the search index creation has not been completed. */
  PENDING_INDEX_CREATION = 2,
  /**
   * BASE_TABLE_TRUNCATED - Indicates the base table has been truncated (rows have been removed
   * from table with TRUNCATE TABLE statement) since the last time the search
   * index was refreshed.
   */
  BASE_TABLE_TRUNCATED = 3,
  /**
   * INDEX_CONFIG_MODIFIED - Indicates the search index configuration has been changed since the last
   * time the search index was refreshed.
   */
  INDEX_CONFIG_MODIFIED = 4,
  /**
   * TIME_TRAVEL_QUERY - Indicates the search query accesses data at a timestamp before the last
   * time the search index was refreshed.
   */
  TIME_TRAVEL_QUERY = 5,
  /**
   * NO_PRUNING_POWER - Indicates the usage of search index will not contribute to any pruning
   * improvement for the search function, e.g. when the search predicate is in
   * a disjunction with other non-search predicates.
   */
  NO_PRUNING_POWER = 6,
  /**
   * UNINDEXED_SEARCH_FIELDS - Indicates the search index does not cover all fields in the search
   * function.
   */
  UNINDEXED_SEARCH_FIELDS = 7,
  /**
   * UNSUPPORTED_SEARCH_PATTERN - Indicates the search index does not support the given search query
   * pattern.
   */
  UNSUPPORTED_SEARCH_PATTERN = 8,
  /** OPTIMIZED_WITH_MATERIALIZED_VIEW - Indicates the query has been optimized by using a materialized view. */
  OPTIMIZED_WITH_MATERIALIZED_VIEW = 9,
  /**
   * SECURED_BY_DATA_MASKING - Indicates the query has been secured by data masking, and thus search
   * indexes are not applicable.
   */
  SECURED_BY_DATA_MASKING = 11,
  /**
   * MISMATCHED_TEXT_ANALYZER - Indicates that the search index and the search function call do not
   * have the same text analyzer.
   */
  MISMATCHED_TEXT_ANALYZER = 12,
  /**
   * BASE_TABLE_TOO_SMALL - Indicates the base table is too small (below a certain threshold).
   * The index does not provide noticeable search performance gains
   * when the base table is too small.
   */
  BASE_TABLE_TOO_SMALL = 13,
  /**
   * BASE_TABLE_TOO_LARGE - Indicates that the total size of indexed base tables in your organization
   * exceeds your region's limit and the index is not used in the query. To
   * index larger base tables, you can
   * <a
   * href="https://cloud.google.com/bigquery/docs/search-index#use_your_own_reservation">use
   * your own reservation</a> for index-management jobs.
   */
  BASE_TABLE_TOO_LARGE = 14,
  /**
   * ESTIMATED_PERFORMANCE_GAIN_TOO_LOW - Indicates that the estimated performance gain from using the search index
   * is too low for the given search query.
   */
  ESTIMATED_PERFORMANCE_GAIN_TOO_LOW = 15,
  /**
   * NOT_SUPPORTED_IN_STANDARD_EDITION - Indicates that search indexes can not be used for search query with
   * STANDARD edition.
   */
  NOT_SUPPORTED_IN_STANDARD_EDITION = 17,
  /**
   * INDEX_SUPPRESSED_BY_FUNCTION_OPTION - Indicates that an option in the search function that cannot make use of
   * the index has been selected.
   */
  INDEX_SUPPRESSED_BY_FUNCTION_OPTION = 18,
  /**
   * QUERY_CACHE_HIT - Indicates that the query was cached, and thus the search index was not
   * used.
   */
  QUERY_CACHE_HIT = 19,
  /** STALE_INDEX - The index cannot be used in the search query because it is stale. */
  STALE_INDEX = 20,
  /** INTERNAL_ERROR - Indicates an internal error that causes the search index to be unused. */
  INTERNAL_ERROR = 10,
  /**
   * OTHER_REASON - Indicates that the reason search indexes cannot be used in the query is
   * not covered by any of the other IndexUnusedReason options.
   */
  OTHER_REASON = 16,
  UNRECOGNIZED = -1,
}

export function indexUnusedReason_CodeFromJSON(object: any): IndexUnusedReason_Code {
  switch (object) {
    case 0:
    case "CODE_UNSPECIFIED":
      return IndexUnusedReason_Code.CODE_UNSPECIFIED;
    case 1:
    case "INDEX_CONFIG_NOT_AVAILABLE":
      return IndexUnusedReason_Code.INDEX_CONFIG_NOT_AVAILABLE;
    case 2:
    case "PENDING_INDEX_CREATION":
      return IndexUnusedReason_Code.PENDING_INDEX_CREATION;
    case 3:
    case "BASE_TABLE_TRUNCATED":
      return IndexUnusedReason_Code.BASE_TABLE_TRUNCATED;
    case 4:
    case "INDEX_CONFIG_MODIFIED":
      return IndexUnusedReason_Code.INDEX_CONFIG_MODIFIED;
    case 5:
    case "TIME_TRAVEL_QUERY":
      return IndexUnusedReason_Code.TIME_TRAVEL_QUERY;
    case 6:
    case "NO_PRUNING_POWER":
      return IndexUnusedReason_Code.NO_PRUNING_POWER;
    case 7:
    case "UNINDEXED_SEARCH_FIELDS":
      return IndexUnusedReason_Code.UNINDEXED_SEARCH_FIELDS;
    case 8:
    case "UNSUPPORTED_SEARCH_PATTERN":
      return IndexUnusedReason_Code.UNSUPPORTED_SEARCH_PATTERN;
    case 9:
    case "OPTIMIZED_WITH_MATERIALIZED_VIEW":
      return IndexUnusedReason_Code.OPTIMIZED_WITH_MATERIALIZED_VIEW;
    case 11:
    case "SECURED_BY_DATA_MASKING":
      return IndexUnusedReason_Code.SECURED_BY_DATA_MASKING;
    case 12:
    case "MISMATCHED_TEXT_ANALYZER":
      return IndexUnusedReason_Code.MISMATCHED_TEXT_ANALYZER;
    case 13:
    case "BASE_TABLE_TOO_SMALL":
      return IndexUnusedReason_Code.BASE_TABLE_TOO_SMALL;
    case 14:
    case "BASE_TABLE_TOO_LARGE":
      return IndexUnusedReason_Code.BASE_TABLE_TOO_LARGE;
    case 15:
    case "ESTIMATED_PERFORMANCE_GAIN_TOO_LOW":
      return IndexUnusedReason_Code.ESTIMATED_PERFORMANCE_GAIN_TOO_LOW;
    case 17:
    case "NOT_SUPPORTED_IN_STANDARD_EDITION":
      return IndexUnusedReason_Code.NOT_SUPPORTED_IN_STANDARD_EDITION;
    case 18:
    case "INDEX_SUPPRESSED_BY_FUNCTION_OPTION":
      return IndexUnusedReason_Code.INDEX_SUPPRESSED_BY_FUNCTION_OPTION;
    case 19:
    case "QUERY_CACHE_HIT":
      return IndexUnusedReason_Code.QUERY_CACHE_HIT;
    case 20:
    case "STALE_INDEX":
      return IndexUnusedReason_Code.STALE_INDEX;
    case 10:
    case "INTERNAL_ERROR":
      return IndexUnusedReason_Code.INTERNAL_ERROR;
    case 16:
    case "OTHER_REASON":
      return IndexUnusedReason_Code.OTHER_REASON;
    case -1:
    case "UNRECOGNIZED":
    default:
      return IndexUnusedReason_Code.UNRECOGNIZED;
  }
}

export function indexUnusedReason_CodeToJSON(object: IndexUnusedReason_Code): string {
  switch (object) {
    case IndexUnusedReason_Code.CODE_UNSPECIFIED:
      return "CODE_UNSPECIFIED";
    case IndexUnusedReason_Code.INDEX_CONFIG_NOT_AVAILABLE:
      return "INDEX_CONFIG_NOT_AVAILABLE";
    case IndexUnusedReason_Code.PENDING_INDEX_CREATION:
      return "PENDING_INDEX_CREATION";
    case IndexUnusedReason_Code.BASE_TABLE_TRUNCATED:
      return "BASE_TABLE_TRUNCATED";
    case IndexUnusedReason_Code.INDEX_CONFIG_MODIFIED:
      return "INDEX_CONFIG_MODIFIED";
    case IndexUnusedReason_Code.TIME_TRAVEL_QUERY:
      return "TIME_TRAVEL_QUERY";
    case IndexUnusedReason_Code.NO_PRUNING_POWER:
      return "NO_PRUNING_POWER";
    case IndexUnusedReason_Code.UNINDEXED_SEARCH_FIELDS:
      return "UNINDEXED_SEARCH_FIELDS";
    case IndexUnusedReason_Code.UNSUPPORTED_SEARCH_PATTERN:
      return "UNSUPPORTED_SEARCH_PATTERN";
    case IndexUnusedReason_Code.OPTIMIZED_WITH_MATERIALIZED_VIEW:
      return "OPTIMIZED_WITH_MATERIALIZED_VIEW";
    case IndexUnusedReason_Code.SECURED_BY_DATA_MASKING:
      return "SECURED_BY_DATA_MASKING";
    case IndexUnusedReason_Code.MISMATCHED_TEXT_ANALYZER:
      return "MISMATCHED_TEXT_ANALYZER";
    case IndexUnusedReason_Code.BASE_TABLE_TOO_SMALL:
      return "BASE_TABLE_TOO_SMALL";
    case IndexUnusedReason_Code.BASE_TABLE_TOO_LARGE:
      return "BASE_TABLE_TOO_LARGE";
    case IndexUnusedReason_Code.ESTIMATED_PERFORMANCE_GAIN_TOO_LOW:
      return "ESTIMATED_PERFORMANCE_GAIN_TOO_LOW";
    case IndexUnusedReason_Code.NOT_SUPPORTED_IN_STANDARD_EDITION:
      return "NOT_SUPPORTED_IN_STANDARD_EDITION";
    case IndexUnusedReason_Code.INDEX_SUPPRESSED_BY_FUNCTION_OPTION:
      return "INDEX_SUPPRESSED_BY_FUNCTION_OPTION";
    case IndexUnusedReason_Code.QUERY_CACHE_HIT:
      return "QUERY_CACHE_HIT";
    case IndexUnusedReason_Code.STALE_INDEX:
      return "STALE_INDEX";
    case IndexUnusedReason_Code.INTERNAL_ERROR:
      return "INTERNAL_ERROR";
    case IndexUnusedReason_Code.OTHER_REASON:
      return "OTHER_REASON";
    case IndexUnusedReason_Code.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Statistics for a search query.
 * Populated as part of JobStatistics2.
 */
export interface SearchStatistics {
  /** Specifies the index usage mode for the query. */
  indexUsageMode: SearchStatistics_IndexUsageMode;
  /**
   * When `indexUsageMode` is `UNUSED` or `PARTIALLY_USED`, this field explains
   * why indexes were not used in all or part of the search query. If
   * `indexUsageMode` is `FULLY_USED`, this field is not populated.
   */
  indexUnusedReasons: IndexUnusedReason[];
}

/** Indicates the type of search index usage in the entire search query. */
export enum SearchStatistics_IndexUsageMode {
  /** INDEX_USAGE_MODE_UNSPECIFIED - Index usage mode not specified. */
  INDEX_USAGE_MODE_UNSPECIFIED = 0,
  /**
   * UNUSED - No search indexes were used in the search query. See
   * [`indexUnusedReasons`]
   * (/bigquery/docs/reference/rest/v2/Job#IndexUnusedReason)
   * for detailed reasons.
   */
  UNUSED = 1,
  /**
   * PARTIALLY_USED - Part of the search query used search indexes. See [`indexUnusedReasons`]
   * (/bigquery/docs/reference/rest/v2/Job#IndexUnusedReason)
   * for why other parts of the query did not use search indexes.
   */
  PARTIALLY_USED = 2,
  /** FULLY_USED - The entire search query used search indexes. */
  FULLY_USED = 4,
  UNRECOGNIZED = -1,
}

export function searchStatistics_IndexUsageModeFromJSON(object: any): SearchStatistics_IndexUsageMode {
  switch (object) {
    case 0:
    case "INDEX_USAGE_MODE_UNSPECIFIED":
      return SearchStatistics_IndexUsageMode.INDEX_USAGE_MODE_UNSPECIFIED;
    case 1:
    case "UNUSED":
      return SearchStatistics_IndexUsageMode.UNUSED;
    case 2:
    case "PARTIALLY_USED":
      return SearchStatistics_IndexUsageMode.PARTIALLY_USED;
    case 4:
    case "FULLY_USED":
      return SearchStatistics_IndexUsageMode.FULLY_USED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SearchStatistics_IndexUsageMode.UNRECOGNIZED;
  }
}

export function searchStatistics_IndexUsageModeToJSON(object: SearchStatistics_IndexUsageMode): string {
  switch (object) {
    case SearchStatistics_IndexUsageMode.INDEX_USAGE_MODE_UNSPECIFIED:
      return "INDEX_USAGE_MODE_UNSPECIFIED";
    case SearchStatistics_IndexUsageMode.UNUSED:
      return "UNUSED";
    case SearchStatistics_IndexUsageMode.PARTIALLY_USED:
      return "PARTIALLY_USED";
    case SearchStatistics_IndexUsageMode.FULLY_USED:
      return "FULLY_USED";
    case SearchStatistics_IndexUsageMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Statistics for a vector search query.
 * Populated as part of JobStatistics2.
 */
export interface VectorSearchStatistics {
  /** Specifies the index usage mode for the query. */
  indexUsageMode: VectorSearchStatistics_IndexUsageMode;
  /**
   * When `indexUsageMode` is `UNUSED` or `PARTIALLY_USED`, this field explains
   * why indexes were not used in all or part of the vector search query. If
   * `indexUsageMode` is `FULLY_USED`, this field is not populated.
   */
  indexUnusedReasons: IndexUnusedReason[];
}

/** Indicates the type of vector index usage in the entire vector search query. */
export enum VectorSearchStatistics_IndexUsageMode {
  /** INDEX_USAGE_MODE_UNSPECIFIED - Index usage mode not specified. */
  INDEX_USAGE_MODE_UNSPECIFIED = 0,
  /**
   * UNUSED - No vector indexes were used in the vector search query. See
   * [`indexUnusedReasons`]
   * (/bigquery/docs/reference/rest/v2/Job#IndexUnusedReason)
   * for detailed reasons.
   */
  UNUSED = 1,
  /**
   * PARTIALLY_USED - Part of the vector search query used vector indexes. See
   * [`indexUnusedReasons`]
   * (/bigquery/docs/reference/rest/v2/Job#IndexUnusedReason)
   * for why other parts of the query did not use vector indexes.
   */
  PARTIALLY_USED = 2,
  /** FULLY_USED - The entire vector search query used vector indexes. */
  FULLY_USED = 4,
  UNRECOGNIZED = -1,
}

export function vectorSearchStatistics_IndexUsageModeFromJSON(object: any): VectorSearchStatistics_IndexUsageMode {
  switch (object) {
    case 0:
    case "INDEX_USAGE_MODE_UNSPECIFIED":
      return VectorSearchStatistics_IndexUsageMode.INDEX_USAGE_MODE_UNSPECIFIED;
    case 1:
    case "UNUSED":
      return VectorSearchStatistics_IndexUsageMode.UNUSED;
    case 2:
    case "PARTIALLY_USED":
      return VectorSearchStatistics_IndexUsageMode.PARTIALLY_USED;
    case 4:
    case "FULLY_USED":
      return VectorSearchStatistics_IndexUsageMode.FULLY_USED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return VectorSearchStatistics_IndexUsageMode.UNRECOGNIZED;
  }
}

export function vectorSearchStatistics_IndexUsageModeToJSON(object: VectorSearchStatistics_IndexUsageMode): string {
  switch (object) {
    case VectorSearchStatistics_IndexUsageMode.INDEX_USAGE_MODE_UNSPECIFIED:
      return "INDEX_USAGE_MODE_UNSPECIFIED";
    case VectorSearchStatistics_IndexUsageMode.UNUSED:
      return "UNUSED";
    case VectorSearchStatistics_IndexUsageMode.PARTIALLY_USED:
      return "PARTIALLY_USED";
    case VectorSearchStatistics_IndexUsageMode.FULLY_USED:
      return "FULLY_USED";
    case VectorSearchStatistics_IndexUsageMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Query optimization information for a QUERY job. */
export interface QueryInfo {
  /** Output only. Information about query optimizations. */
  optimizationDetails: { [key: string]: any } | undefined;
}

/** Statistics for a LOAD query. */
export interface LoadQueryStatistics {
  /** Output only. Number of source files in a LOAD query. */
  inputFiles:
    | Long
    | undefined;
  /** Output only. Number of bytes of source data in a LOAD query. */
  inputFileBytes:
    | Long
    | undefined;
  /**
   * Output only. Number of rows imported in a LOAD query.
   * Note that while a LOAD query is in the running state, this value may
   * change.
   */
  outputRows:
    | Long
    | undefined;
  /**
   * Output only. Size of the loaded data in bytes. Note that while a LOAD query
   * is in the running state, this value may change.
   */
  outputBytes:
    | Long
    | undefined;
  /**
   * Output only. The number of bad records encountered while processing a LOAD
   * query. Note that if the job has failed because of more bad records
   * encountered than the maximum allowed in the load job configuration, then
   * this number can be less than the total number of bad records present in the
   * input data.
   */
  badRecords: Long | undefined;
}

/** Statistics for a query job. */
export interface JobStatistics2 {
  /** Output only. Describes execution plan for the query. */
  queryPlan: ExplainQueryStage[];
  /** Output only. The original estimate of bytes processed for the job. */
  estimatedBytesProcessed:
    | Long
    | undefined;
  /** Output only. Describes a timeline of job execution. */
  timeline: QueryTimelineSample[];
  /**
   * Output only. Total number of partitions processed from all partitioned
   * tables referenced in the job.
   */
  totalPartitionsProcessed:
    | Long
    | undefined;
  /** Output only. Total bytes processed for the job. */
  totalBytesProcessed:
    | Long
    | undefined;
  /**
   * Output only. For dry-run jobs, totalBytesProcessed is an estimate and this
   * field specifies the accuracy of the estimate. Possible values can be:
   * UNKNOWN: accuracy of the estimate is unknown.
   * PRECISE: estimate is precise.
   * LOWER_BOUND: estimate is lower bound of what the query would cost.
   * UPPER_BOUND: estimate is upper bound of what the query would cost.
   */
  totalBytesProcessedAccuracy: string;
  /**
   * Output only. If the project is configured to use on-demand pricing,
   * then this field contains the total bytes billed for the job.
   * If the project is configured to use flat-rate pricing, then you are
   * not billed for bytes and this field is informational only.
   */
  totalBytesBilled:
    | Long
    | undefined;
  /**
   * Output only. Billing tier for the job. This is a BigQuery-specific concept
   * which is not related to the Google Cloud notion of "free tier". The value
   * here is a measure of the query's resource consumption relative to the
   * amount of data scanned. For on-demand queries, the limit is 100, and all
   * queries within this limit are billed at the standard on-demand rates.
   * On-demand queries that exceed this limit will fail with a
   * billingTierLimitExceeded error.
   */
  billingTier:
    | number
    | undefined;
  /** Output only. Slot-milliseconds for the job. */
  totalSlotMs:
    | Long
    | undefined;
  /** Output only. Whether the query result was fetched from the query cache. */
  cacheHit:
    | boolean
    | undefined;
  /**
   * Output only. Referenced tables for the job. Queries that reference more
   * than 50 tables will not have a complete list.
   */
  referencedTables: TableReference[];
  /** Output only. Referenced routines for the job. */
  referencedRoutines: RoutineReference[];
  /**
   * Output only. The schema of the results. Present only for successful dry
   * run of non-legacy SQL queries.
   */
  schema:
    | TableSchema
    | undefined;
  /**
   * Output only. The number of rows affected by a DML statement. Present
   * only for DML statements INSERT, UPDATE or DELETE.
   */
  numDmlAffectedRows:
    | Long
    | undefined;
  /**
   * Output only. Detailed statistics for DML statements INSERT, UPDATE, DELETE,
   * MERGE or TRUNCATE.
   */
  dmlStats:
    | DmlStats
    | undefined;
  /**
   * Output only. GoogleSQL only: list of undeclared query
   * parameters detected during a dry run validation.
   */
  undeclaredQueryParameters: QueryParameter[];
  /**
   * Output only. The type of query statement, if valid.
   * Possible values:
   *
   * * `SELECT`:
   * [`SELECT`](https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#select_list)
   * statement.
   * * `ASSERT`:
   * [`ASSERT`](https://cloud.google.com/bigquery/docs/reference/standard-sql/debugging-statements#assert)
   * statement.
   * * `INSERT`:
   * [`INSERT`](https://cloud.google.com/bigquery/docs/reference/standard-sql/dml-syntax#insert_statement)
   * statement.
   * * `UPDATE`:
   * [`UPDATE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#update_statement)
   * statement.
   * * `DELETE`:
   * [`DELETE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-manipulation-language)
   * statement.
   * * `MERGE`:
   * [`MERGE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-manipulation-language)
   * statement.
   * * `CREATE_TABLE`: [`CREATE
   * TABLE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_table_statement)
   * statement, without `AS SELECT`.
   * * `CREATE_TABLE_AS_SELECT`: [`CREATE TABLE AS
   * SELECT`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#query_statement)
   * statement.
   * * `CREATE_VIEW`: [`CREATE
   * VIEW`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_view_statement)
   * statement.
   * * `CREATE_MODEL`: [`CREATE
   * MODEL`](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create#create_model_statement)
   * statement.
   * * `CREATE_MATERIALIZED_VIEW`: [`CREATE MATERIALIZED
   * VIEW`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_materialized_view_statement)
   * statement.
   * * `CREATE_FUNCTION`: [`CREATE
   * FUNCTION`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_function_statement)
   * statement.
   * * `CREATE_TABLE_FUNCTION`: [`CREATE TABLE
   * FUNCTION`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_table_function_statement)
   * statement.
   * * `CREATE_PROCEDURE`: [`CREATE
   * PROCEDURE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_procedure)
   * statement.
   * * `CREATE_ROW_ACCESS_POLICY`: [`CREATE ROW ACCESS
   * POLICY`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_row_access_policy_statement)
   * statement.
   * * `CREATE_SCHEMA`: [`CREATE
   * SCHEMA`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_schema_statement)
   * statement.
   * * `CREATE_SNAPSHOT_TABLE`: [`CREATE SNAPSHOT
   * TABLE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_snapshot_table_statement)
   * statement.
   * * `CREATE_SEARCH_INDEX`: [`CREATE SEARCH
   * INDEX`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_search_index_statement)
   * statement.
   * * `DROP_TABLE`: [`DROP
   * TABLE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_table_statement)
   * statement.
   * * `DROP_EXTERNAL_TABLE`: [`DROP EXTERNAL
   * TABLE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_external_table_statement)
   * statement.
   * * `DROP_VIEW`: [`DROP
   * VIEW`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_view_statement)
   * statement.
   * * `DROP_MODEL`: [`DROP
   * MODEL`](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-drop-model)
   * statement.
   * * `DROP_MATERIALIZED_VIEW`: [`DROP MATERIALIZED
   *  VIEW`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_materialized_view_statement)
   * statement.
   * * `DROP_FUNCTION` : [`DROP
   * FUNCTION`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_function_statement)
   * statement.
   * * `DROP_TABLE_FUNCTION` : [`DROP TABLE
   * FUNCTION`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_table_function)
   * statement.
   * * `DROP_PROCEDURE`: [`DROP
   * PROCEDURE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_procedure_statement)
   * statement.
   * * `DROP_SEARCH_INDEX`: [`DROP SEARCH
   * INDEX`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_search_index)
   * statement.
   * * `DROP_SCHEMA`: [`DROP
   * SCHEMA`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_schema_statement)
   * statement.
   * * `DROP_SNAPSHOT_TABLE`: [`DROP SNAPSHOT
   * TABLE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_snapshot_table_statement)
   * statement.
   * * `DROP_ROW_ACCESS_POLICY`: [`DROP [ALL] ROW ACCESS
   * POLICY|POLICIES`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_row_access_policy_statement)
   * statement.
   * * `ALTER_TABLE`: [`ALTER
   * TABLE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#alter_table_set_options_statement)
   * statement.
   * * `ALTER_VIEW`: [`ALTER
   * VIEW`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#alter_view_set_options_statement)
   * statement.
   * * `ALTER_MATERIALIZED_VIEW`: [`ALTER MATERIALIZED
   * VIEW`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#alter_materialized_view_set_options_statement)
   * statement.
   * * `ALTER_SCHEMA`: [`ALTER
   * SCHEMA`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#aalter_schema_set_options_statement)
   * statement.
   * * `SCRIPT`:
   * [`SCRIPT`](https://cloud.google.com/bigquery/docs/reference/standard-sql/procedural-language).
   * * `TRUNCATE_TABLE`: [`TRUNCATE
   * TABLE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/dml-syntax#truncate_table_statement)
   * statement.
   * * `CREATE_EXTERNAL_TABLE`: [`CREATE EXTERNAL
   * TABLE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_external_table_statement)
   * statement.
   * * `EXPORT_DATA`: [`EXPORT
   * DATA`](https://cloud.google.com/bigquery/docs/reference/standard-sql/other-statements#export_data_statement)
   * statement.
   * * `EXPORT_MODEL`: [`EXPORT
   * MODEL`](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-export-model)
   * statement.
   * * `LOAD_DATA`: [`LOAD
   * DATA`](https://cloud.google.com/bigquery/docs/reference/standard-sql/other-statements#load_data_statement)
   * statement.
   * * `CALL`:
   * [`CALL`](https://cloud.google.com/bigquery/docs/reference/standard-sql/procedural-language#call)
   * statement.
   */
  statementType: string;
  /**
   * Output only. The DDL operation performed, possibly
   * dependent on the pre-existence of the DDL target.
   */
  ddlOperationPerformed: string;
  /**
   * Output only. The DDL target table. Present only for
   * CREATE/DROP TABLE/VIEW and DROP ALL ROW ACCESS POLICIES queries.
   */
  ddlTargetTable:
    | TableReference
    | undefined;
  /**
   * Output only. The table after rename. Present only for ALTER TABLE RENAME TO
   * query.
   */
  ddlDestinationTable:
    | TableReference
    | undefined;
  /**
   * Output only. The DDL target row access policy. Present only for
   * CREATE/DROP ROW ACCESS POLICY queries.
   */
  ddlTargetRowAccessPolicy:
    | RowAccessPolicyReference
    | undefined;
  /**
   * Output only. The number of row access policies affected by a DDL statement.
   * Present only for DROP ALL ROW ACCESS POLICIES queries.
   */
  ddlAffectedRowAccessPolicyCount:
    | Long
    | undefined;
  /**
   * Output only. [Beta] The DDL target routine. Present only for
   * CREATE/DROP FUNCTION/PROCEDURE queries.
   */
  ddlTargetRoutine:
    | RoutineReference
    | undefined;
  /**
   * Output only. The DDL target dataset. Present only for CREATE/ALTER/DROP
   * SCHEMA(dataset) queries.
   */
  ddlTargetDataset:
    | DatasetReference
    | undefined;
  /** Output only. Statistics of a BigQuery ML training job. */
  mlStatistics:
    | MlStatistics
    | undefined;
  /** Output only. Stats for EXPORT DATA statement. */
  exportDataStatistics:
    | ExportDataStatistics
    | undefined;
  /**
   * Output only. Job cost breakdown as bigquery internal cost and external
   * service costs.
   */
  externalServiceCosts: ExternalServiceCost[];
  /** Output only. BI Engine specific Statistics. */
  biEngineStatistics:
    | BiEngineStatistics
    | undefined;
  /** Output only. Statistics for a LOAD query. */
  loadQueryStatistics:
    | LoadQueryStatistics
    | undefined;
  /** Output only. Referenced table for DCL statement. */
  dclTargetTable:
    | TableReference
    | undefined;
  /** Output only. Referenced view for DCL statement. */
  dclTargetView:
    | TableReference
    | undefined;
  /** Output only. Referenced dataset for DCL statement. */
  dclTargetDataset:
    | DatasetReference
    | undefined;
  /** Output only. Search query specific statistics. */
  searchStatistics:
    | SearchStatistics
    | undefined;
  /** Output only. Vector Search query specific statistics. */
  vectorSearchStatistics:
    | VectorSearchStatistics
    | undefined;
  /** Output only. Performance insights. */
  performanceInsights:
    | PerformanceInsights
    | undefined;
  /** Output only. Query optimization information for a QUERY job. */
  queryInfo:
    | QueryInfo
    | undefined;
  /** Output only. Statistics of a Spark procedure job. */
  sparkStatistics:
    | SparkStatistics
    | undefined;
  /**
   * Output only. Total bytes transferred for cross-cloud queries such as Cross
   * Cloud Transfer and CREATE TABLE AS SELECT (CTAS).
   */
  transferredBytes:
    | Long
    | undefined;
  /** Output only. Statistics of materialized views of a query job. */
  materializedViewStatistics:
    | MaterializedViewStatistics
    | undefined;
  /**
   * Output only. Statistics of metadata cache usage in a query for BigLake
   * tables.
   */
  metadataCacheStatistics: MetadataCacheStatistics | undefined;
}

/** Statistics for a load job. */
export interface JobStatistics3 {
  /** Output only. Number of source files in a load job. */
  inputFiles:
    | Long
    | undefined;
  /** Output only. Number of bytes of source data in a load job. */
  inputFileBytes:
    | Long
    | undefined;
  /**
   * Output only. Number of rows imported in a load job.
   * Note that while an import job is in the running state, this
   * value may change.
   */
  outputRows:
    | Long
    | undefined;
  /**
   * Output only. Size of the loaded data in bytes. Note
   * that while a load job is in the running state, this value may change.
   */
  outputBytes:
    | Long
    | undefined;
  /**
   * Output only. The number of bad records encountered. Note that if the job
   * has failed because of more bad records encountered than the maximum
   * allowed in the load job configuration, then this number can be less than
   * the total number of bad records present in the input data.
   */
  badRecords:
    | Long
    | undefined;
  /** Output only. Describes a timeline of job execution. */
  timeline: QueryTimelineSample[];
}

/** Statistics for an extract job. */
export interface JobStatistics4 {
  /**
   * Output only. Number of files per destination URI or URI pattern
   * specified in the extract configuration. These values will be in the same
   * order as the URIs specified in the 'destinationUris' field.
   */
  destinationUriFileCounts: Long[];
  /**
   * Output only. Number of user bytes extracted into the result. This is the
   * byte count as computed by BigQuery for billing purposes
   * and doesn't have any relationship with the number of actual
   * result bytes extracted in the desired format.
   */
  inputBytes:
    | Long
    | undefined;
  /** Output only. Describes a timeline of job execution. */
  timeline: QueryTimelineSample[];
}

/** Statistics for a copy job. */
export interface CopyJobStatistics {
  /** Output only. Number of rows copied to the destination table. */
  copiedRows:
    | Long
    | undefined;
  /** Output only. Number of logical bytes copied to the destination table. */
  copiedLogicalBytes: Long | undefined;
}

/** Job statistics specific to a BigQuery ML training job. */
export interface MlStatistics {
  /**
   * Output only. Maximum number of iterations specified as max_iterations in
   * the 'CREATE MODEL' query. The actual number of iterations may be less than
   * this number due to early stop.
   */
  maxIterations: Long;
  /**
   * Results for all completed iterations.
   * Empty for [hyperparameter tuning
   * jobs](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-hp-tuning-overview).
   */
  iterationResults: Model_TrainingRun_IterationResult[];
  /** Output only. The type of the model that is being trained. */
  modelType: Model_ModelType;
  /** Output only. Training type of the job. */
  trainingType: MlStatistics_TrainingType;
  /**
   * Output only. Trials of a [hyperparameter tuning
   * job](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-hp-tuning-overview)
   * sorted by trial_id.
   */
  hparamTrials: Model_HparamTuningTrial[];
}

/** Training type. */
export enum MlStatistics_TrainingType {
  /** TRAINING_TYPE_UNSPECIFIED - Unspecified training type. */
  TRAINING_TYPE_UNSPECIFIED = 0,
  /** SINGLE_TRAINING - Single training with fixed parameter space. */
  SINGLE_TRAINING = 1,
  /**
   * HPARAM_TUNING - [Hyperparameter tuning
   * training](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-hp-tuning-overview).
   */
  HPARAM_TUNING = 2,
  UNRECOGNIZED = -1,
}

export function mlStatistics_TrainingTypeFromJSON(object: any): MlStatistics_TrainingType {
  switch (object) {
    case 0:
    case "TRAINING_TYPE_UNSPECIFIED":
      return MlStatistics_TrainingType.TRAINING_TYPE_UNSPECIFIED;
    case 1:
    case "SINGLE_TRAINING":
      return MlStatistics_TrainingType.SINGLE_TRAINING;
    case 2:
    case "HPARAM_TUNING":
      return MlStatistics_TrainingType.HPARAM_TUNING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return MlStatistics_TrainingType.UNRECOGNIZED;
  }
}

export function mlStatistics_TrainingTypeToJSON(object: MlStatistics_TrainingType): string {
  switch (object) {
    case MlStatistics_TrainingType.TRAINING_TYPE_UNSPECIFIED:
      return "TRAINING_TYPE_UNSPECIFIED";
    case MlStatistics_TrainingType.SINGLE_TRAINING:
      return "SINGLE_TRAINING";
    case MlStatistics_TrainingType.HPARAM_TUNING:
      return "HPARAM_TUNING";
    case MlStatistics_TrainingType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Job statistics specific to the child job of a script. */
export interface ScriptStatistics {
  /** Whether this child job was a statement or expression. */
  evaluationKind: ScriptStatistics_EvaluationKind;
  /**
   * Stack trace showing the line/column/procedure name of each frame on the
   * stack at the point where the current evaluation happened. The leaf frame
   * is first, the primary script is last. Never empty.
   */
  stackFrames: ScriptStatistics_ScriptStackFrame[];
}

/** Describes how the job is evaluated. */
export enum ScriptStatistics_EvaluationKind {
  /** EVALUATION_KIND_UNSPECIFIED - Default value. */
  EVALUATION_KIND_UNSPECIFIED = 0,
  /** STATEMENT - The statement appears directly in the script. */
  STATEMENT = 1,
  /** EXPRESSION - The statement evaluates an expression that appears in the script. */
  EXPRESSION = 2,
  UNRECOGNIZED = -1,
}

export function scriptStatistics_EvaluationKindFromJSON(object: any): ScriptStatistics_EvaluationKind {
  switch (object) {
    case 0:
    case "EVALUATION_KIND_UNSPECIFIED":
      return ScriptStatistics_EvaluationKind.EVALUATION_KIND_UNSPECIFIED;
    case 1:
    case "STATEMENT":
      return ScriptStatistics_EvaluationKind.STATEMENT;
    case 2:
    case "EXPRESSION":
      return ScriptStatistics_EvaluationKind.EXPRESSION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ScriptStatistics_EvaluationKind.UNRECOGNIZED;
  }
}

export function scriptStatistics_EvaluationKindToJSON(object: ScriptStatistics_EvaluationKind): string {
  switch (object) {
    case ScriptStatistics_EvaluationKind.EVALUATION_KIND_UNSPECIFIED:
      return "EVALUATION_KIND_UNSPECIFIED";
    case ScriptStatistics_EvaluationKind.STATEMENT:
      return "STATEMENT";
    case ScriptStatistics_EvaluationKind.EXPRESSION:
      return "EXPRESSION";
    case ScriptStatistics_EvaluationKind.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Represents the location of the statement/expression being evaluated.
 * Line and column numbers are defined as follows:
 *
 * - Line and column numbers start with one.  That is, line 1 column 1 denotes
 *   the start of the script.
 * - When inside a stored procedure, all line/column numbers are relative
 *   to the procedure body, not the script in which the procedure was defined.
 * - Start/end positions exclude leading/trailing comments and whitespace.
 *   The end position always ends with a ";", when present.
 * - Multi-byte Unicode characters are treated as just one column.
 * - If the original script (or procedure definition) contains TAB characters,
 *   a tab "snaps" the indentation forward to the nearest multiple of 8
 *   characters, plus 1. For example, a TAB on column 1, 2, 3, 4, 5, 6 , or 8
 *   will advance the next character to column 9.  A TAB on column 9, 10, 11,
 *   12, 13, 14, 15, or 16 will advance the next character to column 17.
 */
export interface ScriptStatistics_ScriptStackFrame {
  /** Output only. One-based start line. */
  startLine: number;
  /** Output only. One-based start column. */
  startColumn: number;
  /** Output only. One-based end line. */
  endLine: number;
  /** Output only. One-based end column. */
  endColumn: number;
  /**
   * Output only. Name of the active procedure, empty if in a top-level
   * script.
   */
  procedureId: string;
  /** Output only. Text of the current statement/expression. */
  text: string;
}

/** Statistics for row-level security. */
export interface RowLevelSecurityStatistics {
  /** Whether any accessed data was protected by row access policies. */
  rowLevelSecurityApplied: boolean;
}

/** Statistics for data-masking. */
export interface DataMaskingStatistics {
  /** Whether any accessed data was protected by the data masking. */
  dataMaskingApplied: boolean;
}

/** Statistics for a single job execution. */
export interface JobStatistics {
  /**
   * Output only. Creation time of this job, in milliseconds since the epoch.
   * This field will be present on all jobs.
   */
  creationTime: Long;
  /**
   * Output only. Start time of this job, in milliseconds since the epoch.
   * This field will be present when the job transitions from the PENDING state
   * to either RUNNING or DONE.
   */
  startTime: Long;
  /**
   * Output only. End time of this job, in milliseconds since the epoch. This
   * field will be present whenever a job is in the DONE state.
   */
  endTime: Long;
  /** Output only. Total bytes processed for the job. */
  totalBytesProcessed:
    | Long
    | undefined;
  /**
   * Output only. [TrustedTester] Job progress (0.0 -> 1.0) for LOAD and
   * EXTRACT jobs.
   */
  completionRatio:
    | number
    | undefined;
  /** Output only. Quotas which delayed this job's start time. */
  quotaDeferments: string[];
  /** Output only. Statistics for a query job. */
  query:
    | JobStatistics2
    | undefined;
  /** Output only. Statistics for a load job. */
  load:
    | JobStatistics3
    | undefined;
  /** Output only. Statistics for an extract job. */
  extract:
    | JobStatistics4
    | undefined;
  /** Output only. Statistics for a copy job. */
  copy:
    | CopyJobStatistics
    | undefined;
  /** Output only. Slot-milliseconds for the job. */
  totalSlotMs:
    | Long
    | undefined;
  /**
   * Output only. Name of the primary reservation assigned to this job. Note
   * that this could be different than reservations reported in the reservation
   * usage field if parent reservations were used to execute this job.
   */
  reservationId: string;
  /** Output only. Number of child jobs executed. */
  numChildJobs: Long;
  /** Output only. If this is a child job, specifies the job ID of the parent. */
  parentJobId: string;
  /**
   * Output only. If this a child job of a script, specifies information about
   * the context of this job within the script.
   */
  scriptStatistics:
    | ScriptStatistics
    | undefined;
  /**
   * Output only. Statistics for row-level security. Present only for query and
   * extract jobs.
   */
  rowLevelSecurityStatistics:
    | RowLevelSecurityStatistics
    | undefined;
  /**
   * Output only. Statistics for data-masking. Present only for query and
   * extract jobs.
   */
  dataMaskingStatistics:
    | DataMaskingStatistics
    | undefined;
  /**
   * Output only. [Alpha] Information of the multi-statement transaction if this
   * job is part of one.
   *
   * This property is only expected on a child job or a job that is in a
   * session. A script parent job is not part of the transaction started in the
   * script.
   */
  transactionInfo:
    | JobStatistics_TransactionInfo
    | undefined;
  /** Output only. Information of the session if this job is part of one. */
  sessionInfo:
    | SessionInfo
    | undefined;
  /**
   * Output only. The duration in milliseconds of the execution of the final
   * attempt of this job, as BigQuery may internally re-attempt to execute the
   * job.
   */
  finalExecutionDurationMs: Long;
  /**
   * Output only. Name of edition corresponding to the reservation for this job
   * at the time of this update.
   */
  edition: ReservationEdition;
}

/** [Alpha] Information of a multi-statement transaction. */
export interface JobStatistics_TransactionInfo {
  /** Output only. [Alpha] Id of the transaction. */
  transactionId: string;
}

/** Detailed statistics for DML statements */
export interface DmlStats {
  /**
   * Output only. Number of inserted Rows. Populated by DML INSERT and MERGE
   * statements
   */
  insertedRowCount:
    | Long
    | undefined;
  /**
   * Output only. Number of deleted Rows. populated by DML DELETE, MERGE and
   * TRUNCATE statements.
   */
  deletedRowCount:
    | Long
    | undefined;
  /**
   * Output only. Number of updated Rows. Populated by DML UPDATE and MERGE
   * statements.
   */
  updatedRowCount: Long | undefined;
}

/** Performance insights for the job. */
export interface PerformanceInsights {
  /**
   * Output only. Average execution ms of previous runs. Indicates the job ran
   * slow compared to previous executions. To find previous executions, use
   * INFORMATION_SCHEMA tables and filter jobs with same query hash.
   */
  avgPreviousExecutionMs: Long;
  /**
   * Output only. Standalone query stage performance insights, for exploring
   * potential improvements.
   */
  stagePerformanceStandaloneInsights: StagePerformanceStandaloneInsight[];
  /**
   * Output only. Query stage performance insights compared to previous runs,
   * for diagnosing performance regression.
   */
  stagePerformanceChangeInsights: StagePerformanceChangeInsight[];
}

/**
 * Performance insights compared to the previous executions for a specific
 * stage.
 */
export interface StagePerformanceChangeInsight {
  /** Output only. The stage id that the insight mapped to. */
  stageId: Long;
  /** Output only. Input data change insight of the query stage. */
  inputDataChange?: InputDataChange | undefined;
}

/** Details about the input data change insight. */
export interface InputDataChange {
  /** Output only. Records read difference percentage compared to a previous run. */
  recordsReadDiffPercentage: number;
}

/** Standalone performance insights for a specific stage. */
export interface StagePerformanceStandaloneInsight {
  /** Output only. The stage id that the insight mapped to. */
  stageId: Long;
  /** Output only. True if the stage has a slot contention issue. */
  slotContention?:
    | boolean
    | undefined;
  /** Output only. True if the stage has insufficient shuffle quota. */
  insufficientShuffleQuota?:
    | boolean
    | undefined;
  /**
   * Output only. If present, the stage had the following reasons for being
   * disqualified from BI Engine execution.
   */
  biEngineReasons: BiEngineReason[];
  /** Output only. High cardinality joins in the stage. */
  highCardinalityJoins: HighCardinalityJoin[];
  /** Output only. Partition skew in the stage. */
  partitionSkew?: PartitionSkew | undefined;
}

/** High cardinality join detailed information. */
export interface HighCardinalityJoin {
  /** Output only. Count of left input rows. */
  leftRows: Long;
  /** Output only. Count of right input rows. */
  rightRows: Long;
  /** Output only. Count of the output rows. */
  outputRows: Long;
  /** Output only. The index of the join operator in the ExplainQueryStep lists. */
  stepIndex: number;
}

/** Partition skew detailed information. */
export interface PartitionSkew {
  /** Output only. Source stages which produce skewed data. */
  skewSources: PartitionSkew_SkewSource[];
}

/** Details about source stages which produce skewed data. */
export interface PartitionSkew_SkewSource {
  /** Output only. Stage id of the skew source stage. */
  stageId: Long;
}

/**
 * Statistics for a BigSpark query.
 * Populated as part of JobStatistics2
 */
export interface SparkStatistics {
  /** Output only. Spark job ID if a Spark job is created successfully. */
  sparkJobId?:
    | string
    | undefined;
  /**
   * Output only. Location where the Spark job is executed.
   * A location is selected by BigQueury for jobs configured to run in a
   * multi-region.
   */
  sparkJobLocation?:
    | string
    | undefined;
  /**
   * Output only. Endpoints returned from Dataproc.
   * Key list:
   *  - history_server_endpoint: A link to Spark job UI.
   */
  endpoints: { [key: string]: string };
  /** Output only. Logging info is used to generate a link to Cloud Logging. */
  loggingInfo?:
    | SparkStatistics_LoggingInfo
    | undefined;
  /**
   * Output only. The Cloud KMS encryption key that is used to protect the
   * resources created by the Spark job. If the Spark procedure uses the invoker
   * security mode, the Cloud KMS encryption key is either inferred from the
   * provided system variable,
   * `@@spark_proc_properties.kms_key_name`, or the default key of the BigQuery
   * job's project (if the CMEK organization policy is enforced). Otherwise, the
   * Cloud KMS key is either inferred from the Spark connection associated with
   * the procedure (if it is provided), or from the default key of the Spark
   * connection's project if the CMEK organization policy is enforced.
   *
   * Example:
   *
   * * `projects/[kms_project_id]/locations/[region]/keyRings/[key_region]/cryptoKeys/[key]`
   */
  kmsKeyName?:
    | string
    | undefined;
  /**
   * Output only. The Google Cloud Storage bucket that is used as the default
   * file system by the Spark application. This field is only filled when the
   * Spark procedure uses the invoker security mode. The `gcsStagingBucket`
   * bucket is inferred from the `@@spark_proc_properties.staging_bucket` system
   * variable (if it is provided). Otherwise, BigQuery creates a default staging
   * bucket for the job and returns the bucket name in this field.
   *
   * Example:
   *
   * * `gs://[bucket_name]`
   */
  gcsStagingBucket?: string | undefined;
}

/** Spark job logs can be filtered by these fields in Cloud Logging. */
export interface SparkStatistics_LoggingInfo {
  /** Output only. Resource type used for logging. */
  resourceType: string;
  /** Output only. Project ID where the Spark logs were written. */
  projectId: string;
}

export interface SparkStatistics_EndpointsEntry {
  key: string;
  value: string;
}

/** Statistics of materialized views considered in a query job. */
export interface MaterializedViewStatistics {
  /**
   * Materialized views considered for the query job. Only certain materialized
   * views are used. For a detailed list, see the child message.
   *
   * If many materialized views are considered, then the list might be
   * incomplete.
   */
  materializedView: MaterializedView[];
}

/** A materialized view considered for a query job. */
export interface MaterializedView {
  /** The candidate materialized view. */
  tableReference?:
    | TableReference
    | undefined;
  /**
   * Whether the materialized view is chosen for the query.
   *
   * A materialized view can be chosen to rewrite multiple parts of the same
   * query. If a materialized view is chosen to rewrite any part of the query,
   * then this field is true, even if the materialized view was not chosen to
   * rewrite others parts.
   */
  chosen?:
    | boolean
    | undefined;
  /**
   * If present, specifies a best-effort estimation of the bytes saved by using
   * the materialized view rather than its base tables.
   */
  estimatedBytesSaved?:
    | Long
    | undefined;
  /**
   * If present, specifies the reason why the materialized view was not chosen
   * for the query.
   */
  rejectedReason?: MaterializedView_RejectedReason | undefined;
}

/**
 * Reason why a materialized view was not chosen for a query. For more
 * information, see [Understand why materialized views were
 * rejected](https://cloud.google.com/bigquery/docs/materialized-views-use#understand-rejected).
 */
export enum MaterializedView_RejectedReason {
  /** REJECTED_REASON_UNSPECIFIED - Default unspecified value. */
  REJECTED_REASON_UNSPECIFIED = 0,
  /** NO_DATA - View has no cached data because it has not refreshed yet. */
  NO_DATA = 1,
  /**
   * COST - The estimated cost of the view is more expensive than another view or the
   * base table.
   *
   * Note: The estimate cost might not match the billed cost.
   */
  COST = 2,
  /** BASE_TABLE_TRUNCATED - View has no cached data because a base table is truncated. */
  BASE_TABLE_TRUNCATED = 3,
  /**
   * BASE_TABLE_DATA_CHANGE - View is invalidated because of a data change in one or more base tables.
   * It could be any recent change if the
   * [`max_staleness`](https://cloud.google.com/bigquery/docs/materialized-views-create#max_staleness)
   * option is not set for the view, or otherwise any change outside of the
   * staleness window.
   */
  BASE_TABLE_DATA_CHANGE = 4,
  /**
   * BASE_TABLE_PARTITION_EXPIRATION_CHANGE - View is invalidated because a base table's partition expiration has
   * changed.
   */
  BASE_TABLE_PARTITION_EXPIRATION_CHANGE = 5,
  /** BASE_TABLE_EXPIRED_PARTITION - View is invalidated because a base table's partition has expired. */
  BASE_TABLE_EXPIRED_PARTITION = 6,
  /**
   * BASE_TABLE_INCOMPATIBLE_METADATA_CHANGE - View is invalidated because a base table has an incompatible metadata
   * change.
   */
  BASE_TABLE_INCOMPATIBLE_METADATA_CHANGE = 7,
  /**
   * TIME_ZONE - View is invalidated because it was refreshed with a time zone other than
   * that of the current job.
   */
  TIME_ZONE = 8,
  /** OUT_OF_TIME_TRAVEL_WINDOW - View is outside the time travel window. */
  OUT_OF_TIME_TRAVEL_WINDOW = 9,
  /**
   * BASE_TABLE_FINE_GRAINED_SECURITY_POLICY - View is inaccessible to the user because of a fine-grained security
   * policy on one of its base tables.
   */
  BASE_TABLE_FINE_GRAINED_SECURITY_POLICY = 10,
  /**
   * BASE_TABLE_TOO_STALE - One of the view's base tables is too stale. For example, the cached
   * metadata of a BigLake external table needs to be updated.
   */
  BASE_TABLE_TOO_STALE = 11,
  UNRECOGNIZED = -1,
}

export function materializedView_RejectedReasonFromJSON(object: any): MaterializedView_RejectedReason {
  switch (object) {
    case 0:
    case "REJECTED_REASON_UNSPECIFIED":
      return MaterializedView_RejectedReason.REJECTED_REASON_UNSPECIFIED;
    case 1:
    case "NO_DATA":
      return MaterializedView_RejectedReason.NO_DATA;
    case 2:
    case "COST":
      return MaterializedView_RejectedReason.COST;
    case 3:
    case "BASE_TABLE_TRUNCATED":
      return MaterializedView_RejectedReason.BASE_TABLE_TRUNCATED;
    case 4:
    case "BASE_TABLE_DATA_CHANGE":
      return MaterializedView_RejectedReason.BASE_TABLE_DATA_CHANGE;
    case 5:
    case "BASE_TABLE_PARTITION_EXPIRATION_CHANGE":
      return MaterializedView_RejectedReason.BASE_TABLE_PARTITION_EXPIRATION_CHANGE;
    case 6:
    case "BASE_TABLE_EXPIRED_PARTITION":
      return MaterializedView_RejectedReason.BASE_TABLE_EXPIRED_PARTITION;
    case 7:
    case "BASE_TABLE_INCOMPATIBLE_METADATA_CHANGE":
      return MaterializedView_RejectedReason.BASE_TABLE_INCOMPATIBLE_METADATA_CHANGE;
    case 8:
    case "TIME_ZONE":
      return MaterializedView_RejectedReason.TIME_ZONE;
    case 9:
    case "OUT_OF_TIME_TRAVEL_WINDOW":
      return MaterializedView_RejectedReason.OUT_OF_TIME_TRAVEL_WINDOW;
    case 10:
    case "BASE_TABLE_FINE_GRAINED_SECURITY_POLICY":
      return MaterializedView_RejectedReason.BASE_TABLE_FINE_GRAINED_SECURITY_POLICY;
    case 11:
    case "BASE_TABLE_TOO_STALE":
      return MaterializedView_RejectedReason.BASE_TABLE_TOO_STALE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return MaterializedView_RejectedReason.UNRECOGNIZED;
  }
}

export function materializedView_RejectedReasonToJSON(object: MaterializedView_RejectedReason): string {
  switch (object) {
    case MaterializedView_RejectedReason.REJECTED_REASON_UNSPECIFIED:
      return "REJECTED_REASON_UNSPECIFIED";
    case MaterializedView_RejectedReason.NO_DATA:
      return "NO_DATA";
    case MaterializedView_RejectedReason.COST:
      return "COST";
    case MaterializedView_RejectedReason.BASE_TABLE_TRUNCATED:
      return "BASE_TABLE_TRUNCATED";
    case MaterializedView_RejectedReason.BASE_TABLE_DATA_CHANGE:
      return "BASE_TABLE_DATA_CHANGE";
    case MaterializedView_RejectedReason.BASE_TABLE_PARTITION_EXPIRATION_CHANGE:
      return "BASE_TABLE_PARTITION_EXPIRATION_CHANGE";
    case MaterializedView_RejectedReason.BASE_TABLE_EXPIRED_PARTITION:
      return "BASE_TABLE_EXPIRED_PARTITION";
    case MaterializedView_RejectedReason.BASE_TABLE_INCOMPATIBLE_METADATA_CHANGE:
      return "BASE_TABLE_INCOMPATIBLE_METADATA_CHANGE";
    case MaterializedView_RejectedReason.TIME_ZONE:
      return "TIME_ZONE";
    case MaterializedView_RejectedReason.OUT_OF_TIME_TRAVEL_WINDOW:
      return "OUT_OF_TIME_TRAVEL_WINDOW";
    case MaterializedView_RejectedReason.BASE_TABLE_FINE_GRAINED_SECURITY_POLICY:
      return "BASE_TABLE_FINE_GRAINED_SECURITY_POLICY";
    case MaterializedView_RejectedReason.BASE_TABLE_TOO_STALE:
      return "BASE_TABLE_TOO_STALE";
    case MaterializedView_RejectedReason.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Table level detail on the usage of metadata caching. Only set for Metadata
 * caching eligible tables referenced in the query.
 */
export interface TableMetadataCacheUsage {
  /** Metadata caching eligible table referenced in the query. */
  tableReference?:
    | TableReference
    | undefined;
  /** Reason for not using metadata caching for the table. */
  unusedReason?:
    | TableMetadataCacheUsage_UnusedReason
    | undefined;
  /**
   * Free form human-readable reason metadata caching was unused for
   * the job.
   */
  explanation?:
    | string
    | undefined;
  /**
   * Duration since last refresh as of this job for managed tables (indicates
   * metadata cache staleness as seen by this job).
   */
  staleness:
    | Duration
    | undefined;
  /**
   * [Table
   * type](https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#Table.FIELDS.type).
   */
  tableType: string;
}

/** Reasons for not using metadata caching. */
export enum TableMetadataCacheUsage_UnusedReason {
  /** UNUSED_REASON_UNSPECIFIED - Unused reasons not specified. */
  UNUSED_REASON_UNSPECIFIED = 0,
  /** EXCEEDED_MAX_STALENESS - Metadata cache was outside the table's maxStaleness. */
  EXCEEDED_MAX_STALENESS = 1,
  /**
   * METADATA_CACHING_NOT_ENABLED - Metadata caching feature is not enabled. [Update BigLake tables]
   * (/bigquery/docs/create-cloud-storage-table-biglake#update-biglake-tables)
   * to enable the metadata caching.
   */
  METADATA_CACHING_NOT_ENABLED = 3,
  /** OTHER_REASON - Other unknown reason. */
  OTHER_REASON = 2,
  UNRECOGNIZED = -1,
}

export function tableMetadataCacheUsage_UnusedReasonFromJSON(object: any): TableMetadataCacheUsage_UnusedReason {
  switch (object) {
    case 0:
    case "UNUSED_REASON_UNSPECIFIED":
      return TableMetadataCacheUsage_UnusedReason.UNUSED_REASON_UNSPECIFIED;
    case 1:
    case "EXCEEDED_MAX_STALENESS":
      return TableMetadataCacheUsage_UnusedReason.EXCEEDED_MAX_STALENESS;
    case 3:
    case "METADATA_CACHING_NOT_ENABLED":
      return TableMetadataCacheUsage_UnusedReason.METADATA_CACHING_NOT_ENABLED;
    case 2:
    case "OTHER_REASON":
      return TableMetadataCacheUsage_UnusedReason.OTHER_REASON;
    case -1:
    case "UNRECOGNIZED":
    default:
      return TableMetadataCacheUsage_UnusedReason.UNRECOGNIZED;
  }
}

export function tableMetadataCacheUsage_UnusedReasonToJSON(object: TableMetadataCacheUsage_UnusedReason): string {
  switch (object) {
    case TableMetadataCacheUsage_UnusedReason.UNUSED_REASON_UNSPECIFIED:
      return "UNUSED_REASON_UNSPECIFIED";
    case TableMetadataCacheUsage_UnusedReason.EXCEEDED_MAX_STALENESS:
      return "EXCEEDED_MAX_STALENESS";
    case TableMetadataCacheUsage_UnusedReason.METADATA_CACHING_NOT_ENABLED:
      return "METADATA_CACHING_NOT_ENABLED";
    case TableMetadataCacheUsage_UnusedReason.OTHER_REASON:
      return "OTHER_REASON";
    case TableMetadataCacheUsage_UnusedReason.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Statistics for metadata caching in BigLake tables. */
export interface MetadataCacheStatistics {
  /** Set for the Metadata caching eligible tables referenced in the query. */
  tableMetadataCacheUsage: TableMetadataCacheUsage[];
}

function createBaseExplainQueryStep(): ExplainQueryStep {
  return { kind: "", substeps: [] };
}

export const ExplainQueryStep: MessageFns<ExplainQueryStep> = {
  encode(message: ExplainQueryStep, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kind !== "") {
      writer.uint32(10).string(message.kind);
    }
    for (const v of message.substeps) {
      writer.uint32(18).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplainQueryStep {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplainQueryStep();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kind = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.substeps.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplainQueryStep {
    return {
      kind: isSet(object.kind) ? globalThis.String(object.kind) : "",
      substeps: globalThis.Array.isArray(object?.substeps) ? object.substeps.map((e: any) => globalThis.String(e)) : [],
    };
  },

  toJSON(message: ExplainQueryStep): unknown {
    const obj: any = {};
    if (message.kind !== "") {
      obj.kind = message.kind;
    }
    if (message.substeps?.length) {
      obj.substeps = message.substeps;
    }
    return obj;
  },

  create(base?: DeepPartial<ExplainQueryStep>): ExplainQueryStep {
    return ExplainQueryStep.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplainQueryStep>): ExplainQueryStep {
    const message = createBaseExplainQueryStep();
    message.kind = object.kind ?? "";
    message.substeps = object.substeps?.map((e) => e) || [];
    return message;
  },
};

function createBaseExplainQueryStage(): ExplainQueryStage {
  return {
    name: "",
    id: undefined,
    startMs: Long.ZERO,
    endMs: Long.ZERO,
    inputStages: [],
    waitRatioAvg: undefined,
    waitMsAvg: undefined,
    waitRatioMax: undefined,
    waitMsMax: undefined,
    readRatioAvg: undefined,
    readMsAvg: undefined,
    readRatioMax: undefined,
    readMsMax: undefined,
    computeRatioAvg: undefined,
    computeMsAvg: undefined,
    computeRatioMax: undefined,
    computeMsMax: undefined,
    writeRatioAvg: undefined,
    writeMsAvg: undefined,
    writeRatioMax: undefined,
    writeMsMax: undefined,
    shuffleOutputBytes: undefined,
    shuffleOutputBytesSpilled: undefined,
    recordsRead: undefined,
    recordsWritten: undefined,
    parallelInputs: undefined,
    completedParallelInputs: undefined,
    status: "",
    steps: [],
    slotMs: undefined,
    computeMode: 0,
  };
}

export const ExplainQueryStage: MessageFns<ExplainQueryStage> = {
  encode(message: ExplainQueryStage, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.id !== undefined) {
      Int64Value.encode({ value: message.id! }, writer.uint32(18).fork()).join();
    }
    if (!message.startMs.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.startMs.toString());
    }
    if (!message.endMs.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.endMs.toString());
    }
    writer.uint32(42).fork();
    for (const v of message.inputStages) {
      writer.int64(v.toString());
    }
    writer.join();
    if (message.waitRatioAvg !== undefined) {
      DoubleValue.encode({ value: message.waitRatioAvg! }, writer.uint32(50).fork()).join();
    }
    if (message.waitMsAvg !== undefined) {
      Int64Value.encode({ value: message.waitMsAvg! }, writer.uint32(58).fork()).join();
    }
    if (message.waitRatioMax !== undefined) {
      DoubleValue.encode({ value: message.waitRatioMax! }, writer.uint32(66).fork()).join();
    }
    if (message.waitMsMax !== undefined) {
      Int64Value.encode({ value: message.waitMsMax! }, writer.uint32(74).fork()).join();
    }
    if (message.readRatioAvg !== undefined) {
      DoubleValue.encode({ value: message.readRatioAvg! }, writer.uint32(82).fork()).join();
    }
    if (message.readMsAvg !== undefined) {
      Int64Value.encode({ value: message.readMsAvg! }, writer.uint32(90).fork()).join();
    }
    if (message.readRatioMax !== undefined) {
      DoubleValue.encode({ value: message.readRatioMax! }, writer.uint32(98).fork()).join();
    }
    if (message.readMsMax !== undefined) {
      Int64Value.encode({ value: message.readMsMax! }, writer.uint32(106).fork()).join();
    }
    if (message.computeRatioAvg !== undefined) {
      DoubleValue.encode({ value: message.computeRatioAvg! }, writer.uint32(114).fork()).join();
    }
    if (message.computeMsAvg !== undefined) {
      Int64Value.encode({ value: message.computeMsAvg! }, writer.uint32(122).fork()).join();
    }
    if (message.computeRatioMax !== undefined) {
      DoubleValue.encode({ value: message.computeRatioMax! }, writer.uint32(130).fork()).join();
    }
    if (message.computeMsMax !== undefined) {
      Int64Value.encode({ value: message.computeMsMax! }, writer.uint32(138).fork()).join();
    }
    if (message.writeRatioAvg !== undefined) {
      DoubleValue.encode({ value: message.writeRatioAvg! }, writer.uint32(146).fork()).join();
    }
    if (message.writeMsAvg !== undefined) {
      Int64Value.encode({ value: message.writeMsAvg! }, writer.uint32(154).fork()).join();
    }
    if (message.writeRatioMax !== undefined) {
      DoubleValue.encode({ value: message.writeRatioMax! }, writer.uint32(162).fork()).join();
    }
    if (message.writeMsMax !== undefined) {
      Int64Value.encode({ value: message.writeMsMax! }, writer.uint32(170).fork()).join();
    }
    if (message.shuffleOutputBytes !== undefined) {
      Int64Value.encode({ value: message.shuffleOutputBytes! }, writer.uint32(178).fork()).join();
    }
    if (message.shuffleOutputBytesSpilled !== undefined) {
      Int64Value.encode({ value: message.shuffleOutputBytesSpilled! }, writer.uint32(186).fork()).join();
    }
    if (message.recordsRead !== undefined) {
      Int64Value.encode({ value: message.recordsRead! }, writer.uint32(194).fork()).join();
    }
    if (message.recordsWritten !== undefined) {
      Int64Value.encode({ value: message.recordsWritten! }, writer.uint32(202).fork()).join();
    }
    if (message.parallelInputs !== undefined) {
      Int64Value.encode({ value: message.parallelInputs! }, writer.uint32(210).fork()).join();
    }
    if (message.completedParallelInputs !== undefined) {
      Int64Value.encode({ value: message.completedParallelInputs! }, writer.uint32(218).fork()).join();
    }
    if (message.status !== "") {
      writer.uint32(226).string(message.status);
    }
    for (const v of message.steps) {
      ExplainQueryStep.encode(v!, writer.uint32(234).fork()).join();
    }
    if (message.slotMs !== undefined) {
      Int64Value.encode({ value: message.slotMs! }, writer.uint32(242).fork()).join();
    }
    if (message.computeMode !== 0) {
      writer.uint32(248).int32(message.computeMode);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExplainQueryStage {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExplainQueryStage();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.id = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.startMs = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.endMs = Long.fromString(reader.int64().toString());
          continue;
        case 5:
          if (tag === 40) {
            message.inputStages.push(Long.fromString(reader.int64().toString()));

            continue;
          }

          if (tag === 42) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.inputStages.push(Long.fromString(reader.int64().toString()));
            }

            continue;
          }

          break;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.waitRatioAvg = DoubleValue.decode(reader, reader.uint32()).value;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.waitMsAvg = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.waitRatioMax = DoubleValue.decode(reader, reader.uint32()).value;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.waitMsMax = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.readRatioAvg = DoubleValue.decode(reader, reader.uint32()).value;
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.readMsAvg = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.readRatioMax = DoubleValue.decode(reader, reader.uint32()).value;
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.readMsMax = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.computeRatioAvg = DoubleValue.decode(reader, reader.uint32()).value;
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.computeMsAvg = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.computeRatioMax = DoubleValue.decode(reader, reader.uint32()).value;
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.computeMsMax = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.writeRatioAvg = DoubleValue.decode(reader, reader.uint32()).value;
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.writeMsAvg = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.writeRatioMax = DoubleValue.decode(reader, reader.uint32()).value;
          continue;
        case 21:
          if (tag !== 170) {
            break;
          }

          message.writeMsMax = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 22:
          if (tag !== 178) {
            break;
          }

          message.shuffleOutputBytes = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.shuffleOutputBytesSpilled = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 24:
          if (tag !== 194) {
            break;
          }

          message.recordsRead = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 25:
          if (tag !== 202) {
            break;
          }

          message.recordsWritten = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 26:
          if (tag !== 210) {
            break;
          }

          message.parallelInputs = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 27:
          if (tag !== 218) {
            break;
          }

          message.completedParallelInputs = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 28:
          if (tag !== 226) {
            break;
          }

          message.status = reader.string();
          continue;
        case 29:
          if (tag !== 234) {
            break;
          }

          message.steps.push(ExplainQueryStep.decode(reader, reader.uint32()));
          continue;
        case 30:
          if (tag !== 242) {
            break;
          }

          message.slotMs = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 31:
          if (tag !== 248) {
            break;
          }

          message.computeMode = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExplainQueryStage {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      id: isSet(object.id) ? Long.fromValue(object.id) : undefined,
      startMs: isSet(object.startMs) ? Long.fromValue(object.startMs) : Long.ZERO,
      endMs: isSet(object.endMs) ? Long.fromValue(object.endMs) : Long.ZERO,
      inputStages: globalThis.Array.isArray(object?.inputStages)
        ? object.inputStages.map((e: any) => Long.fromValue(e))
        : [],
      waitRatioAvg: isSet(object.waitRatioAvg) ? Number(object.waitRatioAvg) : undefined,
      waitMsAvg: isSet(object.waitMsAvg) ? Long.fromValue(object.waitMsAvg) : undefined,
      waitRatioMax: isSet(object.waitRatioMax) ? Number(object.waitRatioMax) : undefined,
      waitMsMax: isSet(object.waitMsMax) ? Long.fromValue(object.waitMsMax) : undefined,
      readRatioAvg: isSet(object.readRatioAvg) ? Number(object.readRatioAvg) : undefined,
      readMsAvg: isSet(object.readMsAvg) ? Long.fromValue(object.readMsAvg) : undefined,
      readRatioMax: isSet(object.readRatioMax) ? Number(object.readRatioMax) : undefined,
      readMsMax: isSet(object.readMsMax) ? Long.fromValue(object.readMsMax) : undefined,
      computeRatioAvg: isSet(object.computeRatioAvg) ? Number(object.computeRatioAvg) : undefined,
      computeMsAvg: isSet(object.computeMsAvg) ? Long.fromValue(object.computeMsAvg) : undefined,
      computeRatioMax: isSet(object.computeRatioMax) ? Number(object.computeRatioMax) : undefined,
      computeMsMax: isSet(object.computeMsMax) ? Long.fromValue(object.computeMsMax) : undefined,
      writeRatioAvg: isSet(object.writeRatioAvg) ? Number(object.writeRatioAvg) : undefined,
      writeMsAvg: isSet(object.writeMsAvg) ? Long.fromValue(object.writeMsAvg) : undefined,
      writeRatioMax: isSet(object.writeRatioMax) ? Number(object.writeRatioMax) : undefined,
      writeMsMax: isSet(object.writeMsMax) ? Long.fromValue(object.writeMsMax) : undefined,
      shuffleOutputBytes: isSet(object.shuffleOutputBytes) ? Long.fromValue(object.shuffleOutputBytes) : undefined,
      shuffleOutputBytesSpilled: isSet(object.shuffleOutputBytesSpilled)
        ? Long.fromValue(object.shuffleOutputBytesSpilled)
        : undefined,
      recordsRead: isSet(object.recordsRead) ? Long.fromValue(object.recordsRead) : undefined,
      recordsWritten: isSet(object.recordsWritten) ? Long.fromValue(object.recordsWritten) : undefined,
      parallelInputs: isSet(object.parallelInputs) ? Long.fromValue(object.parallelInputs) : undefined,
      completedParallelInputs: isSet(object.completedParallelInputs)
        ? Long.fromValue(object.completedParallelInputs)
        : undefined,
      status: isSet(object.status) ? globalThis.String(object.status) : "",
      steps: globalThis.Array.isArray(object?.steps) ? object.steps.map((e: any) => ExplainQueryStep.fromJSON(e)) : [],
      slotMs: isSet(object.slotMs) ? Long.fromValue(object.slotMs) : undefined,
      computeMode: isSet(object.computeMode) ? explainQueryStage_ComputeModeFromJSON(object.computeMode) : 0,
    };
  },

  toJSON(message: ExplainQueryStage): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.id !== undefined) {
      obj.id = message.id;
    }
    if (!message.startMs.equals(Long.ZERO)) {
      obj.startMs = (message.startMs || Long.ZERO).toString();
    }
    if (!message.endMs.equals(Long.ZERO)) {
      obj.endMs = (message.endMs || Long.ZERO).toString();
    }
    if (message.inputStages?.length) {
      obj.inputStages = message.inputStages.map((e) => (e || Long.ZERO).toString());
    }
    if (message.waitRatioAvg !== undefined) {
      obj.waitRatioAvg = message.waitRatioAvg;
    }
    if (message.waitMsAvg !== undefined) {
      obj.waitMsAvg = message.waitMsAvg;
    }
    if (message.waitRatioMax !== undefined) {
      obj.waitRatioMax = message.waitRatioMax;
    }
    if (message.waitMsMax !== undefined) {
      obj.waitMsMax = message.waitMsMax;
    }
    if (message.readRatioAvg !== undefined) {
      obj.readRatioAvg = message.readRatioAvg;
    }
    if (message.readMsAvg !== undefined) {
      obj.readMsAvg = message.readMsAvg;
    }
    if (message.readRatioMax !== undefined) {
      obj.readRatioMax = message.readRatioMax;
    }
    if (message.readMsMax !== undefined) {
      obj.readMsMax = message.readMsMax;
    }
    if (message.computeRatioAvg !== undefined) {
      obj.computeRatioAvg = message.computeRatioAvg;
    }
    if (message.computeMsAvg !== undefined) {
      obj.computeMsAvg = message.computeMsAvg;
    }
    if (message.computeRatioMax !== undefined) {
      obj.computeRatioMax = message.computeRatioMax;
    }
    if (message.computeMsMax !== undefined) {
      obj.computeMsMax = message.computeMsMax;
    }
    if (message.writeRatioAvg !== undefined) {
      obj.writeRatioAvg = message.writeRatioAvg;
    }
    if (message.writeMsAvg !== undefined) {
      obj.writeMsAvg = message.writeMsAvg;
    }
    if (message.writeRatioMax !== undefined) {
      obj.writeRatioMax = message.writeRatioMax;
    }
    if (message.writeMsMax !== undefined) {
      obj.writeMsMax = message.writeMsMax;
    }
    if (message.shuffleOutputBytes !== undefined) {
      obj.shuffleOutputBytes = message.shuffleOutputBytes;
    }
    if (message.shuffleOutputBytesSpilled !== undefined) {
      obj.shuffleOutputBytesSpilled = message.shuffleOutputBytesSpilled;
    }
    if (message.recordsRead !== undefined) {
      obj.recordsRead = message.recordsRead;
    }
    if (message.recordsWritten !== undefined) {
      obj.recordsWritten = message.recordsWritten;
    }
    if (message.parallelInputs !== undefined) {
      obj.parallelInputs = message.parallelInputs;
    }
    if (message.completedParallelInputs !== undefined) {
      obj.completedParallelInputs = message.completedParallelInputs;
    }
    if (message.status !== "") {
      obj.status = message.status;
    }
    if (message.steps?.length) {
      obj.steps = message.steps.map((e) => ExplainQueryStep.toJSON(e));
    }
    if (message.slotMs !== undefined) {
      obj.slotMs = message.slotMs;
    }
    if (message.computeMode !== 0) {
      obj.computeMode = explainQueryStage_ComputeModeToJSON(message.computeMode);
    }
    return obj;
  },

  create(base?: DeepPartial<ExplainQueryStage>): ExplainQueryStage {
    return ExplainQueryStage.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExplainQueryStage>): ExplainQueryStage {
    const message = createBaseExplainQueryStage();
    message.name = object.name ?? "";
    message.id = (object.id !== undefined && object.id !== null) ? Long.fromValue(object.id) : undefined;
    message.startMs = (object.startMs !== undefined && object.startMs !== null)
      ? Long.fromValue(object.startMs)
      : Long.ZERO;
    message.endMs = (object.endMs !== undefined && object.endMs !== null) ? Long.fromValue(object.endMs) : Long.ZERO;
    message.inputStages = object.inputStages?.map((e) => Long.fromValue(e)) || [];
    message.waitRatioAvg = object.waitRatioAvg ?? undefined;
    message.waitMsAvg = (object.waitMsAvg !== undefined && object.waitMsAvg !== null)
      ? Long.fromValue(object.waitMsAvg)
      : undefined;
    message.waitRatioMax = object.waitRatioMax ?? undefined;
    message.waitMsMax = (object.waitMsMax !== undefined && object.waitMsMax !== null)
      ? Long.fromValue(object.waitMsMax)
      : undefined;
    message.readRatioAvg = object.readRatioAvg ?? undefined;
    message.readMsAvg = (object.readMsAvg !== undefined && object.readMsAvg !== null)
      ? Long.fromValue(object.readMsAvg)
      : undefined;
    message.readRatioMax = object.readRatioMax ?? undefined;
    message.readMsMax = (object.readMsMax !== undefined && object.readMsMax !== null)
      ? Long.fromValue(object.readMsMax)
      : undefined;
    message.computeRatioAvg = object.computeRatioAvg ?? undefined;
    message.computeMsAvg = (object.computeMsAvg !== undefined && object.computeMsAvg !== null)
      ? Long.fromValue(object.computeMsAvg)
      : undefined;
    message.computeRatioMax = object.computeRatioMax ?? undefined;
    message.computeMsMax = (object.computeMsMax !== undefined && object.computeMsMax !== null)
      ? Long.fromValue(object.computeMsMax)
      : undefined;
    message.writeRatioAvg = object.writeRatioAvg ?? undefined;
    message.writeMsAvg = (object.writeMsAvg !== undefined && object.writeMsAvg !== null)
      ? Long.fromValue(object.writeMsAvg)
      : undefined;
    message.writeRatioMax = object.writeRatioMax ?? undefined;
    message.writeMsMax = (object.writeMsMax !== undefined && object.writeMsMax !== null)
      ? Long.fromValue(object.writeMsMax)
      : undefined;
    message.shuffleOutputBytes = (object.shuffleOutputBytes !== undefined && object.shuffleOutputBytes !== null)
      ? Long.fromValue(object.shuffleOutputBytes)
      : undefined;
    message.shuffleOutputBytesSpilled =
      (object.shuffleOutputBytesSpilled !== undefined && object.shuffleOutputBytesSpilled !== null)
        ? Long.fromValue(object.shuffleOutputBytesSpilled)
        : undefined;
    message.recordsRead = (object.recordsRead !== undefined && object.recordsRead !== null)
      ? Long.fromValue(object.recordsRead)
      : undefined;
    message.recordsWritten = (object.recordsWritten !== undefined && object.recordsWritten !== null)
      ? Long.fromValue(object.recordsWritten)
      : undefined;
    message.parallelInputs = (object.parallelInputs !== undefined && object.parallelInputs !== null)
      ? Long.fromValue(object.parallelInputs)
      : undefined;
    message.completedParallelInputs =
      (object.completedParallelInputs !== undefined && object.completedParallelInputs !== null)
        ? Long.fromValue(object.completedParallelInputs)
        : undefined;
    message.status = object.status ?? "";
    message.steps = object.steps?.map((e) => ExplainQueryStep.fromPartial(e)) || [];
    message.slotMs = (object.slotMs !== undefined && object.slotMs !== null)
      ? Long.fromValue(object.slotMs)
      : undefined;
    message.computeMode = object.computeMode ?? 0;
    return message;
  },
};

function createBaseQueryTimelineSample(): QueryTimelineSample {
  return {
    elapsedMs: undefined,
    totalSlotMs: undefined,
    pendingUnits: undefined,
    completedUnits: undefined,
    activeUnits: undefined,
    estimatedRunnableUnits: undefined,
  };
}

export const QueryTimelineSample: MessageFns<QueryTimelineSample> = {
  encode(message: QueryTimelineSample, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.elapsedMs !== undefined) {
      Int64Value.encode({ value: message.elapsedMs! }, writer.uint32(10).fork()).join();
    }
    if (message.totalSlotMs !== undefined) {
      Int64Value.encode({ value: message.totalSlotMs! }, writer.uint32(18).fork()).join();
    }
    if (message.pendingUnits !== undefined) {
      Int64Value.encode({ value: message.pendingUnits! }, writer.uint32(26).fork()).join();
    }
    if (message.completedUnits !== undefined) {
      Int64Value.encode({ value: message.completedUnits! }, writer.uint32(34).fork()).join();
    }
    if (message.activeUnits !== undefined) {
      Int64Value.encode({ value: message.activeUnits! }, writer.uint32(42).fork()).join();
    }
    if (message.estimatedRunnableUnits !== undefined) {
      Int64Value.encode({ value: message.estimatedRunnableUnits! }, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryTimelineSample {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryTimelineSample();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.elapsedMs = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.totalSlotMs = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pendingUnits = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.completedUnits = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.activeUnits = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.estimatedRunnableUnits = Int64Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryTimelineSample {
    return {
      elapsedMs: isSet(object.elapsedMs) ? Long.fromValue(object.elapsedMs) : undefined,
      totalSlotMs: isSet(object.totalSlotMs) ? Long.fromValue(object.totalSlotMs) : undefined,
      pendingUnits: isSet(object.pendingUnits) ? Long.fromValue(object.pendingUnits) : undefined,
      completedUnits: isSet(object.completedUnits) ? Long.fromValue(object.completedUnits) : undefined,
      activeUnits: isSet(object.activeUnits) ? Long.fromValue(object.activeUnits) : undefined,
      estimatedRunnableUnits: isSet(object.estimatedRunnableUnits)
        ? Long.fromValue(object.estimatedRunnableUnits)
        : undefined,
    };
  },

  toJSON(message: QueryTimelineSample): unknown {
    const obj: any = {};
    if (message.elapsedMs !== undefined) {
      obj.elapsedMs = message.elapsedMs;
    }
    if (message.totalSlotMs !== undefined) {
      obj.totalSlotMs = message.totalSlotMs;
    }
    if (message.pendingUnits !== undefined) {
      obj.pendingUnits = message.pendingUnits;
    }
    if (message.completedUnits !== undefined) {
      obj.completedUnits = message.completedUnits;
    }
    if (message.activeUnits !== undefined) {
      obj.activeUnits = message.activeUnits;
    }
    if (message.estimatedRunnableUnits !== undefined) {
      obj.estimatedRunnableUnits = message.estimatedRunnableUnits;
    }
    return obj;
  },

  create(base?: DeepPartial<QueryTimelineSample>): QueryTimelineSample {
    return QueryTimelineSample.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<QueryTimelineSample>): QueryTimelineSample {
    const message = createBaseQueryTimelineSample();
    message.elapsedMs = (object.elapsedMs !== undefined && object.elapsedMs !== null)
      ? Long.fromValue(object.elapsedMs)
      : undefined;
    message.totalSlotMs = (object.totalSlotMs !== undefined && object.totalSlotMs !== null)
      ? Long.fromValue(object.totalSlotMs)
      : undefined;
    message.pendingUnits = (object.pendingUnits !== undefined && object.pendingUnits !== null)
      ? Long.fromValue(object.pendingUnits)
      : undefined;
    message.completedUnits = (object.completedUnits !== undefined && object.completedUnits !== null)
      ? Long.fromValue(object.completedUnits)
      : undefined;
    message.activeUnits = (object.activeUnits !== undefined && object.activeUnits !== null)
      ? Long.fromValue(object.activeUnits)
      : undefined;
    message.estimatedRunnableUnits =
      (object.estimatedRunnableUnits !== undefined && object.estimatedRunnableUnits !== null)
        ? Long.fromValue(object.estimatedRunnableUnits)
        : undefined;
    return message;
  },
};

function createBaseExternalServiceCost(): ExternalServiceCost {
  return {
    externalService: "",
    bytesProcessed: undefined,
    bytesBilled: undefined,
    slotMs: undefined,
    reservedSlotCount: Long.ZERO,
  };
}

export const ExternalServiceCost: MessageFns<ExternalServiceCost> = {
  encode(message: ExternalServiceCost, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.externalService !== "") {
      writer.uint32(10).string(message.externalService);
    }
    if (message.bytesProcessed !== undefined) {
      Int64Value.encode({ value: message.bytesProcessed! }, writer.uint32(18).fork()).join();
    }
    if (message.bytesBilled !== undefined) {
      Int64Value.encode({ value: message.bytesBilled! }, writer.uint32(26).fork()).join();
    }
    if (message.slotMs !== undefined) {
      Int64Value.encode({ value: message.slotMs! }, writer.uint32(34).fork()).join();
    }
    if (!message.reservedSlotCount.equals(Long.ZERO)) {
      writer.uint32(40).int64(message.reservedSlotCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExternalServiceCost {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExternalServiceCost();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.externalService = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.bytesProcessed = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.bytesBilled = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.slotMs = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.reservedSlotCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExternalServiceCost {
    return {
      externalService: isSet(object.externalService) ? globalThis.String(object.externalService) : "",
      bytesProcessed: isSet(object.bytesProcessed) ? Long.fromValue(object.bytesProcessed) : undefined,
      bytesBilled: isSet(object.bytesBilled) ? Long.fromValue(object.bytesBilled) : undefined,
      slotMs: isSet(object.slotMs) ? Long.fromValue(object.slotMs) : undefined,
      reservedSlotCount: isSet(object.reservedSlotCount) ? Long.fromValue(object.reservedSlotCount) : Long.ZERO,
    };
  },

  toJSON(message: ExternalServiceCost): unknown {
    const obj: any = {};
    if (message.externalService !== "") {
      obj.externalService = message.externalService;
    }
    if (message.bytesProcessed !== undefined) {
      obj.bytesProcessed = message.bytesProcessed;
    }
    if (message.bytesBilled !== undefined) {
      obj.bytesBilled = message.bytesBilled;
    }
    if (message.slotMs !== undefined) {
      obj.slotMs = message.slotMs;
    }
    if (!message.reservedSlotCount.equals(Long.ZERO)) {
      obj.reservedSlotCount = (message.reservedSlotCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ExternalServiceCost>): ExternalServiceCost {
    return ExternalServiceCost.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExternalServiceCost>): ExternalServiceCost {
    const message = createBaseExternalServiceCost();
    message.externalService = object.externalService ?? "";
    message.bytesProcessed = (object.bytesProcessed !== undefined && object.bytesProcessed !== null)
      ? Long.fromValue(object.bytesProcessed)
      : undefined;
    message.bytesBilled = (object.bytesBilled !== undefined && object.bytesBilled !== null)
      ? Long.fromValue(object.bytesBilled)
      : undefined;
    message.slotMs = (object.slotMs !== undefined && object.slotMs !== null)
      ? Long.fromValue(object.slotMs)
      : undefined;
    message.reservedSlotCount = (object.reservedSlotCount !== undefined && object.reservedSlotCount !== null)
      ? Long.fromValue(object.reservedSlotCount)
      : Long.ZERO;
    return message;
  },
};

function createBaseExportDataStatistics(): ExportDataStatistics {
  return { fileCount: undefined, rowCount: undefined };
}

export const ExportDataStatistics: MessageFns<ExportDataStatistics> = {
  encode(message: ExportDataStatistics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.fileCount !== undefined) {
      Int64Value.encode({ value: message.fileCount! }, writer.uint32(10).fork()).join();
    }
    if (message.rowCount !== undefined) {
      Int64Value.encode({ value: message.rowCount! }, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportDataStatistics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportDataStatistics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.fileCount = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.rowCount = Int64Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportDataStatistics {
    return {
      fileCount: isSet(object.fileCount) ? Long.fromValue(object.fileCount) : undefined,
      rowCount: isSet(object.rowCount) ? Long.fromValue(object.rowCount) : undefined,
    };
  },

  toJSON(message: ExportDataStatistics): unknown {
    const obj: any = {};
    if (message.fileCount !== undefined) {
      obj.fileCount = message.fileCount;
    }
    if (message.rowCount !== undefined) {
      obj.rowCount = message.rowCount;
    }
    return obj;
  },

  create(base?: DeepPartial<ExportDataStatistics>): ExportDataStatistics {
    return ExportDataStatistics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportDataStatistics>): ExportDataStatistics {
    const message = createBaseExportDataStatistics();
    message.fileCount = (object.fileCount !== undefined && object.fileCount !== null)
      ? Long.fromValue(object.fileCount)
      : undefined;
    message.rowCount = (object.rowCount !== undefined && object.rowCount !== null)
      ? Long.fromValue(object.rowCount)
      : undefined;
    return message;
  },
};

function createBaseBiEngineReason(): BiEngineReason {
  return { code: 0, message: "" };
}

export const BiEngineReason: MessageFns<BiEngineReason> = {
  encode(message: BiEngineReason, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.code !== 0) {
      writer.uint32(8).int32(message.code);
    }
    if (message.message !== "") {
      writer.uint32(18).string(message.message);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BiEngineReason {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBiEngineReason();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.code = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.message = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BiEngineReason {
    return {
      code: isSet(object.code) ? biEngineReason_CodeFromJSON(object.code) : 0,
      message: isSet(object.message) ? globalThis.String(object.message) : "",
    };
  },

  toJSON(message: BiEngineReason): unknown {
    const obj: any = {};
    if (message.code !== 0) {
      obj.code = biEngineReason_CodeToJSON(message.code);
    }
    if (message.message !== "") {
      obj.message = message.message;
    }
    return obj;
  },

  create(base?: DeepPartial<BiEngineReason>): BiEngineReason {
    return BiEngineReason.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BiEngineReason>): BiEngineReason {
    const message = createBaseBiEngineReason();
    message.code = object.code ?? 0;
    message.message = object.message ?? "";
    return message;
  },
};

function createBaseBiEngineStatistics(): BiEngineStatistics {
  return { biEngineMode: 0, accelerationMode: 0, biEngineReasons: [] };
}

export const BiEngineStatistics: MessageFns<BiEngineStatistics> = {
  encode(message: BiEngineStatistics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.biEngineMode !== 0) {
      writer.uint32(8).int32(message.biEngineMode);
    }
    if (message.accelerationMode !== 0) {
      writer.uint32(24).int32(message.accelerationMode);
    }
    for (const v of message.biEngineReasons) {
      BiEngineReason.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BiEngineStatistics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBiEngineStatistics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.biEngineMode = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.accelerationMode = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.biEngineReasons.push(BiEngineReason.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BiEngineStatistics {
    return {
      biEngineMode: isSet(object.biEngineMode) ? biEngineStatistics_BiEngineModeFromJSON(object.biEngineMode) : 0,
      accelerationMode: isSet(object.accelerationMode)
        ? biEngineStatistics_BiEngineAccelerationModeFromJSON(object.accelerationMode)
        : 0,
      biEngineReasons: globalThis.Array.isArray(object?.biEngineReasons)
        ? object.biEngineReasons.map((e: any) => BiEngineReason.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BiEngineStatistics): unknown {
    const obj: any = {};
    if (message.biEngineMode !== 0) {
      obj.biEngineMode = biEngineStatistics_BiEngineModeToJSON(message.biEngineMode);
    }
    if (message.accelerationMode !== 0) {
      obj.accelerationMode = biEngineStatistics_BiEngineAccelerationModeToJSON(message.accelerationMode);
    }
    if (message.biEngineReasons?.length) {
      obj.biEngineReasons = message.biEngineReasons.map((e) => BiEngineReason.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BiEngineStatistics>): BiEngineStatistics {
    return BiEngineStatistics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BiEngineStatistics>): BiEngineStatistics {
    const message = createBaseBiEngineStatistics();
    message.biEngineMode = object.biEngineMode ?? 0;
    message.accelerationMode = object.accelerationMode ?? 0;
    message.biEngineReasons = object.biEngineReasons?.map((e) => BiEngineReason.fromPartial(e)) || [];
    return message;
  },
};

function createBaseIndexUnusedReason(): IndexUnusedReason {
  return { code: undefined, message: undefined, baseTable: undefined, indexName: undefined };
}

export const IndexUnusedReason: MessageFns<IndexUnusedReason> = {
  encode(message: IndexUnusedReason, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.code !== undefined) {
      writer.uint32(8).int32(message.code);
    }
    if (message.message !== undefined) {
      writer.uint32(18).string(message.message);
    }
    if (message.baseTable !== undefined) {
      TableReference.encode(message.baseTable, writer.uint32(26).fork()).join();
    }
    if (message.indexName !== undefined) {
      writer.uint32(34).string(message.indexName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): IndexUnusedReason {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseIndexUnusedReason();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.code = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.message = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.baseTable = TableReference.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.indexName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): IndexUnusedReason {
    return {
      code: isSet(object.code) ? indexUnusedReason_CodeFromJSON(object.code) : undefined,
      message: isSet(object.message) ? globalThis.String(object.message) : undefined,
      baseTable: isSet(object.baseTable) ? TableReference.fromJSON(object.baseTable) : undefined,
      indexName: isSet(object.indexName) ? globalThis.String(object.indexName) : undefined,
    };
  },

  toJSON(message: IndexUnusedReason): unknown {
    const obj: any = {};
    if (message.code !== undefined) {
      obj.code = indexUnusedReason_CodeToJSON(message.code);
    }
    if (message.message !== undefined) {
      obj.message = message.message;
    }
    if (message.baseTable !== undefined) {
      obj.baseTable = TableReference.toJSON(message.baseTable);
    }
    if (message.indexName !== undefined) {
      obj.indexName = message.indexName;
    }
    return obj;
  },

  create(base?: DeepPartial<IndexUnusedReason>): IndexUnusedReason {
    return IndexUnusedReason.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<IndexUnusedReason>): IndexUnusedReason {
    const message = createBaseIndexUnusedReason();
    message.code = object.code ?? undefined;
    message.message = object.message ?? undefined;
    message.baseTable = (object.baseTable !== undefined && object.baseTable !== null)
      ? TableReference.fromPartial(object.baseTable)
      : undefined;
    message.indexName = object.indexName ?? undefined;
    return message;
  },
};

function createBaseSearchStatistics(): SearchStatistics {
  return { indexUsageMode: 0, indexUnusedReasons: [] };
}

export const SearchStatistics: MessageFns<SearchStatistics> = {
  encode(message: SearchStatistics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.indexUsageMode !== 0) {
      writer.uint32(8).int32(message.indexUsageMode);
    }
    for (const v of message.indexUnusedReasons) {
      IndexUnusedReason.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SearchStatistics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSearchStatistics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.indexUsageMode = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.indexUnusedReasons.push(IndexUnusedReason.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SearchStatistics {
    return {
      indexUsageMode: isSet(object.indexUsageMode) ? searchStatistics_IndexUsageModeFromJSON(object.indexUsageMode) : 0,
      indexUnusedReasons: globalThis.Array.isArray(object?.indexUnusedReasons)
        ? object.indexUnusedReasons.map((e: any) => IndexUnusedReason.fromJSON(e))
        : [],
    };
  },

  toJSON(message: SearchStatistics): unknown {
    const obj: any = {};
    if (message.indexUsageMode !== 0) {
      obj.indexUsageMode = searchStatistics_IndexUsageModeToJSON(message.indexUsageMode);
    }
    if (message.indexUnusedReasons?.length) {
      obj.indexUnusedReasons = message.indexUnusedReasons.map((e) => IndexUnusedReason.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<SearchStatistics>): SearchStatistics {
    return SearchStatistics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SearchStatistics>): SearchStatistics {
    const message = createBaseSearchStatistics();
    message.indexUsageMode = object.indexUsageMode ?? 0;
    message.indexUnusedReasons = object.indexUnusedReasons?.map((e) => IndexUnusedReason.fromPartial(e)) || [];
    return message;
  },
};

function createBaseVectorSearchStatistics(): VectorSearchStatistics {
  return { indexUsageMode: 0, indexUnusedReasons: [] };
}

export const VectorSearchStatistics: MessageFns<VectorSearchStatistics> = {
  encode(message: VectorSearchStatistics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.indexUsageMode !== 0) {
      writer.uint32(8).int32(message.indexUsageMode);
    }
    for (const v of message.indexUnusedReasons) {
      IndexUnusedReason.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VectorSearchStatistics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVectorSearchStatistics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.indexUsageMode = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.indexUnusedReasons.push(IndexUnusedReason.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VectorSearchStatistics {
    return {
      indexUsageMode: isSet(object.indexUsageMode)
        ? vectorSearchStatistics_IndexUsageModeFromJSON(object.indexUsageMode)
        : 0,
      indexUnusedReasons: globalThis.Array.isArray(object?.indexUnusedReasons)
        ? object.indexUnusedReasons.map((e: any) => IndexUnusedReason.fromJSON(e))
        : [],
    };
  },

  toJSON(message: VectorSearchStatistics): unknown {
    const obj: any = {};
    if (message.indexUsageMode !== 0) {
      obj.indexUsageMode = vectorSearchStatistics_IndexUsageModeToJSON(message.indexUsageMode);
    }
    if (message.indexUnusedReasons?.length) {
      obj.indexUnusedReasons = message.indexUnusedReasons.map((e) => IndexUnusedReason.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<VectorSearchStatistics>): VectorSearchStatistics {
    return VectorSearchStatistics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VectorSearchStatistics>): VectorSearchStatistics {
    const message = createBaseVectorSearchStatistics();
    message.indexUsageMode = object.indexUsageMode ?? 0;
    message.indexUnusedReasons = object.indexUnusedReasons?.map((e) => IndexUnusedReason.fromPartial(e)) || [];
    return message;
  },
};

function createBaseQueryInfo(): QueryInfo {
  return { optimizationDetails: undefined };
}

export const QueryInfo: MessageFns<QueryInfo> = {
  encode(message: QueryInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.optimizationDetails !== undefined) {
      Struct.encode(Struct.wrap(message.optimizationDetails), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.optimizationDetails = Struct.unwrap(Struct.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryInfo {
    return { optimizationDetails: isObject(object.optimizationDetails) ? object.optimizationDetails : undefined };
  },

  toJSON(message: QueryInfo): unknown {
    const obj: any = {};
    if (message.optimizationDetails !== undefined) {
      obj.optimizationDetails = message.optimizationDetails;
    }
    return obj;
  },

  create(base?: DeepPartial<QueryInfo>): QueryInfo {
    return QueryInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<QueryInfo>): QueryInfo {
    const message = createBaseQueryInfo();
    message.optimizationDetails = object.optimizationDetails ?? undefined;
    return message;
  },
};

function createBaseLoadQueryStatistics(): LoadQueryStatistics {
  return {
    inputFiles: undefined,
    inputFileBytes: undefined,
    outputRows: undefined,
    outputBytes: undefined,
    badRecords: undefined,
  };
}

export const LoadQueryStatistics: MessageFns<LoadQueryStatistics> = {
  encode(message: LoadQueryStatistics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputFiles !== undefined) {
      Int64Value.encode({ value: message.inputFiles! }, writer.uint32(10).fork()).join();
    }
    if (message.inputFileBytes !== undefined) {
      Int64Value.encode({ value: message.inputFileBytes! }, writer.uint32(18).fork()).join();
    }
    if (message.outputRows !== undefined) {
      Int64Value.encode({ value: message.outputRows! }, writer.uint32(26).fork()).join();
    }
    if (message.outputBytes !== undefined) {
      Int64Value.encode({ value: message.outputBytes! }, writer.uint32(34).fork()).join();
    }
    if (message.badRecords !== undefined) {
      Int64Value.encode({ value: message.badRecords! }, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LoadQueryStatistics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLoadQueryStatistics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputFiles = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inputFileBytes = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.outputRows = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.outputBytes = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.badRecords = Int64Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LoadQueryStatistics {
    return {
      inputFiles: isSet(object.inputFiles) ? Long.fromValue(object.inputFiles) : undefined,
      inputFileBytes: isSet(object.inputFileBytes) ? Long.fromValue(object.inputFileBytes) : undefined,
      outputRows: isSet(object.outputRows) ? Long.fromValue(object.outputRows) : undefined,
      outputBytes: isSet(object.outputBytes) ? Long.fromValue(object.outputBytes) : undefined,
      badRecords: isSet(object.badRecords) ? Long.fromValue(object.badRecords) : undefined,
    };
  },

  toJSON(message: LoadQueryStatistics): unknown {
    const obj: any = {};
    if (message.inputFiles !== undefined) {
      obj.inputFiles = message.inputFiles;
    }
    if (message.inputFileBytes !== undefined) {
      obj.inputFileBytes = message.inputFileBytes;
    }
    if (message.outputRows !== undefined) {
      obj.outputRows = message.outputRows;
    }
    if (message.outputBytes !== undefined) {
      obj.outputBytes = message.outputBytes;
    }
    if (message.badRecords !== undefined) {
      obj.badRecords = message.badRecords;
    }
    return obj;
  },

  create(base?: DeepPartial<LoadQueryStatistics>): LoadQueryStatistics {
    return LoadQueryStatistics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LoadQueryStatistics>): LoadQueryStatistics {
    const message = createBaseLoadQueryStatistics();
    message.inputFiles = (object.inputFiles !== undefined && object.inputFiles !== null)
      ? Long.fromValue(object.inputFiles)
      : undefined;
    message.inputFileBytes = (object.inputFileBytes !== undefined && object.inputFileBytes !== null)
      ? Long.fromValue(object.inputFileBytes)
      : undefined;
    message.outputRows = (object.outputRows !== undefined && object.outputRows !== null)
      ? Long.fromValue(object.outputRows)
      : undefined;
    message.outputBytes = (object.outputBytes !== undefined && object.outputBytes !== null)
      ? Long.fromValue(object.outputBytes)
      : undefined;
    message.badRecords = (object.badRecords !== undefined && object.badRecords !== null)
      ? Long.fromValue(object.badRecords)
      : undefined;
    return message;
  },
};

function createBaseJobStatistics2(): JobStatistics2 {
  return {
    queryPlan: [],
    estimatedBytesProcessed: undefined,
    timeline: [],
    totalPartitionsProcessed: undefined,
    totalBytesProcessed: undefined,
    totalBytesProcessedAccuracy: "",
    totalBytesBilled: undefined,
    billingTier: undefined,
    totalSlotMs: undefined,
    cacheHit: undefined,
    referencedTables: [],
    referencedRoutines: [],
    schema: undefined,
    numDmlAffectedRows: undefined,
    dmlStats: undefined,
    undeclaredQueryParameters: [],
    statementType: "",
    ddlOperationPerformed: "",
    ddlTargetTable: undefined,
    ddlDestinationTable: undefined,
    ddlTargetRowAccessPolicy: undefined,
    ddlAffectedRowAccessPolicyCount: undefined,
    ddlTargetRoutine: undefined,
    ddlTargetDataset: undefined,
    mlStatistics: undefined,
    exportDataStatistics: undefined,
    externalServiceCosts: [],
    biEngineStatistics: undefined,
    loadQueryStatistics: undefined,
    dclTargetTable: undefined,
    dclTargetView: undefined,
    dclTargetDataset: undefined,
    searchStatistics: undefined,
    vectorSearchStatistics: undefined,
    performanceInsights: undefined,
    queryInfo: undefined,
    sparkStatistics: undefined,
    transferredBytes: undefined,
    materializedViewStatistics: undefined,
    metadataCacheStatistics: undefined,
  };
}

export const JobStatistics2: MessageFns<JobStatistics2> = {
  encode(message: JobStatistics2, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.queryPlan) {
      ExplainQueryStage.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.estimatedBytesProcessed !== undefined) {
      Int64Value.encode({ value: message.estimatedBytesProcessed! }, writer.uint32(18).fork()).join();
    }
    for (const v of message.timeline) {
      QueryTimelineSample.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.totalPartitionsProcessed !== undefined) {
      Int64Value.encode({ value: message.totalPartitionsProcessed! }, writer.uint32(34).fork()).join();
    }
    if (message.totalBytesProcessed !== undefined) {
      Int64Value.encode({ value: message.totalBytesProcessed! }, writer.uint32(42).fork()).join();
    }
    if (message.totalBytesProcessedAccuracy !== "") {
      writer.uint32(170).string(message.totalBytesProcessedAccuracy);
    }
    if (message.totalBytesBilled !== undefined) {
      Int64Value.encode({ value: message.totalBytesBilled! }, writer.uint32(50).fork()).join();
    }
    if (message.billingTier !== undefined) {
      Int32Value.encode({ value: message.billingTier! }, writer.uint32(58).fork()).join();
    }
    if (message.totalSlotMs !== undefined) {
      Int64Value.encode({ value: message.totalSlotMs! }, writer.uint32(66).fork()).join();
    }
    if (message.cacheHit !== undefined) {
      BoolValue.encode({ value: message.cacheHit! }, writer.uint32(74).fork()).join();
    }
    for (const v of message.referencedTables) {
      TableReference.encode(v!, writer.uint32(82).fork()).join();
    }
    for (const v of message.referencedRoutines) {
      RoutineReference.encode(v!, writer.uint32(194).fork()).join();
    }
    if (message.schema !== undefined) {
      TableSchema.encode(message.schema, writer.uint32(90).fork()).join();
    }
    if (message.numDmlAffectedRows !== undefined) {
      Int64Value.encode({ value: message.numDmlAffectedRows! }, writer.uint32(98).fork()).join();
    }
    if (message.dmlStats !== undefined) {
      DmlStats.encode(message.dmlStats, writer.uint32(258).fork()).join();
    }
    for (const v of message.undeclaredQueryParameters) {
      QueryParameter.encode(v!, writer.uint32(106).fork()).join();
    }
    if (message.statementType !== "") {
      writer.uint32(114).string(message.statementType);
    }
    if (message.ddlOperationPerformed !== "") {
      writer.uint32(122).string(message.ddlOperationPerformed);
    }
    if (message.ddlTargetTable !== undefined) {
      TableReference.encode(message.ddlTargetTable, writer.uint32(130).fork()).join();
    }
    if (message.ddlDestinationTable !== undefined) {
      TableReference.encode(message.ddlDestinationTable, writer.uint32(250).fork()).join();
    }
    if (message.ddlTargetRowAccessPolicy !== undefined) {
      RowAccessPolicyReference.encode(message.ddlTargetRowAccessPolicy, writer.uint32(210).fork()).join();
    }
    if (message.ddlAffectedRowAccessPolicyCount !== undefined) {
      Int64Value.encode({ value: message.ddlAffectedRowAccessPolicyCount! }, writer.uint32(218).fork()).join();
    }
    if (message.ddlTargetRoutine !== undefined) {
      RoutineReference.encode(message.ddlTargetRoutine, writer.uint32(178).fork()).join();
    }
    if (message.ddlTargetDataset !== undefined) {
      DatasetReference.encode(message.ddlTargetDataset, writer.uint32(242).fork()).join();
    }
    if (message.mlStatistics !== undefined) {
      MlStatistics.encode(message.mlStatistics, writer.uint32(186).fork()).join();
    }
    if (message.exportDataStatistics !== undefined) {
      ExportDataStatistics.encode(message.exportDataStatistics, writer.uint32(202).fork()).join();
    }
    for (const v of message.externalServiceCosts) {
      ExternalServiceCost.encode(v!, writer.uint32(226).fork()).join();
    }
    if (message.biEngineStatistics !== undefined) {
      BiEngineStatistics.encode(message.biEngineStatistics, writer.uint32(234).fork()).join();
    }
    if (message.loadQueryStatistics !== undefined) {
      LoadQueryStatistics.encode(message.loadQueryStatistics, writer.uint32(266).fork()).join();
    }
    if (message.dclTargetTable !== undefined) {
      TableReference.encode(message.dclTargetTable, writer.uint32(274).fork()).join();
    }
    if (message.dclTargetView !== undefined) {
      TableReference.encode(message.dclTargetView, writer.uint32(282).fork()).join();
    }
    if (message.dclTargetDataset !== undefined) {
      DatasetReference.encode(message.dclTargetDataset, writer.uint32(290).fork()).join();
    }
    if (message.searchStatistics !== undefined) {
      SearchStatistics.encode(message.searchStatistics, writer.uint32(298).fork()).join();
    }
    if (message.vectorSearchStatistics !== undefined) {
      VectorSearchStatistics.encode(message.vectorSearchStatistics, writer.uint32(354).fork()).join();
    }
    if (message.performanceInsights !== undefined) {
      PerformanceInsights.encode(message.performanceInsights, writer.uint32(306).fork()).join();
    }
    if (message.queryInfo !== undefined) {
      QueryInfo.encode(message.queryInfo, writer.uint32(314).fork()).join();
    }
    if (message.sparkStatistics !== undefined) {
      SparkStatistics.encode(message.sparkStatistics, writer.uint32(322).fork()).join();
    }
    if (message.transferredBytes !== undefined) {
      Int64Value.encode({ value: message.transferredBytes! }, writer.uint32(330).fork()).join();
    }
    if (message.materializedViewStatistics !== undefined) {
      MaterializedViewStatistics.encode(message.materializedViewStatistics, writer.uint32(338).fork()).join();
    }
    if (message.metadataCacheStatistics !== undefined) {
      MetadataCacheStatistics.encode(message.metadataCacheStatistics, writer.uint32(346).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobStatistics2 {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobStatistics2();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.queryPlan.push(ExplainQueryStage.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.estimatedBytesProcessed = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.timeline.push(QueryTimelineSample.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.totalPartitionsProcessed = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.totalBytesProcessed = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 21:
          if (tag !== 170) {
            break;
          }

          message.totalBytesProcessedAccuracy = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.totalBytesBilled = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.billingTier = Int32Value.decode(reader, reader.uint32()).value;
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.totalSlotMs = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.cacheHit = BoolValue.decode(reader, reader.uint32()).value;
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.referencedTables.push(TableReference.decode(reader, reader.uint32()));
          continue;
        case 24:
          if (tag !== 194) {
            break;
          }

          message.referencedRoutines.push(RoutineReference.decode(reader, reader.uint32()));
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.schema = TableSchema.decode(reader, reader.uint32());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.numDmlAffectedRows = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 32:
          if (tag !== 258) {
            break;
          }

          message.dmlStats = DmlStats.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.undeclaredQueryParameters.push(QueryParameter.decode(reader, reader.uint32()));
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.statementType = reader.string();
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.ddlOperationPerformed = reader.string();
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.ddlTargetTable = TableReference.decode(reader, reader.uint32());
          continue;
        case 31:
          if (tag !== 250) {
            break;
          }

          message.ddlDestinationTable = TableReference.decode(reader, reader.uint32());
          continue;
        case 26:
          if (tag !== 210) {
            break;
          }

          message.ddlTargetRowAccessPolicy = RowAccessPolicyReference.decode(reader, reader.uint32());
          continue;
        case 27:
          if (tag !== 218) {
            break;
          }

          message.ddlAffectedRowAccessPolicyCount = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 22:
          if (tag !== 178) {
            break;
          }

          message.ddlTargetRoutine = RoutineReference.decode(reader, reader.uint32());
          continue;
        case 30:
          if (tag !== 242) {
            break;
          }

          message.ddlTargetDataset = DatasetReference.decode(reader, reader.uint32());
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.mlStatistics = MlStatistics.decode(reader, reader.uint32());
          continue;
        case 25:
          if (tag !== 202) {
            break;
          }

          message.exportDataStatistics = ExportDataStatistics.decode(reader, reader.uint32());
          continue;
        case 28:
          if (tag !== 226) {
            break;
          }

          message.externalServiceCosts.push(ExternalServiceCost.decode(reader, reader.uint32()));
          continue;
        case 29:
          if (tag !== 234) {
            break;
          }

          message.biEngineStatistics = BiEngineStatistics.decode(reader, reader.uint32());
          continue;
        case 33:
          if (tag !== 266) {
            break;
          }

          message.loadQueryStatistics = LoadQueryStatistics.decode(reader, reader.uint32());
          continue;
        case 34:
          if (tag !== 274) {
            break;
          }

          message.dclTargetTable = TableReference.decode(reader, reader.uint32());
          continue;
        case 35:
          if (tag !== 282) {
            break;
          }

          message.dclTargetView = TableReference.decode(reader, reader.uint32());
          continue;
        case 36:
          if (tag !== 290) {
            break;
          }

          message.dclTargetDataset = DatasetReference.decode(reader, reader.uint32());
          continue;
        case 37:
          if (tag !== 298) {
            break;
          }

          message.searchStatistics = SearchStatistics.decode(reader, reader.uint32());
          continue;
        case 44:
          if (tag !== 354) {
            break;
          }

          message.vectorSearchStatistics = VectorSearchStatistics.decode(reader, reader.uint32());
          continue;
        case 38:
          if (tag !== 306) {
            break;
          }

          message.performanceInsights = PerformanceInsights.decode(reader, reader.uint32());
          continue;
        case 39:
          if (tag !== 314) {
            break;
          }

          message.queryInfo = QueryInfo.decode(reader, reader.uint32());
          continue;
        case 40:
          if (tag !== 322) {
            break;
          }

          message.sparkStatistics = SparkStatistics.decode(reader, reader.uint32());
          continue;
        case 41:
          if (tag !== 330) {
            break;
          }

          message.transferredBytes = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 42:
          if (tag !== 338) {
            break;
          }

          message.materializedViewStatistics = MaterializedViewStatistics.decode(reader, reader.uint32());
          continue;
        case 43:
          if (tag !== 346) {
            break;
          }

          message.metadataCacheStatistics = MetadataCacheStatistics.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobStatistics2 {
    return {
      queryPlan: globalThis.Array.isArray(object?.queryPlan)
        ? object.queryPlan.map((e: any) => ExplainQueryStage.fromJSON(e))
        : [],
      estimatedBytesProcessed: isSet(object.estimatedBytesProcessed)
        ? Long.fromValue(object.estimatedBytesProcessed)
        : undefined,
      timeline: globalThis.Array.isArray(object?.timeline)
        ? object.timeline.map((e: any) => QueryTimelineSample.fromJSON(e))
        : [],
      totalPartitionsProcessed: isSet(object.totalPartitionsProcessed)
        ? Long.fromValue(object.totalPartitionsProcessed)
        : undefined,
      totalBytesProcessed: isSet(object.totalBytesProcessed) ? Long.fromValue(object.totalBytesProcessed) : undefined,
      totalBytesProcessedAccuracy: isSet(object.totalBytesProcessedAccuracy)
        ? globalThis.String(object.totalBytesProcessedAccuracy)
        : "",
      totalBytesBilled: isSet(object.totalBytesBilled) ? Long.fromValue(object.totalBytesBilled) : undefined,
      billingTier: isSet(object.billingTier) ? Number(object.billingTier) : undefined,
      totalSlotMs: isSet(object.totalSlotMs) ? Long.fromValue(object.totalSlotMs) : undefined,
      cacheHit: isSet(object.cacheHit) ? Boolean(object.cacheHit) : undefined,
      referencedTables: globalThis.Array.isArray(object?.referencedTables)
        ? object.referencedTables.map((e: any) => TableReference.fromJSON(e))
        : [],
      referencedRoutines: globalThis.Array.isArray(object?.referencedRoutines)
        ? object.referencedRoutines.map((e: any) => RoutineReference.fromJSON(e))
        : [],
      schema: isSet(object.schema) ? TableSchema.fromJSON(object.schema) : undefined,
      numDmlAffectedRows: isSet(object.numDmlAffectedRows) ? Long.fromValue(object.numDmlAffectedRows) : undefined,
      dmlStats: isSet(object.dmlStats) ? DmlStats.fromJSON(object.dmlStats) : undefined,
      undeclaredQueryParameters: globalThis.Array.isArray(object?.undeclaredQueryParameters)
        ? object.undeclaredQueryParameters.map((e: any) => QueryParameter.fromJSON(e))
        : [],
      statementType: isSet(object.statementType) ? globalThis.String(object.statementType) : "",
      ddlOperationPerformed: isSet(object.ddlOperationPerformed) ? globalThis.String(object.ddlOperationPerformed) : "",
      ddlTargetTable: isSet(object.ddlTargetTable) ? TableReference.fromJSON(object.ddlTargetTable) : undefined,
      ddlDestinationTable: isSet(object.ddlDestinationTable)
        ? TableReference.fromJSON(object.ddlDestinationTable)
        : undefined,
      ddlTargetRowAccessPolicy: isSet(object.ddlTargetRowAccessPolicy)
        ? RowAccessPolicyReference.fromJSON(object.ddlTargetRowAccessPolicy)
        : undefined,
      ddlAffectedRowAccessPolicyCount: isSet(object.ddlAffectedRowAccessPolicyCount)
        ? Long.fromValue(object.ddlAffectedRowAccessPolicyCount)
        : undefined,
      ddlTargetRoutine: isSet(object.ddlTargetRoutine) ? RoutineReference.fromJSON(object.ddlTargetRoutine) : undefined,
      ddlTargetDataset: isSet(object.ddlTargetDataset) ? DatasetReference.fromJSON(object.ddlTargetDataset) : undefined,
      mlStatistics: isSet(object.mlStatistics) ? MlStatistics.fromJSON(object.mlStatistics) : undefined,
      exportDataStatistics: isSet(object.exportDataStatistics)
        ? ExportDataStatistics.fromJSON(object.exportDataStatistics)
        : undefined,
      externalServiceCosts: globalThis.Array.isArray(object?.externalServiceCosts)
        ? object.externalServiceCosts.map((e: any) => ExternalServiceCost.fromJSON(e))
        : [],
      biEngineStatistics: isSet(object.biEngineStatistics)
        ? BiEngineStatistics.fromJSON(object.biEngineStatistics)
        : undefined,
      loadQueryStatistics: isSet(object.loadQueryStatistics)
        ? LoadQueryStatistics.fromJSON(object.loadQueryStatistics)
        : undefined,
      dclTargetTable: isSet(object.dclTargetTable) ? TableReference.fromJSON(object.dclTargetTable) : undefined,
      dclTargetView: isSet(object.dclTargetView) ? TableReference.fromJSON(object.dclTargetView) : undefined,
      dclTargetDataset: isSet(object.dclTargetDataset) ? DatasetReference.fromJSON(object.dclTargetDataset) : undefined,
      searchStatistics: isSet(object.searchStatistics) ? SearchStatistics.fromJSON(object.searchStatistics) : undefined,
      vectorSearchStatistics: isSet(object.vectorSearchStatistics)
        ? VectorSearchStatistics.fromJSON(object.vectorSearchStatistics)
        : undefined,
      performanceInsights: isSet(object.performanceInsights)
        ? PerformanceInsights.fromJSON(object.performanceInsights)
        : undefined,
      queryInfo: isSet(object.queryInfo) ? QueryInfo.fromJSON(object.queryInfo) : undefined,
      sparkStatistics: isSet(object.sparkStatistics) ? SparkStatistics.fromJSON(object.sparkStatistics) : undefined,
      transferredBytes: isSet(object.transferredBytes) ? Long.fromValue(object.transferredBytes) : undefined,
      materializedViewStatistics: isSet(object.materializedViewStatistics)
        ? MaterializedViewStatistics.fromJSON(object.materializedViewStatistics)
        : undefined,
      metadataCacheStatistics: isSet(object.metadataCacheStatistics)
        ? MetadataCacheStatistics.fromJSON(object.metadataCacheStatistics)
        : undefined,
    };
  },

  toJSON(message: JobStatistics2): unknown {
    const obj: any = {};
    if (message.queryPlan?.length) {
      obj.queryPlan = message.queryPlan.map((e) => ExplainQueryStage.toJSON(e));
    }
    if (message.estimatedBytesProcessed !== undefined) {
      obj.estimatedBytesProcessed = message.estimatedBytesProcessed;
    }
    if (message.timeline?.length) {
      obj.timeline = message.timeline.map((e) => QueryTimelineSample.toJSON(e));
    }
    if (message.totalPartitionsProcessed !== undefined) {
      obj.totalPartitionsProcessed = message.totalPartitionsProcessed;
    }
    if (message.totalBytesProcessed !== undefined) {
      obj.totalBytesProcessed = message.totalBytesProcessed;
    }
    if (message.totalBytesProcessedAccuracy !== "") {
      obj.totalBytesProcessedAccuracy = message.totalBytesProcessedAccuracy;
    }
    if (message.totalBytesBilled !== undefined) {
      obj.totalBytesBilled = message.totalBytesBilled;
    }
    if (message.billingTier !== undefined) {
      obj.billingTier = message.billingTier;
    }
    if (message.totalSlotMs !== undefined) {
      obj.totalSlotMs = message.totalSlotMs;
    }
    if (message.cacheHit !== undefined) {
      obj.cacheHit = message.cacheHit;
    }
    if (message.referencedTables?.length) {
      obj.referencedTables = message.referencedTables.map((e) => TableReference.toJSON(e));
    }
    if (message.referencedRoutines?.length) {
      obj.referencedRoutines = message.referencedRoutines.map((e) => RoutineReference.toJSON(e));
    }
    if (message.schema !== undefined) {
      obj.schema = TableSchema.toJSON(message.schema);
    }
    if (message.numDmlAffectedRows !== undefined) {
      obj.numDmlAffectedRows = message.numDmlAffectedRows;
    }
    if (message.dmlStats !== undefined) {
      obj.dmlStats = DmlStats.toJSON(message.dmlStats);
    }
    if (message.undeclaredQueryParameters?.length) {
      obj.undeclaredQueryParameters = message.undeclaredQueryParameters.map((e) => QueryParameter.toJSON(e));
    }
    if (message.statementType !== "") {
      obj.statementType = message.statementType;
    }
    if (message.ddlOperationPerformed !== "") {
      obj.ddlOperationPerformed = message.ddlOperationPerformed;
    }
    if (message.ddlTargetTable !== undefined) {
      obj.ddlTargetTable = TableReference.toJSON(message.ddlTargetTable);
    }
    if (message.ddlDestinationTable !== undefined) {
      obj.ddlDestinationTable = TableReference.toJSON(message.ddlDestinationTable);
    }
    if (message.ddlTargetRowAccessPolicy !== undefined) {
      obj.ddlTargetRowAccessPolicy = RowAccessPolicyReference.toJSON(message.ddlTargetRowAccessPolicy);
    }
    if (message.ddlAffectedRowAccessPolicyCount !== undefined) {
      obj.ddlAffectedRowAccessPolicyCount = message.ddlAffectedRowAccessPolicyCount;
    }
    if (message.ddlTargetRoutine !== undefined) {
      obj.ddlTargetRoutine = RoutineReference.toJSON(message.ddlTargetRoutine);
    }
    if (message.ddlTargetDataset !== undefined) {
      obj.ddlTargetDataset = DatasetReference.toJSON(message.ddlTargetDataset);
    }
    if (message.mlStatistics !== undefined) {
      obj.mlStatistics = MlStatistics.toJSON(message.mlStatistics);
    }
    if (message.exportDataStatistics !== undefined) {
      obj.exportDataStatistics = ExportDataStatistics.toJSON(message.exportDataStatistics);
    }
    if (message.externalServiceCosts?.length) {
      obj.externalServiceCosts = message.externalServiceCosts.map((e) => ExternalServiceCost.toJSON(e));
    }
    if (message.biEngineStatistics !== undefined) {
      obj.biEngineStatistics = BiEngineStatistics.toJSON(message.biEngineStatistics);
    }
    if (message.loadQueryStatistics !== undefined) {
      obj.loadQueryStatistics = LoadQueryStatistics.toJSON(message.loadQueryStatistics);
    }
    if (message.dclTargetTable !== undefined) {
      obj.dclTargetTable = TableReference.toJSON(message.dclTargetTable);
    }
    if (message.dclTargetView !== undefined) {
      obj.dclTargetView = TableReference.toJSON(message.dclTargetView);
    }
    if (message.dclTargetDataset !== undefined) {
      obj.dclTargetDataset = DatasetReference.toJSON(message.dclTargetDataset);
    }
    if (message.searchStatistics !== undefined) {
      obj.searchStatistics = SearchStatistics.toJSON(message.searchStatistics);
    }
    if (message.vectorSearchStatistics !== undefined) {
      obj.vectorSearchStatistics = VectorSearchStatistics.toJSON(message.vectorSearchStatistics);
    }
    if (message.performanceInsights !== undefined) {
      obj.performanceInsights = PerformanceInsights.toJSON(message.performanceInsights);
    }
    if (message.queryInfo !== undefined) {
      obj.queryInfo = QueryInfo.toJSON(message.queryInfo);
    }
    if (message.sparkStatistics !== undefined) {
      obj.sparkStatistics = SparkStatistics.toJSON(message.sparkStatistics);
    }
    if (message.transferredBytes !== undefined) {
      obj.transferredBytes = message.transferredBytes;
    }
    if (message.materializedViewStatistics !== undefined) {
      obj.materializedViewStatistics = MaterializedViewStatistics.toJSON(message.materializedViewStatistics);
    }
    if (message.metadataCacheStatistics !== undefined) {
      obj.metadataCacheStatistics = MetadataCacheStatistics.toJSON(message.metadataCacheStatistics);
    }
    return obj;
  },

  create(base?: DeepPartial<JobStatistics2>): JobStatistics2 {
    return JobStatistics2.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobStatistics2>): JobStatistics2 {
    const message = createBaseJobStatistics2();
    message.queryPlan = object.queryPlan?.map((e) => ExplainQueryStage.fromPartial(e)) || [];
    message.estimatedBytesProcessed =
      (object.estimatedBytesProcessed !== undefined && object.estimatedBytesProcessed !== null)
        ? Long.fromValue(object.estimatedBytesProcessed)
        : undefined;
    message.timeline = object.timeline?.map((e) => QueryTimelineSample.fromPartial(e)) || [];
    message.totalPartitionsProcessed =
      (object.totalPartitionsProcessed !== undefined && object.totalPartitionsProcessed !== null)
        ? Long.fromValue(object.totalPartitionsProcessed)
        : undefined;
    message.totalBytesProcessed = (object.totalBytesProcessed !== undefined && object.totalBytesProcessed !== null)
      ? Long.fromValue(object.totalBytesProcessed)
      : undefined;
    message.totalBytesProcessedAccuracy = object.totalBytesProcessedAccuracy ?? "";
    message.totalBytesBilled = (object.totalBytesBilled !== undefined && object.totalBytesBilled !== null)
      ? Long.fromValue(object.totalBytesBilled)
      : undefined;
    message.billingTier = object.billingTier ?? undefined;
    message.totalSlotMs = (object.totalSlotMs !== undefined && object.totalSlotMs !== null)
      ? Long.fromValue(object.totalSlotMs)
      : undefined;
    message.cacheHit = object.cacheHit ?? undefined;
    message.referencedTables = object.referencedTables?.map((e) => TableReference.fromPartial(e)) || [];
    message.referencedRoutines = object.referencedRoutines?.map((e) => RoutineReference.fromPartial(e)) || [];
    message.schema = (object.schema !== undefined && object.schema !== null)
      ? TableSchema.fromPartial(object.schema)
      : undefined;
    message.numDmlAffectedRows = (object.numDmlAffectedRows !== undefined && object.numDmlAffectedRows !== null)
      ? Long.fromValue(object.numDmlAffectedRows)
      : undefined;
    message.dmlStats = (object.dmlStats !== undefined && object.dmlStats !== null)
      ? DmlStats.fromPartial(object.dmlStats)
      : undefined;
    message.undeclaredQueryParameters = object.undeclaredQueryParameters?.map((e) => QueryParameter.fromPartial(e)) ||
      [];
    message.statementType = object.statementType ?? "";
    message.ddlOperationPerformed = object.ddlOperationPerformed ?? "";
    message.ddlTargetTable = (object.ddlTargetTable !== undefined && object.ddlTargetTable !== null)
      ? TableReference.fromPartial(object.ddlTargetTable)
      : undefined;
    message.ddlDestinationTable = (object.ddlDestinationTable !== undefined && object.ddlDestinationTable !== null)
      ? TableReference.fromPartial(object.ddlDestinationTable)
      : undefined;
    message.ddlTargetRowAccessPolicy =
      (object.ddlTargetRowAccessPolicy !== undefined && object.ddlTargetRowAccessPolicy !== null)
        ? RowAccessPolicyReference.fromPartial(object.ddlTargetRowAccessPolicy)
        : undefined;
    message.ddlAffectedRowAccessPolicyCount =
      (object.ddlAffectedRowAccessPolicyCount !== undefined && object.ddlAffectedRowAccessPolicyCount !== null)
        ? Long.fromValue(object.ddlAffectedRowAccessPolicyCount)
        : undefined;
    message.ddlTargetRoutine = (object.ddlTargetRoutine !== undefined && object.ddlTargetRoutine !== null)
      ? RoutineReference.fromPartial(object.ddlTargetRoutine)
      : undefined;
    message.ddlTargetDataset = (object.ddlTargetDataset !== undefined && object.ddlTargetDataset !== null)
      ? DatasetReference.fromPartial(object.ddlTargetDataset)
      : undefined;
    message.mlStatistics = (object.mlStatistics !== undefined && object.mlStatistics !== null)
      ? MlStatistics.fromPartial(object.mlStatistics)
      : undefined;
    message.exportDataStatistics = (object.exportDataStatistics !== undefined && object.exportDataStatistics !== null)
      ? ExportDataStatistics.fromPartial(object.exportDataStatistics)
      : undefined;
    message.externalServiceCosts = object.externalServiceCosts?.map((e) => ExternalServiceCost.fromPartial(e)) || [];
    message.biEngineStatistics = (object.biEngineStatistics !== undefined && object.biEngineStatistics !== null)
      ? BiEngineStatistics.fromPartial(object.biEngineStatistics)
      : undefined;
    message.loadQueryStatistics = (object.loadQueryStatistics !== undefined && object.loadQueryStatistics !== null)
      ? LoadQueryStatistics.fromPartial(object.loadQueryStatistics)
      : undefined;
    message.dclTargetTable = (object.dclTargetTable !== undefined && object.dclTargetTable !== null)
      ? TableReference.fromPartial(object.dclTargetTable)
      : undefined;
    message.dclTargetView = (object.dclTargetView !== undefined && object.dclTargetView !== null)
      ? TableReference.fromPartial(object.dclTargetView)
      : undefined;
    message.dclTargetDataset = (object.dclTargetDataset !== undefined && object.dclTargetDataset !== null)
      ? DatasetReference.fromPartial(object.dclTargetDataset)
      : undefined;
    message.searchStatistics = (object.searchStatistics !== undefined && object.searchStatistics !== null)
      ? SearchStatistics.fromPartial(object.searchStatistics)
      : undefined;
    message.vectorSearchStatistics =
      (object.vectorSearchStatistics !== undefined && object.vectorSearchStatistics !== null)
        ? VectorSearchStatistics.fromPartial(object.vectorSearchStatistics)
        : undefined;
    message.performanceInsights = (object.performanceInsights !== undefined && object.performanceInsights !== null)
      ? PerformanceInsights.fromPartial(object.performanceInsights)
      : undefined;
    message.queryInfo = (object.queryInfo !== undefined && object.queryInfo !== null)
      ? QueryInfo.fromPartial(object.queryInfo)
      : undefined;
    message.sparkStatistics = (object.sparkStatistics !== undefined && object.sparkStatistics !== null)
      ? SparkStatistics.fromPartial(object.sparkStatistics)
      : undefined;
    message.transferredBytes = (object.transferredBytes !== undefined && object.transferredBytes !== null)
      ? Long.fromValue(object.transferredBytes)
      : undefined;
    message.materializedViewStatistics =
      (object.materializedViewStatistics !== undefined && object.materializedViewStatistics !== null)
        ? MaterializedViewStatistics.fromPartial(object.materializedViewStatistics)
        : undefined;
    message.metadataCacheStatistics =
      (object.metadataCacheStatistics !== undefined && object.metadataCacheStatistics !== null)
        ? MetadataCacheStatistics.fromPartial(object.metadataCacheStatistics)
        : undefined;
    return message;
  },
};

function createBaseJobStatistics3(): JobStatistics3 {
  return {
    inputFiles: undefined,
    inputFileBytes: undefined,
    outputRows: undefined,
    outputBytes: undefined,
    badRecords: undefined,
    timeline: [],
  };
}

export const JobStatistics3: MessageFns<JobStatistics3> = {
  encode(message: JobStatistics3, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputFiles !== undefined) {
      Int64Value.encode({ value: message.inputFiles! }, writer.uint32(10).fork()).join();
    }
    if (message.inputFileBytes !== undefined) {
      Int64Value.encode({ value: message.inputFileBytes! }, writer.uint32(18).fork()).join();
    }
    if (message.outputRows !== undefined) {
      Int64Value.encode({ value: message.outputRows! }, writer.uint32(26).fork()).join();
    }
    if (message.outputBytes !== undefined) {
      Int64Value.encode({ value: message.outputBytes! }, writer.uint32(34).fork()).join();
    }
    if (message.badRecords !== undefined) {
      Int64Value.encode({ value: message.badRecords! }, writer.uint32(42).fork()).join();
    }
    for (const v of message.timeline) {
      QueryTimelineSample.encode(v!, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobStatistics3 {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobStatistics3();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputFiles = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inputFileBytes = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.outputRows = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.outputBytes = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.badRecords = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.timeline.push(QueryTimelineSample.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobStatistics3 {
    return {
      inputFiles: isSet(object.inputFiles) ? Long.fromValue(object.inputFiles) : undefined,
      inputFileBytes: isSet(object.inputFileBytes) ? Long.fromValue(object.inputFileBytes) : undefined,
      outputRows: isSet(object.outputRows) ? Long.fromValue(object.outputRows) : undefined,
      outputBytes: isSet(object.outputBytes) ? Long.fromValue(object.outputBytes) : undefined,
      badRecords: isSet(object.badRecords) ? Long.fromValue(object.badRecords) : undefined,
      timeline: globalThis.Array.isArray(object?.timeline)
        ? object.timeline.map((e: any) => QueryTimelineSample.fromJSON(e))
        : [],
    };
  },

  toJSON(message: JobStatistics3): unknown {
    const obj: any = {};
    if (message.inputFiles !== undefined) {
      obj.inputFiles = message.inputFiles;
    }
    if (message.inputFileBytes !== undefined) {
      obj.inputFileBytes = message.inputFileBytes;
    }
    if (message.outputRows !== undefined) {
      obj.outputRows = message.outputRows;
    }
    if (message.outputBytes !== undefined) {
      obj.outputBytes = message.outputBytes;
    }
    if (message.badRecords !== undefined) {
      obj.badRecords = message.badRecords;
    }
    if (message.timeline?.length) {
      obj.timeline = message.timeline.map((e) => QueryTimelineSample.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<JobStatistics3>): JobStatistics3 {
    return JobStatistics3.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobStatistics3>): JobStatistics3 {
    const message = createBaseJobStatistics3();
    message.inputFiles = (object.inputFiles !== undefined && object.inputFiles !== null)
      ? Long.fromValue(object.inputFiles)
      : undefined;
    message.inputFileBytes = (object.inputFileBytes !== undefined && object.inputFileBytes !== null)
      ? Long.fromValue(object.inputFileBytes)
      : undefined;
    message.outputRows = (object.outputRows !== undefined && object.outputRows !== null)
      ? Long.fromValue(object.outputRows)
      : undefined;
    message.outputBytes = (object.outputBytes !== undefined && object.outputBytes !== null)
      ? Long.fromValue(object.outputBytes)
      : undefined;
    message.badRecords = (object.badRecords !== undefined && object.badRecords !== null)
      ? Long.fromValue(object.badRecords)
      : undefined;
    message.timeline = object.timeline?.map((e) => QueryTimelineSample.fromPartial(e)) || [];
    return message;
  },
};

function createBaseJobStatistics4(): JobStatistics4 {
  return { destinationUriFileCounts: [], inputBytes: undefined, timeline: [] };
}

export const JobStatistics4: MessageFns<JobStatistics4> = {
  encode(message: JobStatistics4, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    writer.uint32(10).fork();
    for (const v of message.destinationUriFileCounts) {
      writer.int64(v.toString());
    }
    writer.join();
    if (message.inputBytes !== undefined) {
      Int64Value.encode({ value: message.inputBytes! }, writer.uint32(18).fork()).join();
    }
    for (const v of message.timeline) {
      QueryTimelineSample.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobStatistics4 {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobStatistics4();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag === 8) {
            message.destinationUriFileCounts.push(Long.fromString(reader.int64().toString()));

            continue;
          }

          if (tag === 10) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.destinationUriFileCounts.push(Long.fromString(reader.int64().toString()));
            }

            continue;
          }

          break;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inputBytes = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.timeline.push(QueryTimelineSample.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobStatistics4 {
    return {
      destinationUriFileCounts: globalThis.Array.isArray(object?.destinationUriFileCounts)
        ? object.destinationUriFileCounts.map((e: any) => Long.fromValue(e))
        : [],
      inputBytes: isSet(object.inputBytes) ? Long.fromValue(object.inputBytes) : undefined,
      timeline: globalThis.Array.isArray(object?.timeline)
        ? object.timeline.map((e: any) => QueryTimelineSample.fromJSON(e))
        : [],
    };
  },

  toJSON(message: JobStatistics4): unknown {
    const obj: any = {};
    if (message.destinationUriFileCounts?.length) {
      obj.destinationUriFileCounts = message.destinationUriFileCounts.map((e) => (e || Long.ZERO).toString());
    }
    if (message.inputBytes !== undefined) {
      obj.inputBytes = message.inputBytes;
    }
    if (message.timeline?.length) {
      obj.timeline = message.timeline.map((e) => QueryTimelineSample.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<JobStatistics4>): JobStatistics4 {
    return JobStatistics4.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobStatistics4>): JobStatistics4 {
    const message = createBaseJobStatistics4();
    message.destinationUriFileCounts = object.destinationUriFileCounts?.map((e) => Long.fromValue(e)) || [];
    message.inputBytes = (object.inputBytes !== undefined && object.inputBytes !== null)
      ? Long.fromValue(object.inputBytes)
      : undefined;
    message.timeline = object.timeline?.map((e) => QueryTimelineSample.fromPartial(e)) || [];
    return message;
  },
};

function createBaseCopyJobStatistics(): CopyJobStatistics {
  return { copiedRows: undefined, copiedLogicalBytes: undefined };
}

export const CopyJobStatistics: MessageFns<CopyJobStatistics> = {
  encode(message: CopyJobStatistics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.copiedRows !== undefined) {
      Int64Value.encode({ value: message.copiedRows! }, writer.uint32(10).fork()).join();
    }
    if (message.copiedLogicalBytes !== undefined) {
      Int64Value.encode({ value: message.copiedLogicalBytes! }, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CopyJobStatistics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCopyJobStatistics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.copiedRows = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.copiedLogicalBytes = Int64Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CopyJobStatistics {
    return {
      copiedRows: isSet(object.copiedRows) ? Long.fromValue(object.copiedRows) : undefined,
      copiedLogicalBytes: isSet(object.copiedLogicalBytes) ? Long.fromValue(object.copiedLogicalBytes) : undefined,
    };
  },

  toJSON(message: CopyJobStatistics): unknown {
    const obj: any = {};
    if (message.copiedRows !== undefined) {
      obj.copiedRows = message.copiedRows;
    }
    if (message.copiedLogicalBytes !== undefined) {
      obj.copiedLogicalBytes = message.copiedLogicalBytes;
    }
    return obj;
  },

  create(base?: DeepPartial<CopyJobStatistics>): CopyJobStatistics {
    return CopyJobStatistics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CopyJobStatistics>): CopyJobStatistics {
    const message = createBaseCopyJobStatistics();
    message.copiedRows = (object.copiedRows !== undefined && object.copiedRows !== null)
      ? Long.fromValue(object.copiedRows)
      : undefined;
    message.copiedLogicalBytes = (object.copiedLogicalBytes !== undefined && object.copiedLogicalBytes !== null)
      ? Long.fromValue(object.copiedLogicalBytes)
      : undefined;
    return message;
  },
};

function createBaseMlStatistics(): MlStatistics {
  return { maxIterations: Long.ZERO, iterationResults: [], modelType: 0, trainingType: 0, hparamTrials: [] };
}

export const MlStatistics: MessageFns<MlStatistics> = {
  encode(message: MlStatistics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.maxIterations.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.maxIterations.toString());
    }
    for (const v of message.iterationResults) {
      Model_TrainingRun_IterationResult.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.modelType !== 0) {
      writer.uint32(24).int32(message.modelType);
    }
    if (message.trainingType !== 0) {
      writer.uint32(32).int32(message.trainingType);
    }
    for (const v of message.hparamTrials) {
      Model_HparamTuningTrial.encode(v!, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MlStatistics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMlStatistics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.maxIterations = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.iterationResults.push(Model_TrainingRun_IterationResult.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.modelType = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.trainingType = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.hparamTrials.push(Model_HparamTuningTrial.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MlStatistics {
    return {
      maxIterations: isSet(object.maxIterations) ? Long.fromValue(object.maxIterations) : Long.ZERO,
      iterationResults: globalThis.Array.isArray(object?.iterationResults)
        ? object.iterationResults.map((e: any) => Model_TrainingRun_IterationResult.fromJSON(e))
        : [],
      modelType: isSet(object.modelType) ? model_ModelTypeFromJSON(object.modelType) : 0,
      trainingType: isSet(object.trainingType) ? mlStatistics_TrainingTypeFromJSON(object.trainingType) : 0,
      hparamTrials: globalThis.Array.isArray(object?.hparamTrials)
        ? object.hparamTrials.map((e: any) => Model_HparamTuningTrial.fromJSON(e))
        : [],
    };
  },

  toJSON(message: MlStatistics): unknown {
    const obj: any = {};
    if (!message.maxIterations.equals(Long.ZERO)) {
      obj.maxIterations = (message.maxIterations || Long.ZERO).toString();
    }
    if (message.iterationResults?.length) {
      obj.iterationResults = message.iterationResults.map((e) => Model_TrainingRun_IterationResult.toJSON(e));
    }
    if (message.modelType !== 0) {
      obj.modelType = model_ModelTypeToJSON(message.modelType);
    }
    if (message.trainingType !== 0) {
      obj.trainingType = mlStatistics_TrainingTypeToJSON(message.trainingType);
    }
    if (message.hparamTrials?.length) {
      obj.hparamTrials = message.hparamTrials.map((e) => Model_HparamTuningTrial.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<MlStatistics>): MlStatistics {
    return MlStatistics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MlStatistics>): MlStatistics {
    const message = createBaseMlStatistics();
    message.maxIterations = (object.maxIterations !== undefined && object.maxIterations !== null)
      ? Long.fromValue(object.maxIterations)
      : Long.ZERO;
    message.iterationResults = object.iterationResults?.map((e) => Model_TrainingRun_IterationResult.fromPartial(e)) ||
      [];
    message.modelType = object.modelType ?? 0;
    message.trainingType = object.trainingType ?? 0;
    message.hparamTrials = object.hparamTrials?.map((e) => Model_HparamTuningTrial.fromPartial(e)) || [];
    return message;
  },
};

function createBaseScriptStatistics(): ScriptStatistics {
  return { evaluationKind: 0, stackFrames: [] };
}

export const ScriptStatistics: MessageFns<ScriptStatistics> = {
  encode(message: ScriptStatistics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.evaluationKind !== 0) {
      writer.uint32(8).int32(message.evaluationKind);
    }
    for (const v of message.stackFrames) {
      ScriptStatistics_ScriptStackFrame.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ScriptStatistics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseScriptStatistics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.evaluationKind = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.stackFrames.push(ScriptStatistics_ScriptStackFrame.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ScriptStatistics {
    return {
      evaluationKind: isSet(object.evaluationKind) ? scriptStatistics_EvaluationKindFromJSON(object.evaluationKind) : 0,
      stackFrames: globalThis.Array.isArray(object?.stackFrames)
        ? object.stackFrames.map((e: any) => ScriptStatistics_ScriptStackFrame.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ScriptStatistics): unknown {
    const obj: any = {};
    if (message.evaluationKind !== 0) {
      obj.evaluationKind = scriptStatistics_EvaluationKindToJSON(message.evaluationKind);
    }
    if (message.stackFrames?.length) {
      obj.stackFrames = message.stackFrames.map((e) => ScriptStatistics_ScriptStackFrame.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ScriptStatistics>): ScriptStatistics {
    return ScriptStatistics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ScriptStatistics>): ScriptStatistics {
    const message = createBaseScriptStatistics();
    message.evaluationKind = object.evaluationKind ?? 0;
    message.stackFrames = object.stackFrames?.map((e) => ScriptStatistics_ScriptStackFrame.fromPartial(e)) || [];
    return message;
  },
};

function createBaseScriptStatistics_ScriptStackFrame(): ScriptStatistics_ScriptStackFrame {
  return { startLine: 0, startColumn: 0, endLine: 0, endColumn: 0, procedureId: "", text: "" };
}

export const ScriptStatistics_ScriptStackFrame: MessageFns<ScriptStatistics_ScriptStackFrame> = {
  encode(message: ScriptStatistics_ScriptStackFrame, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startLine !== 0) {
      writer.uint32(8).int32(message.startLine);
    }
    if (message.startColumn !== 0) {
      writer.uint32(16).int32(message.startColumn);
    }
    if (message.endLine !== 0) {
      writer.uint32(24).int32(message.endLine);
    }
    if (message.endColumn !== 0) {
      writer.uint32(32).int32(message.endColumn);
    }
    if (message.procedureId !== "") {
      writer.uint32(42).string(message.procedureId);
    }
    if (message.text !== "") {
      writer.uint32(50).string(message.text);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ScriptStatistics_ScriptStackFrame {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseScriptStatistics_ScriptStackFrame();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.startLine = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.startColumn = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.endLine = reader.int32();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.endColumn = reader.int32();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.procedureId = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.text = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ScriptStatistics_ScriptStackFrame {
    return {
      startLine: isSet(object.startLine) ? globalThis.Number(object.startLine) : 0,
      startColumn: isSet(object.startColumn) ? globalThis.Number(object.startColumn) : 0,
      endLine: isSet(object.endLine) ? globalThis.Number(object.endLine) : 0,
      endColumn: isSet(object.endColumn) ? globalThis.Number(object.endColumn) : 0,
      procedureId: isSet(object.procedureId) ? globalThis.String(object.procedureId) : "",
      text: isSet(object.text) ? globalThis.String(object.text) : "",
    };
  },

  toJSON(message: ScriptStatistics_ScriptStackFrame): unknown {
    const obj: any = {};
    if (message.startLine !== 0) {
      obj.startLine = Math.round(message.startLine);
    }
    if (message.startColumn !== 0) {
      obj.startColumn = Math.round(message.startColumn);
    }
    if (message.endLine !== 0) {
      obj.endLine = Math.round(message.endLine);
    }
    if (message.endColumn !== 0) {
      obj.endColumn = Math.round(message.endColumn);
    }
    if (message.procedureId !== "") {
      obj.procedureId = message.procedureId;
    }
    if (message.text !== "") {
      obj.text = message.text;
    }
    return obj;
  },

  create(base?: DeepPartial<ScriptStatistics_ScriptStackFrame>): ScriptStatistics_ScriptStackFrame {
    return ScriptStatistics_ScriptStackFrame.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ScriptStatistics_ScriptStackFrame>): ScriptStatistics_ScriptStackFrame {
    const message = createBaseScriptStatistics_ScriptStackFrame();
    message.startLine = object.startLine ?? 0;
    message.startColumn = object.startColumn ?? 0;
    message.endLine = object.endLine ?? 0;
    message.endColumn = object.endColumn ?? 0;
    message.procedureId = object.procedureId ?? "";
    message.text = object.text ?? "";
    return message;
  },
};

function createBaseRowLevelSecurityStatistics(): RowLevelSecurityStatistics {
  return { rowLevelSecurityApplied: false };
}

export const RowLevelSecurityStatistics: MessageFns<RowLevelSecurityStatistics> = {
  encode(message: RowLevelSecurityStatistics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.rowLevelSecurityApplied !== false) {
      writer.uint32(8).bool(message.rowLevelSecurityApplied);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RowLevelSecurityStatistics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRowLevelSecurityStatistics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.rowLevelSecurityApplied = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RowLevelSecurityStatistics {
    return {
      rowLevelSecurityApplied: isSet(object.rowLevelSecurityApplied)
        ? globalThis.Boolean(object.rowLevelSecurityApplied)
        : false,
    };
  },

  toJSON(message: RowLevelSecurityStatistics): unknown {
    const obj: any = {};
    if (message.rowLevelSecurityApplied !== false) {
      obj.rowLevelSecurityApplied = message.rowLevelSecurityApplied;
    }
    return obj;
  },

  create(base?: DeepPartial<RowLevelSecurityStatistics>): RowLevelSecurityStatistics {
    return RowLevelSecurityStatistics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RowLevelSecurityStatistics>): RowLevelSecurityStatistics {
    const message = createBaseRowLevelSecurityStatistics();
    message.rowLevelSecurityApplied = object.rowLevelSecurityApplied ?? false;
    return message;
  },
};

function createBaseDataMaskingStatistics(): DataMaskingStatistics {
  return { dataMaskingApplied: false };
}

export const DataMaskingStatistics: MessageFns<DataMaskingStatistics> = {
  encode(message: DataMaskingStatistics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dataMaskingApplied !== false) {
      writer.uint32(8).bool(message.dataMaskingApplied);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DataMaskingStatistics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDataMaskingStatistics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.dataMaskingApplied = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DataMaskingStatistics {
    return {
      dataMaskingApplied: isSet(object.dataMaskingApplied) ? globalThis.Boolean(object.dataMaskingApplied) : false,
    };
  },

  toJSON(message: DataMaskingStatistics): unknown {
    const obj: any = {};
    if (message.dataMaskingApplied !== false) {
      obj.dataMaskingApplied = message.dataMaskingApplied;
    }
    return obj;
  },

  create(base?: DeepPartial<DataMaskingStatistics>): DataMaskingStatistics {
    return DataMaskingStatistics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DataMaskingStatistics>): DataMaskingStatistics {
    const message = createBaseDataMaskingStatistics();
    message.dataMaskingApplied = object.dataMaskingApplied ?? false;
    return message;
  },
};

function createBaseJobStatistics(): JobStatistics {
  return {
    creationTime: Long.ZERO,
    startTime: Long.ZERO,
    endTime: Long.ZERO,
    totalBytesProcessed: undefined,
    completionRatio: undefined,
    quotaDeferments: [],
    query: undefined,
    load: undefined,
    extract: undefined,
    copy: undefined,
    totalSlotMs: undefined,
    reservationId: "",
    numChildJobs: Long.ZERO,
    parentJobId: "",
    scriptStatistics: undefined,
    rowLevelSecurityStatistics: undefined,
    dataMaskingStatistics: undefined,
    transactionInfo: undefined,
    sessionInfo: undefined,
    finalExecutionDurationMs: Long.ZERO,
    edition: 0,
  };
}

export const JobStatistics: MessageFns<JobStatistics> = {
  encode(message: JobStatistics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.creationTime.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.creationTime.toString());
    }
    if (!message.startTime.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.startTime.toString());
    }
    if (!message.endTime.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.endTime.toString());
    }
    if (message.totalBytesProcessed !== undefined) {
      Int64Value.encode({ value: message.totalBytesProcessed! }, writer.uint32(34).fork()).join();
    }
    if (message.completionRatio !== undefined) {
      DoubleValue.encode({ value: message.completionRatio! }, writer.uint32(42).fork()).join();
    }
    for (const v of message.quotaDeferments) {
      writer.uint32(74).string(v!);
    }
    if (message.query !== undefined) {
      JobStatistics2.encode(message.query, writer.uint32(50).fork()).join();
    }
    if (message.load !== undefined) {
      JobStatistics3.encode(message.load, writer.uint32(58).fork()).join();
    }
    if (message.extract !== undefined) {
      JobStatistics4.encode(message.extract, writer.uint32(66).fork()).join();
    }
    if (message.copy !== undefined) {
      CopyJobStatistics.encode(message.copy, writer.uint32(170).fork()).join();
    }
    if (message.totalSlotMs !== undefined) {
      Int64Value.encode({ value: message.totalSlotMs! }, writer.uint32(82).fork()).join();
    }
    if (message.reservationId !== "") {
      writer.uint32(122).string(message.reservationId);
    }
    if (!message.numChildJobs.equals(Long.ZERO)) {
      writer.uint32(96).int64(message.numChildJobs.toString());
    }
    if (message.parentJobId !== "") {
      writer.uint32(106).string(message.parentJobId);
    }
    if (message.scriptStatistics !== undefined) {
      ScriptStatistics.encode(message.scriptStatistics, writer.uint32(114).fork()).join();
    }
    if (message.rowLevelSecurityStatistics !== undefined) {
      RowLevelSecurityStatistics.encode(message.rowLevelSecurityStatistics, writer.uint32(130).fork()).join();
    }
    if (message.dataMaskingStatistics !== undefined) {
      DataMaskingStatistics.encode(message.dataMaskingStatistics, writer.uint32(162).fork()).join();
    }
    if (message.transactionInfo !== undefined) {
      JobStatistics_TransactionInfo.encode(message.transactionInfo, writer.uint32(138).fork()).join();
    }
    if (message.sessionInfo !== undefined) {
      SessionInfo.encode(message.sessionInfo, writer.uint32(146).fork()).join();
    }
    if (!message.finalExecutionDurationMs.equals(Long.ZERO)) {
      writer.uint32(176).int64(message.finalExecutionDurationMs.toString());
    }
    if (message.edition !== 0) {
      writer.uint32(192).int32(message.edition);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobStatistics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobStatistics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.creationTime = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.startTime = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.endTime = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.totalBytesProcessed = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.completionRatio = DoubleValue.decode(reader, reader.uint32()).value;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.quotaDeferments.push(reader.string());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.query = JobStatistics2.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.load = JobStatistics3.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.extract = JobStatistics4.decode(reader, reader.uint32());
          continue;
        case 21:
          if (tag !== 170) {
            break;
          }

          message.copy = CopyJobStatistics.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.totalSlotMs = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.reservationId = reader.string();
          continue;
        case 12:
          if (tag !== 96) {
            break;
          }

          message.numChildJobs = Long.fromString(reader.int64().toString());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.parentJobId = reader.string();
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.scriptStatistics = ScriptStatistics.decode(reader, reader.uint32());
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.rowLevelSecurityStatistics = RowLevelSecurityStatistics.decode(reader, reader.uint32());
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.dataMaskingStatistics = DataMaskingStatistics.decode(reader, reader.uint32());
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.transactionInfo = JobStatistics_TransactionInfo.decode(reader, reader.uint32());
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.sessionInfo = SessionInfo.decode(reader, reader.uint32());
          continue;
        case 22:
          if (tag !== 176) {
            break;
          }

          message.finalExecutionDurationMs = Long.fromString(reader.int64().toString());
          continue;
        case 24:
          if (tag !== 192) {
            break;
          }

          message.edition = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobStatistics {
    return {
      creationTime: isSet(object.creationTime) ? Long.fromValue(object.creationTime) : Long.ZERO,
      startTime: isSet(object.startTime) ? Long.fromValue(object.startTime) : Long.ZERO,
      endTime: isSet(object.endTime) ? Long.fromValue(object.endTime) : Long.ZERO,
      totalBytesProcessed: isSet(object.totalBytesProcessed) ? Long.fromValue(object.totalBytesProcessed) : undefined,
      completionRatio: isSet(object.completionRatio) ? Number(object.completionRatio) : undefined,
      quotaDeferments: globalThis.Array.isArray(object?.quotaDeferments)
        ? object.quotaDeferments.map((e: any) => globalThis.String(e))
        : [],
      query: isSet(object.query) ? JobStatistics2.fromJSON(object.query) : undefined,
      load: isSet(object.load) ? JobStatistics3.fromJSON(object.load) : undefined,
      extract: isSet(object.extract) ? JobStatistics4.fromJSON(object.extract) : undefined,
      copy: isSet(object.copy) ? CopyJobStatistics.fromJSON(object.copy) : undefined,
      totalSlotMs: isSet(object.totalSlotMs) ? Long.fromValue(object.totalSlotMs) : undefined,
      reservationId: isSet(object.reservationId) ? globalThis.String(object.reservationId) : "",
      numChildJobs: isSet(object.numChildJobs) ? Long.fromValue(object.numChildJobs) : Long.ZERO,
      parentJobId: isSet(object.parentJobId) ? globalThis.String(object.parentJobId) : "",
      scriptStatistics: isSet(object.scriptStatistics) ? ScriptStatistics.fromJSON(object.scriptStatistics) : undefined,
      rowLevelSecurityStatistics: isSet(object.rowLevelSecurityStatistics)
        ? RowLevelSecurityStatistics.fromJSON(object.rowLevelSecurityStatistics)
        : undefined,
      dataMaskingStatistics: isSet(object.dataMaskingStatistics)
        ? DataMaskingStatistics.fromJSON(object.dataMaskingStatistics)
        : undefined,
      transactionInfo: isSet(object.transactionInfo)
        ? JobStatistics_TransactionInfo.fromJSON(object.transactionInfo)
        : undefined,
      sessionInfo: isSet(object.sessionInfo) ? SessionInfo.fromJSON(object.sessionInfo) : undefined,
      finalExecutionDurationMs: isSet(object.finalExecutionDurationMs)
        ? Long.fromValue(object.finalExecutionDurationMs)
        : Long.ZERO,
      edition: isSet(object.edition) ? reservationEditionFromJSON(object.edition) : 0,
    };
  },

  toJSON(message: JobStatistics): unknown {
    const obj: any = {};
    if (!message.creationTime.equals(Long.ZERO)) {
      obj.creationTime = (message.creationTime || Long.ZERO).toString();
    }
    if (!message.startTime.equals(Long.ZERO)) {
      obj.startTime = (message.startTime || Long.ZERO).toString();
    }
    if (!message.endTime.equals(Long.ZERO)) {
      obj.endTime = (message.endTime || Long.ZERO).toString();
    }
    if (message.totalBytesProcessed !== undefined) {
      obj.totalBytesProcessed = message.totalBytesProcessed;
    }
    if (message.completionRatio !== undefined) {
      obj.completionRatio = message.completionRatio;
    }
    if (message.quotaDeferments?.length) {
      obj.quotaDeferments = message.quotaDeferments;
    }
    if (message.query !== undefined) {
      obj.query = JobStatistics2.toJSON(message.query);
    }
    if (message.load !== undefined) {
      obj.load = JobStatistics3.toJSON(message.load);
    }
    if (message.extract !== undefined) {
      obj.extract = JobStatistics4.toJSON(message.extract);
    }
    if (message.copy !== undefined) {
      obj.copy = CopyJobStatistics.toJSON(message.copy);
    }
    if (message.totalSlotMs !== undefined) {
      obj.totalSlotMs = message.totalSlotMs;
    }
    if (message.reservationId !== "") {
      obj.reservationId = message.reservationId;
    }
    if (!message.numChildJobs.equals(Long.ZERO)) {
      obj.numChildJobs = (message.numChildJobs || Long.ZERO).toString();
    }
    if (message.parentJobId !== "") {
      obj.parentJobId = message.parentJobId;
    }
    if (message.scriptStatistics !== undefined) {
      obj.scriptStatistics = ScriptStatistics.toJSON(message.scriptStatistics);
    }
    if (message.rowLevelSecurityStatistics !== undefined) {
      obj.rowLevelSecurityStatistics = RowLevelSecurityStatistics.toJSON(message.rowLevelSecurityStatistics);
    }
    if (message.dataMaskingStatistics !== undefined) {
      obj.dataMaskingStatistics = DataMaskingStatistics.toJSON(message.dataMaskingStatistics);
    }
    if (message.transactionInfo !== undefined) {
      obj.transactionInfo = JobStatistics_TransactionInfo.toJSON(message.transactionInfo);
    }
    if (message.sessionInfo !== undefined) {
      obj.sessionInfo = SessionInfo.toJSON(message.sessionInfo);
    }
    if (!message.finalExecutionDurationMs.equals(Long.ZERO)) {
      obj.finalExecutionDurationMs = (message.finalExecutionDurationMs || Long.ZERO).toString();
    }
    if (message.edition !== 0) {
      obj.edition = reservationEditionToJSON(message.edition);
    }
    return obj;
  },

  create(base?: DeepPartial<JobStatistics>): JobStatistics {
    return JobStatistics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobStatistics>): JobStatistics {
    const message = createBaseJobStatistics();
    message.creationTime = (object.creationTime !== undefined && object.creationTime !== null)
      ? Long.fromValue(object.creationTime)
      : Long.ZERO;
    message.startTime = (object.startTime !== undefined && object.startTime !== null)
      ? Long.fromValue(object.startTime)
      : Long.ZERO;
    message.endTime = (object.endTime !== undefined && object.endTime !== null)
      ? Long.fromValue(object.endTime)
      : Long.ZERO;
    message.totalBytesProcessed = (object.totalBytesProcessed !== undefined && object.totalBytesProcessed !== null)
      ? Long.fromValue(object.totalBytesProcessed)
      : undefined;
    message.completionRatio = object.completionRatio ?? undefined;
    message.quotaDeferments = object.quotaDeferments?.map((e) => e) || [];
    message.query = (object.query !== undefined && object.query !== null)
      ? JobStatistics2.fromPartial(object.query)
      : undefined;
    message.load = (object.load !== undefined && object.load !== null)
      ? JobStatistics3.fromPartial(object.load)
      : undefined;
    message.extract = (object.extract !== undefined && object.extract !== null)
      ? JobStatistics4.fromPartial(object.extract)
      : undefined;
    message.copy = (object.copy !== undefined && object.copy !== null)
      ? CopyJobStatistics.fromPartial(object.copy)
      : undefined;
    message.totalSlotMs = (object.totalSlotMs !== undefined && object.totalSlotMs !== null)
      ? Long.fromValue(object.totalSlotMs)
      : undefined;
    message.reservationId = object.reservationId ?? "";
    message.numChildJobs = (object.numChildJobs !== undefined && object.numChildJobs !== null)
      ? Long.fromValue(object.numChildJobs)
      : Long.ZERO;
    message.parentJobId = object.parentJobId ?? "";
    message.scriptStatistics = (object.scriptStatistics !== undefined && object.scriptStatistics !== null)
      ? ScriptStatistics.fromPartial(object.scriptStatistics)
      : undefined;
    message.rowLevelSecurityStatistics =
      (object.rowLevelSecurityStatistics !== undefined && object.rowLevelSecurityStatistics !== null)
        ? RowLevelSecurityStatistics.fromPartial(object.rowLevelSecurityStatistics)
        : undefined;
    message.dataMaskingStatistics =
      (object.dataMaskingStatistics !== undefined && object.dataMaskingStatistics !== null)
        ? DataMaskingStatistics.fromPartial(object.dataMaskingStatistics)
        : undefined;
    message.transactionInfo = (object.transactionInfo !== undefined && object.transactionInfo !== null)
      ? JobStatistics_TransactionInfo.fromPartial(object.transactionInfo)
      : undefined;
    message.sessionInfo = (object.sessionInfo !== undefined && object.sessionInfo !== null)
      ? SessionInfo.fromPartial(object.sessionInfo)
      : undefined;
    message.finalExecutionDurationMs =
      (object.finalExecutionDurationMs !== undefined && object.finalExecutionDurationMs !== null)
        ? Long.fromValue(object.finalExecutionDurationMs)
        : Long.ZERO;
    message.edition = object.edition ?? 0;
    return message;
  },
};

function createBaseJobStatistics_TransactionInfo(): JobStatistics_TransactionInfo {
  return { transactionId: "" };
}

export const JobStatistics_TransactionInfo: MessageFns<JobStatistics_TransactionInfo> = {
  encode(message: JobStatistics_TransactionInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.transactionId !== "") {
      writer.uint32(10).string(message.transactionId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobStatistics_TransactionInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobStatistics_TransactionInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.transactionId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobStatistics_TransactionInfo {
    return { transactionId: isSet(object.transactionId) ? globalThis.String(object.transactionId) : "" };
  },

  toJSON(message: JobStatistics_TransactionInfo): unknown {
    const obj: any = {};
    if (message.transactionId !== "") {
      obj.transactionId = message.transactionId;
    }
    return obj;
  },

  create(base?: DeepPartial<JobStatistics_TransactionInfo>): JobStatistics_TransactionInfo {
    return JobStatistics_TransactionInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobStatistics_TransactionInfo>): JobStatistics_TransactionInfo {
    const message = createBaseJobStatistics_TransactionInfo();
    message.transactionId = object.transactionId ?? "";
    return message;
  },
};

function createBaseDmlStats(): DmlStats {
  return { insertedRowCount: undefined, deletedRowCount: undefined, updatedRowCount: undefined };
}

export const DmlStats: MessageFns<DmlStats> = {
  encode(message: DmlStats, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.insertedRowCount !== undefined) {
      Int64Value.encode({ value: message.insertedRowCount! }, writer.uint32(10).fork()).join();
    }
    if (message.deletedRowCount !== undefined) {
      Int64Value.encode({ value: message.deletedRowCount! }, writer.uint32(18).fork()).join();
    }
    if (message.updatedRowCount !== undefined) {
      Int64Value.encode({ value: message.updatedRowCount! }, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DmlStats {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDmlStats();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.insertedRowCount = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.deletedRowCount = Int64Value.decode(reader, reader.uint32()).value;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.updatedRowCount = Int64Value.decode(reader, reader.uint32()).value;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DmlStats {
    return {
      insertedRowCount: isSet(object.insertedRowCount) ? Long.fromValue(object.insertedRowCount) : undefined,
      deletedRowCount: isSet(object.deletedRowCount) ? Long.fromValue(object.deletedRowCount) : undefined,
      updatedRowCount: isSet(object.updatedRowCount) ? Long.fromValue(object.updatedRowCount) : undefined,
    };
  },

  toJSON(message: DmlStats): unknown {
    const obj: any = {};
    if (message.insertedRowCount !== undefined) {
      obj.insertedRowCount = message.insertedRowCount;
    }
    if (message.deletedRowCount !== undefined) {
      obj.deletedRowCount = message.deletedRowCount;
    }
    if (message.updatedRowCount !== undefined) {
      obj.updatedRowCount = message.updatedRowCount;
    }
    return obj;
  },

  create(base?: DeepPartial<DmlStats>): DmlStats {
    return DmlStats.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DmlStats>): DmlStats {
    const message = createBaseDmlStats();
    message.insertedRowCount = (object.insertedRowCount !== undefined && object.insertedRowCount !== null)
      ? Long.fromValue(object.insertedRowCount)
      : undefined;
    message.deletedRowCount = (object.deletedRowCount !== undefined && object.deletedRowCount !== null)
      ? Long.fromValue(object.deletedRowCount)
      : undefined;
    message.updatedRowCount = (object.updatedRowCount !== undefined && object.updatedRowCount !== null)
      ? Long.fromValue(object.updatedRowCount)
      : undefined;
    return message;
  },
};

function createBasePerformanceInsights(): PerformanceInsights {
  return {
    avgPreviousExecutionMs: Long.ZERO,
    stagePerformanceStandaloneInsights: [],
    stagePerformanceChangeInsights: [],
  };
}

export const PerformanceInsights: MessageFns<PerformanceInsights> = {
  encode(message: PerformanceInsights, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.avgPreviousExecutionMs.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.avgPreviousExecutionMs.toString());
    }
    for (const v of message.stagePerformanceStandaloneInsights) {
      StagePerformanceStandaloneInsight.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.stagePerformanceChangeInsights) {
      StagePerformanceChangeInsight.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PerformanceInsights {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePerformanceInsights();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.avgPreviousExecutionMs = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.stagePerformanceStandaloneInsights.push(
            StagePerformanceStandaloneInsight.decode(reader, reader.uint32()),
          );
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.stagePerformanceChangeInsights.push(StagePerformanceChangeInsight.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PerformanceInsights {
    return {
      avgPreviousExecutionMs: isSet(object.avgPreviousExecutionMs)
        ? Long.fromValue(object.avgPreviousExecutionMs)
        : Long.ZERO,
      stagePerformanceStandaloneInsights: globalThis.Array.isArray(object?.stagePerformanceStandaloneInsights)
        ? object.stagePerformanceStandaloneInsights.map((e: any) => StagePerformanceStandaloneInsight.fromJSON(e))
        : [],
      stagePerformanceChangeInsights: globalThis.Array.isArray(object?.stagePerformanceChangeInsights)
        ? object.stagePerformanceChangeInsights.map((e: any) => StagePerformanceChangeInsight.fromJSON(e))
        : [],
    };
  },

  toJSON(message: PerformanceInsights): unknown {
    const obj: any = {};
    if (!message.avgPreviousExecutionMs.equals(Long.ZERO)) {
      obj.avgPreviousExecutionMs = (message.avgPreviousExecutionMs || Long.ZERO).toString();
    }
    if (message.stagePerformanceStandaloneInsights?.length) {
      obj.stagePerformanceStandaloneInsights = message.stagePerformanceStandaloneInsights.map((e) =>
        StagePerformanceStandaloneInsight.toJSON(e)
      );
    }
    if (message.stagePerformanceChangeInsights?.length) {
      obj.stagePerformanceChangeInsights = message.stagePerformanceChangeInsights.map((e) =>
        StagePerformanceChangeInsight.toJSON(e)
      );
    }
    return obj;
  },

  create(base?: DeepPartial<PerformanceInsights>): PerformanceInsights {
    return PerformanceInsights.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PerformanceInsights>): PerformanceInsights {
    const message = createBasePerformanceInsights();
    message.avgPreviousExecutionMs =
      (object.avgPreviousExecutionMs !== undefined && object.avgPreviousExecutionMs !== null)
        ? Long.fromValue(object.avgPreviousExecutionMs)
        : Long.ZERO;
    message.stagePerformanceStandaloneInsights =
      object.stagePerformanceStandaloneInsights?.map((e) => StagePerformanceStandaloneInsight.fromPartial(e)) || [];
    message.stagePerformanceChangeInsights =
      object.stagePerformanceChangeInsights?.map((e) => StagePerformanceChangeInsight.fromPartial(e)) || [];
    return message;
  },
};

function createBaseStagePerformanceChangeInsight(): StagePerformanceChangeInsight {
  return { stageId: Long.ZERO, inputDataChange: undefined };
}

export const StagePerformanceChangeInsight: MessageFns<StagePerformanceChangeInsight> = {
  encode(message: StagePerformanceChangeInsight, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.stageId.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.stageId.toString());
    }
    if (message.inputDataChange !== undefined) {
      InputDataChange.encode(message.inputDataChange, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StagePerformanceChangeInsight {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStagePerformanceChangeInsight();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.stageId = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inputDataChange = InputDataChange.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StagePerformanceChangeInsight {
    return {
      stageId: isSet(object.stageId) ? Long.fromValue(object.stageId) : Long.ZERO,
      inputDataChange: isSet(object.inputDataChange) ? InputDataChange.fromJSON(object.inputDataChange) : undefined,
    };
  },

  toJSON(message: StagePerformanceChangeInsight): unknown {
    const obj: any = {};
    if (!message.stageId.equals(Long.ZERO)) {
      obj.stageId = (message.stageId || Long.ZERO).toString();
    }
    if (message.inputDataChange !== undefined) {
      obj.inputDataChange = InputDataChange.toJSON(message.inputDataChange);
    }
    return obj;
  },

  create(base?: DeepPartial<StagePerformanceChangeInsight>): StagePerformanceChangeInsight {
    return StagePerformanceChangeInsight.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StagePerformanceChangeInsight>): StagePerformanceChangeInsight {
    const message = createBaseStagePerformanceChangeInsight();
    message.stageId = (object.stageId !== undefined && object.stageId !== null)
      ? Long.fromValue(object.stageId)
      : Long.ZERO;
    message.inputDataChange = (object.inputDataChange !== undefined && object.inputDataChange !== null)
      ? InputDataChange.fromPartial(object.inputDataChange)
      : undefined;
    return message;
  },
};

function createBaseInputDataChange(): InputDataChange {
  return { recordsReadDiffPercentage: 0 };
}

export const InputDataChange: MessageFns<InputDataChange> = {
  encode(message: InputDataChange, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.recordsReadDiffPercentage !== 0) {
      writer.uint32(13).float(message.recordsReadDiffPercentage);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InputDataChange {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInputDataChange();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 13) {
            break;
          }

          message.recordsReadDiffPercentage = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InputDataChange {
    return {
      recordsReadDiffPercentage: isSet(object.recordsReadDiffPercentage)
        ? globalThis.Number(object.recordsReadDiffPercentage)
        : 0,
    };
  },

  toJSON(message: InputDataChange): unknown {
    const obj: any = {};
    if (message.recordsReadDiffPercentage !== 0) {
      obj.recordsReadDiffPercentage = message.recordsReadDiffPercentage;
    }
    return obj;
  },

  create(base?: DeepPartial<InputDataChange>): InputDataChange {
    return InputDataChange.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InputDataChange>): InputDataChange {
    const message = createBaseInputDataChange();
    message.recordsReadDiffPercentage = object.recordsReadDiffPercentage ?? 0;
    return message;
  },
};

function createBaseStagePerformanceStandaloneInsight(): StagePerformanceStandaloneInsight {
  return {
    stageId: Long.ZERO,
    slotContention: undefined,
    insufficientShuffleQuota: undefined,
    biEngineReasons: [],
    highCardinalityJoins: [],
    partitionSkew: undefined,
  };
}

export const StagePerformanceStandaloneInsight: MessageFns<StagePerformanceStandaloneInsight> = {
  encode(message: StagePerformanceStandaloneInsight, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.stageId.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.stageId.toString());
    }
    if (message.slotContention !== undefined) {
      writer.uint32(16).bool(message.slotContention);
    }
    if (message.insufficientShuffleQuota !== undefined) {
      writer.uint32(24).bool(message.insufficientShuffleQuota);
    }
    for (const v of message.biEngineReasons) {
      BiEngineReason.encode(v!, writer.uint32(42).fork()).join();
    }
    for (const v of message.highCardinalityJoins) {
      HighCardinalityJoin.encode(v!, writer.uint32(50).fork()).join();
    }
    if (message.partitionSkew !== undefined) {
      PartitionSkew.encode(message.partitionSkew, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StagePerformanceStandaloneInsight {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStagePerformanceStandaloneInsight();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.stageId = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.slotContention = reader.bool();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.insufficientShuffleQuota = reader.bool();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.biEngineReasons.push(BiEngineReason.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.highCardinalityJoins.push(HighCardinalityJoin.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.partitionSkew = PartitionSkew.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StagePerformanceStandaloneInsight {
    return {
      stageId: isSet(object.stageId) ? Long.fromValue(object.stageId) : Long.ZERO,
      slotContention: isSet(object.slotContention) ? globalThis.Boolean(object.slotContention) : undefined,
      insufficientShuffleQuota: isSet(object.insufficientShuffleQuota)
        ? globalThis.Boolean(object.insufficientShuffleQuota)
        : undefined,
      biEngineReasons: globalThis.Array.isArray(object?.biEngineReasons)
        ? object.biEngineReasons.map((e: any) => BiEngineReason.fromJSON(e))
        : [],
      highCardinalityJoins: globalThis.Array.isArray(object?.highCardinalityJoins)
        ? object.highCardinalityJoins.map((e: any) => HighCardinalityJoin.fromJSON(e))
        : [],
      partitionSkew: isSet(object.partitionSkew) ? PartitionSkew.fromJSON(object.partitionSkew) : undefined,
    };
  },

  toJSON(message: StagePerformanceStandaloneInsight): unknown {
    const obj: any = {};
    if (!message.stageId.equals(Long.ZERO)) {
      obj.stageId = (message.stageId || Long.ZERO).toString();
    }
    if (message.slotContention !== undefined) {
      obj.slotContention = message.slotContention;
    }
    if (message.insufficientShuffleQuota !== undefined) {
      obj.insufficientShuffleQuota = message.insufficientShuffleQuota;
    }
    if (message.biEngineReasons?.length) {
      obj.biEngineReasons = message.biEngineReasons.map((e) => BiEngineReason.toJSON(e));
    }
    if (message.highCardinalityJoins?.length) {
      obj.highCardinalityJoins = message.highCardinalityJoins.map((e) => HighCardinalityJoin.toJSON(e));
    }
    if (message.partitionSkew !== undefined) {
      obj.partitionSkew = PartitionSkew.toJSON(message.partitionSkew);
    }
    return obj;
  },

  create(base?: DeepPartial<StagePerformanceStandaloneInsight>): StagePerformanceStandaloneInsight {
    return StagePerformanceStandaloneInsight.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StagePerformanceStandaloneInsight>): StagePerformanceStandaloneInsight {
    const message = createBaseStagePerformanceStandaloneInsight();
    message.stageId = (object.stageId !== undefined && object.stageId !== null)
      ? Long.fromValue(object.stageId)
      : Long.ZERO;
    message.slotContention = object.slotContention ?? undefined;
    message.insufficientShuffleQuota = object.insufficientShuffleQuota ?? undefined;
    message.biEngineReasons = object.biEngineReasons?.map((e) => BiEngineReason.fromPartial(e)) || [];
    message.highCardinalityJoins = object.highCardinalityJoins?.map((e) => HighCardinalityJoin.fromPartial(e)) || [];
    message.partitionSkew = (object.partitionSkew !== undefined && object.partitionSkew !== null)
      ? PartitionSkew.fromPartial(object.partitionSkew)
      : undefined;
    return message;
  },
};

function createBaseHighCardinalityJoin(): HighCardinalityJoin {
  return { leftRows: Long.ZERO, rightRows: Long.ZERO, outputRows: Long.ZERO, stepIndex: 0 };
}

export const HighCardinalityJoin: MessageFns<HighCardinalityJoin> = {
  encode(message: HighCardinalityJoin, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.leftRows.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.leftRows.toString());
    }
    if (!message.rightRows.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.rightRows.toString());
    }
    if (!message.outputRows.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.outputRows.toString());
    }
    if (message.stepIndex !== 0) {
      writer.uint32(32).int32(message.stepIndex);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): HighCardinalityJoin {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseHighCardinalityJoin();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.leftRows = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.rightRows = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.outputRows = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.stepIndex = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): HighCardinalityJoin {
    return {
      leftRows: isSet(object.leftRows) ? Long.fromValue(object.leftRows) : Long.ZERO,
      rightRows: isSet(object.rightRows) ? Long.fromValue(object.rightRows) : Long.ZERO,
      outputRows: isSet(object.outputRows) ? Long.fromValue(object.outputRows) : Long.ZERO,
      stepIndex: isSet(object.stepIndex) ? globalThis.Number(object.stepIndex) : 0,
    };
  },

  toJSON(message: HighCardinalityJoin): unknown {
    const obj: any = {};
    if (!message.leftRows.equals(Long.ZERO)) {
      obj.leftRows = (message.leftRows || Long.ZERO).toString();
    }
    if (!message.rightRows.equals(Long.ZERO)) {
      obj.rightRows = (message.rightRows || Long.ZERO).toString();
    }
    if (!message.outputRows.equals(Long.ZERO)) {
      obj.outputRows = (message.outputRows || Long.ZERO).toString();
    }
    if (message.stepIndex !== 0) {
      obj.stepIndex = Math.round(message.stepIndex);
    }
    return obj;
  },

  create(base?: DeepPartial<HighCardinalityJoin>): HighCardinalityJoin {
    return HighCardinalityJoin.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<HighCardinalityJoin>): HighCardinalityJoin {
    const message = createBaseHighCardinalityJoin();
    message.leftRows = (object.leftRows !== undefined && object.leftRows !== null)
      ? Long.fromValue(object.leftRows)
      : Long.ZERO;
    message.rightRows = (object.rightRows !== undefined && object.rightRows !== null)
      ? Long.fromValue(object.rightRows)
      : Long.ZERO;
    message.outputRows = (object.outputRows !== undefined && object.outputRows !== null)
      ? Long.fromValue(object.outputRows)
      : Long.ZERO;
    message.stepIndex = object.stepIndex ?? 0;
    return message;
  },
};

function createBasePartitionSkew(): PartitionSkew {
  return { skewSources: [] };
}

export const PartitionSkew: MessageFns<PartitionSkew> = {
  encode(message: PartitionSkew, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.skewSources) {
      PartitionSkew_SkewSource.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PartitionSkew {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePartitionSkew();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.skewSources.push(PartitionSkew_SkewSource.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PartitionSkew {
    return {
      skewSources: globalThis.Array.isArray(object?.skewSources)
        ? object.skewSources.map((e: any) => PartitionSkew_SkewSource.fromJSON(e))
        : [],
    };
  },

  toJSON(message: PartitionSkew): unknown {
    const obj: any = {};
    if (message.skewSources?.length) {
      obj.skewSources = message.skewSources.map((e) => PartitionSkew_SkewSource.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<PartitionSkew>): PartitionSkew {
    return PartitionSkew.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PartitionSkew>): PartitionSkew {
    const message = createBasePartitionSkew();
    message.skewSources = object.skewSources?.map((e) => PartitionSkew_SkewSource.fromPartial(e)) || [];
    return message;
  },
};

function createBasePartitionSkew_SkewSource(): PartitionSkew_SkewSource {
  return { stageId: Long.ZERO };
}

export const PartitionSkew_SkewSource: MessageFns<PartitionSkew_SkewSource> = {
  encode(message: PartitionSkew_SkewSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.stageId.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.stageId.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PartitionSkew_SkewSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePartitionSkew_SkewSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.stageId = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PartitionSkew_SkewSource {
    return { stageId: isSet(object.stageId) ? Long.fromValue(object.stageId) : Long.ZERO };
  },

  toJSON(message: PartitionSkew_SkewSource): unknown {
    const obj: any = {};
    if (!message.stageId.equals(Long.ZERO)) {
      obj.stageId = (message.stageId || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<PartitionSkew_SkewSource>): PartitionSkew_SkewSource {
    return PartitionSkew_SkewSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PartitionSkew_SkewSource>): PartitionSkew_SkewSource {
    const message = createBasePartitionSkew_SkewSource();
    message.stageId = (object.stageId !== undefined && object.stageId !== null)
      ? Long.fromValue(object.stageId)
      : Long.ZERO;
    return message;
  },
};

function createBaseSparkStatistics(): SparkStatistics {
  return {
    sparkJobId: undefined,
    sparkJobLocation: undefined,
    endpoints: {},
    loggingInfo: undefined,
    kmsKeyName: undefined,
    gcsStagingBucket: undefined,
  };
}

export const SparkStatistics: MessageFns<SparkStatistics> = {
  encode(message: SparkStatistics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.sparkJobId !== undefined) {
      writer.uint32(10).string(message.sparkJobId);
    }
    if (message.sparkJobLocation !== undefined) {
      writer.uint32(18).string(message.sparkJobLocation);
    }
    Object.entries(message.endpoints).forEach(([key, value]) => {
      SparkStatistics_EndpointsEntry.encode({ key: key as any, value }, writer.uint32(26).fork()).join();
    });
    if (message.loggingInfo !== undefined) {
      SparkStatistics_LoggingInfo.encode(message.loggingInfo, writer.uint32(34).fork()).join();
    }
    if (message.kmsKeyName !== undefined) {
      writer.uint32(42).string(message.kmsKeyName);
    }
    if (message.gcsStagingBucket !== undefined) {
      writer.uint32(50).string(message.gcsStagingBucket);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SparkStatistics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSparkStatistics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.sparkJobId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.sparkJobLocation = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          const entry3 = SparkStatistics_EndpointsEntry.decode(reader, reader.uint32());
          if (entry3.value !== undefined) {
            message.endpoints[entry3.key] = entry3.value;
          }
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.loggingInfo = SparkStatistics_LoggingInfo.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.kmsKeyName = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.gcsStagingBucket = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SparkStatistics {
    return {
      sparkJobId: isSet(object.sparkJobId) ? globalThis.String(object.sparkJobId) : undefined,
      sparkJobLocation: isSet(object.sparkJobLocation) ? globalThis.String(object.sparkJobLocation) : undefined,
      endpoints: isObject(object.endpoints)
        ? Object.entries(object.endpoints).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      loggingInfo: isSet(object.loggingInfo) ? SparkStatistics_LoggingInfo.fromJSON(object.loggingInfo) : undefined,
      kmsKeyName: isSet(object.kmsKeyName) ? globalThis.String(object.kmsKeyName) : undefined,
      gcsStagingBucket: isSet(object.gcsStagingBucket) ? globalThis.String(object.gcsStagingBucket) : undefined,
    };
  },

  toJSON(message: SparkStatistics): unknown {
    const obj: any = {};
    if (message.sparkJobId !== undefined) {
      obj.sparkJobId = message.sparkJobId;
    }
    if (message.sparkJobLocation !== undefined) {
      obj.sparkJobLocation = message.sparkJobLocation;
    }
    if (message.endpoints) {
      const entries = Object.entries(message.endpoints);
      if (entries.length > 0) {
        obj.endpoints = {};
        entries.forEach(([k, v]) => {
          obj.endpoints[k] = v;
        });
      }
    }
    if (message.loggingInfo !== undefined) {
      obj.loggingInfo = SparkStatistics_LoggingInfo.toJSON(message.loggingInfo);
    }
    if (message.kmsKeyName !== undefined) {
      obj.kmsKeyName = message.kmsKeyName;
    }
    if (message.gcsStagingBucket !== undefined) {
      obj.gcsStagingBucket = message.gcsStagingBucket;
    }
    return obj;
  },

  create(base?: DeepPartial<SparkStatistics>): SparkStatistics {
    return SparkStatistics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SparkStatistics>): SparkStatistics {
    const message = createBaseSparkStatistics();
    message.sparkJobId = object.sparkJobId ?? undefined;
    message.sparkJobLocation = object.sparkJobLocation ?? undefined;
    message.endpoints = Object.entries(object.endpoints ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.loggingInfo = (object.loggingInfo !== undefined && object.loggingInfo !== null)
      ? SparkStatistics_LoggingInfo.fromPartial(object.loggingInfo)
      : undefined;
    message.kmsKeyName = object.kmsKeyName ?? undefined;
    message.gcsStagingBucket = object.gcsStagingBucket ?? undefined;
    return message;
  },
};

function createBaseSparkStatistics_LoggingInfo(): SparkStatistics_LoggingInfo {
  return { resourceType: "", projectId: "" };
}

export const SparkStatistics_LoggingInfo: MessageFns<SparkStatistics_LoggingInfo> = {
  encode(message: SparkStatistics_LoggingInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.resourceType !== "") {
      writer.uint32(10).string(message.resourceType);
    }
    if (message.projectId !== "") {
      writer.uint32(18).string(message.projectId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SparkStatistics_LoggingInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSparkStatistics_LoggingInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.resourceType = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.projectId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SparkStatistics_LoggingInfo {
    return {
      resourceType: isSet(object.resourceType) ? globalThis.String(object.resourceType) : "",
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
    };
  },

  toJSON(message: SparkStatistics_LoggingInfo): unknown {
    const obj: any = {};
    if (message.resourceType !== "") {
      obj.resourceType = message.resourceType;
    }
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    return obj;
  },

  create(base?: DeepPartial<SparkStatistics_LoggingInfo>): SparkStatistics_LoggingInfo {
    return SparkStatistics_LoggingInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SparkStatistics_LoggingInfo>): SparkStatistics_LoggingInfo {
    const message = createBaseSparkStatistics_LoggingInfo();
    message.resourceType = object.resourceType ?? "";
    message.projectId = object.projectId ?? "";
    return message;
  },
};

function createBaseSparkStatistics_EndpointsEntry(): SparkStatistics_EndpointsEntry {
  return { key: "", value: "" };
}

export const SparkStatistics_EndpointsEntry: MessageFns<SparkStatistics_EndpointsEntry> = {
  encode(message: SparkStatistics_EndpointsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SparkStatistics_EndpointsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSparkStatistics_EndpointsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SparkStatistics_EndpointsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: SparkStatistics_EndpointsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<SparkStatistics_EndpointsEntry>): SparkStatistics_EndpointsEntry {
    return SparkStatistics_EndpointsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SparkStatistics_EndpointsEntry>): SparkStatistics_EndpointsEntry {
    const message = createBaseSparkStatistics_EndpointsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseMaterializedViewStatistics(): MaterializedViewStatistics {
  return { materializedView: [] };
}

export const MaterializedViewStatistics: MessageFns<MaterializedViewStatistics> = {
  encode(message: MaterializedViewStatistics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.materializedView) {
      MaterializedView.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MaterializedViewStatistics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMaterializedViewStatistics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.materializedView.push(MaterializedView.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MaterializedViewStatistics {
    return {
      materializedView: globalThis.Array.isArray(object?.materializedView)
        ? object.materializedView.map((e: any) => MaterializedView.fromJSON(e))
        : [],
    };
  },

  toJSON(message: MaterializedViewStatistics): unknown {
    const obj: any = {};
    if (message.materializedView?.length) {
      obj.materializedView = message.materializedView.map((e) => MaterializedView.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<MaterializedViewStatistics>): MaterializedViewStatistics {
    return MaterializedViewStatistics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MaterializedViewStatistics>): MaterializedViewStatistics {
    const message = createBaseMaterializedViewStatistics();
    message.materializedView = object.materializedView?.map((e) => MaterializedView.fromPartial(e)) || [];
    return message;
  },
};

function createBaseMaterializedView(): MaterializedView {
  return { tableReference: undefined, chosen: undefined, estimatedBytesSaved: undefined, rejectedReason: undefined };
}

export const MaterializedView: MessageFns<MaterializedView> = {
  encode(message: MaterializedView, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.tableReference !== undefined) {
      TableReference.encode(message.tableReference, writer.uint32(10).fork()).join();
    }
    if (message.chosen !== undefined) {
      writer.uint32(16).bool(message.chosen);
    }
    if (message.estimatedBytesSaved !== undefined) {
      writer.uint32(24).int64(message.estimatedBytesSaved.toString());
    }
    if (message.rejectedReason !== undefined) {
      writer.uint32(32).int32(message.rejectedReason);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MaterializedView {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMaterializedView();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.tableReference = TableReference.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.chosen = reader.bool();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.estimatedBytesSaved = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.rejectedReason = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MaterializedView {
    return {
      tableReference: isSet(object.tableReference) ? TableReference.fromJSON(object.tableReference) : undefined,
      chosen: isSet(object.chosen) ? globalThis.Boolean(object.chosen) : undefined,
      estimatedBytesSaved: isSet(object.estimatedBytesSaved) ? Long.fromValue(object.estimatedBytesSaved) : undefined,
      rejectedReason: isSet(object.rejectedReason)
        ? materializedView_RejectedReasonFromJSON(object.rejectedReason)
        : undefined,
    };
  },

  toJSON(message: MaterializedView): unknown {
    const obj: any = {};
    if (message.tableReference !== undefined) {
      obj.tableReference = TableReference.toJSON(message.tableReference);
    }
    if (message.chosen !== undefined) {
      obj.chosen = message.chosen;
    }
    if (message.estimatedBytesSaved !== undefined) {
      obj.estimatedBytesSaved = (message.estimatedBytesSaved || Long.ZERO).toString();
    }
    if (message.rejectedReason !== undefined) {
      obj.rejectedReason = materializedView_RejectedReasonToJSON(message.rejectedReason);
    }
    return obj;
  },

  create(base?: DeepPartial<MaterializedView>): MaterializedView {
    return MaterializedView.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MaterializedView>): MaterializedView {
    const message = createBaseMaterializedView();
    message.tableReference = (object.tableReference !== undefined && object.tableReference !== null)
      ? TableReference.fromPartial(object.tableReference)
      : undefined;
    message.chosen = object.chosen ?? undefined;
    message.estimatedBytesSaved = (object.estimatedBytesSaved !== undefined && object.estimatedBytesSaved !== null)
      ? Long.fromValue(object.estimatedBytesSaved)
      : undefined;
    message.rejectedReason = object.rejectedReason ?? undefined;
    return message;
  },
};

function createBaseTableMetadataCacheUsage(): TableMetadataCacheUsage {
  return {
    tableReference: undefined,
    unusedReason: undefined,
    explanation: undefined,
    staleness: undefined,
    tableType: "",
  };
}

export const TableMetadataCacheUsage: MessageFns<TableMetadataCacheUsage> = {
  encode(message: TableMetadataCacheUsage, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.tableReference !== undefined) {
      TableReference.encode(message.tableReference, writer.uint32(10).fork()).join();
    }
    if (message.unusedReason !== undefined) {
      writer.uint32(16).int32(message.unusedReason);
    }
    if (message.explanation !== undefined) {
      writer.uint32(26).string(message.explanation);
    }
    if (message.staleness !== undefined) {
      Duration.encode(message.staleness, writer.uint32(42).fork()).join();
    }
    if (message.tableType !== "") {
      writer.uint32(50).string(message.tableType);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TableMetadataCacheUsage {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTableMetadataCacheUsage();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.tableReference = TableReference.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.unusedReason = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.explanation = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.staleness = Duration.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.tableType = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TableMetadataCacheUsage {
    return {
      tableReference: isSet(object.tableReference) ? TableReference.fromJSON(object.tableReference) : undefined,
      unusedReason: isSet(object.unusedReason)
        ? tableMetadataCacheUsage_UnusedReasonFromJSON(object.unusedReason)
        : undefined,
      explanation: isSet(object.explanation) ? globalThis.String(object.explanation) : undefined,
      staleness: isSet(object.staleness) ? Duration.fromJSON(object.staleness) : undefined,
      tableType: isSet(object.tableType) ? globalThis.String(object.tableType) : "",
    };
  },

  toJSON(message: TableMetadataCacheUsage): unknown {
    const obj: any = {};
    if (message.tableReference !== undefined) {
      obj.tableReference = TableReference.toJSON(message.tableReference);
    }
    if (message.unusedReason !== undefined) {
      obj.unusedReason = tableMetadataCacheUsage_UnusedReasonToJSON(message.unusedReason);
    }
    if (message.explanation !== undefined) {
      obj.explanation = message.explanation;
    }
    if (message.staleness !== undefined) {
      obj.staleness = Duration.toJSON(message.staleness);
    }
    if (message.tableType !== "") {
      obj.tableType = message.tableType;
    }
    return obj;
  },

  create(base?: DeepPartial<TableMetadataCacheUsage>): TableMetadataCacheUsage {
    return TableMetadataCacheUsage.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TableMetadataCacheUsage>): TableMetadataCacheUsage {
    const message = createBaseTableMetadataCacheUsage();
    message.tableReference = (object.tableReference !== undefined && object.tableReference !== null)
      ? TableReference.fromPartial(object.tableReference)
      : undefined;
    message.unusedReason = object.unusedReason ?? undefined;
    message.explanation = object.explanation ?? undefined;
    message.staleness = (object.staleness !== undefined && object.staleness !== null)
      ? Duration.fromPartial(object.staleness)
      : undefined;
    message.tableType = object.tableType ?? "";
    return message;
  },
};

function createBaseMetadataCacheStatistics(): MetadataCacheStatistics {
  return { tableMetadataCacheUsage: [] };
}

export const MetadataCacheStatistics: MessageFns<MetadataCacheStatistics> = {
  encode(message: MetadataCacheStatistics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.tableMetadataCacheUsage) {
      TableMetadataCacheUsage.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MetadataCacheStatistics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMetadataCacheStatistics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.tableMetadataCacheUsage.push(TableMetadataCacheUsage.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MetadataCacheStatistics {
    return {
      tableMetadataCacheUsage: globalThis.Array.isArray(object?.tableMetadataCacheUsage)
        ? object.tableMetadataCacheUsage.map((e: any) => TableMetadataCacheUsage.fromJSON(e))
        : [],
    };
  },

  toJSON(message: MetadataCacheStatistics): unknown {
    const obj: any = {};
    if (message.tableMetadataCacheUsage?.length) {
      obj.tableMetadataCacheUsage = message.tableMetadataCacheUsage.map((e) => TableMetadataCacheUsage.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<MetadataCacheStatistics>): MetadataCacheStatistics {
    return MetadataCacheStatistics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MetadataCacheStatistics>): MetadataCacheStatistics {
    const message = createBaseMetadataCacheStatistics();
    message.tableMetadataCacheUsage =
      object.tableMetadataCacheUsage?.map((e) => TableMetadataCacheUsage.fromPartial(e)) || [];
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
