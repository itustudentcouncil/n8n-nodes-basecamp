// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/dataproc/v1/shared.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Duration } from "../../../protobuf/duration.js";
import { Timestamp } from "../../../protobuf/timestamp.js";

export const protobufPackage = "google.cloud.dataproc.v1";

/** Cluster components that can be activated. */
export enum Component {
  /** COMPONENT_UNSPECIFIED - Unspecified component. Specifying this will cause Cluster creation to fail. */
  COMPONENT_UNSPECIFIED = 0,
  /**
   * ANACONDA - The Anaconda component is no longer supported or applicable to
   * [supported Dataproc on Compute Engine image versions]
   * (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-version-clusters#supported-dataproc-image-versions).
   * It cannot be activated on clusters created with supported Dataproc on
   * Compute Engine image versions.
   */
  ANACONDA = 5,
  /** DOCKER - Docker */
  DOCKER = 13,
  /** DRUID - The Druid query engine. (alpha) */
  DRUID = 9,
  /** FLINK - Flink */
  FLINK = 14,
  /** HBASE - HBase. (beta) */
  HBASE = 11,
  /** HIVE_WEBHCAT - The Hive Web HCatalog (the REST service for accessing HCatalog). */
  HIVE_WEBHCAT = 3,
  /** HUDI - Hudi. */
  HUDI = 18,
  /** JUPYTER - The Jupyter Notebook. */
  JUPYTER = 1,
  /** PRESTO - The Presto query engine. */
  PRESTO = 6,
  /** TRINO - The Trino query engine. */
  TRINO = 17,
  /** RANGER - The Ranger service. */
  RANGER = 12,
  /** SOLR - The Solr service. */
  SOLR = 10,
  /** ZEPPELIN - The Zeppelin notebook. */
  ZEPPELIN = 4,
  /** ZOOKEEPER - The Zookeeper service. */
  ZOOKEEPER = 8,
  UNRECOGNIZED = -1,
}

export function componentFromJSON(object: any): Component {
  switch (object) {
    case 0:
    case "COMPONENT_UNSPECIFIED":
      return Component.COMPONENT_UNSPECIFIED;
    case 5:
    case "ANACONDA":
      return Component.ANACONDA;
    case 13:
    case "DOCKER":
      return Component.DOCKER;
    case 9:
    case "DRUID":
      return Component.DRUID;
    case 14:
    case "FLINK":
      return Component.FLINK;
    case 11:
    case "HBASE":
      return Component.HBASE;
    case 3:
    case "HIVE_WEBHCAT":
      return Component.HIVE_WEBHCAT;
    case 18:
    case "HUDI":
      return Component.HUDI;
    case 1:
    case "JUPYTER":
      return Component.JUPYTER;
    case 6:
    case "PRESTO":
      return Component.PRESTO;
    case 17:
    case "TRINO":
      return Component.TRINO;
    case 12:
    case "RANGER":
      return Component.RANGER;
    case 10:
    case "SOLR":
      return Component.SOLR;
    case 4:
    case "ZEPPELIN":
      return Component.ZEPPELIN;
    case 8:
    case "ZOOKEEPER":
      return Component.ZOOKEEPER;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Component.UNRECOGNIZED;
  }
}

export function componentToJSON(object: Component): string {
  switch (object) {
    case Component.COMPONENT_UNSPECIFIED:
      return "COMPONENT_UNSPECIFIED";
    case Component.ANACONDA:
      return "ANACONDA";
    case Component.DOCKER:
      return "DOCKER";
    case Component.DRUID:
      return "DRUID";
    case Component.FLINK:
      return "FLINK";
    case Component.HBASE:
      return "HBASE";
    case Component.HIVE_WEBHCAT:
      return "HIVE_WEBHCAT";
    case Component.HUDI:
      return "HUDI";
    case Component.JUPYTER:
      return "JUPYTER";
    case Component.PRESTO:
      return "PRESTO";
    case Component.TRINO:
      return "TRINO";
    case Component.RANGER:
      return "RANGER";
    case Component.SOLR:
      return "SOLR";
    case Component.ZEPPELIN:
      return "ZEPPELIN";
    case Component.ZOOKEEPER:
      return "ZOOKEEPER";
    case Component.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Actions in response to failure of a resource associated with a cluster. */
export enum FailureAction {
  /** FAILURE_ACTION_UNSPECIFIED - When FailureAction is unspecified, failure action defaults to NO_ACTION. */
  FAILURE_ACTION_UNSPECIFIED = 0,
  /**
   * NO_ACTION - Take no action on failure to create a cluster resource. NO_ACTION is the
   * default.
   */
  NO_ACTION = 1,
  /** DELETE - Delete the failed cluster resource. */
  DELETE = 2,
  UNRECOGNIZED = -1,
}

export function failureActionFromJSON(object: any): FailureAction {
  switch (object) {
    case 0:
    case "FAILURE_ACTION_UNSPECIFIED":
      return FailureAction.FAILURE_ACTION_UNSPECIFIED;
    case 1:
    case "NO_ACTION":
      return FailureAction.NO_ACTION;
    case 2:
    case "DELETE":
      return FailureAction.DELETE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return FailureAction.UNRECOGNIZED;
  }
}

export function failureActionToJSON(object: FailureAction): string {
  switch (object) {
    case FailureAction.FAILURE_ACTION_UNSPECIFIED:
      return "FAILURE_ACTION_UNSPECIFIED";
    case FailureAction.NO_ACTION:
      return "NO_ACTION";
    case FailureAction.DELETE:
      return "DELETE";
    case FailureAction.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Runtime configuration for a workload. */
export interface RuntimeConfig {
  /** Optional. Version of the batch runtime. */
  version: string;
  /**
   * Optional. Optional custom container image for the job runtime environment.
   * If not specified, a default container image will be used.
   */
  containerImage: string;
  /**
   * Optional. A mapping of property names to values, which are used to
   * configure workload execution.
   */
  properties: { [key: string]: string };
  /** Optional. Dependency repository configuration. */
  repositoryConfig:
    | RepositoryConfig
    | undefined;
  /** Optional. Autotuning configuration of the workload. */
  autotuningConfig:
    | AutotuningConfig
    | undefined;
  /**
   * Optional. Cohort identifier. Identifies families of the workloads having
   * the same shape, e.g. daily ETL jobs.
   */
  cohort: string;
}

export interface RuntimeConfig_PropertiesEntry {
  key: string;
  value: string;
}

/** Environment configuration for a workload. */
export interface EnvironmentConfig {
  /** Optional. Execution configuration for a workload. */
  executionConfig:
    | ExecutionConfig
    | undefined;
  /** Optional. Peripherals configuration that workload has access to. */
  peripheralsConfig: PeripheralsConfig | undefined;
}

/** Execution configuration for a workload. */
export interface ExecutionConfig {
  /** Optional. Service account that used to execute workload. */
  serviceAccount: string;
  /** Optional. Network URI to connect workload to. */
  networkUri?:
    | string
    | undefined;
  /** Optional. Subnetwork URI to connect workload to. */
  subnetworkUri?:
    | string
    | undefined;
  /** Optional. Tags used for network traffic control. */
  networkTags: string[];
  /** Optional. The Cloud KMS key to use for encryption. */
  kmsKey: string;
  /**
   * Optional. Applies to sessions only. The duration to keep the session alive
   * while it's idling. Exceeding this threshold causes the session to
   * terminate. This field cannot be set on a batch workload. Minimum value is
   * 10 minutes; maximum value is 14 days (see JSON representation of
   * [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
   * Defaults to 1 hour if not set.
   * If both `ttl` and `idle_ttl` are specified for an interactive session,
   * the conditions are treated as `OR` conditions: the workload will be
   * terminated when it has been idle for `idle_ttl` or when `ttl` has been
   * exceeded, whichever occurs first.
   */
  idleTtl:
    | Duration
    | undefined;
  /**
   * Optional. The duration after which the workload will be terminated,
   * specified as the JSON representation for
   * [Duration](https://protobuf.dev/programming-guides/proto3/#json).
   * When the workload exceeds this duration, it will be unconditionally
   * terminated without waiting for ongoing work to finish. If `ttl` is not
   * specified for a batch workload, the workload will be allowed to run until
   * it exits naturally (or run forever without exiting). If `ttl` is not
   * specified for an interactive session, it defaults to 24 hours. If `ttl` is
   * not specified for a batch that uses 2.1+ runtime version, it defaults to 4
   * hours. Minimum value is 10 minutes; maximum value is 14 days. If both `ttl`
   * and `idle_ttl` are specified (for an interactive session), the conditions
   * are treated as `OR` conditions: the workload will be terminated when it has
   * been idle for `idle_ttl` or when `ttl` has been exceeded, whichever occurs
   * first.
   */
  ttl:
    | Duration
    | undefined;
  /**
   * Optional. A Cloud Storage bucket used to stage workload dependencies,
   * config files, and store workload output and other ephemeral data, such as
   * Spark history files. If you do not specify a staging bucket, Cloud Dataproc
   * will determine a Cloud Storage location according to the region where your
   * workload is running, and then create and manage project-level, per-location
   * staging and temporary buckets.
   * **This field requires a Cloud Storage bucket name, not a `gs://...` URI to
   * a Cloud Storage bucket.**
   */
  stagingBucket: string;
}

/** Spark History Server configuration for the workload. */
export interface SparkHistoryServerConfig {
  /**
   * Optional. Resource name of an existing Dataproc Cluster to act as a Spark
   * History Server for the workload.
   *
   * Example:
   *
   * * `projects/[project_id]/regions/[region]/clusters/[cluster_name]`
   */
  dataprocCluster: string;
}

/** Auxiliary services configuration for a workload. */
export interface PeripheralsConfig {
  /**
   * Optional. Resource name of an existing Dataproc Metastore service.
   *
   * Example:
   *
   * * `projects/[project_id]/locations/[region]/services/[service_id]`
   */
  metastoreService: string;
  /** Optional. The Spark History Server configuration for the workload. */
  sparkHistoryServerConfig: SparkHistoryServerConfig | undefined;
}

/** Runtime information about workload execution. */
export interface RuntimeInfo {
  /**
   * Output only. Map of remote access endpoints (such as web interfaces and
   * APIs) to their URIs.
   */
  endpoints: { [key: string]: string };
  /**
   * Output only. A URI pointing to the location of the stdout and stderr of the
   * workload.
   */
  outputUri: string;
  /** Output only. A URI pointing to the location of the diagnostics tarball. */
  diagnosticOutputUri: string;
  /**
   * Output only. Approximate workload resource usage, calculated when
   * the workload completes (see [Dataproc Serverless pricing]
   * (https://cloud.google.com/dataproc-serverless/pricing)).
   *
   * **Note:** This metric calculation may change in the future, for
   * example, to capture cumulative workload resource
   * consumption during workload execution (see the
   * [Dataproc Serverless release notes]
   * (https://cloud.google.com/dataproc-serverless/docs/release-notes)
   * for announcements, changes, fixes
   * and other Dataproc developments).
   */
  approximateUsage:
    | UsageMetrics
    | undefined;
  /** Output only. Snapshot of current workload resource usage. */
  currentUsage: UsageSnapshot | undefined;
}

export interface RuntimeInfo_EndpointsEntry {
  key: string;
  value: string;
}

/** Usage metrics represent approximate total resources consumed by a workload. */
export interface UsageMetrics {
  /**
   * Optional. DCU (Dataproc Compute Units) usage in (`milliDCU` x `seconds`)
   * (see [Dataproc Serverless pricing]
   * (https://cloud.google.com/dataproc-serverless/pricing)).
   */
  milliDcuSeconds: Long;
  /**
   * Optional. Shuffle storage usage in (`GB` x `seconds`) (see
   * [Dataproc Serverless pricing]
   * (https://cloud.google.com/dataproc-serverless/pricing)).
   */
  shuffleStorageGbSeconds: Long;
  /**
   * Optional. Accelerator usage in (`milliAccelerator` x `seconds`) (see
   * [Dataproc Serverless pricing]
   * (https://cloud.google.com/dataproc-serverless/pricing)).
   */
  milliAcceleratorSeconds: Long;
  /** Optional. Accelerator type being used, if any */
  acceleratorType: string;
}

/**
 * The usage snapshot represents the resources consumed by a workload at a
 * specified time.
 */
export interface UsageSnapshot {
  /**
   * Optional. Milli (one-thousandth) Dataproc Compute Units (DCUs) (see
   * [Dataproc Serverless pricing]
   * (https://cloud.google.com/dataproc-serverless/pricing)).
   */
  milliDcu: Long;
  /**
   * Optional. Shuffle Storage in gigabytes (GB). (see [Dataproc Serverless
   * pricing] (https://cloud.google.com/dataproc-serverless/pricing))
   */
  shuffleStorageGb: Long;
  /**
   * Optional. Milli (one-thousandth) Dataproc Compute Units (DCUs) charged at
   * premium tier (see [Dataproc Serverless pricing]
   * (https://cloud.google.com/dataproc-serverless/pricing)).
   */
  milliDcuPremium: Long;
  /**
   * Optional. Shuffle Storage in gigabytes (GB) charged at premium tier. (see
   * [Dataproc Serverless pricing]
   * (https://cloud.google.com/dataproc-serverless/pricing))
   */
  shuffleStorageGbPremium: Long;
  /**
   * Optional. Milli (one-thousandth) accelerator. (see [Dataproc
   * Serverless pricing] (https://cloud.google.com/dataproc-serverless/pricing))
   */
  milliAccelerator: Long;
  /** Optional. Accelerator type being used, if any */
  acceleratorType: string;
  /** Optional. The timestamp of the usage snapshot. */
  snapshotTime: Date | undefined;
}

/** The cluster's GKE config. */
export interface GkeClusterConfig {
  /**
   * Optional. A target GKE cluster to deploy to. It must be in the same project
   * and region as the Dataproc cluster (the GKE cluster can be zonal or
   * regional). Format:
   * 'projects/{project}/locations/{location}/clusters/{cluster_id}'
   */
  gkeClusterTarget: string;
  /**
   * Optional. GKE node pools where workloads will be scheduled. At least one
   * node pool must be assigned the `DEFAULT`
   * [GkeNodePoolTarget.Role][google.cloud.dataproc.v1.GkeNodePoolTarget.Role].
   * If a `GkeNodePoolTarget` is not specified, Dataproc constructs a `DEFAULT`
   * `GkeNodePoolTarget`. Each role can be given to only one
   * `GkeNodePoolTarget`. All node pools must have the same location settings.
   */
  nodePoolTarget: GkeNodePoolTarget[];
}

/** The configuration for running the Dataproc cluster on Kubernetes. */
export interface KubernetesClusterConfig {
  /**
   * Optional. A namespace within the Kubernetes cluster to deploy into. If this
   * namespace does not exist, it is created. If it exists, Dataproc verifies
   * that another Dataproc VirtualCluster is not installed into it. If not
   * specified, the name of the Dataproc Cluster is used.
   */
  kubernetesNamespace: string;
  /** Required. The configuration for running the Dataproc cluster on GKE. */
  gkeClusterConfig?:
    | GkeClusterConfig
    | undefined;
  /**
   * Optional. The software configuration for this Dataproc cluster running on
   * Kubernetes.
   */
  kubernetesSoftwareConfig: KubernetesSoftwareConfig | undefined;
}

/** The software configuration for this Dataproc cluster running on Kubernetes. */
export interface KubernetesSoftwareConfig {
  /**
   * The components that should be installed in this Dataproc cluster. The key
   * must be a string from the KubernetesComponent enumeration. The value is
   * the version of the software to be installed.
   * At least one entry must be specified.
   */
  componentVersion: { [key: string]: string };
  /**
   * The properties to set on daemon config files.
   *
   * Property keys are specified in `prefix:property` format, for example
   * `spark:spark.kubernetes.container.image`. The following are supported
   * prefixes and their mappings:
   *
   * * spark:  `spark-defaults.conf`
   *
   * For more information, see [Cluster
   * properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
   */
  properties: { [key: string]: string };
}

export interface KubernetesSoftwareConfig_ComponentVersionEntry {
  key: string;
  value: string;
}

export interface KubernetesSoftwareConfig_PropertiesEntry {
  key: string;
  value: string;
}

/** GKE node pools that Dataproc workloads run on. */
export interface GkeNodePoolTarget {
  /**
   * Required. The target GKE node pool.
   * Format:
   * 'projects/{project}/locations/{location}/clusters/{cluster}/nodePools/{node_pool}'
   */
  nodePool: string;
  /** Required. The roles associated with the GKE node pool. */
  roles: GkeNodePoolTarget_Role[];
  /**
   * Input only. The configuration for the GKE node pool.
   *
   * If specified, Dataproc attempts to create a node pool with the
   * specified shape. If one with the same name already exists, it is
   * verified against all specified fields. If a field differs, the
   * virtual cluster creation will fail.
   *
   * If omitted, any node pool with the specified name is used. If a
   * node pool with the specified name does not exist, Dataproc create a
   * node pool with default values.
   *
   * This is an input only field. It will not be returned by the API.
   */
  nodePoolConfig: GkeNodePoolConfig | undefined;
}

/**
 * `Role` specifies the tasks that will run on the node pool. Roles can be
 * specific to workloads. Exactly one
 * [GkeNodePoolTarget][google.cloud.dataproc.v1.GkeNodePoolTarget] within the
 * virtual cluster must have the `DEFAULT` role, which is used to run all
 * workloads that are not associated with a node pool.
 */
export enum GkeNodePoolTarget_Role {
  /** ROLE_UNSPECIFIED - Role is unspecified. */
  ROLE_UNSPECIFIED = 0,
  /**
   * DEFAULT - At least one node pool must have the `DEFAULT` role.
   * Work assigned to a role that is not associated with a node pool
   * is assigned to the node pool with the `DEFAULT` role. For example,
   * work assigned to the `CONTROLLER` role will be assigned to the node pool
   * with the `DEFAULT` role if no node pool has the `CONTROLLER` role.
   */
  DEFAULT = 1,
  /**
   * CONTROLLER - Run work associated with the Dataproc control plane (for example,
   * controllers and webhooks). Very low resource requirements.
   */
  CONTROLLER = 2,
  /** SPARK_DRIVER - Run work associated with a Spark driver of a job. */
  SPARK_DRIVER = 3,
  /** SPARK_EXECUTOR - Run work associated with a Spark executor of a job. */
  SPARK_EXECUTOR = 4,
  UNRECOGNIZED = -1,
}

export function gkeNodePoolTarget_RoleFromJSON(object: any): GkeNodePoolTarget_Role {
  switch (object) {
    case 0:
    case "ROLE_UNSPECIFIED":
      return GkeNodePoolTarget_Role.ROLE_UNSPECIFIED;
    case 1:
    case "DEFAULT":
      return GkeNodePoolTarget_Role.DEFAULT;
    case 2:
    case "CONTROLLER":
      return GkeNodePoolTarget_Role.CONTROLLER;
    case 3:
    case "SPARK_DRIVER":
      return GkeNodePoolTarget_Role.SPARK_DRIVER;
    case 4:
    case "SPARK_EXECUTOR":
      return GkeNodePoolTarget_Role.SPARK_EXECUTOR;
    case -1:
    case "UNRECOGNIZED":
    default:
      return GkeNodePoolTarget_Role.UNRECOGNIZED;
  }
}

export function gkeNodePoolTarget_RoleToJSON(object: GkeNodePoolTarget_Role): string {
  switch (object) {
    case GkeNodePoolTarget_Role.ROLE_UNSPECIFIED:
      return "ROLE_UNSPECIFIED";
    case GkeNodePoolTarget_Role.DEFAULT:
      return "DEFAULT";
    case GkeNodePoolTarget_Role.CONTROLLER:
      return "CONTROLLER";
    case GkeNodePoolTarget_Role.SPARK_DRIVER:
      return "SPARK_DRIVER";
    case GkeNodePoolTarget_Role.SPARK_EXECUTOR:
      return "SPARK_EXECUTOR";
    case GkeNodePoolTarget_Role.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * The configuration of a GKE node pool used by a [Dataproc-on-GKE
 * cluster](https://cloud.google.com/dataproc/docs/concepts/jobs/dataproc-gke#create-a-dataproc-on-gke-cluster).
 */
export interface GkeNodePoolConfig {
  /** Optional. The node pool configuration. */
  config:
    | GkeNodePoolConfig_GkeNodeConfig
    | undefined;
  /**
   * Optional. The list of Compute Engine
   * [zones](https://cloud.google.com/compute/docs/zones#available) where
   * node pool nodes associated with a Dataproc on GKE virtual cluster
   * will be located.
   *
   * **Note:** All node pools associated with a virtual cluster
   * must be located in the same region as the virtual cluster, and they must
   * be located in the same zone within that region.
   *
   * If a location is not specified during node pool creation, Dataproc on GKE
   * will choose the zone.
   */
  locations: string[];
  /**
   * Optional. The autoscaler configuration for this node pool. The autoscaler
   * is enabled only when a valid configuration is present.
   */
  autoscaling: GkeNodePoolConfig_GkeNodePoolAutoscalingConfig | undefined;
}

/** Parameters that describe cluster nodes. */
export interface GkeNodePoolConfig_GkeNodeConfig {
  /**
   * Optional. The name of a Compute Engine [machine
   * type](https://cloud.google.com/compute/docs/machine-types).
   */
  machineType: string;
  /**
   * Optional. The number of local SSD disks to attach to the node, which is
   * limited by the maximum number of disks allowable per zone (see [Adding
   * Local SSDs](https://cloud.google.com/compute/docs/disks/local-ssd)).
   */
  localSsdCount: number;
  /**
   * Optional. Whether the nodes are created as legacy [preemptible VM
   * instances] (https://cloud.google.com/compute/docs/instances/preemptible).
   * Also see
   * [Spot][google.cloud.dataproc.v1.GkeNodePoolConfig.GkeNodeConfig.spot]
   * VMs, preemptible VM instances without a maximum lifetime. Legacy and Spot
   * preemptible nodes cannot be used in a node pool with the `CONTROLLER`
   * [role]
   * (/dataproc/docs/reference/rest/v1/projects.regions.clusters#role)
   * or in the DEFAULT node pool if the CONTROLLER role is not assigned (the
   * DEFAULT node pool will assume the CONTROLLER role).
   */
  preemptible: boolean;
  /**
   * Optional. A list of [hardware
   * accelerators](https://cloud.google.com/compute/docs/gpus) to attach to
   * each node.
   */
  accelerators: GkeNodePoolConfig_GkeNodePoolAcceleratorConfig[];
  /**
   * Optional. [Minimum CPU
   * platform](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
   * to be used by this instance. The instance may be scheduled on the
   * specified or a newer CPU platform. Specify the friendly names of CPU
   * platforms, such as "Intel Haswell"` or Intel Sandy Bridge".
   */
  minCpuPlatform: string;
  /**
   * Optional. The [Customer Managed Encryption Key (CMEK)]
   * (https://cloud.google.com/kubernetes-engine/docs/how-to/using-cmek)
   * used to encrypt the boot disk attached to each node in the node pool.
   * Specify the key using the following format:
   * <code>projects/<var>KEY_PROJECT_ID</var>/locations/<var>LOCATION</var>/keyRings/<var>RING_NAME</var>/cryptoKeys/<var>KEY_NAME</var></code>.
   */
  bootDiskKmsKey: string;
  /**
   * Optional. Whether the nodes are created as [Spot VM instances]
   * (https://cloud.google.com/compute/docs/instances/spot).
   * Spot VMs are the latest update to legacy
   * [preemptible
   * VMs][google.cloud.dataproc.v1.GkeNodePoolConfig.GkeNodeConfig.preemptible].
   * Spot VMs do not have a maximum lifetime. Legacy and Spot preemptible
   * nodes cannot be used in a node pool with the `CONTROLLER`
   * [role](/dataproc/docs/reference/rest/v1/projects.regions.clusters#role)
   * or in the DEFAULT node pool if the CONTROLLER role is not assigned (the
   * DEFAULT node pool will assume the CONTROLLER role).
   */
  spot: boolean;
}

/**
 * A GkeNodeConfigAcceleratorConfig represents a Hardware Accelerator request
 * for a node pool.
 */
export interface GkeNodePoolConfig_GkeNodePoolAcceleratorConfig {
  /** The number of accelerator cards exposed to an instance. */
  acceleratorCount: Long;
  /** The accelerator type resource namename (see GPUs on Compute Engine). */
  acceleratorType: string;
  /**
   * Size of partitions to create on the GPU. Valid values are described in
   * the NVIDIA [mig user
   * guide](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/#partitioning).
   */
  gpuPartitionSize: string;
}

/**
 * GkeNodePoolAutoscaling contains information the cluster autoscaler needs to
 * adjust the size of the node pool to the current cluster usage.
 */
export interface GkeNodePoolConfig_GkeNodePoolAutoscalingConfig {
  /**
   * The minimum number of nodes in the node pool. Must be >= 0 and <=
   * max_node_count.
   */
  minNodeCount: number;
  /**
   * The maximum number of nodes in the node pool. Must be >= min_node_count,
   * and must be > 0.
   * **Note:** Quota must be sufficient to scale up the cluster.
   */
  maxNodeCount: number;
}

/** Autotuning configuration of the workload. */
export interface AutotuningConfig {
  /** Optional. Scenarios for which tunings are applied. */
  scenarios: AutotuningConfig_Scenario[];
}

/**
 * Scenario represents a specific goal that autotuning will attempt to achieve
 * by modifying workloads.
 */
export enum AutotuningConfig_Scenario {
  /** SCENARIO_UNSPECIFIED - Default value. */
  SCENARIO_UNSPECIFIED = 0,
  /** SCALING - Scaling recommendations such as initialExecutors. */
  SCALING = 2,
  /** BROADCAST_HASH_JOIN - Adding hints for potential relation broadcasts. */
  BROADCAST_HASH_JOIN = 3,
  /** MEMORY - Memory management for workloads. */
  MEMORY = 4,
  UNRECOGNIZED = -1,
}

export function autotuningConfig_ScenarioFromJSON(object: any): AutotuningConfig_Scenario {
  switch (object) {
    case 0:
    case "SCENARIO_UNSPECIFIED":
      return AutotuningConfig_Scenario.SCENARIO_UNSPECIFIED;
    case 2:
    case "SCALING":
      return AutotuningConfig_Scenario.SCALING;
    case 3:
    case "BROADCAST_HASH_JOIN":
      return AutotuningConfig_Scenario.BROADCAST_HASH_JOIN;
    case 4:
    case "MEMORY":
      return AutotuningConfig_Scenario.MEMORY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AutotuningConfig_Scenario.UNRECOGNIZED;
  }
}

export function autotuningConfig_ScenarioToJSON(object: AutotuningConfig_Scenario): string {
  switch (object) {
    case AutotuningConfig_Scenario.SCENARIO_UNSPECIFIED:
      return "SCENARIO_UNSPECIFIED";
    case AutotuningConfig_Scenario.SCALING:
      return "SCALING";
    case AutotuningConfig_Scenario.BROADCAST_HASH_JOIN:
      return "BROADCAST_HASH_JOIN";
    case AutotuningConfig_Scenario.MEMORY:
      return "MEMORY";
    case AutotuningConfig_Scenario.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Configuration for dependency repositories */
export interface RepositoryConfig {
  /** Optional. Configuration for PyPi repository. */
  pypiRepositoryConfig: PyPiRepositoryConfig | undefined;
}

/** Configuration for PyPi repository */
export interface PyPiRepositoryConfig {
  /** Optional. PyPi repository address */
  pypiRepository: string;
}

function createBaseRuntimeConfig(): RuntimeConfig {
  return {
    version: "",
    containerImage: "",
    properties: {},
    repositoryConfig: undefined,
    autotuningConfig: undefined,
    cohort: "",
  };
}

export const RuntimeConfig: MessageFns<RuntimeConfig> = {
  encode(message: RuntimeConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.version !== "") {
      writer.uint32(10).string(message.version);
    }
    if (message.containerImage !== "") {
      writer.uint32(18).string(message.containerImage);
    }
    Object.entries(message.properties).forEach(([key, value]) => {
      RuntimeConfig_PropertiesEntry.encode({ key: key as any, value }, writer.uint32(26).fork()).join();
    });
    if (message.repositoryConfig !== undefined) {
      RepositoryConfig.encode(message.repositoryConfig, writer.uint32(42).fork()).join();
    }
    if (message.autotuningConfig !== undefined) {
      AutotuningConfig.encode(message.autotuningConfig, writer.uint32(50).fork()).join();
    }
    if (message.cohort !== "") {
      writer.uint32(58).string(message.cohort);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RuntimeConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRuntimeConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.version = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.containerImage = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          const entry3 = RuntimeConfig_PropertiesEntry.decode(reader, reader.uint32());
          if (entry3.value !== undefined) {
            message.properties[entry3.key] = entry3.value;
          }
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.repositoryConfig = RepositoryConfig.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.autotuningConfig = AutotuningConfig.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.cohort = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RuntimeConfig {
    return {
      version: isSet(object.version) ? globalThis.String(object.version) : "",
      containerImage: isSet(object.containerImage) ? globalThis.String(object.containerImage) : "",
      properties: isObject(object.properties)
        ? Object.entries(object.properties).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      repositoryConfig: isSet(object.repositoryConfig) ? RepositoryConfig.fromJSON(object.repositoryConfig) : undefined,
      autotuningConfig: isSet(object.autotuningConfig) ? AutotuningConfig.fromJSON(object.autotuningConfig) : undefined,
      cohort: isSet(object.cohort) ? globalThis.String(object.cohort) : "",
    };
  },

  toJSON(message: RuntimeConfig): unknown {
    const obj: any = {};
    if (message.version !== "") {
      obj.version = message.version;
    }
    if (message.containerImage !== "") {
      obj.containerImage = message.containerImage;
    }
    if (message.properties) {
      const entries = Object.entries(message.properties);
      if (entries.length > 0) {
        obj.properties = {};
        entries.forEach(([k, v]) => {
          obj.properties[k] = v;
        });
      }
    }
    if (message.repositoryConfig !== undefined) {
      obj.repositoryConfig = RepositoryConfig.toJSON(message.repositoryConfig);
    }
    if (message.autotuningConfig !== undefined) {
      obj.autotuningConfig = AutotuningConfig.toJSON(message.autotuningConfig);
    }
    if (message.cohort !== "") {
      obj.cohort = message.cohort;
    }
    return obj;
  },

  create(base?: DeepPartial<RuntimeConfig>): RuntimeConfig {
    return RuntimeConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RuntimeConfig>): RuntimeConfig {
    const message = createBaseRuntimeConfig();
    message.version = object.version ?? "";
    message.containerImage = object.containerImage ?? "";
    message.properties = Object.entries(object.properties ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.repositoryConfig = (object.repositoryConfig !== undefined && object.repositoryConfig !== null)
      ? RepositoryConfig.fromPartial(object.repositoryConfig)
      : undefined;
    message.autotuningConfig = (object.autotuningConfig !== undefined && object.autotuningConfig !== null)
      ? AutotuningConfig.fromPartial(object.autotuningConfig)
      : undefined;
    message.cohort = object.cohort ?? "";
    return message;
  },
};

function createBaseRuntimeConfig_PropertiesEntry(): RuntimeConfig_PropertiesEntry {
  return { key: "", value: "" };
}

export const RuntimeConfig_PropertiesEntry: MessageFns<RuntimeConfig_PropertiesEntry> = {
  encode(message: RuntimeConfig_PropertiesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RuntimeConfig_PropertiesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRuntimeConfig_PropertiesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RuntimeConfig_PropertiesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: RuntimeConfig_PropertiesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<RuntimeConfig_PropertiesEntry>): RuntimeConfig_PropertiesEntry {
    return RuntimeConfig_PropertiesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RuntimeConfig_PropertiesEntry>): RuntimeConfig_PropertiesEntry {
    const message = createBaseRuntimeConfig_PropertiesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseEnvironmentConfig(): EnvironmentConfig {
  return { executionConfig: undefined, peripheralsConfig: undefined };
}

export const EnvironmentConfig: MessageFns<EnvironmentConfig> = {
  encode(message: EnvironmentConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.executionConfig !== undefined) {
      ExecutionConfig.encode(message.executionConfig, writer.uint32(10).fork()).join();
    }
    if (message.peripheralsConfig !== undefined) {
      PeripheralsConfig.encode(message.peripheralsConfig, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EnvironmentConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEnvironmentConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.executionConfig = ExecutionConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.peripheralsConfig = PeripheralsConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EnvironmentConfig {
    return {
      executionConfig: isSet(object.executionConfig) ? ExecutionConfig.fromJSON(object.executionConfig) : undefined,
      peripheralsConfig: isSet(object.peripheralsConfig)
        ? PeripheralsConfig.fromJSON(object.peripheralsConfig)
        : undefined,
    };
  },

  toJSON(message: EnvironmentConfig): unknown {
    const obj: any = {};
    if (message.executionConfig !== undefined) {
      obj.executionConfig = ExecutionConfig.toJSON(message.executionConfig);
    }
    if (message.peripheralsConfig !== undefined) {
      obj.peripheralsConfig = PeripheralsConfig.toJSON(message.peripheralsConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<EnvironmentConfig>): EnvironmentConfig {
    return EnvironmentConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EnvironmentConfig>): EnvironmentConfig {
    const message = createBaseEnvironmentConfig();
    message.executionConfig = (object.executionConfig !== undefined && object.executionConfig !== null)
      ? ExecutionConfig.fromPartial(object.executionConfig)
      : undefined;
    message.peripheralsConfig = (object.peripheralsConfig !== undefined && object.peripheralsConfig !== null)
      ? PeripheralsConfig.fromPartial(object.peripheralsConfig)
      : undefined;
    return message;
  },
};

function createBaseExecutionConfig(): ExecutionConfig {
  return {
    serviceAccount: "",
    networkUri: undefined,
    subnetworkUri: undefined,
    networkTags: [],
    kmsKey: "",
    idleTtl: undefined,
    ttl: undefined,
    stagingBucket: "",
  };
}

export const ExecutionConfig: MessageFns<ExecutionConfig> = {
  encode(message: ExecutionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.serviceAccount !== "") {
      writer.uint32(18).string(message.serviceAccount);
    }
    if (message.networkUri !== undefined) {
      writer.uint32(34).string(message.networkUri);
    }
    if (message.subnetworkUri !== undefined) {
      writer.uint32(42).string(message.subnetworkUri);
    }
    for (const v of message.networkTags) {
      writer.uint32(50).string(v!);
    }
    if (message.kmsKey !== "") {
      writer.uint32(58).string(message.kmsKey);
    }
    if (message.idleTtl !== undefined) {
      Duration.encode(message.idleTtl, writer.uint32(66).fork()).join();
    }
    if (message.ttl !== undefined) {
      Duration.encode(message.ttl, writer.uint32(74).fork()).join();
    }
    if (message.stagingBucket !== "") {
      writer.uint32(82).string(message.stagingBucket);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecutionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecutionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.serviceAccount = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.networkUri = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.subnetworkUri = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.networkTags.push(reader.string());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.kmsKey = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.idleTtl = Duration.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.ttl = Duration.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.stagingBucket = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecutionConfig {
    return {
      serviceAccount: isSet(object.serviceAccount) ? globalThis.String(object.serviceAccount) : "",
      networkUri: isSet(object.networkUri) ? globalThis.String(object.networkUri) : undefined,
      subnetworkUri: isSet(object.subnetworkUri) ? globalThis.String(object.subnetworkUri) : undefined,
      networkTags: globalThis.Array.isArray(object?.networkTags)
        ? object.networkTags.map((e: any) => globalThis.String(e))
        : [],
      kmsKey: isSet(object.kmsKey) ? globalThis.String(object.kmsKey) : "",
      idleTtl: isSet(object.idleTtl) ? Duration.fromJSON(object.idleTtl) : undefined,
      ttl: isSet(object.ttl) ? Duration.fromJSON(object.ttl) : undefined,
      stagingBucket: isSet(object.stagingBucket) ? globalThis.String(object.stagingBucket) : "",
    };
  },

  toJSON(message: ExecutionConfig): unknown {
    const obj: any = {};
    if (message.serviceAccount !== "") {
      obj.serviceAccount = message.serviceAccount;
    }
    if (message.networkUri !== undefined) {
      obj.networkUri = message.networkUri;
    }
    if (message.subnetworkUri !== undefined) {
      obj.subnetworkUri = message.subnetworkUri;
    }
    if (message.networkTags?.length) {
      obj.networkTags = message.networkTags;
    }
    if (message.kmsKey !== "") {
      obj.kmsKey = message.kmsKey;
    }
    if (message.idleTtl !== undefined) {
      obj.idleTtl = Duration.toJSON(message.idleTtl);
    }
    if (message.ttl !== undefined) {
      obj.ttl = Duration.toJSON(message.ttl);
    }
    if (message.stagingBucket !== "") {
      obj.stagingBucket = message.stagingBucket;
    }
    return obj;
  },

  create(base?: DeepPartial<ExecutionConfig>): ExecutionConfig {
    return ExecutionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecutionConfig>): ExecutionConfig {
    const message = createBaseExecutionConfig();
    message.serviceAccount = object.serviceAccount ?? "";
    message.networkUri = object.networkUri ?? undefined;
    message.subnetworkUri = object.subnetworkUri ?? undefined;
    message.networkTags = object.networkTags?.map((e) => e) || [];
    message.kmsKey = object.kmsKey ?? "";
    message.idleTtl = (object.idleTtl !== undefined && object.idleTtl !== null)
      ? Duration.fromPartial(object.idleTtl)
      : undefined;
    message.ttl = (object.ttl !== undefined && object.ttl !== null) ? Duration.fromPartial(object.ttl) : undefined;
    message.stagingBucket = object.stagingBucket ?? "";
    return message;
  },
};

function createBaseSparkHistoryServerConfig(): SparkHistoryServerConfig {
  return { dataprocCluster: "" };
}

export const SparkHistoryServerConfig: MessageFns<SparkHistoryServerConfig> = {
  encode(message: SparkHistoryServerConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dataprocCluster !== "") {
      writer.uint32(10).string(message.dataprocCluster);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SparkHistoryServerConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSparkHistoryServerConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dataprocCluster = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SparkHistoryServerConfig {
    return { dataprocCluster: isSet(object.dataprocCluster) ? globalThis.String(object.dataprocCluster) : "" };
  },

  toJSON(message: SparkHistoryServerConfig): unknown {
    const obj: any = {};
    if (message.dataprocCluster !== "") {
      obj.dataprocCluster = message.dataprocCluster;
    }
    return obj;
  },

  create(base?: DeepPartial<SparkHistoryServerConfig>): SparkHistoryServerConfig {
    return SparkHistoryServerConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SparkHistoryServerConfig>): SparkHistoryServerConfig {
    const message = createBaseSparkHistoryServerConfig();
    message.dataprocCluster = object.dataprocCluster ?? "";
    return message;
  },
};

function createBasePeripheralsConfig(): PeripheralsConfig {
  return { metastoreService: "", sparkHistoryServerConfig: undefined };
}

export const PeripheralsConfig: MessageFns<PeripheralsConfig> = {
  encode(message: PeripheralsConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.metastoreService !== "") {
      writer.uint32(10).string(message.metastoreService);
    }
    if (message.sparkHistoryServerConfig !== undefined) {
      SparkHistoryServerConfig.encode(message.sparkHistoryServerConfig, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PeripheralsConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePeripheralsConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.metastoreService = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.sparkHistoryServerConfig = SparkHistoryServerConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PeripheralsConfig {
    return {
      metastoreService: isSet(object.metastoreService) ? globalThis.String(object.metastoreService) : "",
      sparkHistoryServerConfig: isSet(object.sparkHistoryServerConfig)
        ? SparkHistoryServerConfig.fromJSON(object.sparkHistoryServerConfig)
        : undefined,
    };
  },

  toJSON(message: PeripheralsConfig): unknown {
    const obj: any = {};
    if (message.metastoreService !== "") {
      obj.metastoreService = message.metastoreService;
    }
    if (message.sparkHistoryServerConfig !== undefined) {
      obj.sparkHistoryServerConfig = SparkHistoryServerConfig.toJSON(message.sparkHistoryServerConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<PeripheralsConfig>): PeripheralsConfig {
    return PeripheralsConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PeripheralsConfig>): PeripheralsConfig {
    const message = createBasePeripheralsConfig();
    message.metastoreService = object.metastoreService ?? "";
    message.sparkHistoryServerConfig =
      (object.sparkHistoryServerConfig !== undefined && object.sparkHistoryServerConfig !== null)
        ? SparkHistoryServerConfig.fromPartial(object.sparkHistoryServerConfig)
        : undefined;
    return message;
  },
};

function createBaseRuntimeInfo(): RuntimeInfo {
  return {
    endpoints: {},
    outputUri: "",
    diagnosticOutputUri: "",
    approximateUsage: undefined,
    currentUsage: undefined,
  };
}

export const RuntimeInfo: MessageFns<RuntimeInfo> = {
  encode(message: RuntimeInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.endpoints).forEach(([key, value]) => {
      RuntimeInfo_EndpointsEntry.encode({ key: key as any, value }, writer.uint32(10).fork()).join();
    });
    if (message.outputUri !== "") {
      writer.uint32(18).string(message.outputUri);
    }
    if (message.diagnosticOutputUri !== "") {
      writer.uint32(26).string(message.diagnosticOutputUri);
    }
    if (message.approximateUsage !== undefined) {
      UsageMetrics.encode(message.approximateUsage, writer.uint32(50).fork()).join();
    }
    if (message.currentUsage !== undefined) {
      UsageSnapshot.encode(message.currentUsage, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RuntimeInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRuntimeInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          const entry1 = RuntimeInfo_EndpointsEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.endpoints[entry1.key] = entry1.value;
          }
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.outputUri = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.diagnosticOutputUri = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.approximateUsage = UsageMetrics.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.currentUsage = UsageSnapshot.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RuntimeInfo {
    return {
      endpoints: isObject(object.endpoints)
        ? Object.entries(object.endpoints).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      outputUri: isSet(object.outputUri) ? globalThis.String(object.outputUri) : "",
      diagnosticOutputUri: isSet(object.diagnosticOutputUri) ? globalThis.String(object.diagnosticOutputUri) : "",
      approximateUsage: isSet(object.approximateUsage) ? UsageMetrics.fromJSON(object.approximateUsage) : undefined,
      currentUsage: isSet(object.currentUsage) ? UsageSnapshot.fromJSON(object.currentUsage) : undefined,
    };
  },

  toJSON(message: RuntimeInfo): unknown {
    const obj: any = {};
    if (message.endpoints) {
      const entries = Object.entries(message.endpoints);
      if (entries.length > 0) {
        obj.endpoints = {};
        entries.forEach(([k, v]) => {
          obj.endpoints[k] = v;
        });
      }
    }
    if (message.outputUri !== "") {
      obj.outputUri = message.outputUri;
    }
    if (message.diagnosticOutputUri !== "") {
      obj.diagnosticOutputUri = message.diagnosticOutputUri;
    }
    if (message.approximateUsage !== undefined) {
      obj.approximateUsage = UsageMetrics.toJSON(message.approximateUsage);
    }
    if (message.currentUsage !== undefined) {
      obj.currentUsage = UsageSnapshot.toJSON(message.currentUsage);
    }
    return obj;
  },

  create(base?: DeepPartial<RuntimeInfo>): RuntimeInfo {
    return RuntimeInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RuntimeInfo>): RuntimeInfo {
    const message = createBaseRuntimeInfo();
    message.endpoints = Object.entries(object.endpoints ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.outputUri = object.outputUri ?? "";
    message.diagnosticOutputUri = object.diagnosticOutputUri ?? "";
    message.approximateUsage = (object.approximateUsage !== undefined && object.approximateUsage !== null)
      ? UsageMetrics.fromPartial(object.approximateUsage)
      : undefined;
    message.currentUsage = (object.currentUsage !== undefined && object.currentUsage !== null)
      ? UsageSnapshot.fromPartial(object.currentUsage)
      : undefined;
    return message;
  },
};

function createBaseRuntimeInfo_EndpointsEntry(): RuntimeInfo_EndpointsEntry {
  return { key: "", value: "" };
}

export const RuntimeInfo_EndpointsEntry: MessageFns<RuntimeInfo_EndpointsEntry> = {
  encode(message: RuntimeInfo_EndpointsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RuntimeInfo_EndpointsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRuntimeInfo_EndpointsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RuntimeInfo_EndpointsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: RuntimeInfo_EndpointsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<RuntimeInfo_EndpointsEntry>): RuntimeInfo_EndpointsEntry {
    return RuntimeInfo_EndpointsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RuntimeInfo_EndpointsEntry>): RuntimeInfo_EndpointsEntry {
    const message = createBaseRuntimeInfo_EndpointsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseUsageMetrics(): UsageMetrics {
  return {
    milliDcuSeconds: Long.ZERO,
    shuffleStorageGbSeconds: Long.ZERO,
    milliAcceleratorSeconds: Long.ZERO,
    acceleratorType: "",
  };
}

export const UsageMetrics: MessageFns<UsageMetrics> = {
  encode(message: UsageMetrics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.milliDcuSeconds.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.milliDcuSeconds.toString());
    }
    if (!message.shuffleStorageGbSeconds.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.shuffleStorageGbSeconds.toString());
    }
    if (!message.milliAcceleratorSeconds.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.milliAcceleratorSeconds.toString());
    }
    if (message.acceleratorType !== "") {
      writer.uint32(34).string(message.acceleratorType);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UsageMetrics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUsageMetrics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.milliDcuSeconds = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.shuffleStorageGbSeconds = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.milliAcceleratorSeconds = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.acceleratorType = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UsageMetrics {
    return {
      milliDcuSeconds: isSet(object.milliDcuSeconds) ? Long.fromValue(object.milliDcuSeconds) : Long.ZERO,
      shuffleStorageGbSeconds: isSet(object.shuffleStorageGbSeconds)
        ? Long.fromValue(object.shuffleStorageGbSeconds)
        : Long.ZERO,
      milliAcceleratorSeconds: isSet(object.milliAcceleratorSeconds)
        ? Long.fromValue(object.milliAcceleratorSeconds)
        : Long.ZERO,
      acceleratorType: isSet(object.acceleratorType) ? globalThis.String(object.acceleratorType) : "",
    };
  },

  toJSON(message: UsageMetrics): unknown {
    const obj: any = {};
    if (!message.milliDcuSeconds.equals(Long.ZERO)) {
      obj.milliDcuSeconds = (message.milliDcuSeconds || Long.ZERO).toString();
    }
    if (!message.shuffleStorageGbSeconds.equals(Long.ZERO)) {
      obj.shuffleStorageGbSeconds = (message.shuffleStorageGbSeconds || Long.ZERO).toString();
    }
    if (!message.milliAcceleratorSeconds.equals(Long.ZERO)) {
      obj.milliAcceleratorSeconds = (message.milliAcceleratorSeconds || Long.ZERO).toString();
    }
    if (message.acceleratorType !== "") {
      obj.acceleratorType = message.acceleratorType;
    }
    return obj;
  },

  create(base?: DeepPartial<UsageMetrics>): UsageMetrics {
    return UsageMetrics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UsageMetrics>): UsageMetrics {
    const message = createBaseUsageMetrics();
    message.milliDcuSeconds = (object.milliDcuSeconds !== undefined && object.milliDcuSeconds !== null)
      ? Long.fromValue(object.milliDcuSeconds)
      : Long.ZERO;
    message.shuffleStorageGbSeconds =
      (object.shuffleStorageGbSeconds !== undefined && object.shuffleStorageGbSeconds !== null)
        ? Long.fromValue(object.shuffleStorageGbSeconds)
        : Long.ZERO;
    message.milliAcceleratorSeconds =
      (object.milliAcceleratorSeconds !== undefined && object.milliAcceleratorSeconds !== null)
        ? Long.fromValue(object.milliAcceleratorSeconds)
        : Long.ZERO;
    message.acceleratorType = object.acceleratorType ?? "";
    return message;
  },
};

function createBaseUsageSnapshot(): UsageSnapshot {
  return {
    milliDcu: Long.ZERO,
    shuffleStorageGb: Long.ZERO,
    milliDcuPremium: Long.ZERO,
    shuffleStorageGbPremium: Long.ZERO,
    milliAccelerator: Long.ZERO,
    acceleratorType: "",
    snapshotTime: undefined,
  };
}

export const UsageSnapshot: MessageFns<UsageSnapshot> = {
  encode(message: UsageSnapshot, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.milliDcu.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.milliDcu.toString());
    }
    if (!message.shuffleStorageGb.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.shuffleStorageGb.toString());
    }
    if (!message.milliDcuPremium.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.milliDcuPremium.toString());
    }
    if (!message.shuffleStorageGbPremium.equals(Long.ZERO)) {
      writer.uint32(40).int64(message.shuffleStorageGbPremium.toString());
    }
    if (!message.milliAccelerator.equals(Long.ZERO)) {
      writer.uint32(48).int64(message.milliAccelerator.toString());
    }
    if (message.acceleratorType !== "") {
      writer.uint32(58).string(message.acceleratorType);
    }
    if (message.snapshotTime !== undefined) {
      Timestamp.encode(toTimestamp(message.snapshotTime), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UsageSnapshot {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUsageSnapshot();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.milliDcu = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.shuffleStorageGb = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.milliDcuPremium = Long.fromString(reader.int64().toString());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.shuffleStorageGbPremium = Long.fromString(reader.int64().toString());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.milliAccelerator = Long.fromString(reader.int64().toString());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.acceleratorType = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.snapshotTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UsageSnapshot {
    return {
      milliDcu: isSet(object.milliDcu) ? Long.fromValue(object.milliDcu) : Long.ZERO,
      shuffleStorageGb: isSet(object.shuffleStorageGb) ? Long.fromValue(object.shuffleStorageGb) : Long.ZERO,
      milliDcuPremium: isSet(object.milliDcuPremium) ? Long.fromValue(object.milliDcuPremium) : Long.ZERO,
      shuffleStorageGbPremium: isSet(object.shuffleStorageGbPremium)
        ? Long.fromValue(object.shuffleStorageGbPremium)
        : Long.ZERO,
      milliAccelerator: isSet(object.milliAccelerator) ? Long.fromValue(object.milliAccelerator) : Long.ZERO,
      acceleratorType: isSet(object.acceleratorType) ? globalThis.String(object.acceleratorType) : "",
      snapshotTime: isSet(object.snapshotTime) ? fromJsonTimestamp(object.snapshotTime) : undefined,
    };
  },

  toJSON(message: UsageSnapshot): unknown {
    const obj: any = {};
    if (!message.milliDcu.equals(Long.ZERO)) {
      obj.milliDcu = (message.milliDcu || Long.ZERO).toString();
    }
    if (!message.shuffleStorageGb.equals(Long.ZERO)) {
      obj.shuffleStorageGb = (message.shuffleStorageGb || Long.ZERO).toString();
    }
    if (!message.milliDcuPremium.equals(Long.ZERO)) {
      obj.milliDcuPremium = (message.milliDcuPremium || Long.ZERO).toString();
    }
    if (!message.shuffleStorageGbPremium.equals(Long.ZERO)) {
      obj.shuffleStorageGbPremium = (message.shuffleStorageGbPremium || Long.ZERO).toString();
    }
    if (!message.milliAccelerator.equals(Long.ZERO)) {
      obj.milliAccelerator = (message.milliAccelerator || Long.ZERO).toString();
    }
    if (message.acceleratorType !== "") {
      obj.acceleratorType = message.acceleratorType;
    }
    if (message.snapshotTime !== undefined) {
      obj.snapshotTime = message.snapshotTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<UsageSnapshot>): UsageSnapshot {
    return UsageSnapshot.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UsageSnapshot>): UsageSnapshot {
    const message = createBaseUsageSnapshot();
    message.milliDcu = (object.milliDcu !== undefined && object.milliDcu !== null)
      ? Long.fromValue(object.milliDcu)
      : Long.ZERO;
    message.shuffleStorageGb = (object.shuffleStorageGb !== undefined && object.shuffleStorageGb !== null)
      ? Long.fromValue(object.shuffleStorageGb)
      : Long.ZERO;
    message.milliDcuPremium = (object.milliDcuPremium !== undefined && object.milliDcuPremium !== null)
      ? Long.fromValue(object.milliDcuPremium)
      : Long.ZERO;
    message.shuffleStorageGbPremium =
      (object.shuffleStorageGbPremium !== undefined && object.shuffleStorageGbPremium !== null)
        ? Long.fromValue(object.shuffleStorageGbPremium)
        : Long.ZERO;
    message.milliAccelerator = (object.milliAccelerator !== undefined && object.milliAccelerator !== null)
      ? Long.fromValue(object.milliAccelerator)
      : Long.ZERO;
    message.acceleratorType = object.acceleratorType ?? "";
    message.snapshotTime = object.snapshotTime ?? undefined;
    return message;
  },
};

function createBaseGkeClusterConfig(): GkeClusterConfig {
  return { gkeClusterTarget: "", nodePoolTarget: [] };
}

export const GkeClusterConfig: MessageFns<GkeClusterConfig> = {
  encode(message: GkeClusterConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gkeClusterTarget !== "") {
      writer.uint32(18).string(message.gkeClusterTarget);
    }
    for (const v of message.nodePoolTarget) {
      GkeNodePoolTarget.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GkeClusterConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGkeClusterConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.gkeClusterTarget = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.nodePoolTarget.push(GkeNodePoolTarget.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GkeClusterConfig {
    return {
      gkeClusterTarget: isSet(object.gkeClusterTarget) ? globalThis.String(object.gkeClusterTarget) : "",
      nodePoolTarget: globalThis.Array.isArray(object?.nodePoolTarget)
        ? object.nodePoolTarget.map((e: any) => GkeNodePoolTarget.fromJSON(e))
        : [],
    };
  },

  toJSON(message: GkeClusterConfig): unknown {
    const obj: any = {};
    if (message.gkeClusterTarget !== "") {
      obj.gkeClusterTarget = message.gkeClusterTarget;
    }
    if (message.nodePoolTarget?.length) {
      obj.nodePoolTarget = message.nodePoolTarget.map((e) => GkeNodePoolTarget.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<GkeClusterConfig>): GkeClusterConfig {
    return GkeClusterConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GkeClusterConfig>): GkeClusterConfig {
    const message = createBaseGkeClusterConfig();
    message.gkeClusterTarget = object.gkeClusterTarget ?? "";
    message.nodePoolTarget = object.nodePoolTarget?.map((e) => GkeNodePoolTarget.fromPartial(e)) || [];
    return message;
  },
};

function createBaseKubernetesClusterConfig(): KubernetesClusterConfig {
  return { kubernetesNamespace: "", gkeClusterConfig: undefined, kubernetesSoftwareConfig: undefined };
}

export const KubernetesClusterConfig: MessageFns<KubernetesClusterConfig> = {
  encode(message: KubernetesClusterConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kubernetesNamespace !== "") {
      writer.uint32(10).string(message.kubernetesNamespace);
    }
    if (message.gkeClusterConfig !== undefined) {
      GkeClusterConfig.encode(message.gkeClusterConfig, writer.uint32(18).fork()).join();
    }
    if (message.kubernetesSoftwareConfig !== undefined) {
      KubernetesSoftwareConfig.encode(message.kubernetesSoftwareConfig, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KubernetesClusterConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKubernetesClusterConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kubernetesNamespace = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.gkeClusterConfig = GkeClusterConfig.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.kubernetesSoftwareConfig = KubernetesSoftwareConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KubernetesClusterConfig {
    return {
      kubernetesNamespace: isSet(object.kubernetesNamespace) ? globalThis.String(object.kubernetesNamespace) : "",
      gkeClusterConfig: isSet(object.gkeClusterConfig) ? GkeClusterConfig.fromJSON(object.gkeClusterConfig) : undefined,
      kubernetesSoftwareConfig: isSet(object.kubernetesSoftwareConfig)
        ? KubernetesSoftwareConfig.fromJSON(object.kubernetesSoftwareConfig)
        : undefined,
    };
  },

  toJSON(message: KubernetesClusterConfig): unknown {
    const obj: any = {};
    if (message.kubernetesNamespace !== "") {
      obj.kubernetesNamespace = message.kubernetesNamespace;
    }
    if (message.gkeClusterConfig !== undefined) {
      obj.gkeClusterConfig = GkeClusterConfig.toJSON(message.gkeClusterConfig);
    }
    if (message.kubernetesSoftwareConfig !== undefined) {
      obj.kubernetesSoftwareConfig = KubernetesSoftwareConfig.toJSON(message.kubernetesSoftwareConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<KubernetesClusterConfig>): KubernetesClusterConfig {
    return KubernetesClusterConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<KubernetesClusterConfig>): KubernetesClusterConfig {
    const message = createBaseKubernetesClusterConfig();
    message.kubernetesNamespace = object.kubernetesNamespace ?? "";
    message.gkeClusterConfig = (object.gkeClusterConfig !== undefined && object.gkeClusterConfig !== null)
      ? GkeClusterConfig.fromPartial(object.gkeClusterConfig)
      : undefined;
    message.kubernetesSoftwareConfig =
      (object.kubernetesSoftwareConfig !== undefined && object.kubernetesSoftwareConfig !== null)
        ? KubernetesSoftwareConfig.fromPartial(object.kubernetesSoftwareConfig)
        : undefined;
    return message;
  },
};

function createBaseKubernetesSoftwareConfig(): KubernetesSoftwareConfig {
  return { componentVersion: {}, properties: {} };
}

export const KubernetesSoftwareConfig: MessageFns<KubernetesSoftwareConfig> = {
  encode(message: KubernetesSoftwareConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.componentVersion).forEach(([key, value]) => {
      KubernetesSoftwareConfig_ComponentVersionEntry.encode({ key: key as any, value }, writer.uint32(10).fork())
        .join();
    });
    Object.entries(message.properties).forEach(([key, value]) => {
      KubernetesSoftwareConfig_PropertiesEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KubernetesSoftwareConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKubernetesSoftwareConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          const entry1 = KubernetesSoftwareConfig_ComponentVersionEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.componentVersion[entry1.key] = entry1.value;
          }
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = KubernetesSoftwareConfig_PropertiesEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.properties[entry2.key] = entry2.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KubernetesSoftwareConfig {
    return {
      componentVersion: isObject(object.componentVersion)
        ? Object.entries(object.componentVersion).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      properties: isObject(object.properties)
        ? Object.entries(object.properties).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: KubernetesSoftwareConfig): unknown {
    const obj: any = {};
    if (message.componentVersion) {
      const entries = Object.entries(message.componentVersion);
      if (entries.length > 0) {
        obj.componentVersion = {};
        entries.forEach(([k, v]) => {
          obj.componentVersion[k] = v;
        });
      }
    }
    if (message.properties) {
      const entries = Object.entries(message.properties);
      if (entries.length > 0) {
        obj.properties = {};
        entries.forEach(([k, v]) => {
          obj.properties[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<KubernetesSoftwareConfig>): KubernetesSoftwareConfig {
    return KubernetesSoftwareConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<KubernetesSoftwareConfig>): KubernetesSoftwareConfig {
    const message = createBaseKubernetesSoftwareConfig();
    message.componentVersion = Object.entries(object.componentVersion ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.properties = Object.entries(object.properties ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseKubernetesSoftwareConfig_ComponentVersionEntry(): KubernetesSoftwareConfig_ComponentVersionEntry {
  return { key: "", value: "" };
}

export const KubernetesSoftwareConfig_ComponentVersionEntry: MessageFns<
  KubernetesSoftwareConfig_ComponentVersionEntry
> = {
  encode(
    message: KubernetesSoftwareConfig_ComponentVersionEntry,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KubernetesSoftwareConfig_ComponentVersionEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKubernetesSoftwareConfig_ComponentVersionEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KubernetesSoftwareConfig_ComponentVersionEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: KubernetesSoftwareConfig_ComponentVersionEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(
    base?: DeepPartial<KubernetesSoftwareConfig_ComponentVersionEntry>,
  ): KubernetesSoftwareConfig_ComponentVersionEntry {
    return KubernetesSoftwareConfig_ComponentVersionEntry.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<KubernetesSoftwareConfig_ComponentVersionEntry>,
  ): KubernetesSoftwareConfig_ComponentVersionEntry {
    const message = createBaseKubernetesSoftwareConfig_ComponentVersionEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseKubernetesSoftwareConfig_PropertiesEntry(): KubernetesSoftwareConfig_PropertiesEntry {
  return { key: "", value: "" };
}

export const KubernetesSoftwareConfig_PropertiesEntry: MessageFns<KubernetesSoftwareConfig_PropertiesEntry> = {
  encode(message: KubernetesSoftwareConfig_PropertiesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KubernetesSoftwareConfig_PropertiesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKubernetesSoftwareConfig_PropertiesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KubernetesSoftwareConfig_PropertiesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: KubernetesSoftwareConfig_PropertiesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<KubernetesSoftwareConfig_PropertiesEntry>): KubernetesSoftwareConfig_PropertiesEntry {
    return KubernetesSoftwareConfig_PropertiesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<KubernetesSoftwareConfig_PropertiesEntry>): KubernetesSoftwareConfig_PropertiesEntry {
    const message = createBaseKubernetesSoftwareConfig_PropertiesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseGkeNodePoolTarget(): GkeNodePoolTarget {
  return { nodePool: "", roles: [], nodePoolConfig: undefined };
}

export const GkeNodePoolTarget: MessageFns<GkeNodePoolTarget> = {
  encode(message: GkeNodePoolTarget, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.nodePool !== "") {
      writer.uint32(10).string(message.nodePool);
    }
    writer.uint32(18).fork();
    for (const v of message.roles) {
      writer.int32(v);
    }
    writer.join();
    if (message.nodePoolConfig !== undefined) {
      GkeNodePoolConfig.encode(message.nodePoolConfig, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GkeNodePoolTarget {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGkeNodePoolTarget();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.nodePool = reader.string();
          continue;
        case 2:
          if (tag === 16) {
            message.roles.push(reader.int32() as any);

            continue;
          }

          if (tag === 18) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.roles.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.nodePoolConfig = GkeNodePoolConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GkeNodePoolTarget {
    return {
      nodePool: isSet(object.nodePool) ? globalThis.String(object.nodePool) : "",
      roles: globalThis.Array.isArray(object?.roles)
        ? object.roles.map((e: any) => gkeNodePoolTarget_RoleFromJSON(e))
        : [],
      nodePoolConfig: isSet(object.nodePoolConfig) ? GkeNodePoolConfig.fromJSON(object.nodePoolConfig) : undefined,
    };
  },

  toJSON(message: GkeNodePoolTarget): unknown {
    const obj: any = {};
    if (message.nodePool !== "") {
      obj.nodePool = message.nodePool;
    }
    if (message.roles?.length) {
      obj.roles = message.roles.map((e) => gkeNodePoolTarget_RoleToJSON(e));
    }
    if (message.nodePoolConfig !== undefined) {
      obj.nodePoolConfig = GkeNodePoolConfig.toJSON(message.nodePoolConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<GkeNodePoolTarget>): GkeNodePoolTarget {
    return GkeNodePoolTarget.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GkeNodePoolTarget>): GkeNodePoolTarget {
    const message = createBaseGkeNodePoolTarget();
    message.nodePool = object.nodePool ?? "";
    message.roles = object.roles?.map((e) => e) || [];
    message.nodePoolConfig = (object.nodePoolConfig !== undefined && object.nodePoolConfig !== null)
      ? GkeNodePoolConfig.fromPartial(object.nodePoolConfig)
      : undefined;
    return message;
  },
};

function createBaseGkeNodePoolConfig(): GkeNodePoolConfig {
  return { config: undefined, locations: [], autoscaling: undefined };
}

export const GkeNodePoolConfig: MessageFns<GkeNodePoolConfig> = {
  encode(message: GkeNodePoolConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.config !== undefined) {
      GkeNodePoolConfig_GkeNodeConfig.encode(message.config, writer.uint32(18).fork()).join();
    }
    for (const v of message.locations) {
      writer.uint32(106).string(v!);
    }
    if (message.autoscaling !== undefined) {
      GkeNodePoolConfig_GkeNodePoolAutoscalingConfig.encode(message.autoscaling, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GkeNodePoolConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGkeNodePoolConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.config = GkeNodePoolConfig_GkeNodeConfig.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.locations.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.autoscaling = GkeNodePoolConfig_GkeNodePoolAutoscalingConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GkeNodePoolConfig {
    return {
      config: isSet(object.config) ? GkeNodePoolConfig_GkeNodeConfig.fromJSON(object.config) : undefined,
      locations: globalThis.Array.isArray(object?.locations)
        ? object.locations.map((e: any) => globalThis.String(e))
        : [],
      autoscaling: isSet(object.autoscaling)
        ? GkeNodePoolConfig_GkeNodePoolAutoscalingConfig.fromJSON(object.autoscaling)
        : undefined,
    };
  },

  toJSON(message: GkeNodePoolConfig): unknown {
    const obj: any = {};
    if (message.config !== undefined) {
      obj.config = GkeNodePoolConfig_GkeNodeConfig.toJSON(message.config);
    }
    if (message.locations?.length) {
      obj.locations = message.locations;
    }
    if (message.autoscaling !== undefined) {
      obj.autoscaling = GkeNodePoolConfig_GkeNodePoolAutoscalingConfig.toJSON(message.autoscaling);
    }
    return obj;
  },

  create(base?: DeepPartial<GkeNodePoolConfig>): GkeNodePoolConfig {
    return GkeNodePoolConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GkeNodePoolConfig>): GkeNodePoolConfig {
    const message = createBaseGkeNodePoolConfig();
    message.config = (object.config !== undefined && object.config !== null)
      ? GkeNodePoolConfig_GkeNodeConfig.fromPartial(object.config)
      : undefined;
    message.locations = object.locations?.map((e) => e) || [];
    message.autoscaling = (object.autoscaling !== undefined && object.autoscaling !== null)
      ? GkeNodePoolConfig_GkeNodePoolAutoscalingConfig.fromPartial(object.autoscaling)
      : undefined;
    return message;
  },
};

function createBaseGkeNodePoolConfig_GkeNodeConfig(): GkeNodePoolConfig_GkeNodeConfig {
  return {
    machineType: "",
    localSsdCount: 0,
    preemptible: false,
    accelerators: [],
    minCpuPlatform: "",
    bootDiskKmsKey: "",
    spot: false,
  };
}

export const GkeNodePoolConfig_GkeNodeConfig: MessageFns<GkeNodePoolConfig_GkeNodeConfig> = {
  encode(message: GkeNodePoolConfig_GkeNodeConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.machineType !== "") {
      writer.uint32(10).string(message.machineType);
    }
    if (message.localSsdCount !== 0) {
      writer.uint32(56).int32(message.localSsdCount);
    }
    if (message.preemptible !== false) {
      writer.uint32(80).bool(message.preemptible);
    }
    for (const v of message.accelerators) {
      GkeNodePoolConfig_GkeNodePoolAcceleratorConfig.encode(v!, writer.uint32(90).fork()).join();
    }
    if (message.minCpuPlatform !== "") {
      writer.uint32(106).string(message.minCpuPlatform);
    }
    if (message.bootDiskKmsKey !== "") {
      writer.uint32(186).string(message.bootDiskKmsKey);
    }
    if (message.spot !== false) {
      writer.uint32(256).bool(message.spot);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GkeNodePoolConfig_GkeNodeConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGkeNodePoolConfig_GkeNodeConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.machineType = reader.string();
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.localSsdCount = reader.int32();
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.preemptible = reader.bool();
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.accelerators.push(GkeNodePoolConfig_GkeNodePoolAcceleratorConfig.decode(reader, reader.uint32()));
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.minCpuPlatform = reader.string();
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.bootDiskKmsKey = reader.string();
          continue;
        case 32:
          if (tag !== 256) {
            break;
          }

          message.spot = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GkeNodePoolConfig_GkeNodeConfig {
    return {
      machineType: isSet(object.machineType) ? globalThis.String(object.machineType) : "",
      localSsdCount: isSet(object.localSsdCount) ? globalThis.Number(object.localSsdCount) : 0,
      preemptible: isSet(object.preemptible) ? globalThis.Boolean(object.preemptible) : false,
      accelerators: globalThis.Array.isArray(object?.accelerators)
        ? object.accelerators.map((e: any) => GkeNodePoolConfig_GkeNodePoolAcceleratorConfig.fromJSON(e))
        : [],
      minCpuPlatform: isSet(object.minCpuPlatform) ? globalThis.String(object.minCpuPlatform) : "",
      bootDiskKmsKey: isSet(object.bootDiskKmsKey) ? globalThis.String(object.bootDiskKmsKey) : "",
      spot: isSet(object.spot) ? globalThis.Boolean(object.spot) : false,
    };
  },

  toJSON(message: GkeNodePoolConfig_GkeNodeConfig): unknown {
    const obj: any = {};
    if (message.machineType !== "") {
      obj.machineType = message.machineType;
    }
    if (message.localSsdCount !== 0) {
      obj.localSsdCount = Math.round(message.localSsdCount);
    }
    if (message.preemptible !== false) {
      obj.preemptible = message.preemptible;
    }
    if (message.accelerators?.length) {
      obj.accelerators = message.accelerators.map((e) => GkeNodePoolConfig_GkeNodePoolAcceleratorConfig.toJSON(e));
    }
    if (message.minCpuPlatform !== "") {
      obj.minCpuPlatform = message.minCpuPlatform;
    }
    if (message.bootDiskKmsKey !== "") {
      obj.bootDiskKmsKey = message.bootDiskKmsKey;
    }
    if (message.spot !== false) {
      obj.spot = message.spot;
    }
    return obj;
  },

  create(base?: DeepPartial<GkeNodePoolConfig_GkeNodeConfig>): GkeNodePoolConfig_GkeNodeConfig {
    return GkeNodePoolConfig_GkeNodeConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GkeNodePoolConfig_GkeNodeConfig>): GkeNodePoolConfig_GkeNodeConfig {
    const message = createBaseGkeNodePoolConfig_GkeNodeConfig();
    message.machineType = object.machineType ?? "";
    message.localSsdCount = object.localSsdCount ?? 0;
    message.preemptible = object.preemptible ?? false;
    message.accelerators =
      object.accelerators?.map((e) => GkeNodePoolConfig_GkeNodePoolAcceleratorConfig.fromPartial(e)) || [];
    message.minCpuPlatform = object.minCpuPlatform ?? "";
    message.bootDiskKmsKey = object.bootDiskKmsKey ?? "";
    message.spot = object.spot ?? false;
    return message;
  },
};

function createBaseGkeNodePoolConfig_GkeNodePoolAcceleratorConfig(): GkeNodePoolConfig_GkeNodePoolAcceleratorConfig {
  return { acceleratorCount: Long.ZERO, acceleratorType: "", gpuPartitionSize: "" };
}

export const GkeNodePoolConfig_GkeNodePoolAcceleratorConfig: MessageFns<
  GkeNodePoolConfig_GkeNodePoolAcceleratorConfig
> = {
  encode(
    message: GkeNodePoolConfig_GkeNodePoolAcceleratorConfig,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (!message.acceleratorCount.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.acceleratorCount.toString());
    }
    if (message.acceleratorType !== "") {
      writer.uint32(18).string(message.acceleratorType);
    }
    if (message.gpuPartitionSize !== "") {
      writer.uint32(26).string(message.gpuPartitionSize);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GkeNodePoolConfig_GkeNodePoolAcceleratorConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGkeNodePoolConfig_GkeNodePoolAcceleratorConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.acceleratorCount = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.acceleratorType = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.gpuPartitionSize = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GkeNodePoolConfig_GkeNodePoolAcceleratorConfig {
    return {
      acceleratorCount: isSet(object.acceleratorCount) ? Long.fromValue(object.acceleratorCount) : Long.ZERO,
      acceleratorType: isSet(object.acceleratorType) ? globalThis.String(object.acceleratorType) : "",
      gpuPartitionSize: isSet(object.gpuPartitionSize) ? globalThis.String(object.gpuPartitionSize) : "",
    };
  },

  toJSON(message: GkeNodePoolConfig_GkeNodePoolAcceleratorConfig): unknown {
    const obj: any = {};
    if (!message.acceleratorCount.equals(Long.ZERO)) {
      obj.acceleratorCount = (message.acceleratorCount || Long.ZERO).toString();
    }
    if (message.acceleratorType !== "") {
      obj.acceleratorType = message.acceleratorType;
    }
    if (message.gpuPartitionSize !== "") {
      obj.gpuPartitionSize = message.gpuPartitionSize;
    }
    return obj;
  },

  create(
    base?: DeepPartial<GkeNodePoolConfig_GkeNodePoolAcceleratorConfig>,
  ): GkeNodePoolConfig_GkeNodePoolAcceleratorConfig {
    return GkeNodePoolConfig_GkeNodePoolAcceleratorConfig.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<GkeNodePoolConfig_GkeNodePoolAcceleratorConfig>,
  ): GkeNodePoolConfig_GkeNodePoolAcceleratorConfig {
    const message = createBaseGkeNodePoolConfig_GkeNodePoolAcceleratorConfig();
    message.acceleratorCount = (object.acceleratorCount !== undefined && object.acceleratorCount !== null)
      ? Long.fromValue(object.acceleratorCount)
      : Long.ZERO;
    message.acceleratorType = object.acceleratorType ?? "";
    message.gpuPartitionSize = object.gpuPartitionSize ?? "";
    return message;
  },
};

function createBaseGkeNodePoolConfig_GkeNodePoolAutoscalingConfig(): GkeNodePoolConfig_GkeNodePoolAutoscalingConfig {
  return { minNodeCount: 0, maxNodeCount: 0 };
}

export const GkeNodePoolConfig_GkeNodePoolAutoscalingConfig: MessageFns<
  GkeNodePoolConfig_GkeNodePoolAutoscalingConfig
> = {
  encode(
    message: GkeNodePoolConfig_GkeNodePoolAutoscalingConfig,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.minNodeCount !== 0) {
      writer.uint32(16).int32(message.minNodeCount);
    }
    if (message.maxNodeCount !== 0) {
      writer.uint32(24).int32(message.maxNodeCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GkeNodePoolConfig_GkeNodePoolAutoscalingConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGkeNodePoolConfig_GkeNodePoolAutoscalingConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 16) {
            break;
          }

          message.minNodeCount = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.maxNodeCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GkeNodePoolConfig_GkeNodePoolAutoscalingConfig {
    return {
      minNodeCount: isSet(object.minNodeCount) ? globalThis.Number(object.minNodeCount) : 0,
      maxNodeCount: isSet(object.maxNodeCount) ? globalThis.Number(object.maxNodeCount) : 0,
    };
  },

  toJSON(message: GkeNodePoolConfig_GkeNodePoolAutoscalingConfig): unknown {
    const obj: any = {};
    if (message.minNodeCount !== 0) {
      obj.minNodeCount = Math.round(message.minNodeCount);
    }
    if (message.maxNodeCount !== 0) {
      obj.maxNodeCount = Math.round(message.maxNodeCount);
    }
    return obj;
  },

  create(
    base?: DeepPartial<GkeNodePoolConfig_GkeNodePoolAutoscalingConfig>,
  ): GkeNodePoolConfig_GkeNodePoolAutoscalingConfig {
    return GkeNodePoolConfig_GkeNodePoolAutoscalingConfig.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<GkeNodePoolConfig_GkeNodePoolAutoscalingConfig>,
  ): GkeNodePoolConfig_GkeNodePoolAutoscalingConfig {
    const message = createBaseGkeNodePoolConfig_GkeNodePoolAutoscalingConfig();
    message.minNodeCount = object.minNodeCount ?? 0;
    message.maxNodeCount = object.maxNodeCount ?? 0;
    return message;
  },
};

function createBaseAutotuningConfig(): AutotuningConfig {
  return { scenarios: [] };
}

export const AutotuningConfig: MessageFns<AutotuningConfig> = {
  encode(message: AutotuningConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    writer.uint32(18).fork();
    for (const v of message.scenarios) {
      writer.int32(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutotuningConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutotuningConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag === 16) {
            message.scenarios.push(reader.int32() as any);

            continue;
          }

          if (tag === 18) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.scenarios.push(reader.int32() as any);
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutotuningConfig {
    return {
      scenarios: globalThis.Array.isArray(object?.scenarios)
        ? object.scenarios.map((e: any) => autotuningConfig_ScenarioFromJSON(e))
        : [],
    };
  },

  toJSON(message: AutotuningConfig): unknown {
    const obj: any = {};
    if (message.scenarios?.length) {
      obj.scenarios = message.scenarios.map((e) => autotuningConfig_ScenarioToJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<AutotuningConfig>): AutotuningConfig {
    return AutotuningConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutotuningConfig>): AutotuningConfig {
    const message = createBaseAutotuningConfig();
    message.scenarios = object.scenarios?.map((e) => e) || [];
    return message;
  },
};

function createBaseRepositoryConfig(): RepositoryConfig {
  return { pypiRepositoryConfig: undefined };
}

export const RepositoryConfig: MessageFns<RepositoryConfig> = {
  encode(message: RepositoryConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.pypiRepositoryConfig !== undefined) {
      PyPiRepositoryConfig.encode(message.pypiRepositoryConfig, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RepositoryConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRepositoryConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.pypiRepositoryConfig = PyPiRepositoryConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RepositoryConfig {
    return {
      pypiRepositoryConfig: isSet(object.pypiRepositoryConfig)
        ? PyPiRepositoryConfig.fromJSON(object.pypiRepositoryConfig)
        : undefined,
    };
  },

  toJSON(message: RepositoryConfig): unknown {
    const obj: any = {};
    if (message.pypiRepositoryConfig !== undefined) {
      obj.pypiRepositoryConfig = PyPiRepositoryConfig.toJSON(message.pypiRepositoryConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<RepositoryConfig>): RepositoryConfig {
    return RepositoryConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RepositoryConfig>): RepositoryConfig {
    const message = createBaseRepositoryConfig();
    message.pypiRepositoryConfig = (object.pypiRepositoryConfig !== undefined && object.pypiRepositoryConfig !== null)
      ? PyPiRepositoryConfig.fromPartial(object.pypiRepositoryConfig)
      : undefined;
    return message;
  },
};

function createBasePyPiRepositoryConfig(): PyPiRepositoryConfig {
  return { pypiRepository: "" };
}

export const PyPiRepositoryConfig: MessageFns<PyPiRepositoryConfig> = {
  encode(message: PyPiRepositoryConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.pypiRepository !== "") {
      writer.uint32(10).string(message.pypiRepository);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PyPiRepositoryConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePyPiRepositoryConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.pypiRepository = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PyPiRepositoryConfig {
    return { pypiRepository: isSet(object.pypiRepository) ? globalThis.String(object.pypiRepository) : "" };
  },

  toJSON(message: PyPiRepositoryConfig): unknown {
    const obj: any = {};
    if (message.pypiRepository !== "") {
      obj.pypiRepository = message.pypiRepository;
    }
    return obj;
  },

  create(base?: DeepPartial<PyPiRepositoryConfig>): PyPiRepositoryConfig {
    return PyPiRepositoryConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PyPiRepositoryConfig>): PyPiRepositoryConfig {
    const message = createBasePyPiRepositoryConfig();
    message.pypiRepository = object.pypiRepository ?? "";
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
