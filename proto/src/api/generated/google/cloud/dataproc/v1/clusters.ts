// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/dataproc/v1/clusters.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Operation } from "../../../longrunning/operations.js";
import { Duration } from "../../../protobuf/duration.js";
import { FieldMask } from "../../../protobuf/field_mask.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { Interval } from "../../../type/interval.js";
import {
  Component,
  componentFromJSON,
  componentToJSON,
  FailureAction,
  failureActionFromJSON,
  failureActionToJSON,
  KubernetesClusterConfig,
  SparkHistoryServerConfig,
} from "./shared.js";

export const protobufPackage = "google.cloud.dataproc.v1";

/**
 * Describes the identifying information, config, and status of
 * a Dataproc cluster
 */
export interface Cluster {
  /** Required. The Google Cloud Platform project ID that the cluster belongs to. */
  projectId: string;
  /**
   * Required. The cluster name, which must be unique within a project.
   * The name must start with a lowercase letter, and can contain
   * up to 51 lowercase letters, numbers, and hyphens. It cannot end
   * with a hyphen. The name of a deleted cluster can be reused.
   */
  clusterName: string;
  /**
   * Optional. The cluster config for a cluster of Compute Engine Instances.
   * Note that Dataproc may set default values, and values may change
   * when clusters are updated.
   *
   * Exactly one of ClusterConfig or VirtualClusterConfig must be specified.
   */
  config:
    | ClusterConfig
    | undefined;
  /**
   * Optional. The virtual cluster config is used when creating a Dataproc
   * cluster that does not directly control the underlying compute resources,
   * for example, when creating a [Dataproc-on-GKE
   * cluster](https://cloud.google.com/dataproc/docs/guides/dpgke/dataproc-gke-overview).
   * Dataproc may set default values, and values may change when
   * clusters are updated. Exactly one of
   * [config][google.cloud.dataproc.v1.Cluster.config] or
   * [virtual_cluster_config][google.cloud.dataproc.v1.Cluster.virtual_cluster_config]
   * must be specified.
   */
  virtualClusterConfig:
    | VirtualClusterConfig
    | undefined;
  /**
   * Optional. The labels to associate with this cluster.
   * Label **keys** must contain 1 to 63 characters, and must conform to
   * [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).
   * Label **values** may be empty, but, if present, must contain 1 to 63
   * characters, and must conform to [RFC
   * 1035](https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be
   * associated with a cluster.
   */
  labels: { [key: string]: string };
  /** Output only. Cluster status. */
  status:
    | ClusterStatus
    | undefined;
  /** Output only. The previous cluster status. */
  statusHistory: ClusterStatus[];
  /**
   * Output only. A cluster UUID (Unique Universal Identifier). Dataproc
   * generates this value when it creates the cluster.
   */
  clusterUuid: string;
  /**
   * Output only. Contains cluster daemon metrics such as HDFS and YARN stats.
   *
   * **Beta Feature**: This report is available for testing purposes only. It
   * may be changed before final release.
   */
  metrics: ClusterMetrics | undefined;
}

export interface Cluster_LabelsEntry {
  key: string;
  value: string;
}

/** The cluster config. */
export interface ClusterConfig {
  /**
   * Optional. A Cloud Storage bucket used to stage job
   * dependencies, config files, and job driver console output.
   * If you do not specify a staging bucket, Cloud
   * Dataproc will determine a Cloud Storage location (US,
   * ASIA, or EU) for your cluster's staging bucket according to the
   * Compute Engine zone where your cluster is deployed, and then create
   * and manage this project-level, per-location bucket (see
   * [Dataproc staging and temp
   * buckets](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
   * **This field requires a Cloud Storage bucket name, not a `gs://...` URI to
   * a Cloud Storage bucket.**
   */
  configBucket: string;
  /**
   * Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs
   * data, such as Spark and MapReduce history files. If you do not specify a
   * temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or
   * EU) for your cluster's temp bucket according to the Compute Engine zone
   * where your cluster is deployed, and then create and manage this
   * project-level, per-location bucket. The default bucket has a TTL of 90
   * days, but you can use any TTL (or none) if you specify a bucket (see
   * [Dataproc staging and temp
   * buckets](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
   * **This field requires a Cloud Storage bucket name, not a `gs://...` URI to
   * a Cloud Storage bucket.**
   */
  tempBucket: string;
  /**
   * Optional. The shared Compute Engine config settings for
   * all instances in a cluster.
   */
  gceClusterConfig:
    | GceClusterConfig
    | undefined;
  /**
   * Optional. The Compute Engine config settings for
   * the cluster's master instance.
   */
  masterConfig:
    | InstanceGroupConfig
    | undefined;
  /**
   * Optional. The Compute Engine config settings for
   * the cluster's worker instances.
   */
  workerConfig:
    | InstanceGroupConfig
    | undefined;
  /**
   * Optional. The Compute Engine config settings for
   * a cluster's secondary worker instances
   */
  secondaryWorkerConfig:
    | InstanceGroupConfig
    | undefined;
  /** Optional. The config settings for cluster software. */
  softwareConfig:
    | SoftwareConfig
    | undefined;
  /**
   * Optional. Commands to execute on each node after config is
   * completed. By default, executables are run on master and all worker nodes.
   * You can test a node's `role` metadata to run an executable on
   * a master or worker node, as shown below using `curl` (you can also use
   * `wget`):
   *
   *     ROLE=$(curl -H Metadata-Flavor:Google
   *     http://metadata/computeMetadata/v1/instance/attributes/dataproc-role)
   *     if [[ "${ROLE}" == 'Master' ]]; then
   *       ... master specific actions ...
   *     else
   *       ... worker specific actions ...
   *     fi
   */
  initializationActions: NodeInitializationAction[];
  /** Optional. Encryption settings for the cluster. */
  encryptionConfig:
    | EncryptionConfig
    | undefined;
  /**
   * Optional. Autoscaling config for the policy associated with the cluster.
   * Cluster does not autoscale if this field is unset.
   */
  autoscalingConfig:
    | AutoscalingConfig
    | undefined;
  /** Optional. Security settings for the cluster. */
  securityConfig:
    | SecurityConfig
    | undefined;
  /** Optional. Lifecycle setting for the cluster. */
  lifecycleConfig:
    | LifecycleConfig
    | undefined;
  /** Optional. Port/endpoint configuration for this cluster */
  endpointConfig:
    | EndpointConfig
    | undefined;
  /** Optional. Metastore configuration. */
  metastoreConfig:
    | MetastoreConfig
    | undefined;
  /** Optional. The config for Dataproc metrics. */
  dataprocMetricConfig:
    | DataprocMetricConfig
    | undefined;
  /** Optional. The node group settings. */
  auxiliaryNodeGroups: AuxiliaryNodeGroup[];
}

/**
 * The Dataproc cluster config for a cluster that does not directly control the
 * underlying compute resources, such as a [Dataproc-on-GKE
 * cluster](https://cloud.google.com/dataproc/docs/guides/dpgke/dataproc-gke-overview).
 */
export interface VirtualClusterConfig {
  /**
   * Optional. A Cloud Storage bucket used to stage job
   * dependencies, config files, and job driver console output.
   * If you do not specify a staging bucket, Cloud
   * Dataproc will determine a Cloud Storage location (US,
   * ASIA, or EU) for your cluster's staging bucket according to the
   * Compute Engine zone where your cluster is deployed, and then create
   * and manage this project-level, per-location bucket (see
   * [Dataproc staging and temp
   * buckets](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
   * **This field requires a Cloud Storage bucket name, not a `gs://...` URI to
   * a Cloud Storage bucket.**
   */
  stagingBucket: string;
  /**
   * Required. The configuration for running the Dataproc cluster on
   * Kubernetes.
   */
  kubernetesClusterConfig?:
    | KubernetesClusterConfig
    | undefined;
  /** Optional. Configuration of auxiliary services used by this cluster. */
  auxiliaryServicesConfig: AuxiliaryServicesConfig | undefined;
}

/** Auxiliary services configuration for a Cluster. */
export interface AuxiliaryServicesConfig {
  /** Optional. The Hive Metastore configuration for this workload. */
  metastoreConfig:
    | MetastoreConfig
    | undefined;
  /** Optional. The Spark History Server configuration for the workload. */
  sparkHistoryServerConfig: SparkHistoryServerConfig | undefined;
}

/** Endpoint config for this cluster */
export interface EndpointConfig {
  /**
   * Output only. The map of port descriptions to URLs. Will only be populated
   * if enable_http_port_access is true.
   */
  httpPorts: { [key: string]: string };
  /**
   * Optional. If true, enable http access to specific ports on the cluster
   * from external sources. Defaults to false.
   */
  enableHttpPortAccess: boolean;
}

export interface EndpointConfig_HttpPortsEntry {
  key: string;
  value: string;
}

/** Autoscaling Policy config associated with the cluster. */
export interface AutoscalingConfig {
  /**
   * Optional. The autoscaling policy used by the cluster.
   *
   * Only resource names including projectid and location (region) are valid.
   * Examples:
   *
   * * `https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]`
   * * `projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]`
   *
   * Note that the policy must be in the same project and Dataproc region.
   */
  policyUri: string;
}

/** Encryption settings for the cluster. */
export interface EncryptionConfig {
  /**
   * Optional. The Cloud KMS key resource name to use for persistent disk
   * encryption for all instances in the cluster. See [Use CMEK with cluster
   * data]
   * (https://cloud.google.com//dataproc/docs/concepts/configuring-clusters/customer-managed-encryption#use_cmek_with_cluster_data)
   * for more information.
   */
  gcePdKmsKeyName: string;
  /**
   * Optional. The Cloud KMS key resource name to use for cluster persistent
   * disk and job argument encryption. See [Use CMEK with cluster data]
   * (https://cloud.google.com//dataproc/docs/concepts/configuring-clusters/customer-managed-encryption#use_cmek_with_cluster_data)
   * for more information.
   *
   * When this key resource name is provided, the following job arguments of
   * the following job types submitted to the cluster are encrypted using CMEK:
   *
   * * [FlinkJob
   * args](https://cloud.google.com/dataproc/docs/reference/rest/v1/FlinkJob)
   * * [HadoopJob
   * args](https://cloud.google.com/dataproc/docs/reference/rest/v1/HadoopJob)
   * * [SparkJob
   * args](https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkJob)
   * * [SparkRJob
   * args](https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkRJob)
   * * [PySparkJob
   * args](https://cloud.google.com/dataproc/docs/reference/rest/v1/PySparkJob)
   * * [SparkSqlJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkSqlJob)
   *   scriptVariables and queryList.queries
   * * [HiveJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/HiveJob)
   *   scriptVariables and queryList.queries
   * * [PigJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/PigJob)
   *   scriptVariables and queryList.queries
   * * [PrestoJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/PrestoJob)
   *   scriptVariables and queryList.queries
   */
  kmsKey: string;
}

/**
 * Common config settings for resources of Compute Engine cluster
 * instances, applicable to all instances in the cluster.
 */
export interface GceClusterConfig {
  /**
   * Optional. The Compute Engine zone where the Dataproc cluster will be
   * located. If omitted, the service will pick a zone in the cluster's Compute
   * Engine region. On a get request, zone will always be present.
   *
   * A full URL, partial URI, or short name are valid. Examples:
   *
   * * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]`
   * * `projects/[project_id]/zones/[zone]`
   * * `[zone]`
   */
  zoneUri: string;
  /**
   * Optional. The Compute Engine network to be used for machine
   * communications. Cannot be specified with subnetwork_uri. If neither
   * `network_uri` nor `subnetwork_uri` is specified, the "default" network of
   * the project is used, if it exists. Cannot be a "Custom Subnet Network" (see
   * [Using Subnetworks](https://cloud.google.com/compute/docs/subnetworks) for
   * more information).
   *
   * A full URL, partial URI, or short name are valid. Examples:
   *
   * * `https://www.googleapis.com/compute/v1/projects/[project_id]/global/networks/default`
   * * `projects/[project_id]/global/networks/default`
   * * `default`
   */
  networkUri: string;
  /**
   * Optional. The Compute Engine subnetwork to be used for machine
   * communications. Cannot be specified with network_uri.
   *
   * A full URL, partial URI, or short name are valid. Examples:
   *
   * * `https://www.googleapis.com/compute/v1/projects/[project_id]/regions/[region]/subnetworks/sub0`
   * * `projects/[project_id]/regions/[region]/subnetworks/sub0`
   * * `sub0`
   */
  subnetworkUri: string;
  /**
   * Optional. This setting applies to subnetwork-enabled networks. It is set to
   * `true` by default in clusters created with image versions 2.2.x.
   *
   * When set to `true`:
   *
   * * All cluster VMs have internal IP addresses.
   * * [Google Private Access]
   * (https://cloud.google.com/vpc/docs/private-google-access)
   * must be enabled to access Dataproc and other Google Cloud APIs.
   * * Off-cluster dependencies must be configured to be accessible
   * without external IP addresses.
   *
   * When set to `false`:
   *
   * * Cluster VMs are not restricted to internal IP addresses.
   * * Ephemeral external IP addresses are assigned to each cluster VM.
   */
  internalIpOnly?:
    | boolean
    | undefined;
  /** Optional. The type of IPv6 access for a cluster. */
  privateIpv6GoogleAccess: GceClusterConfig_PrivateIpv6GoogleAccess;
  /**
   * Optional. The [Dataproc service
   * account](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc)
   * (also see [VM Data Plane
   * identity](https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity))
   * used by Dataproc cluster VM instances to access Google Cloud Platform
   * services.
   *
   * If not specified, the
   * [Compute Engine default service
   * account](https://cloud.google.com/compute/docs/access/service-accounts#default_service_account)
   * is used.
   */
  serviceAccount: string;
  /**
   * Optional. The URIs of service account scopes to be included in
   * Compute Engine instances. The following base set of scopes is always
   * included:
   *
   * * https://www.googleapis.com/auth/cloud.useraccounts.readonly
   * * https://www.googleapis.com/auth/devstorage.read_write
   * * https://www.googleapis.com/auth/logging.write
   *
   * If no scopes are specified, the following defaults are also provided:
   *
   * * https://www.googleapis.com/auth/bigquery
   * * https://www.googleapis.com/auth/bigtable.admin.table
   * * https://www.googleapis.com/auth/bigtable.data
   * * https://www.googleapis.com/auth/devstorage.full_control
   */
  serviceAccountScopes: string[];
  /**
   * The Compute Engine network tags to add to all instances (see [Tagging
   * instances](https://cloud.google.com/vpc/docs/add-remove-network-tags)).
   */
  tags: string[];
  /**
   * Optional. The Compute Engine metadata entries to add to all instances (see
   * [Project and instance
   * metadata](https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
   */
  metadata: { [key: string]: string };
  /** Optional. Reservation Affinity for consuming Zonal reservation. */
  reservationAffinity:
    | ReservationAffinity
    | undefined;
  /** Optional. Node Group Affinity for sole-tenant clusters. */
  nodeGroupAffinity:
    | NodeGroupAffinity
    | undefined;
  /**
   * Optional. Shielded Instance Config for clusters using [Compute Engine
   * Shielded
   * VMs](https://cloud.google.com/security/shielded-cloud/shielded-vm).
   */
  shieldedInstanceConfig:
    | ShieldedInstanceConfig
    | undefined;
  /**
   * Optional. Confidential Instance Config for clusters using [Confidential
   * VMs](https://cloud.google.com/compute/confidential-vm/docs).
   */
  confidentialInstanceConfig: ConfidentialInstanceConfig | undefined;
}

/**
 * `PrivateIpv6GoogleAccess` controls whether and how Dataproc cluster nodes
 * can communicate with Google Services through gRPC over IPv6.
 * These values are directly mapped to corresponding values in the
 * [Compute Engine Instance
 * fields](https://cloud.google.com/compute/docs/reference/rest/v1/instances).
 */
export enum GceClusterConfig_PrivateIpv6GoogleAccess {
  /**
   * PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED - If unspecified, Compute Engine default behavior will apply, which
   * is the same as
   * [INHERIT_FROM_SUBNETWORK][google.cloud.dataproc.v1.GceClusterConfig.PrivateIpv6GoogleAccess.INHERIT_FROM_SUBNETWORK].
   */
  PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED = 0,
  /**
   * INHERIT_FROM_SUBNETWORK - Private access to and from Google Services configuration
   * inherited from the subnetwork configuration. This is the
   * default Compute Engine behavior.
   */
  INHERIT_FROM_SUBNETWORK = 1,
  /**
   * OUTBOUND - Enables outbound private IPv6 access to Google Services from the Dataproc
   * cluster.
   */
  OUTBOUND = 2,
  /**
   * BIDIRECTIONAL - Enables bidirectional private IPv6 access between Google Services and the
   * Dataproc cluster.
   */
  BIDIRECTIONAL = 3,
  UNRECOGNIZED = -1,
}

export function gceClusterConfig_PrivateIpv6GoogleAccessFromJSON(
  object: any,
): GceClusterConfig_PrivateIpv6GoogleAccess {
  switch (object) {
    case 0:
    case "PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED":
      return GceClusterConfig_PrivateIpv6GoogleAccess.PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED;
    case 1:
    case "INHERIT_FROM_SUBNETWORK":
      return GceClusterConfig_PrivateIpv6GoogleAccess.INHERIT_FROM_SUBNETWORK;
    case 2:
    case "OUTBOUND":
      return GceClusterConfig_PrivateIpv6GoogleAccess.OUTBOUND;
    case 3:
    case "BIDIRECTIONAL":
      return GceClusterConfig_PrivateIpv6GoogleAccess.BIDIRECTIONAL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return GceClusterConfig_PrivateIpv6GoogleAccess.UNRECOGNIZED;
  }
}

export function gceClusterConfig_PrivateIpv6GoogleAccessToJSON(
  object: GceClusterConfig_PrivateIpv6GoogleAccess,
): string {
  switch (object) {
    case GceClusterConfig_PrivateIpv6GoogleAccess.PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED:
      return "PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED";
    case GceClusterConfig_PrivateIpv6GoogleAccess.INHERIT_FROM_SUBNETWORK:
      return "INHERIT_FROM_SUBNETWORK";
    case GceClusterConfig_PrivateIpv6GoogleAccess.OUTBOUND:
      return "OUTBOUND";
    case GceClusterConfig_PrivateIpv6GoogleAccess.BIDIRECTIONAL:
      return "BIDIRECTIONAL";
    case GceClusterConfig_PrivateIpv6GoogleAccess.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export interface GceClusterConfig_MetadataEntry {
  key: string;
  value: string;
}

/**
 * Node Group Affinity for clusters using sole-tenant node groups.
 * **The Dataproc `NodeGroupAffinity` resource is not related to the
 * Dataproc [NodeGroup][google.cloud.dataproc.v1.NodeGroup] resource.**
 */
export interface NodeGroupAffinity {
  /**
   * Required. The URI of a
   * sole-tenant [node group
   * resource](https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups)
   * that the cluster will be created on.
   *
   * A full URL, partial URI, or node group name are valid. Examples:
   *
   * * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/nodeGroups/node-group-1`
   * * `projects/[project_id]/zones/[zone]/nodeGroups/node-group-1`
   * * `node-group-1`
   */
  nodeGroupUri: string;
}

/**
 * Shielded Instance Config for clusters using [Compute Engine Shielded
 * VMs](https://cloud.google.com/security/shielded-cloud/shielded-vm).
 */
export interface ShieldedInstanceConfig {
  /** Optional. Defines whether instances have Secure Boot enabled. */
  enableSecureBoot?:
    | boolean
    | undefined;
  /** Optional. Defines whether instances have the vTPM enabled. */
  enableVtpm?:
    | boolean
    | undefined;
  /** Optional. Defines whether instances have integrity monitoring enabled. */
  enableIntegrityMonitoring?: boolean | undefined;
}

/**
 * Confidential Instance Config for clusters using [Confidential
 * VMs](https://cloud.google.com/compute/confidential-vm/docs)
 */
export interface ConfidentialInstanceConfig {
  /**
   * Optional. Defines whether the instance should have confidential compute
   * enabled.
   */
  enableConfidentialCompute: boolean;
}

/**
 * The config settings for Compute Engine resources in
 * an instance group, such as a master or worker group.
 */
export interface InstanceGroupConfig {
  /**
   * Optional. The number of VM instances in the instance group.
   * For [HA
   * cluster](/dataproc/docs/concepts/configuring-clusters/high-availability)
   * [master_config](#FIELDS.master_config) groups, **must be set to 3**.
   * For standard cluster [master_config](#FIELDS.master_config) groups,
   * **must be set to 1**.
   */
  numInstances: number;
  /**
   * Output only. The list of instance names. Dataproc derives the names
   * from `cluster_name`, `num_instances`, and the instance group.
   */
  instanceNames: string[];
  /** Output only. List of references to Compute Engine instances. */
  instanceReferences: InstanceReference[];
  /**
   * Optional. The Compute Engine image resource used for cluster instances.
   *
   * The URI can represent an image or image family.
   *
   * Image examples:
   *
   * * `https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/[image-id]`
   * * `projects/[project_id]/global/images/[image-id]`
   * * `image-id`
   *
   * Image family examples. Dataproc will use the most recent
   * image from the family:
   *
   * * `https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/family/[custom-image-family-name]`
   * * `projects/[project_id]/global/images/family/[custom-image-family-name]`
   *
   * If the URI is unspecified, it will be inferred from
   * `SoftwareConfig.image_version` or the system default.
   */
  imageUri: string;
  /**
   * Optional. The Compute Engine machine type used for cluster instances.
   *
   * A full URL, partial URI, or short name are valid. Examples:
   *
   * * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2`
   * * `projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2`
   * * `n1-standard-2`
   *
   * **Auto Zone Exception**: If you are using the Dataproc
   * [Auto Zone
   * Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
   * feature, you must use the short name of the machine type
   * resource, for example, `n1-standard-2`.
   */
  machineTypeUri: string;
  /** Optional. Disk option config settings. */
  diskConfig:
    | DiskConfig
    | undefined;
  /**
   * Output only. Specifies that this instance group contains preemptible
   * instances.
   */
  isPreemptible: boolean;
  /**
   * Optional. Specifies the preemptibility of the instance group.
   *
   * The default value for master and worker groups is
   * `NON_PREEMPTIBLE`. This default cannot be changed.
   *
   * The default value for secondary instances is
   * `PREEMPTIBLE`.
   */
  preemptibility: InstanceGroupConfig_Preemptibility;
  /**
   * Output only. The config for Compute Engine Instance Group
   * Manager that manages this group.
   * This is only used for preemptible instance groups.
   */
  managedGroupConfig:
    | ManagedGroupConfig
    | undefined;
  /**
   * Optional. The Compute Engine accelerator configuration for these
   * instances.
   */
  accelerators: AcceleratorConfig[];
  /**
   * Optional. Specifies the minimum cpu platform for the Instance Group.
   * See [Dataproc -> Minimum CPU
   * Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
   */
  minCpuPlatform: string;
  /**
   * Optional. The minimum number of primary worker instances to create.
   * If `min_num_instances` is set, cluster creation will succeed if
   * the number of primary workers created is at least equal to the
   * `min_num_instances` number.
   *
   * Example: Cluster creation request with `num_instances` = `5` and
   * `min_num_instances` = `3`:
   *
   * *  If 4 VMs are created and 1 instance fails,
   *    the failed VM is deleted. The cluster is
   *    resized to 4 instances and placed in a `RUNNING` state.
   * *  If 2 instances are created and 3 instances fail,
   *    the cluster in placed in an `ERROR` state. The failed VMs
   *    are not deleted.
   */
  minNumInstances: number;
  /**
   * Optional. Instance flexibility Policy allowing a mixture of VM shapes and
   * provisioning models.
   */
  instanceFlexibilityPolicy:
    | InstanceFlexibilityPolicy
    | undefined;
  /**
   * Optional. Configuration to handle the startup of instances during cluster
   * create and update process.
   */
  startupConfig: StartupConfig | undefined;
}

/** Controls the use of preemptible instances within the group. */
export enum InstanceGroupConfig_Preemptibility {
  /**
   * PREEMPTIBILITY_UNSPECIFIED - Preemptibility is unspecified, the system will choose the
   * appropriate setting for each instance group.
   */
  PREEMPTIBILITY_UNSPECIFIED = 0,
  /**
   * NON_PREEMPTIBLE - Instances are non-preemptible.
   *
   * This option is allowed for all instance groups and is the only valid
   * value for Master and Worker instance groups.
   */
  NON_PREEMPTIBLE = 1,
  /**
   * PREEMPTIBLE - Instances are [preemptible]
   * (https://cloud.google.com/compute/docs/instances/preemptible).
   *
   * This option is allowed only for [secondary worker]
   * (https://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms)
   * groups.
   */
  PREEMPTIBLE = 2,
  /**
   * SPOT - Instances are [Spot VMs]
   * (https://cloud.google.com/compute/docs/instances/spot).
   *
   * This option is allowed only for [secondary worker]
   * (https://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms)
   * groups. Spot VMs are the latest version of [preemptible VMs]
   * (https://cloud.google.com/compute/docs/instances/preemptible), and
   * provide additional features.
   */
  SPOT = 3,
  UNRECOGNIZED = -1,
}

export function instanceGroupConfig_PreemptibilityFromJSON(object: any): InstanceGroupConfig_Preemptibility {
  switch (object) {
    case 0:
    case "PREEMPTIBILITY_UNSPECIFIED":
      return InstanceGroupConfig_Preemptibility.PREEMPTIBILITY_UNSPECIFIED;
    case 1:
    case "NON_PREEMPTIBLE":
      return InstanceGroupConfig_Preemptibility.NON_PREEMPTIBLE;
    case 2:
    case "PREEMPTIBLE":
      return InstanceGroupConfig_Preemptibility.PREEMPTIBLE;
    case 3:
    case "SPOT":
      return InstanceGroupConfig_Preemptibility.SPOT;
    case -1:
    case "UNRECOGNIZED":
    default:
      return InstanceGroupConfig_Preemptibility.UNRECOGNIZED;
  }
}

export function instanceGroupConfig_PreemptibilityToJSON(object: InstanceGroupConfig_Preemptibility): string {
  switch (object) {
    case InstanceGroupConfig_Preemptibility.PREEMPTIBILITY_UNSPECIFIED:
      return "PREEMPTIBILITY_UNSPECIFIED";
    case InstanceGroupConfig_Preemptibility.NON_PREEMPTIBLE:
      return "NON_PREEMPTIBLE";
    case InstanceGroupConfig_Preemptibility.PREEMPTIBLE:
      return "PREEMPTIBLE";
    case InstanceGroupConfig_Preemptibility.SPOT:
      return "SPOT";
    case InstanceGroupConfig_Preemptibility.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Configuration to handle the startup of instances during cluster create and
 * update process.
 */
export interface StartupConfig {
  /**
   * Optional. The config setting to enable cluster creation/ updation to be
   * successful only after required_registration_fraction of instances are up
   * and running. This configuration is applicable to only secondary workers for
   * now. The cluster will fail if required_registration_fraction of instances
   * are not available. This will include instance creation, agent registration,
   * and service registration (if enabled).
   */
  requiredRegistrationFraction?: number | undefined;
}

/** A reference to a Compute Engine instance. */
export interface InstanceReference {
  /** The user-friendly name of the Compute Engine instance. */
  instanceName: string;
  /** The unique identifier of the Compute Engine instance. */
  instanceId: string;
  /** The public RSA key used for sharing data with this instance. */
  publicKey: string;
  /** The public ECIES key used for sharing data with this instance. */
  publicEciesKey: string;
}

/** Specifies the resources used to actively manage an instance group. */
export interface ManagedGroupConfig {
  /**
   * Output only. The name of the Instance Template used for the Managed
   * Instance Group.
   */
  instanceTemplateName: string;
  /** Output only. The name of the Instance Group Manager for this group. */
  instanceGroupManagerName: string;
  /**
   * Output only. The partial URI to the instance group manager for this group.
   * E.g. projects/my-project/regions/us-central1/instanceGroupManagers/my-igm.
   */
  instanceGroupManagerUri: string;
}

/**
 * Instance flexibility Policy allowing a mixture of VM shapes and provisioning
 * models.
 */
export interface InstanceFlexibilityPolicy {
  /**
   * Optional. List of instance selection options that the group will use when
   * creating new VMs.
   */
  instanceSelectionList: InstanceFlexibilityPolicy_InstanceSelection[];
  /** Output only. A list of instance selection results in the group. */
  instanceSelectionResults: InstanceFlexibilityPolicy_InstanceSelectionResult[];
}

/** Defines machines types and a rank to which the machines types belong. */
export interface InstanceFlexibilityPolicy_InstanceSelection {
  /** Optional. Full machine-type names, e.g. "n1-standard-16". */
  machineTypes: string[];
  /**
   * Optional. Preference of this instance selection. Lower number means
   * higher preference. Dataproc will first try to create a VM based on the
   * machine-type with priority rank and fallback to next rank based on
   * availability. Machine types and instance selections with the same
   * priority have the same preference.
   */
  rank: number;
}

/**
 * Defines a mapping from machine types to the number of VMs that are created
 * with each machine type.
 */
export interface InstanceFlexibilityPolicy_InstanceSelectionResult {
  /** Output only. Full machine-type names, e.g. "n1-standard-16". */
  machineType?:
    | string
    | undefined;
  /** Output only. Number of VM provisioned with the machine_type. */
  vmCount?: number | undefined;
}

/**
 * Specifies the type and number of accelerator cards attached to the instances
 * of an instance. See [GPUs on Compute
 * Engine](https://cloud.google.com/compute/docs/gpus/).
 */
export interface AcceleratorConfig {
  /**
   * Full URL, partial URI, or short name of the accelerator type resource to
   * expose to this instance. See
   * [Compute Engine
   * AcceleratorTypes](https://cloud.google.com/compute/docs/reference/v1/acceleratorTypes).
   *
   * Examples:
   *
   * * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-t4`
   * * `projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-t4`
   * * `nvidia-tesla-t4`
   *
   * **Auto Zone Exception**: If you are using the Dataproc
   * [Auto Zone
   * Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
   * feature, you must use the short name of the accelerator type
   * resource, for example, `nvidia-tesla-t4`.
   */
  acceleratorTypeUri: string;
  /** The number of the accelerator cards of this type exposed to this instance. */
  acceleratorCount: number;
}

/** Specifies the config of disk options for a group of VM instances. */
export interface DiskConfig {
  /**
   * Optional. Type of the boot disk (default is "pd-standard").
   * Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive),
   * "pd-ssd" (Persistent Disk Solid State Drive),
   * or "pd-standard" (Persistent Disk Hard Disk Drive).
   * See [Disk types](https://cloud.google.com/compute/docs/disks#disk-types).
   */
  bootDiskType: string;
  /** Optional. Size in GB of the boot disk (default is 500GB). */
  bootDiskSizeGb: number;
  /**
   * Optional. Number of attached SSDs, from 0 to 8 (default is 0).
   * If SSDs are not attached, the boot disk is used to store runtime logs and
   * [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data.
   * If one or more SSDs are attached, this runtime bulk
   * data is spread across them, and the boot disk contains only basic
   * config and installed binaries.
   *
   * Note: Local SSD options may vary by machine type and number of vCPUs
   * selected.
   */
  numLocalSsds: number;
  /**
   * Optional. Interface type of local SSDs (default is "scsi").
   * Valid values: "scsi" (Small Computer System Interface),
   * "nvme" (Non-Volatile Memory Express).
   * See [local SSD
   * performance](https://cloud.google.com/compute/docs/disks/local-ssd#performance).
   */
  localSsdInterface: string;
}

/** Node group identification and configuration information. */
export interface AuxiliaryNodeGroup {
  /** Required. Node group configuration. */
  nodeGroup:
    | NodeGroup
    | undefined;
  /**
   * Optional. A node group ID. Generated if not specified.
   *
   * The ID must contain only letters (a-z, A-Z), numbers (0-9),
   * underscores (_), and hyphens (-). Cannot begin or end with underscore
   * or hyphen. Must consist of from 3 to 33 characters.
   */
  nodeGroupId: string;
}

/**
 * Dataproc Node Group.
 * **The Dataproc `NodeGroup` resource is not related to the
 * Dataproc [NodeGroupAffinity][google.cloud.dataproc.v1.NodeGroupAffinity]
 * resource.**
 */
export interface NodeGroup {
  /** The Node group [resource name](https://aip.dev/122). */
  name: string;
  /** Required. Node group roles. */
  roles: NodeGroup_Role[];
  /** Optional. The node group instance group configuration. */
  nodeGroupConfig:
    | InstanceGroupConfig
    | undefined;
  /**
   * Optional. Node group labels.
   *
   * * Label **keys** must consist of from 1 to 63 characters and conform to
   *   [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).
   * * Label **values** can be empty. If specified, they must consist of from
   *   1 to 63 characters and conform to [RFC 1035]
   *   (https://www.ietf.org/rfc/rfc1035.txt).
   * * The node group must have no more than 32 labels.
   */
  labels: { [key: string]: string };
}

/** Node pool roles. */
export enum NodeGroup_Role {
  /** ROLE_UNSPECIFIED - Required unspecified role. */
  ROLE_UNSPECIFIED = 0,
  /** DRIVER - Job drivers run on the node pool. */
  DRIVER = 1,
  UNRECOGNIZED = -1,
}

export function nodeGroup_RoleFromJSON(object: any): NodeGroup_Role {
  switch (object) {
    case 0:
    case "ROLE_UNSPECIFIED":
      return NodeGroup_Role.ROLE_UNSPECIFIED;
    case 1:
    case "DRIVER":
      return NodeGroup_Role.DRIVER;
    case -1:
    case "UNRECOGNIZED":
    default:
      return NodeGroup_Role.UNRECOGNIZED;
  }
}

export function nodeGroup_RoleToJSON(object: NodeGroup_Role): string {
  switch (object) {
    case NodeGroup_Role.ROLE_UNSPECIFIED:
      return "ROLE_UNSPECIFIED";
    case NodeGroup_Role.DRIVER:
      return "DRIVER";
    case NodeGroup_Role.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export interface NodeGroup_LabelsEntry {
  key: string;
  value: string;
}

/**
 * Specifies an executable to run on a fully configured node and a
 * timeout period for executable completion.
 */
export interface NodeInitializationAction {
  /** Required. Cloud Storage URI of executable file. */
  executableFile: string;
  /**
   * Optional. Amount of time executable has to complete. Default is
   * 10 minutes (see JSON representation of
   * [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
   *
   * Cluster creation fails with an explanatory error message (the
   * name of the executable that caused the error and the exceeded timeout
   * period) if the executable is not completed at end of the timeout period.
   */
  executionTimeout: Duration | undefined;
}

/** The status of a cluster and its instances. */
export interface ClusterStatus {
  /** Output only. The cluster's state. */
  state: ClusterStatus_State;
  /** Optional. Output only. Details of cluster's state. */
  detail: string;
  /**
   * Output only. Time when this state was entered (see JSON representation of
   * [Timestamp](https://developers.google.com/protocol-buffers/docs/proto3#json)).
   */
  stateStartTime:
    | Date
    | undefined;
  /**
   * Output only. Additional state information that includes
   * status reported by the agent.
   */
  substate: ClusterStatus_Substate;
}

/** The cluster state. */
export enum ClusterStatus_State {
  /** UNKNOWN - The cluster state is unknown. */
  UNKNOWN = 0,
  /** CREATING - The cluster is being created and set up. It is not ready for use. */
  CREATING = 1,
  /**
   * RUNNING - The cluster is currently running and healthy. It is ready for use.
   *
   * **Note:** The cluster state changes from "creating" to "running" status
   * after the master node(s), first two primary worker nodes (and the last
   * primary worker node if primary workers > 2) are running.
   */
  RUNNING = 2,
  /** ERROR - The cluster encountered an error. It is not ready for use. */
  ERROR = 3,
  /**
   * ERROR_DUE_TO_UPDATE - The cluster has encountered an error while being updated. Jobs can
   * be submitted to the cluster, but the cluster cannot be updated.
   */
  ERROR_DUE_TO_UPDATE = 9,
  /** DELETING - The cluster is being deleted. It cannot be used. */
  DELETING = 4,
  /** UPDATING - The cluster is being updated. It continues to accept and process jobs. */
  UPDATING = 5,
  /** STOPPING - The cluster is being stopped. It cannot be used. */
  STOPPING = 6,
  /** STOPPED - The cluster is currently stopped. It is not ready for use. */
  STOPPED = 7,
  /** STARTING - The cluster is being started. It is not ready for use. */
  STARTING = 8,
  /** REPAIRING - The cluster is being repaired. It is not ready for use. */
  REPAIRING = 10,
  UNRECOGNIZED = -1,
}

export function clusterStatus_StateFromJSON(object: any): ClusterStatus_State {
  switch (object) {
    case 0:
    case "UNKNOWN":
      return ClusterStatus_State.UNKNOWN;
    case 1:
    case "CREATING":
      return ClusterStatus_State.CREATING;
    case 2:
    case "RUNNING":
      return ClusterStatus_State.RUNNING;
    case 3:
    case "ERROR":
      return ClusterStatus_State.ERROR;
    case 9:
    case "ERROR_DUE_TO_UPDATE":
      return ClusterStatus_State.ERROR_DUE_TO_UPDATE;
    case 4:
    case "DELETING":
      return ClusterStatus_State.DELETING;
    case 5:
    case "UPDATING":
      return ClusterStatus_State.UPDATING;
    case 6:
    case "STOPPING":
      return ClusterStatus_State.STOPPING;
    case 7:
    case "STOPPED":
      return ClusterStatus_State.STOPPED;
    case 8:
    case "STARTING":
      return ClusterStatus_State.STARTING;
    case 10:
    case "REPAIRING":
      return ClusterStatus_State.REPAIRING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ClusterStatus_State.UNRECOGNIZED;
  }
}

export function clusterStatus_StateToJSON(object: ClusterStatus_State): string {
  switch (object) {
    case ClusterStatus_State.UNKNOWN:
      return "UNKNOWN";
    case ClusterStatus_State.CREATING:
      return "CREATING";
    case ClusterStatus_State.RUNNING:
      return "RUNNING";
    case ClusterStatus_State.ERROR:
      return "ERROR";
    case ClusterStatus_State.ERROR_DUE_TO_UPDATE:
      return "ERROR_DUE_TO_UPDATE";
    case ClusterStatus_State.DELETING:
      return "DELETING";
    case ClusterStatus_State.UPDATING:
      return "UPDATING";
    case ClusterStatus_State.STOPPING:
      return "STOPPING";
    case ClusterStatus_State.STOPPED:
      return "STOPPED";
    case ClusterStatus_State.STARTING:
      return "STARTING";
    case ClusterStatus_State.REPAIRING:
      return "REPAIRING";
    case ClusterStatus_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The cluster substate. */
export enum ClusterStatus_Substate {
  /** UNSPECIFIED - The cluster substate is unknown. */
  UNSPECIFIED = 0,
  /**
   * UNHEALTHY - The cluster is known to be in an unhealthy state
   * (for example, critical daemons are not running or HDFS capacity is
   * exhausted).
   *
   * Applies to RUNNING state.
   */
  UNHEALTHY = 1,
  /**
   * STALE_STATUS - The agent-reported status is out of date (may occur if
   * Dataproc loses communication with Agent).
   *
   * Applies to RUNNING state.
   */
  STALE_STATUS = 2,
  UNRECOGNIZED = -1,
}

export function clusterStatus_SubstateFromJSON(object: any): ClusterStatus_Substate {
  switch (object) {
    case 0:
    case "UNSPECIFIED":
      return ClusterStatus_Substate.UNSPECIFIED;
    case 1:
    case "UNHEALTHY":
      return ClusterStatus_Substate.UNHEALTHY;
    case 2:
    case "STALE_STATUS":
      return ClusterStatus_Substate.STALE_STATUS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ClusterStatus_Substate.UNRECOGNIZED;
  }
}

export function clusterStatus_SubstateToJSON(object: ClusterStatus_Substate): string {
  switch (object) {
    case ClusterStatus_Substate.UNSPECIFIED:
      return "UNSPECIFIED";
    case ClusterStatus_Substate.UNHEALTHY:
      return "UNHEALTHY";
    case ClusterStatus_Substate.STALE_STATUS:
      return "STALE_STATUS";
    case ClusterStatus_Substate.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Security related configuration, including encryption, Kerberos, etc. */
export interface SecurityConfig {
  /** Optional. Kerberos related configuration. */
  kerberosConfig:
    | KerberosConfig
    | undefined;
  /**
   * Optional. Identity related configuration, including service account based
   * secure multi-tenancy user mappings.
   */
  identityConfig: IdentityConfig | undefined;
}

/** Specifies Kerberos related configuration. */
export interface KerberosConfig {
  /**
   * Optional. Flag to indicate whether to Kerberize the cluster (default:
   * false). Set this field to true to enable Kerberos on a cluster.
   */
  enableKerberos: boolean;
  /**
   * Optional. The Cloud Storage URI of a KMS encrypted file containing the root
   * principal password.
   */
  rootPrincipalPasswordUri: string;
  /**
   * Optional. The URI of the KMS key used to encrypt sensitive
   * files.
   */
  kmsKeyUri: string;
  /**
   * Optional. The Cloud Storage URI of the keystore file used for SSL
   * encryption. If not provided, Dataproc will provide a self-signed
   * certificate.
   */
  keystoreUri: string;
  /**
   * Optional. The Cloud Storage URI of the truststore file used for SSL
   * encryption. If not provided, Dataproc will provide a self-signed
   * certificate.
   */
  truststoreUri: string;
  /**
   * Optional. The Cloud Storage URI of a KMS encrypted file containing the
   * password to the user provided keystore. For the self-signed certificate,
   * this password is generated by Dataproc.
   */
  keystorePasswordUri: string;
  /**
   * Optional. The Cloud Storage URI of a KMS encrypted file containing the
   * password to the user provided key. For the self-signed certificate, this
   * password is generated by Dataproc.
   */
  keyPasswordUri: string;
  /**
   * Optional. The Cloud Storage URI of a KMS encrypted file containing the
   * password to the user provided truststore. For the self-signed certificate,
   * this password is generated by Dataproc.
   */
  truststorePasswordUri: string;
  /**
   * Optional. The remote realm the Dataproc on-cluster KDC will trust, should
   * the user enable cross realm trust.
   */
  crossRealmTrustRealm: string;
  /**
   * Optional. The KDC (IP or hostname) for the remote trusted realm in a cross
   * realm trust relationship.
   */
  crossRealmTrustKdc: string;
  /**
   * Optional. The admin server (IP or hostname) for the remote trusted realm in
   * a cross realm trust relationship.
   */
  crossRealmTrustAdminServer: string;
  /**
   * Optional. The Cloud Storage URI of a KMS encrypted file containing the
   * shared password between the on-cluster Kerberos realm and the remote
   * trusted realm, in a cross realm trust relationship.
   */
  crossRealmTrustSharedPasswordUri: string;
  /**
   * Optional. The Cloud Storage URI of a KMS encrypted file containing the
   * master key of the KDC database.
   */
  kdcDbKeyUri: string;
  /**
   * Optional. The lifetime of the ticket granting ticket, in hours.
   * If not specified, or user specifies 0, then default value 10
   * will be used.
   */
  tgtLifetimeHours: number;
  /**
   * Optional. The name of the on-cluster Kerberos realm.
   * If not specified, the uppercased domain of hostnames will be the realm.
   */
  realm: string;
}

/**
 * Identity related configuration, including service account based
 * secure multi-tenancy user mappings.
 */
export interface IdentityConfig {
  /** Required. Map of user to service account. */
  userServiceAccountMapping: { [key: string]: string };
}

export interface IdentityConfig_UserServiceAccountMappingEntry {
  key: string;
  value: string;
}

/** Specifies the selection and config of software inside the cluster. */
export interface SoftwareConfig {
  /**
   * Optional. The version of software inside the cluster. It must be one of the
   * supported [Dataproc
   * Versions](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported-dataproc-image-versions),
   * such as "1.2" (including a subminor version, such as "1.2.29"), or the
   * ["preview"
   * version](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions).
   * If unspecified, it defaults to the latest Debian version.
   */
  imageVersion: string;
  /**
   * Optional. The properties to set on daemon config files.
   *
   * Property keys are specified in `prefix:property` format, for example
   * `core:hadoop.tmp.dir`. The following are supported prefixes
   * and their mappings:
   *
   * * capacity-scheduler: `capacity-scheduler.xml`
   * * core:   `core-site.xml`
   * * distcp: `distcp-default.xml`
   * * hdfs:   `hdfs-site.xml`
   * * hive:   `hive-site.xml`
   * * mapred: `mapred-site.xml`
   * * pig:    `pig.properties`
   * * spark:  `spark-defaults.conf`
   * * yarn:   `yarn-site.xml`
   *
   * For more information, see [Cluster
   * properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
   */
  properties: { [key: string]: string };
  /** Optional. The set of components to activate on the cluster. */
  optionalComponents: Component[];
}

export interface SoftwareConfig_PropertiesEntry {
  key: string;
  value: string;
}

/** Specifies the cluster auto-delete schedule configuration. */
export interface LifecycleConfig {
  /**
   * Optional. The duration to keep the cluster alive while idling (when no jobs
   * are running). Passing this threshold will cause the cluster to be
   * deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON
   * representation of
   * [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
   */
  idleDeleteTtl:
    | Duration
    | undefined;
  /**
   * Optional. The time when cluster will be auto-deleted (see JSON
   * representation of
   * [Timestamp](https://developers.google.com/protocol-buffers/docs/proto3#json)).
   */
  autoDeleteTime?:
    | Date
    | undefined;
  /**
   * Optional. The lifetime duration of cluster. The cluster will be
   * auto-deleted at the end of this period. Minimum value is 10 minutes;
   * maximum value is 14 days (see JSON representation of
   * [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
   */
  autoDeleteTtl?:
    | Duration
    | undefined;
  /**
   * Output only. The time when cluster became idle (most recent job finished)
   * and became eligible for deletion due to idleness (see JSON representation
   * of
   * [Timestamp](https://developers.google.com/protocol-buffers/docs/proto3#json)).
   */
  idleStartTime: Date | undefined;
}

/** Specifies a Metastore configuration. */
export interface MetastoreConfig {
  /**
   * Required. Resource name of an existing Dataproc Metastore service.
   *
   * Example:
   *
   * * `projects/[project_id]/locations/[dataproc_region]/services/[service-name]`
   */
  dataprocMetastoreService: string;
}

/**
 * Contains cluster daemon metrics, such as HDFS and YARN stats.
 *
 * **Beta Feature**: This report is available for testing purposes only. It may
 * be changed before final release.
 */
export interface ClusterMetrics {
  /** The HDFS metrics. */
  hdfsMetrics: { [key: string]: Long };
  /** YARN metrics. */
  yarnMetrics: { [key: string]: Long };
}

export interface ClusterMetrics_HdfsMetricsEntry {
  key: string;
  value: Long;
}

export interface ClusterMetrics_YarnMetricsEntry {
  key: string;
  value: Long;
}

/** Dataproc metric config. */
export interface DataprocMetricConfig {
  /** Required. Metrics sources to enable. */
  metrics: DataprocMetricConfig_Metric[];
}

/**
 * A source for the collection of Dataproc custom metrics (see [Custom
 * metrics]
 * (https://cloud.google.com//dataproc/docs/guides/dataproc-metrics#custom_metrics)).
 */
export enum DataprocMetricConfig_MetricSource {
  /** METRIC_SOURCE_UNSPECIFIED - Required unspecified metric source. */
  METRIC_SOURCE_UNSPECIFIED = 0,
  /**
   * MONITORING_AGENT_DEFAULTS - Monitoring agent metrics. If this source is enabled,
   * Dataproc enables the monitoring agent in Compute Engine,
   * and collects monitoring agent metrics, which are published
   * with an `agent.googleapis.com` prefix.
   */
  MONITORING_AGENT_DEFAULTS = 1,
  /** HDFS - HDFS metric source. */
  HDFS = 2,
  /** SPARK - Spark metric source. */
  SPARK = 3,
  /** YARN - YARN metric source. */
  YARN = 4,
  /** SPARK_HISTORY_SERVER - Spark History Server metric source. */
  SPARK_HISTORY_SERVER = 5,
  /** HIVESERVER2 - Hiveserver2 metric source. */
  HIVESERVER2 = 6,
  /** HIVEMETASTORE - hivemetastore metric source */
  HIVEMETASTORE = 7,
  /** FLINK - flink metric source */
  FLINK = 8,
  UNRECOGNIZED = -1,
}

export function dataprocMetricConfig_MetricSourceFromJSON(object: any): DataprocMetricConfig_MetricSource {
  switch (object) {
    case 0:
    case "METRIC_SOURCE_UNSPECIFIED":
      return DataprocMetricConfig_MetricSource.METRIC_SOURCE_UNSPECIFIED;
    case 1:
    case "MONITORING_AGENT_DEFAULTS":
      return DataprocMetricConfig_MetricSource.MONITORING_AGENT_DEFAULTS;
    case 2:
    case "HDFS":
      return DataprocMetricConfig_MetricSource.HDFS;
    case 3:
    case "SPARK":
      return DataprocMetricConfig_MetricSource.SPARK;
    case 4:
    case "YARN":
      return DataprocMetricConfig_MetricSource.YARN;
    case 5:
    case "SPARK_HISTORY_SERVER":
      return DataprocMetricConfig_MetricSource.SPARK_HISTORY_SERVER;
    case 6:
    case "HIVESERVER2":
      return DataprocMetricConfig_MetricSource.HIVESERVER2;
    case 7:
    case "HIVEMETASTORE":
      return DataprocMetricConfig_MetricSource.HIVEMETASTORE;
    case 8:
    case "FLINK":
      return DataprocMetricConfig_MetricSource.FLINK;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DataprocMetricConfig_MetricSource.UNRECOGNIZED;
  }
}

export function dataprocMetricConfig_MetricSourceToJSON(object: DataprocMetricConfig_MetricSource): string {
  switch (object) {
    case DataprocMetricConfig_MetricSource.METRIC_SOURCE_UNSPECIFIED:
      return "METRIC_SOURCE_UNSPECIFIED";
    case DataprocMetricConfig_MetricSource.MONITORING_AGENT_DEFAULTS:
      return "MONITORING_AGENT_DEFAULTS";
    case DataprocMetricConfig_MetricSource.HDFS:
      return "HDFS";
    case DataprocMetricConfig_MetricSource.SPARK:
      return "SPARK";
    case DataprocMetricConfig_MetricSource.YARN:
      return "YARN";
    case DataprocMetricConfig_MetricSource.SPARK_HISTORY_SERVER:
      return "SPARK_HISTORY_SERVER";
    case DataprocMetricConfig_MetricSource.HIVESERVER2:
      return "HIVESERVER2";
    case DataprocMetricConfig_MetricSource.HIVEMETASTORE:
      return "HIVEMETASTORE";
    case DataprocMetricConfig_MetricSource.FLINK:
      return "FLINK";
    case DataprocMetricConfig_MetricSource.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** A Dataproc custom metric. */
export interface DataprocMetricConfig_Metric {
  /**
   * Required. A standard set of metrics is collected unless `metricOverrides`
   * are specified for the metric source (see [Custom metrics]
   * (https://cloud.google.com/dataproc/docs/guides/dataproc-metrics#custom_metrics)
   * for more information).
   */
  metricSource: DataprocMetricConfig_MetricSource;
  /**
   * Optional. Specify one or more [Custom metrics]
   * (https://cloud.google.com/dataproc/docs/guides/dataproc-metrics#custom_metrics)
   * to collect for the metric course (for the `SPARK` metric source (any
   * [Spark metric]
   * (https://spark.apache.org/docs/latest/monitoring.html#metrics) can be
   * specified).
   *
   * Provide metrics in the following format:
   * <code><var>METRIC_SOURCE</var>:<var>INSTANCE</var>:<var>GROUP</var>:<var>METRIC</var></code>
   * Use camelcase as appropriate.
   *
   * Examples:
   *
   * ```
   * yarn:ResourceManager:QueueMetrics:AppsCompleted
   * spark:driver:DAGScheduler:job.allJobs
   * sparkHistoryServer:JVM:Memory:NonHeapMemoryUsage.committed
   * hiveserver2:JVM:Memory:NonHeapMemoryUsage.used
   * ```
   *
   * Notes:
   *
   * * Only the specified overridden metrics are collected for the
   *   metric source. For example, if one or more `spark:executive` metrics
   *   are listed as metric overrides, other `SPARK` metrics are not
   *   collected. The collection of the metrics for other enabled custom
   *   metric sources is unaffected. For example, if both `SPARK` andd `YARN`
   *   metric sources are enabled, and overrides are provided for Spark
   *   metrics only, all YARN metrics are collected.
   */
  metricOverrides: string[];
}

/** A request to create a cluster. */
export interface CreateClusterRequest {
  /**
   * Required. The ID of the Google Cloud Platform project that the cluster
   * belongs to.
   */
  projectId: string;
  /** Required. The Dataproc region in which to handle the request. */
  region: string;
  /** Required. The cluster to create. */
  cluster:
    | Cluster
    | undefined;
  /**
   * Optional. A unique ID used to identify the request. If the server receives
   * two
   * [CreateClusterRequest](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.CreateClusterRequest)s
   * with the same id, then the second request will be ignored and the
   * first [google.longrunning.Operation][google.longrunning.Operation] created
   * and stored in the backend is returned.
   *
   * It is recommended to always set this value to a
   * [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).
   *
   * The ID must contain only letters (a-z, A-Z), numbers (0-9),
   * underscores (_), and hyphens (-). The maximum length is 40 characters.
   */
  requestId: string;
  /** Optional. Failure action when primary worker creation fails. */
  actionOnFailedPrimaryWorkers: FailureAction;
}

/** A request to update a cluster. */
export interface UpdateClusterRequest {
  /**
   * Required. The ID of the Google Cloud Platform project the
   * cluster belongs to.
   */
  projectId: string;
  /** Required. The Dataproc region in which to handle the request. */
  region: string;
  /** Required. The cluster name. */
  clusterName: string;
  /** Required. The changes to the cluster. */
  cluster:
    | Cluster
    | undefined;
  /**
   * Optional. Timeout for graceful YARN decommissioning. Graceful
   * decommissioning allows removing nodes from the cluster without
   * interrupting jobs in progress. Timeout specifies how long to wait for jobs
   * in progress to finish before forcefully removing nodes (and potentially
   * interrupting jobs). Default timeout is 0 (for forceful decommission), and
   * the maximum allowed timeout is 1 day. (see JSON representation of
   * [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
   *
   * Only supported on Dataproc image versions 1.2 and higher.
   */
  gracefulDecommissionTimeout:
    | Duration
    | undefined;
  /**
   * Required. Specifies the path, relative to `Cluster`, of
   * the field to update. For example, to change the number of workers
   * in a cluster to 5, the `update_mask` parameter would be
   * specified as `config.worker_config.num_instances`,
   * and the `PATCH` request body would specify the new value, as follows:
   *
   *     {
   *       "config":{
   *         "workerConfig":{
   *           "numInstances":"5"
   *         }
   *       }
   *     }
   * Similarly, to change the number of preemptible workers in a cluster to 5,
   * the `update_mask` parameter would be
   * `config.secondary_worker_config.num_instances`, and the `PATCH` request
   * body would be set as follows:
   *
   *     {
   *       "config":{
   *         "secondaryWorkerConfig":{
   *           "numInstances":"5"
   *         }
   *       }
   *     }
   * <strong>Note:</strong> Currently, only the following fields can be updated:
   *
   *  <table>
   *  <tbody>
   *  <tr>
   *  <td><strong>Mask</strong></td>
   *  <td><strong>Purpose</strong></td>
   *  </tr>
   *  <tr>
   *  <td><strong><em>labels</em></strong></td>
   *  <td>Update labels</td>
   *  </tr>
   *  <tr>
   *  <td><strong><em>config.worker_config.num_instances</em></strong></td>
   *  <td>Resize primary worker group</td>
   *  </tr>
   *  <tr>
   *  <td><strong><em>config.secondary_worker_config.num_instances</em></strong></td>
   *  <td>Resize secondary worker group</td>
   *  </tr>
   *  <tr>
   *  <td>config.autoscaling_config.policy_uri</td><td>Use, stop using, or
   *  change autoscaling policies</td>
   *  </tr>
   *  </tbody>
   *  </table>
   */
  updateMask:
    | string[]
    | undefined;
  /**
   * Optional. A unique ID used to identify the request. If the server
   * receives two
   * [UpdateClusterRequest](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.UpdateClusterRequest)s
   * with the same id, then the second request will be ignored and the
   * first [google.longrunning.Operation][google.longrunning.Operation] created
   * and stored in the backend is returned.
   *
   * It is recommended to always set this value to a
   * [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).
   *
   * The ID must contain only letters (a-z, A-Z), numbers (0-9),
   * underscores (_), and hyphens (-). The maximum length is 40 characters.
   */
  requestId: string;
}

/** A request to stop a cluster. */
export interface StopClusterRequest {
  /**
   * Required. The ID of the Google Cloud Platform project the
   * cluster belongs to.
   */
  projectId: string;
  /** Required. The Dataproc region in which to handle the request. */
  region: string;
  /** Required. The cluster name. */
  clusterName: string;
  /**
   * Optional. Specifying the `cluster_uuid` means the RPC will fail
   * (with error NOT_FOUND) if a cluster with the specified UUID does not exist.
   */
  clusterUuid: string;
  /**
   * Optional. A unique ID used to identify the request. If the server
   * receives two
   * [StopClusterRequest](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.StopClusterRequest)s
   * with the same id, then the second request will be ignored and the
   * first [google.longrunning.Operation][google.longrunning.Operation] created
   * and stored in the backend is returned.
   *
   * Recommendation: Set this value to a
   * [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).
   *
   * The ID must contain only letters (a-z, A-Z), numbers (0-9),
   * underscores (_), and hyphens (-). The maximum length is 40 characters.
   */
  requestId: string;
}

/** A request to start a cluster. */
export interface StartClusterRequest {
  /**
   * Required. The ID of the Google Cloud Platform project the
   * cluster belongs to.
   */
  projectId: string;
  /** Required. The Dataproc region in which to handle the request. */
  region: string;
  /** Required. The cluster name. */
  clusterName: string;
  /**
   * Optional. Specifying the `cluster_uuid` means the RPC will fail
   * (with error NOT_FOUND) if a cluster with the specified UUID does not exist.
   */
  clusterUuid: string;
  /**
   * Optional. A unique ID used to identify the request. If the server
   * receives two
   * [StartClusterRequest](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.StartClusterRequest)s
   * with the same id, then the second request will be ignored and the
   * first [google.longrunning.Operation][google.longrunning.Operation] created
   * and stored in the backend is returned.
   *
   * Recommendation: Set this value to a
   * [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).
   *
   * The ID must contain only letters (a-z, A-Z), numbers (0-9),
   * underscores (_), and hyphens (-). The maximum length is 40 characters.
   */
  requestId: string;
}

/** A request to delete a cluster. */
export interface DeleteClusterRequest {
  /**
   * Required. The ID of the Google Cloud Platform project that the cluster
   * belongs to.
   */
  projectId: string;
  /** Required. The Dataproc region in which to handle the request. */
  region: string;
  /** Required. The cluster name. */
  clusterName: string;
  /**
   * Optional. Specifying the `cluster_uuid` means the RPC should fail
   * (with error NOT_FOUND) if cluster with specified UUID does not exist.
   */
  clusterUuid: string;
  /**
   * Optional. A unique ID used to identify the request. If the server
   * receives two
   * [DeleteClusterRequest](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.DeleteClusterRequest)s
   * with the same id, then the second request will be ignored and the
   * first [google.longrunning.Operation][google.longrunning.Operation] created
   * and stored in the backend is returned.
   *
   * It is recommended to always set this value to a
   * [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).
   *
   * The ID must contain only letters (a-z, A-Z), numbers (0-9),
   * underscores (_), and hyphens (-). The maximum length is 40 characters.
   */
  requestId: string;
}

/** Request to get the resource representation for a cluster in a project. */
export interface GetClusterRequest {
  /**
   * Required. The ID of the Google Cloud Platform project that the cluster
   * belongs to.
   */
  projectId: string;
  /** Required. The Dataproc region in which to handle the request. */
  region: string;
  /** Required. The cluster name. */
  clusterName: string;
}

/** A request to list the clusters in a project. */
export interface ListClustersRequest {
  /**
   * Required. The ID of the Google Cloud Platform project that the cluster
   * belongs to.
   */
  projectId: string;
  /** Required. The Dataproc region in which to handle the request. */
  region: string;
  /**
   * Optional. A filter constraining the clusters to list. Filters are
   * case-sensitive and have the following syntax:
   *
   * field = value [AND [field = value]] ...
   *
   * where **field** is one of `status.state`, `clusterName`, or `labels.[KEY]`,
   * and `[KEY]` is a label key. **value** can be `*` to match all values.
   * `status.state` can be one of the following: `ACTIVE`, `INACTIVE`,
   * `CREATING`, `RUNNING`, `ERROR`, `DELETING`, `UPDATING`, `STOPPING`, or
   * `STOPPED`. `ACTIVE` contains the `CREATING`, `UPDATING`, and `RUNNING`
   * states. `INACTIVE` contains the `DELETING`, `ERROR`, `STOPPING`, and
   * `STOPPED` states. `clusterName` is the name of the cluster provided at
   * creation time. Only the logical `AND` operator is supported;
   * space-separated items are treated as having an implicit `AND` operator.
   *
   * Example filter:
   *
   * status.state = ACTIVE AND clusterName = mycluster
   * AND labels.env = staging AND labels.starred = *
   */
  filter: string;
  /** Optional. The standard List page size. */
  pageSize: number;
  /** Optional. The standard List page token. */
  pageToken: string;
}

/** The list of all clusters in a project. */
export interface ListClustersResponse {
  /** Output only. The clusters in the project. */
  clusters: Cluster[];
  /**
   * Output only. This token is included in the response if there are more
   * results to fetch. To fetch additional results, provide this value as the
   * `page_token` in a subsequent `ListClustersRequest`.
   */
  nextPageToken: string;
}

/** A request to collect cluster diagnostic information. */
export interface DiagnoseClusterRequest {
  /**
   * Required. The ID of the Google Cloud Platform project that the cluster
   * belongs to.
   */
  projectId: string;
  /** Required. The Dataproc region in which to handle the request. */
  region: string;
  /** Required. The cluster name. */
  clusterName: string;
  /**
   * Optional. (Optional) The output Cloud Storage directory for the diagnostic
   * tarball. If not specified, a task-specific directory in the cluster's
   * staging bucket will be used.
   */
  tarballGcsDir: string;
  /**
   * Optional. (Optional) The access type to the diagnostic tarball. If not
   * specified, falls back to default access of the bucket
   */
  tarballAccess: DiagnoseClusterRequest_TarballAccess;
  /**
   * Optional. Time interval in which diagnosis should be carried out on the
   * cluster.
   */
  diagnosisInterval:
    | Interval
    | undefined;
  /**
   * Optional. Specifies a list of jobs on which diagnosis is to be performed.
   * Format: projects/{project}/regions/{region}/jobs/{job}
   */
  jobs: string[];
  /**
   * Optional. Specifies a list of yarn applications on which diagnosis is to be
   * performed.
   */
  yarnApplicationIds: string[];
}

/** Defines who has access to the diagnostic tarball */
export enum DiagnoseClusterRequest_TarballAccess {
  /** TARBALL_ACCESS_UNSPECIFIED - Tarball Access unspecified. Falls back to default access of the bucket */
  TARBALL_ACCESS_UNSPECIFIED = 0,
  /**
   * GOOGLE_CLOUD_SUPPORT - Google Cloud Support group has read access to the
   * diagnostic tarball
   */
  GOOGLE_CLOUD_SUPPORT = 1,
  /**
   * GOOGLE_DATAPROC_DIAGNOSE - Google Cloud Dataproc Diagnose service account has read access to the
   * diagnostic tarball
   */
  GOOGLE_DATAPROC_DIAGNOSE = 2,
  UNRECOGNIZED = -1,
}

export function diagnoseClusterRequest_TarballAccessFromJSON(object: any): DiagnoseClusterRequest_TarballAccess {
  switch (object) {
    case 0:
    case "TARBALL_ACCESS_UNSPECIFIED":
      return DiagnoseClusterRequest_TarballAccess.TARBALL_ACCESS_UNSPECIFIED;
    case 1:
    case "GOOGLE_CLOUD_SUPPORT":
      return DiagnoseClusterRequest_TarballAccess.GOOGLE_CLOUD_SUPPORT;
    case 2:
    case "GOOGLE_DATAPROC_DIAGNOSE":
      return DiagnoseClusterRequest_TarballAccess.GOOGLE_DATAPROC_DIAGNOSE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DiagnoseClusterRequest_TarballAccess.UNRECOGNIZED;
  }
}

export function diagnoseClusterRequest_TarballAccessToJSON(object: DiagnoseClusterRequest_TarballAccess): string {
  switch (object) {
    case DiagnoseClusterRequest_TarballAccess.TARBALL_ACCESS_UNSPECIFIED:
      return "TARBALL_ACCESS_UNSPECIFIED";
    case DiagnoseClusterRequest_TarballAccess.GOOGLE_CLOUD_SUPPORT:
      return "GOOGLE_CLOUD_SUPPORT";
    case DiagnoseClusterRequest_TarballAccess.GOOGLE_DATAPROC_DIAGNOSE:
      return "GOOGLE_DATAPROC_DIAGNOSE";
    case DiagnoseClusterRequest_TarballAccess.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The location of diagnostic output. */
export interface DiagnoseClusterResults {
  /**
   * Output only. The Cloud Storage URI of the diagnostic output.
   * The output report is a plain text file with a summary of collected
   * diagnostics.
   */
  outputUri: string;
}

/** Reservation Affinity for consuming Zonal reservation. */
export interface ReservationAffinity {
  /** Optional. Type of reservation to consume */
  consumeReservationType: ReservationAffinity_Type;
  /** Optional. Corresponds to the label key of reservation resource. */
  key: string;
  /** Optional. Corresponds to the label values of reservation resource. */
  values: string[];
}

/** Indicates whether to consume capacity from an reservation or not. */
export enum ReservationAffinity_Type {
  TYPE_UNSPECIFIED = 0,
  /** NO_RESERVATION - Do not consume from any allocated capacity. */
  NO_RESERVATION = 1,
  /** ANY_RESERVATION - Consume any reservation available. */
  ANY_RESERVATION = 2,
  /**
   * SPECIFIC_RESERVATION - Must consume from a specific reservation. Must specify key value fields
   * for specifying the reservations.
   */
  SPECIFIC_RESERVATION = 3,
  UNRECOGNIZED = -1,
}

export function reservationAffinity_TypeFromJSON(object: any): ReservationAffinity_Type {
  switch (object) {
    case 0:
    case "TYPE_UNSPECIFIED":
      return ReservationAffinity_Type.TYPE_UNSPECIFIED;
    case 1:
    case "NO_RESERVATION":
      return ReservationAffinity_Type.NO_RESERVATION;
    case 2:
    case "ANY_RESERVATION":
      return ReservationAffinity_Type.ANY_RESERVATION;
    case 3:
    case "SPECIFIC_RESERVATION":
      return ReservationAffinity_Type.SPECIFIC_RESERVATION;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ReservationAffinity_Type.UNRECOGNIZED;
  }
}

export function reservationAffinity_TypeToJSON(object: ReservationAffinity_Type): string {
  switch (object) {
    case ReservationAffinity_Type.TYPE_UNSPECIFIED:
      return "TYPE_UNSPECIFIED";
    case ReservationAffinity_Type.NO_RESERVATION:
      return "NO_RESERVATION";
    case ReservationAffinity_Type.ANY_RESERVATION:
      return "ANY_RESERVATION";
    case ReservationAffinity_Type.SPECIFIC_RESERVATION:
      return "SPECIFIC_RESERVATION";
    case ReservationAffinity_Type.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseCluster(): Cluster {
  return {
    projectId: "",
    clusterName: "",
    config: undefined,
    virtualClusterConfig: undefined,
    labels: {},
    status: undefined,
    statusHistory: [],
    clusterUuid: "",
    metrics: undefined,
  };
}

export const Cluster: MessageFns<Cluster> = {
  encode(message: Cluster, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.clusterName !== "") {
      writer.uint32(18).string(message.clusterName);
    }
    if (message.config !== undefined) {
      ClusterConfig.encode(message.config, writer.uint32(26).fork()).join();
    }
    if (message.virtualClusterConfig !== undefined) {
      VirtualClusterConfig.encode(message.virtualClusterConfig, writer.uint32(82).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      Cluster_LabelsEntry.encode({ key: key as any, value }, writer.uint32(66).fork()).join();
    });
    if (message.status !== undefined) {
      ClusterStatus.encode(message.status, writer.uint32(34).fork()).join();
    }
    for (const v of message.statusHistory) {
      ClusterStatus.encode(v!, writer.uint32(58).fork()).join();
    }
    if (message.clusterUuid !== "") {
      writer.uint32(50).string(message.clusterUuid);
    }
    if (message.metrics !== undefined) {
      ClusterMetrics.encode(message.metrics, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.clusterName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.config = ClusterConfig.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.virtualClusterConfig = VirtualClusterConfig.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          const entry8 = Cluster_LabelsEntry.decode(reader, reader.uint32());
          if (entry8.value !== undefined) {
            message.labels[entry8.key] = entry8.value;
          }
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.status = ClusterStatus.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.statusHistory.push(ClusterStatus.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.clusterUuid = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.metrics = ClusterMetrics.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      clusterName: isSet(object.clusterName) ? globalThis.String(object.clusterName) : "",
      config: isSet(object.config) ? ClusterConfig.fromJSON(object.config) : undefined,
      virtualClusterConfig: isSet(object.virtualClusterConfig)
        ? VirtualClusterConfig.fromJSON(object.virtualClusterConfig)
        : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      status: isSet(object.status) ? ClusterStatus.fromJSON(object.status) : undefined,
      statusHistory: globalThis.Array.isArray(object?.statusHistory)
        ? object.statusHistory.map((e: any) => ClusterStatus.fromJSON(e))
        : [],
      clusterUuid: isSet(object.clusterUuid) ? globalThis.String(object.clusterUuid) : "",
      metrics: isSet(object.metrics) ? ClusterMetrics.fromJSON(object.metrics) : undefined,
    };
  },

  toJSON(message: Cluster): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.clusterName !== "") {
      obj.clusterName = message.clusterName;
    }
    if (message.config !== undefined) {
      obj.config = ClusterConfig.toJSON(message.config);
    }
    if (message.virtualClusterConfig !== undefined) {
      obj.virtualClusterConfig = VirtualClusterConfig.toJSON(message.virtualClusterConfig);
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.status !== undefined) {
      obj.status = ClusterStatus.toJSON(message.status);
    }
    if (message.statusHistory?.length) {
      obj.statusHistory = message.statusHistory.map((e) => ClusterStatus.toJSON(e));
    }
    if (message.clusterUuid !== "") {
      obj.clusterUuid = message.clusterUuid;
    }
    if (message.metrics !== undefined) {
      obj.metrics = ClusterMetrics.toJSON(message.metrics);
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster>): Cluster {
    return Cluster.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster>): Cluster {
    const message = createBaseCluster();
    message.projectId = object.projectId ?? "";
    message.clusterName = object.clusterName ?? "";
    message.config = (object.config !== undefined && object.config !== null)
      ? ClusterConfig.fromPartial(object.config)
      : undefined;
    message.virtualClusterConfig = (object.virtualClusterConfig !== undefined && object.virtualClusterConfig !== null)
      ? VirtualClusterConfig.fromPartial(object.virtualClusterConfig)
      : undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.status = (object.status !== undefined && object.status !== null)
      ? ClusterStatus.fromPartial(object.status)
      : undefined;
    message.statusHistory = object.statusHistory?.map((e) => ClusterStatus.fromPartial(e)) || [];
    message.clusterUuid = object.clusterUuid ?? "";
    message.metrics = (object.metrics !== undefined && object.metrics !== null)
      ? ClusterMetrics.fromPartial(object.metrics)
      : undefined;
    return message;
  },
};

function createBaseCluster_LabelsEntry(): Cluster_LabelsEntry {
  return { key: "", value: "" };
}

export const Cluster_LabelsEntry: MessageFns<Cluster_LabelsEntry> = {
  encode(message: Cluster_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Cluster_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCluster_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Cluster_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Cluster_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Cluster_LabelsEntry>): Cluster_LabelsEntry {
    return Cluster_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Cluster_LabelsEntry>): Cluster_LabelsEntry {
    const message = createBaseCluster_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseClusterConfig(): ClusterConfig {
  return {
    configBucket: "",
    tempBucket: "",
    gceClusterConfig: undefined,
    masterConfig: undefined,
    workerConfig: undefined,
    secondaryWorkerConfig: undefined,
    softwareConfig: undefined,
    initializationActions: [],
    encryptionConfig: undefined,
    autoscalingConfig: undefined,
    securityConfig: undefined,
    lifecycleConfig: undefined,
    endpointConfig: undefined,
    metastoreConfig: undefined,
    dataprocMetricConfig: undefined,
    auxiliaryNodeGroups: [],
  };
}

export const ClusterConfig: MessageFns<ClusterConfig> = {
  encode(message: ClusterConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.configBucket !== "") {
      writer.uint32(10).string(message.configBucket);
    }
    if (message.tempBucket !== "") {
      writer.uint32(18).string(message.tempBucket);
    }
    if (message.gceClusterConfig !== undefined) {
      GceClusterConfig.encode(message.gceClusterConfig, writer.uint32(66).fork()).join();
    }
    if (message.masterConfig !== undefined) {
      InstanceGroupConfig.encode(message.masterConfig, writer.uint32(74).fork()).join();
    }
    if (message.workerConfig !== undefined) {
      InstanceGroupConfig.encode(message.workerConfig, writer.uint32(82).fork()).join();
    }
    if (message.secondaryWorkerConfig !== undefined) {
      InstanceGroupConfig.encode(message.secondaryWorkerConfig, writer.uint32(98).fork()).join();
    }
    if (message.softwareConfig !== undefined) {
      SoftwareConfig.encode(message.softwareConfig, writer.uint32(106).fork()).join();
    }
    for (const v of message.initializationActions) {
      NodeInitializationAction.encode(v!, writer.uint32(90).fork()).join();
    }
    if (message.encryptionConfig !== undefined) {
      EncryptionConfig.encode(message.encryptionConfig, writer.uint32(122).fork()).join();
    }
    if (message.autoscalingConfig !== undefined) {
      AutoscalingConfig.encode(message.autoscalingConfig, writer.uint32(146).fork()).join();
    }
    if (message.securityConfig !== undefined) {
      SecurityConfig.encode(message.securityConfig, writer.uint32(130).fork()).join();
    }
    if (message.lifecycleConfig !== undefined) {
      LifecycleConfig.encode(message.lifecycleConfig, writer.uint32(138).fork()).join();
    }
    if (message.endpointConfig !== undefined) {
      EndpointConfig.encode(message.endpointConfig, writer.uint32(154).fork()).join();
    }
    if (message.metastoreConfig !== undefined) {
      MetastoreConfig.encode(message.metastoreConfig, writer.uint32(162).fork()).join();
    }
    if (message.dataprocMetricConfig !== undefined) {
      DataprocMetricConfig.encode(message.dataprocMetricConfig, writer.uint32(186).fork()).join();
    }
    for (const v of message.auxiliaryNodeGroups) {
      AuxiliaryNodeGroup.encode(v!, writer.uint32(202).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.configBucket = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.tempBucket = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.gceClusterConfig = GceClusterConfig.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.masterConfig = InstanceGroupConfig.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.workerConfig = InstanceGroupConfig.decode(reader, reader.uint32());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.secondaryWorkerConfig = InstanceGroupConfig.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.softwareConfig = SoftwareConfig.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.initializationActions.push(NodeInitializationAction.decode(reader, reader.uint32()));
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.encryptionConfig = EncryptionConfig.decode(reader, reader.uint32());
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.autoscalingConfig = AutoscalingConfig.decode(reader, reader.uint32());
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.securityConfig = SecurityConfig.decode(reader, reader.uint32());
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.lifecycleConfig = LifecycleConfig.decode(reader, reader.uint32());
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.endpointConfig = EndpointConfig.decode(reader, reader.uint32());
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.metastoreConfig = MetastoreConfig.decode(reader, reader.uint32());
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.dataprocMetricConfig = DataprocMetricConfig.decode(reader, reader.uint32());
          continue;
        case 25:
          if (tag !== 202) {
            break;
          }

          message.auxiliaryNodeGroups.push(AuxiliaryNodeGroup.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterConfig {
    return {
      configBucket: isSet(object.configBucket) ? globalThis.String(object.configBucket) : "",
      tempBucket: isSet(object.tempBucket) ? globalThis.String(object.tempBucket) : "",
      gceClusterConfig: isSet(object.gceClusterConfig) ? GceClusterConfig.fromJSON(object.gceClusterConfig) : undefined,
      masterConfig: isSet(object.masterConfig) ? InstanceGroupConfig.fromJSON(object.masterConfig) : undefined,
      workerConfig: isSet(object.workerConfig) ? InstanceGroupConfig.fromJSON(object.workerConfig) : undefined,
      secondaryWorkerConfig: isSet(object.secondaryWorkerConfig)
        ? InstanceGroupConfig.fromJSON(object.secondaryWorkerConfig)
        : undefined,
      softwareConfig: isSet(object.softwareConfig) ? SoftwareConfig.fromJSON(object.softwareConfig) : undefined,
      initializationActions: globalThis.Array.isArray(object?.initializationActions)
        ? object.initializationActions.map((e: any) => NodeInitializationAction.fromJSON(e))
        : [],
      encryptionConfig: isSet(object.encryptionConfig) ? EncryptionConfig.fromJSON(object.encryptionConfig) : undefined,
      autoscalingConfig: isSet(object.autoscalingConfig)
        ? AutoscalingConfig.fromJSON(object.autoscalingConfig)
        : undefined,
      securityConfig: isSet(object.securityConfig) ? SecurityConfig.fromJSON(object.securityConfig) : undefined,
      lifecycleConfig: isSet(object.lifecycleConfig) ? LifecycleConfig.fromJSON(object.lifecycleConfig) : undefined,
      endpointConfig: isSet(object.endpointConfig) ? EndpointConfig.fromJSON(object.endpointConfig) : undefined,
      metastoreConfig: isSet(object.metastoreConfig) ? MetastoreConfig.fromJSON(object.metastoreConfig) : undefined,
      dataprocMetricConfig: isSet(object.dataprocMetricConfig)
        ? DataprocMetricConfig.fromJSON(object.dataprocMetricConfig)
        : undefined,
      auxiliaryNodeGroups: globalThis.Array.isArray(object?.auxiliaryNodeGroups)
        ? object.auxiliaryNodeGroups.map((e: any) => AuxiliaryNodeGroup.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ClusterConfig): unknown {
    const obj: any = {};
    if (message.configBucket !== "") {
      obj.configBucket = message.configBucket;
    }
    if (message.tempBucket !== "") {
      obj.tempBucket = message.tempBucket;
    }
    if (message.gceClusterConfig !== undefined) {
      obj.gceClusterConfig = GceClusterConfig.toJSON(message.gceClusterConfig);
    }
    if (message.masterConfig !== undefined) {
      obj.masterConfig = InstanceGroupConfig.toJSON(message.masterConfig);
    }
    if (message.workerConfig !== undefined) {
      obj.workerConfig = InstanceGroupConfig.toJSON(message.workerConfig);
    }
    if (message.secondaryWorkerConfig !== undefined) {
      obj.secondaryWorkerConfig = InstanceGroupConfig.toJSON(message.secondaryWorkerConfig);
    }
    if (message.softwareConfig !== undefined) {
      obj.softwareConfig = SoftwareConfig.toJSON(message.softwareConfig);
    }
    if (message.initializationActions?.length) {
      obj.initializationActions = message.initializationActions.map((e) => NodeInitializationAction.toJSON(e));
    }
    if (message.encryptionConfig !== undefined) {
      obj.encryptionConfig = EncryptionConfig.toJSON(message.encryptionConfig);
    }
    if (message.autoscalingConfig !== undefined) {
      obj.autoscalingConfig = AutoscalingConfig.toJSON(message.autoscalingConfig);
    }
    if (message.securityConfig !== undefined) {
      obj.securityConfig = SecurityConfig.toJSON(message.securityConfig);
    }
    if (message.lifecycleConfig !== undefined) {
      obj.lifecycleConfig = LifecycleConfig.toJSON(message.lifecycleConfig);
    }
    if (message.endpointConfig !== undefined) {
      obj.endpointConfig = EndpointConfig.toJSON(message.endpointConfig);
    }
    if (message.metastoreConfig !== undefined) {
      obj.metastoreConfig = MetastoreConfig.toJSON(message.metastoreConfig);
    }
    if (message.dataprocMetricConfig !== undefined) {
      obj.dataprocMetricConfig = DataprocMetricConfig.toJSON(message.dataprocMetricConfig);
    }
    if (message.auxiliaryNodeGroups?.length) {
      obj.auxiliaryNodeGroups = message.auxiliaryNodeGroups.map((e) => AuxiliaryNodeGroup.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterConfig>): ClusterConfig {
    return ClusterConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterConfig>): ClusterConfig {
    const message = createBaseClusterConfig();
    message.configBucket = object.configBucket ?? "";
    message.tempBucket = object.tempBucket ?? "";
    message.gceClusterConfig = (object.gceClusterConfig !== undefined && object.gceClusterConfig !== null)
      ? GceClusterConfig.fromPartial(object.gceClusterConfig)
      : undefined;
    message.masterConfig = (object.masterConfig !== undefined && object.masterConfig !== null)
      ? InstanceGroupConfig.fromPartial(object.masterConfig)
      : undefined;
    message.workerConfig = (object.workerConfig !== undefined && object.workerConfig !== null)
      ? InstanceGroupConfig.fromPartial(object.workerConfig)
      : undefined;
    message.secondaryWorkerConfig =
      (object.secondaryWorkerConfig !== undefined && object.secondaryWorkerConfig !== null)
        ? InstanceGroupConfig.fromPartial(object.secondaryWorkerConfig)
        : undefined;
    message.softwareConfig = (object.softwareConfig !== undefined && object.softwareConfig !== null)
      ? SoftwareConfig.fromPartial(object.softwareConfig)
      : undefined;
    message.initializationActions = object.initializationActions?.map((e) => NodeInitializationAction.fromPartial(e)) ||
      [];
    message.encryptionConfig = (object.encryptionConfig !== undefined && object.encryptionConfig !== null)
      ? EncryptionConfig.fromPartial(object.encryptionConfig)
      : undefined;
    message.autoscalingConfig = (object.autoscalingConfig !== undefined && object.autoscalingConfig !== null)
      ? AutoscalingConfig.fromPartial(object.autoscalingConfig)
      : undefined;
    message.securityConfig = (object.securityConfig !== undefined && object.securityConfig !== null)
      ? SecurityConfig.fromPartial(object.securityConfig)
      : undefined;
    message.lifecycleConfig = (object.lifecycleConfig !== undefined && object.lifecycleConfig !== null)
      ? LifecycleConfig.fromPartial(object.lifecycleConfig)
      : undefined;
    message.endpointConfig = (object.endpointConfig !== undefined && object.endpointConfig !== null)
      ? EndpointConfig.fromPartial(object.endpointConfig)
      : undefined;
    message.metastoreConfig = (object.metastoreConfig !== undefined && object.metastoreConfig !== null)
      ? MetastoreConfig.fromPartial(object.metastoreConfig)
      : undefined;
    message.dataprocMetricConfig = (object.dataprocMetricConfig !== undefined && object.dataprocMetricConfig !== null)
      ? DataprocMetricConfig.fromPartial(object.dataprocMetricConfig)
      : undefined;
    message.auxiliaryNodeGroups = object.auxiliaryNodeGroups?.map((e) => AuxiliaryNodeGroup.fromPartial(e)) || [];
    return message;
  },
};

function createBaseVirtualClusterConfig(): VirtualClusterConfig {
  return { stagingBucket: "", kubernetesClusterConfig: undefined, auxiliaryServicesConfig: undefined };
}

export const VirtualClusterConfig: MessageFns<VirtualClusterConfig> = {
  encode(message: VirtualClusterConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.stagingBucket !== "") {
      writer.uint32(10).string(message.stagingBucket);
    }
    if (message.kubernetesClusterConfig !== undefined) {
      KubernetesClusterConfig.encode(message.kubernetesClusterConfig, writer.uint32(50).fork()).join();
    }
    if (message.auxiliaryServicesConfig !== undefined) {
      AuxiliaryServicesConfig.encode(message.auxiliaryServicesConfig, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VirtualClusterConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVirtualClusterConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.stagingBucket = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.kubernetesClusterConfig = KubernetesClusterConfig.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.auxiliaryServicesConfig = AuxiliaryServicesConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VirtualClusterConfig {
    return {
      stagingBucket: isSet(object.stagingBucket) ? globalThis.String(object.stagingBucket) : "",
      kubernetesClusterConfig: isSet(object.kubernetesClusterConfig)
        ? KubernetesClusterConfig.fromJSON(object.kubernetesClusterConfig)
        : undefined,
      auxiliaryServicesConfig: isSet(object.auxiliaryServicesConfig)
        ? AuxiliaryServicesConfig.fromJSON(object.auxiliaryServicesConfig)
        : undefined,
    };
  },

  toJSON(message: VirtualClusterConfig): unknown {
    const obj: any = {};
    if (message.stagingBucket !== "") {
      obj.stagingBucket = message.stagingBucket;
    }
    if (message.kubernetesClusterConfig !== undefined) {
      obj.kubernetesClusterConfig = KubernetesClusterConfig.toJSON(message.kubernetesClusterConfig);
    }
    if (message.auxiliaryServicesConfig !== undefined) {
      obj.auxiliaryServicesConfig = AuxiliaryServicesConfig.toJSON(message.auxiliaryServicesConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<VirtualClusterConfig>): VirtualClusterConfig {
    return VirtualClusterConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VirtualClusterConfig>): VirtualClusterConfig {
    const message = createBaseVirtualClusterConfig();
    message.stagingBucket = object.stagingBucket ?? "";
    message.kubernetesClusterConfig =
      (object.kubernetesClusterConfig !== undefined && object.kubernetesClusterConfig !== null)
        ? KubernetesClusterConfig.fromPartial(object.kubernetesClusterConfig)
        : undefined;
    message.auxiliaryServicesConfig =
      (object.auxiliaryServicesConfig !== undefined && object.auxiliaryServicesConfig !== null)
        ? AuxiliaryServicesConfig.fromPartial(object.auxiliaryServicesConfig)
        : undefined;
    return message;
  },
};

function createBaseAuxiliaryServicesConfig(): AuxiliaryServicesConfig {
  return { metastoreConfig: undefined, sparkHistoryServerConfig: undefined };
}

export const AuxiliaryServicesConfig: MessageFns<AuxiliaryServicesConfig> = {
  encode(message: AuxiliaryServicesConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.metastoreConfig !== undefined) {
      MetastoreConfig.encode(message.metastoreConfig, writer.uint32(10).fork()).join();
    }
    if (message.sparkHistoryServerConfig !== undefined) {
      SparkHistoryServerConfig.encode(message.sparkHistoryServerConfig, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AuxiliaryServicesConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAuxiliaryServicesConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.metastoreConfig = MetastoreConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.sparkHistoryServerConfig = SparkHistoryServerConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AuxiliaryServicesConfig {
    return {
      metastoreConfig: isSet(object.metastoreConfig) ? MetastoreConfig.fromJSON(object.metastoreConfig) : undefined,
      sparkHistoryServerConfig: isSet(object.sparkHistoryServerConfig)
        ? SparkHistoryServerConfig.fromJSON(object.sparkHistoryServerConfig)
        : undefined,
    };
  },

  toJSON(message: AuxiliaryServicesConfig): unknown {
    const obj: any = {};
    if (message.metastoreConfig !== undefined) {
      obj.metastoreConfig = MetastoreConfig.toJSON(message.metastoreConfig);
    }
    if (message.sparkHistoryServerConfig !== undefined) {
      obj.sparkHistoryServerConfig = SparkHistoryServerConfig.toJSON(message.sparkHistoryServerConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<AuxiliaryServicesConfig>): AuxiliaryServicesConfig {
    return AuxiliaryServicesConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AuxiliaryServicesConfig>): AuxiliaryServicesConfig {
    const message = createBaseAuxiliaryServicesConfig();
    message.metastoreConfig = (object.metastoreConfig !== undefined && object.metastoreConfig !== null)
      ? MetastoreConfig.fromPartial(object.metastoreConfig)
      : undefined;
    message.sparkHistoryServerConfig =
      (object.sparkHistoryServerConfig !== undefined && object.sparkHistoryServerConfig !== null)
        ? SparkHistoryServerConfig.fromPartial(object.sparkHistoryServerConfig)
        : undefined;
    return message;
  },
};

function createBaseEndpointConfig(): EndpointConfig {
  return { httpPorts: {}, enableHttpPortAccess: false };
}

export const EndpointConfig: MessageFns<EndpointConfig> = {
  encode(message: EndpointConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.httpPorts).forEach(([key, value]) => {
      EndpointConfig_HttpPortsEntry.encode({ key: key as any, value }, writer.uint32(10).fork()).join();
    });
    if (message.enableHttpPortAccess !== false) {
      writer.uint32(16).bool(message.enableHttpPortAccess);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EndpointConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEndpointConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          const entry1 = EndpointConfig_HttpPortsEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.httpPorts[entry1.key] = entry1.value;
          }
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.enableHttpPortAccess = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EndpointConfig {
    return {
      httpPorts: isObject(object.httpPorts)
        ? Object.entries(object.httpPorts).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      enableHttpPortAccess: isSet(object.enableHttpPortAccess)
        ? globalThis.Boolean(object.enableHttpPortAccess)
        : false,
    };
  },

  toJSON(message: EndpointConfig): unknown {
    const obj: any = {};
    if (message.httpPorts) {
      const entries = Object.entries(message.httpPorts);
      if (entries.length > 0) {
        obj.httpPorts = {};
        entries.forEach(([k, v]) => {
          obj.httpPorts[k] = v;
        });
      }
    }
    if (message.enableHttpPortAccess !== false) {
      obj.enableHttpPortAccess = message.enableHttpPortAccess;
    }
    return obj;
  },

  create(base?: DeepPartial<EndpointConfig>): EndpointConfig {
    return EndpointConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EndpointConfig>): EndpointConfig {
    const message = createBaseEndpointConfig();
    message.httpPorts = Object.entries(object.httpPorts ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.enableHttpPortAccess = object.enableHttpPortAccess ?? false;
    return message;
  },
};

function createBaseEndpointConfig_HttpPortsEntry(): EndpointConfig_HttpPortsEntry {
  return { key: "", value: "" };
}

export const EndpointConfig_HttpPortsEntry: MessageFns<EndpointConfig_HttpPortsEntry> = {
  encode(message: EndpointConfig_HttpPortsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EndpointConfig_HttpPortsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEndpointConfig_HttpPortsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EndpointConfig_HttpPortsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: EndpointConfig_HttpPortsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<EndpointConfig_HttpPortsEntry>): EndpointConfig_HttpPortsEntry {
    return EndpointConfig_HttpPortsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EndpointConfig_HttpPortsEntry>): EndpointConfig_HttpPortsEntry {
    const message = createBaseEndpointConfig_HttpPortsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseAutoscalingConfig(): AutoscalingConfig {
  return { policyUri: "" };
}

export const AutoscalingConfig: MessageFns<AutoscalingConfig> = {
  encode(message: AutoscalingConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.policyUri !== "") {
      writer.uint32(10).string(message.policyUri);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutoscalingConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutoscalingConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.policyUri = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutoscalingConfig {
    return { policyUri: isSet(object.policyUri) ? globalThis.String(object.policyUri) : "" };
  },

  toJSON(message: AutoscalingConfig): unknown {
    const obj: any = {};
    if (message.policyUri !== "") {
      obj.policyUri = message.policyUri;
    }
    return obj;
  },

  create(base?: DeepPartial<AutoscalingConfig>): AutoscalingConfig {
    return AutoscalingConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutoscalingConfig>): AutoscalingConfig {
    const message = createBaseAutoscalingConfig();
    message.policyUri = object.policyUri ?? "";
    return message;
  },
};

function createBaseEncryptionConfig(): EncryptionConfig {
  return { gcePdKmsKeyName: "", kmsKey: "" };
}

export const EncryptionConfig: MessageFns<EncryptionConfig> = {
  encode(message: EncryptionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcePdKmsKeyName !== "") {
      writer.uint32(10).string(message.gcePdKmsKeyName);
    }
    if (message.kmsKey !== "") {
      writer.uint32(18).string(message.kmsKey);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EncryptionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEncryptionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.gcePdKmsKeyName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.kmsKey = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EncryptionConfig {
    return {
      gcePdKmsKeyName: isSet(object.gcePdKmsKeyName) ? globalThis.String(object.gcePdKmsKeyName) : "",
      kmsKey: isSet(object.kmsKey) ? globalThis.String(object.kmsKey) : "",
    };
  },

  toJSON(message: EncryptionConfig): unknown {
    const obj: any = {};
    if (message.gcePdKmsKeyName !== "") {
      obj.gcePdKmsKeyName = message.gcePdKmsKeyName;
    }
    if (message.kmsKey !== "") {
      obj.kmsKey = message.kmsKey;
    }
    return obj;
  },

  create(base?: DeepPartial<EncryptionConfig>): EncryptionConfig {
    return EncryptionConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EncryptionConfig>): EncryptionConfig {
    const message = createBaseEncryptionConfig();
    message.gcePdKmsKeyName = object.gcePdKmsKeyName ?? "";
    message.kmsKey = object.kmsKey ?? "";
    return message;
  },
};

function createBaseGceClusterConfig(): GceClusterConfig {
  return {
    zoneUri: "",
    networkUri: "",
    subnetworkUri: "",
    internalIpOnly: undefined,
    privateIpv6GoogleAccess: 0,
    serviceAccount: "",
    serviceAccountScopes: [],
    tags: [],
    metadata: {},
    reservationAffinity: undefined,
    nodeGroupAffinity: undefined,
    shieldedInstanceConfig: undefined,
    confidentialInstanceConfig: undefined,
  };
}

export const GceClusterConfig: MessageFns<GceClusterConfig> = {
  encode(message: GceClusterConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.zoneUri !== "") {
      writer.uint32(10).string(message.zoneUri);
    }
    if (message.networkUri !== "") {
      writer.uint32(18).string(message.networkUri);
    }
    if (message.subnetworkUri !== "") {
      writer.uint32(50).string(message.subnetworkUri);
    }
    if (message.internalIpOnly !== undefined) {
      writer.uint32(56).bool(message.internalIpOnly);
    }
    if (message.privateIpv6GoogleAccess !== 0) {
      writer.uint32(96).int32(message.privateIpv6GoogleAccess);
    }
    if (message.serviceAccount !== "") {
      writer.uint32(66).string(message.serviceAccount);
    }
    for (const v of message.serviceAccountScopes) {
      writer.uint32(26).string(v!);
    }
    for (const v of message.tags) {
      writer.uint32(34).string(v!);
    }
    Object.entries(message.metadata).forEach(([key, value]) => {
      GceClusterConfig_MetadataEntry.encode({ key: key as any, value }, writer.uint32(42).fork()).join();
    });
    if (message.reservationAffinity !== undefined) {
      ReservationAffinity.encode(message.reservationAffinity, writer.uint32(90).fork()).join();
    }
    if (message.nodeGroupAffinity !== undefined) {
      NodeGroupAffinity.encode(message.nodeGroupAffinity, writer.uint32(106).fork()).join();
    }
    if (message.shieldedInstanceConfig !== undefined) {
      ShieldedInstanceConfig.encode(message.shieldedInstanceConfig, writer.uint32(114).fork()).join();
    }
    if (message.confidentialInstanceConfig !== undefined) {
      ConfidentialInstanceConfig.encode(message.confidentialInstanceConfig, writer.uint32(122).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GceClusterConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGceClusterConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.zoneUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.networkUri = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.subnetworkUri = reader.string();
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.internalIpOnly = reader.bool();
          continue;
        case 12:
          if (tag !== 96) {
            break;
          }

          message.privateIpv6GoogleAccess = reader.int32() as any;
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.serviceAccount = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.serviceAccountScopes.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.tags.push(reader.string());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          const entry5 = GceClusterConfig_MetadataEntry.decode(reader, reader.uint32());
          if (entry5.value !== undefined) {
            message.metadata[entry5.key] = entry5.value;
          }
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.reservationAffinity = ReservationAffinity.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.nodeGroupAffinity = NodeGroupAffinity.decode(reader, reader.uint32());
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.shieldedInstanceConfig = ShieldedInstanceConfig.decode(reader, reader.uint32());
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.confidentialInstanceConfig = ConfidentialInstanceConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GceClusterConfig {
    return {
      zoneUri: isSet(object.zoneUri) ? globalThis.String(object.zoneUri) : "",
      networkUri: isSet(object.networkUri) ? globalThis.String(object.networkUri) : "",
      subnetworkUri: isSet(object.subnetworkUri) ? globalThis.String(object.subnetworkUri) : "",
      internalIpOnly: isSet(object.internalIpOnly) ? globalThis.Boolean(object.internalIpOnly) : undefined,
      privateIpv6GoogleAccess: isSet(object.privateIpv6GoogleAccess)
        ? gceClusterConfig_PrivateIpv6GoogleAccessFromJSON(object.privateIpv6GoogleAccess)
        : 0,
      serviceAccount: isSet(object.serviceAccount) ? globalThis.String(object.serviceAccount) : "",
      serviceAccountScopes: globalThis.Array.isArray(object?.serviceAccountScopes)
        ? object.serviceAccountScopes.map((e: any) => globalThis.String(e))
        : [],
      tags: globalThis.Array.isArray(object?.tags) ? object.tags.map((e: any) => globalThis.String(e)) : [],
      metadata: isObject(object.metadata)
        ? Object.entries(object.metadata).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      reservationAffinity: isSet(object.reservationAffinity)
        ? ReservationAffinity.fromJSON(object.reservationAffinity)
        : undefined,
      nodeGroupAffinity: isSet(object.nodeGroupAffinity)
        ? NodeGroupAffinity.fromJSON(object.nodeGroupAffinity)
        : undefined,
      shieldedInstanceConfig: isSet(object.shieldedInstanceConfig)
        ? ShieldedInstanceConfig.fromJSON(object.shieldedInstanceConfig)
        : undefined,
      confidentialInstanceConfig: isSet(object.confidentialInstanceConfig)
        ? ConfidentialInstanceConfig.fromJSON(object.confidentialInstanceConfig)
        : undefined,
    };
  },

  toJSON(message: GceClusterConfig): unknown {
    const obj: any = {};
    if (message.zoneUri !== "") {
      obj.zoneUri = message.zoneUri;
    }
    if (message.networkUri !== "") {
      obj.networkUri = message.networkUri;
    }
    if (message.subnetworkUri !== "") {
      obj.subnetworkUri = message.subnetworkUri;
    }
    if (message.internalIpOnly !== undefined) {
      obj.internalIpOnly = message.internalIpOnly;
    }
    if (message.privateIpv6GoogleAccess !== 0) {
      obj.privateIpv6GoogleAccess = gceClusterConfig_PrivateIpv6GoogleAccessToJSON(message.privateIpv6GoogleAccess);
    }
    if (message.serviceAccount !== "") {
      obj.serviceAccount = message.serviceAccount;
    }
    if (message.serviceAccountScopes?.length) {
      obj.serviceAccountScopes = message.serviceAccountScopes;
    }
    if (message.tags?.length) {
      obj.tags = message.tags;
    }
    if (message.metadata) {
      const entries = Object.entries(message.metadata);
      if (entries.length > 0) {
        obj.metadata = {};
        entries.forEach(([k, v]) => {
          obj.metadata[k] = v;
        });
      }
    }
    if (message.reservationAffinity !== undefined) {
      obj.reservationAffinity = ReservationAffinity.toJSON(message.reservationAffinity);
    }
    if (message.nodeGroupAffinity !== undefined) {
      obj.nodeGroupAffinity = NodeGroupAffinity.toJSON(message.nodeGroupAffinity);
    }
    if (message.shieldedInstanceConfig !== undefined) {
      obj.shieldedInstanceConfig = ShieldedInstanceConfig.toJSON(message.shieldedInstanceConfig);
    }
    if (message.confidentialInstanceConfig !== undefined) {
      obj.confidentialInstanceConfig = ConfidentialInstanceConfig.toJSON(message.confidentialInstanceConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<GceClusterConfig>): GceClusterConfig {
    return GceClusterConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GceClusterConfig>): GceClusterConfig {
    const message = createBaseGceClusterConfig();
    message.zoneUri = object.zoneUri ?? "";
    message.networkUri = object.networkUri ?? "";
    message.subnetworkUri = object.subnetworkUri ?? "";
    message.internalIpOnly = object.internalIpOnly ?? undefined;
    message.privateIpv6GoogleAccess = object.privateIpv6GoogleAccess ?? 0;
    message.serviceAccount = object.serviceAccount ?? "";
    message.serviceAccountScopes = object.serviceAccountScopes?.map((e) => e) || [];
    message.tags = object.tags?.map((e) => e) || [];
    message.metadata = Object.entries(object.metadata ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.reservationAffinity = (object.reservationAffinity !== undefined && object.reservationAffinity !== null)
      ? ReservationAffinity.fromPartial(object.reservationAffinity)
      : undefined;
    message.nodeGroupAffinity = (object.nodeGroupAffinity !== undefined && object.nodeGroupAffinity !== null)
      ? NodeGroupAffinity.fromPartial(object.nodeGroupAffinity)
      : undefined;
    message.shieldedInstanceConfig =
      (object.shieldedInstanceConfig !== undefined && object.shieldedInstanceConfig !== null)
        ? ShieldedInstanceConfig.fromPartial(object.shieldedInstanceConfig)
        : undefined;
    message.confidentialInstanceConfig =
      (object.confidentialInstanceConfig !== undefined && object.confidentialInstanceConfig !== null)
        ? ConfidentialInstanceConfig.fromPartial(object.confidentialInstanceConfig)
        : undefined;
    return message;
  },
};

function createBaseGceClusterConfig_MetadataEntry(): GceClusterConfig_MetadataEntry {
  return { key: "", value: "" };
}

export const GceClusterConfig_MetadataEntry: MessageFns<GceClusterConfig_MetadataEntry> = {
  encode(message: GceClusterConfig_MetadataEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GceClusterConfig_MetadataEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGceClusterConfig_MetadataEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GceClusterConfig_MetadataEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: GceClusterConfig_MetadataEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<GceClusterConfig_MetadataEntry>): GceClusterConfig_MetadataEntry {
    return GceClusterConfig_MetadataEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GceClusterConfig_MetadataEntry>): GceClusterConfig_MetadataEntry {
    const message = createBaseGceClusterConfig_MetadataEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseNodeGroupAffinity(): NodeGroupAffinity {
  return { nodeGroupUri: "" };
}

export const NodeGroupAffinity: MessageFns<NodeGroupAffinity> = {
  encode(message: NodeGroupAffinity, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.nodeGroupUri !== "") {
      writer.uint32(10).string(message.nodeGroupUri);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NodeGroupAffinity {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNodeGroupAffinity();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.nodeGroupUri = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NodeGroupAffinity {
    return { nodeGroupUri: isSet(object.nodeGroupUri) ? globalThis.String(object.nodeGroupUri) : "" };
  },

  toJSON(message: NodeGroupAffinity): unknown {
    const obj: any = {};
    if (message.nodeGroupUri !== "") {
      obj.nodeGroupUri = message.nodeGroupUri;
    }
    return obj;
  },

  create(base?: DeepPartial<NodeGroupAffinity>): NodeGroupAffinity {
    return NodeGroupAffinity.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<NodeGroupAffinity>): NodeGroupAffinity {
    const message = createBaseNodeGroupAffinity();
    message.nodeGroupUri = object.nodeGroupUri ?? "";
    return message;
  },
};

function createBaseShieldedInstanceConfig(): ShieldedInstanceConfig {
  return { enableSecureBoot: undefined, enableVtpm: undefined, enableIntegrityMonitoring: undefined };
}

export const ShieldedInstanceConfig: MessageFns<ShieldedInstanceConfig> = {
  encode(message: ShieldedInstanceConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.enableSecureBoot !== undefined) {
      writer.uint32(8).bool(message.enableSecureBoot);
    }
    if (message.enableVtpm !== undefined) {
      writer.uint32(16).bool(message.enableVtpm);
    }
    if (message.enableIntegrityMonitoring !== undefined) {
      writer.uint32(24).bool(message.enableIntegrityMonitoring);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ShieldedInstanceConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseShieldedInstanceConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.enableSecureBoot = reader.bool();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.enableVtpm = reader.bool();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.enableIntegrityMonitoring = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ShieldedInstanceConfig {
    return {
      enableSecureBoot: isSet(object.enableSecureBoot) ? globalThis.Boolean(object.enableSecureBoot) : undefined,
      enableVtpm: isSet(object.enableVtpm) ? globalThis.Boolean(object.enableVtpm) : undefined,
      enableIntegrityMonitoring: isSet(object.enableIntegrityMonitoring)
        ? globalThis.Boolean(object.enableIntegrityMonitoring)
        : undefined,
    };
  },

  toJSON(message: ShieldedInstanceConfig): unknown {
    const obj: any = {};
    if (message.enableSecureBoot !== undefined) {
      obj.enableSecureBoot = message.enableSecureBoot;
    }
    if (message.enableVtpm !== undefined) {
      obj.enableVtpm = message.enableVtpm;
    }
    if (message.enableIntegrityMonitoring !== undefined) {
      obj.enableIntegrityMonitoring = message.enableIntegrityMonitoring;
    }
    return obj;
  },

  create(base?: DeepPartial<ShieldedInstanceConfig>): ShieldedInstanceConfig {
    return ShieldedInstanceConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ShieldedInstanceConfig>): ShieldedInstanceConfig {
    const message = createBaseShieldedInstanceConfig();
    message.enableSecureBoot = object.enableSecureBoot ?? undefined;
    message.enableVtpm = object.enableVtpm ?? undefined;
    message.enableIntegrityMonitoring = object.enableIntegrityMonitoring ?? undefined;
    return message;
  },
};

function createBaseConfidentialInstanceConfig(): ConfidentialInstanceConfig {
  return { enableConfidentialCompute: false };
}

export const ConfidentialInstanceConfig: MessageFns<ConfidentialInstanceConfig> = {
  encode(message: ConfidentialInstanceConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.enableConfidentialCompute !== false) {
      writer.uint32(8).bool(message.enableConfidentialCompute);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ConfidentialInstanceConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseConfidentialInstanceConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.enableConfidentialCompute = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ConfidentialInstanceConfig {
    return {
      enableConfidentialCompute: isSet(object.enableConfidentialCompute)
        ? globalThis.Boolean(object.enableConfidentialCompute)
        : false,
    };
  },

  toJSON(message: ConfidentialInstanceConfig): unknown {
    const obj: any = {};
    if (message.enableConfidentialCompute !== false) {
      obj.enableConfidentialCompute = message.enableConfidentialCompute;
    }
    return obj;
  },

  create(base?: DeepPartial<ConfidentialInstanceConfig>): ConfidentialInstanceConfig {
    return ConfidentialInstanceConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ConfidentialInstanceConfig>): ConfidentialInstanceConfig {
    const message = createBaseConfidentialInstanceConfig();
    message.enableConfidentialCompute = object.enableConfidentialCompute ?? false;
    return message;
  },
};

function createBaseInstanceGroupConfig(): InstanceGroupConfig {
  return {
    numInstances: 0,
    instanceNames: [],
    instanceReferences: [],
    imageUri: "",
    machineTypeUri: "",
    diskConfig: undefined,
    isPreemptible: false,
    preemptibility: 0,
    managedGroupConfig: undefined,
    accelerators: [],
    minCpuPlatform: "",
    minNumInstances: 0,
    instanceFlexibilityPolicy: undefined,
    startupConfig: undefined,
  };
}

export const InstanceGroupConfig: MessageFns<InstanceGroupConfig> = {
  encode(message: InstanceGroupConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.numInstances !== 0) {
      writer.uint32(8).int32(message.numInstances);
    }
    for (const v of message.instanceNames) {
      writer.uint32(18).string(v!);
    }
    for (const v of message.instanceReferences) {
      InstanceReference.encode(v!, writer.uint32(90).fork()).join();
    }
    if (message.imageUri !== "") {
      writer.uint32(26).string(message.imageUri);
    }
    if (message.machineTypeUri !== "") {
      writer.uint32(34).string(message.machineTypeUri);
    }
    if (message.diskConfig !== undefined) {
      DiskConfig.encode(message.diskConfig, writer.uint32(42).fork()).join();
    }
    if (message.isPreemptible !== false) {
      writer.uint32(48).bool(message.isPreemptible);
    }
    if (message.preemptibility !== 0) {
      writer.uint32(80).int32(message.preemptibility);
    }
    if (message.managedGroupConfig !== undefined) {
      ManagedGroupConfig.encode(message.managedGroupConfig, writer.uint32(58).fork()).join();
    }
    for (const v of message.accelerators) {
      AcceleratorConfig.encode(v!, writer.uint32(66).fork()).join();
    }
    if (message.minCpuPlatform !== "") {
      writer.uint32(74).string(message.minCpuPlatform);
    }
    if (message.minNumInstances !== 0) {
      writer.uint32(96).int32(message.minNumInstances);
    }
    if (message.instanceFlexibilityPolicy !== undefined) {
      InstanceFlexibilityPolicy.encode(message.instanceFlexibilityPolicy, writer.uint32(106).fork()).join();
    }
    if (message.startupConfig !== undefined) {
      StartupConfig.encode(message.startupConfig, writer.uint32(114).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstanceGroupConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstanceGroupConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.numInstances = reader.int32();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.instanceNames.push(reader.string());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.instanceReferences.push(InstanceReference.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.imageUri = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.machineTypeUri = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.diskConfig = DiskConfig.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.isPreemptible = reader.bool();
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.preemptibility = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.managedGroupConfig = ManagedGroupConfig.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.accelerators.push(AcceleratorConfig.decode(reader, reader.uint32()));
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.minCpuPlatform = reader.string();
          continue;
        case 12:
          if (tag !== 96) {
            break;
          }

          message.minNumInstances = reader.int32();
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.instanceFlexibilityPolicy = InstanceFlexibilityPolicy.decode(reader, reader.uint32());
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.startupConfig = StartupConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstanceGroupConfig {
    return {
      numInstances: isSet(object.numInstances) ? globalThis.Number(object.numInstances) : 0,
      instanceNames: globalThis.Array.isArray(object?.instanceNames)
        ? object.instanceNames.map((e: any) => globalThis.String(e))
        : [],
      instanceReferences: globalThis.Array.isArray(object?.instanceReferences)
        ? object.instanceReferences.map((e: any) => InstanceReference.fromJSON(e))
        : [],
      imageUri: isSet(object.imageUri) ? globalThis.String(object.imageUri) : "",
      machineTypeUri: isSet(object.machineTypeUri) ? globalThis.String(object.machineTypeUri) : "",
      diskConfig: isSet(object.diskConfig) ? DiskConfig.fromJSON(object.diskConfig) : undefined,
      isPreemptible: isSet(object.isPreemptible) ? globalThis.Boolean(object.isPreemptible) : false,
      preemptibility: isSet(object.preemptibility)
        ? instanceGroupConfig_PreemptibilityFromJSON(object.preemptibility)
        : 0,
      managedGroupConfig: isSet(object.managedGroupConfig)
        ? ManagedGroupConfig.fromJSON(object.managedGroupConfig)
        : undefined,
      accelerators: globalThis.Array.isArray(object?.accelerators)
        ? object.accelerators.map((e: any) => AcceleratorConfig.fromJSON(e))
        : [],
      minCpuPlatform: isSet(object.minCpuPlatform) ? globalThis.String(object.minCpuPlatform) : "",
      minNumInstances: isSet(object.minNumInstances) ? globalThis.Number(object.minNumInstances) : 0,
      instanceFlexibilityPolicy: isSet(object.instanceFlexibilityPolicy)
        ? InstanceFlexibilityPolicy.fromJSON(object.instanceFlexibilityPolicy)
        : undefined,
      startupConfig: isSet(object.startupConfig) ? StartupConfig.fromJSON(object.startupConfig) : undefined,
    };
  },

  toJSON(message: InstanceGroupConfig): unknown {
    const obj: any = {};
    if (message.numInstances !== 0) {
      obj.numInstances = Math.round(message.numInstances);
    }
    if (message.instanceNames?.length) {
      obj.instanceNames = message.instanceNames;
    }
    if (message.instanceReferences?.length) {
      obj.instanceReferences = message.instanceReferences.map((e) => InstanceReference.toJSON(e));
    }
    if (message.imageUri !== "") {
      obj.imageUri = message.imageUri;
    }
    if (message.machineTypeUri !== "") {
      obj.machineTypeUri = message.machineTypeUri;
    }
    if (message.diskConfig !== undefined) {
      obj.diskConfig = DiskConfig.toJSON(message.diskConfig);
    }
    if (message.isPreemptible !== false) {
      obj.isPreemptible = message.isPreemptible;
    }
    if (message.preemptibility !== 0) {
      obj.preemptibility = instanceGroupConfig_PreemptibilityToJSON(message.preemptibility);
    }
    if (message.managedGroupConfig !== undefined) {
      obj.managedGroupConfig = ManagedGroupConfig.toJSON(message.managedGroupConfig);
    }
    if (message.accelerators?.length) {
      obj.accelerators = message.accelerators.map((e) => AcceleratorConfig.toJSON(e));
    }
    if (message.minCpuPlatform !== "") {
      obj.minCpuPlatform = message.minCpuPlatform;
    }
    if (message.minNumInstances !== 0) {
      obj.minNumInstances = Math.round(message.minNumInstances);
    }
    if (message.instanceFlexibilityPolicy !== undefined) {
      obj.instanceFlexibilityPolicy = InstanceFlexibilityPolicy.toJSON(message.instanceFlexibilityPolicy);
    }
    if (message.startupConfig !== undefined) {
      obj.startupConfig = StartupConfig.toJSON(message.startupConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<InstanceGroupConfig>): InstanceGroupConfig {
    return InstanceGroupConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstanceGroupConfig>): InstanceGroupConfig {
    const message = createBaseInstanceGroupConfig();
    message.numInstances = object.numInstances ?? 0;
    message.instanceNames = object.instanceNames?.map((e) => e) || [];
    message.instanceReferences = object.instanceReferences?.map((e) => InstanceReference.fromPartial(e)) || [];
    message.imageUri = object.imageUri ?? "";
    message.machineTypeUri = object.machineTypeUri ?? "";
    message.diskConfig = (object.diskConfig !== undefined && object.diskConfig !== null)
      ? DiskConfig.fromPartial(object.diskConfig)
      : undefined;
    message.isPreemptible = object.isPreemptible ?? false;
    message.preemptibility = object.preemptibility ?? 0;
    message.managedGroupConfig = (object.managedGroupConfig !== undefined && object.managedGroupConfig !== null)
      ? ManagedGroupConfig.fromPartial(object.managedGroupConfig)
      : undefined;
    message.accelerators = object.accelerators?.map((e) => AcceleratorConfig.fromPartial(e)) || [];
    message.minCpuPlatform = object.minCpuPlatform ?? "";
    message.minNumInstances = object.minNumInstances ?? 0;
    message.instanceFlexibilityPolicy =
      (object.instanceFlexibilityPolicy !== undefined && object.instanceFlexibilityPolicy !== null)
        ? InstanceFlexibilityPolicy.fromPartial(object.instanceFlexibilityPolicy)
        : undefined;
    message.startupConfig = (object.startupConfig !== undefined && object.startupConfig !== null)
      ? StartupConfig.fromPartial(object.startupConfig)
      : undefined;
    return message;
  },
};

function createBaseStartupConfig(): StartupConfig {
  return { requiredRegistrationFraction: undefined };
}

export const StartupConfig: MessageFns<StartupConfig> = {
  encode(message: StartupConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.requiredRegistrationFraction !== undefined) {
      writer.uint32(9).double(message.requiredRegistrationFraction);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StartupConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStartupConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 9) {
            break;
          }

          message.requiredRegistrationFraction = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StartupConfig {
    return {
      requiredRegistrationFraction: isSet(object.requiredRegistrationFraction)
        ? globalThis.Number(object.requiredRegistrationFraction)
        : undefined,
    };
  },

  toJSON(message: StartupConfig): unknown {
    const obj: any = {};
    if (message.requiredRegistrationFraction !== undefined) {
      obj.requiredRegistrationFraction = message.requiredRegistrationFraction;
    }
    return obj;
  },

  create(base?: DeepPartial<StartupConfig>): StartupConfig {
    return StartupConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StartupConfig>): StartupConfig {
    const message = createBaseStartupConfig();
    message.requiredRegistrationFraction = object.requiredRegistrationFraction ?? undefined;
    return message;
  },
};

function createBaseInstanceReference(): InstanceReference {
  return { instanceName: "", instanceId: "", publicKey: "", publicEciesKey: "" };
}

export const InstanceReference: MessageFns<InstanceReference> = {
  encode(message: InstanceReference, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.instanceName !== "") {
      writer.uint32(10).string(message.instanceName);
    }
    if (message.instanceId !== "") {
      writer.uint32(18).string(message.instanceId);
    }
    if (message.publicKey !== "") {
      writer.uint32(26).string(message.publicKey);
    }
    if (message.publicEciesKey !== "") {
      writer.uint32(34).string(message.publicEciesKey);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstanceReference {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstanceReference();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.instanceName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.instanceId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.publicKey = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.publicEciesKey = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstanceReference {
    return {
      instanceName: isSet(object.instanceName) ? globalThis.String(object.instanceName) : "",
      instanceId: isSet(object.instanceId) ? globalThis.String(object.instanceId) : "",
      publicKey: isSet(object.publicKey) ? globalThis.String(object.publicKey) : "",
      publicEciesKey: isSet(object.publicEciesKey) ? globalThis.String(object.publicEciesKey) : "",
    };
  },

  toJSON(message: InstanceReference): unknown {
    const obj: any = {};
    if (message.instanceName !== "") {
      obj.instanceName = message.instanceName;
    }
    if (message.instanceId !== "") {
      obj.instanceId = message.instanceId;
    }
    if (message.publicKey !== "") {
      obj.publicKey = message.publicKey;
    }
    if (message.publicEciesKey !== "") {
      obj.publicEciesKey = message.publicEciesKey;
    }
    return obj;
  },

  create(base?: DeepPartial<InstanceReference>): InstanceReference {
    return InstanceReference.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstanceReference>): InstanceReference {
    const message = createBaseInstanceReference();
    message.instanceName = object.instanceName ?? "";
    message.instanceId = object.instanceId ?? "";
    message.publicKey = object.publicKey ?? "";
    message.publicEciesKey = object.publicEciesKey ?? "";
    return message;
  },
};

function createBaseManagedGroupConfig(): ManagedGroupConfig {
  return { instanceTemplateName: "", instanceGroupManagerName: "", instanceGroupManagerUri: "" };
}

export const ManagedGroupConfig: MessageFns<ManagedGroupConfig> = {
  encode(message: ManagedGroupConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.instanceTemplateName !== "") {
      writer.uint32(10).string(message.instanceTemplateName);
    }
    if (message.instanceGroupManagerName !== "") {
      writer.uint32(18).string(message.instanceGroupManagerName);
    }
    if (message.instanceGroupManagerUri !== "") {
      writer.uint32(26).string(message.instanceGroupManagerUri);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ManagedGroupConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseManagedGroupConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.instanceTemplateName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.instanceGroupManagerName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.instanceGroupManagerUri = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ManagedGroupConfig {
    return {
      instanceTemplateName: isSet(object.instanceTemplateName) ? globalThis.String(object.instanceTemplateName) : "",
      instanceGroupManagerName: isSet(object.instanceGroupManagerName)
        ? globalThis.String(object.instanceGroupManagerName)
        : "",
      instanceGroupManagerUri: isSet(object.instanceGroupManagerUri)
        ? globalThis.String(object.instanceGroupManagerUri)
        : "",
    };
  },

  toJSON(message: ManagedGroupConfig): unknown {
    const obj: any = {};
    if (message.instanceTemplateName !== "") {
      obj.instanceTemplateName = message.instanceTemplateName;
    }
    if (message.instanceGroupManagerName !== "") {
      obj.instanceGroupManagerName = message.instanceGroupManagerName;
    }
    if (message.instanceGroupManagerUri !== "") {
      obj.instanceGroupManagerUri = message.instanceGroupManagerUri;
    }
    return obj;
  },

  create(base?: DeepPartial<ManagedGroupConfig>): ManagedGroupConfig {
    return ManagedGroupConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ManagedGroupConfig>): ManagedGroupConfig {
    const message = createBaseManagedGroupConfig();
    message.instanceTemplateName = object.instanceTemplateName ?? "";
    message.instanceGroupManagerName = object.instanceGroupManagerName ?? "";
    message.instanceGroupManagerUri = object.instanceGroupManagerUri ?? "";
    return message;
  },
};

function createBaseInstanceFlexibilityPolicy(): InstanceFlexibilityPolicy {
  return { instanceSelectionList: [], instanceSelectionResults: [] };
}

export const InstanceFlexibilityPolicy: MessageFns<InstanceFlexibilityPolicy> = {
  encode(message: InstanceFlexibilityPolicy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.instanceSelectionList) {
      InstanceFlexibilityPolicy_InstanceSelection.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.instanceSelectionResults) {
      InstanceFlexibilityPolicy_InstanceSelectionResult.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstanceFlexibilityPolicy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstanceFlexibilityPolicy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.instanceSelectionList.push(
            InstanceFlexibilityPolicy_InstanceSelection.decode(reader, reader.uint32()),
          );
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.instanceSelectionResults.push(
            InstanceFlexibilityPolicy_InstanceSelectionResult.decode(reader, reader.uint32()),
          );
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstanceFlexibilityPolicy {
    return {
      instanceSelectionList: globalThis.Array.isArray(object?.instanceSelectionList)
        ? object.instanceSelectionList.map((e: any) => InstanceFlexibilityPolicy_InstanceSelection.fromJSON(e))
        : [],
      instanceSelectionResults: globalThis.Array.isArray(object?.instanceSelectionResults)
        ? object.instanceSelectionResults.map((e: any) => InstanceFlexibilityPolicy_InstanceSelectionResult.fromJSON(e))
        : [],
    };
  },

  toJSON(message: InstanceFlexibilityPolicy): unknown {
    const obj: any = {};
    if (message.instanceSelectionList?.length) {
      obj.instanceSelectionList = message.instanceSelectionList.map((e) =>
        InstanceFlexibilityPolicy_InstanceSelection.toJSON(e)
      );
    }
    if (message.instanceSelectionResults?.length) {
      obj.instanceSelectionResults = message.instanceSelectionResults.map((e) =>
        InstanceFlexibilityPolicy_InstanceSelectionResult.toJSON(e)
      );
    }
    return obj;
  },

  create(base?: DeepPartial<InstanceFlexibilityPolicy>): InstanceFlexibilityPolicy {
    return InstanceFlexibilityPolicy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InstanceFlexibilityPolicy>): InstanceFlexibilityPolicy {
    const message = createBaseInstanceFlexibilityPolicy();
    message.instanceSelectionList =
      object.instanceSelectionList?.map((e) => InstanceFlexibilityPolicy_InstanceSelection.fromPartial(e)) || [];
    message.instanceSelectionResults =
      object.instanceSelectionResults?.map((e) => InstanceFlexibilityPolicy_InstanceSelectionResult.fromPartial(e)) ||
      [];
    return message;
  },
};

function createBaseInstanceFlexibilityPolicy_InstanceSelection(): InstanceFlexibilityPolicy_InstanceSelection {
  return { machineTypes: [], rank: 0 };
}

export const InstanceFlexibilityPolicy_InstanceSelection: MessageFns<InstanceFlexibilityPolicy_InstanceSelection> = {
  encode(
    message: InstanceFlexibilityPolicy_InstanceSelection,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    for (const v of message.machineTypes) {
      writer.uint32(10).string(v!);
    }
    if (message.rank !== 0) {
      writer.uint32(16).int32(message.rank);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstanceFlexibilityPolicy_InstanceSelection {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstanceFlexibilityPolicy_InstanceSelection();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.machineTypes.push(reader.string());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.rank = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstanceFlexibilityPolicy_InstanceSelection {
    return {
      machineTypes: globalThis.Array.isArray(object?.machineTypes)
        ? object.machineTypes.map((e: any) => globalThis.String(e))
        : [],
      rank: isSet(object.rank) ? globalThis.Number(object.rank) : 0,
    };
  },

  toJSON(message: InstanceFlexibilityPolicy_InstanceSelection): unknown {
    const obj: any = {};
    if (message.machineTypes?.length) {
      obj.machineTypes = message.machineTypes;
    }
    if (message.rank !== 0) {
      obj.rank = Math.round(message.rank);
    }
    return obj;
  },

  create(base?: DeepPartial<InstanceFlexibilityPolicy_InstanceSelection>): InstanceFlexibilityPolicy_InstanceSelection {
    return InstanceFlexibilityPolicy_InstanceSelection.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<InstanceFlexibilityPolicy_InstanceSelection>,
  ): InstanceFlexibilityPolicy_InstanceSelection {
    const message = createBaseInstanceFlexibilityPolicy_InstanceSelection();
    message.machineTypes = object.machineTypes?.map((e) => e) || [];
    message.rank = object.rank ?? 0;
    return message;
  },
};

function createBaseInstanceFlexibilityPolicy_InstanceSelectionResult(): InstanceFlexibilityPolicy_InstanceSelectionResult {
  return { machineType: undefined, vmCount: undefined };
}

export const InstanceFlexibilityPolicy_InstanceSelectionResult: MessageFns<
  InstanceFlexibilityPolicy_InstanceSelectionResult
> = {
  encode(
    message: InstanceFlexibilityPolicy_InstanceSelectionResult,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.machineType !== undefined) {
      writer.uint32(10).string(message.machineType);
    }
    if (message.vmCount !== undefined) {
      writer.uint32(16).int32(message.vmCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InstanceFlexibilityPolicy_InstanceSelectionResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInstanceFlexibilityPolicy_InstanceSelectionResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.machineType = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.vmCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InstanceFlexibilityPolicy_InstanceSelectionResult {
    return {
      machineType: isSet(object.machineType) ? globalThis.String(object.machineType) : undefined,
      vmCount: isSet(object.vmCount) ? globalThis.Number(object.vmCount) : undefined,
    };
  },

  toJSON(message: InstanceFlexibilityPolicy_InstanceSelectionResult): unknown {
    const obj: any = {};
    if (message.machineType !== undefined) {
      obj.machineType = message.machineType;
    }
    if (message.vmCount !== undefined) {
      obj.vmCount = Math.round(message.vmCount);
    }
    return obj;
  },

  create(
    base?: DeepPartial<InstanceFlexibilityPolicy_InstanceSelectionResult>,
  ): InstanceFlexibilityPolicy_InstanceSelectionResult {
    return InstanceFlexibilityPolicy_InstanceSelectionResult.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<InstanceFlexibilityPolicy_InstanceSelectionResult>,
  ): InstanceFlexibilityPolicy_InstanceSelectionResult {
    const message = createBaseInstanceFlexibilityPolicy_InstanceSelectionResult();
    message.machineType = object.machineType ?? undefined;
    message.vmCount = object.vmCount ?? undefined;
    return message;
  },
};

function createBaseAcceleratorConfig(): AcceleratorConfig {
  return { acceleratorTypeUri: "", acceleratorCount: 0 };
}

export const AcceleratorConfig: MessageFns<AcceleratorConfig> = {
  encode(message: AcceleratorConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.acceleratorTypeUri !== "") {
      writer.uint32(10).string(message.acceleratorTypeUri);
    }
    if (message.acceleratorCount !== 0) {
      writer.uint32(16).int32(message.acceleratorCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AcceleratorConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAcceleratorConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.acceleratorTypeUri = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.acceleratorCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AcceleratorConfig {
    return {
      acceleratorTypeUri: isSet(object.acceleratorTypeUri) ? globalThis.String(object.acceleratorTypeUri) : "",
      acceleratorCount: isSet(object.acceleratorCount) ? globalThis.Number(object.acceleratorCount) : 0,
    };
  },

  toJSON(message: AcceleratorConfig): unknown {
    const obj: any = {};
    if (message.acceleratorTypeUri !== "") {
      obj.acceleratorTypeUri = message.acceleratorTypeUri;
    }
    if (message.acceleratorCount !== 0) {
      obj.acceleratorCount = Math.round(message.acceleratorCount);
    }
    return obj;
  },

  create(base?: DeepPartial<AcceleratorConfig>): AcceleratorConfig {
    return AcceleratorConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AcceleratorConfig>): AcceleratorConfig {
    const message = createBaseAcceleratorConfig();
    message.acceleratorTypeUri = object.acceleratorTypeUri ?? "";
    message.acceleratorCount = object.acceleratorCount ?? 0;
    return message;
  },
};

function createBaseDiskConfig(): DiskConfig {
  return { bootDiskType: "", bootDiskSizeGb: 0, numLocalSsds: 0, localSsdInterface: "" };
}

export const DiskConfig: MessageFns<DiskConfig> = {
  encode(message: DiskConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.bootDiskType !== "") {
      writer.uint32(26).string(message.bootDiskType);
    }
    if (message.bootDiskSizeGb !== 0) {
      writer.uint32(8).int32(message.bootDiskSizeGb);
    }
    if (message.numLocalSsds !== 0) {
      writer.uint32(16).int32(message.numLocalSsds);
    }
    if (message.localSsdInterface !== "") {
      writer.uint32(34).string(message.localSsdInterface);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DiskConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDiskConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 26) {
            break;
          }

          message.bootDiskType = reader.string();
          continue;
        case 1:
          if (tag !== 8) {
            break;
          }

          message.bootDiskSizeGb = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.numLocalSsds = reader.int32();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.localSsdInterface = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DiskConfig {
    return {
      bootDiskType: isSet(object.bootDiskType) ? globalThis.String(object.bootDiskType) : "",
      bootDiskSizeGb: isSet(object.bootDiskSizeGb) ? globalThis.Number(object.bootDiskSizeGb) : 0,
      numLocalSsds: isSet(object.numLocalSsds) ? globalThis.Number(object.numLocalSsds) : 0,
      localSsdInterface: isSet(object.localSsdInterface) ? globalThis.String(object.localSsdInterface) : "",
    };
  },

  toJSON(message: DiskConfig): unknown {
    const obj: any = {};
    if (message.bootDiskType !== "") {
      obj.bootDiskType = message.bootDiskType;
    }
    if (message.bootDiskSizeGb !== 0) {
      obj.bootDiskSizeGb = Math.round(message.bootDiskSizeGb);
    }
    if (message.numLocalSsds !== 0) {
      obj.numLocalSsds = Math.round(message.numLocalSsds);
    }
    if (message.localSsdInterface !== "") {
      obj.localSsdInterface = message.localSsdInterface;
    }
    return obj;
  },

  create(base?: DeepPartial<DiskConfig>): DiskConfig {
    return DiskConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DiskConfig>): DiskConfig {
    const message = createBaseDiskConfig();
    message.bootDiskType = object.bootDiskType ?? "";
    message.bootDiskSizeGb = object.bootDiskSizeGb ?? 0;
    message.numLocalSsds = object.numLocalSsds ?? 0;
    message.localSsdInterface = object.localSsdInterface ?? "";
    return message;
  },
};

function createBaseAuxiliaryNodeGroup(): AuxiliaryNodeGroup {
  return { nodeGroup: undefined, nodeGroupId: "" };
}

export const AuxiliaryNodeGroup: MessageFns<AuxiliaryNodeGroup> = {
  encode(message: AuxiliaryNodeGroup, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.nodeGroup !== undefined) {
      NodeGroup.encode(message.nodeGroup, writer.uint32(10).fork()).join();
    }
    if (message.nodeGroupId !== "") {
      writer.uint32(18).string(message.nodeGroupId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AuxiliaryNodeGroup {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAuxiliaryNodeGroup();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.nodeGroup = NodeGroup.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nodeGroupId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AuxiliaryNodeGroup {
    return {
      nodeGroup: isSet(object.nodeGroup) ? NodeGroup.fromJSON(object.nodeGroup) : undefined,
      nodeGroupId: isSet(object.nodeGroupId) ? globalThis.String(object.nodeGroupId) : "",
    };
  },

  toJSON(message: AuxiliaryNodeGroup): unknown {
    const obj: any = {};
    if (message.nodeGroup !== undefined) {
      obj.nodeGroup = NodeGroup.toJSON(message.nodeGroup);
    }
    if (message.nodeGroupId !== "") {
      obj.nodeGroupId = message.nodeGroupId;
    }
    return obj;
  },

  create(base?: DeepPartial<AuxiliaryNodeGroup>): AuxiliaryNodeGroup {
    return AuxiliaryNodeGroup.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AuxiliaryNodeGroup>): AuxiliaryNodeGroup {
    const message = createBaseAuxiliaryNodeGroup();
    message.nodeGroup = (object.nodeGroup !== undefined && object.nodeGroup !== null)
      ? NodeGroup.fromPartial(object.nodeGroup)
      : undefined;
    message.nodeGroupId = object.nodeGroupId ?? "";
    return message;
  },
};

function createBaseNodeGroup(): NodeGroup {
  return { name: "", roles: [], nodeGroupConfig: undefined, labels: {} };
}

export const NodeGroup: MessageFns<NodeGroup> = {
  encode(message: NodeGroup, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    writer.uint32(18).fork();
    for (const v of message.roles) {
      writer.int32(v);
    }
    writer.join();
    if (message.nodeGroupConfig !== undefined) {
      InstanceGroupConfig.encode(message.nodeGroupConfig, writer.uint32(26).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      NodeGroup_LabelsEntry.encode({ key: key as any, value }, writer.uint32(34).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NodeGroup {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNodeGroup();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag === 16) {
            message.roles.push(reader.int32() as any);

            continue;
          }

          if (tag === 18) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.roles.push(reader.int32() as any);
            }

            continue;
          }

          break;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.nodeGroupConfig = InstanceGroupConfig.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          const entry4 = NodeGroup_LabelsEntry.decode(reader, reader.uint32());
          if (entry4.value !== undefined) {
            message.labels[entry4.key] = entry4.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NodeGroup {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      roles: globalThis.Array.isArray(object?.roles) ? object.roles.map((e: any) => nodeGroup_RoleFromJSON(e)) : [],
      nodeGroupConfig: isSet(object.nodeGroupConfig) ? InstanceGroupConfig.fromJSON(object.nodeGroupConfig) : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: NodeGroup): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.roles?.length) {
      obj.roles = message.roles.map((e) => nodeGroup_RoleToJSON(e));
    }
    if (message.nodeGroupConfig !== undefined) {
      obj.nodeGroupConfig = InstanceGroupConfig.toJSON(message.nodeGroupConfig);
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<NodeGroup>): NodeGroup {
    return NodeGroup.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<NodeGroup>): NodeGroup {
    const message = createBaseNodeGroup();
    message.name = object.name ?? "";
    message.roles = object.roles?.map((e) => e) || [];
    message.nodeGroupConfig = (object.nodeGroupConfig !== undefined && object.nodeGroupConfig !== null)
      ? InstanceGroupConfig.fromPartial(object.nodeGroupConfig)
      : undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseNodeGroup_LabelsEntry(): NodeGroup_LabelsEntry {
  return { key: "", value: "" };
}

export const NodeGroup_LabelsEntry: MessageFns<NodeGroup_LabelsEntry> = {
  encode(message: NodeGroup_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NodeGroup_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNodeGroup_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NodeGroup_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: NodeGroup_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<NodeGroup_LabelsEntry>): NodeGroup_LabelsEntry {
    return NodeGroup_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<NodeGroup_LabelsEntry>): NodeGroup_LabelsEntry {
    const message = createBaseNodeGroup_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseNodeInitializationAction(): NodeInitializationAction {
  return { executableFile: "", executionTimeout: undefined };
}

export const NodeInitializationAction: MessageFns<NodeInitializationAction> = {
  encode(message: NodeInitializationAction, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.executableFile !== "") {
      writer.uint32(10).string(message.executableFile);
    }
    if (message.executionTimeout !== undefined) {
      Duration.encode(message.executionTimeout, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NodeInitializationAction {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNodeInitializationAction();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.executableFile = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.executionTimeout = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NodeInitializationAction {
    return {
      executableFile: isSet(object.executableFile) ? globalThis.String(object.executableFile) : "",
      executionTimeout: isSet(object.executionTimeout) ? Duration.fromJSON(object.executionTimeout) : undefined,
    };
  },

  toJSON(message: NodeInitializationAction): unknown {
    const obj: any = {};
    if (message.executableFile !== "") {
      obj.executableFile = message.executableFile;
    }
    if (message.executionTimeout !== undefined) {
      obj.executionTimeout = Duration.toJSON(message.executionTimeout);
    }
    return obj;
  },

  create(base?: DeepPartial<NodeInitializationAction>): NodeInitializationAction {
    return NodeInitializationAction.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<NodeInitializationAction>): NodeInitializationAction {
    const message = createBaseNodeInitializationAction();
    message.executableFile = object.executableFile ?? "";
    message.executionTimeout = (object.executionTimeout !== undefined && object.executionTimeout !== null)
      ? Duration.fromPartial(object.executionTimeout)
      : undefined;
    return message;
  },
};

function createBaseClusterStatus(): ClusterStatus {
  return { state: 0, detail: "", stateStartTime: undefined, substate: 0 };
}

export const ClusterStatus: MessageFns<ClusterStatus> = {
  encode(message: ClusterStatus, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.state !== 0) {
      writer.uint32(8).int32(message.state);
    }
    if (message.detail !== "") {
      writer.uint32(18).string(message.detail);
    }
    if (message.stateStartTime !== undefined) {
      Timestamp.encode(toTimestamp(message.stateStartTime), writer.uint32(26).fork()).join();
    }
    if (message.substate !== 0) {
      writer.uint32(32).int32(message.substate);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterStatus {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterStatus();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.detail = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.stateStartTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.substate = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterStatus {
    return {
      state: isSet(object.state) ? clusterStatus_StateFromJSON(object.state) : 0,
      detail: isSet(object.detail) ? globalThis.String(object.detail) : "",
      stateStartTime: isSet(object.stateStartTime) ? fromJsonTimestamp(object.stateStartTime) : undefined,
      substate: isSet(object.substate) ? clusterStatus_SubstateFromJSON(object.substate) : 0,
    };
  },

  toJSON(message: ClusterStatus): unknown {
    const obj: any = {};
    if (message.state !== 0) {
      obj.state = clusterStatus_StateToJSON(message.state);
    }
    if (message.detail !== "") {
      obj.detail = message.detail;
    }
    if (message.stateStartTime !== undefined) {
      obj.stateStartTime = message.stateStartTime.toISOString();
    }
    if (message.substate !== 0) {
      obj.substate = clusterStatus_SubstateToJSON(message.substate);
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterStatus>): ClusterStatus {
    return ClusterStatus.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterStatus>): ClusterStatus {
    const message = createBaseClusterStatus();
    message.state = object.state ?? 0;
    message.detail = object.detail ?? "";
    message.stateStartTime = object.stateStartTime ?? undefined;
    message.substate = object.substate ?? 0;
    return message;
  },
};

function createBaseSecurityConfig(): SecurityConfig {
  return { kerberosConfig: undefined, identityConfig: undefined };
}

export const SecurityConfig: MessageFns<SecurityConfig> = {
  encode(message: SecurityConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.kerberosConfig !== undefined) {
      KerberosConfig.encode(message.kerberosConfig, writer.uint32(10).fork()).join();
    }
    if (message.identityConfig !== undefined) {
      IdentityConfig.encode(message.identityConfig, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SecurityConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSecurityConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.kerberosConfig = KerberosConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.identityConfig = IdentityConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SecurityConfig {
    return {
      kerberosConfig: isSet(object.kerberosConfig) ? KerberosConfig.fromJSON(object.kerberosConfig) : undefined,
      identityConfig: isSet(object.identityConfig) ? IdentityConfig.fromJSON(object.identityConfig) : undefined,
    };
  },

  toJSON(message: SecurityConfig): unknown {
    const obj: any = {};
    if (message.kerberosConfig !== undefined) {
      obj.kerberosConfig = KerberosConfig.toJSON(message.kerberosConfig);
    }
    if (message.identityConfig !== undefined) {
      obj.identityConfig = IdentityConfig.toJSON(message.identityConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<SecurityConfig>): SecurityConfig {
    return SecurityConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SecurityConfig>): SecurityConfig {
    const message = createBaseSecurityConfig();
    message.kerberosConfig = (object.kerberosConfig !== undefined && object.kerberosConfig !== null)
      ? KerberosConfig.fromPartial(object.kerberosConfig)
      : undefined;
    message.identityConfig = (object.identityConfig !== undefined && object.identityConfig !== null)
      ? IdentityConfig.fromPartial(object.identityConfig)
      : undefined;
    return message;
  },
};

function createBaseKerberosConfig(): KerberosConfig {
  return {
    enableKerberos: false,
    rootPrincipalPasswordUri: "",
    kmsKeyUri: "",
    keystoreUri: "",
    truststoreUri: "",
    keystorePasswordUri: "",
    keyPasswordUri: "",
    truststorePasswordUri: "",
    crossRealmTrustRealm: "",
    crossRealmTrustKdc: "",
    crossRealmTrustAdminServer: "",
    crossRealmTrustSharedPasswordUri: "",
    kdcDbKeyUri: "",
    tgtLifetimeHours: 0,
    realm: "",
  };
}

export const KerberosConfig: MessageFns<KerberosConfig> = {
  encode(message: KerberosConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.enableKerberos !== false) {
      writer.uint32(8).bool(message.enableKerberos);
    }
    if (message.rootPrincipalPasswordUri !== "") {
      writer.uint32(18).string(message.rootPrincipalPasswordUri);
    }
    if (message.kmsKeyUri !== "") {
      writer.uint32(26).string(message.kmsKeyUri);
    }
    if (message.keystoreUri !== "") {
      writer.uint32(34).string(message.keystoreUri);
    }
    if (message.truststoreUri !== "") {
      writer.uint32(42).string(message.truststoreUri);
    }
    if (message.keystorePasswordUri !== "") {
      writer.uint32(50).string(message.keystorePasswordUri);
    }
    if (message.keyPasswordUri !== "") {
      writer.uint32(58).string(message.keyPasswordUri);
    }
    if (message.truststorePasswordUri !== "") {
      writer.uint32(66).string(message.truststorePasswordUri);
    }
    if (message.crossRealmTrustRealm !== "") {
      writer.uint32(74).string(message.crossRealmTrustRealm);
    }
    if (message.crossRealmTrustKdc !== "") {
      writer.uint32(82).string(message.crossRealmTrustKdc);
    }
    if (message.crossRealmTrustAdminServer !== "") {
      writer.uint32(90).string(message.crossRealmTrustAdminServer);
    }
    if (message.crossRealmTrustSharedPasswordUri !== "") {
      writer.uint32(98).string(message.crossRealmTrustSharedPasswordUri);
    }
    if (message.kdcDbKeyUri !== "") {
      writer.uint32(106).string(message.kdcDbKeyUri);
    }
    if (message.tgtLifetimeHours !== 0) {
      writer.uint32(112).int32(message.tgtLifetimeHours);
    }
    if (message.realm !== "") {
      writer.uint32(122).string(message.realm);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KerberosConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKerberosConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.enableKerberos = reader.bool();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.rootPrincipalPasswordUri = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.kmsKeyUri = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.keystoreUri = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.truststoreUri = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.keystorePasswordUri = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.keyPasswordUri = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.truststorePasswordUri = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.crossRealmTrustRealm = reader.string();
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.crossRealmTrustKdc = reader.string();
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.crossRealmTrustAdminServer = reader.string();
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.crossRealmTrustSharedPasswordUri = reader.string();
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.kdcDbKeyUri = reader.string();
          continue;
        case 14:
          if (tag !== 112) {
            break;
          }

          message.tgtLifetimeHours = reader.int32();
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.realm = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KerberosConfig {
    return {
      enableKerberos: isSet(object.enableKerberos) ? globalThis.Boolean(object.enableKerberos) : false,
      rootPrincipalPasswordUri: isSet(object.rootPrincipalPasswordUri)
        ? globalThis.String(object.rootPrincipalPasswordUri)
        : "",
      kmsKeyUri: isSet(object.kmsKeyUri) ? globalThis.String(object.kmsKeyUri) : "",
      keystoreUri: isSet(object.keystoreUri) ? globalThis.String(object.keystoreUri) : "",
      truststoreUri: isSet(object.truststoreUri) ? globalThis.String(object.truststoreUri) : "",
      keystorePasswordUri: isSet(object.keystorePasswordUri) ? globalThis.String(object.keystorePasswordUri) : "",
      keyPasswordUri: isSet(object.keyPasswordUri) ? globalThis.String(object.keyPasswordUri) : "",
      truststorePasswordUri: isSet(object.truststorePasswordUri) ? globalThis.String(object.truststorePasswordUri) : "",
      crossRealmTrustRealm: isSet(object.crossRealmTrustRealm) ? globalThis.String(object.crossRealmTrustRealm) : "",
      crossRealmTrustKdc: isSet(object.crossRealmTrustKdc) ? globalThis.String(object.crossRealmTrustKdc) : "",
      crossRealmTrustAdminServer: isSet(object.crossRealmTrustAdminServer)
        ? globalThis.String(object.crossRealmTrustAdminServer)
        : "",
      crossRealmTrustSharedPasswordUri: isSet(object.crossRealmTrustSharedPasswordUri)
        ? globalThis.String(object.crossRealmTrustSharedPasswordUri)
        : "",
      kdcDbKeyUri: isSet(object.kdcDbKeyUri) ? globalThis.String(object.kdcDbKeyUri) : "",
      tgtLifetimeHours: isSet(object.tgtLifetimeHours) ? globalThis.Number(object.tgtLifetimeHours) : 0,
      realm: isSet(object.realm) ? globalThis.String(object.realm) : "",
    };
  },

  toJSON(message: KerberosConfig): unknown {
    const obj: any = {};
    if (message.enableKerberos !== false) {
      obj.enableKerberos = message.enableKerberos;
    }
    if (message.rootPrincipalPasswordUri !== "") {
      obj.rootPrincipalPasswordUri = message.rootPrincipalPasswordUri;
    }
    if (message.kmsKeyUri !== "") {
      obj.kmsKeyUri = message.kmsKeyUri;
    }
    if (message.keystoreUri !== "") {
      obj.keystoreUri = message.keystoreUri;
    }
    if (message.truststoreUri !== "") {
      obj.truststoreUri = message.truststoreUri;
    }
    if (message.keystorePasswordUri !== "") {
      obj.keystorePasswordUri = message.keystorePasswordUri;
    }
    if (message.keyPasswordUri !== "") {
      obj.keyPasswordUri = message.keyPasswordUri;
    }
    if (message.truststorePasswordUri !== "") {
      obj.truststorePasswordUri = message.truststorePasswordUri;
    }
    if (message.crossRealmTrustRealm !== "") {
      obj.crossRealmTrustRealm = message.crossRealmTrustRealm;
    }
    if (message.crossRealmTrustKdc !== "") {
      obj.crossRealmTrustKdc = message.crossRealmTrustKdc;
    }
    if (message.crossRealmTrustAdminServer !== "") {
      obj.crossRealmTrustAdminServer = message.crossRealmTrustAdminServer;
    }
    if (message.crossRealmTrustSharedPasswordUri !== "") {
      obj.crossRealmTrustSharedPasswordUri = message.crossRealmTrustSharedPasswordUri;
    }
    if (message.kdcDbKeyUri !== "") {
      obj.kdcDbKeyUri = message.kdcDbKeyUri;
    }
    if (message.tgtLifetimeHours !== 0) {
      obj.tgtLifetimeHours = Math.round(message.tgtLifetimeHours);
    }
    if (message.realm !== "") {
      obj.realm = message.realm;
    }
    return obj;
  },

  create(base?: DeepPartial<KerberosConfig>): KerberosConfig {
    return KerberosConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<KerberosConfig>): KerberosConfig {
    const message = createBaseKerberosConfig();
    message.enableKerberos = object.enableKerberos ?? false;
    message.rootPrincipalPasswordUri = object.rootPrincipalPasswordUri ?? "";
    message.kmsKeyUri = object.kmsKeyUri ?? "";
    message.keystoreUri = object.keystoreUri ?? "";
    message.truststoreUri = object.truststoreUri ?? "";
    message.keystorePasswordUri = object.keystorePasswordUri ?? "";
    message.keyPasswordUri = object.keyPasswordUri ?? "";
    message.truststorePasswordUri = object.truststorePasswordUri ?? "";
    message.crossRealmTrustRealm = object.crossRealmTrustRealm ?? "";
    message.crossRealmTrustKdc = object.crossRealmTrustKdc ?? "";
    message.crossRealmTrustAdminServer = object.crossRealmTrustAdminServer ?? "";
    message.crossRealmTrustSharedPasswordUri = object.crossRealmTrustSharedPasswordUri ?? "";
    message.kdcDbKeyUri = object.kdcDbKeyUri ?? "";
    message.tgtLifetimeHours = object.tgtLifetimeHours ?? 0;
    message.realm = object.realm ?? "";
    return message;
  },
};

function createBaseIdentityConfig(): IdentityConfig {
  return { userServiceAccountMapping: {} };
}

export const IdentityConfig: MessageFns<IdentityConfig> = {
  encode(message: IdentityConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.userServiceAccountMapping).forEach(([key, value]) => {
      IdentityConfig_UserServiceAccountMappingEntry.encode({ key: key as any, value }, writer.uint32(10).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): IdentityConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseIdentityConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          const entry1 = IdentityConfig_UserServiceAccountMappingEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.userServiceAccountMapping[entry1.key] = entry1.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): IdentityConfig {
    return {
      userServiceAccountMapping: isObject(object.userServiceAccountMapping)
        ? Object.entries(object.userServiceAccountMapping).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: IdentityConfig): unknown {
    const obj: any = {};
    if (message.userServiceAccountMapping) {
      const entries = Object.entries(message.userServiceAccountMapping);
      if (entries.length > 0) {
        obj.userServiceAccountMapping = {};
        entries.forEach(([k, v]) => {
          obj.userServiceAccountMapping[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<IdentityConfig>): IdentityConfig {
    return IdentityConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<IdentityConfig>): IdentityConfig {
    const message = createBaseIdentityConfig();
    message.userServiceAccountMapping = Object.entries(object.userServiceAccountMapping ?? {}).reduce<
      { [key: string]: string }
    >((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseIdentityConfig_UserServiceAccountMappingEntry(): IdentityConfig_UserServiceAccountMappingEntry {
  return { key: "", value: "" };
}

export const IdentityConfig_UserServiceAccountMappingEntry: MessageFns<IdentityConfig_UserServiceAccountMappingEntry> =
  {
    encode(
      message: IdentityConfig_UserServiceAccountMappingEntry,
      writer: BinaryWriter = new BinaryWriter(),
    ): BinaryWriter {
      if (message.key !== "") {
        writer.uint32(10).string(message.key);
      }
      if (message.value !== "") {
        writer.uint32(18).string(message.value);
      }
      return writer;
    },

    decode(input: BinaryReader | Uint8Array, length?: number): IdentityConfig_UserServiceAccountMappingEntry {
      const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
      let end = length === undefined ? reader.len : reader.pos + length;
      const message = createBaseIdentityConfig_UserServiceAccountMappingEntry();
      while (reader.pos < end) {
        const tag = reader.uint32();
        switch (tag >>> 3) {
          case 1:
            if (tag !== 10) {
              break;
            }

            message.key = reader.string();
            continue;
          case 2:
            if (tag !== 18) {
              break;
            }

            message.value = reader.string();
            continue;
        }
        if ((tag & 7) === 4 || tag === 0) {
          break;
        }
        reader.skip(tag & 7);
      }
      return message;
    },

    fromJSON(object: any): IdentityConfig_UserServiceAccountMappingEntry {
      return {
        key: isSet(object.key) ? globalThis.String(object.key) : "",
        value: isSet(object.value) ? globalThis.String(object.value) : "",
      };
    },

    toJSON(message: IdentityConfig_UserServiceAccountMappingEntry): unknown {
      const obj: any = {};
      if (message.key !== "") {
        obj.key = message.key;
      }
      if (message.value !== "") {
        obj.value = message.value;
      }
      return obj;
    },

    create(
      base?: DeepPartial<IdentityConfig_UserServiceAccountMappingEntry>,
    ): IdentityConfig_UserServiceAccountMappingEntry {
      return IdentityConfig_UserServiceAccountMappingEntry.fromPartial(base ?? {});
    },
    fromPartial(
      object: DeepPartial<IdentityConfig_UserServiceAccountMappingEntry>,
    ): IdentityConfig_UserServiceAccountMappingEntry {
      const message = createBaseIdentityConfig_UserServiceAccountMappingEntry();
      message.key = object.key ?? "";
      message.value = object.value ?? "";
      return message;
    },
  };

function createBaseSoftwareConfig(): SoftwareConfig {
  return { imageVersion: "", properties: {}, optionalComponents: [] };
}

export const SoftwareConfig: MessageFns<SoftwareConfig> = {
  encode(message: SoftwareConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.imageVersion !== "") {
      writer.uint32(10).string(message.imageVersion);
    }
    Object.entries(message.properties).forEach(([key, value]) => {
      SoftwareConfig_PropertiesEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    writer.uint32(26).fork();
    for (const v of message.optionalComponents) {
      writer.int32(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SoftwareConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSoftwareConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.imageVersion = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = SoftwareConfig_PropertiesEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.properties[entry2.key] = entry2.value;
          }
          continue;
        case 3:
          if (tag === 24) {
            message.optionalComponents.push(reader.int32() as any);

            continue;
          }

          if (tag === 26) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.optionalComponents.push(reader.int32() as any);
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SoftwareConfig {
    return {
      imageVersion: isSet(object.imageVersion) ? globalThis.String(object.imageVersion) : "",
      properties: isObject(object.properties)
        ? Object.entries(object.properties).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      optionalComponents: globalThis.Array.isArray(object?.optionalComponents)
        ? object.optionalComponents.map((e: any) => componentFromJSON(e))
        : [],
    };
  },

  toJSON(message: SoftwareConfig): unknown {
    const obj: any = {};
    if (message.imageVersion !== "") {
      obj.imageVersion = message.imageVersion;
    }
    if (message.properties) {
      const entries = Object.entries(message.properties);
      if (entries.length > 0) {
        obj.properties = {};
        entries.forEach(([k, v]) => {
          obj.properties[k] = v;
        });
      }
    }
    if (message.optionalComponents?.length) {
      obj.optionalComponents = message.optionalComponents.map((e) => componentToJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<SoftwareConfig>): SoftwareConfig {
    return SoftwareConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SoftwareConfig>): SoftwareConfig {
    const message = createBaseSoftwareConfig();
    message.imageVersion = object.imageVersion ?? "";
    message.properties = Object.entries(object.properties ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.optionalComponents = object.optionalComponents?.map((e) => e) || [];
    return message;
  },
};

function createBaseSoftwareConfig_PropertiesEntry(): SoftwareConfig_PropertiesEntry {
  return { key: "", value: "" };
}

export const SoftwareConfig_PropertiesEntry: MessageFns<SoftwareConfig_PropertiesEntry> = {
  encode(message: SoftwareConfig_PropertiesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SoftwareConfig_PropertiesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSoftwareConfig_PropertiesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SoftwareConfig_PropertiesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: SoftwareConfig_PropertiesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<SoftwareConfig_PropertiesEntry>): SoftwareConfig_PropertiesEntry {
    return SoftwareConfig_PropertiesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SoftwareConfig_PropertiesEntry>): SoftwareConfig_PropertiesEntry {
    const message = createBaseSoftwareConfig_PropertiesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseLifecycleConfig(): LifecycleConfig {
  return { idleDeleteTtl: undefined, autoDeleteTime: undefined, autoDeleteTtl: undefined, idleStartTime: undefined };
}

export const LifecycleConfig: MessageFns<LifecycleConfig> = {
  encode(message: LifecycleConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.idleDeleteTtl !== undefined) {
      Duration.encode(message.idleDeleteTtl, writer.uint32(10).fork()).join();
    }
    if (message.autoDeleteTime !== undefined) {
      Timestamp.encode(toTimestamp(message.autoDeleteTime), writer.uint32(18).fork()).join();
    }
    if (message.autoDeleteTtl !== undefined) {
      Duration.encode(message.autoDeleteTtl, writer.uint32(26).fork()).join();
    }
    if (message.idleStartTime !== undefined) {
      Timestamp.encode(toTimestamp(message.idleStartTime), writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LifecycleConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLifecycleConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.idleDeleteTtl = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.autoDeleteTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.autoDeleteTtl = Duration.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.idleStartTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LifecycleConfig {
    return {
      idleDeleteTtl: isSet(object.idleDeleteTtl) ? Duration.fromJSON(object.idleDeleteTtl) : undefined,
      autoDeleteTime: isSet(object.autoDeleteTime) ? fromJsonTimestamp(object.autoDeleteTime) : undefined,
      autoDeleteTtl: isSet(object.autoDeleteTtl) ? Duration.fromJSON(object.autoDeleteTtl) : undefined,
      idleStartTime: isSet(object.idleStartTime) ? fromJsonTimestamp(object.idleStartTime) : undefined,
    };
  },

  toJSON(message: LifecycleConfig): unknown {
    const obj: any = {};
    if (message.idleDeleteTtl !== undefined) {
      obj.idleDeleteTtl = Duration.toJSON(message.idleDeleteTtl);
    }
    if (message.autoDeleteTime !== undefined) {
      obj.autoDeleteTime = message.autoDeleteTime.toISOString();
    }
    if (message.autoDeleteTtl !== undefined) {
      obj.autoDeleteTtl = Duration.toJSON(message.autoDeleteTtl);
    }
    if (message.idleStartTime !== undefined) {
      obj.idleStartTime = message.idleStartTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<LifecycleConfig>): LifecycleConfig {
    return LifecycleConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LifecycleConfig>): LifecycleConfig {
    const message = createBaseLifecycleConfig();
    message.idleDeleteTtl = (object.idleDeleteTtl !== undefined && object.idleDeleteTtl !== null)
      ? Duration.fromPartial(object.idleDeleteTtl)
      : undefined;
    message.autoDeleteTime = object.autoDeleteTime ?? undefined;
    message.autoDeleteTtl = (object.autoDeleteTtl !== undefined && object.autoDeleteTtl !== null)
      ? Duration.fromPartial(object.autoDeleteTtl)
      : undefined;
    message.idleStartTime = object.idleStartTime ?? undefined;
    return message;
  },
};

function createBaseMetastoreConfig(): MetastoreConfig {
  return { dataprocMetastoreService: "" };
}

export const MetastoreConfig: MessageFns<MetastoreConfig> = {
  encode(message: MetastoreConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dataprocMetastoreService !== "") {
      writer.uint32(10).string(message.dataprocMetastoreService);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): MetastoreConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMetastoreConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dataprocMetastoreService = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): MetastoreConfig {
    return {
      dataprocMetastoreService: isSet(object.dataprocMetastoreService)
        ? globalThis.String(object.dataprocMetastoreService)
        : "",
    };
  },

  toJSON(message: MetastoreConfig): unknown {
    const obj: any = {};
    if (message.dataprocMetastoreService !== "") {
      obj.dataprocMetastoreService = message.dataprocMetastoreService;
    }
    return obj;
  },

  create(base?: DeepPartial<MetastoreConfig>): MetastoreConfig {
    return MetastoreConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<MetastoreConfig>): MetastoreConfig {
    const message = createBaseMetastoreConfig();
    message.dataprocMetastoreService = object.dataprocMetastoreService ?? "";
    return message;
  },
};

function createBaseClusterMetrics(): ClusterMetrics {
  return { hdfsMetrics: {}, yarnMetrics: {} };
}

export const ClusterMetrics: MessageFns<ClusterMetrics> = {
  encode(message: ClusterMetrics, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.hdfsMetrics).forEach(([key, value]) => {
      ClusterMetrics_HdfsMetricsEntry.encode({ key: key as any, value }, writer.uint32(10).fork()).join();
    });
    Object.entries(message.yarnMetrics).forEach(([key, value]) => {
      ClusterMetrics_YarnMetricsEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterMetrics {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterMetrics();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          const entry1 = ClusterMetrics_HdfsMetricsEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.hdfsMetrics[entry1.key] = entry1.value;
          }
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = ClusterMetrics_YarnMetricsEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.yarnMetrics[entry2.key] = entry2.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterMetrics {
    return {
      hdfsMetrics: isObject(object.hdfsMetrics)
        ? Object.entries(object.hdfsMetrics).reduce<{ [key: string]: Long }>((acc, [key, value]) => {
          acc[key] = Long.fromValue(value as Long | string);
          return acc;
        }, {})
        : {},
      yarnMetrics: isObject(object.yarnMetrics)
        ? Object.entries(object.yarnMetrics).reduce<{ [key: string]: Long }>((acc, [key, value]) => {
          acc[key] = Long.fromValue(value as Long | string);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: ClusterMetrics): unknown {
    const obj: any = {};
    if (message.hdfsMetrics) {
      const entries = Object.entries(message.hdfsMetrics);
      if (entries.length > 0) {
        obj.hdfsMetrics = {};
        entries.forEach(([k, v]) => {
          obj.hdfsMetrics[k] = v.toString();
        });
      }
    }
    if (message.yarnMetrics) {
      const entries = Object.entries(message.yarnMetrics);
      if (entries.length > 0) {
        obj.yarnMetrics = {};
        entries.forEach(([k, v]) => {
          obj.yarnMetrics[k] = v.toString();
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterMetrics>): ClusterMetrics {
    return ClusterMetrics.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterMetrics>): ClusterMetrics {
    const message = createBaseClusterMetrics();
    message.hdfsMetrics = Object.entries(object.hdfsMetrics ?? {}).reduce<{ [key: string]: Long }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = Long.fromValue(value);
        }
        return acc;
      },
      {},
    );
    message.yarnMetrics = Object.entries(object.yarnMetrics ?? {}).reduce<{ [key: string]: Long }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = Long.fromValue(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseClusterMetrics_HdfsMetricsEntry(): ClusterMetrics_HdfsMetricsEntry {
  return { key: "", value: Long.ZERO };
}

export const ClusterMetrics_HdfsMetricsEntry: MessageFns<ClusterMetrics_HdfsMetricsEntry> = {
  encode(message: ClusterMetrics_HdfsMetricsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (!message.value.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.value.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterMetrics_HdfsMetricsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterMetrics_HdfsMetricsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.value = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterMetrics_HdfsMetricsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? Long.fromValue(object.value) : Long.ZERO,
    };
  },

  toJSON(message: ClusterMetrics_HdfsMetricsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (!message.value.equals(Long.ZERO)) {
      obj.value = (message.value || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterMetrics_HdfsMetricsEntry>): ClusterMetrics_HdfsMetricsEntry {
    return ClusterMetrics_HdfsMetricsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterMetrics_HdfsMetricsEntry>): ClusterMetrics_HdfsMetricsEntry {
    const message = createBaseClusterMetrics_HdfsMetricsEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null) ? Long.fromValue(object.value) : Long.ZERO;
    return message;
  },
};

function createBaseClusterMetrics_YarnMetricsEntry(): ClusterMetrics_YarnMetricsEntry {
  return { key: "", value: Long.ZERO };
}

export const ClusterMetrics_YarnMetricsEntry: MessageFns<ClusterMetrics_YarnMetricsEntry> = {
  encode(message: ClusterMetrics_YarnMetricsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (!message.value.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.value.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClusterMetrics_YarnMetricsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClusterMetrics_YarnMetricsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.value = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClusterMetrics_YarnMetricsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? Long.fromValue(object.value) : Long.ZERO,
    };
  },

  toJSON(message: ClusterMetrics_YarnMetricsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (!message.value.equals(Long.ZERO)) {
      obj.value = (message.value || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ClusterMetrics_YarnMetricsEntry>): ClusterMetrics_YarnMetricsEntry {
    return ClusterMetrics_YarnMetricsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ClusterMetrics_YarnMetricsEntry>): ClusterMetrics_YarnMetricsEntry {
    const message = createBaseClusterMetrics_YarnMetricsEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null) ? Long.fromValue(object.value) : Long.ZERO;
    return message;
  },
};

function createBaseDataprocMetricConfig(): DataprocMetricConfig {
  return { metrics: [] };
}

export const DataprocMetricConfig: MessageFns<DataprocMetricConfig> = {
  encode(message: DataprocMetricConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.metrics) {
      DataprocMetricConfig_Metric.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DataprocMetricConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDataprocMetricConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.metrics.push(DataprocMetricConfig_Metric.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DataprocMetricConfig {
    return {
      metrics: globalThis.Array.isArray(object?.metrics)
        ? object.metrics.map((e: any) => DataprocMetricConfig_Metric.fromJSON(e))
        : [],
    };
  },

  toJSON(message: DataprocMetricConfig): unknown {
    const obj: any = {};
    if (message.metrics?.length) {
      obj.metrics = message.metrics.map((e) => DataprocMetricConfig_Metric.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<DataprocMetricConfig>): DataprocMetricConfig {
    return DataprocMetricConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DataprocMetricConfig>): DataprocMetricConfig {
    const message = createBaseDataprocMetricConfig();
    message.metrics = object.metrics?.map((e) => DataprocMetricConfig_Metric.fromPartial(e)) || [];
    return message;
  },
};

function createBaseDataprocMetricConfig_Metric(): DataprocMetricConfig_Metric {
  return { metricSource: 0, metricOverrides: [] };
}

export const DataprocMetricConfig_Metric: MessageFns<DataprocMetricConfig_Metric> = {
  encode(message: DataprocMetricConfig_Metric, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.metricSource !== 0) {
      writer.uint32(8).int32(message.metricSource);
    }
    for (const v of message.metricOverrides) {
      writer.uint32(18).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DataprocMetricConfig_Metric {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDataprocMetricConfig_Metric();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.metricSource = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.metricOverrides.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DataprocMetricConfig_Metric {
    return {
      metricSource: isSet(object.metricSource) ? dataprocMetricConfig_MetricSourceFromJSON(object.metricSource) : 0,
      metricOverrides: globalThis.Array.isArray(object?.metricOverrides)
        ? object.metricOverrides.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: DataprocMetricConfig_Metric): unknown {
    const obj: any = {};
    if (message.metricSource !== 0) {
      obj.metricSource = dataprocMetricConfig_MetricSourceToJSON(message.metricSource);
    }
    if (message.metricOverrides?.length) {
      obj.metricOverrides = message.metricOverrides;
    }
    return obj;
  },

  create(base?: DeepPartial<DataprocMetricConfig_Metric>): DataprocMetricConfig_Metric {
    return DataprocMetricConfig_Metric.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DataprocMetricConfig_Metric>): DataprocMetricConfig_Metric {
    const message = createBaseDataprocMetricConfig_Metric();
    message.metricSource = object.metricSource ?? 0;
    message.metricOverrides = object.metricOverrides?.map((e) => e) || [];
    return message;
  },
};

function createBaseCreateClusterRequest(): CreateClusterRequest {
  return { projectId: "", region: "", cluster: undefined, requestId: "", actionOnFailedPrimaryWorkers: 0 };
}

export const CreateClusterRequest: MessageFns<CreateClusterRequest> = {
  encode(message: CreateClusterRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.region !== "") {
      writer.uint32(26).string(message.region);
    }
    if (message.cluster !== undefined) {
      Cluster.encode(message.cluster, writer.uint32(18).fork()).join();
    }
    if (message.requestId !== "") {
      writer.uint32(34).string(message.requestId);
    }
    if (message.actionOnFailedPrimaryWorkers !== 0) {
      writer.uint32(40).int32(message.actionOnFailedPrimaryWorkers);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateClusterRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateClusterRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.region = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.cluster = Cluster.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.requestId = reader.string();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.actionOnFailedPrimaryWorkers = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateClusterRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      cluster: isSet(object.cluster) ? Cluster.fromJSON(object.cluster) : undefined,
      requestId: isSet(object.requestId) ? globalThis.String(object.requestId) : "",
      actionOnFailedPrimaryWorkers: isSet(object.actionOnFailedPrimaryWorkers)
        ? failureActionFromJSON(object.actionOnFailedPrimaryWorkers)
        : 0,
    };
  },

  toJSON(message: CreateClusterRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.cluster !== undefined) {
      obj.cluster = Cluster.toJSON(message.cluster);
    }
    if (message.requestId !== "") {
      obj.requestId = message.requestId;
    }
    if (message.actionOnFailedPrimaryWorkers !== 0) {
      obj.actionOnFailedPrimaryWorkers = failureActionToJSON(message.actionOnFailedPrimaryWorkers);
    }
    return obj;
  },

  create(base?: DeepPartial<CreateClusterRequest>): CreateClusterRequest {
    return CreateClusterRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateClusterRequest>): CreateClusterRequest {
    const message = createBaseCreateClusterRequest();
    message.projectId = object.projectId ?? "";
    message.region = object.region ?? "";
    message.cluster = (object.cluster !== undefined && object.cluster !== null)
      ? Cluster.fromPartial(object.cluster)
      : undefined;
    message.requestId = object.requestId ?? "";
    message.actionOnFailedPrimaryWorkers = object.actionOnFailedPrimaryWorkers ?? 0;
    return message;
  },
};

function createBaseUpdateClusterRequest(): UpdateClusterRequest {
  return {
    projectId: "",
    region: "",
    clusterName: "",
    cluster: undefined,
    gracefulDecommissionTimeout: undefined,
    updateMask: undefined,
    requestId: "",
  };
}

export const UpdateClusterRequest: MessageFns<UpdateClusterRequest> = {
  encode(message: UpdateClusterRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.region !== "") {
      writer.uint32(42).string(message.region);
    }
    if (message.clusterName !== "") {
      writer.uint32(18).string(message.clusterName);
    }
    if (message.cluster !== undefined) {
      Cluster.encode(message.cluster, writer.uint32(26).fork()).join();
    }
    if (message.gracefulDecommissionTimeout !== undefined) {
      Duration.encode(message.gracefulDecommissionTimeout, writer.uint32(50).fork()).join();
    }
    if (message.updateMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.updateMask), writer.uint32(34).fork()).join();
    }
    if (message.requestId !== "") {
      writer.uint32(58).string(message.requestId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateClusterRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateClusterRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.region = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.clusterName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.cluster = Cluster.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.gracefulDecommissionTimeout = Duration.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.updateMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.requestId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateClusterRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      clusterName: isSet(object.clusterName) ? globalThis.String(object.clusterName) : "",
      cluster: isSet(object.cluster) ? Cluster.fromJSON(object.cluster) : undefined,
      gracefulDecommissionTimeout: isSet(object.gracefulDecommissionTimeout)
        ? Duration.fromJSON(object.gracefulDecommissionTimeout)
        : undefined,
      updateMask: isSet(object.updateMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.updateMask)) : undefined,
      requestId: isSet(object.requestId) ? globalThis.String(object.requestId) : "",
    };
  },

  toJSON(message: UpdateClusterRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.clusterName !== "") {
      obj.clusterName = message.clusterName;
    }
    if (message.cluster !== undefined) {
      obj.cluster = Cluster.toJSON(message.cluster);
    }
    if (message.gracefulDecommissionTimeout !== undefined) {
      obj.gracefulDecommissionTimeout = Duration.toJSON(message.gracefulDecommissionTimeout);
    }
    if (message.updateMask !== undefined) {
      obj.updateMask = FieldMask.toJSON(FieldMask.wrap(message.updateMask));
    }
    if (message.requestId !== "") {
      obj.requestId = message.requestId;
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateClusterRequest>): UpdateClusterRequest {
    return UpdateClusterRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateClusterRequest>): UpdateClusterRequest {
    const message = createBaseUpdateClusterRequest();
    message.projectId = object.projectId ?? "";
    message.region = object.region ?? "";
    message.clusterName = object.clusterName ?? "";
    message.cluster = (object.cluster !== undefined && object.cluster !== null)
      ? Cluster.fromPartial(object.cluster)
      : undefined;
    message.gracefulDecommissionTimeout =
      (object.gracefulDecommissionTimeout !== undefined && object.gracefulDecommissionTimeout !== null)
        ? Duration.fromPartial(object.gracefulDecommissionTimeout)
        : undefined;
    message.updateMask = object.updateMask ?? undefined;
    message.requestId = object.requestId ?? "";
    return message;
  },
};

function createBaseStopClusterRequest(): StopClusterRequest {
  return { projectId: "", region: "", clusterName: "", clusterUuid: "", requestId: "" };
}

export const StopClusterRequest: MessageFns<StopClusterRequest> = {
  encode(message: StopClusterRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.region !== "") {
      writer.uint32(18).string(message.region);
    }
    if (message.clusterName !== "") {
      writer.uint32(26).string(message.clusterName);
    }
    if (message.clusterUuid !== "") {
      writer.uint32(34).string(message.clusterUuid);
    }
    if (message.requestId !== "") {
      writer.uint32(42).string(message.requestId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StopClusterRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStopClusterRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.region = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.clusterName = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.clusterUuid = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.requestId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StopClusterRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      clusterName: isSet(object.clusterName) ? globalThis.String(object.clusterName) : "",
      clusterUuid: isSet(object.clusterUuid) ? globalThis.String(object.clusterUuid) : "",
      requestId: isSet(object.requestId) ? globalThis.String(object.requestId) : "",
    };
  },

  toJSON(message: StopClusterRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.clusterName !== "") {
      obj.clusterName = message.clusterName;
    }
    if (message.clusterUuid !== "") {
      obj.clusterUuid = message.clusterUuid;
    }
    if (message.requestId !== "") {
      obj.requestId = message.requestId;
    }
    return obj;
  },

  create(base?: DeepPartial<StopClusterRequest>): StopClusterRequest {
    return StopClusterRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StopClusterRequest>): StopClusterRequest {
    const message = createBaseStopClusterRequest();
    message.projectId = object.projectId ?? "";
    message.region = object.region ?? "";
    message.clusterName = object.clusterName ?? "";
    message.clusterUuid = object.clusterUuid ?? "";
    message.requestId = object.requestId ?? "";
    return message;
  },
};

function createBaseStartClusterRequest(): StartClusterRequest {
  return { projectId: "", region: "", clusterName: "", clusterUuid: "", requestId: "" };
}

export const StartClusterRequest: MessageFns<StartClusterRequest> = {
  encode(message: StartClusterRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.region !== "") {
      writer.uint32(18).string(message.region);
    }
    if (message.clusterName !== "") {
      writer.uint32(26).string(message.clusterName);
    }
    if (message.clusterUuid !== "") {
      writer.uint32(34).string(message.clusterUuid);
    }
    if (message.requestId !== "") {
      writer.uint32(42).string(message.requestId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StartClusterRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStartClusterRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.region = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.clusterName = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.clusterUuid = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.requestId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StartClusterRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      clusterName: isSet(object.clusterName) ? globalThis.String(object.clusterName) : "",
      clusterUuid: isSet(object.clusterUuid) ? globalThis.String(object.clusterUuid) : "",
      requestId: isSet(object.requestId) ? globalThis.String(object.requestId) : "",
    };
  },

  toJSON(message: StartClusterRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.clusterName !== "") {
      obj.clusterName = message.clusterName;
    }
    if (message.clusterUuid !== "") {
      obj.clusterUuid = message.clusterUuid;
    }
    if (message.requestId !== "") {
      obj.requestId = message.requestId;
    }
    return obj;
  },

  create(base?: DeepPartial<StartClusterRequest>): StartClusterRequest {
    return StartClusterRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StartClusterRequest>): StartClusterRequest {
    const message = createBaseStartClusterRequest();
    message.projectId = object.projectId ?? "";
    message.region = object.region ?? "";
    message.clusterName = object.clusterName ?? "";
    message.clusterUuid = object.clusterUuid ?? "";
    message.requestId = object.requestId ?? "";
    return message;
  },
};

function createBaseDeleteClusterRequest(): DeleteClusterRequest {
  return { projectId: "", region: "", clusterName: "", clusterUuid: "", requestId: "" };
}

export const DeleteClusterRequest: MessageFns<DeleteClusterRequest> = {
  encode(message: DeleteClusterRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.region !== "") {
      writer.uint32(26).string(message.region);
    }
    if (message.clusterName !== "") {
      writer.uint32(18).string(message.clusterName);
    }
    if (message.clusterUuid !== "") {
      writer.uint32(34).string(message.clusterUuid);
    }
    if (message.requestId !== "") {
      writer.uint32(42).string(message.requestId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeleteClusterRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeleteClusterRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.region = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.clusterName = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.clusterUuid = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.requestId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeleteClusterRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      clusterName: isSet(object.clusterName) ? globalThis.String(object.clusterName) : "",
      clusterUuid: isSet(object.clusterUuid) ? globalThis.String(object.clusterUuid) : "",
      requestId: isSet(object.requestId) ? globalThis.String(object.requestId) : "",
    };
  },

  toJSON(message: DeleteClusterRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.clusterName !== "") {
      obj.clusterName = message.clusterName;
    }
    if (message.clusterUuid !== "") {
      obj.clusterUuid = message.clusterUuid;
    }
    if (message.requestId !== "") {
      obj.requestId = message.requestId;
    }
    return obj;
  },

  create(base?: DeepPartial<DeleteClusterRequest>): DeleteClusterRequest {
    return DeleteClusterRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeleteClusterRequest>): DeleteClusterRequest {
    const message = createBaseDeleteClusterRequest();
    message.projectId = object.projectId ?? "";
    message.region = object.region ?? "";
    message.clusterName = object.clusterName ?? "";
    message.clusterUuid = object.clusterUuid ?? "";
    message.requestId = object.requestId ?? "";
    return message;
  },
};

function createBaseGetClusterRequest(): GetClusterRequest {
  return { projectId: "", region: "", clusterName: "" };
}

export const GetClusterRequest: MessageFns<GetClusterRequest> = {
  encode(message: GetClusterRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.region !== "") {
      writer.uint32(26).string(message.region);
    }
    if (message.clusterName !== "") {
      writer.uint32(18).string(message.clusterName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetClusterRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetClusterRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.region = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.clusterName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetClusterRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      clusterName: isSet(object.clusterName) ? globalThis.String(object.clusterName) : "",
    };
  },

  toJSON(message: GetClusterRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.clusterName !== "") {
      obj.clusterName = message.clusterName;
    }
    return obj;
  },

  create(base?: DeepPartial<GetClusterRequest>): GetClusterRequest {
    return GetClusterRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetClusterRequest>): GetClusterRequest {
    const message = createBaseGetClusterRequest();
    message.projectId = object.projectId ?? "";
    message.region = object.region ?? "";
    message.clusterName = object.clusterName ?? "";
    return message;
  },
};

function createBaseListClustersRequest(): ListClustersRequest {
  return { projectId: "", region: "", filter: "", pageSize: 0, pageToken: "" };
}

export const ListClustersRequest: MessageFns<ListClustersRequest> = {
  encode(message: ListClustersRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.region !== "") {
      writer.uint32(34).string(message.region);
    }
    if (message.filter !== "") {
      writer.uint32(42).string(message.filter);
    }
    if (message.pageSize !== 0) {
      writer.uint32(16).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(26).string(message.pageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListClustersRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListClustersRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.region = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.filter = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListClustersRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      filter: isSet(object.filter) ? globalThis.String(object.filter) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
    };
  },

  toJSON(message: ListClustersRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.filter !== "") {
      obj.filter = message.filter;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListClustersRequest>): ListClustersRequest {
    return ListClustersRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListClustersRequest>): ListClustersRequest {
    const message = createBaseListClustersRequest();
    message.projectId = object.projectId ?? "";
    message.region = object.region ?? "";
    message.filter = object.filter ?? "";
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    return message;
  },
};

function createBaseListClustersResponse(): ListClustersResponse {
  return { clusters: [], nextPageToken: "" };
}

export const ListClustersResponse: MessageFns<ListClustersResponse> = {
  encode(message: ListClustersResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.clusters) {
      Cluster.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListClustersResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListClustersResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.clusters.push(Cluster.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListClustersResponse {
    return {
      clusters: globalThis.Array.isArray(object?.clusters) ? object.clusters.map((e: any) => Cluster.fromJSON(e)) : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: ListClustersResponse): unknown {
    const obj: any = {};
    if (message.clusters?.length) {
      obj.clusters = message.clusters.map((e) => Cluster.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListClustersResponse>): ListClustersResponse {
    return ListClustersResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListClustersResponse>): ListClustersResponse {
    const message = createBaseListClustersResponse();
    message.clusters = object.clusters?.map((e) => Cluster.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBaseDiagnoseClusterRequest(): DiagnoseClusterRequest {
  return {
    projectId: "",
    region: "",
    clusterName: "",
    tarballGcsDir: "",
    tarballAccess: 0,
    diagnosisInterval: undefined,
    jobs: [],
    yarnApplicationIds: [],
  };
}

export const DiagnoseClusterRequest: MessageFns<DiagnoseClusterRequest> = {
  encode(message: DiagnoseClusterRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.region !== "") {
      writer.uint32(26).string(message.region);
    }
    if (message.clusterName !== "") {
      writer.uint32(18).string(message.clusterName);
    }
    if (message.tarballGcsDir !== "") {
      writer.uint32(34).string(message.tarballGcsDir);
    }
    if (message.tarballAccess !== 0) {
      writer.uint32(40).int32(message.tarballAccess);
    }
    if (message.diagnosisInterval !== undefined) {
      Interval.encode(message.diagnosisInterval, writer.uint32(50).fork()).join();
    }
    for (const v of message.jobs) {
      writer.uint32(82).string(v!);
    }
    for (const v of message.yarnApplicationIds) {
      writer.uint32(90).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DiagnoseClusterRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDiagnoseClusterRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.region = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.clusterName = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.tarballGcsDir = reader.string();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.tarballAccess = reader.int32() as any;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.diagnosisInterval = Interval.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.jobs.push(reader.string());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.yarnApplicationIds.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DiagnoseClusterRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      clusterName: isSet(object.clusterName) ? globalThis.String(object.clusterName) : "",
      tarballGcsDir: isSet(object.tarballGcsDir) ? globalThis.String(object.tarballGcsDir) : "",
      tarballAccess: isSet(object.tarballAccess)
        ? diagnoseClusterRequest_TarballAccessFromJSON(object.tarballAccess)
        : 0,
      diagnosisInterval: isSet(object.diagnosisInterval) ? Interval.fromJSON(object.diagnosisInterval) : undefined,
      jobs: globalThis.Array.isArray(object?.jobs) ? object.jobs.map((e: any) => globalThis.String(e)) : [],
      yarnApplicationIds: globalThis.Array.isArray(object?.yarnApplicationIds)
        ? object.yarnApplicationIds.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: DiagnoseClusterRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.clusterName !== "") {
      obj.clusterName = message.clusterName;
    }
    if (message.tarballGcsDir !== "") {
      obj.tarballGcsDir = message.tarballGcsDir;
    }
    if (message.tarballAccess !== 0) {
      obj.tarballAccess = diagnoseClusterRequest_TarballAccessToJSON(message.tarballAccess);
    }
    if (message.diagnosisInterval !== undefined) {
      obj.diagnosisInterval = Interval.toJSON(message.diagnosisInterval);
    }
    if (message.jobs?.length) {
      obj.jobs = message.jobs;
    }
    if (message.yarnApplicationIds?.length) {
      obj.yarnApplicationIds = message.yarnApplicationIds;
    }
    return obj;
  },

  create(base?: DeepPartial<DiagnoseClusterRequest>): DiagnoseClusterRequest {
    return DiagnoseClusterRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DiagnoseClusterRequest>): DiagnoseClusterRequest {
    const message = createBaseDiagnoseClusterRequest();
    message.projectId = object.projectId ?? "";
    message.region = object.region ?? "";
    message.clusterName = object.clusterName ?? "";
    message.tarballGcsDir = object.tarballGcsDir ?? "";
    message.tarballAccess = object.tarballAccess ?? 0;
    message.diagnosisInterval = (object.diagnosisInterval !== undefined && object.diagnosisInterval !== null)
      ? Interval.fromPartial(object.diagnosisInterval)
      : undefined;
    message.jobs = object.jobs?.map((e) => e) || [];
    message.yarnApplicationIds = object.yarnApplicationIds?.map((e) => e) || [];
    return message;
  },
};

function createBaseDiagnoseClusterResults(): DiagnoseClusterResults {
  return { outputUri: "" };
}

export const DiagnoseClusterResults: MessageFns<DiagnoseClusterResults> = {
  encode(message: DiagnoseClusterResults, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.outputUri !== "") {
      writer.uint32(10).string(message.outputUri);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DiagnoseClusterResults {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDiagnoseClusterResults();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.outputUri = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DiagnoseClusterResults {
    return { outputUri: isSet(object.outputUri) ? globalThis.String(object.outputUri) : "" };
  },

  toJSON(message: DiagnoseClusterResults): unknown {
    const obj: any = {};
    if (message.outputUri !== "") {
      obj.outputUri = message.outputUri;
    }
    return obj;
  },

  create(base?: DeepPartial<DiagnoseClusterResults>): DiagnoseClusterResults {
    return DiagnoseClusterResults.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DiagnoseClusterResults>): DiagnoseClusterResults {
    const message = createBaseDiagnoseClusterResults();
    message.outputUri = object.outputUri ?? "";
    return message;
  },
};

function createBaseReservationAffinity(): ReservationAffinity {
  return { consumeReservationType: 0, key: "", values: [] };
}

export const ReservationAffinity: MessageFns<ReservationAffinity> = {
  encode(message: ReservationAffinity, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.consumeReservationType !== 0) {
      writer.uint32(8).int32(message.consumeReservationType);
    }
    if (message.key !== "") {
      writer.uint32(18).string(message.key);
    }
    for (const v of message.values) {
      writer.uint32(26).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReservationAffinity {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReservationAffinity();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.consumeReservationType = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.key = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.values.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReservationAffinity {
    return {
      consumeReservationType: isSet(object.consumeReservationType)
        ? reservationAffinity_TypeFromJSON(object.consumeReservationType)
        : 0,
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      values: globalThis.Array.isArray(object?.values) ? object.values.map((e: any) => globalThis.String(e)) : [],
    };
  },

  toJSON(message: ReservationAffinity): unknown {
    const obj: any = {};
    if (message.consumeReservationType !== 0) {
      obj.consumeReservationType = reservationAffinity_TypeToJSON(message.consumeReservationType);
    }
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.values?.length) {
      obj.values = message.values;
    }
    return obj;
  },

  create(base?: DeepPartial<ReservationAffinity>): ReservationAffinity {
    return ReservationAffinity.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReservationAffinity>): ReservationAffinity {
    const message = createBaseReservationAffinity();
    message.consumeReservationType = object.consumeReservationType ?? 0;
    message.key = object.key ?? "";
    message.values = object.values?.map((e) => e) || [];
    return message;
  },
};

/**
 * The ClusterControllerService provides methods to manage clusters
 * of Compute Engine instances.
 */
export type ClusterControllerDefinition = typeof ClusterControllerDefinition;
export const ClusterControllerDefinition = {
  name: "ClusterController",
  fullName: "google.cloud.dataproc.v1.ClusterController",
  methods: {
    /**
     * Creates a cluster in a project. The returned
     * [Operation.metadata][google.longrunning.Operation.metadata] will be
     * [ClusterOperationMetadata](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#clusteroperationmetadata).
     */
    createCluster: {
      name: "CreateCluster",
      requestType: CreateClusterRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              60,
              10,
              7,
              67,
              108,
              117,
              115,
              116,
              101,
              114,
              18,
              49,
              103,
              111,
              111,
              103,
              108,
              101,
              46,
              99,
              108,
              111,
              117,
              100,
              46,
              100,
              97,
              116,
              97,
              112,
              114,
              111,
              99,
              46,
              118,
              49,
              46,
              67,
              108,
              117,
              115,
              116,
              101,
              114,
              79,
              112,
              101,
              114,
              97,
              116,
              105,
              111,
              110,
              77,
              101,
              116,
              97,
              100,
              97,
              116,
              97,
            ]),
          ],
          8410: [
            Buffer.from([
              25,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              44,
              114,
              101,
              103,
              105,
              111,
              110,
              44,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
            ]),
          ],
          578365826: [
            Buffer.from([
              62,
              58,
              7,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              34,
              51,
              47,
              118,
              49,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              114,
              101,
              103,
              105,
              111,
              110,
              115,
              47,
              123,
              114,
              101,
              103,
              105,
              111,
              110,
              125,
              47,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Updates a cluster in a project. The returned
     * [Operation.metadata][google.longrunning.Operation.metadata] will be
     * [ClusterOperationMetadata](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#clusteroperationmetadata).
     * The cluster must be in a
     * [`RUNNING`][google.cloud.dataproc.v1.ClusterStatus.State] state or an error
     * is returned.
     */
    updateCluster: {
      name: "UpdateCluster",
      requestType: UpdateClusterRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              60,
              10,
              7,
              67,
              108,
              117,
              115,
              116,
              101,
              114,
              18,
              49,
              103,
              111,
              111,
              103,
              108,
              101,
              46,
              99,
              108,
              111,
              117,
              100,
              46,
              100,
              97,
              116,
              97,
              112,
              114,
              111,
              99,
              46,
              118,
              49,
              46,
              67,
              108,
              117,
              115,
              116,
              101,
              114,
              79,
              112,
              101,
              114,
              97,
              116,
              105,
              111,
              110,
              77,
              101,
              116,
              97,
              100,
              97,
              116,
              97,
            ]),
          ],
          8410: [
            Buffer.from([
              50,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              44,
              114,
              101,
              103,
              105,
              111,
              110,
              44,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              95,
              110,
              97,
              109,
              101,
              44,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              44,
              117,
              112,
              100,
              97,
              116,
              101,
              95,
              109,
              97,
              115,
              107,
            ]),
          ],
          578365826: [
            Buffer.from([
              77,
              58,
              7,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              50,
              66,
              47,
              118,
              49,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              114,
              101,
              103,
              105,
              111,
              110,
              115,
              47,
              123,
              114,
              101,
              103,
              105,
              111,
              110,
              125,
              47,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              115,
              47,
              123,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              95,
              110,
              97,
              109,
              101,
              125,
            ]),
          ],
        },
      },
    },
    /** Stops a cluster in a project. */
    stopCluster: {
      name: "StopCluster",
      requestType: StopClusterRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              60,
              10,
              7,
              67,
              108,
              117,
              115,
              116,
              101,
              114,
              18,
              49,
              103,
              111,
              111,
              103,
              108,
              101,
              46,
              99,
              108,
              111,
              117,
              100,
              46,
              100,
              97,
              116,
              97,
              112,
              114,
              111,
              99,
              46,
              118,
              49,
              46,
              67,
              108,
              117,
              115,
              116,
              101,
              114,
              79,
              112,
              101,
              114,
              97,
              116,
              105,
              111,
              110,
              77,
              101,
              116,
              97,
              100,
              97,
              116,
              97,
            ]),
          ],
          578365826: [
            Buffer.from([
              76,
              58,
              1,
              42,
              34,
              71,
              47,
              118,
              49,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              114,
              101,
              103,
              105,
              111,
              110,
              115,
              47,
              123,
              114,
              101,
              103,
              105,
              111,
              110,
              125,
              47,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              115,
              47,
              123,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              95,
              110,
              97,
              109,
              101,
              125,
              58,
              115,
              116,
              111,
              112,
            ]),
          ],
        },
      },
    },
    /** Starts a cluster in a project. */
    startCluster: {
      name: "StartCluster",
      requestType: StartClusterRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              60,
              10,
              7,
              67,
              108,
              117,
              115,
              116,
              101,
              114,
              18,
              49,
              103,
              111,
              111,
              103,
              108,
              101,
              46,
              99,
              108,
              111,
              117,
              100,
              46,
              100,
              97,
              116,
              97,
              112,
              114,
              111,
              99,
              46,
              118,
              49,
              46,
              67,
              108,
              117,
              115,
              116,
              101,
              114,
              79,
              112,
              101,
              114,
              97,
              116,
              105,
              111,
              110,
              77,
              101,
              116,
              97,
              100,
              97,
              116,
              97,
            ]),
          ],
          578365826: [
            Buffer.from([
              77,
              58,
              1,
              42,
              34,
              72,
              47,
              118,
              49,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              114,
              101,
              103,
              105,
              111,
              110,
              115,
              47,
              123,
              114,
              101,
              103,
              105,
              111,
              110,
              125,
              47,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              115,
              47,
              123,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              95,
              110,
              97,
              109,
              101,
              125,
              58,
              115,
              116,
              97,
              114,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * Deletes a cluster in a project. The returned
     * [Operation.metadata][google.longrunning.Operation.metadata] will be
     * [ClusterOperationMetadata](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#clusteroperationmetadata).
     */
    deleteCluster: {
      name: "DeleteCluster",
      requestType: DeleteClusterRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              74,
              10,
              21,
              103,
              111,
              111,
              103,
              108,
              101,
              46,
              112,
              114,
              111,
              116,
              111,
              98,
              117,
              102,
              46,
              69,
              109,
              112,
              116,
              121,
              18,
              49,
              103,
              111,
              111,
              103,
              108,
              101,
              46,
              99,
              108,
              111,
              117,
              100,
              46,
              100,
              97,
              116,
              97,
              112,
              114,
              111,
              99,
              46,
              118,
              49,
              46,
              67,
              108,
              117,
              115,
              116,
              101,
              114,
              79,
              112,
              101,
              114,
              97,
              116,
              105,
              111,
              110,
              77,
              101,
              116,
              97,
              100,
              97,
              116,
              97,
            ]),
          ],
          8410: [
            Buffer.from([
              30,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              44,
              114,
              101,
              103,
              105,
              111,
              110,
              44,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              95,
              110,
              97,
              109,
              101,
            ]),
          ],
          578365826: [
            Buffer.from([
              68,
              42,
              66,
              47,
              118,
              49,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              114,
              101,
              103,
              105,
              111,
              110,
              115,
              47,
              123,
              114,
              101,
              103,
              105,
              111,
              110,
              125,
              47,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              115,
              47,
              123,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              95,
              110,
              97,
              109,
              101,
              125,
            ]),
          ],
        },
      },
    },
    /** Gets the resource representation for a cluster in a project. */
    getCluster: {
      name: "GetCluster",
      requestType: GetClusterRequest,
      requestStream: false,
      responseType: Cluster,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              30,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              44,
              114,
              101,
              103,
              105,
              111,
              110,
              44,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              95,
              110,
              97,
              109,
              101,
            ]),
          ],
          578365826: [
            Buffer.from([
              68,
              18,
              66,
              47,
              118,
              49,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              114,
              101,
              103,
              105,
              111,
              110,
              115,
              47,
              123,
              114,
              101,
              103,
              105,
              111,
              110,
              125,
              47,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              115,
              47,
              123,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              95,
              110,
              97,
              109,
              101,
              125,
            ]),
          ],
        },
      },
    },
    /** Lists all regions/{region}/clusters in a project alphabetically. */
    listClusters: {
      name: "ListClusters",
      requestType: ListClustersRequest,
      requestStream: false,
      responseType: ListClustersResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([17, 112, 114, 111, 106, 101, 99, 116, 95, 105, 100, 44, 114, 101, 103, 105, 111, 110]),
            Buffer.from([
              24,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              44,
              114,
              101,
              103,
              105,
              111,
              110,
              44,
              102,
              105,
              108,
              116,
              101,
              114,
            ]),
          ],
          578365826: [
            Buffer.from([
              53,
              18,
              51,
              47,
              118,
              49,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              114,
              101,
              103,
              105,
              111,
              110,
              115,
              47,
              123,
              114,
              101,
              103,
              105,
              111,
              110,
              125,
              47,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Gets cluster diagnostic information. The returned
     * [Operation.metadata][google.longrunning.Operation.metadata] will be
     * [ClusterOperationMetadata](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#clusteroperationmetadata).
     * After the operation completes,
     * [Operation.response][google.longrunning.Operation.response]
     * contains
     * [DiagnoseClusterResults](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#diagnoseclusterresults).
     */
    diagnoseCluster: {
      name: "DiagnoseCluster",
      requestType: DiagnoseClusterRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              75,
              10,
              22,
              68,
              105,
              97,
              103,
              110,
              111,
              115,
              101,
              67,
              108,
              117,
              115,
              116,
              101,
              114,
              82,
              101,
              115,
              117,
              108,
              116,
              115,
              18,
              49,
              103,
              111,
              111,
              103,
              108,
              101,
              46,
              99,
              108,
              111,
              117,
              100,
              46,
              100,
              97,
              116,
              97,
              112,
              114,
              111,
              99,
              46,
              118,
              49,
              46,
              67,
              108,
              117,
              115,
              116,
              101,
              114,
              79,
              112,
              101,
              114,
              97,
              116,
              105,
              111,
              110,
              77,
              101,
              116,
              97,
              100,
              97,
              116,
              97,
            ]),
          ],
          8410: [
            Buffer.from([
              30,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              44,
              114,
              101,
              103,
              105,
              111,
              110,
              44,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              95,
              110,
              97,
              109,
              101,
            ]),
          ],
          578365826: [
            Buffer.from([
              80,
              58,
              1,
              42,
              34,
              75,
              47,
              118,
              49,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              114,
              101,
              103,
              105,
              111,
              110,
              115,
              47,
              123,
              114,
              101,
              103,
              105,
              111,
              110,
              125,
              47,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              115,
              47,
              123,
              99,
              108,
              117,
              115,
              116,
              101,
              114,
              95,
              110,
              97,
              109,
              101,
              125,
              58,
              100,
              105,
              97,
              103,
              110,
              111,
              115,
              101,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface ClusterControllerServiceImplementation<CallContextExt = {}> {
  /**
   * Creates a cluster in a project. The returned
   * [Operation.metadata][google.longrunning.Operation.metadata] will be
   * [ClusterOperationMetadata](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#clusteroperationmetadata).
   */
  createCluster(request: CreateClusterRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
  /**
   * Updates a cluster in a project. The returned
   * [Operation.metadata][google.longrunning.Operation.metadata] will be
   * [ClusterOperationMetadata](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#clusteroperationmetadata).
   * The cluster must be in a
   * [`RUNNING`][google.cloud.dataproc.v1.ClusterStatus.State] state or an error
   * is returned.
   */
  updateCluster(request: UpdateClusterRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
  /** Stops a cluster in a project. */
  stopCluster(request: StopClusterRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
  /** Starts a cluster in a project. */
  startCluster(request: StartClusterRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
  /**
   * Deletes a cluster in a project. The returned
   * [Operation.metadata][google.longrunning.Operation.metadata] will be
   * [ClusterOperationMetadata](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#clusteroperationmetadata).
   */
  deleteCluster(request: DeleteClusterRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
  /** Gets the resource representation for a cluster in a project. */
  getCluster(request: GetClusterRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Cluster>>;
  /** Lists all regions/{region}/clusters in a project alphabetically. */
  listClusters(
    request: ListClustersRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ListClustersResponse>>;
  /**
   * Gets cluster diagnostic information. The returned
   * [Operation.metadata][google.longrunning.Operation.metadata] will be
   * [ClusterOperationMetadata](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#clusteroperationmetadata).
   * After the operation completes,
   * [Operation.response][google.longrunning.Operation.response]
   * contains
   * [DiagnoseClusterResults](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#diagnoseclusterresults).
   */
  diagnoseCluster(
    request: DiagnoseClusterRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Operation>>;
}

export interface ClusterControllerClient<CallOptionsExt = {}> {
  /**
   * Creates a cluster in a project. The returned
   * [Operation.metadata][google.longrunning.Operation.metadata] will be
   * [ClusterOperationMetadata](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#clusteroperationmetadata).
   */
  createCluster(request: DeepPartial<CreateClusterRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
  /**
   * Updates a cluster in a project. The returned
   * [Operation.metadata][google.longrunning.Operation.metadata] will be
   * [ClusterOperationMetadata](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#clusteroperationmetadata).
   * The cluster must be in a
   * [`RUNNING`][google.cloud.dataproc.v1.ClusterStatus.State] state or an error
   * is returned.
   */
  updateCluster(request: DeepPartial<UpdateClusterRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
  /** Stops a cluster in a project. */
  stopCluster(request: DeepPartial<StopClusterRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
  /** Starts a cluster in a project. */
  startCluster(request: DeepPartial<StartClusterRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
  /**
   * Deletes a cluster in a project. The returned
   * [Operation.metadata][google.longrunning.Operation.metadata] will be
   * [ClusterOperationMetadata](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#clusteroperationmetadata).
   */
  deleteCluster(request: DeepPartial<DeleteClusterRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
  /** Gets the resource representation for a cluster in a project. */
  getCluster(request: DeepPartial<GetClusterRequest>, options?: CallOptions & CallOptionsExt): Promise<Cluster>;
  /** Lists all regions/{region}/clusters in a project alphabetically. */
  listClusters(
    request: DeepPartial<ListClustersRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ListClustersResponse>;
  /**
   * Gets cluster diagnostic information. The returned
   * [Operation.metadata][google.longrunning.Operation.metadata] will be
   * [ClusterOperationMetadata](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#clusteroperationmetadata).
   * After the operation completes,
   * [Operation.response][google.longrunning.Operation.response]
   * contains
   * [DiagnoseClusterResults](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#diagnoseclusterresults).
   */
  diagnoseCluster(
    request: DeepPartial<DiagnoseClusterRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Operation>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
