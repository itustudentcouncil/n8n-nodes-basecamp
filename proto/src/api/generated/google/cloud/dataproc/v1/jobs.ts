// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/dataproc/v1/jobs.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Operation } from "../../../longrunning/operations.js";
import { Empty } from "../../../protobuf/empty.js";
import { FieldMask } from "../../../protobuf/field_mask.js";
import { Timestamp } from "../../../protobuf/timestamp.js";

export const protobufPackage = "google.cloud.dataproc.v1";

/** The runtime logging config of the job. */
export interface LoggingConfig {
  /**
   * The per-package log levels for the driver. This can include
   * "root" package name to configure rootLogger.
   * Examples:
   * - 'com.google = FATAL'
   * - 'root = INFO'
   * - 'org.apache = DEBUG'
   */
  driverLogLevels: { [key: string]: LoggingConfig_Level };
}

/**
 * The Log4j level for job execution. When running an
 * [Apache Hive](https://hive.apache.org/) job, Cloud
 * Dataproc configures the Hive client to an equivalent verbosity level.
 */
export enum LoggingConfig_Level {
  /** LEVEL_UNSPECIFIED - Level is unspecified. Use default level for log4j. */
  LEVEL_UNSPECIFIED = 0,
  /** ALL - Use ALL level for log4j. */
  ALL = 1,
  /** TRACE - Use TRACE level for log4j. */
  TRACE = 2,
  /** DEBUG - Use DEBUG level for log4j. */
  DEBUG = 3,
  /** INFO - Use INFO level for log4j. */
  INFO = 4,
  /** WARN - Use WARN level for log4j. */
  WARN = 5,
  /** ERROR - Use ERROR level for log4j. */
  ERROR = 6,
  /** FATAL - Use FATAL level for log4j. */
  FATAL = 7,
  /** OFF - Turn off log4j. */
  OFF = 8,
  UNRECOGNIZED = -1,
}

export function loggingConfig_LevelFromJSON(object: any): LoggingConfig_Level {
  switch (object) {
    case 0:
    case "LEVEL_UNSPECIFIED":
      return LoggingConfig_Level.LEVEL_UNSPECIFIED;
    case 1:
    case "ALL":
      return LoggingConfig_Level.ALL;
    case 2:
    case "TRACE":
      return LoggingConfig_Level.TRACE;
    case 3:
    case "DEBUG":
      return LoggingConfig_Level.DEBUG;
    case 4:
    case "INFO":
      return LoggingConfig_Level.INFO;
    case 5:
    case "WARN":
      return LoggingConfig_Level.WARN;
    case 6:
    case "ERROR":
      return LoggingConfig_Level.ERROR;
    case 7:
    case "FATAL":
      return LoggingConfig_Level.FATAL;
    case 8:
    case "OFF":
      return LoggingConfig_Level.OFF;
    case -1:
    case "UNRECOGNIZED":
    default:
      return LoggingConfig_Level.UNRECOGNIZED;
  }
}

export function loggingConfig_LevelToJSON(object: LoggingConfig_Level): string {
  switch (object) {
    case LoggingConfig_Level.LEVEL_UNSPECIFIED:
      return "LEVEL_UNSPECIFIED";
    case LoggingConfig_Level.ALL:
      return "ALL";
    case LoggingConfig_Level.TRACE:
      return "TRACE";
    case LoggingConfig_Level.DEBUG:
      return "DEBUG";
    case LoggingConfig_Level.INFO:
      return "INFO";
    case LoggingConfig_Level.WARN:
      return "WARN";
    case LoggingConfig_Level.ERROR:
      return "ERROR";
    case LoggingConfig_Level.FATAL:
      return "FATAL";
    case LoggingConfig_Level.OFF:
      return "OFF";
    case LoggingConfig_Level.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export interface LoggingConfig_DriverLogLevelsEntry {
  key: string;
  value: LoggingConfig_Level;
}

/**
 * A Dataproc job for running
 * [Apache Hadoop
 * MapReduce](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)
 * jobs on [Apache Hadoop
 * YARN](https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).
 */
export interface HadoopJob {
  /**
   * The HCFS URI of the jar file containing the main class.
   * Examples:
   *     'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar'
   *     'hdfs:/tmp/test-samples/custom-wordcount.jar'
   *     'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
   */
  mainJarFileUri?:
    | string
    | undefined;
  /**
   * The name of the driver's main class. The jar file containing the class
   * must be in the default CLASSPATH or specified in `jar_file_uris`.
   */
  mainClass?:
    | string
    | undefined;
  /**
   * Optional. The arguments to pass to the driver. Do not
   * include arguments, such as `-libjars` or `-Dfoo=bar`, that can be set as
   * job properties, since a collision might occur that causes an incorrect job
   * submission.
   */
  args: string[];
  /**
   * Optional. Jar file URIs to add to the CLASSPATHs of the
   * Hadoop driver and tasks.
   */
  jarFileUris: string[];
  /**
   * Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied
   * to the working directory of Hadoop drivers and distributed tasks. Useful
   * for naively parallel tasks.
   */
  fileUris: string[];
  /**
   * Optional. HCFS URIs of archives to be extracted in the working directory of
   * Hadoop drivers and tasks. Supported file types:
   * .jar, .tar, .tar.gz, .tgz, or .zip.
   */
  archiveUris: string[];
  /**
   * Optional. A mapping of property names to values, used to configure Hadoop.
   * Properties that conflict with values set by the Dataproc API might be
   * overwritten. Can include properties set in `/etc/hadoop/conf/*-site` and
   * classes in user code.
   */
  properties: { [key: string]: string };
  /** Optional. The runtime log config for job execution. */
  loggingConfig: LoggingConfig | undefined;
}

export interface HadoopJob_PropertiesEntry {
  key: string;
  value: string;
}

/**
 * A Dataproc job for running [Apache Spark](https://spark.apache.org/)
 * applications on YARN.
 */
export interface SparkJob {
  /** The HCFS URI of the jar file that contains the main class. */
  mainJarFileUri?:
    | string
    | undefined;
  /**
   * The name of the driver's main class. The jar file that contains the class
   * must be in the default CLASSPATH or specified in
   * SparkJob.jar_file_uris.
   */
  mainClass?:
    | string
    | undefined;
  /**
   * Optional. The arguments to pass to the driver. Do not include arguments,
   * such as `--conf`, that can be set as job properties, since a collision may
   * occur that causes an incorrect job submission.
   */
  args: string[];
  /**
   * Optional. HCFS URIs of jar files to add to the CLASSPATHs of the
   * Spark driver and tasks.
   */
  jarFileUris: string[];
  /**
   * Optional. HCFS URIs of files to be placed in the working directory of
   * each executor. Useful for naively parallel tasks.
   */
  fileUris: string[];
  /**
   * Optional. HCFS URIs of archives to be extracted into the working directory
   * of each executor. Supported file types:
   * .jar, .tar, .tar.gz, .tgz, and .zip.
   */
  archiveUris: string[];
  /**
   * Optional. A mapping of property names to values, used to configure Spark.
   * Properties that conflict with values set by the Dataproc API might be
   * overwritten. Can include properties set in
   * /etc/spark/conf/spark-defaults.conf and classes in user code.
   */
  properties: { [key: string]: string };
  /** Optional. The runtime log config for job execution. */
  loggingConfig: LoggingConfig | undefined;
}

export interface SparkJob_PropertiesEntry {
  key: string;
  value: string;
}

/**
 * A Dataproc job for running
 * [Apache
 * PySpark](https://spark.apache.org/docs/0.9.0/python-programming-guide.html)
 * applications on YARN.
 */
export interface PySparkJob {
  /**
   * Required. The HCFS URI of the main Python file to use as the driver. Must
   * be a .py file.
   */
  mainPythonFileUri: string;
  /**
   * Optional. The arguments to pass to the driver.  Do not include arguments,
   * such as `--conf`, that can be set as job properties, since a collision may
   * occur that causes an incorrect job submission.
   */
  args: string[];
  /**
   * Optional. HCFS file URIs of Python files to pass to the PySpark
   * framework. Supported file types: .py, .egg, and .zip.
   */
  pythonFileUris: string[];
  /**
   * Optional. HCFS URIs of jar files to add to the CLASSPATHs of the
   * Python driver and tasks.
   */
  jarFileUris: string[];
  /**
   * Optional. HCFS URIs of files to be placed in the working directory of
   * each executor. Useful for naively parallel tasks.
   */
  fileUris: string[];
  /**
   * Optional. HCFS URIs of archives to be extracted into the working directory
   * of each executor. Supported file types:
   * .jar, .tar, .tar.gz, .tgz, and .zip.
   */
  archiveUris: string[];
  /**
   * Optional. A mapping of property names to values, used to configure PySpark.
   * Properties that conflict with values set by the Dataproc API might be
   * overwritten. Can include properties set in
   * /etc/spark/conf/spark-defaults.conf and classes in user code.
   */
  properties: { [key: string]: string };
  /** Optional. The runtime log config for job execution. */
  loggingConfig: LoggingConfig | undefined;
}

export interface PySparkJob_PropertiesEntry {
  key: string;
  value: string;
}

/** A list of queries to run on a cluster. */
export interface QueryList {
  /**
   * Required. The queries to execute. You do not need to end a query expression
   * with a semicolon. Multiple queries can be specified in one
   * string by separating each with a semicolon. Here is an example of a
   * Dataproc API snippet that uses a QueryList to specify a HiveJob:
   *
   *     "hiveJob": {
   *       "queryList": {
   *         "queries": [
   *           "query1",
   *           "query2",
   *           "query3;query4",
   *         ]
   *       }
   *     }
   */
  queries: string[];
}

/**
 * A Dataproc job for running [Apache Hive](https://hive.apache.org/)
 * queries on YARN.
 */
export interface HiveJob {
  /** The HCFS URI of the script that contains Hive queries. */
  queryFileUri?:
    | string
    | undefined;
  /** A list of queries. */
  queryList?:
    | QueryList
    | undefined;
  /**
   * Optional. Whether to continue executing queries if a query fails.
   * The default value is `false`. Setting to `true` can be useful when
   * executing independent parallel queries.
   */
  continueOnFailure: boolean;
  /**
   * Optional. Mapping of query variable names to values (equivalent to the
   * Hive command: `SET name="value";`).
   */
  scriptVariables: { [key: string]: string };
  /**
   * Optional. A mapping of property names and values, used to configure Hive.
   * Properties that conflict with values set by the Dataproc API might be
   * overwritten. Can include properties set in `/etc/hadoop/conf/*-site.xml`,
   * /etc/hive/conf/hive-site.xml, and classes in user code.
   */
  properties: { [key: string]: string };
  /**
   * Optional. HCFS URIs of jar files to add to the CLASSPATH of the
   * Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes
   * and UDFs.
   */
  jarFileUris: string[];
}

export interface HiveJob_ScriptVariablesEntry {
  key: string;
  value: string;
}

export interface HiveJob_PropertiesEntry {
  key: string;
  value: string;
}

/**
 * A Dataproc job for running [Apache Spark
 * SQL](https://spark.apache.org/sql/) queries.
 */
export interface SparkSqlJob {
  /** The HCFS URI of the script that contains SQL queries. */
  queryFileUri?:
    | string
    | undefined;
  /** A list of queries. */
  queryList?:
    | QueryList
    | undefined;
  /**
   * Optional. Mapping of query variable names to values (equivalent to the
   * Spark SQL command: SET `name="value";`).
   */
  scriptVariables: { [key: string]: string };
  /**
   * Optional. A mapping of property names to values, used to configure
   * Spark SQL's SparkConf. Properties that conflict with values set by the
   * Dataproc API might be overwritten.
   */
  properties: { [key: string]: string };
  /** Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH. */
  jarFileUris: string[];
  /** Optional. The runtime log config for job execution. */
  loggingConfig: LoggingConfig | undefined;
}

export interface SparkSqlJob_ScriptVariablesEntry {
  key: string;
  value: string;
}

export interface SparkSqlJob_PropertiesEntry {
  key: string;
  value: string;
}

/**
 * A Dataproc job for running [Apache Pig](https://pig.apache.org/)
 * queries on YARN.
 */
export interface PigJob {
  /** The HCFS URI of the script that contains the Pig queries. */
  queryFileUri?:
    | string
    | undefined;
  /** A list of queries. */
  queryList?:
    | QueryList
    | undefined;
  /**
   * Optional. Whether to continue executing queries if a query fails.
   * The default value is `false`. Setting to `true` can be useful when
   * executing independent parallel queries.
   */
  continueOnFailure: boolean;
  /**
   * Optional. Mapping of query variable names to values (equivalent to the Pig
   * command: `name=[value]`).
   */
  scriptVariables: { [key: string]: string };
  /**
   * Optional. A mapping of property names to values, used to configure Pig.
   * Properties that conflict with values set by the Dataproc API might be
   * overwritten. Can include properties set in `/etc/hadoop/conf/*-site.xml`,
   * /etc/pig/conf/pig.properties, and classes in user code.
   */
  properties: { [key: string]: string };
  /**
   * Optional. HCFS URIs of jar files to add to the CLASSPATH of
   * the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
   */
  jarFileUris: string[];
  /** Optional. The runtime log config for job execution. */
  loggingConfig: LoggingConfig | undefined;
}

export interface PigJob_ScriptVariablesEntry {
  key: string;
  value: string;
}

export interface PigJob_PropertiesEntry {
  key: string;
  value: string;
}

/**
 * A Dataproc job for running
 * [Apache SparkR](https://spark.apache.org/docs/latest/sparkr.html)
 * applications on YARN.
 */
export interface SparkRJob {
  /**
   * Required. The HCFS URI of the main R file to use as the driver.
   * Must be a .R file.
   */
  mainRFileUri: string;
  /**
   * Optional. The arguments to pass to the driver.  Do not include arguments,
   * such as `--conf`, that can be set as job properties, since a collision may
   * occur that causes an incorrect job submission.
   */
  args: string[];
  /**
   * Optional. HCFS URIs of files to be placed in the working directory of
   * each executor. Useful for naively parallel tasks.
   */
  fileUris: string[];
  /**
   * Optional. HCFS URIs of archives to be extracted into the working directory
   * of each executor. Supported file types:
   * .jar, .tar, .tar.gz, .tgz, and .zip.
   */
  archiveUris: string[];
  /**
   * Optional. A mapping of property names to values, used to configure SparkR.
   * Properties that conflict with values set by the Dataproc API might be
   * overwritten. Can include properties set in
   * /etc/spark/conf/spark-defaults.conf and classes in user code.
   */
  properties: { [key: string]: string };
  /** Optional. The runtime log config for job execution. */
  loggingConfig: LoggingConfig | undefined;
}

export interface SparkRJob_PropertiesEntry {
  key: string;
  value: string;
}

/**
 * A Dataproc job for running [Presto](https://prestosql.io/) queries.
 * **IMPORTANT**: The [Dataproc Presto Optional
 * Component](https://cloud.google.com/dataproc/docs/concepts/components/presto)
 * must be enabled when the cluster is created to submit a Presto job to the
 * cluster.
 */
export interface PrestoJob {
  /** The HCFS URI of the script that contains SQL queries. */
  queryFileUri?:
    | string
    | undefined;
  /** A list of queries. */
  queryList?:
    | QueryList
    | undefined;
  /**
   * Optional. Whether to continue executing queries if a query fails.
   * The default value is `false`. Setting to `true` can be useful when
   * executing independent parallel queries.
   */
  continueOnFailure: boolean;
  /**
   * Optional. The format in which query output will be displayed. See the
   * Presto documentation for supported output formats
   */
  outputFormat: string;
  /** Optional. Presto client tags to attach to this query */
  clientTags: string[];
  /**
   * Optional. A mapping of property names to values. Used to set Presto
   * [session properties](https://prestodb.io/docs/current/sql/set-session.html)
   * Equivalent to using the --session flag in the Presto CLI
   */
  properties: { [key: string]: string };
  /** Optional. The runtime log config for job execution. */
  loggingConfig: LoggingConfig | undefined;
}

export interface PrestoJob_PropertiesEntry {
  key: string;
  value: string;
}

/**
 * A Dataproc job for running [Trino](https://trino.io/) queries.
 * **IMPORTANT**: The [Dataproc Trino Optional
 * Component](https://cloud.google.com/dataproc/docs/concepts/components/trino)
 * must be enabled when the cluster is created to submit a Trino job to the
 * cluster.
 */
export interface TrinoJob {
  /** The HCFS URI of the script that contains SQL queries. */
  queryFileUri?:
    | string
    | undefined;
  /** A list of queries. */
  queryList?:
    | QueryList
    | undefined;
  /**
   * Optional. Whether to continue executing queries if a query fails.
   * The default value is `false`. Setting to `true` can be useful when
   * executing independent parallel queries.
   */
  continueOnFailure: boolean;
  /**
   * Optional. The format in which query output will be displayed. See the
   * Trino documentation for supported output formats
   */
  outputFormat: string;
  /** Optional. Trino client tags to attach to this query */
  clientTags: string[];
  /**
   * Optional. A mapping of property names to values. Used to set Trino
   * [session properties](https://trino.io/docs/current/sql/set-session.html)
   * Equivalent to using the --session flag in the Trino CLI
   */
  properties: { [key: string]: string };
  /** Optional. The runtime log config for job execution. */
  loggingConfig: LoggingConfig | undefined;
}

export interface TrinoJob_PropertiesEntry {
  key: string;
  value: string;
}

/** A Dataproc job for running Apache Flink applications on YARN. */
export interface FlinkJob {
  /** The HCFS URI of the jar file that contains the main class. */
  mainJarFileUri?:
    | string
    | undefined;
  /**
   * The name of the driver's main class. The jar file that contains the class
   * must be in the default CLASSPATH or specified in
   * [jarFileUris][google.cloud.dataproc.v1.FlinkJob.jar_file_uris].
   */
  mainClass?:
    | string
    | undefined;
  /**
   * Optional. The arguments to pass to the driver. Do not include arguments,
   * such as `--conf`, that can be set as job properties, since a collision
   * might occur that causes an incorrect job submission.
   */
  args: string[];
  /**
   * Optional. HCFS URIs of jar files to add to the CLASSPATHs of the
   * Flink driver and tasks.
   */
  jarFileUris: string[];
  /**
   * Optional. HCFS URI of the savepoint, which contains the last saved progress
   * for starting the current job.
   */
  savepointUri: string;
  /**
   * Optional. A mapping of property names to values, used to configure Flink.
   * Properties that conflict with values set by the Dataproc API might be
   * overwritten. Can include properties set in
   * `/etc/flink/conf/flink-defaults.conf` and classes in user code.
   */
  properties: { [key: string]: string };
  /** Optional. The runtime log config for job execution. */
  loggingConfig: LoggingConfig | undefined;
}

export interface FlinkJob_PropertiesEntry {
  key: string;
  value: string;
}

/** Dataproc job config. */
export interface JobPlacement {
  /** Required. The name of the cluster where the job will be submitted. */
  clusterName: string;
  /**
   * Output only. A cluster UUID generated by the Dataproc service when
   * the job is submitted.
   */
  clusterUuid: string;
  /**
   * Optional. Cluster labels to identify a cluster where the job will be
   * submitted.
   */
  clusterLabels: { [key: string]: string };
}

export interface JobPlacement_ClusterLabelsEntry {
  key: string;
  value: string;
}

/** Dataproc job status. */
export interface JobStatus {
  /** Output only. A state message specifying the overall job state. */
  state: JobStatus_State;
  /**
   * Optional. Output only. Job state details, such as an error
   * description if the state is `ERROR`.
   */
  details: string;
  /** Output only. The time when this state was entered. */
  stateStartTime:
    | Date
    | undefined;
  /**
   * Output only. Additional state information, which includes
   * status reported by the agent.
   */
  substate: JobStatus_Substate;
}

/** The job state. */
export enum JobStatus_State {
  /** STATE_UNSPECIFIED - The job state is unknown. */
  STATE_UNSPECIFIED = 0,
  /** PENDING - The job is pending; it has been submitted, but is not yet running. */
  PENDING = 1,
  /**
   * SETUP_DONE - Job has been received by the service and completed initial setup;
   * it will soon be submitted to the cluster.
   */
  SETUP_DONE = 8,
  /** RUNNING - The job is running on the cluster. */
  RUNNING = 2,
  /** CANCEL_PENDING - A CancelJob request has been received, but is pending. */
  CANCEL_PENDING = 3,
  /**
   * CANCEL_STARTED - Transient in-flight resources have been canceled, and the request to
   * cancel the running job has been issued to the cluster.
   */
  CANCEL_STARTED = 7,
  /** CANCELLED - The job cancellation was successful. */
  CANCELLED = 4,
  /** DONE - The job has completed successfully. */
  DONE = 5,
  /** ERROR - The job has completed, but encountered an error. */
  ERROR = 6,
  /**
   * ATTEMPT_FAILURE - Job attempt has failed. The detail field contains failure details for
   * this attempt.
   *
   * Applies to restartable jobs only.
   */
  ATTEMPT_FAILURE = 9,
  UNRECOGNIZED = -1,
}

export function jobStatus_StateFromJSON(object: any): JobStatus_State {
  switch (object) {
    case 0:
    case "STATE_UNSPECIFIED":
      return JobStatus_State.STATE_UNSPECIFIED;
    case 1:
    case "PENDING":
      return JobStatus_State.PENDING;
    case 8:
    case "SETUP_DONE":
      return JobStatus_State.SETUP_DONE;
    case 2:
    case "RUNNING":
      return JobStatus_State.RUNNING;
    case 3:
    case "CANCEL_PENDING":
      return JobStatus_State.CANCEL_PENDING;
    case 7:
    case "CANCEL_STARTED":
      return JobStatus_State.CANCEL_STARTED;
    case 4:
    case "CANCELLED":
      return JobStatus_State.CANCELLED;
    case 5:
    case "DONE":
      return JobStatus_State.DONE;
    case 6:
    case "ERROR":
      return JobStatus_State.ERROR;
    case 9:
    case "ATTEMPT_FAILURE":
      return JobStatus_State.ATTEMPT_FAILURE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return JobStatus_State.UNRECOGNIZED;
  }
}

export function jobStatus_StateToJSON(object: JobStatus_State): string {
  switch (object) {
    case JobStatus_State.STATE_UNSPECIFIED:
      return "STATE_UNSPECIFIED";
    case JobStatus_State.PENDING:
      return "PENDING";
    case JobStatus_State.SETUP_DONE:
      return "SETUP_DONE";
    case JobStatus_State.RUNNING:
      return "RUNNING";
    case JobStatus_State.CANCEL_PENDING:
      return "CANCEL_PENDING";
    case JobStatus_State.CANCEL_STARTED:
      return "CANCEL_STARTED";
    case JobStatus_State.CANCELLED:
      return "CANCELLED";
    case JobStatus_State.DONE:
      return "DONE";
    case JobStatus_State.ERROR:
      return "ERROR";
    case JobStatus_State.ATTEMPT_FAILURE:
      return "ATTEMPT_FAILURE";
    case JobStatus_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The job substate. */
export enum JobStatus_Substate {
  /** UNSPECIFIED - The job substate is unknown. */
  UNSPECIFIED = 0,
  /**
   * SUBMITTED - The Job is submitted to the agent.
   *
   * Applies to RUNNING state.
   */
  SUBMITTED = 1,
  /**
   * QUEUED - The Job has been received and is awaiting execution (it might be waiting
   * for a condition to be met). See the "details" field for the reason for
   * the delay.
   *
   * Applies to RUNNING state.
   */
  QUEUED = 2,
  /**
   * STALE_STATUS - The agent-reported status is out of date, which can be caused by a
   * loss of communication between the agent and Dataproc. If the
   * agent does not send a timely update, the job will fail.
   *
   * Applies to RUNNING state.
   */
  STALE_STATUS = 3,
  UNRECOGNIZED = -1,
}

export function jobStatus_SubstateFromJSON(object: any): JobStatus_Substate {
  switch (object) {
    case 0:
    case "UNSPECIFIED":
      return JobStatus_Substate.UNSPECIFIED;
    case 1:
    case "SUBMITTED":
      return JobStatus_Substate.SUBMITTED;
    case 2:
    case "QUEUED":
      return JobStatus_Substate.QUEUED;
    case 3:
    case "STALE_STATUS":
      return JobStatus_Substate.STALE_STATUS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return JobStatus_Substate.UNRECOGNIZED;
  }
}

export function jobStatus_SubstateToJSON(object: JobStatus_Substate): string {
  switch (object) {
    case JobStatus_Substate.UNSPECIFIED:
      return "UNSPECIFIED";
    case JobStatus_Substate.SUBMITTED:
      return "SUBMITTED";
    case JobStatus_Substate.QUEUED:
      return "QUEUED";
    case JobStatus_Substate.STALE_STATUS:
      return "STALE_STATUS";
    case JobStatus_Substate.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Encapsulates the full scoping used to reference a job. */
export interface JobReference {
  /**
   * Optional. The ID of the Google Cloud Platform project that the job belongs
   * to. If specified, must match the request project ID.
   */
  projectId: string;
  /**
   * Optional. The job ID, which must be unique within the project.
   *
   * The ID must contain only letters (a-z, A-Z), numbers (0-9),
   * underscores (_), or hyphens (-). The maximum length is 100 characters.
   *
   * If not specified by the caller, the job ID will be provided by the server.
   */
  jobId: string;
}

/**
 * A YARN application created by a job. Application information is a subset of
 * <code>org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto</code>.
 *
 * **Beta Feature**: This report is available for testing purposes only. It may
 * be changed before final release.
 */
export interface YarnApplication {
  /** Required. The application name. */
  name: string;
  /** Required. The application state. */
  state: YarnApplication_State;
  /** Required. The numerical progress of the application, from 1 to 100. */
  progress: number;
  /**
   * Optional. The HTTP URL of the ApplicationMaster, HistoryServer, or
   * TimelineServer that provides application-specific information. The URL uses
   * the internal hostname, and requires a proxy server for resolution and,
   * possibly, access.
   */
  trackingUrl: string;
}

/**
 * The application state, corresponding to
 * <code>YarnProtos.YarnApplicationStateProto</code>.
 */
export enum YarnApplication_State {
  /** STATE_UNSPECIFIED - Status is unspecified. */
  STATE_UNSPECIFIED = 0,
  /** NEW - Status is NEW. */
  NEW = 1,
  /** NEW_SAVING - Status is NEW_SAVING. */
  NEW_SAVING = 2,
  /** SUBMITTED - Status is SUBMITTED. */
  SUBMITTED = 3,
  /** ACCEPTED - Status is ACCEPTED. */
  ACCEPTED = 4,
  /** RUNNING - Status is RUNNING. */
  RUNNING = 5,
  /** FINISHED - Status is FINISHED. */
  FINISHED = 6,
  /** FAILED - Status is FAILED. */
  FAILED = 7,
  /** KILLED - Status is KILLED. */
  KILLED = 8,
  UNRECOGNIZED = -1,
}

export function yarnApplication_StateFromJSON(object: any): YarnApplication_State {
  switch (object) {
    case 0:
    case "STATE_UNSPECIFIED":
      return YarnApplication_State.STATE_UNSPECIFIED;
    case 1:
    case "NEW":
      return YarnApplication_State.NEW;
    case 2:
    case "NEW_SAVING":
      return YarnApplication_State.NEW_SAVING;
    case 3:
    case "SUBMITTED":
      return YarnApplication_State.SUBMITTED;
    case 4:
    case "ACCEPTED":
      return YarnApplication_State.ACCEPTED;
    case 5:
    case "RUNNING":
      return YarnApplication_State.RUNNING;
    case 6:
    case "FINISHED":
      return YarnApplication_State.FINISHED;
    case 7:
    case "FAILED":
      return YarnApplication_State.FAILED;
    case 8:
    case "KILLED":
      return YarnApplication_State.KILLED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return YarnApplication_State.UNRECOGNIZED;
  }
}

export function yarnApplication_StateToJSON(object: YarnApplication_State): string {
  switch (object) {
    case YarnApplication_State.STATE_UNSPECIFIED:
      return "STATE_UNSPECIFIED";
    case YarnApplication_State.NEW:
      return "NEW";
    case YarnApplication_State.NEW_SAVING:
      return "NEW_SAVING";
    case YarnApplication_State.SUBMITTED:
      return "SUBMITTED";
    case YarnApplication_State.ACCEPTED:
      return "ACCEPTED";
    case YarnApplication_State.RUNNING:
      return "RUNNING";
    case YarnApplication_State.FINISHED:
      return "FINISHED";
    case YarnApplication_State.FAILED:
      return "FAILED";
    case YarnApplication_State.KILLED:
      return "KILLED";
    case YarnApplication_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** A Dataproc job resource. */
export interface Job {
  /**
   * Optional. The fully qualified reference to the job, which can be used to
   * obtain the equivalent REST path of the job resource. If this property
   * is not specified when a job is created, the server generates a
   * <code>job_id</code>.
   */
  reference:
    | JobReference
    | undefined;
  /**
   * Required. Job information, including how, when, and where to
   * run the job.
   */
  placement:
    | JobPlacement
    | undefined;
  /** Optional. Job is a Hadoop job. */
  hadoopJob?:
    | HadoopJob
    | undefined;
  /** Optional. Job is a Spark job. */
  sparkJob?:
    | SparkJob
    | undefined;
  /** Optional. Job is a PySpark job. */
  pysparkJob?:
    | PySparkJob
    | undefined;
  /** Optional. Job is a Hive job. */
  hiveJob?:
    | HiveJob
    | undefined;
  /** Optional. Job is a Pig job. */
  pigJob?:
    | PigJob
    | undefined;
  /** Optional. Job is a SparkR job. */
  sparkRJob?:
    | SparkRJob
    | undefined;
  /** Optional. Job is a SparkSql job. */
  sparkSqlJob?:
    | SparkSqlJob
    | undefined;
  /** Optional. Job is a Presto job. */
  prestoJob?:
    | PrestoJob
    | undefined;
  /** Optional. Job is a Trino job. */
  trinoJob?:
    | TrinoJob
    | undefined;
  /** Optional. Job is a Flink job. */
  flinkJob?:
    | FlinkJob
    | undefined;
  /**
   * Output only. The job status. Additional application-specific
   * status information might be contained in the <code>type_job</code>
   * and <code>yarn_applications</code> fields.
   */
  status:
    | JobStatus
    | undefined;
  /** Output only. The previous job status. */
  statusHistory: JobStatus[];
  /**
   * Output only. The collection of YARN applications spun up by this job.
   *
   * **Beta** Feature: This report is available for testing purposes only. It
   * might be changed before final release.
   */
  yarnApplications: YarnApplication[];
  /**
   * Output only. A URI pointing to the location of the stdout of the job's
   * driver program.
   */
  driverOutputResourceUri: string;
  /**
   * Output only. If present, the location of miscellaneous control files
   * which can be used as part of job setup and handling. If not present,
   * control files might be placed in the same location as `driver_output_uri`.
   */
  driverControlFilesUri: string;
  /**
   * Optional. The labels to associate with this job.
   * Label **keys** must contain 1 to 63 characters, and must conform to
   * [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).
   * Label **values** can be empty, but, if present, must contain 1 to 63
   * characters, and must conform to [RFC
   * 1035](https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be
   * associated with a job.
   */
  labels: { [key: string]: string };
  /** Optional. Job scheduling configuration. */
  scheduling:
    | JobScheduling
    | undefined;
  /**
   * Output only. A UUID that uniquely identifies a job within the project
   * over time. This is in contrast to a user-settable reference.job_id that
   * might be reused over time.
   */
  jobUuid: string;
  /**
   * Output only. Indicates whether the job is completed. If the value is
   * `false`, the job is still in progress. If `true`, the job is completed, and
   * `status.state` field will indicate if it was successful, failed,
   * or cancelled.
   */
  done: boolean;
  /** Optional. Driver scheduling configuration. */
  driverSchedulingConfig: DriverSchedulingConfig | undefined;
}

export interface Job_LabelsEntry {
  key: string;
  value: string;
}

/** Driver scheduling configuration. */
export interface DriverSchedulingConfig {
  /** Required. The amount of memory in MB the driver is requesting. */
  memoryMb: number;
  /** Required. The number of vCPUs the driver is requesting. */
  vcores: number;
}

/** Job scheduling options. */
export interface JobScheduling {
  /**
   * Optional. Maximum number of times per hour a driver can be restarted as
   * a result of driver exiting with non-zero code before job is
   * reported failed.
   *
   * A job might be reported as thrashing if the driver exits with a non-zero
   * code four times within a 10-minute window.
   *
   * Maximum value is 10.
   *
   * **Note:** This restartable job option is not supported in Dataproc
   * [workflow templates]
   * (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).
   */
  maxFailuresPerHour: number;
  /**
   * Optional. Maximum total number of times a driver can be restarted as a
   * result of the driver exiting with a non-zero code. After the maximum number
   * is reached, the job will be reported as failed.
   *
   * Maximum value is 240.
   *
   * **Note:** Currently, this restartable job option is
   * not supported in Dataproc
   * [workflow
   * templates](https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).
   */
  maxFailuresTotal: number;
}

/** A request to submit a job. */
export interface SubmitJobRequest {
  /**
   * Required. The ID of the Google Cloud Platform project that the job
   * belongs to.
   */
  projectId: string;
  /** Required. The Dataproc region in which to handle the request. */
  region: string;
  /** Required. The job resource. */
  job:
    | Job
    | undefined;
  /**
   * Optional. A unique id used to identify the request. If the server
   * receives two
   * [SubmitJobRequest](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.SubmitJobRequest)s
   * with the same id, then the second request will be ignored and the
   * first [Job][google.cloud.dataproc.v1.Job] created and stored in the backend
   * is returned.
   *
   * It is recommended to always set this value to a
   * [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).
   *
   * The id must contain only letters (a-z, A-Z), numbers (0-9),
   * underscores (_), and hyphens (-). The maximum length is 40 characters.
   */
  requestId: string;
}

/** Job Operation metadata. */
export interface JobMetadata {
  /** Output only. The job id. */
  jobId: string;
  /** Output only. Most recent job status. */
  status:
    | JobStatus
    | undefined;
  /** Output only. Operation type. */
  operationType: string;
  /** Output only. Job submission time. */
  startTime: Date | undefined;
}

/** A request to get the resource representation for a job in a project. */
export interface GetJobRequest {
  /**
   * Required. The ID of the Google Cloud Platform project that the job
   * belongs to.
   */
  projectId: string;
  /** Required. The Dataproc region in which to handle the request. */
  region: string;
  /** Required. The job ID. */
  jobId: string;
}

/** A request to list jobs in a project. */
export interface ListJobsRequest {
  /**
   * Required. The ID of the Google Cloud Platform project that the job
   * belongs to.
   */
  projectId: string;
  /** Required. The Dataproc region in which to handle the request. */
  region: string;
  /** Optional. The number of results to return in each response. */
  pageSize: number;
  /**
   * Optional. The page token, returned by a previous call, to request the
   * next page of results.
   */
  pageToken: string;
  /**
   * Optional. If set, the returned jobs list includes only jobs that were
   * submitted to the named cluster.
   */
  clusterName: string;
  /**
   * Optional. Specifies enumerated categories of jobs to list.
   * (default = match ALL jobs).
   *
   * If `filter` is provided, `jobStateMatcher` will be ignored.
   */
  jobStateMatcher: ListJobsRequest_JobStateMatcher;
  /**
   * Optional. A filter constraining the jobs to list. Filters are
   * case-sensitive and have the following syntax:
   *
   * [field = value] AND [field [= value]] ...
   *
   * where **field** is `status.state` or `labels.[KEY]`, and `[KEY]` is a label
   * key. **value** can be `*` to match all values.
   * `status.state` can be either `ACTIVE` or `NON_ACTIVE`.
   * Only the logical `AND` operator is supported; space-separated items are
   * treated as having an implicit `AND` operator.
   *
   * Example filter:
   *
   * status.state = ACTIVE AND labels.env = staging AND labels.starred = *
   */
  filter: string;
}

/** A matcher that specifies categories of job states. */
export enum ListJobsRequest_JobStateMatcher {
  /** ALL - Match all jobs, regardless of state. */
  ALL = 0,
  /**
   * ACTIVE - Only match jobs in non-terminal states: PENDING, RUNNING, or
   * CANCEL_PENDING.
   */
  ACTIVE = 1,
  /** NON_ACTIVE - Only match jobs in terminal states: CANCELLED, DONE, or ERROR. */
  NON_ACTIVE = 2,
  UNRECOGNIZED = -1,
}

export function listJobsRequest_JobStateMatcherFromJSON(object: any): ListJobsRequest_JobStateMatcher {
  switch (object) {
    case 0:
    case "ALL":
      return ListJobsRequest_JobStateMatcher.ALL;
    case 1:
    case "ACTIVE":
      return ListJobsRequest_JobStateMatcher.ACTIVE;
    case 2:
    case "NON_ACTIVE":
      return ListJobsRequest_JobStateMatcher.NON_ACTIVE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ListJobsRequest_JobStateMatcher.UNRECOGNIZED;
  }
}

export function listJobsRequest_JobStateMatcherToJSON(object: ListJobsRequest_JobStateMatcher): string {
  switch (object) {
    case ListJobsRequest_JobStateMatcher.ALL:
      return "ALL";
    case ListJobsRequest_JobStateMatcher.ACTIVE:
      return "ACTIVE";
    case ListJobsRequest_JobStateMatcher.NON_ACTIVE:
      return "NON_ACTIVE";
    case ListJobsRequest_JobStateMatcher.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** A request to update a job. */
export interface UpdateJobRequest {
  /**
   * Required. The ID of the Google Cloud Platform project that the job
   * belongs to.
   */
  projectId: string;
  /** Required. The Dataproc region in which to handle the request. */
  region: string;
  /** Required. The job ID. */
  jobId: string;
  /** Required. The changes to the job. */
  job:
    | Job
    | undefined;
  /**
   * Required. Specifies the path, relative to <code>Job</code>, of
   * the field to update. For example, to update the labels of a Job the
   * <code>update_mask</code> parameter would be specified as
   * <code>labels</code>, and the `PATCH` request body would specify the new
   * value. <strong>Note:</strong> Currently, <code>labels</code> is the only
   * field that can be updated.
   */
  updateMask: string[] | undefined;
}

/** A list of jobs in a project. */
export interface ListJobsResponse {
  /** Output only. Jobs list. */
  jobs: Job[];
  /**
   * Optional. This token is included in the response if there are more results
   * to fetch. To fetch additional results, provide this value as the
   * `page_token` in a subsequent <code>ListJobsRequest</code>.
   */
  nextPageToken: string;
  /**
   * Output only. List of jobs with
   * [kms_key][google.cloud.dataproc.v1.EncryptionConfig.kms_key]-encrypted
   * parameters that could not be decrypted. A response to a `jobs.get` request
   * may indicate the reason for the decryption failure for a specific job.
   */
  unreachable: string[];
}

/** A request to cancel a job. */
export interface CancelJobRequest {
  /**
   * Required. The ID of the Google Cloud Platform project that the job
   * belongs to.
   */
  projectId: string;
  /** Required. The Dataproc region in which to handle the request. */
  region: string;
  /** Required. The job ID. */
  jobId: string;
}

/** A request to delete a job. */
export interface DeleteJobRequest {
  /**
   * Required. The ID of the Google Cloud Platform project that the job
   * belongs to.
   */
  projectId: string;
  /** Required. The Dataproc region in which to handle the request. */
  region: string;
  /** Required. The job ID. */
  jobId: string;
}

function createBaseLoggingConfig(): LoggingConfig {
  return { driverLogLevels: {} };
}

export const LoggingConfig: MessageFns<LoggingConfig> = {
  encode(message: LoggingConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.driverLogLevels).forEach(([key, value]) => {
      LoggingConfig_DriverLogLevelsEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LoggingConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLoggingConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = LoggingConfig_DriverLogLevelsEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.driverLogLevels[entry2.key] = entry2.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LoggingConfig {
    return {
      driverLogLevels: isObject(object.driverLogLevels)
        ? Object.entries(object.driverLogLevels).reduce<{ [key: string]: LoggingConfig_Level }>((acc, [key, value]) => {
          acc[key] = loggingConfig_LevelFromJSON(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: LoggingConfig): unknown {
    const obj: any = {};
    if (message.driverLogLevels) {
      const entries = Object.entries(message.driverLogLevels);
      if (entries.length > 0) {
        obj.driverLogLevels = {};
        entries.forEach(([k, v]) => {
          obj.driverLogLevels[k] = loggingConfig_LevelToJSON(v);
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<LoggingConfig>): LoggingConfig {
    return LoggingConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LoggingConfig>): LoggingConfig {
    const message = createBaseLoggingConfig();
    message.driverLogLevels = Object.entries(object.driverLogLevels ?? {}).reduce<
      { [key: string]: LoggingConfig_Level }
    >((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = value as LoggingConfig_Level;
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseLoggingConfig_DriverLogLevelsEntry(): LoggingConfig_DriverLogLevelsEntry {
  return { key: "", value: 0 };
}

export const LoggingConfig_DriverLogLevelsEntry: MessageFns<LoggingConfig_DriverLogLevelsEntry> = {
  encode(message: LoggingConfig_DriverLogLevelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== 0) {
      writer.uint32(16).int32(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LoggingConfig_DriverLogLevelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLoggingConfig_DriverLogLevelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.value = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LoggingConfig_DriverLogLevelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? loggingConfig_LevelFromJSON(object.value) : 0,
    };
  },

  toJSON(message: LoggingConfig_DriverLogLevelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== 0) {
      obj.value = loggingConfig_LevelToJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<LoggingConfig_DriverLogLevelsEntry>): LoggingConfig_DriverLogLevelsEntry {
    return LoggingConfig_DriverLogLevelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LoggingConfig_DriverLogLevelsEntry>): LoggingConfig_DriverLogLevelsEntry {
    const message = createBaseLoggingConfig_DriverLogLevelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? 0;
    return message;
  },
};

function createBaseHadoopJob(): HadoopJob {
  return {
    mainJarFileUri: undefined,
    mainClass: undefined,
    args: [],
    jarFileUris: [],
    fileUris: [],
    archiveUris: [],
    properties: {},
    loggingConfig: undefined,
  };
}

export const HadoopJob: MessageFns<HadoopJob> = {
  encode(message: HadoopJob, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.mainJarFileUri !== undefined) {
      writer.uint32(10).string(message.mainJarFileUri);
    }
    if (message.mainClass !== undefined) {
      writer.uint32(18).string(message.mainClass);
    }
    for (const v of message.args) {
      writer.uint32(26).string(v!);
    }
    for (const v of message.jarFileUris) {
      writer.uint32(34).string(v!);
    }
    for (const v of message.fileUris) {
      writer.uint32(42).string(v!);
    }
    for (const v of message.archiveUris) {
      writer.uint32(50).string(v!);
    }
    Object.entries(message.properties).forEach(([key, value]) => {
      HadoopJob_PropertiesEntry.encode({ key: key as any, value }, writer.uint32(58).fork()).join();
    });
    if (message.loggingConfig !== undefined) {
      LoggingConfig.encode(message.loggingConfig, writer.uint32(66).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): HadoopJob {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseHadoopJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.mainJarFileUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.mainClass = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.args.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.jarFileUris.push(reader.string());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.fileUris.push(reader.string());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.archiveUris.push(reader.string());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          const entry7 = HadoopJob_PropertiesEntry.decode(reader, reader.uint32());
          if (entry7.value !== undefined) {
            message.properties[entry7.key] = entry7.value;
          }
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.loggingConfig = LoggingConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): HadoopJob {
    return {
      mainJarFileUri: isSet(object.mainJarFileUri) ? globalThis.String(object.mainJarFileUri) : undefined,
      mainClass: isSet(object.mainClass) ? globalThis.String(object.mainClass) : undefined,
      args: globalThis.Array.isArray(object?.args) ? object.args.map((e: any) => globalThis.String(e)) : [],
      jarFileUris: globalThis.Array.isArray(object?.jarFileUris)
        ? object.jarFileUris.map((e: any) => globalThis.String(e))
        : [],
      fileUris: globalThis.Array.isArray(object?.fileUris) ? object.fileUris.map((e: any) => globalThis.String(e)) : [],
      archiveUris: globalThis.Array.isArray(object?.archiveUris)
        ? object.archiveUris.map((e: any) => globalThis.String(e))
        : [],
      properties: isObject(object.properties)
        ? Object.entries(object.properties).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      loggingConfig: isSet(object.loggingConfig) ? LoggingConfig.fromJSON(object.loggingConfig) : undefined,
    };
  },

  toJSON(message: HadoopJob): unknown {
    const obj: any = {};
    if (message.mainJarFileUri !== undefined) {
      obj.mainJarFileUri = message.mainJarFileUri;
    }
    if (message.mainClass !== undefined) {
      obj.mainClass = message.mainClass;
    }
    if (message.args?.length) {
      obj.args = message.args;
    }
    if (message.jarFileUris?.length) {
      obj.jarFileUris = message.jarFileUris;
    }
    if (message.fileUris?.length) {
      obj.fileUris = message.fileUris;
    }
    if (message.archiveUris?.length) {
      obj.archiveUris = message.archiveUris;
    }
    if (message.properties) {
      const entries = Object.entries(message.properties);
      if (entries.length > 0) {
        obj.properties = {};
        entries.forEach(([k, v]) => {
          obj.properties[k] = v;
        });
      }
    }
    if (message.loggingConfig !== undefined) {
      obj.loggingConfig = LoggingConfig.toJSON(message.loggingConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<HadoopJob>): HadoopJob {
    return HadoopJob.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<HadoopJob>): HadoopJob {
    const message = createBaseHadoopJob();
    message.mainJarFileUri = object.mainJarFileUri ?? undefined;
    message.mainClass = object.mainClass ?? undefined;
    message.args = object.args?.map((e) => e) || [];
    message.jarFileUris = object.jarFileUris?.map((e) => e) || [];
    message.fileUris = object.fileUris?.map((e) => e) || [];
    message.archiveUris = object.archiveUris?.map((e) => e) || [];
    message.properties = Object.entries(object.properties ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.loggingConfig = (object.loggingConfig !== undefined && object.loggingConfig !== null)
      ? LoggingConfig.fromPartial(object.loggingConfig)
      : undefined;
    return message;
  },
};

function createBaseHadoopJob_PropertiesEntry(): HadoopJob_PropertiesEntry {
  return { key: "", value: "" };
}

export const HadoopJob_PropertiesEntry: MessageFns<HadoopJob_PropertiesEntry> = {
  encode(message: HadoopJob_PropertiesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): HadoopJob_PropertiesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseHadoopJob_PropertiesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): HadoopJob_PropertiesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: HadoopJob_PropertiesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<HadoopJob_PropertiesEntry>): HadoopJob_PropertiesEntry {
    return HadoopJob_PropertiesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<HadoopJob_PropertiesEntry>): HadoopJob_PropertiesEntry {
    const message = createBaseHadoopJob_PropertiesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseSparkJob(): SparkJob {
  return {
    mainJarFileUri: undefined,
    mainClass: undefined,
    args: [],
    jarFileUris: [],
    fileUris: [],
    archiveUris: [],
    properties: {},
    loggingConfig: undefined,
  };
}

export const SparkJob: MessageFns<SparkJob> = {
  encode(message: SparkJob, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.mainJarFileUri !== undefined) {
      writer.uint32(10).string(message.mainJarFileUri);
    }
    if (message.mainClass !== undefined) {
      writer.uint32(18).string(message.mainClass);
    }
    for (const v of message.args) {
      writer.uint32(26).string(v!);
    }
    for (const v of message.jarFileUris) {
      writer.uint32(34).string(v!);
    }
    for (const v of message.fileUris) {
      writer.uint32(42).string(v!);
    }
    for (const v of message.archiveUris) {
      writer.uint32(50).string(v!);
    }
    Object.entries(message.properties).forEach(([key, value]) => {
      SparkJob_PropertiesEntry.encode({ key: key as any, value }, writer.uint32(58).fork()).join();
    });
    if (message.loggingConfig !== undefined) {
      LoggingConfig.encode(message.loggingConfig, writer.uint32(66).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SparkJob {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSparkJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.mainJarFileUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.mainClass = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.args.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.jarFileUris.push(reader.string());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.fileUris.push(reader.string());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.archiveUris.push(reader.string());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          const entry7 = SparkJob_PropertiesEntry.decode(reader, reader.uint32());
          if (entry7.value !== undefined) {
            message.properties[entry7.key] = entry7.value;
          }
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.loggingConfig = LoggingConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SparkJob {
    return {
      mainJarFileUri: isSet(object.mainJarFileUri) ? globalThis.String(object.mainJarFileUri) : undefined,
      mainClass: isSet(object.mainClass) ? globalThis.String(object.mainClass) : undefined,
      args: globalThis.Array.isArray(object?.args) ? object.args.map((e: any) => globalThis.String(e)) : [],
      jarFileUris: globalThis.Array.isArray(object?.jarFileUris)
        ? object.jarFileUris.map((e: any) => globalThis.String(e))
        : [],
      fileUris: globalThis.Array.isArray(object?.fileUris) ? object.fileUris.map((e: any) => globalThis.String(e)) : [],
      archiveUris: globalThis.Array.isArray(object?.archiveUris)
        ? object.archiveUris.map((e: any) => globalThis.String(e))
        : [],
      properties: isObject(object.properties)
        ? Object.entries(object.properties).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      loggingConfig: isSet(object.loggingConfig) ? LoggingConfig.fromJSON(object.loggingConfig) : undefined,
    };
  },

  toJSON(message: SparkJob): unknown {
    const obj: any = {};
    if (message.mainJarFileUri !== undefined) {
      obj.mainJarFileUri = message.mainJarFileUri;
    }
    if (message.mainClass !== undefined) {
      obj.mainClass = message.mainClass;
    }
    if (message.args?.length) {
      obj.args = message.args;
    }
    if (message.jarFileUris?.length) {
      obj.jarFileUris = message.jarFileUris;
    }
    if (message.fileUris?.length) {
      obj.fileUris = message.fileUris;
    }
    if (message.archiveUris?.length) {
      obj.archiveUris = message.archiveUris;
    }
    if (message.properties) {
      const entries = Object.entries(message.properties);
      if (entries.length > 0) {
        obj.properties = {};
        entries.forEach(([k, v]) => {
          obj.properties[k] = v;
        });
      }
    }
    if (message.loggingConfig !== undefined) {
      obj.loggingConfig = LoggingConfig.toJSON(message.loggingConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<SparkJob>): SparkJob {
    return SparkJob.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SparkJob>): SparkJob {
    const message = createBaseSparkJob();
    message.mainJarFileUri = object.mainJarFileUri ?? undefined;
    message.mainClass = object.mainClass ?? undefined;
    message.args = object.args?.map((e) => e) || [];
    message.jarFileUris = object.jarFileUris?.map((e) => e) || [];
    message.fileUris = object.fileUris?.map((e) => e) || [];
    message.archiveUris = object.archiveUris?.map((e) => e) || [];
    message.properties = Object.entries(object.properties ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.loggingConfig = (object.loggingConfig !== undefined && object.loggingConfig !== null)
      ? LoggingConfig.fromPartial(object.loggingConfig)
      : undefined;
    return message;
  },
};

function createBaseSparkJob_PropertiesEntry(): SparkJob_PropertiesEntry {
  return { key: "", value: "" };
}

export const SparkJob_PropertiesEntry: MessageFns<SparkJob_PropertiesEntry> = {
  encode(message: SparkJob_PropertiesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SparkJob_PropertiesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSparkJob_PropertiesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SparkJob_PropertiesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: SparkJob_PropertiesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<SparkJob_PropertiesEntry>): SparkJob_PropertiesEntry {
    return SparkJob_PropertiesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SparkJob_PropertiesEntry>): SparkJob_PropertiesEntry {
    const message = createBaseSparkJob_PropertiesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBasePySparkJob(): PySparkJob {
  return {
    mainPythonFileUri: "",
    args: [],
    pythonFileUris: [],
    jarFileUris: [],
    fileUris: [],
    archiveUris: [],
    properties: {},
    loggingConfig: undefined,
  };
}

export const PySparkJob: MessageFns<PySparkJob> = {
  encode(message: PySparkJob, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.mainPythonFileUri !== "") {
      writer.uint32(10).string(message.mainPythonFileUri);
    }
    for (const v of message.args) {
      writer.uint32(18).string(v!);
    }
    for (const v of message.pythonFileUris) {
      writer.uint32(26).string(v!);
    }
    for (const v of message.jarFileUris) {
      writer.uint32(34).string(v!);
    }
    for (const v of message.fileUris) {
      writer.uint32(42).string(v!);
    }
    for (const v of message.archiveUris) {
      writer.uint32(50).string(v!);
    }
    Object.entries(message.properties).forEach(([key, value]) => {
      PySparkJob_PropertiesEntry.encode({ key: key as any, value }, writer.uint32(58).fork()).join();
    });
    if (message.loggingConfig !== undefined) {
      LoggingConfig.encode(message.loggingConfig, writer.uint32(66).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PySparkJob {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePySparkJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.mainPythonFileUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.args.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pythonFileUris.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.jarFileUris.push(reader.string());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.fileUris.push(reader.string());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.archiveUris.push(reader.string());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          const entry7 = PySparkJob_PropertiesEntry.decode(reader, reader.uint32());
          if (entry7.value !== undefined) {
            message.properties[entry7.key] = entry7.value;
          }
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.loggingConfig = LoggingConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PySparkJob {
    return {
      mainPythonFileUri: isSet(object.mainPythonFileUri) ? globalThis.String(object.mainPythonFileUri) : "",
      args: globalThis.Array.isArray(object?.args) ? object.args.map((e: any) => globalThis.String(e)) : [],
      pythonFileUris: globalThis.Array.isArray(object?.pythonFileUris)
        ? object.pythonFileUris.map((e: any) => globalThis.String(e))
        : [],
      jarFileUris: globalThis.Array.isArray(object?.jarFileUris)
        ? object.jarFileUris.map((e: any) => globalThis.String(e))
        : [],
      fileUris: globalThis.Array.isArray(object?.fileUris) ? object.fileUris.map((e: any) => globalThis.String(e)) : [],
      archiveUris: globalThis.Array.isArray(object?.archiveUris)
        ? object.archiveUris.map((e: any) => globalThis.String(e))
        : [],
      properties: isObject(object.properties)
        ? Object.entries(object.properties).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      loggingConfig: isSet(object.loggingConfig) ? LoggingConfig.fromJSON(object.loggingConfig) : undefined,
    };
  },

  toJSON(message: PySparkJob): unknown {
    const obj: any = {};
    if (message.mainPythonFileUri !== "") {
      obj.mainPythonFileUri = message.mainPythonFileUri;
    }
    if (message.args?.length) {
      obj.args = message.args;
    }
    if (message.pythonFileUris?.length) {
      obj.pythonFileUris = message.pythonFileUris;
    }
    if (message.jarFileUris?.length) {
      obj.jarFileUris = message.jarFileUris;
    }
    if (message.fileUris?.length) {
      obj.fileUris = message.fileUris;
    }
    if (message.archiveUris?.length) {
      obj.archiveUris = message.archiveUris;
    }
    if (message.properties) {
      const entries = Object.entries(message.properties);
      if (entries.length > 0) {
        obj.properties = {};
        entries.forEach(([k, v]) => {
          obj.properties[k] = v;
        });
      }
    }
    if (message.loggingConfig !== undefined) {
      obj.loggingConfig = LoggingConfig.toJSON(message.loggingConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<PySparkJob>): PySparkJob {
    return PySparkJob.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PySparkJob>): PySparkJob {
    const message = createBasePySparkJob();
    message.mainPythonFileUri = object.mainPythonFileUri ?? "";
    message.args = object.args?.map((e) => e) || [];
    message.pythonFileUris = object.pythonFileUris?.map((e) => e) || [];
    message.jarFileUris = object.jarFileUris?.map((e) => e) || [];
    message.fileUris = object.fileUris?.map((e) => e) || [];
    message.archiveUris = object.archiveUris?.map((e) => e) || [];
    message.properties = Object.entries(object.properties ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.loggingConfig = (object.loggingConfig !== undefined && object.loggingConfig !== null)
      ? LoggingConfig.fromPartial(object.loggingConfig)
      : undefined;
    return message;
  },
};

function createBasePySparkJob_PropertiesEntry(): PySparkJob_PropertiesEntry {
  return { key: "", value: "" };
}

export const PySparkJob_PropertiesEntry: MessageFns<PySparkJob_PropertiesEntry> = {
  encode(message: PySparkJob_PropertiesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PySparkJob_PropertiesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePySparkJob_PropertiesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PySparkJob_PropertiesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: PySparkJob_PropertiesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<PySparkJob_PropertiesEntry>): PySparkJob_PropertiesEntry {
    return PySparkJob_PropertiesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PySparkJob_PropertiesEntry>): PySparkJob_PropertiesEntry {
    const message = createBasePySparkJob_PropertiesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseQueryList(): QueryList {
  return { queries: [] };
}

export const QueryList: MessageFns<QueryList> = {
  encode(message: QueryList, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.queries) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryList {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryList();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.queries.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryList {
    return {
      queries: globalThis.Array.isArray(object?.queries) ? object.queries.map((e: any) => globalThis.String(e)) : [],
    };
  },

  toJSON(message: QueryList): unknown {
    const obj: any = {};
    if (message.queries?.length) {
      obj.queries = message.queries;
    }
    return obj;
  },

  create(base?: DeepPartial<QueryList>): QueryList {
    return QueryList.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<QueryList>): QueryList {
    const message = createBaseQueryList();
    message.queries = object.queries?.map((e) => e) || [];
    return message;
  },
};

function createBaseHiveJob(): HiveJob {
  return {
    queryFileUri: undefined,
    queryList: undefined,
    continueOnFailure: false,
    scriptVariables: {},
    properties: {},
    jarFileUris: [],
  };
}

export const HiveJob: MessageFns<HiveJob> = {
  encode(message: HiveJob, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.queryFileUri !== undefined) {
      writer.uint32(10).string(message.queryFileUri);
    }
    if (message.queryList !== undefined) {
      QueryList.encode(message.queryList, writer.uint32(18).fork()).join();
    }
    if (message.continueOnFailure !== false) {
      writer.uint32(24).bool(message.continueOnFailure);
    }
    Object.entries(message.scriptVariables).forEach(([key, value]) => {
      HiveJob_ScriptVariablesEntry.encode({ key: key as any, value }, writer.uint32(34).fork()).join();
    });
    Object.entries(message.properties).forEach(([key, value]) => {
      HiveJob_PropertiesEntry.encode({ key: key as any, value }, writer.uint32(42).fork()).join();
    });
    for (const v of message.jarFileUris) {
      writer.uint32(50).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): HiveJob {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseHiveJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.queryFileUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.queryList = QueryList.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.continueOnFailure = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          const entry4 = HiveJob_ScriptVariablesEntry.decode(reader, reader.uint32());
          if (entry4.value !== undefined) {
            message.scriptVariables[entry4.key] = entry4.value;
          }
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          const entry5 = HiveJob_PropertiesEntry.decode(reader, reader.uint32());
          if (entry5.value !== undefined) {
            message.properties[entry5.key] = entry5.value;
          }
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.jarFileUris.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): HiveJob {
    return {
      queryFileUri: isSet(object.queryFileUri) ? globalThis.String(object.queryFileUri) : undefined,
      queryList: isSet(object.queryList) ? QueryList.fromJSON(object.queryList) : undefined,
      continueOnFailure: isSet(object.continueOnFailure) ? globalThis.Boolean(object.continueOnFailure) : false,
      scriptVariables: isObject(object.scriptVariables)
        ? Object.entries(object.scriptVariables).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      properties: isObject(object.properties)
        ? Object.entries(object.properties).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      jarFileUris: globalThis.Array.isArray(object?.jarFileUris)
        ? object.jarFileUris.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: HiveJob): unknown {
    const obj: any = {};
    if (message.queryFileUri !== undefined) {
      obj.queryFileUri = message.queryFileUri;
    }
    if (message.queryList !== undefined) {
      obj.queryList = QueryList.toJSON(message.queryList);
    }
    if (message.continueOnFailure !== false) {
      obj.continueOnFailure = message.continueOnFailure;
    }
    if (message.scriptVariables) {
      const entries = Object.entries(message.scriptVariables);
      if (entries.length > 0) {
        obj.scriptVariables = {};
        entries.forEach(([k, v]) => {
          obj.scriptVariables[k] = v;
        });
      }
    }
    if (message.properties) {
      const entries = Object.entries(message.properties);
      if (entries.length > 0) {
        obj.properties = {};
        entries.forEach(([k, v]) => {
          obj.properties[k] = v;
        });
      }
    }
    if (message.jarFileUris?.length) {
      obj.jarFileUris = message.jarFileUris;
    }
    return obj;
  },

  create(base?: DeepPartial<HiveJob>): HiveJob {
    return HiveJob.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<HiveJob>): HiveJob {
    const message = createBaseHiveJob();
    message.queryFileUri = object.queryFileUri ?? undefined;
    message.queryList = (object.queryList !== undefined && object.queryList !== null)
      ? QueryList.fromPartial(object.queryList)
      : undefined;
    message.continueOnFailure = object.continueOnFailure ?? false;
    message.scriptVariables = Object.entries(object.scriptVariables ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.properties = Object.entries(object.properties ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.jarFileUris = object.jarFileUris?.map((e) => e) || [];
    return message;
  },
};

function createBaseHiveJob_ScriptVariablesEntry(): HiveJob_ScriptVariablesEntry {
  return { key: "", value: "" };
}

export const HiveJob_ScriptVariablesEntry: MessageFns<HiveJob_ScriptVariablesEntry> = {
  encode(message: HiveJob_ScriptVariablesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): HiveJob_ScriptVariablesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseHiveJob_ScriptVariablesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): HiveJob_ScriptVariablesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: HiveJob_ScriptVariablesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<HiveJob_ScriptVariablesEntry>): HiveJob_ScriptVariablesEntry {
    return HiveJob_ScriptVariablesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<HiveJob_ScriptVariablesEntry>): HiveJob_ScriptVariablesEntry {
    const message = createBaseHiveJob_ScriptVariablesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseHiveJob_PropertiesEntry(): HiveJob_PropertiesEntry {
  return { key: "", value: "" };
}

export const HiveJob_PropertiesEntry: MessageFns<HiveJob_PropertiesEntry> = {
  encode(message: HiveJob_PropertiesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): HiveJob_PropertiesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseHiveJob_PropertiesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): HiveJob_PropertiesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: HiveJob_PropertiesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<HiveJob_PropertiesEntry>): HiveJob_PropertiesEntry {
    return HiveJob_PropertiesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<HiveJob_PropertiesEntry>): HiveJob_PropertiesEntry {
    const message = createBaseHiveJob_PropertiesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseSparkSqlJob(): SparkSqlJob {
  return {
    queryFileUri: undefined,
    queryList: undefined,
    scriptVariables: {},
    properties: {},
    jarFileUris: [],
    loggingConfig: undefined,
  };
}

export const SparkSqlJob: MessageFns<SparkSqlJob> = {
  encode(message: SparkSqlJob, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.queryFileUri !== undefined) {
      writer.uint32(10).string(message.queryFileUri);
    }
    if (message.queryList !== undefined) {
      QueryList.encode(message.queryList, writer.uint32(18).fork()).join();
    }
    Object.entries(message.scriptVariables).forEach(([key, value]) => {
      SparkSqlJob_ScriptVariablesEntry.encode({ key: key as any, value }, writer.uint32(26).fork()).join();
    });
    Object.entries(message.properties).forEach(([key, value]) => {
      SparkSqlJob_PropertiesEntry.encode({ key: key as any, value }, writer.uint32(34).fork()).join();
    });
    for (const v of message.jarFileUris) {
      writer.uint32(450).string(v!);
    }
    if (message.loggingConfig !== undefined) {
      LoggingConfig.encode(message.loggingConfig, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SparkSqlJob {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSparkSqlJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.queryFileUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.queryList = QueryList.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          const entry3 = SparkSqlJob_ScriptVariablesEntry.decode(reader, reader.uint32());
          if (entry3.value !== undefined) {
            message.scriptVariables[entry3.key] = entry3.value;
          }
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          const entry4 = SparkSqlJob_PropertiesEntry.decode(reader, reader.uint32());
          if (entry4.value !== undefined) {
            message.properties[entry4.key] = entry4.value;
          }
          continue;
        case 56:
          if (tag !== 450) {
            break;
          }

          message.jarFileUris.push(reader.string());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.loggingConfig = LoggingConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SparkSqlJob {
    return {
      queryFileUri: isSet(object.queryFileUri) ? globalThis.String(object.queryFileUri) : undefined,
      queryList: isSet(object.queryList) ? QueryList.fromJSON(object.queryList) : undefined,
      scriptVariables: isObject(object.scriptVariables)
        ? Object.entries(object.scriptVariables).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      properties: isObject(object.properties)
        ? Object.entries(object.properties).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      jarFileUris: globalThis.Array.isArray(object?.jarFileUris)
        ? object.jarFileUris.map((e: any) => globalThis.String(e))
        : [],
      loggingConfig: isSet(object.loggingConfig) ? LoggingConfig.fromJSON(object.loggingConfig) : undefined,
    };
  },

  toJSON(message: SparkSqlJob): unknown {
    const obj: any = {};
    if (message.queryFileUri !== undefined) {
      obj.queryFileUri = message.queryFileUri;
    }
    if (message.queryList !== undefined) {
      obj.queryList = QueryList.toJSON(message.queryList);
    }
    if (message.scriptVariables) {
      const entries = Object.entries(message.scriptVariables);
      if (entries.length > 0) {
        obj.scriptVariables = {};
        entries.forEach(([k, v]) => {
          obj.scriptVariables[k] = v;
        });
      }
    }
    if (message.properties) {
      const entries = Object.entries(message.properties);
      if (entries.length > 0) {
        obj.properties = {};
        entries.forEach(([k, v]) => {
          obj.properties[k] = v;
        });
      }
    }
    if (message.jarFileUris?.length) {
      obj.jarFileUris = message.jarFileUris;
    }
    if (message.loggingConfig !== undefined) {
      obj.loggingConfig = LoggingConfig.toJSON(message.loggingConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<SparkSqlJob>): SparkSqlJob {
    return SparkSqlJob.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SparkSqlJob>): SparkSqlJob {
    const message = createBaseSparkSqlJob();
    message.queryFileUri = object.queryFileUri ?? undefined;
    message.queryList = (object.queryList !== undefined && object.queryList !== null)
      ? QueryList.fromPartial(object.queryList)
      : undefined;
    message.scriptVariables = Object.entries(object.scriptVariables ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.properties = Object.entries(object.properties ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.jarFileUris = object.jarFileUris?.map((e) => e) || [];
    message.loggingConfig = (object.loggingConfig !== undefined && object.loggingConfig !== null)
      ? LoggingConfig.fromPartial(object.loggingConfig)
      : undefined;
    return message;
  },
};

function createBaseSparkSqlJob_ScriptVariablesEntry(): SparkSqlJob_ScriptVariablesEntry {
  return { key: "", value: "" };
}

export const SparkSqlJob_ScriptVariablesEntry: MessageFns<SparkSqlJob_ScriptVariablesEntry> = {
  encode(message: SparkSqlJob_ScriptVariablesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SparkSqlJob_ScriptVariablesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSparkSqlJob_ScriptVariablesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SparkSqlJob_ScriptVariablesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: SparkSqlJob_ScriptVariablesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<SparkSqlJob_ScriptVariablesEntry>): SparkSqlJob_ScriptVariablesEntry {
    return SparkSqlJob_ScriptVariablesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SparkSqlJob_ScriptVariablesEntry>): SparkSqlJob_ScriptVariablesEntry {
    const message = createBaseSparkSqlJob_ScriptVariablesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseSparkSqlJob_PropertiesEntry(): SparkSqlJob_PropertiesEntry {
  return { key: "", value: "" };
}

export const SparkSqlJob_PropertiesEntry: MessageFns<SparkSqlJob_PropertiesEntry> = {
  encode(message: SparkSqlJob_PropertiesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SparkSqlJob_PropertiesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSparkSqlJob_PropertiesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SparkSqlJob_PropertiesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: SparkSqlJob_PropertiesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<SparkSqlJob_PropertiesEntry>): SparkSqlJob_PropertiesEntry {
    return SparkSqlJob_PropertiesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SparkSqlJob_PropertiesEntry>): SparkSqlJob_PropertiesEntry {
    const message = createBaseSparkSqlJob_PropertiesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBasePigJob(): PigJob {
  return {
    queryFileUri: undefined,
    queryList: undefined,
    continueOnFailure: false,
    scriptVariables: {},
    properties: {},
    jarFileUris: [],
    loggingConfig: undefined,
  };
}

export const PigJob: MessageFns<PigJob> = {
  encode(message: PigJob, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.queryFileUri !== undefined) {
      writer.uint32(10).string(message.queryFileUri);
    }
    if (message.queryList !== undefined) {
      QueryList.encode(message.queryList, writer.uint32(18).fork()).join();
    }
    if (message.continueOnFailure !== false) {
      writer.uint32(24).bool(message.continueOnFailure);
    }
    Object.entries(message.scriptVariables).forEach(([key, value]) => {
      PigJob_ScriptVariablesEntry.encode({ key: key as any, value }, writer.uint32(34).fork()).join();
    });
    Object.entries(message.properties).forEach(([key, value]) => {
      PigJob_PropertiesEntry.encode({ key: key as any, value }, writer.uint32(42).fork()).join();
    });
    for (const v of message.jarFileUris) {
      writer.uint32(50).string(v!);
    }
    if (message.loggingConfig !== undefined) {
      LoggingConfig.encode(message.loggingConfig, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PigJob {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePigJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.queryFileUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.queryList = QueryList.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.continueOnFailure = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          const entry4 = PigJob_ScriptVariablesEntry.decode(reader, reader.uint32());
          if (entry4.value !== undefined) {
            message.scriptVariables[entry4.key] = entry4.value;
          }
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          const entry5 = PigJob_PropertiesEntry.decode(reader, reader.uint32());
          if (entry5.value !== undefined) {
            message.properties[entry5.key] = entry5.value;
          }
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.jarFileUris.push(reader.string());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.loggingConfig = LoggingConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PigJob {
    return {
      queryFileUri: isSet(object.queryFileUri) ? globalThis.String(object.queryFileUri) : undefined,
      queryList: isSet(object.queryList) ? QueryList.fromJSON(object.queryList) : undefined,
      continueOnFailure: isSet(object.continueOnFailure) ? globalThis.Boolean(object.continueOnFailure) : false,
      scriptVariables: isObject(object.scriptVariables)
        ? Object.entries(object.scriptVariables).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      properties: isObject(object.properties)
        ? Object.entries(object.properties).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      jarFileUris: globalThis.Array.isArray(object?.jarFileUris)
        ? object.jarFileUris.map((e: any) => globalThis.String(e))
        : [],
      loggingConfig: isSet(object.loggingConfig) ? LoggingConfig.fromJSON(object.loggingConfig) : undefined,
    };
  },

  toJSON(message: PigJob): unknown {
    const obj: any = {};
    if (message.queryFileUri !== undefined) {
      obj.queryFileUri = message.queryFileUri;
    }
    if (message.queryList !== undefined) {
      obj.queryList = QueryList.toJSON(message.queryList);
    }
    if (message.continueOnFailure !== false) {
      obj.continueOnFailure = message.continueOnFailure;
    }
    if (message.scriptVariables) {
      const entries = Object.entries(message.scriptVariables);
      if (entries.length > 0) {
        obj.scriptVariables = {};
        entries.forEach(([k, v]) => {
          obj.scriptVariables[k] = v;
        });
      }
    }
    if (message.properties) {
      const entries = Object.entries(message.properties);
      if (entries.length > 0) {
        obj.properties = {};
        entries.forEach(([k, v]) => {
          obj.properties[k] = v;
        });
      }
    }
    if (message.jarFileUris?.length) {
      obj.jarFileUris = message.jarFileUris;
    }
    if (message.loggingConfig !== undefined) {
      obj.loggingConfig = LoggingConfig.toJSON(message.loggingConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<PigJob>): PigJob {
    return PigJob.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PigJob>): PigJob {
    const message = createBasePigJob();
    message.queryFileUri = object.queryFileUri ?? undefined;
    message.queryList = (object.queryList !== undefined && object.queryList !== null)
      ? QueryList.fromPartial(object.queryList)
      : undefined;
    message.continueOnFailure = object.continueOnFailure ?? false;
    message.scriptVariables = Object.entries(object.scriptVariables ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.properties = Object.entries(object.properties ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.jarFileUris = object.jarFileUris?.map((e) => e) || [];
    message.loggingConfig = (object.loggingConfig !== undefined && object.loggingConfig !== null)
      ? LoggingConfig.fromPartial(object.loggingConfig)
      : undefined;
    return message;
  },
};

function createBasePigJob_ScriptVariablesEntry(): PigJob_ScriptVariablesEntry {
  return { key: "", value: "" };
}

export const PigJob_ScriptVariablesEntry: MessageFns<PigJob_ScriptVariablesEntry> = {
  encode(message: PigJob_ScriptVariablesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PigJob_ScriptVariablesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePigJob_ScriptVariablesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PigJob_ScriptVariablesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: PigJob_ScriptVariablesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<PigJob_ScriptVariablesEntry>): PigJob_ScriptVariablesEntry {
    return PigJob_ScriptVariablesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PigJob_ScriptVariablesEntry>): PigJob_ScriptVariablesEntry {
    const message = createBasePigJob_ScriptVariablesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBasePigJob_PropertiesEntry(): PigJob_PropertiesEntry {
  return { key: "", value: "" };
}

export const PigJob_PropertiesEntry: MessageFns<PigJob_PropertiesEntry> = {
  encode(message: PigJob_PropertiesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PigJob_PropertiesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePigJob_PropertiesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PigJob_PropertiesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: PigJob_PropertiesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<PigJob_PropertiesEntry>): PigJob_PropertiesEntry {
    return PigJob_PropertiesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PigJob_PropertiesEntry>): PigJob_PropertiesEntry {
    const message = createBasePigJob_PropertiesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseSparkRJob(): SparkRJob {
  return { mainRFileUri: "", args: [], fileUris: [], archiveUris: [], properties: {}, loggingConfig: undefined };
}

export const SparkRJob: MessageFns<SparkRJob> = {
  encode(message: SparkRJob, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.mainRFileUri !== "") {
      writer.uint32(10).string(message.mainRFileUri);
    }
    for (const v of message.args) {
      writer.uint32(18).string(v!);
    }
    for (const v of message.fileUris) {
      writer.uint32(26).string(v!);
    }
    for (const v of message.archiveUris) {
      writer.uint32(34).string(v!);
    }
    Object.entries(message.properties).forEach(([key, value]) => {
      SparkRJob_PropertiesEntry.encode({ key: key as any, value }, writer.uint32(42).fork()).join();
    });
    if (message.loggingConfig !== undefined) {
      LoggingConfig.encode(message.loggingConfig, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SparkRJob {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSparkRJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.mainRFileUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.args.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.fileUris.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.archiveUris.push(reader.string());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          const entry5 = SparkRJob_PropertiesEntry.decode(reader, reader.uint32());
          if (entry5.value !== undefined) {
            message.properties[entry5.key] = entry5.value;
          }
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.loggingConfig = LoggingConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SparkRJob {
    return {
      mainRFileUri: isSet(object.mainRFileUri) ? globalThis.String(object.mainRFileUri) : "",
      args: globalThis.Array.isArray(object?.args) ? object.args.map((e: any) => globalThis.String(e)) : [],
      fileUris: globalThis.Array.isArray(object?.fileUris) ? object.fileUris.map((e: any) => globalThis.String(e)) : [],
      archiveUris: globalThis.Array.isArray(object?.archiveUris)
        ? object.archiveUris.map((e: any) => globalThis.String(e))
        : [],
      properties: isObject(object.properties)
        ? Object.entries(object.properties).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      loggingConfig: isSet(object.loggingConfig) ? LoggingConfig.fromJSON(object.loggingConfig) : undefined,
    };
  },

  toJSON(message: SparkRJob): unknown {
    const obj: any = {};
    if (message.mainRFileUri !== "") {
      obj.mainRFileUri = message.mainRFileUri;
    }
    if (message.args?.length) {
      obj.args = message.args;
    }
    if (message.fileUris?.length) {
      obj.fileUris = message.fileUris;
    }
    if (message.archiveUris?.length) {
      obj.archiveUris = message.archiveUris;
    }
    if (message.properties) {
      const entries = Object.entries(message.properties);
      if (entries.length > 0) {
        obj.properties = {};
        entries.forEach(([k, v]) => {
          obj.properties[k] = v;
        });
      }
    }
    if (message.loggingConfig !== undefined) {
      obj.loggingConfig = LoggingConfig.toJSON(message.loggingConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<SparkRJob>): SparkRJob {
    return SparkRJob.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SparkRJob>): SparkRJob {
    const message = createBaseSparkRJob();
    message.mainRFileUri = object.mainRFileUri ?? "";
    message.args = object.args?.map((e) => e) || [];
    message.fileUris = object.fileUris?.map((e) => e) || [];
    message.archiveUris = object.archiveUris?.map((e) => e) || [];
    message.properties = Object.entries(object.properties ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.loggingConfig = (object.loggingConfig !== undefined && object.loggingConfig !== null)
      ? LoggingConfig.fromPartial(object.loggingConfig)
      : undefined;
    return message;
  },
};

function createBaseSparkRJob_PropertiesEntry(): SparkRJob_PropertiesEntry {
  return { key: "", value: "" };
}

export const SparkRJob_PropertiesEntry: MessageFns<SparkRJob_PropertiesEntry> = {
  encode(message: SparkRJob_PropertiesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SparkRJob_PropertiesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSparkRJob_PropertiesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SparkRJob_PropertiesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: SparkRJob_PropertiesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<SparkRJob_PropertiesEntry>): SparkRJob_PropertiesEntry {
    return SparkRJob_PropertiesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SparkRJob_PropertiesEntry>): SparkRJob_PropertiesEntry {
    const message = createBaseSparkRJob_PropertiesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBasePrestoJob(): PrestoJob {
  return {
    queryFileUri: undefined,
    queryList: undefined,
    continueOnFailure: false,
    outputFormat: "",
    clientTags: [],
    properties: {},
    loggingConfig: undefined,
  };
}

export const PrestoJob: MessageFns<PrestoJob> = {
  encode(message: PrestoJob, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.queryFileUri !== undefined) {
      writer.uint32(10).string(message.queryFileUri);
    }
    if (message.queryList !== undefined) {
      QueryList.encode(message.queryList, writer.uint32(18).fork()).join();
    }
    if (message.continueOnFailure !== false) {
      writer.uint32(24).bool(message.continueOnFailure);
    }
    if (message.outputFormat !== "") {
      writer.uint32(34).string(message.outputFormat);
    }
    for (const v of message.clientTags) {
      writer.uint32(42).string(v!);
    }
    Object.entries(message.properties).forEach(([key, value]) => {
      PrestoJob_PropertiesEntry.encode({ key: key as any, value }, writer.uint32(50).fork()).join();
    });
    if (message.loggingConfig !== undefined) {
      LoggingConfig.encode(message.loggingConfig, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PrestoJob {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePrestoJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.queryFileUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.queryList = QueryList.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.continueOnFailure = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.outputFormat = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.clientTags.push(reader.string());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          const entry6 = PrestoJob_PropertiesEntry.decode(reader, reader.uint32());
          if (entry6.value !== undefined) {
            message.properties[entry6.key] = entry6.value;
          }
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.loggingConfig = LoggingConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PrestoJob {
    return {
      queryFileUri: isSet(object.queryFileUri) ? globalThis.String(object.queryFileUri) : undefined,
      queryList: isSet(object.queryList) ? QueryList.fromJSON(object.queryList) : undefined,
      continueOnFailure: isSet(object.continueOnFailure) ? globalThis.Boolean(object.continueOnFailure) : false,
      outputFormat: isSet(object.outputFormat) ? globalThis.String(object.outputFormat) : "",
      clientTags: globalThis.Array.isArray(object?.clientTags)
        ? object.clientTags.map((e: any) => globalThis.String(e))
        : [],
      properties: isObject(object.properties)
        ? Object.entries(object.properties).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      loggingConfig: isSet(object.loggingConfig) ? LoggingConfig.fromJSON(object.loggingConfig) : undefined,
    };
  },

  toJSON(message: PrestoJob): unknown {
    const obj: any = {};
    if (message.queryFileUri !== undefined) {
      obj.queryFileUri = message.queryFileUri;
    }
    if (message.queryList !== undefined) {
      obj.queryList = QueryList.toJSON(message.queryList);
    }
    if (message.continueOnFailure !== false) {
      obj.continueOnFailure = message.continueOnFailure;
    }
    if (message.outputFormat !== "") {
      obj.outputFormat = message.outputFormat;
    }
    if (message.clientTags?.length) {
      obj.clientTags = message.clientTags;
    }
    if (message.properties) {
      const entries = Object.entries(message.properties);
      if (entries.length > 0) {
        obj.properties = {};
        entries.forEach(([k, v]) => {
          obj.properties[k] = v;
        });
      }
    }
    if (message.loggingConfig !== undefined) {
      obj.loggingConfig = LoggingConfig.toJSON(message.loggingConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<PrestoJob>): PrestoJob {
    return PrestoJob.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PrestoJob>): PrestoJob {
    const message = createBasePrestoJob();
    message.queryFileUri = object.queryFileUri ?? undefined;
    message.queryList = (object.queryList !== undefined && object.queryList !== null)
      ? QueryList.fromPartial(object.queryList)
      : undefined;
    message.continueOnFailure = object.continueOnFailure ?? false;
    message.outputFormat = object.outputFormat ?? "";
    message.clientTags = object.clientTags?.map((e) => e) || [];
    message.properties = Object.entries(object.properties ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.loggingConfig = (object.loggingConfig !== undefined && object.loggingConfig !== null)
      ? LoggingConfig.fromPartial(object.loggingConfig)
      : undefined;
    return message;
  },
};

function createBasePrestoJob_PropertiesEntry(): PrestoJob_PropertiesEntry {
  return { key: "", value: "" };
}

export const PrestoJob_PropertiesEntry: MessageFns<PrestoJob_PropertiesEntry> = {
  encode(message: PrestoJob_PropertiesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PrestoJob_PropertiesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePrestoJob_PropertiesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PrestoJob_PropertiesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: PrestoJob_PropertiesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<PrestoJob_PropertiesEntry>): PrestoJob_PropertiesEntry {
    return PrestoJob_PropertiesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PrestoJob_PropertiesEntry>): PrestoJob_PropertiesEntry {
    const message = createBasePrestoJob_PropertiesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseTrinoJob(): TrinoJob {
  return {
    queryFileUri: undefined,
    queryList: undefined,
    continueOnFailure: false,
    outputFormat: "",
    clientTags: [],
    properties: {},
    loggingConfig: undefined,
  };
}

export const TrinoJob: MessageFns<TrinoJob> = {
  encode(message: TrinoJob, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.queryFileUri !== undefined) {
      writer.uint32(10).string(message.queryFileUri);
    }
    if (message.queryList !== undefined) {
      QueryList.encode(message.queryList, writer.uint32(18).fork()).join();
    }
    if (message.continueOnFailure !== false) {
      writer.uint32(24).bool(message.continueOnFailure);
    }
    if (message.outputFormat !== "") {
      writer.uint32(34).string(message.outputFormat);
    }
    for (const v of message.clientTags) {
      writer.uint32(42).string(v!);
    }
    Object.entries(message.properties).forEach(([key, value]) => {
      TrinoJob_PropertiesEntry.encode({ key: key as any, value }, writer.uint32(50).fork()).join();
    });
    if (message.loggingConfig !== undefined) {
      LoggingConfig.encode(message.loggingConfig, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TrinoJob {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTrinoJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.queryFileUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.queryList = QueryList.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.continueOnFailure = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.outputFormat = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.clientTags.push(reader.string());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          const entry6 = TrinoJob_PropertiesEntry.decode(reader, reader.uint32());
          if (entry6.value !== undefined) {
            message.properties[entry6.key] = entry6.value;
          }
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.loggingConfig = LoggingConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TrinoJob {
    return {
      queryFileUri: isSet(object.queryFileUri) ? globalThis.String(object.queryFileUri) : undefined,
      queryList: isSet(object.queryList) ? QueryList.fromJSON(object.queryList) : undefined,
      continueOnFailure: isSet(object.continueOnFailure) ? globalThis.Boolean(object.continueOnFailure) : false,
      outputFormat: isSet(object.outputFormat) ? globalThis.String(object.outputFormat) : "",
      clientTags: globalThis.Array.isArray(object?.clientTags)
        ? object.clientTags.map((e: any) => globalThis.String(e))
        : [],
      properties: isObject(object.properties)
        ? Object.entries(object.properties).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      loggingConfig: isSet(object.loggingConfig) ? LoggingConfig.fromJSON(object.loggingConfig) : undefined,
    };
  },

  toJSON(message: TrinoJob): unknown {
    const obj: any = {};
    if (message.queryFileUri !== undefined) {
      obj.queryFileUri = message.queryFileUri;
    }
    if (message.queryList !== undefined) {
      obj.queryList = QueryList.toJSON(message.queryList);
    }
    if (message.continueOnFailure !== false) {
      obj.continueOnFailure = message.continueOnFailure;
    }
    if (message.outputFormat !== "") {
      obj.outputFormat = message.outputFormat;
    }
    if (message.clientTags?.length) {
      obj.clientTags = message.clientTags;
    }
    if (message.properties) {
      const entries = Object.entries(message.properties);
      if (entries.length > 0) {
        obj.properties = {};
        entries.forEach(([k, v]) => {
          obj.properties[k] = v;
        });
      }
    }
    if (message.loggingConfig !== undefined) {
      obj.loggingConfig = LoggingConfig.toJSON(message.loggingConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<TrinoJob>): TrinoJob {
    return TrinoJob.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TrinoJob>): TrinoJob {
    const message = createBaseTrinoJob();
    message.queryFileUri = object.queryFileUri ?? undefined;
    message.queryList = (object.queryList !== undefined && object.queryList !== null)
      ? QueryList.fromPartial(object.queryList)
      : undefined;
    message.continueOnFailure = object.continueOnFailure ?? false;
    message.outputFormat = object.outputFormat ?? "";
    message.clientTags = object.clientTags?.map((e) => e) || [];
    message.properties = Object.entries(object.properties ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.loggingConfig = (object.loggingConfig !== undefined && object.loggingConfig !== null)
      ? LoggingConfig.fromPartial(object.loggingConfig)
      : undefined;
    return message;
  },
};

function createBaseTrinoJob_PropertiesEntry(): TrinoJob_PropertiesEntry {
  return { key: "", value: "" };
}

export const TrinoJob_PropertiesEntry: MessageFns<TrinoJob_PropertiesEntry> = {
  encode(message: TrinoJob_PropertiesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TrinoJob_PropertiesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTrinoJob_PropertiesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TrinoJob_PropertiesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: TrinoJob_PropertiesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<TrinoJob_PropertiesEntry>): TrinoJob_PropertiesEntry {
    return TrinoJob_PropertiesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TrinoJob_PropertiesEntry>): TrinoJob_PropertiesEntry {
    const message = createBaseTrinoJob_PropertiesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseFlinkJob(): FlinkJob {
  return {
    mainJarFileUri: undefined,
    mainClass: undefined,
    args: [],
    jarFileUris: [],
    savepointUri: "",
    properties: {},
    loggingConfig: undefined,
  };
}

export const FlinkJob: MessageFns<FlinkJob> = {
  encode(message: FlinkJob, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.mainJarFileUri !== undefined) {
      writer.uint32(10).string(message.mainJarFileUri);
    }
    if (message.mainClass !== undefined) {
      writer.uint32(18).string(message.mainClass);
    }
    for (const v of message.args) {
      writer.uint32(26).string(v!);
    }
    for (const v of message.jarFileUris) {
      writer.uint32(34).string(v!);
    }
    if (message.savepointUri !== "") {
      writer.uint32(74).string(message.savepointUri);
    }
    Object.entries(message.properties).forEach(([key, value]) => {
      FlinkJob_PropertiesEntry.encode({ key: key as any, value }, writer.uint32(58).fork()).join();
    });
    if (message.loggingConfig !== undefined) {
      LoggingConfig.encode(message.loggingConfig, writer.uint32(66).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FlinkJob {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFlinkJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.mainJarFileUri = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.mainClass = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.args.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.jarFileUris.push(reader.string());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.savepointUri = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          const entry7 = FlinkJob_PropertiesEntry.decode(reader, reader.uint32());
          if (entry7.value !== undefined) {
            message.properties[entry7.key] = entry7.value;
          }
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.loggingConfig = LoggingConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FlinkJob {
    return {
      mainJarFileUri: isSet(object.mainJarFileUri) ? globalThis.String(object.mainJarFileUri) : undefined,
      mainClass: isSet(object.mainClass) ? globalThis.String(object.mainClass) : undefined,
      args: globalThis.Array.isArray(object?.args) ? object.args.map((e: any) => globalThis.String(e)) : [],
      jarFileUris: globalThis.Array.isArray(object?.jarFileUris)
        ? object.jarFileUris.map((e: any) => globalThis.String(e))
        : [],
      savepointUri: isSet(object.savepointUri) ? globalThis.String(object.savepointUri) : "",
      properties: isObject(object.properties)
        ? Object.entries(object.properties).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      loggingConfig: isSet(object.loggingConfig) ? LoggingConfig.fromJSON(object.loggingConfig) : undefined,
    };
  },

  toJSON(message: FlinkJob): unknown {
    const obj: any = {};
    if (message.mainJarFileUri !== undefined) {
      obj.mainJarFileUri = message.mainJarFileUri;
    }
    if (message.mainClass !== undefined) {
      obj.mainClass = message.mainClass;
    }
    if (message.args?.length) {
      obj.args = message.args;
    }
    if (message.jarFileUris?.length) {
      obj.jarFileUris = message.jarFileUris;
    }
    if (message.savepointUri !== "") {
      obj.savepointUri = message.savepointUri;
    }
    if (message.properties) {
      const entries = Object.entries(message.properties);
      if (entries.length > 0) {
        obj.properties = {};
        entries.forEach(([k, v]) => {
          obj.properties[k] = v;
        });
      }
    }
    if (message.loggingConfig !== undefined) {
      obj.loggingConfig = LoggingConfig.toJSON(message.loggingConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<FlinkJob>): FlinkJob {
    return FlinkJob.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FlinkJob>): FlinkJob {
    const message = createBaseFlinkJob();
    message.mainJarFileUri = object.mainJarFileUri ?? undefined;
    message.mainClass = object.mainClass ?? undefined;
    message.args = object.args?.map((e) => e) || [];
    message.jarFileUris = object.jarFileUris?.map((e) => e) || [];
    message.savepointUri = object.savepointUri ?? "";
    message.properties = Object.entries(object.properties ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.loggingConfig = (object.loggingConfig !== undefined && object.loggingConfig !== null)
      ? LoggingConfig.fromPartial(object.loggingConfig)
      : undefined;
    return message;
  },
};

function createBaseFlinkJob_PropertiesEntry(): FlinkJob_PropertiesEntry {
  return { key: "", value: "" };
}

export const FlinkJob_PropertiesEntry: MessageFns<FlinkJob_PropertiesEntry> = {
  encode(message: FlinkJob_PropertiesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FlinkJob_PropertiesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFlinkJob_PropertiesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FlinkJob_PropertiesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: FlinkJob_PropertiesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<FlinkJob_PropertiesEntry>): FlinkJob_PropertiesEntry {
    return FlinkJob_PropertiesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FlinkJob_PropertiesEntry>): FlinkJob_PropertiesEntry {
    const message = createBaseFlinkJob_PropertiesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseJobPlacement(): JobPlacement {
  return { clusterName: "", clusterUuid: "", clusterLabels: {} };
}

export const JobPlacement: MessageFns<JobPlacement> = {
  encode(message: JobPlacement, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.clusterName !== "") {
      writer.uint32(10).string(message.clusterName);
    }
    if (message.clusterUuid !== "") {
      writer.uint32(18).string(message.clusterUuid);
    }
    Object.entries(message.clusterLabels).forEach(([key, value]) => {
      JobPlacement_ClusterLabelsEntry.encode({ key: key as any, value }, writer.uint32(26).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobPlacement {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobPlacement();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.clusterName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.clusterUuid = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          const entry3 = JobPlacement_ClusterLabelsEntry.decode(reader, reader.uint32());
          if (entry3.value !== undefined) {
            message.clusterLabels[entry3.key] = entry3.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobPlacement {
    return {
      clusterName: isSet(object.clusterName) ? globalThis.String(object.clusterName) : "",
      clusterUuid: isSet(object.clusterUuid) ? globalThis.String(object.clusterUuid) : "",
      clusterLabels: isObject(object.clusterLabels)
        ? Object.entries(object.clusterLabels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: JobPlacement): unknown {
    const obj: any = {};
    if (message.clusterName !== "") {
      obj.clusterName = message.clusterName;
    }
    if (message.clusterUuid !== "") {
      obj.clusterUuid = message.clusterUuid;
    }
    if (message.clusterLabels) {
      const entries = Object.entries(message.clusterLabels);
      if (entries.length > 0) {
        obj.clusterLabels = {};
        entries.forEach(([k, v]) => {
          obj.clusterLabels[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<JobPlacement>): JobPlacement {
    return JobPlacement.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobPlacement>): JobPlacement {
    const message = createBaseJobPlacement();
    message.clusterName = object.clusterName ?? "";
    message.clusterUuid = object.clusterUuid ?? "";
    message.clusterLabels = Object.entries(object.clusterLabels ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseJobPlacement_ClusterLabelsEntry(): JobPlacement_ClusterLabelsEntry {
  return { key: "", value: "" };
}

export const JobPlacement_ClusterLabelsEntry: MessageFns<JobPlacement_ClusterLabelsEntry> = {
  encode(message: JobPlacement_ClusterLabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobPlacement_ClusterLabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobPlacement_ClusterLabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobPlacement_ClusterLabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: JobPlacement_ClusterLabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<JobPlacement_ClusterLabelsEntry>): JobPlacement_ClusterLabelsEntry {
    return JobPlacement_ClusterLabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobPlacement_ClusterLabelsEntry>): JobPlacement_ClusterLabelsEntry {
    const message = createBaseJobPlacement_ClusterLabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseJobStatus(): JobStatus {
  return { state: 0, details: "", stateStartTime: undefined, substate: 0 };
}

export const JobStatus: MessageFns<JobStatus> = {
  encode(message: JobStatus, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.state !== 0) {
      writer.uint32(8).int32(message.state);
    }
    if (message.details !== "") {
      writer.uint32(18).string(message.details);
    }
    if (message.stateStartTime !== undefined) {
      Timestamp.encode(toTimestamp(message.stateStartTime), writer.uint32(50).fork()).join();
    }
    if (message.substate !== 0) {
      writer.uint32(56).int32(message.substate);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobStatus {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobStatus();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.details = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.stateStartTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.substate = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobStatus {
    return {
      state: isSet(object.state) ? jobStatus_StateFromJSON(object.state) : 0,
      details: isSet(object.details) ? globalThis.String(object.details) : "",
      stateStartTime: isSet(object.stateStartTime) ? fromJsonTimestamp(object.stateStartTime) : undefined,
      substate: isSet(object.substate) ? jobStatus_SubstateFromJSON(object.substate) : 0,
    };
  },

  toJSON(message: JobStatus): unknown {
    const obj: any = {};
    if (message.state !== 0) {
      obj.state = jobStatus_StateToJSON(message.state);
    }
    if (message.details !== "") {
      obj.details = message.details;
    }
    if (message.stateStartTime !== undefined) {
      obj.stateStartTime = message.stateStartTime.toISOString();
    }
    if (message.substate !== 0) {
      obj.substate = jobStatus_SubstateToJSON(message.substate);
    }
    return obj;
  },

  create(base?: DeepPartial<JobStatus>): JobStatus {
    return JobStatus.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobStatus>): JobStatus {
    const message = createBaseJobStatus();
    message.state = object.state ?? 0;
    message.details = object.details ?? "";
    message.stateStartTime = object.stateStartTime ?? undefined;
    message.substate = object.substate ?? 0;
    return message;
  },
};

function createBaseJobReference(): JobReference {
  return { projectId: "", jobId: "" };
}

export const JobReference: MessageFns<JobReference> = {
  encode(message: JobReference, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.jobId !== "") {
      writer.uint32(18).string(message.jobId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobReference {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobReference();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.jobId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobReference {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      jobId: isSet(object.jobId) ? globalThis.String(object.jobId) : "",
    };
  },

  toJSON(message: JobReference): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.jobId !== "") {
      obj.jobId = message.jobId;
    }
    return obj;
  },

  create(base?: DeepPartial<JobReference>): JobReference {
    return JobReference.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobReference>): JobReference {
    const message = createBaseJobReference();
    message.projectId = object.projectId ?? "";
    message.jobId = object.jobId ?? "";
    return message;
  },
};

function createBaseYarnApplication(): YarnApplication {
  return { name: "", state: 0, progress: 0, trackingUrl: "" };
}

export const YarnApplication: MessageFns<YarnApplication> = {
  encode(message: YarnApplication, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.state !== 0) {
      writer.uint32(16).int32(message.state);
    }
    if (message.progress !== 0) {
      writer.uint32(29).float(message.progress);
    }
    if (message.trackingUrl !== "") {
      writer.uint32(34).string(message.trackingUrl);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): YarnApplication {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseYarnApplication();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 29) {
            break;
          }

          message.progress = reader.float();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.trackingUrl = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): YarnApplication {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      state: isSet(object.state) ? yarnApplication_StateFromJSON(object.state) : 0,
      progress: isSet(object.progress) ? globalThis.Number(object.progress) : 0,
      trackingUrl: isSet(object.trackingUrl) ? globalThis.String(object.trackingUrl) : "",
    };
  },

  toJSON(message: YarnApplication): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.state !== 0) {
      obj.state = yarnApplication_StateToJSON(message.state);
    }
    if (message.progress !== 0) {
      obj.progress = message.progress;
    }
    if (message.trackingUrl !== "") {
      obj.trackingUrl = message.trackingUrl;
    }
    return obj;
  },

  create(base?: DeepPartial<YarnApplication>): YarnApplication {
    return YarnApplication.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<YarnApplication>): YarnApplication {
    const message = createBaseYarnApplication();
    message.name = object.name ?? "";
    message.state = object.state ?? 0;
    message.progress = object.progress ?? 0;
    message.trackingUrl = object.trackingUrl ?? "";
    return message;
  },
};

function createBaseJob(): Job {
  return {
    reference: undefined,
    placement: undefined,
    hadoopJob: undefined,
    sparkJob: undefined,
    pysparkJob: undefined,
    hiveJob: undefined,
    pigJob: undefined,
    sparkRJob: undefined,
    sparkSqlJob: undefined,
    prestoJob: undefined,
    trinoJob: undefined,
    flinkJob: undefined,
    status: undefined,
    statusHistory: [],
    yarnApplications: [],
    driverOutputResourceUri: "",
    driverControlFilesUri: "",
    labels: {},
    scheduling: undefined,
    jobUuid: "",
    done: false,
    driverSchedulingConfig: undefined,
  };
}

export const Job: MessageFns<Job> = {
  encode(message: Job, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.reference !== undefined) {
      JobReference.encode(message.reference, writer.uint32(10).fork()).join();
    }
    if (message.placement !== undefined) {
      JobPlacement.encode(message.placement, writer.uint32(18).fork()).join();
    }
    if (message.hadoopJob !== undefined) {
      HadoopJob.encode(message.hadoopJob, writer.uint32(26).fork()).join();
    }
    if (message.sparkJob !== undefined) {
      SparkJob.encode(message.sparkJob, writer.uint32(34).fork()).join();
    }
    if (message.pysparkJob !== undefined) {
      PySparkJob.encode(message.pysparkJob, writer.uint32(42).fork()).join();
    }
    if (message.hiveJob !== undefined) {
      HiveJob.encode(message.hiveJob, writer.uint32(50).fork()).join();
    }
    if (message.pigJob !== undefined) {
      PigJob.encode(message.pigJob, writer.uint32(58).fork()).join();
    }
    if (message.sparkRJob !== undefined) {
      SparkRJob.encode(message.sparkRJob, writer.uint32(170).fork()).join();
    }
    if (message.sparkSqlJob !== undefined) {
      SparkSqlJob.encode(message.sparkSqlJob, writer.uint32(98).fork()).join();
    }
    if (message.prestoJob !== undefined) {
      PrestoJob.encode(message.prestoJob, writer.uint32(186).fork()).join();
    }
    if (message.trinoJob !== undefined) {
      TrinoJob.encode(message.trinoJob, writer.uint32(226).fork()).join();
    }
    if (message.flinkJob !== undefined) {
      FlinkJob.encode(message.flinkJob, writer.uint32(234).fork()).join();
    }
    if (message.status !== undefined) {
      JobStatus.encode(message.status, writer.uint32(66).fork()).join();
    }
    for (const v of message.statusHistory) {
      JobStatus.encode(v!, writer.uint32(106).fork()).join();
    }
    for (const v of message.yarnApplications) {
      YarnApplication.encode(v!, writer.uint32(74).fork()).join();
    }
    if (message.driverOutputResourceUri !== "") {
      writer.uint32(138).string(message.driverOutputResourceUri);
    }
    if (message.driverControlFilesUri !== "") {
      writer.uint32(122).string(message.driverControlFilesUri);
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      Job_LabelsEntry.encode({ key: key as any, value }, writer.uint32(146).fork()).join();
    });
    if (message.scheduling !== undefined) {
      JobScheduling.encode(message.scheduling, writer.uint32(162).fork()).join();
    }
    if (message.jobUuid !== "") {
      writer.uint32(178).string(message.jobUuid);
    }
    if (message.done !== false) {
      writer.uint32(192).bool(message.done);
    }
    if (message.driverSchedulingConfig !== undefined) {
      DriverSchedulingConfig.encode(message.driverSchedulingConfig, writer.uint32(218).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Job {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.reference = JobReference.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.placement = JobPlacement.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.hadoopJob = HadoopJob.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.sparkJob = SparkJob.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.pysparkJob = PySparkJob.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.hiveJob = HiveJob.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.pigJob = PigJob.decode(reader, reader.uint32());
          continue;
        case 21:
          if (tag !== 170) {
            break;
          }

          message.sparkRJob = SparkRJob.decode(reader, reader.uint32());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.sparkSqlJob = SparkSqlJob.decode(reader, reader.uint32());
          continue;
        case 23:
          if (tag !== 186) {
            break;
          }

          message.prestoJob = PrestoJob.decode(reader, reader.uint32());
          continue;
        case 28:
          if (tag !== 226) {
            break;
          }

          message.trinoJob = TrinoJob.decode(reader, reader.uint32());
          continue;
        case 29:
          if (tag !== 234) {
            break;
          }

          message.flinkJob = FlinkJob.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.status = JobStatus.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.statusHistory.push(JobStatus.decode(reader, reader.uint32()));
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.yarnApplications.push(YarnApplication.decode(reader, reader.uint32()));
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.driverOutputResourceUri = reader.string();
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.driverControlFilesUri = reader.string();
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          const entry18 = Job_LabelsEntry.decode(reader, reader.uint32());
          if (entry18.value !== undefined) {
            message.labels[entry18.key] = entry18.value;
          }
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.scheduling = JobScheduling.decode(reader, reader.uint32());
          continue;
        case 22:
          if (tag !== 178) {
            break;
          }

          message.jobUuid = reader.string();
          continue;
        case 24:
          if (tag !== 192) {
            break;
          }

          message.done = reader.bool();
          continue;
        case 27:
          if (tag !== 218) {
            break;
          }

          message.driverSchedulingConfig = DriverSchedulingConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Job {
    return {
      reference: isSet(object.reference) ? JobReference.fromJSON(object.reference) : undefined,
      placement: isSet(object.placement) ? JobPlacement.fromJSON(object.placement) : undefined,
      hadoopJob: isSet(object.hadoopJob) ? HadoopJob.fromJSON(object.hadoopJob) : undefined,
      sparkJob: isSet(object.sparkJob) ? SparkJob.fromJSON(object.sparkJob) : undefined,
      pysparkJob: isSet(object.pysparkJob) ? PySparkJob.fromJSON(object.pysparkJob) : undefined,
      hiveJob: isSet(object.hiveJob) ? HiveJob.fromJSON(object.hiveJob) : undefined,
      pigJob: isSet(object.pigJob) ? PigJob.fromJSON(object.pigJob) : undefined,
      sparkRJob: isSet(object.sparkRJob) ? SparkRJob.fromJSON(object.sparkRJob) : undefined,
      sparkSqlJob: isSet(object.sparkSqlJob) ? SparkSqlJob.fromJSON(object.sparkSqlJob) : undefined,
      prestoJob: isSet(object.prestoJob) ? PrestoJob.fromJSON(object.prestoJob) : undefined,
      trinoJob: isSet(object.trinoJob) ? TrinoJob.fromJSON(object.trinoJob) : undefined,
      flinkJob: isSet(object.flinkJob) ? FlinkJob.fromJSON(object.flinkJob) : undefined,
      status: isSet(object.status) ? JobStatus.fromJSON(object.status) : undefined,
      statusHistory: globalThis.Array.isArray(object?.statusHistory)
        ? object.statusHistory.map((e: any) => JobStatus.fromJSON(e))
        : [],
      yarnApplications: globalThis.Array.isArray(object?.yarnApplications)
        ? object.yarnApplications.map((e: any) => YarnApplication.fromJSON(e))
        : [],
      driverOutputResourceUri: isSet(object.driverOutputResourceUri)
        ? globalThis.String(object.driverOutputResourceUri)
        : "",
      driverControlFilesUri: isSet(object.driverControlFilesUri) ? globalThis.String(object.driverControlFilesUri) : "",
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      scheduling: isSet(object.scheduling) ? JobScheduling.fromJSON(object.scheduling) : undefined,
      jobUuid: isSet(object.jobUuid) ? globalThis.String(object.jobUuid) : "",
      done: isSet(object.done) ? globalThis.Boolean(object.done) : false,
      driverSchedulingConfig: isSet(object.driverSchedulingConfig)
        ? DriverSchedulingConfig.fromJSON(object.driverSchedulingConfig)
        : undefined,
    };
  },

  toJSON(message: Job): unknown {
    const obj: any = {};
    if (message.reference !== undefined) {
      obj.reference = JobReference.toJSON(message.reference);
    }
    if (message.placement !== undefined) {
      obj.placement = JobPlacement.toJSON(message.placement);
    }
    if (message.hadoopJob !== undefined) {
      obj.hadoopJob = HadoopJob.toJSON(message.hadoopJob);
    }
    if (message.sparkJob !== undefined) {
      obj.sparkJob = SparkJob.toJSON(message.sparkJob);
    }
    if (message.pysparkJob !== undefined) {
      obj.pysparkJob = PySparkJob.toJSON(message.pysparkJob);
    }
    if (message.hiveJob !== undefined) {
      obj.hiveJob = HiveJob.toJSON(message.hiveJob);
    }
    if (message.pigJob !== undefined) {
      obj.pigJob = PigJob.toJSON(message.pigJob);
    }
    if (message.sparkRJob !== undefined) {
      obj.sparkRJob = SparkRJob.toJSON(message.sparkRJob);
    }
    if (message.sparkSqlJob !== undefined) {
      obj.sparkSqlJob = SparkSqlJob.toJSON(message.sparkSqlJob);
    }
    if (message.prestoJob !== undefined) {
      obj.prestoJob = PrestoJob.toJSON(message.prestoJob);
    }
    if (message.trinoJob !== undefined) {
      obj.trinoJob = TrinoJob.toJSON(message.trinoJob);
    }
    if (message.flinkJob !== undefined) {
      obj.flinkJob = FlinkJob.toJSON(message.flinkJob);
    }
    if (message.status !== undefined) {
      obj.status = JobStatus.toJSON(message.status);
    }
    if (message.statusHistory?.length) {
      obj.statusHistory = message.statusHistory.map((e) => JobStatus.toJSON(e));
    }
    if (message.yarnApplications?.length) {
      obj.yarnApplications = message.yarnApplications.map((e) => YarnApplication.toJSON(e));
    }
    if (message.driverOutputResourceUri !== "") {
      obj.driverOutputResourceUri = message.driverOutputResourceUri;
    }
    if (message.driverControlFilesUri !== "") {
      obj.driverControlFilesUri = message.driverControlFilesUri;
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.scheduling !== undefined) {
      obj.scheduling = JobScheduling.toJSON(message.scheduling);
    }
    if (message.jobUuid !== "") {
      obj.jobUuid = message.jobUuid;
    }
    if (message.done !== false) {
      obj.done = message.done;
    }
    if (message.driverSchedulingConfig !== undefined) {
      obj.driverSchedulingConfig = DriverSchedulingConfig.toJSON(message.driverSchedulingConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<Job>): Job {
    return Job.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Job>): Job {
    const message = createBaseJob();
    message.reference = (object.reference !== undefined && object.reference !== null)
      ? JobReference.fromPartial(object.reference)
      : undefined;
    message.placement = (object.placement !== undefined && object.placement !== null)
      ? JobPlacement.fromPartial(object.placement)
      : undefined;
    message.hadoopJob = (object.hadoopJob !== undefined && object.hadoopJob !== null)
      ? HadoopJob.fromPartial(object.hadoopJob)
      : undefined;
    message.sparkJob = (object.sparkJob !== undefined && object.sparkJob !== null)
      ? SparkJob.fromPartial(object.sparkJob)
      : undefined;
    message.pysparkJob = (object.pysparkJob !== undefined && object.pysparkJob !== null)
      ? PySparkJob.fromPartial(object.pysparkJob)
      : undefined;
    message.hiveJob = (object.hiveJob !== undefined && object.hiveJob !== null)
      ? HiveJob.fromPartial(object.hiveJob)
      : undefined;
    message.pigJob = (object.pigJob !== undefined && object.pigJob !== null)
      ? PigJob.fromPartial(object.pigJob)
      : undefined;
    message.sparkRJob = (object.sparkRJob !== undefined && object.sparkRJob !== null)
      ? SparkRJob.fromPartial(object.sparkRJob)
      : undefined;
    message.sparkSqlJob = (object.sparkSqlJob !== undefined && object.sparkSqlJob !== null)
      ? SparkSqlJob.fromPartial(object.sparkSqlJob)
      : undefined;
    message.prestoJob = (object.prestoJob !== undefined && object.prestoJob !== null)
      ? PrestoJob.fromPartial(object.prestoJob)
      : undefined;
    message.trinoJob = (object.trinoJob !== undefined && object.trinoJob !== null)
      ? TrinoJob.fromPartial(object.trinoJob)
      : undefined;
    message.flinkJob = (object.flinkJob !== undefined && object.flinkJob !== null)
      ? FlinkJob.fromPartial(object.flinkJob)
      : undefined;
    message.status = (object.status !== undefined && object.status !== null)
      ? JobStatus.fromPartial(object.status)
      : undefined;
    message.statusHistory = object.statusHistory?.map((e) => JobStatus.fromPartial(e)) || [];
    message.yarnApplications = object.yarnApplications?.map((e) => YarnApplication.fromPartial(e)) || [];
    message.driverOutputResourceUri = object.driverOutputResourceUri ?? "";
    message.driverControlFilesUri = object.driverControlFilesUri ?? "";
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.scheduling = (object.scheduling !== undefined && object.scheduling !== null)
      ? JobScheduling.fromPartial(object.scheduling)
      : undefined;
    message.jobUuid = object.jobUuid ?? "";
    message.done = object.done ?? false;
    message.driverSchedulingConfig =
      (object.driverSchedulingConfig !== undefined && object.driverSchedulingConfig !== null)
        ? DriverSchedulingConfig.fromPartial(object.driverSchedulingConfig)
        : undefined;
    return message;
  },
};

function createBaseJob_LabelsEntry(): Job_LabelsEntry {
  return { key: "", value: "" };
}

export const Job_LabelsEntry: MessageFns<Job_LabelsEntry> = {
  encode(message: Job_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Job_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJob_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Job_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Job_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Job_LabelsEntry>): Job_LabelsEntry {
    return Job_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Job_LabelsEntry>): Job_LabelsEntry {
    const message = createBaseJob_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseDriverSchedulingConfig(): DriverSchedulingConfig {
  return { memoryMb: 0, vcores: 0 };
}

export const DriverSchedulingConfig: MessageFns<DriverSchedulingConfig> = {
  encode(message: DriverSchedulingConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.memoryMb !== 0) {
      writer.uint32(8).int32(message.memoryMb);
    }
    if (message.vcores !== 0) {
      writer.uint32(16).int32(message.vcores);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DriverSchedulingConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDriverSchedulingConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.memoryMb = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.vcores = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DriverSchedulingConfig {
    return {
      memoryMb: isSet(object.memoryMb) ? globalThis.Number(object.memoryMb) : 0,
      vcores: isSet(object.vcores) ? globalThis.Number(object.vcores) : 0,
    };
  },

  toJSON(message: DriverSchedulingConfig): unknown {
    const obj: any = {};
    if (message.memoryMb !== 0) {
      obj.memoryMb = Math.round(message.memoryMb);
    }
    if (message.vcores !== 0) {
      obj.vcores = Math.round(message.vcores);
    }
    return obj;
  },

  create(base?: DeepPartial<DriverSchedulingConfig>): DriverSchedulingConfig {
    return DriverSchedulingConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DriverSchedulingConfig>): DriverSchedulingConfig {
    const message = createBaseDriverSchedulingConfig();
    message.memoryMb = object.memoryMb ?? 0;
    message.vcores = object.vcores ?? 0;
    return message;
  },
};

function createBaseJobScheduling(): JobScheduling {
  return { maxFailuresPerHour: 0, maxFailuresTotal: 0 };
}

export const JobScheduling: MessageFns<JobScheduling> = {
  encode(message: JobScheduling, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.maxFailuresPerHour !== 0) {
      writer.uint32(8).int32(message.maxFailuresPerHour);
    }
    if (message.maxFailuresTotal !== 0) {
      writer.uint32(16).int32(message.maxFailuresTotal);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobScheduling {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobScheduling();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.maxFailuresPerHour = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.maxFailuresTotal = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobScheduling {
    return {
      maxFailuresPerHour: isSet(object.maxFailuresPerHour) ? globalThis.Number(object.maxFailuresPerHour) : 0,
      maxFailuresTotal: isSet(object.maxFailuresTotal) ? globalThis.Number(object.maxFailuresTotal) : 0,
    };
  },

  toJSON(message: JobScheduling): unknown {
    const obj: any = {};
    if (message.maxFailuresPerHour !== 0) {
      obj.maxFailuresPerHour = Math.round(message.maxFailuresPerHour);
    }
    if (message.maxFailuresTotal !== 0) {
      obj.maxFailuresTotal = Math.round(message.maxFailuresTotal);
    }
    return obj;
  },

  create(base?: DeepPartial<JobScheduling>): JobScheduling {
    return JobScheduling.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobScheduling>): JobScheduling {
    const message = createBaseJobScheduling();
    message.maxFailuresPerHour = object.maxFailuresPerHour ?? 0;
    message.maxFailuresTotal = object.maxFailuresTotal ?? 0;
    return message;
  },
};

function createBaseSubmitJobRequest(): SubmitJobRequest {
  return { projectId: "", region: "", job: undefined, requestId: "" };
}

export const SubmitJobRequest: MessageFns<SubmitJobRequest> = {
  encode(message: SubmitJobRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.region !== "") {
      writer.uint32(26).string(message.region);
    }
    if (message.job !== undefined) {
      Job.encode(message.job, writer.uint32(18).fork()).join();
    }
    if (message.requestId !== "") {
      writer.uint32(34).string(message.requestId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SubmitJobRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSubmitJobRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.region = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.job = Job.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.requestId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SubmitJobRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      job: isSet(object.job) ? Job.fromJSON(object.job) : undefined,
      requestId: isSet(object.requestId) ? globalThis.String(object.requestId) : "",
    };
  },

  toJSON(message: SubmitJobRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.job !== undefined) {
      obj.job = Job.toJSON(message.job);
    }
    if (message.requestId !== "") {
      obj.requestId = message.requestId;
    }
    return obj;
  },

  create(base?: DeepPartial<SubmitJobRequest>): SubmitJobRequest {
    return SubmitJobRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SubmitJobRequest>): SubmitJobRequest {
    const message = createBaseSubmitJobRequest();
    message.projectId = object.projectId ?? "";
    message.region = object.region ?? "";
    message.job = (object.job !== undefined && object.job !== null) ? Job.fromPartial(object.job) : undefined;
    message.requestId = object.requestId ?? "";
    return message;
  },
};

function createBaseJobMetadata(): JobMetadata {
  return { jobId: "", status: undefined, operationType: "", startTime: undefined };
}

export const JobMetadata: MessageFns<JobMetadata> = {
  encode(message: JobMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.jobId !== "") {
      writer.uint32(10).string(message.jobId);
    }
    if (message.status !== undefined) {
      JobStatus.encode(message.status, writer.uint32(18).fork()).join();
    }
    if (message.operationType !== "") {
      writer.uint32(26).string(message.operationType);
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): JobMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseJobMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.jobId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.status = JobStatus.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.operationType = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): JobMetadata {
    return {
      jobId: isSet(object.jobId) ? globalThis.String(object.jobId) : "",
      status: isSet(object.status) ? JobStatus.fromJSON(object.status) : undefined,
      operationType: isSet(object.operationType) ? globalThis.String(object.operationType) : "",
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
    };
  },

  toJSON(message: JobMetadata): unknown {
    const obj: any = {};
    if (message.jobId !== "") {
      obj.jobId = message.jobId;
    }
    if (message.status !== undefined) {
      obj.status = JobStatus.toJSON(message.status);
    }
    if (message.operationType !== "") {
      obj.operationType = message.operationType;
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<JobMetadata>): JobMetadata {
    return JobMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<JobMetadata>): JobMetadata {
    const message = createBaseJobMetadata();
    message.jobId = object.jobId ?? "";
    message.status = (object.status !== undefined && object.status !== null)
      ? JobStatus.fromPartial(object.status)
      : undefined;
    message.operationType = object.operationType ?? "";
    message.startTime = object.startTime ?? undefined;
    return message;
  },
};

function createBaseGetJobRequest(): GetJobRequest {
  return { projectId: "", region: "", jobId: "" };
}

export const GetJobRequest: MessageFns<GetJobRequest> = {
  encode(message: GetJobRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.region !== "") {
      writer.uint32(26).string(message.region);
    }
    if (message.jobId !== "") {
      writer.uint32(18).string(message.jobId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetJobRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetJobRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.region = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.jobId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetJobRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      jobId: isSet(object.jobId) ? globalThis.String(object.jobId) : "",
    };
  },

  toJSON(message: GetJobRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.jobId !== "") {
      obj.jobId = message.jobId;
    }
    return obj;
  },

  create(base?: DeepPartial<GetJobRequest>): GetJobRequest {
    return GetJobRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetJobRequest>): GetJobRequest {
    const message = createBaseGetJobRequest();
    message.projectId = object.projectId ?? "";
    message.region = object.region ?? "";
    message.jobId = object.jobId ?? "";
    return message;
  },
};

function createBaseListJobsRequest(): ListJobsRequest {
  return { projectId: "", region: "", pageSize: 0, pageToken: "", clusterName: "", jobStateMatcher: 0, filter: "" };
}

export const ListJobsRequest: MessageFns<ListJobsRequest> = {
  encode(message: ListJobsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.region !== "") {
      writer.uint32(50).string(message.region);
    }
    if (message.pageSize !== 0) {
      writer.uint32(16).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(26).string(message.pageToken);
    }
    if (message.clusterName !== "") {
      writer.uint32(34).string(message.clusterName);
    }
    if (message.jobStateMatcher !== 0) {
      writer.uint32(40).int32(message.jobStateMatcher);
    }
    if (message.filter !== "") {
      writer.uint32(58).string(message.filter);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListJobsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListJobsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.region = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pageToken = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.clusterName = reader.string();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.jobStateMatcher = reader.int32() as any;
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.filter = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListJobsRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
      clusterName: isSet(object.clusterName) ? globalThis.String(object.clusterName) : "",
      jobStateMatcher: isSet(object.jobStateMatcher)
        ? listJobsRequest_JobStateMatcherFromJSON(object.jobStateMatcher)
        : 0,
      filter: isSet(object.filter) ? globalThis.String(object.filter) : "",
    };
  },

  toJSON(message: ListJobsRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    if (message.clusterName !== "") {
      obj.clusterName = message.clusterName;
    }
    if (message.jobStateMatcher !== 0) {
      obj.jobStateMatcher = listJobsRequest_JobStateMatcherToJSON(message.jobStateMatcher);
    }
    if (message.filter !== "") {
      obj.filter = message.filter;
    }
    return obj;
  },

  create(base?: DeepPartial<ListJobsRequest>): ListJobsRequest {
    return ListJobsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListJobsRequest>): ListJobsRequest {
    const message = createBaseListJobsRequest();
    message.projectId = object.projectId ?? "";
    message.region = object.region ?? "";
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    message.clusterName = object.clusterName ?? "";
    message.jobStateMatcher = object.jobStateMatcher ?? 0;
    message.filter = object.filter ?? "";
    return message;
  },
};

function createBaseUpdateJobRequest(): UpdateJobRequest {
  return { projectId: "", region: "", jobId: "", job: undefined, updateMask: undefined };
}

export const UpdateJobRequest: MessageFns<UpdateJobRequest> = {
  encode(message: UpdateJobRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.region !== "") {
      writer.uint32(18).string(message.region);
    }
    if (message.jobId !== "") {
      writer.uint32(26).string(message.jobId);
    }
    if (message.job !== undefined) {
      Job.encode(message.job, writer.uint32(34).fork()).join();
    }
    if (message.updateMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.updateMask), writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateJobRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateJobRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.region = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.jobId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.job = Job.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.updateMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateJobRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      jobId: isSet(object.jobId) ? globalThis.String(object.jobId) : "",
      job: isSet(object.job) ? Job.fromJSON(object.job) : undefined,
      updateMask: isSet(object.updateMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.updateMask)) : undefined,
    };
  },

  toJSON(message: UpdateJobRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.jobId !== "") {
      obj.jobId = message.jobId;
    }
    if (message.job !== undefined) {
      obj.job = Job.toJSON(message.job);
    }
    if (message.updateMask !== undefined) {
      obj.updateMask = FieldMask.toJSON(FieldMask.wrap(message.updateMask));
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateJobRequest>): UpdateJobRequest {
    return UpdateJobRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateJobRequest>): UpdateJobRequest {
    const message = createBaseUpdateJobRequest();
    message.projectId = object.projectId ?? "";
    message.region = object.region ?? "";
    message.jobId = object.jobId ?? "";
    message.job = (object.job !== undefined && object.job !== null) ? Job.fromPartial(object.job) : undefined;
    message.updateMask = object.updateMask ?? undefined;
    return message;
  },
};

function createBaseListJobsResponse(): ListJobsResponse {
  return { jobs: [], nextPageToken: "", unreachable: [] };
}

export const ListJobsResponse: MessageFns<ListJobsResponse> = {
  encode(message: ListJobsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.jobs) {
      Job.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    for (const v of message.unreachable) {
      writer.uint32(26).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListJobsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListJobsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.jobs.push(Job.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.unreachable.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListJobsResponse {
    return {
      jobs: globalThis.Array.isArray(object?.jobs) ? object.jobs.map((e: any) => Job.fromJSON(e)) : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
      unreachable: globalThis.Array.isArray(object?.unreachable)
        ? object.unreachable.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: ListJobsResponse): unknown {
    const obj: any = {};
    if (message.jobs?.length) {
      obj.jobs = message.jobs.map((e) => Job.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    if (message.unreachable?.length) {
      obj.unreachable = message.unreachable;
    }
    return obj;
  },

  create(base?: DeepPartial<ListJobsResponse>): ListJobsResponse {
    return ListJobsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListJobsResponse>): ListJobsResponse {
    const message = createBaseListJobsResponse();
    message.jobs = object.jobs?.map((e) => Job.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    message.unreachable = object.unreachable?.map((e) => e) || [];
    return message;
  },
};

function createBaseCancelJobRequest(): CancelJobRequest {
  return { projectId: "", region: "", jobId: "" };
}

export const CancelJobRequest: MessageFns<CancelJobRequest> = {
  encode(message: CancelJobRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.region !== "") {
      writer.uint32(26).string(message.region);
    }
    if (message.jobId !== "") {
      writer.uint32(18).string(message.jobId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CancelJobRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCancelJobRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.region = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.jobId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CancelJobRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      jobId: isSet(object.jobId) ? globalThis.String(object.jobId) : "",
    };
  },

  toJSON(message: CancelJobRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.jobId !== "") {
      obj.jobId = message.jobId;
    }
    return obj;
  },

  create(base?: DeepPartial<CancelJobRequest>): CancelJobRequest {
    return CancelJobRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CancelJobRequest>): CancelJobRequest {
    const message = createBaseCancelJobRequest();
    message.projectId = object.projectId ?? "";
    message.region = object.region ?? "";
    message.jobId = object.jobId ?? "";
    return message;
  },
};

function createBaseDeleteJobRequest(): DeleteJobRequest {
  return { projectId: "", region: "", jobId: "" };
}

export const DeleteJobRequest: MessageFns<DeleteJobRequest> = {
  encode(message: DeleteJobRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.region !== "") {
      writer.uint32(26).string(message.region);
    }
    if (message.jobId !== "") {
      writer.uint32(18).string(message.jobId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeleteJobRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeleteJobRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.region = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.jobId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeleteJobRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      region: isSet(object.region) ? globalThis.String(object.region) : "",
      jobId: isSet(object.jobId) ? globalThis.String(object.jobId) : "",
    };
  },

  toJSON(message: DeleteJobRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.region !== "") {
      obj.region = message.region;
    }
    if (message.jobId !== "") {
      obj.jobId = message.jobId;
    }
    return obj;
  },

  create(base?: DeepPartial<DeleteJobRequest>): DeleteJobRequest {
    return DeleteJobRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeleteJobRequest>): DeleteJobRequest {
    const message = createBaseDeleteJobRequest();
    message.projectId = object.projectId ?? "";
    message.region = object.region ?? "";
    message.jobId = object.jobId ?? "";
    return message;
  },
};

/** The JobController provides methods to manage jobs. */
export type JobControllerDefinition = typeof JobControllerDefinition;
export const JobControllerDefinition = {
  name: "JobController",
  fullName: "google.cloud.dataproc.v1.JobController",
  methods: {
    /** Submits a job to a cluster. */
    submitJob: {
      name: "SubmitJob",
      requestType: SubmitJobRequest,
      requestStream: false,
      responseType: Job,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              21,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              44,
              114,
              101,
              103,
              105,
              111,
              110,
              44,
              106,
              111,
              98,
            ]),
          ],
          578365826: [
            Buffer.from([
              59,
              58,
              1,
              42,
              34,
              54,
              47,
              118,
              49,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              114,
              101,
              103,
              105,
              111,
              110,
              115,
              47,
              123,
              114,
              101,
              103,
              105,
              111,
              110,
              125,
              47,
              106,
              111,
              98,
              115,
              58,
              115,
              117,
              98,
              109,
              105,
              116,
            ]),
          ],
        },
      },
    },
    /** Submits job to a cluster. */
    submitJobAsOperation: {
      name: "SubmitJobAsOperation",
      requestType: SubmitJobRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [Buffer.from([18, 10, 3, 74, 111, 98, 18, 11, 74, 111, 98, 77, 101, 116, 97, 100, 97, 116, 97])],
          8410: [
            Buffer.from([
              23,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              44,
              32,
              114,
              101,
              103,
              105,
              111,
              110,
              44,
              32,
              106,
              111,
              98,
            ]),
          ],
          578365826: [
            Buffer.from([
              70,
              58,
              1,
              42,
              34,
              65,
              47,
              118,
              49,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              114,
              101,
              103,
              105,
              111,
              110,
              115,
              47,
              123,
              114,
              101,
              103,
              105,
              111,
              110,
              125,
              47,
              106,
              111,
              98,
              115,
              58,
              115,
              117,
              98,
              109,
              105,
              116,
              65,
              115,
              79,
              112,
              101,
              114,
              97,
              116,
              105,
              111,
              110,
            ]),
          ],
        },
      },
    },
    /** Gets the resource representation for a job in a project. */
    getJob: {
      name: "GetJob",
      requestType: GetJobRequest,
      requestStream: false,
      responseType: Job,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              24,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              44,
              114,
              101,
              103,
              105,
              111,
              110,
              44,
              106,
              111,
              98,
              95,
              105,
              100,
            ]),
          ],
          578365826: [
            Buffer.from([
              58,
              18,
              56,
              47,
              118,
              49,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              114,
              101,
              103,
              105,
              111,
              110,
              115,
              47,
              123,
              114,
              101,
              103,
              105,
              111,
              110,
              125,
              47,
              106,
              111,
              98,
              115,
              47,
              123,
              106,
              111,
              98,
              95,
              105,
              100,
              125,
            ]),
          ],
        },
      },
    },
    /** Lists regions/{region}/jobs in a project. */
    listJobs: {
      name: "ListJobs",
      requestType: ListJobsRequest,
      requestStream: false,
      responseType: ListJobsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([17, 112, 114, 111, 106, 101, 99, 116, 95, 105, 100, 44, 114, 101, 103, 105, 111, 110]),
            Buffer.from([
              24,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              44,
              114,
              101,
              103,
              105,
              111,
              110,
              44,
              102,
              105,
              108,
              116,
              101,
              114,
            ]),
          ],
          578365826: [
            Buffer.from([
              49,
              18,
              47,
              47,
              118,
              49,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              114,
              101,
              103,
              105,
              111,
              110,
              115,
              47,
              123,
              114,
              101,
              103,
              105,
              111,
              110,
              125,
              47,
              106,
              111,
              98,
              115,
            ]),
          ],
        },
      },
    },
    /** Updates a job in a project. */
    updateJob: {
      name: "UpdateJob",
      requestType: UpdateJobRequest,
      requestStream: false,
      responseType: Job,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              63,
              58,
              3,
              106,
              111,
              98,
              50,
              56,
              47,
              118,
              49,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              114,
              101,
              103,
              105,
              111,
              110,
              115,
              47,
              123,
              114,
              101,
              103,
              105,
              111,
              110,
              125,
              47,
              106,
              111,
              98,
              115,
              47,
              123,
              106,
              111,
              98,
              95,
              105,
              100,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Starts a job cancellation request. To access the job resource
     * after cancellation, call
     * [regions/{region}/jobs.list](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/list)
     * or
     * [regions/{region}/jobs.get](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/get).
     */
    cancelJob: {
      name: "CancelJob",
      requestType: CancelJobRequest,
      requestStream: false,
      responseType: Job,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              24,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              44,
              114,
              101,
              103,
              105,
              111,
              110,
              44,
              106,
              111,
              98,
              95,
              105,
              100,
            ]),
          ],
          578365826: [
            Buffer.from([
              68,
              58,
              1,
              42,
              34,
              63,
              47,
              118,
              49,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              114,
              101,
              103,
              105,
              111,
              110,
              115,
              47,
              123,
              114,
              101,
              103,
              105,
              111,
              110,
              125,
              47,
              106,
              111,
              98,
              115,
              47,
              123,
              106,
              111,
              98,
              95,
              105,
              100,
              125,
              58,
              99,
              97,
              110,
              99,
              101,
              108,
            ]),
          ],
        },
      },
    },
    /**
     * Deletes the job from the project. If the job is active, the delete fails,
     * and the response returns `FAILED_PRECONDITION`.
     */
    deleteJob: {
      name: "DeleteJob",
      requestType: DeleteJobRequest,
      requestStream: false,
      responseType: Empty,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              24,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              44,
              114,
              101,
              103,
              105,
              111,
              110,
              44,
              106,
              111,
              98,
              95,
              105,
              100,
            ]),
          ],
          578365826: [
            Buffer.from([
              58,
              42,
              56,
              47,
              118,
              49,
              47,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              123,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              95,
              105,
              100,
              125,
              47,
              114,
              101,
              103,
              105,
              111,
              110,
              115,
              47,
              123,
              114,
              101,
              103,
              105,
              111,
              110,
              125,
              47,
              106,
              111,
              98,
              115,
              47,
              123,
              106,
              111,
              98,
              95,
              105,
              100,
              125,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface JobControllerServiceImplementation<CallContextExt = {}> {
  /** Submits a job to a cluster. */
  submitJob(request: SubmitJobRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Job>>;
  /** Submits job to a cluster. */
  submitJobAsOperation(
    request: SubmitJobRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Operation>>;
  /** Gets the resource representation for a job in a project. */
  getJob(request: GetJobRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Job>>;
  /** Lists regions/{region}/jobs in a project. */
  listJobs(request: ListJobsRequest, context: CallContext & CallContextExt): Promise<DeepPartial<ListJobsResponse>>;
  /** Updates a job in a project. */
  updateJob(request: UpdateJobRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Job>>;
  /**
   * Starts a job cancellation request. To access the job resource
   * after cancellation, call
   * [regions/{region}/jobs.list](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/list)
   * or
   * [regions/{region}/jobs.get](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/get).
   */
  cancelJob(request: CancelJobRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Job>>;
  /**
   * Deletes the job from the project. If the job is active, the delete fails,
   * and the response returns `FAILED_PRECONDITION`.
   */
  deleteJob(request: DeleteJobRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Empty>>;
}

export interface JobControllerClient<CallOptionsExt = {}> {
  /** Submits a job to a cluster. */
  submitJob(request: DeepPartial<SubmitJobRequest>, options?: CallOptions & CallOptionsExt): Promise<Job>;
  /** Submits job to a cluster. */
  submitJobAsOperation(
    request: DeepPartial<SubmitJobRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Operation>;
  /** Gets the resource representation for a job in a project. */
  getJob(request: DeepPartial<GetJobRequest>, options?: CallOptions & CallOptionsExt): Promise<Job>;
  /** Lists regions/{region}/jobs in a project. */
  listJobs(request: DeepPartial<ListJobsRequest>, options?: CallOptions & CallOptionsExt): Promise<ListJobsResponse>;
  /** Updates a job in a project. */
  updateJob(request: DeepPartial<UpdateJobRequest>, options?: CallOptions & CallOptionsExt): Promise<Job>;
  /**
   * Starts a job cancellation request. To access the job resource
   * after cancellation, call
   * [regions/{region}/jobs.list](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/list)
   * or
   * [regions/{region}/jobs.get](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/get).
   */
  cancelJob(request: DeepPartial<CancelJobRequest>, options?: CallOptions & CallOptionsExt): Promise<Job>;
  /**
   * Deletes the job from the project. If the job is active, the delete fails,
   * and the response returns `FAILED_PRECONDITION`.
   */
  deleteJob(request: DeepPartial<DeleteJobRequest>, options?: CallOptions & CallOptionsExt): Promise<Empty>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
