// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/texttospeech/v1beta1/cloud_tts.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";

export const protobufPackage = "google.cloud.texttospeech.v1beta1";

/**
 * Gender of the voice as described in
 * [SSML voice element](https://www.w3.org/TR/speech-synthesis11/#edef_voice).
 */
export enum SsmlVoiceGender {
  /**
   * SSML_VOICE_GENDER_UNSPECIFIED - An unspecified gender.
   * In VoiceSelectionParams, this means that the client doesn't care which
   * gender the selected voice will have. In the Voice field of
   * ListVoicesResponse, this may mean that the voice doesn't fit any of the
   * other categories in this enum, or that the gender of the voice isn't known.
   */
  SSML_VOICE_GENDER_UNSPECIFIED = 0,
  /** MALE - A male voice. */
  MALE = 1,
  /** FEMALE - A female voice. */
  FEMALE = 2,
  /** NEUTRAL - A gender-neutral voice. This voice is not yet supported. */
  NEUTRAL = 3,
  UNRECOGNIZED = -1,
}

export function ssmlVoiceGenderFromJSON(object: any): SsmlVoiceGender {
  switch (object) {
    case 0:
    case "SSML_VOICE_GENDER_UNSPECIFIED":
      return SsmlVoiceGender.SSML_VOICE_GENDER_UNSPECIFIED;
    case 1:
    case "MALE":
      return SsmlVoiceGender.MALE;
    case 2:
    case "FEMALE":
      return SsmlVoiceGender.FEMALE;
    case 3:
    case "NEUTRAL":
      return SsmlVoiceGender.NEUTRAL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SsmlVoiceGender.UNRECOGNIZED;
  }
}

export function ssmlVoiceGenderToJSON(object: SsmlVoiceGender): string {
  switch (object) {
    case SsmlVoiceGender.SSML_VOICE_GENDER_UNSPECIFIED:
      return "SSML_VOICE_GENDER_UNSPECIFIED";
    case SsmlVoiceGender.MALE:
      return "MALE";
    case SsmlVoiceGender.FEMALE:
      return "FEMALE";
    case SsmlVoiceGender.NEUTRAL:
      return "NEUTRAL";
    case SsmlVoiceGender.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Configuration to set up audio encoder. The encoding determines the output
 * audio format that we'd like.
 */
export enum AudioEncoding {
  /**
   * AUDIO_ENCODING_UNSPECIFIED - Not specified. Will return result
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT].
   */
  AUDIO_ENCODING_UNSPECIFIED = 0,
  /**
   * LINEAR16 - Uncompressed 16-bit signed little-endian samples (Linear PCM).
   * Audio content returned as LINEAR16 also contains a WAV header.
   */
  LINEAR16 = 1,
  /** MP3 - MP3 audio at 32kbps. */
  MP3 = 2,
  /** MP3_64_KBPS - MP3 at 64kbps. */
  MP3_64_KBPS = 4,
  /**
   * OGG_OPUS - Opus encoded audio wrapped in an ogg container. The result will be a
   * file which can be played natively on Android, and in browsers (at least
   * Chrome and Firefox). The quality of the encoding is considerably higher
   * than MP3 while using approximately the same bitrate.
   */
  OGG_OPUS = 3,
  /**
   * MULAW - 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
   * Audio content returned as MULAW also contains a WAV header.
   */
  MULAW = 5,
  /**
   * ALAW - 8-bit samples that compand 14-bit audio samples using G.711 PCMU/A-law.
   * Audio content returned as ALAW also contains a WAV header.
   */
  ALAW = 6,
  UNRECOGNIZED = -1,
}

export function audioEncodingFromJSON(object: any): AudioEncoding {
  switch (object) {
    case 0:
    case "AUDIO_ENCODING_UNSPECIFIED":
      return AudioEncoding.AUDIO_ENCODING_UNSPECIFIED;
    case 1:
    case "LINEAR16":
      return AudioEncoding.LINEAR16;
    case 2:
    case "MP3":
      return AudioEncoding.MP3;
    case 4:
    case "MP3_64_KBPS":
      return AudioEncoding.MP3_64_KBPS;
    case 3:
    case "OGG_OPUS":
      return AudioEncoding.OGG_OPUS;
    case 5:
    case "MULAW":
      return AudioEncoding.MULAW;
    case 6:
    case "ALAW":
      return AudioEncoding.ALAW;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AudioEncoding.UNRECOGNIZED;
  }
}

export function audioEncodingToJSON(object: AudioEncoding): string {
  switch (object) {
    case AudioEncoding.AUDIO_ENCODING_UNSPECIFIED:
      return "AUDIO_ENCODING_UNSPECIFIED";
    case AudioEncoding.LINEAR16:
      return "LINEAR16";
    case AudioEncoding.MP3:
      return "MP3";
    case AudioEncoding.MP3_64_KBPS:
      return "MP3_64_KBPS";
    case AudioEncoding.OGG_OPUS:
      return "OGG_OPUS";
    case AudioEncoding.MULAW:
      return "MULAW";
    case AudioEncoding.ALAW:
      return "ALAW";
    case AudioEncoding.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The top-level message sent by the client for the `ListVoices` method. */
export interface ListVoicesRequest {
  /**
   * Optional. Recommended.
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * If not specified, the API will return all supported voices.
   * If specified, the ListVoices call will only return voices that can be used
   * to synthesize this language_code. For example, if you specify `"en-NZ"`,
   * all `"en-NZ"` voices will be returned. If you specify `"no"`, both
   * `"no-\*"` (Norwegian) and `"nb-\*"` (Norwegian Bokmal) voices will be
   * returned.
   */
  languageCode: string;
}

/** The message returned to the client by the `ListVoices` method. */
export interface ListVoicesResponse {
  /** The list of voices. */
  voices: Voice[];
}

/** Description of a voice supported by the TTS service. */
export interface Voice {
  /**
   * The languages that this voice supports, expressed as
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tags (e.g.
   * "en-US", "es-419", "cmn-tw").
   */
  languageCodes: string[];
  /** The name of this voice.  Each distinct voice has a unique name. */
  name: string;
  /** The gender of this voice. */
  ssmlGender: SsmlVoiceGender;
  /** The natural sample rate (in hertz) for this voice. */
  naturalSampleRateHertz: number;
}

/** The top-level message sent by the client for the `SynthesizeSpeech` method. */
export interface SynthesizeSpeechRequest {
  /** Required. The Synthesizer requires either plain text or SSML as input. */
  input:
    | SynthesisInput
    | undefined;
  /** Required. The desired voice of the synthesized audio. */
  voice:
    | VoiceSelectionParams
    | undefined;
  /** Required. The configuration of the synthesized audio. */
  audioConfig:
    | AudioConfig
    | undefined;
  /** Whether and what timepoints are returned in the response. */
  enableTimePointing: SynthesizeSpeechRequest_TimepointType[];
}

/** The type of timepoint information that is returned in the response. */
export enum SynthesizeSpeechRequest_TimepointType {
  /** TIMEPOINT_TYPE_UNSPECIFIED - Not specified. No timepoint information will be returned. */
  TIMEPOINT_TYPE_UNSPECIFIED = 0,
  /** SSML_MARK - Timepoint information of `<mark>` tags in SSML input will be returned. */
  SSML_MARK = 1,
  UNRECOGNIZED = -1,
}

export function synthesizeSpeechRequest_TimepointTypeFromJSON(object: any): SynthesizeSpeechRequest_TimepointType {
  switch (object) {
    case 0:
    case "TIMEPOINT_TYPE_UNSPECIFIED":
      return SynthesizeSpeechRequest_TimepointType.TIMEPOINT_TYPE_UNSPECIFIED;
    case 1:
    case "SSML_MARK":
      return SynthesizeSpeechRequest_TimepointType.SSML_MARK;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SynthesizeSpeechRequest_TimepointType.UNRECOGNIZED;
  }
}

export function synthesizeSpeechRequest_TimepointTypeToJSON(object: SynthesizeSpeechRequest_TimepointType): string {
  switch (object) {
    case SynthesizeSpeechRequest_TimepointType.TIMEPOINT_TYPE_UNSPECIFIED:
      return "TIMEPOINT_TYPE_UNSPECIFIED";
    case SynthesizeSpeechRequest_TimepointType.SSML_MARK:
      return "SSML_MARK";
    case SynthesizeSpeechRequest_TimepointType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Contains text input to be synthesized. Either `text` or `ssml` must be
 * supplied. Supplying both or neither returns
 * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]. The
 * input size is limited to 5000 bytes.
 */
export interface SynthesisInput {
  /** The raw text to be synthesized. */
  text?:
    | string
    | undefined;
  /**
   * The SSML document to be synthesized. The SSML document must be valid
   * and well-formed. Otherwise the RPC will fail and return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]. For
   * more information, see
   * [SSML](https://cloud.google.com/text-to-speech/docs/ssml).
   */
  ssml?: string | undefined;
}

/** Description of which voice to use for a synthesis request. */
export interface VoiceSelectionParams {
  /**
   * Required. The language (and potentially also the region) of the voice
   * expressed as a [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt)
   * language tag, e.g. "en-US". This should not include a script tag (e.g. use
   * "cmn-cn" rather than "cmn-Hant-cn"), because the script will be inferred
   * from the input provided in the SynthesisInput.  The TTS service
   * will use this parameter to help choose an appropriate voice.  Note that
   * the TTS service may choose a voice with a slightly different language code
   * than the one selected; it may substitute a different region
   * (e.g. using en-US rather than en-CA if there isn't a Canadian voice
   * available), or even a different language, e.g. using "nb" (Norwegian
   * Bokmal) instead of "no" (Norwegian)".
   */
  languageCode: string;
  /**
   * The name of the voice. If both the name and the gender are not set,
   * the service will choose a voice based on the other parameters such as
   * language_code.
   */
  name: string;
  /**
   * The preferred gender of the voice. If not set, the service will
   * choose a voice based on the other parameters such as language_code and
   * name. Note that this is only a preference, not requirement; if a
   * voice of the appropriate gender is not available, the synthesizer should
   * substitute a voice with a different gender rather than failing the request.
   */
  ssmlGender: SsmlVoiceGender;
  /**
   * The configuration for a custom voice. If [CustomVoiceParams.model] is set,
   * the service will choose the custom voice matching the specified
   * configuration.
   */
  customVoice: CustomVoiceParams | undefined;
}

/** Description of audio data to be synthesized. */
export interface AudioConfig {
  /** Required. The format of the audio byte stream. */
  audioEncoding: AudioEncoding;
  /**
   * Optional. Input only. Speaking rate/speed, in the range [0.25, 4.0]. 1.0 is
   * the normal native speed supported by the specific voice. 2.0 is twice as
   * fast, and 0.5 is half as fast. If unset(0.0), defaults to the native 1.0
   * speed. Any other values < 0.25 or > 4.0 will return an error.
   */
  speakingRate: number;
  /**
   * Optional. Input only. Speaking pitch, in the range [-20.0, 20.0]. 20 means
   * increase 20 semitones from the original pitch. -20 means decrease 20
   * semitones from the original pitch.
   */
  pitch: number;
  /**
   * Optional. Input only. Volume gain (in dB) of the normal native volume
   * supported by the specific voice, in the range [-96.0, 16.0]. If unset, or
   * set to a value of 0.0 (dB), will play at normal native signal amplitude. A
   * value of -6.0 (dB) will play at approximately half the amplitude of the
   * normal native signal amplitude. A value of +6.0 (dB) will play at
   * approximately twice the amplitude of the normal native signal amplitude.
   * Strongly recommend not to exceed +10 (dB) as there's usually no effective
   * increase in loudness for any value greater than that.
   */
  volumeGainDb: number;
  /**
   * Optional. The synthesis sample rate (in hertz) for this audio. When this is
   * specified in SynthesizeSpeechRequest, if this is different from the voice's
   * natural sample rate, then the synthesizer will honor this request by
   * converting to the desired sample rate (which might result in worse audio
   * quality), unless the specified sample rate is not supported for the
   * encoding chosen, in which case it will fail the request and return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT].
   */
  sampleRateHertz: number;
  /**
   * Optional. Input only. An identifier which selects 'audio effects' profiles
   * that are applied on (post synthesized) text to speech. Effects are applied
   * on top of each other in the order they are given. See
   * [audio
   * profiles](https://cloud.google.com/text-to-speech/docs/audio-profiles) for
   * current supported profile ids.
   */
  effectsProfileId: string[];
}

/** Description of the custom voice to be synthesized. */
export interface CustomVoiceParams {
  /** Required. The name of the AutoML model that synthesizes the custom voice. */
  model: string;
  /**
   * Optional. Deprecated. The usage of the synthesized audio to be reported.
   *
   * @deprecated
   */
  reportedUsage: CustomVoiceParams_ReportedUsage;
}

/**
 * Deprecated. The usage of the synthesized audio. Usage does not affect
 * billing.
 */
export enum CustomVoiceParams_ReportedUsage {
  /** REPORTED_USAGE_UNSPECIFIED - Request with reported usage unspecified will be rejected. */
  REPORTED_USAGE_UNSPECIFIED = 0,
  /**
   * REALTIME - For scenarios where the synthesized audio is not downloadable and can
   * only be used once. For example, real-time request in IVR system.
   */
  REALTIME = 1,
  /**
   * OFFLINE - For scenarios where the synthesized audio is downloadable and can be
   * reused. For example, the synthesized audio is downloaded, stored in
   * customer service system and played repeatedly.
   */
  OFFLINE = 2,
  UNRECOGNIZED = -1,
}

export function customVoiceParams_ReportedUsageFromJSON(object: any): CustomVoiceParams_ReportedUsage {
  switch (object) {
    case 0:
    case "REPORTED_USAGE_UNSPECIFIED":
      return CustomVoiceParams_ReportedUsage.REPORTED_USAGE_UNSPECIFIED;
    case 1:
    case "REALTIME":
      return CustomVoiceParams_ReportedUsage.REALTIME;
    case 2:
    case "OFFLINE":
      return CustomVoiceParams_ReportedUsage.OFFLINE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return CustomVoiceParams_ReportedUsage.UNRECOGNIZED;
  }
}

export function customVoiceParams_ReportedUsageToJSON(object: CustomVoiceParams_ReportedUsage): string {
  switch (object) {
    case CustomVoiceParams_ReportedUsage.REPORTED_USAGE_UNSPECIFIED:
      return "REPORTED_USAGE_UNSPECIFIED";
    case CustomVoiceParams_ReportedUsage.REALTIME:
      return "REALTIME";
    case CustomVoiceParams_ReportedUsage.OFFLINE:
      return "OFFLINE";
    case CustomVoiceParams_ReportedUsage.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The message returned to the client by the `SynthesizeSpeech` method. */
export interface SynthesizeSpeechResponse {
  /**
   * The audio data bytes encoded as specified in the request, including the
   * header for encodings that are wrapped in containers (e.g. MP3, OGG_OPUS).
   * For LINEAR16 audio, we include the WAV header. Note: as
   * with all bytes fields, protobuffers use a pure binary representation,
   * whereas JSON representations use base64.
   */
  audioContent: Buffer;
  /**
   * A link between a position in the original request input and a corresponding
   * time in the output audio. It's only supported via `<mark>` of SSML input.
   */
  timepoints: Timepoint[];
  /** The audio metadata of `audio_content`. */
  audioConfig: AudioConfig | undefined;
}

/**
 * This contains a mapping between a certain point in the input text and a
 * corresponding time in the output audio.
 */
export interface Timepoint {
  /** Timepoint name as received from the client within `<mark>` tag. */
  markName: string;
  /** Time offset in seconds from the start of the synthesized audio. */
  timeSeconds: number;
}

/** Provides configuration information for the StreamingSynthesize request. */
export interface StreamingSynthesizeConfig {
  /** Required. The desired voice of the synthesized audio. */
  voice: VoiceSelectionParams | undefined;
}

/** Input to be synthesized. */
export interface StreamingSynthesisInput {
  /**
   * The raw text to be synthesized. It is recommended that each input
   * contains complete, terminating sentences, as this will likely result in
   * better prosody in the output audio. That being said, users are free to
   * input text however they please.
   */
  text?: string | undefined;
}

/**
 * Request message for the `StreamingSynthesize` method. Multiple
 * `StreamingSynthesizeRequest` messages are sent in one call.
 * The first message must contain a `streaming_config` that
 * fully specifies the request configuration and must not contain `input`. All
 * subsequent messages must only have `input` set.
 */
export interface StreamingSynthesizeRequest {
  /**
   * StreamingSynthesizeConfig to be used in this streaming attempt. Only
   * specified in the first message sent in a `StreamingSynthesize` call.
   */
  streamingConfig?:
    | StreamingSynthesizeConfig
    | undefined;
  /**
   * Input to synthesize. Specified in all messages but the first in a
   * `StreamingSynthesize` call.
   */
  input?: StreamingSynthesisInput | undefined;
}

/**
 * `StreamingSynthesizeResponse` is the only message returned to the
 * client by `StreamingSynthesize` method. A series of zero or more
 * `StreamingSynthesizeResponse` messages are streamed back to the client.
 */
export interface StreamingSynthesizeResponse {
  /**
   * The audio data bytes encoded as specified in the request. This is
   * headerless LINEAR16 audio with a sample rate of 24000.
   */
  audioContent: Buffer;
}

function createBaseListVoicesRequest(): ListVoicesRequest {
  return { languageCode: "" };
}

export const ListVoicesRequest: MessageFns<ListVoicesRequest> = {
  encode(message: ListVoicesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.languageCode !== "") {
      writer.uint32(10).string(message.languageCode);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListVoicesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListVoicesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.languageCode = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListVoicesRequest {
    return { languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "" };
  },

  toJSON(message: ListVoicesRequest): unknown {
    const obj: any = {};
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    return obj;
  },

  create(base?: DeepPartial<ListVoicesRequest>): ListVoicesRequest {
    return ListVoicesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListVoicesRequest>): ListVoicesRequest {
    const message = createBaseListVoicesRequest();
    message.languageCode = object.languageCode ?? "";
    return message;
  },
};

function createBaseListVoicesResponse(): ListVoicesResponse {
  return { voices: [] };
}

export const ListVoicesResponse: MessageFns<ListVoicesResponse> = {
  encode(message: ListVoicesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.voices) {
      Voice.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListVoicesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListVoicesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.voices.push(Voice.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListVoicesResponse {
    return { voices: globalThis.Array.isArray(object?.voices) ? object.voices.map((e: any) => Voice.fromJSON(e)) : [] };
  },

  toJSON(message: ListVoicesResponse): unknown {
    const obj: any = {};
    if (message.voices?.length) {
      obj.voices = message.voices.map((e) => Voice.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ListVoicesResponse>): ListVoicesResponse {
    return ListVoicesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListVoicesResponse>): ListVoicesResponse {
    const message = createBaseListVoicesResponse();
    message.voices = object.voices?.map((e) => Voice.fromPartial(e)) || [];
    return message;
  },
};

function createBaseVoice(): Voice {
  return { languageCodes: [], name: "", ssmlGender: 0, naturalSampleRateHertz: 0 };
}

export const Voice: MessageFns<Voice> = {
  encode(message: Voice, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.languageCodes) {
      writer.uint32(10).string(v!);
    }
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    if (message.ssmlGender !== 0) {
      writer.uint32(24).int32(message.ssmlGender);
    }
    if (message.naturalSampleRateHertz !== 0) {
      writer.uint32(32).int32(message.naturalSampleRateHertz);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Voice {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVoice();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.languageCodes.push(reader.string());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.ssmlGender = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.naturalSampleRateHertz = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Voice {
    return {
      languageCodes: globalThis.Array.isArray(object?.languageCodes)
        ? object.languageCodes.map((e: any) => globalThis.String(e))
        : [],
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      ssmlGender: isSet(object.ssmlGender) ? ssmlVoiceGenderFromJSON(object.ssmlGender) : 0,
      naturalSampleRateHertz: isSet(object.naturalSampleRateHertz)
        ? globalThis.Number(object.naturalSampleRateHertz)
        : 0,
    };
  },

  toJSON(message: Voice): unknown {
    const obj: any = {};
    if (message.languageCodes?.length) {
      obj.languageCodes = message.languageCodes;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.ssmlGender !== 0) {
      obj.ssmlGender = ssmlVoiceGenderToJSON(message.ssmlGender);
    }
    if (message.naturalSampleRateHertz !== 0) {
      obj.naturalSampleRateHertz = Math.round(message.naturalSampleRateHertz);
    }
    return obj;
  },

  create(base?: DeepPartial<Voice>): Voice {
    return Voice.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Voice>): Voice {
    const message = createBaseVoice();
    message.languageCodes = object.languageCodes?.map((e) => e) || [];
    message.name = object.name ?? "";
    message.ssmlGender = object.ssmlGender ?? 0;
    message.naturalSampleRateHertz = object.naturalSampleRateHertz ?? 0;
    return message;
  },
};

function createBaseSynthesizeSpeechRequest(): SynthesizeSpeechRequest {
  return { input: undefined, voice: undefined, audioConfig: undefined, enableTimePointing: [] };
}

export const SynthesizeSpeechRequest: MessageFns<SynthesizeSpeechRequest> = {
  encode(message: SynthesizeSpeechRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.input !== undefined) {
      SynthesisInput.encode(message.input, writer.uint32(10).fork()).join();
    }
    if (message.voice !== undefined) {
      VoiceSelectionParams.encode(message.voice, writer.uint32(18).fork()).join();
    }
    if (message.audioConfig !== undefined) {
      AudioConfig.encode(message.audioConfig, writer.uint32(26).fork()).join();
    }
    writer.uint32(34).fork();
    for (const v of message.enableTimePointing) {
      writer.int32(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SynthesizeSpeechRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSynthesizeSpeechRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.input = SynthesisInput.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.voice = VoiceSelectionParams.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.audioConfig = AudioConfig.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag === 32) {
            message.enableTimePointing.push(reader.int32() as any);

            continue;
          }

          if (tag === 34) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.enableTimePointing.push(reader.int32() as any);
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SynthesizeSpeechRequest {
    return {
      input: isSet(object.input) ? SynthesisInput.fromJSON(object.input) : undefined,
      voice: isSet(object.voice) ? VoiceSelectionParams.fromJSON(object.voice) : undefined,
      audioConfig: isSet(object.audioConfig) ? AudioConfig.fromJSON(object.audioConfig) : undefined,
      enableTimePointing: globalThis.Array.isArray(object?.enableTimePointing)
        ? object.enableTimePointing.map((e: any) => synthesizeSpeechRequest_TimepointTypeFromJSON(e))
        : [],
    };
  },

  toJSON(message: SynthesizeSpeechRequest): unknown {
    const obj: any = {};
    if (message.input !== undefined) {
      obj.input = SynthesisInput.toJSON(message.input);
    }
    if (message.voice !== undefined) {
      obj.voice = VoiceSelectionParams.toJSON(message.voice);
    }
    if (message.audioConfig !== undefined) {
      obj.audioConfig = AudioConfig.toJSON(message.audioConfig);
    }
    if (message.enableTimePointing?.length) {
      obj.enableTimePointing = message.enableTimePointing.map((e) => synthesizeSpeechRequest_TimepointTypeToJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<SynthesizeSpeechRequest>): SynthesizeSpeechRequest {
    return SynthesizeSpeechRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SynthesizeSpeechRequest>): SynthesizeSpeechRequest {
    const message = createBaseSynthesizeSpeechRequest();
    message.input = (object.input !== undefined && object.input !== null)
      ? SynthesisInput.fromPartial(object.input)
      : undefined;
    message.voice = (object.voice !== undefined && object.voice !== null)
      ? VoiceSelectionParams.fromPartial(object.voice)
      : undefined;
    message.audioConfig = (object.audioConfig !== undefined && object.audioConfig !== null)
      ? AudioConfig.fromPartial(object.audioConfig)
      : undefined;
    message.enableTimePointing = object.enableTimePointing?.map((e) => e) || [];
    return message;
  },
};

function createBaseSynthesisInput(): SynthesisInput {
  return { text: undefined, ssml: undefined };
}

export const SynthesisInput: MessageFns<SynthesisInput> = {
  encode(message: SynthesisInput, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.text !== undefined) {
      writer.uint32(10).string(message.text);
    }
    if (message.ssml !== undefined) {
      writer.uint32(18).string(message.ssml);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SynthesisInput {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSynthesisInput();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.text = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.ssml = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SynthesisInput {
    return {
      text: isSet(object.text) ? globalThis.String(object.text) : undefined,
      ssml: isSet(object.ssml) ? globalThis.String(object.ssml) : undefined,
    };
  },

  toJSON(message: SynthesisInput): unknown {
    const obj: any = {};
    if (message.text !== undefined) {
      obj.text = message.text;
    }
    if (message.ssml !== undefined) {
      obj.ssml = message.ssml;
    }
    return obj;
  },

  create(base?: DeepPartial<SynthesisInput>): SynthesisInput {
    return SynthesisInput.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SynthesisInput>): SynthesisInput {
    const message = createBaseSynthesisInput();
    message.text = object.text ?? undefined;
    message.ssml = object.ssml ?? undefined;
    return message;
  },
};

function createBaseVoiceSelectionParams(): VoiceSelectionParams {
  return { languageCode: "", name: "", ssmlGender: 0, customVoice: undefined };
}

export const VoiceSelectionParams: MessageFns<VoiceSelectionParams> = {
  encode(message: VoiceSelectionParams, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.languageCode !== "") {
      writer.uint32(10).string(message.languageCode);
    }
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    if (message.ssmlGender !== 0) {
      writer.uint32(24).int32(message.ssmlGender);
    }
    if (message.customVoice !== undefined) {
      CustomVoiceParams.encode(message.customVoice, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VoiceSelectionParams {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVoiceSelectionParams();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.languageCode = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.ssmlGender = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.customVoice = CustomVoiceParams.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VoiceSelectionParams {
    return {
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      ssmlGender: isSet(object.ssmlGender) ? ssmlVoiceGenderFromJSON(object.ssmlGender) : 0,
      customVoice: isSet(object.customVoice) ? CustomVoiceParams.fromJSON(object.customVoice) : undefined,
    };
  },

  toJSON(message: VoiceSelectionParams): unknown {
    const obj: any = {};
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.ssmlGender !== 0) {
      obj.ssmlGender = ssmlVoiceGenderToJSON(message.ssmlGender);
    }
    if (message.customVoice !== undefined) {
      obj.customVoice = CustomVoiceParams.toJSON(message.customVoice);
    }
    return obj;
  },

  create(base?: DeepPartial<VoiceSelectionParams>): VoiceSelectionParams {
    return VoiceSelectionParams.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VoiceSelectionParams>): VoiceSelectionParams {
    const message = createBaseVoiceSelectionParams();
    message.languageCode = object.languageCode ?? "";
    message.name = object.name ?? "";
    message.ssmlGender = object.ssmlGender ?? 0;
    message.customVoice = (object.customVoice !== undefined && object.customVoice !== null)
      ? CustomVoiceParams.fromPartial(object.customVoice)
      : undefined;
    return message;
  },
};

function createBaseAudioConfig(): AudioConfig {
  return { audioEncoding: 0, speakingRate: 0, pitch: 0, volumeGainDb: 0, sampleRateHertz: 0, effectsProfileId: [] };
}

export const AudioConfig: MessageFns<AudioConfig> = {
  encode(message: AudioConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.audioEncoding !== 0) {
      writer.uint32(8).int32(message.audioEncoding);
    }
    if (message.speakingRate !== 0) {
      writer.uint32(17).double(message.speakingRate);
    }
    if (message.pitch !== 0) {
      writer.uint32(25).double(message.pitch);
    }
    if (message.volumeGainDb !== 0) {
      writer.uint32(33).double(message.volumeGainDb);
    }
    if (message.sampleRateHertz !== 0) {
      writer.uint32(40).int32(message.sampleRateHertz);
    }
    for (const v of message.effectsProfileId) {
      writer.uint32(50).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AudioConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAudioConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.audioEncoding = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.speakingRate = reader.double();
          continue;
        case 3:
          if (tag !== 25) {
            break;
          }

          message.pitch = reader.double();
          continue;
        case 4:
          if (tag !== 33) {
            break;
          }

          message.volumeGainDb = reader.double();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.sampleRateHertz = reader.int32();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.effectsProfileId.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AudioConfig {
    return {
      audioEncoding: isSet(object.audioEncoding) ? audioEncodingFromJSON(object.audioEncoding) : 0,
      speakingRate: isSet(object.speakingRate) ? globalThis.Number(object.speakingRate) : 0,
      pitch: isSet(object.pitch) ? globalThis.Number(object.pitch) : 0,
      volumeGainDb: isSet(object.volumeGainDb) ? globalThis.Number(object.volumeGainDb) : 0,
      sampleRateHertz: isSet(object.sampleRateHertz) ? globalThis.Number(object.sampleRateHertz) : 0,
      effectsProfileId: globalThis.Array.isArray(object?.effectsProfileId)
        ? object.effectsProfileId.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: AudioConfig): unknown {
    const obj: any = {};
    if (message.audioEncoding !== 0) {
      obj.audioEncoding = audioEncodingToJSON(message.audioEncoding);
    }
    if (message.speakingRate !== 0) {
      obj.speakingRate = message.speakingRate;
    }
    if (message.pitch !== 0) {
      obj.pitch = message.pitch;
    }
    if (message.volumeGainDb !== 0) {
      obj.volumeGainDb = message.volumeGainDb;
    }
    if (message.sampleRateHertz !== 0) {
      obj.sampleRateHertz = Math.round(message.sampleRateHertz);
    }
    if (message.effectsProfileId?.length) {
      obj.effectsProfileId = message.effectsProfileId;
    }
    return obj;
  },

  create(base?: DeepPartial<AudioConfig>): AudioConfig {
    return AudioConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AudioConfig>): AudioConfig {
    const message = createBaseAudioConfig();
    message.audioEncoding = object.audioEncoding ?? 0;
    message.speakingRate = object.speakingRate ?? 0;
    message.pitch = object.pitch ?? 0;
    message.volumeGainDb = object.volumeGainDb ?? 0;
    message.sampleRateHertz = object.sampleRateHertz ?? 0;
    message.effectsProfileId = object.effectsProfileId?.map((e) => e) || [];
    return message;
  },
};

function createBaseCustomVoiceParams(): CustomVoiceParams {
  return { model: "", reportedUsage: 0 };
}

export const CustomVoiceParams: MessageFns<CustomVoiceParams> = {
  encode(message: CustomVoiceParams, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.model !== "") {
      writer.uint32(10).string(message.model);
    }
    if (message.reportedUsage !== 0) {
      writer.uint32(24).int32(message.reportedUsage);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CustomVoiceParams {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCustomVoiceParams();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.model = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.reportedUsage = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CustomVoiceParams {
    return {
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      reportedUsage: isSet(object.reportedUsage) ? customVoiceParams_ReportedUsageFromJSON(object.reportedUsage) : 0,
    };
  },

  toJSON(message: CustomVoiceParams): unknown {
    const obj: any = {};
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.reportedUsage !== 0) {
      obj.reportedUsage = customVoiceParams_ReportedUsageToJSON(message.reportedUsage);
    }
    return obj;
  },

  create(base?: DeepPartial<CustomVoiceParams>): CustomVoiceParams {
    return CustomVoiceParams.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CustomVoiceParams>): CustomVoiceParams {
    const message = createBaseCustomVoiceParams();
    message.model = object.model ?? "";
    message.reportedUsage = object.reportedUsage ?? 0;
    return message;
  },
};

function createBaseSynthesizeSpeechResponse(): SynthesizeSpeechResponse {
  return { audioContent: Buffer.alloc(0), timepoints: [], audioConfig: undefined };
}

export const SynthesizeSpeechResponse: MessageFns<SynthesizeSpeechResponse> = {
  encode(message: SynthesizeSpeechResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.audioContent.length !== 0) {
      writer.uint32(10).bytes(message.audioContent);
    }
    for (const v of message.timepoints) {
      Timepoint.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.audioConfig !== undefined) {
      AudioConfig.encode(message.audioConfig, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SynthesizeSpeechResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSynthesizeSpeechResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.audioContent = Buffer.from(reader.bytes());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.timepoints.push(Timepoint.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.audioConfig = AudioConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SynthesizeSpeechResponse {
    return {
      audioContent: isSet(object.audioContent) ? Buffer.from(bytesFromBase64(object.audioContent)) : Buffer.alloc(0),
      timepoints: globalThis.Array.isArray(object?.timepoints)
        ? object.timepoints.map((e: any) => Timepoint.fromJSON(e))
        : [],
      audioConfig: isSet(object.audioConfig) ? AudioConfig.fromJSON(object.audioConfig) : undefined,
    };
  },

  toJSON(message: SynthesizeSpeechResponse): unknown {
    const obj: any = {};
    if (message.audioContent.length !== 0) {
      obj.audioContent = base64FromBytes(message.audioContent);
    }
    if (message.timepoints?.length) {
      obj.timepoints = message.timepoints.map((e) => Timepoint.toJSON(e));
    }
    if (message.audioConfig !== undefined) {
      obj.audioConfig = AudioConfig.toJSON(message.audioConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<SynthesizeSpeechResponse>): SynthesizeSpeechResponse {
    return SynthesizeSpeechResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SynthesizeSpeechResponse>): SynthesizeSpeechResponse {
    const message = createBaseSynthesizeSpeechResponse();
    message.audioContent = object.audioContent ?? Buffer.alloc(0);
    message.timepoints = object.timepoints?.map((e) => Timepoint.fromPartial(e)) || [];
    message.audioConfig = (object.audioConfig !== undefined && object.audioConfig !== null)
      ? AudioConfig.fromPartial(object.audioConfig)
      : undefined;
    return message;
  },
};

function createBaseTimepoint(): Timepoint {
  return { markName: "", timeSeconds: 0 };
}

export const Timepoint: MessageFns<Timepoint> = {
  encode(message: Timepoint, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.markName !== "") {
      writer.uint32(34).string(message.markName);
    }
    if (message.timeSeconds !== 0) {
      writer.uint32(25).double(message.timeSeconds);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Timepoint {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTimepoint();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 4:
          if (tag !== 34) {
            break;
          }

          message.markName = reader.string();
          continue;
        case 3:
          if (tag !== 25) {
            break;
          }

          message.timeSeconds = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Timepoint {
    return {
      markName: isSet(object.markName) ? globalThis.String(object.markName) : "",
      timeSeconds: isSet(object.timeSeconds) ? globalThis.Number(object.timeSeconds) : 0,
    };
  },

  toJSON(message: Timepoint): unknown {
    const obj: any = {};
    if (message.markName !== "") {
      obj.markName = message.markName;
    }
    if (message.timeSeconds !== 0) {
      obj.timeSeconds = message.timeSeconds;
    }
    return obj;
  },

  create(base?: DeepPartial<Timepoint>): Timepoint {
    return Timepoint.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Timepoint>): Timepoint {
    const message = createBaseTimepoint();
    message.markName = object.markName ?? "";
    message.timeSeconds = object.timeSeconds ?? 0;
    return message;
  },
};

function createBaseStreamingSynthesizeConfig(): StreamingSynthesizeConfig {
  return { voice: undefined };
}

export const StreamingSynthesizeConfig: MessageFns<StreamingSynthesizeConfig> = {
  encode(message: StreamingSynthesizeConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.voice !== undefined) {
      VoiceSelectionParams.encode(message.voice, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingSynthesizeConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingSynthesizeConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.voice = VoiceSelectionParams.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingSynthesizeConfig {
    return { voice: isSet(object.voice) ? VoiceSelectionParams.fromJSON(object.voice) : undefined };
  },

  toJSON(message: StreamingSynthesizeConfig): unknown {
    const obj: any = {};
    if (message.voice !== undefined) {
      obj.voice = VoiceSelectionParams.toJSON(message.voice);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamingSynthesizeConfig>): StreamingSynthesizeConfig {
    return StreamingSynthesizeConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamingSynthesizeConfig>): StreamingSynthesizeConfig {
    const message = createBaseStreamingSynthesizeConfig();
    message.voice = (object.voice !== undefined && object.voice !== null)
      ? VoiceSelectionParams.fromPartial(object.voice)
      : undefined;
    return message;
  },
};

function createBaseStreamingSynthesisInput(): StreamingSynthesisInput {
  return { text: undefined };
}

export const StreamingSynthesisInput: MessageFns<StreamingSynthesisInput> = {
  encode(message: StreamingSynthesisInput, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.text !== undefined) {
      writer.uint32(10).string(message.text);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingSynthesisInput {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingSynthesisInput();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.text = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingSynthesisInput {
    return { text: isSet(object.text) ? globalThis.String(object.text) : undefined };
  },

  toJSON(message: StreamingSynthesisInput): unknown {
    const obj: any = {};
    if (message.text !== undefined) {
      obj.text = message.text;
    }
    return obj;
  },

  create(base?: DeepPartial<StreamingSynthesisInput>): StreamingSynthesisInput {
    return StreamingSynthesisInput.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamingSynthesisInput>): StreamingSynthesisInput {
    const message = createBaseStreamingSynthesisInput();
    message.text = object.text ?? undefined;
    return message;
  },
};

function createBaseStreamingSynthesizeRequest(): StreamingSynthesizeRequest {
  return { streamingConfig: undefined, input: undefined };
}

export const StreamingSynthesizeRequest: MessageFns<StreamingSynthesizeRequest> = {
  encode(message: StreamingSynthesizeRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.streamingConfig !== undefined) {
      StreamingSynthesizeConfig.encode(message.streamingConfig, writer.uint32(10).fork()).join();
    }
    if (message.input !== undefined) {
      StreamingSynthesisInput.encode(message.input, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingSynthesizeRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingSynthesizeRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.streamingConfig = StreamingSynthesizeConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.input = StreamingSynthesisInput.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingSynthesizeRequest {
    return {
      streamingConfig: isSet(object.streamingConfig)
        ? StreamingSynthesizeConfig.fromJSON(object.streamingConfig)
        : undefined,
      input: isSet(object.input) ? StreamingSynthesisInput.fromJSON(object.input) : undefined,
    };
  },

  toJSON(message: StreamingSynthesizeRequest): unknown {
    const obj: any = {};
    if (message.streamingConfig !== undefined) {
      obj.streamingConfig = StreamingSynthesizeConfig.toJSON(message.streamingConfig);
    }
    if (message.input !== undefined) {
      obj.input = StreamingSynthesisInput.toJSON(message.input);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamingSynthesizeRequest>): StreamingSynthesizeRequest {
    return StreamingSynthesizeRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamingSynthesizeRequest>): StreamingSynthesizeRequest {
    const message = createBaseStreamingSynthesizeRequest();
    message.streamingConfig = (object.streamingConfig !== undefined && object.streamingConfig !== null)
      ? StreamingSynthesizeConfig.fromPartial(object.streamingConfig)
      : undefined;
    message.input = (object.input !== undefined && object.input !== null)
      ? StreamingSynthesisInput.fromPartial(object.input)
      : undefined;
    return message;
  },
};

function createBaseStreamingSynthesizeResponse(): StreamingSynthesizeResponse {
  return { audioContent: Buffer.alloc(0) };
}

export const StreamingSynthesizeResponse: MessageFns<StreamingSynthesizeResponse> = {
  encode(message: StreamingSynthesizeResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.audioContent.length !== 0) {
      writer.uint32(10).bytes(message.audioContent);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingSynthesizeResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingSynthesizeResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.audioContent = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingSynthesizeResponse {
    return {
      audioContent: isSet(object.audioContent) ? Buffer.from(bytesFromBase64(object.audioContent)) : Buffer.alloc(0),
    };
  },

  toJSON(message: StreamingSynthesizeResponse): unknown {
    const obj: any = {};
    if (message.audioContent.length !== 0) {
      obj.audioContent = base64FromBytes(message.audioContent);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamingSynthesizeResponse>): StreamingSynthesizeResponse {
    return StreamingSynthesizeResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamingSynthesizeResponse>): StreamingSynthesizeResponse {
    const message = createBaseStreamingSynthesizeResponse();
    message.audioContent = object.audioContent ?? Buffer.alloc(0);
    return message;
  },
};

/** Service that implements Google Cloud Text-to-Speech API. */
export type TextToSpeechDefinition = typeof TextToSpeechDefinition;
export const TextToSpeechDefinition = {
  name: "TextToSpeech",
  fullName: "google.cloud.texttospeech.v1beta1.TextToSpeech",
  methods: {
    /** Returns a list of Voice supported for synthesis. */
    listVoices: {
      name: "ListVoices",
      requestType: ListVoicesRequest,
      requestStream: false,
      responseType: ListVoicesResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([13, 108, 97, 110, 103, 117, 97, 103, 101, 95, 99, 111, 100, 101])],
          578365826: [Buffer.from([17, 18, 15, 47, 118, 49, 98, 101, 116, 97, 49, 47, 118, 111, 105, 99, 101, 115])],
        },
      },
    },
    /**
     * Synthesizes speech synchronously: receive results after all text input
     * has been processed.
     */
    synthesizeSpeech: {
      name: "SynthesizeSpeech",
      requestType: SynthesizeSpeechRequest,
      requestStream: false,
      responseType: SynthesizeSpeechResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              24,
              105,
              110,
              112,
              117,
              116,
              44,
              118,
              111,
              105,
              99,
              101,
              44,
              97,
              117,
              100,
              105,
              111,
              95,
              99,
              111,
              110,
              102,
              105,
              103,
            ]),
          ],
          578365826: [
            Buffer.from([
              29,
              58,
              1,
              42,
              34,
              24,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              116,
              101,
              120,
              116,
              58,
              115,
              121,
              110,
              116,
              104,
              101,
              115,
              105,
              122,
              101,
            ]),
          ],
        },
      },
    },
    /**
     * Performs bidirectional streaming speech synthesis: receive audio while
     * sending text.
     */
    streamingSynthesize: {
      name: "StreamingSynthesize",
      requestType: StreamingSynthesizeRequest,
      requestStream: true,
      responseType: StreamingSynthesizeResponse,
      responseStream: true,
      options: {},
    },
  },
} as const;

export interface TextToSpeechServiceImplementation<CallContextExt = {}> {
  /** Returns a list of Voice supported for synthesis. */
  listVoices(
    request: ListVoicesRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ListVoicesResponse>>;
  /**
   * Synthesizes speech synchronously: receive results after all text input
   * has been processed.
   */
  synthesizeSpeech(
    request: SynthesizeSpeechRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<SynthesizeSpeechResponse>>;
  /**
   * Performs bidirectional streaming speech synthesis: receive audio while
   * sending text.
   */
  streamingSynthesize(
    request: AsyncIterable<StreamingSynthesizeRequest>,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<StreamingSynthesizeResponse>>;
}

export interface TextToSpeechClient<CallOptionsExt = {}> {
  /** Returns a list of Voice supported for synthesis. */
  listVoices(
    request: DeepPartial<ListVoicesRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ListVoicesResponse>;
  /**
   * Synthesizes speech synchronously: receive results after all text input
   * has been processed.
   */
  synthesizeSpeech(
    request: DeepPartial<SynthesizeSpeechRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<SynthesizeSpeechResponse>;
  /**
   * Performs bidirectional streaming speech synthesis: receive audio while
   * sending text.
   */
  streamingSynthesize(
    request: AsyncIterable<DeepPartial<StreamingSynthesizeRequest>>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<StreamingSynthesizeResponse>;
}

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export type ServerStreamingMethodResult<Response> = { [Symbol.asyncIterator](): AsyncIterator<Response, void> };

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
