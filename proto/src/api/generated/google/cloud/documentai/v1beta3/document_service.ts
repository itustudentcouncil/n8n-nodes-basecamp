// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/documentai/v1beta3/document_service.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Operation } from "../../../longrunning/operations.js";
import { FieldMask } from "../../../protobuf/field_mask.js";
import { Status } from "../../../rpc/status.js";
import { BatchDatasetDocuments, Dataset, DatasetSchema, DocumentId } from "./dataset.js";
import { Document } from "./document.js";
import { BatchDocumentsInputConfig } from "./document_io.js";
import { CommonOperationMetadata } from "./operation_metadata.js";

export const protobufPackage = "google.cloud.documentai.v1beta3";

/**
 * Documents belonging to a dataset will be split into different groups
 * referred to as splits: train, test.
 */
export enum DatasetSplitType {
  /** DATASET_SPLIT_TYPE_UNSPECIFIED - Default value if the enum is not set. */
  DATASET_SPLIT_TYPE_UNSPECIFIED = 0,
  /** DATASET_SPLIT_TRAIN - Identifies the train documents. */
  DATASET_SPLIT_TRAIN = 1,
  /** DATASET_SPLIT_TEST - Identifies the test documents. */
  DATASET_SPLIT_TEST = 2,
  /** DATASET_SPLIT_UNASSIGNED - Identifies the unassigned documents. */
  DATASET_SPLIT_UNASSIGNED = 3,
  UNRECOGNIZED = -1,
}

export function datasetSplitTypeFromJSON(object: any): DatasetSplitType {
  switch (object) {
    case 0:
    case "DATASET_SPLIT_TYPE_UNSPECIFIED":
      return DatasetSplitType.DATASET_SPLIT_TYPE_UNSPECIFIED;
    case 1:
    case "DATASET_SPLIT_TRAIN":
      return DatasetSplitType.DATASET_SPLIT_TRAIN;
    case 2:
    case "DATASET_SPLIT_TEST":
      return DatasetSplitType.DATASET_SPLIT_TEST;
    case 3:
    case "DATASET_SPLIT_UNASSIGNED":
      return DatasetSplitType.DATASET_SPLIT_UNASSIGNED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DatasetSplitType.UNRECOGNIZED;
  }
}

export function datasetSplitTypeToJSON(object: DatasetSplitType): string {
  switch (object) {
    case DatasetSplitType.DATASET_SPLIT_TYPE_UNSPECIFIED:
      return "DATASET_SPLIT_TYPE_UNSPECIFIED";
    case DatasetSplitType.DATASET_SPLIT_TRAIN:
      return "DATASET_SPLIT_TRAIN";
    case DatasetSplitType.DATASET_SPLIT_TEST:
      return "DATASET_SPLIT_TEST";
    case DatasetSplitType.DATASET_SPLIT_UNASSIGNED:
      return "DATASET_SPLIT_UNASSIGNED";
    case DatasetSplitType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Describes the labeling status of a document. */
export enum DocumentLabelingState {
  /** DOCUMENT_LABELING_STATE_UNSPECIFIED - Default value if the enum is not set. */
  DOCUMENT_LABELING_STATE_UNSPECIFIED = 0,
  /** DOCUMENT_LABELED - Document has been labeled. */
  DOCUMENT_LABELED = 1,
  /** DOCUMENT_UNLABELED - Document has not been labeled. */
  DOCUMENT_UNLABELED = 2,
  /** DOCUMENT_AUTO_LABELED - Document has been auto-labeled. */
  DOCUMENT_AUTO_LABELED = 3,
  UNRECOGNIZED = -1,
}

export function documentLabelingStateFromJSON(object: any): DocumentLabelingState {
  switch (object) {
    case 0:
    case "DOCUMENT_LABELING_STATE_UNSPECIFIED":
      return DocumentLabelingState.DOCUMENT_LABELING_STATE_UNSPECIFIED;
    case 1:
    case "DOCUMENT_LABELED":
      return DocumentLabelingState.DOCUMENT_LABELED;
    case 2:
    case "DOCUMENT_UNLABELED":
      return DocumentLabelingState.DOCUMENT_UNLABELED;
    case 3:
    case "DOCUMENT_AUTO_LABELED":
      return DocumentLabelingState.DOCUMENT_AUTO_LABELED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DocumentLabelingState.UNRECOGNIZED;
  }
}

export function documentLabelingStateToJSON(object: DocumentLabelingState): string {
  switch (object) {
    case DocumentLabelingState.DOCUMENT_LABELING_STATE_UNSPECIFIED:
      return "DOCUMENT_LABELING_STATE_UNSPECIFIED";
    case DocumentLabelingState.DOCUMENT_LABELED:
      return "DOCUMENT_LABELED";
    case DocumentLabelingState.DOCUMENT_UNLABELED:
      return "DOCUMENT_UNLABELED";
    case DocumentLabelingState.DOCUMENT_AUTO_LABELED:
      return "DOCUMENT_AUTO_LABELED";
    case DocumentLabelingState.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export interface UpdateDatasetRequest {
  /**
   * Required. The `name` field of the `Dataset` is used to identify the
   * resource to be updated.
   */
  dataset:
    | Dataset
    | undefined;
  /** The update mask applies to the resource. */
  updateMask: string[] | undefined;
}

export interface UpdateDatasetOperationMetadata {
  /** The basic metadata of the long-running operation. */
  commonMetadata: CommonOperationMetadata | undefined;
}

export interface ImportDocumentsRequest {
  /**
   * Required. The dataset resource name.
   * Format:
   * projects/{project}/locations/{location}/processors/{processor}/dataset
   */
  dataset: string;
  /**
   * Required. The Cloud Storage uri containing raw documents that must be
   * imported.
   */
  batchDocumentsImportConfigs: ImportDocumentsRequest_BatchDocumentsImportConfig[];
}

/**
 * Config for importing documents.
 * Each batch can have its own dataset split type.
 */
export interface ImportDocumentsRequest_BatchDocumentsImportConfig {
  /** Target dataset split where the documents must be stored. */
  datasetSplit?:
    | DatasetSplitType
    | undefined;
  /**
   * If set, documents will be automatically split into training and test
   * split category with the specified ratio.
   */
  autoSplitConfig?:
    | ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig
    | undefined;
  /** The common config to specify a set of documents used as input. */
  batchInputConfig: BatchDocumentsInputConfig | undefined;
}

/** The config for auto-split. */
export interface ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig {
  /** Ratio of training dataset split. */
  trainingSplitRatio: number;
}

/** Response of the import document operation. */
export interface ImportDocumentsResponse {
}

/** Metadata of the import document operation. */
export interface ImportDocumentsMetadata {
  /** The basic metadata of the long-running operation. */
  commonMetadata:
    | CommonOperationMetadata
    | undefined;
  /** The list of response details of each document. */
  individualImportStatuses: ImportDocumentsMetadata_IndividualImportStatus[];
  /** Validation statuses of the batch documents import config. */
  importConfigValidationResults: ImportDocumentsMetadata_ImportConfigValidationResult[];
  /** Total number of the documents that are qualified for importing. */
  totalDocumentCount: number;
}

/** The status of each individual document in the import process. */
export interface ImportDocumentsMetadata_IndividualImportStatus {
  /** The source Cloud Storage URI of the document. */
  inputGcsSource: string;
  /** The status of the importing of the document. */
  status:
    | Status
    | undefined;
  /**
   * The document id of imported document if it was successful, otherwise
   * empty.
   */
  outputDocumentId: DocumentId | undefined;
}

/**
 * The validation status of each import config. Status is set to an error if
 * there are no documents to import in the `import_config`, or `OK` if the
 * operation will try to proceed with at least one document.
 */
export interface ImportDocumentsMetadata_ImportConfigValidationResult {
  /** The source Cloud Storage URI specified in the import config. */
  inputGcsSource: string;
  /** The validation status of import config. */
  status: Status | undefined;
}

export interface GetDocumentRequest {
  /**
   * Required. The resource name of the dataset that the document belongs to .
   * Format:
   * projects/{project}/locations/{location}/processors/{processor}/dataset
   */
  dataset: string;
  /** Required. Document identifier. */
  documentId:
    | DocumentId
    | undefined;
  /**
   * If set, only fields listed here will be returned. Otherwise, all fields
   * will be returned by default.
   */
  readMask:
    | string[]
    | undefined;
  /**
   * List of pages for which the fields specified in the `read_mask` must
   * be served.
   */
  pageRange: DocumentPageRange | undefined;
}

export interface GetDocumentResponse {
  document: Document | undefined;
}

export interface ListDocumentsRequest {
  /**
   * Required. The resource name of the dataset to be listed.
   * Format:
   * projects/{project}/locations/{location}/processors/{processor}/dataset
   */
  dataset: string;
  /**
   * The maximum number of documents to return. The service may return
   * fewer than this value.
   * If unspecified, at most 20 documents will be returned.
   * The maximum value is 100; values above 100 will be coerced to 100.
   */
  pageSize: number;
  /**
   * A page token, received from a previous `ListDocuments` call.
   * Provide this to retrieve the subsequent page.
   *
   * When paginating, all other parameters provided to `ListDocuments`
   * must match the call that provided the page token.
   */
  pageToken: string;
  /**
   * Optional. Query to filter the documents based on
   * https://google.aip.dev/160.
   * ## Currently support query strings are:
   *
   * `SplitType=DATASET_SPLIT_TEST|DATASET_SPLIT_TRAIN|DATASET_SPLIT_UNASSIGNED`
   * - `LabelingState=DOCUMENT_LABELED|DOCUMENT_UNLABELED|DOCUMENT_AUTO_LABELED`
   * - `DisplayName=\"file_name.pdf\"`
   * - `EntityType=abc/def`
   * - `TagName=\"auto-labeling-running\"|\"sampled\"`
   *
   * Note:
   * - Only `AND`, `=` and `!=` are supported.
   *     e.g. `DisplayName=file_name AND EntityType!=abc` IS supported.
   * - Wildcard `*` is supported only in `DisplayName` filter
   * - No duplicate filter keys are allowed,
   *     e.g. `EntityType=a AND EntityType=b` is NOT supported.
   * - String match is case sensitive (for filter `DisplayName` & `EntityType`).
   */
  filter: string;
  /**
   * Optional. Controls if the request requires a total size of matched
   * documents. See
   * [ListDocumentsResponse.total_size][google.cloud.documentai.v1beta3.ListDocumentsResponse.total_size].
   *
   * Enabling this flag may adversely impact performance.
   *
   * Defaults to false.
   */
  returnTotalSize: boolean;
  /**
   * Optional. Number of results to skip beginning from the `page_token` if
   * provided. https://google.aip.dev/158#skipping-results. It must be a
   * non-negative integer. Negative values will be rejected. Note that this is
   * not the number of pages to skip. If this value causes the cursor to move
   * past the end of results,
   * [ListDocumentsResponse.document_metadata][google.cloud.documentai.v1beta3.ListDocumentsResponse.document_metadata]
   * and
   * [ListDocumentsResponse.next_page_token][google.cloud.documentai.v1beta3.ListDocumentsResponse.next_page_token]
   * will be empty.
   */
  skip: number;
}

export interface ListDocumentsResponse {
  /** Document metadata corresponding to the listed documents. */
  documentMetadata: DocumentMetadata[];
  /**
   * A token, which can be sent as
   * [ListDocumentsRequest.page_token][google.cloud.documentai.v1beta3.ListDocumentsRequest.page_token]
   * to retrieve the next page. If this field is omitted, there are no
   * subsequent pages.
   */
  nextPageToken: string;
  /** Total count of documents queried. */
  totalSize: number;
}

export interface BatchDeleteDocumentsRequest {
  /**
   * Required. The dataset resource name.
   * Format:
   * projects/{project}/locations/{location}/processors/{processor}/dataset
   */
  dataset: string;
  /**
   * Required. Dataset documents input. If given `filter`, all documents
   * satisfying the filter will be deleted. If given documentIds, a maximum of
   * 50 documents can be deleted in a batch. The request will be rejected if
   * more than 50 document_ids are provided.
   */
  datasetDocuments: BatchDatasetDocuments | undefined;
}

/** Response of the delete documents operation. */
export interface BatchDeleteDocumentsResponse {
}

export interface BatchDeleteDocumentsMetadata {
  /** The basic metadata of the long-running operation. */
  commonMetadata:
    | CommonOperationMetadata
    | undefined;
  /** The list of response details of each document. */
  individualBatchDeleteStatuses: BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus[];
  /** Total number of documents deleting from dataset. */
  totalDocumentCount: number;
  /** Total number of documents that failed to be deleted in storage. */
  errorDocumentCount: number;
}

/** The status of each individual document in the batch delete process. */
export interface BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus {
  /** The document id of the document. */
  documentId:
    | DocumentId
    | undefined;
  /** The status of deleting the document in storage. */
  status: Status | undefined;
}

/** Request for `GetDatasetSchema`. */
export interface GetDatasetSchemaRequest {
  /**
   * Required. The dataset schema resource name.
   * Format:
   * projects/{project}/locations/{location}/processors/{processor}/dataset/datasetSchema
   */
  name: string;
  /** If set, only returns the visible fields of the schema. */
  visibleFieldsOnly: boolean;
}

/** Request for `UpdateDatasetSchema`. */
export interface UpdateDatasetSchemaRequest {
  /**
   * Required. The name field of the `DatasetSchema` is used to identify the
   * resource to be updated.
   */
  datasetSchema:
    | DatasetSchema
    | undefined;
  /** The update mask applies to the resource. */
  updateMask: string[] | undefined;
}

/** Range of pages present in a document. */
export interface DocumentPageRange {
  /** First page number (one-based index) to be returned. */
  start: number;
  /** Last page number (one-based index) to be returned. */
  end: number;
}

/** Metadata about a document. */
export interface DocumentMetadata {
  /** Document identifier. */
  documentId:
    | DocumentId
    | undefined;
  /** Number of pages in the document. */
  pageCount: number;
  /** Type of the dataset split to which the document belongs. */
  datasetType: DatasetSplitType;
  /** Labeling state of the document. */
  labelingState: DocumentLabelingState;
  /** The display name of the document. */
  displayName: string;
}

function createBaseUpdateDatasetRequest(): UpdateDatasetRequest {
  return { dataset: undefined, updateMask: undefined };
}

export const UpdateDatasetRequest: MessageFns<UpdateDatasetRequest> = {
  encode(message: UpdateDatasetRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dataset !== undefined) {
      Dataset.encode(message.dataset, writer.uint32(10).fork()).join();
    }
    if (message.updateMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.updateMask), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateDatasetRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateDatasetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dataset = Dataset.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.updateMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateDatasetRequest {
    return {
      dataset: isSet(object.dataset) ? Dataset.fromJSON(object.dataset) : undefined,
      updateMask: isSet(object.updateMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.updateMask)) : undefined,
    };
  },

  toJSON(message: UpdateDatasetRequest): unknown {
    const obj: any = {};
    if (message.dataset !== undefined) {
      obj.dataset = Dataset.toJSON(message.dataset);
    }
    if (message.updateMask !== undefined) {
      obj.updateMask = FieldMask.toJSON(FieldMask.wrap(message.updateMask));
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateDatasetRequest>): UpdateDatasetRequest {
    return UpdateDatasetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateDatasetRequest>): UpdateDatasetRequest {
    const message = createBaseUpdateDatasetRequest();
    message.dataset = (object.dataset !== undefined && object.dataset !== null)
      ? Dataset.fromPartial(object.dataset)
      : undefined;
    message.updateMask = object.updateMask ?? undefined;
    return message;
  },
};

function createBaseUpdateDatasetOperationMetadata(): UpdateDatasetOperationMetadata {
  return { commonMetadata: undefined };
}

export const UpdateDatasetOperationMetadata: MessageFns<UpdateDatasetOperationMetadata> = {
  encode(message: UpdateDatasetOperationMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.commonMetadata !== undefined) {
      CommonOperationMetadata.encode(message.commonMetadata, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateDatasetOperationMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateDatasetOperationMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.commonMetadata = CommonOperationMetadata.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateDatasetOperationMetadata {
    return {
      commonMetadata: isSet(object.commonMetadata)
        ? CommonOperationMetadata.fromJSON(object.commonMetadata)
        : undefined,
    };
  },

  toJSON(message: UpdateDatasetOperationMetadata): unknown {
    const obj: any = {};
    if (message.commonMetadata !== undefined) {
      obj.commonMetadata = CommonOperationMetadata.toJSON(message.commonMetadata);
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateDatasetOperationMetadata>): UpdateDatasetOperationMetadata {
    return UpdateDatasetOperationMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateDatasetOperationMetadata>): UpdateDatasetOperationMetadata {
    const message = createBaseUpdateDatasetOperationMetadata();
    message.commonMetadata = (object.commonMetadata !== undefined && object.commonMetadata !== null)
      ? CommonOperationMetadata.fromPartial(object.commonMetadata)
      : undefined;
    return message;
  },
};

function createBaseImportDocumentsRequest(): ImportDocumentsRequest {
  return { dataset: "", batchDocumentsImportConfigs: [] };
}

export const ImportDocumentsRequest: MessageFns<ImportDocumentsRequest> = {
  encode(message: ImportDocumentsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dataset !== "") {
      writer.uint32(10).string(message.dataset);
    }
    for (const v of message.batchDocumentsImportConfigs) {
      ImportDocumentsRequest_BatchDocumentsImportConfig.encode(v!, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportDocumentsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportDocumentsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dataset = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.batchDocumentsImportConfigs.push(
            ImportDocumentsRequest_BatchDocumentsImportConfig.decode(reader, reader.uint32()),
          );
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportDocumentsRequest {
    return {
      dataset: isSet(object.dataset) ? globalThis.String(object.dataset) : "",
      batchDocumentsImportConfigs: globalThis.Array.isArray(object?.batchDocumentsImportConfigs)
        ? object.batchDocumentsImportConfigs.map((e: any) =>
          ImportDocumentsRequest_BatchDocumentsImportConfig.fromJSON(e)
        )
        : [],
    };
  },

  toJSON(message: ImportDocumentsRequest): unknown {
    const obj: any = {};
    if (message.dataset !== "") {
      obj.dataset = message.dataset;
    }
    if (message.batchDocumentsImportConfigs?.length) {
      obj.batchDocumentsImportConfigs = message.batchDocumentsImportConfigs.map((e) =>
        ImportDocumentsRequest_BatchDocumentsImportConfig.toJSON(e)
      );
    }
    return obj;
  },

  create(base?: DeepPartial<ImportDocumentsRequest>): ImportDocumentsRequest {
    return ImportDocumentsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportDocumentsRequest>): ImportDocumentsRequest {
    const message = createBaseImportDocumentsRequest();
    message.dataset = object.dataset ?? "";
    message.batchDocumentsImportConfigs =
      object.batchDocumentsImportConfigs?.map((e) =>
        ImportDocumentsRequest_BatchDocumentsImportConfig.fromPartial(e)
      ) || [];
    return message;
  },
};

function createBaseImportDocumentsRequest_BatchDocumentsImportConfig(): ImportDocumentsRequest_BatchDocumentsImportConfig {
  return { datasetSplit: undefined, autoSplitConfig: undefined, batchInputConfig: undefined };
}

export const ImportDocumentsRequest_BatchDocumentsImportConfig: MessageFns<
  ImportDocumentsRequest_BatchDocumentsImportConfig
> = {
  encode(
    message: ImportDocumentsRequest_BatchDocumentsImportConfig,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.datasetSplit !== undefined) {
      writer.uint32(16).int32(message.datasetSplit);
    }
    if (message.autoSplitConfig !== undefined) {
      ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig.encode(
        message.autoSplitConfig,
        writer.uint32(26).fork(),
      ).join();
    }
    if (message.batchInputConfig !== undefined) {
      BatchDocumentsInputConfig.encode(message.batchInputConfig, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportDocumentsRequest_BatchDocumentsImportConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportDocumentsRequest_BatchDocumentsImportConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 16) {
            break;
          }

          message.datasetSplit = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.autoSplitConfig = ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig.decode(
            reader,
            reader.uint32(),
          );
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.batchInputConfig = BatchDocumentsInputConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportDocumentsRequest_BatchDocumentsImportConfig {
    return {
      datasetSplit: isSet(object.datasetSplit) ? datasetSplitTypeFromJSON(object.datasetSplit) : undefined,
      autoSplitConfig: isSet(object.autoSplitConfig)
        ? ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig.fromJSON(object.autoSplitConfig)
        : undefined,
      batchInputConfig: isSet(object.batchInputConfig)
        ? BatchDocumentsInputConfig.fromJSON(object.batchInputConfig)
        : undefined,
    };
  },

  toJSON(message: ImportDocumentsRequest_BatchDocumentsImportConfig): unknown {
    const obj: any = {};
    if (message.datasetSplit !== undefined) {
      obj.datasetSplit = datasetSplitTypeToJSON(message.datasetSplit);
    }
    if (message.autoSplitConfig !== undefined) {
      obj.autoSplitConfig = ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig.toJSON(
        message.autoSplitConfig,
      );
    }
    if (message.batchInputConfig !== undefined) {
      obj.batchInputConfig = BatchDocumentsInputConfig.toJSON(message.batchInputConfig);
    }
    return obj;
  },

  create(
    base?: DeepPartial<ImportDocumentsRequest_BatchDocumentsImportConfig>,
  ): ImportDocumentsRequest_BatchDocumentsImportConfig {
    return ImportDocumentsRequest_BatchDocumentsImportConfig.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ImportDocumentsRequest_BatchDocumentsImportConfig>,
  ): ImportDocumentsRequest_BatchDocumentsImportConfig {
    const message = createBaseImportDocumentsRequest_BatchDocumentsImportConfig();
    message.datasetSplit = object.datasetSplit ?? undefined;
    message.autoSplitConfig = (object.autoSplitConfig !== undefined && object.autoSplitConfig !== null)
      ? ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig.fromPartial(object.autoSplitConfig)
      : undefined;
    message.batchInputConfig = (object.batchInputConfig !== undefined && object.batchInputConfig !== null)
      ? BatchDocumentsInputConfig.fromPartial(object.batchInputConfig)
      : undefined;
    return message;
  },
};

function createBaseImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig(): ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig {
  return { trainingSplitRatio: 0 };
}

export const ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig: MessageFns<
  ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig
> = {
  encode(
    message: ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.trainingSplitRatio !== 0) {
      writer.uint32(13).float(message.trainingSplitRatio);
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 13) {
            break;
          }

          message.trainingSplitRatio = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig {
    return { trainingSplitRatio: isSet(object.trainingSplitRatio) ? globalThis.Number(object.trainingSplitRatio) : 0 };
  },

  toJSON(message: ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig): unknown {
    const obj: any = {};
    if (message.trainingSplitRatio !== 0) {
      obj.trainingSplitRatio = message.trainingSplitRatio;
    }
    return obj;
  },

  create(
    base?: DeepPartial<ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig>,
  ): ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig {
    return ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig>,
  ): ImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig {
    const message = createBaseImportDocumentsRequest_BatchDocumentsImportConfig_AutoSplitConfig();
    message.trainingSplitRatio = object.trainingSplitRatio ?? 0;
    return message;
  },
};

function createBaseImportDocumentsResponse(): ImportDocumentsResponse {
  return {};
}

export const ImportDocumentsResponse: MessageFns<ImportDocumentsResponse> = {
  encode(_: ImportDocumentsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportDocumentsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportDocumentsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): ImportDocumentsResponse {
    return {};
  },

  toJSON(_: ImportDocumentsResponse): unknown {
    const obj: any = {};
    return obj;
  },

  create(base?: DeepPartial<ImportDocumentsResponse>): ImportDocumentsResponse {
    return ImportDocumentsResponse.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<ImportDocumentsResponse>): ImportDocumentsResponse {
    const message = createBaseImportDocumentsResponse();
    return message;
  },
};

function createBaseImportDocumentsMetadata(): ImportDocumentsMetadata {
  return {
    commonMetadata: undefined,
    individualImportStatuses: [],
    importConfigValidationResults: [],
    totalDocumentCount: 0,
  };
}

export const ImportDocumentsMetadata: MessageFns<ImportDocumentsMetadata> = {
  encode(message: ImportDocumentsMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.commonMetadata !== undefined) {
      CommonOperationMetadata.encode(message.commonMetadata, writer.uint32(10).fork()).join();
    }
    for (const v of message.individualImportStatuses) {
      ImportDocumentsMetadata_IndividualImportStatus.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.importConfigValidationResults) {
      ImportDocumentsMetadata_ImportConfigValidationResult.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.totalDocumentCount !== 0) {
      writer.uint32(24).int32(message.totalDocumentCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportDocumentsMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportDocumentsMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.commonMetadata = CommonOperationMetadata.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.individualImportStatuses.push(
            ImportDocumentsMetadata_IndividualImportStatus.decode(reader, reader.uint32()),
          );
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.importConfigValidationResults.push(
            ImportDocumentsMetadata_ImportConfigValidationResult.decode(reader, reader.uint32()),
          );
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.totalDocumentCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportDocumentsMetadata {
    return {
      commonMetadata: isSet(object.commonMetadata)
        ? CommonOperationMetadata.fromJSON(object.commonMetadata)
        : undefined,
      individualImportStatuses: globalThis.Array.isArray(object?.individualImportStatuses)
        ? object.individualImportStatuses.map((e: any) => ImportDocumentsMetadata_IndividualImportStatus.fromJSON(e))
        : [],
      importConfigValidationResults: globalThis.Array.isArray(object?.importConfigValidationResults)
        ? object.importConfigValidationResults.map((e: any) =>
          ImportDocumentsMetadata_ImportConfigValidationResult.fromJSON(e)
        )
        : [],
      totalDocumentCount: isSet(object.totalDocumentCount) ? globalThis.Number(object.totalDocumentCount) : 0,
    };
  },

  toJSON(message: ImportDocumentsMetadata): unknown {
    const obj: any = {};
    if (message.commonMetadata !== undefined) {
      obj.commonMetadata = CommonOperationMetadata.toJSON(message.commonMetadata);
    }
    if (message.individualImportStatuses?.length) {
      obj.individualImportStatuses = message.individualImportStatuses.map((e) =>
        ImportDocumentsMetadata_IndividualImportStatus.toJSON(e)
      );
    }
    if (message.importConfigValidationResults?.length) {
      obj.importConfigValidationResults = message.importConfigValidationResults.map((e) =>
        ImportDocumentsMetadata_ImportConfigValidationResult.toJSON(e)
      );
    }
    if (message.totalDocumentCount !== 0) {
      obj.totalDocumentCount = Math.round(message.totalDocumentCount);
    }
    return obj;
  },

  create(base?: DeepPartial<ImportDocumentsMetadata>): ImportDocumentsMetadata {
    return ImportDocumentsMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportDocumentsMetadata>): ImportDocumentsMetadata {
    const message = createBaseImportDocumentsMetadata();
    message.commonMetadata = (object.commonMetadata !== undefined && object.commonMetadata !== null)
      ? CommonOperationMetadata.fromPartial(object.commonMetadata)
      : undefined;
    message.individualImportStatuses =
      object.individualImportStatuses?.map((e) => ImportDocumentsMetadata_IndividualImportStatus.fromPartial(e)) || [];
    message.importConfigValidationResults =
      object.importConfigValidationResults?.map((e) =>
        ImportDocumentsMetadata_ImportConfigValidationResult.fromPartial(e)
      ) || [];
    message.totalDocumentCount = object.totalDocumentCount ?? 0;
    return message;
  },
};

function createBaseImportDocumentsMetadata_IndividualImportStatus(): ImportDocumentsMetadata_IndividualImportStatus {
  return { inputGcsSource: "", status: undefined, outputDocumentId: undefined };
}

export const ImportDocumentsMetadata_IndividualImportStatus: MessageFns<
  ImportDocumentsMetadata_IndividualImportStatus
> = {
  encode(
    message: ImportDocumentsMetadata_IndividualImportStatus,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.inputGcsSource !== "") {
      writer.uint32(10).string(message.inputGcsSource);
    }
    if (message.status !== undefined) {
      Status.encode(message.status, writer.uint32(18).fork()).join();
    }
    if (message.outputDocumentId !== undefined) {
      DocumentId.encode(message.outputDocumentId, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportDocumentsMetadata_IndividualImportStatus {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportDocumentsMetadata_IndividualImportStatus();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputGcsSource = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.status = Status.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.outputDocumentId = DocumentId.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportDocumentsMetadata_IndividualImportStatus {
    return {
      inputGcsSource: isSet(object.inputGcsSource) ? globalThis.String(object.inputGcsSource) : "",
      status: isSet(object.status) ? Status.fromJSON(object.status) : undefined,
      outputDocumentId: isSet(object.outputDocumentId) ? DocumentId.fromJSON(object.outputDocumentId) : undefined,
    };
  },

  toJSON(message: ImportDocumentsMetadata_IndividualImportStatus): unknown {
    const obj: any = {};
    if (message.inputGcsSource !== "") {
      obj.inputGcsSource = message.inputGcsSource;
    }
    if (message.status !== undefined) {
      obj.status = Status.toJSON(message.status);
    }
    if (message.outputDocumentId !== undefined) {
      obj.outputDocumentId = DocumentId.toJSON(message.outputDocumentId);
    }
    return obj;
  },

  create(
    base?: DeepPartial<ImportDocumentsMetadata_IndividualImportStatus>,
  ): ImportDocumentsMetadata_IndividualImportStatus {
    return ImportDocumentsMetadata_IndividualImportStatus.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ImportDocumentsMetadata_IndividualImportStatus>,
  ): ImportDocumentsMetadata_IndividualImportStatus {
    const message = createBaseImportDocumentsMetadata_IndividualImportStatus();
    message.inputGcsSource = object.inputGcsSource ?? "";
    message.status = (object.status !== undefined && object.status !== null)
      ? Status.fromPartial(object.status)
      : undefined;
    message.outputDocumentId = (object.outputDocumentId !== undefined && object.outputDocumentId !== null)
      ? DocumentId.fromPartial(object.outputDocumentId)
      : undefined;
    return message;
  },
};

function createBaseImportDocumentsMetadata_ImportConfigValidationResult(): ImportDocumentsMetadata_ImportConfigValidationResult {
  return { inputGcsSource: "", status: undefined };
}

export const ImportDocumentsMetadata_ImportConfigValidationResult: MessageFns<
  ImportDocumentsMetadata_ImportConfigValidationResult
> = {
  encode(
    message: ImportDocumentsMetadata_ImportConfigValidationResult,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.inputGcsSource !== "") {
      writer.uint32(10).string(message.inputGcsSource);
    }
    if (message.status !== undefined) {
      Status.encode(message.status, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportDocumentsMetadata_ImportConfigValidationResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportDocumentsMetadata_ImportConfigValidationResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputGcsSource = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.status = Status.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportDocumentsMetadata_ImportConfigValidationResult {
    return {
      inputGcsSource: isSet(object.inputGcsSource) ? globalThis.String(object.inputGcsSource) : "",
      status: isSet(object.status) ? Status.fromJSON(object.status) : undefined,
    };
  },

  toJSON(message: ImportDocumentsMetadata_ImportConfigValidationResult): unknown {
    const obj: any = {};
    if (message.inputGcsSource !== "") {
      obj.inputGcsSource = message.inputGcsSource;
    }
    if (message.status !== undefined) {
      obj.status = Status.toJSON(message.status);
    }
    return obj;
  },

  create(
    base?: DeepPartial<ImportDocumentsMetadata_ImportConfigValidationResult>,
  ): ImportDocumentsMetadata_ImportConfigValidationResult {
    return ImportDocumentsMetadata_ImportConfigValidationResult.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ImportDocumentsMetadata_ImportConfigValidationResult>,
  ): ImportDocumentsMetadata_ImportConfigValidationResult {
    const message = createBaseImportDocumentsMetadata_ImportConfigValidationResult();
    message.inputGcsSource = object.inputGcsSource ?? "";
    message.status = (object.status !== undefined && object.status !== null)
      ? Status.fromPartial(object.status)
      : undefined;
    return message;
  },
};

function createBaseGetDocumentRequest(): GetDocumentRequest {
  return { dataset: "", documentId: undefined, readMask: undefined, pageRange: undefined };
}

export const GetDocumentRequest: MessageFns<GetDocumentRequest> = {
  encode(message: GetDocumentRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dataset !== "") {
      writer.uint32(10).string(message.dataset);
    }
    if (message.documentId !== undefined) {
      DocumentId.encode(message.documentId, writer.uint32(18).fork()).join();
    }
    if (message.readMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.readMask), writer.uint32(26).fork()).join();
    }
    if (message.pageRange !== undefined) {
      DocumentPageRange.encode(message.pageRange, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetDocumentRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetDocumentRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dataset = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.documentId = DocumentId.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.readMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.pageRange = DocumentPageRange.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetDocumentRequest {
    return {
      dataset: isSet(object.dataset) ? globalThis.String(object.dataset) : "",
      documentId: isSet(object.documentId) ? DocumentId.fromJSON(object.documentId) : undefined,
      readMask: isSet(object.readMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.readMask)) : undefined,
      pageRange: isSet(object.pageRange) ? DocumentPageRange.fromJSON(object.pageRange) : undefined,
    };
  },

  toJSON(message: GetDocumentRequest): unknown {
    const obj: any = {};
    if (message.dataset !== "") {
      obj.dataset = message.dataset;
    }
    if (message.documentId !== undefined) {
      obj.documentId = DocumentId.toJSON(message.documentId);
    }
    if (message.readMask !== undefined) {
      obj.readMask = FieldMask.toJSON(FieldMask.wrap(message.readMask));
    }
    if (message.pageRange !== undefined) {
      obj.pageRange = DocumentPageRange.toJSON(message.pageRange);
    }
    return obj;
  },

  create(base?: DeepPartial<GetDocumentRequest>): GetDocumentRequest {
    return GetDocumentRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetDocumentRequest>): GetDocumentRequest {
    const message = createBaseGetDocumentRequest();
    message.dataset = object.dataset ?? "";
    message.documentId = (object.documentId !== undefined && object.documentId !== null)
      ? DocumentId.fromPartial(object.documentId)
      : undefined;
    message.readMask = object.readMask ?? undefined;
    message.pageRange = (object.pageRange !== undefined && object.pageRange !== null)
      ? DocumentPageRange.fromPartial(object.pageRange)
      : undefined;
    return message;
  },
};

function createBaseGetDocumentResponse(): GetDocumentResponse {
  return { document: undefined };
}

export const GetDocumentResponse: MessageFns<GetDocumentResponse> = {
  encode(message: GetDocumentResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.document !== undefined) {
      Document.encode(message.document, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetDocumentResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetDocumentResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.document = Document.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetDocumentResponse {
    return { document: isSet(object.document) ? Document.fromJSON(object.document) : undefined };
  },

  toJSON(message: GetDocumentResponse): unknown {
    const obj: any = {};
    if (message.document !== undefined) {
      obj.document = Document.toJSON(message.document);
    }
    return obj;
  },

  create(base?: DeepPartial<GetDocumentResponse>): GetDocumentResponse {
    return GetDocumentResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetDocumentResponse>): GetDocumentResponse {
    const message = createBaseGetDocumentResponse();
    message.document = (object.document !== undefined && object.document !== null)
      ? Document.fromPartial(object.document)
      : undefined;
    return message;
  },
};

function createBaseListDocumentsRequest(): ListDocumentsRequest {
  return { dataset: "", pageSize: 0, pageToken: "", filter: "", returnTotalSize: false, skip: 0 };
}

export const ListDocumentsRequest: MessageFns<ListDocumentsRequest> = {
  encode(message: ListDocumentsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dataset !== "") {
      writer.uint32(10).string(message.dataset);
    }
    if (message.pageSize !== 0) {
      writer.uint32(16).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(26).string(message.pageToken);
    }
    if (message.filter !== "") {
      writer.uint32(34).string(message.filter);
    }
    if (message.returnTotalSize !== false) {
      writer.uint32(48).bool(message.returnTotalSize);
    }
    if (message.skip !== 0) {
      writer.uint32(64).int32(message.skip);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListDocumentsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListDocumentsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dataset = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pageToken = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.filter = reader.string();
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.returnTotalSize = reader.bool();
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.skip = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListDocumentsRequest {
    return {
      dataset: isSet(object.dataset) ? globalThis.String(object.dataset) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
      filter: isSet(object.filter) ? globalThis.String(object.filter) : "",
      returnTotalSize: isSet(object.returnTotalSize) ? globalThis.Boolean(object.returnTotalSize) : false,
      skip: isSet(object.skip) ? globalThis.Number(object.skip) : 0,
    };
  },

  toJSON(message: ListDocumentsRequest): unknown {
    const obj: any = {};
    if (message.dataset !== "") {
      obj.dataset = message.dataset;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    if (message.filter !== "") {
      obj.filter = message.filter;
    }
    if (message.returnTotalSize !== false) {
      obj.returnTotalSize = message.returnTotalSize;
    }
    if (message.skip !== 0) {
      obj.skip = Math.round(message.skip);
    }
    return obj;
  },

  create(base?: DeepPartial<ListDocumentsRequest>): ListDocumentsRequest {
    return ListDocumentsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListDocumentsRequest>): ListDocumentsRequest {
    const message = createBaseListDocumentsRequest();
    message.dataset = object.dataset ?? "";
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    message.filter = object.filter ?? "";
    message.returnTotalSize = object.returnTotalSize ?? false;
    message.skip = object.skip ?? 0;
    return message;
  },
};

function createBaseListDocumentsResponse(): ListDocumentsResponse {
  return { documentMetadata: [], nextPageToken: "", totalSize: 0 };
}

export const ListDocumentsResponse: MessageFns<ListDocumentsResponse> = {
  encode(message: ListDocumentsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.documentMetadata) {
      DocumentMetadata.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    if (message.totalSize !== 0) {
      writer.uint32(24).int32(message.totalSize);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListDocumentsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListDocumentsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.documentMetadata.push(DocumentMetadata.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.totalSize = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListDocumentsResponse {
    return {
      documentMetadata: globalThis.Array.isArray(object?.documentMetadata)
        ? object.documentMetadata.map((e: any) => DocumentMetadata.fromJSON(e))
        : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
      totalSize: isSet(object.totalSize) ? globalThis.Number(object.totalSize) : 0,
    };
  },

  toJSON(message: ListDocumentsResponse): unknown {
    const obj: any = {};
    if (message.documentMetadata?.length) {
      obj.documentMetadata = message.documentMetadata.map((e) => DocumentMetadata.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    if (message.totalSize !== 0) {
      obj.totalSize = Math.round(message.totalSize);
    }
    return obj;
  },

  create(base?: DeepPartial<ListDocumentsResponse>): ListDocumentsResponse {
    return ListDocumentsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListDocumentsResponse>): ListDocumentsResponse {
    const message = createBaseListDocumentsResponse();
    message.documentMetadata = object.documentMetadata?.map((e) => DocumentMetadata.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    message.totalSize = object.totalSize ?? 0;
    return message;
  },
};

function createBaseBatchDeleteDocumentsRequest(): BatchDeleteDocumentsRequest {
  return { dataset: "", datasetDocuments: undefined };
}

export const BatchDeleteDocumentsRequest: MessageFns<BatchDeleteDocumentsRequest> = {
  encode(message: BatchDeleteDocumentsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dataset !== "") {
      writer.uint32(10).string(message.dataset);
    }
    if (message.datasetDocuments !== undefined) {
      BatchDatasetDocuments.encode(message.datasetDocuments, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchDeleteDocumentsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchDeleteDocumentsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dataset = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.datasetDocuments = BatchDatasetDocuments.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchDeleteDocumentsRequest {
    return {
      dataset: isSet(object.dataset) ? globalThis.String(object.dataset) : "",
      datasetDocuments: isSet(object.datasetDocuments)
        ? BatchDatasetDocuments.fromJSON(object.datasetDocuments)
        : undefined,
    };
  },

  toJSON(message: BatchDeleteDocumentsRequest): unknown {
    const obj: any = {};
    if (message.dataset !== "") {
      obj.dataset = message.dataset;
    }
    if (message.datasetDocuments !== undefined) {
      obj.datasetDocuments = BatchDatasetDocuments.toJSON(message.datasetDocuments);
    }
    return obj;
  },

  create(base?: DeepPartial<BatchDeleteDocumentsRequest>): BatchDeleteDocumentsRequest {
    return BatchDeleteDocumentsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchDeleteDocumentsRequest>): BatchDeleteDocumentsRequest {
    const message = createBaseBatchDeleteDocumentsRequest();
    message.dataset = object.dataset ?? "";
    message.datasetDocuments = (object.datasetDocuments !== undefined && object.datasetDocuments !== null)
      ? BatchDatasetDocuments.fromPartial(object.datasetDocuments)
      : undefined;
    return message;
  },
};

function createBaseBatchDeleteDocumentsResponse(): BatchDeleteDocumentsResponse {
  return {};
}

export const BatchDeleteDocumentsResponse: MessageFns<BatchDeleteDocumentsResponse> = {
  encode(_: BatchDeleteDocumentsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchDeleteDocumentsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchDeleteDocumentsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): BatchDeleteDocumentsResponse {
    return {};
  },

  toJSON(_: BatchDeleteDocumentsResponse): unknown {
    const obj: any = {};
    return obj;
  },

  create(base?: DeepPartial<BatchDeleteDocumentsResponse>): BatchDeleteDocumentsResponse {
    return BatchDeleteDocumentsResponse.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<BatchDeleteDocumentsResponse>): BatchDeleteDocumentsResponse {
    const message = createBaseBatchDeleteDocumentsResponse();
    return message;
  },
};

function createBaseBatchDeleteDocumentsMetadata(): BatchDeleteDocumentsMetadata {
  return { commonMetadata: undefined, individualBatchDeleteStatuses: [], totalDocumentCount: 0, errorDocumentCount: 0 };
}

export const BatchDeleteDocumentsMetadata: MessageFns<BatchDeleteDocumentsMetadata> = {
  encode(message: BatchDeleteDocumentsMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.commonMetadata !== undefined) {
      CommonOperationMetadata.encode(message.commonMetadata, writer.uint32(10).fork()).join();
    }
    for (const v of message.individualBatchDeleteStatuses) {
      BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.totalDocumentCount !== 0) {
      writer.uint32(24).int32(message.totalDocumentCount);
    }
    if (message.errorDocumentCount !== 0) {
      writer.uint32(32).int32(message.errorDocumentCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchDeleteDocumentsMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchDeleteDocumentsMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.commonMetadata = CommonOperationMetadata.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.individualBatchDeleteStatuses.push(
            BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus.decode(reader, reader.uint32()),
          );
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.totalDocumentCount = reader.int32();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.errorDocumentCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchDeleteDocumentsMetadata {
    return {
      commonMetadata: isSet(object.commonMetadata)
        ? CommonOperationMetadata.fromJSON(object.commonMetadata)
        : undefined,
      individualBatchDeleteStatuses: globalThis.Array.isArray(object?.individualBatchDeleteStatuses)
        ? object.individualBatchDeleteStatuses.map((e: any) =>
          BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus.fromJSON(e)
        )
        : [],
      totalDocumentCount: isSet(object.totalDocumentCount) ? globalThis.Number(object.totalDocumentCount) : 0,
      errorDocumentCount: isSet(object.errorDocumentCount) ? globalThis.Number(object.errorDocumentCount) : 0,
    };
  },

  toJSON(message: BatchDeleteDocumentsMetadata): unknown {
    const obj: any = {};
    if (message.commonMetadata !== undefined) {
      obj.commonMetadata = CommonOperationMetadata.toJSON(message.commonMetadata);
    }
    if (message.individualBatchDeleteStatuses?.length) {
      obj.individualBatchDeleteStatuses = message.individualBatchDeleteStatuses.map((e) =>
        BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus.toJSON(e)
      );
    }
    if (message.totalDocumentCount !== 0) {
      obj.totalDocumentCount = Math.round(message.totalDocumentCount);
    }
    if (message.errorDocumentCount !== 0) {
      obj.errorDocumentCount = Math.round(message.errorDocumentCount);
    }
    return obj;
  },

  create(base?: DeepPartial<BatchDeleteDocumentsMetadata>): BatchDeleteDocumentsMetadata {
    return BatchDeleteDocumentsMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchDeleteDocumentsMetadata>): BatchDeleteDocumentsMetadata {
    const message = createBaseBatchDeleteDocumentsMetadata();
    message.commonMetadata = (object.commonMetadata !== undefined && object.commonMetadata !== null)
      ? CommonOperationMetadata.fromPartial(object.commonMetadata)
      : undefined;
    message.individualBatchDeleteStatuses =
      object.individualBatchDeleteStatuses?.map((e) =>
        BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus.fromPartial(e)
      ) || [];
    message.totalDocumentCount = object.totalDocumentCount ?? 0;
    message.errorDocumentCount = object.errorDocumentCount ?? 0;
    return message;
  },
};

function createBaseBatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus(): BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus {
  return { documentId: undefined, status: undefined };
}

export const BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus: MessageFns<
  BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus
> = {
  encode(
    message: BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.documentId !== undefined) {
      DocumentId.encode(message.documentId, writer.uint32(10).fork()).join();
    }
    if (message.status !== undefined) {
      Status.encode(message.status, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.documentId = DocumentId.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.status = Status.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus {
    return {
      documentId: isSet(object.documentId) ? DocumentId.fromJSON(object.documentId) : undefined,
      status: isSet(object.status) ? Status.fromJSON(object.status) : undefined,
    };
  },

  toJSON(message: BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus): unknown {
    const obj: any = {};
    if (message.documentId !== undefined) {
      obj.documentId = DocumentId.toJSON(message.documentId);
    }
    if (message.status !== undefined) {
      obj.status = Status.toJSON(message.status);
    }
    return obj;
  },

  create(
    base?: DeepPartial<BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus>,
  ): BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus {
    return BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus>,
  ): BatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus {
    const message = createBaseBatchDeleteDocumentsMetadata_IndividualBatchDeleteStatus();
    message.documentId = (object.documentId !== undefined && object.documentId !== null)
      ? DocumentId.fromPartial(object.documentId)
      : undefined;
    message.status = (object.status !== undefined && object.status !== null)
      ? Status.fromPartial(object.status)
      : undefined;
    return message;
  },
};

function createBaseGetDatasetSchemaRequest(): GetDatasetSchemaRequest {
  return { name: "", visibleFieldsOnly: false };
}

export const GetDatasetSchemaRequest: MessageFns<GetDatasetSchemaRequest> = {
  encode(message: GetDatasetSchemaRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.visibleFieldsOnly !== false) {
      writer.uint32(16).bool(message.visibleFieldsOnly);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetDatasetSchemaRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetDatasetSchemaRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.visibleFieldsOnly = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetDatasetSchemaRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      visibleFieldsOnly: isSet(object.visibleFieldsOnly) ? globalThis.Boolean(object.visibleFieldsOnly) : false,
    };
  },

  toJSON(message: GetDatasetSchemaRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.visibleFieldsOnly !== false) {
      obj.visibleFieldsOnly = message.visibleFieldsOnly;
    }
    return obj;
  },

  create(base?: DeepPartial<GetDatasetSchemaRequest>): GetDatasetSchemaRequest {
    return GetDatasetSchemaRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetDatasetSchemaRequest>): GetDatasetSchemaRequest {
    const message = createBaseGetDatasetSchemaRequest();
    message.name = object.name ?? "";
    message.visibleFieldsOnly = object.visibleFieldsOnly ?? false;
    return message;
  },
};

function createBaseUpdateDatasetSchemaRequest(): UpdateDatasetSchemaRequest {
  return { datasetSchema: undefined, updateMask: undefined };
}

export const UpdateDatasetSchemaRequest: MessageFns<UpdateDatasetSchemaRequest> = {
  encode(message: UpdateDatasetSchemaRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.datasetSchema !== undefined) {
      DatasetSchema.encode(message.datasetSchema, writer.uint32(10).fork()).join();
    }
    if (message.updateMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.updateMask), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateDatasetSchemaRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateDatasetSchemaRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.datasetSchema = DatasetSchema.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.updateMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateDatasetSchemaRequest {
    return {
      datasetSchema: isSet(object.datasetSchema) ? DatasetSchema.fromJSON(object.datasetSchema) : undefined,
      updateMask: isSet(object.updateMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.updateMask)) : undefined,
    };
  },

  toJSON(message: UpdateDatasetSchemaRequest): unknown {
    const obj: any = {};
    if (message.datasetSchema !== undefined) {
      obj.datasetSchema = DatasetSchema.toJSON(message.datasetSchema);
    }
    if (message.updateMask !== undefined) {
      obj.updateMask = FieldMask.toJSON(FieldMask.wrap(message.updateMask));
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateDatasetSchemaRequest>): UpdateDatasetSchemaRequest {
    return UpdateDatasetSchemaRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateDatasetSchemaRequest>): UpdateDatasetSchemaRequest {
    const message = createBaseUpdateDatasetSchemaRequest();
    message.datasetSchema = (object.datasetSchema !== undefined && object.datasetSchema !== null)
      ? DatasetSchema.fromPartial(object.datasetSchema)
      : undefined;
    message.updateMask = object.updateMask ?? undefined;
    return message;
  },
};

function createBaseDocumentPageRange(): DocumentPageRange {
  return { start: 0, end: 0 };
}

export const DocumentPageRange: MessageFns<DocumentPageRange> = {
  encode(message: DocumentPageRange, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.start !== 0) {
      writer.uint32(8).int32(message.start);
    }
    if (message.end !== 0) {
      writer.uint32(16).int32(message.end);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DocumentPageRange {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDocumentPageRange();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.start = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.end = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DocumentPageRange {
    return {
      start: isSet(object.start) ? globalThis.Number(object.start) : 0,
      end: isSet(object.end) ? globalThis.Number(object.end) : 0,
    };
  },

  toJSON(message: DocumentPageRange): unknown {
    const obj: any = {};
    if (message.start !== 0) {
      obj.start = Math.round(message.start);
    }
    if (message.end !== 0) {
      obj.end = Math.round(message.end);
    }
    return obj;
  },

  create(base?: DeepPartial<DocumentPageRange>): DocumentPageRange {
    return DocumentPageRange.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DocumentPageRange>): DocumentPageRange {
    const message = createBaseDocumentPageRange();
    message.start = object.start ?? 0;
    message.end = object.end ?? 0;
    return message;
  },
};

function createBaseDocumentMetadata(): DocumentMetadata {
  return { documentId: undefined, pageCount: 0, datasetType: 0, labelingState: 0, displayName: "" };
}

export const DocumentMetadata: MessageFns<DocumentMetadata> = {
  encode(message: DocumentMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.documentId !== undefined) {
      DocumentId.encode(message.documentId, writer.uint32(10).fork()).join();
    }
    if (message.pageCount !== 0) {
      writer.uint32(16).int32(message.pageCount);
    }
    if (message.datasetType !== 0) {
      writer.uint32(24).int32(message.datasetType);
    }
    if (message.labelingState !== 0) {
      writer.uint32(40).int32(message.labelingState);
    }
    if (message.displayName !== "") {
      writer.uint32(50).string(message.displayName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DocumentMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDocumentMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.documentId = DocumentId.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pageCount = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.datasetType = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.labelingState = reader.int32() as any;
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.displayName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DocumentMetadata {
    return {
      documentId: isSet(object.documentId) ? DocumentId.fromJSON(object.documentId) : undefined,
      pageCount: isSet(object.pageCount) ? globalThis.Number(object.pageCount) : 0,
      datasetType: isSet(object.datasetType) ? datasetSplitTypeFromJSON(object.datasetType) : 0,
      labelingState: isSet(object.labelingState) ? documentLabelingStateFromJSON(object.labelingState) : 0,
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
    };
  },

  toJSON(message: DocumentMetadata): unknown {
    const obj: any = {};
    if (message.documentId !== undefined) {
      obj.documentId = DocumentId.toJSON(message.documentId);
    }
    if (message.pageCount !== 0) {
      obj.pageCount = Math.round(message.pageCount);
    }
    if (message.datasetType !== 0) {
      obj.datasetType = datasetSplitTypeToJSON(message.datasetType);
    }
    if (message.labelingState !== 0) {
      obj.labelingState = documentLabelingStateToJSON(message.labelingState);
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    return obj;
  },

  create(base?: DeepPartial<DocumentMetadata>): DocumentMetadata {
    return DocumentMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DocumentMetadata>): DocumentMetadata {
    const message = createBaseDocumentMetadata();
    message.documentId = (object.documentId !== undefined && object.documentId !== null)
      ? DocumentId.fromPartial(object.documentId)
      : undefined;
    message.pageCount = object.pageCount ?? 0;
    message.datasetType = object.datasetType ?? 0;
    message.labelingState = object.labelingState ?? 0;
    message.displayName = object.displayName ?? "";
    return message;
  },
};

/** Service to call Cloud DocumentAI to manage document collection (dataset). */
export type DocumentServiceDefinition = typeof DocumentServiceDefinition;
export const DocumentServiceDefinition = {
  name: "DocumentService",
  fullName: "google.cloud.documentai.v1beta3.DocumentService",
  methods: {
    /**
     * Updates metadata associated with a dataset.
     * Note that this method requires the
     * `documentai.googleapis.com/datasets.update` permission on the project,
     * which is highly privileged. A user or service account with this permission
     * can create new processors that can interact with any gcs bucket in your
     * project.
     */
    updateDataset: {
      name: "UpdateDataset",
      requestType: UpdateDatasetRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              41,
              10,
              7,
              68,
              97,
              116,
              97,
              115,
              101,
              116,
              18,
              30,
              85,
              112,
              100,
              97,
              116,
              101,
              68,
              97,
              116,
              97,
              115,
              101,
              116,
              79,
              112,
              101,
              114,
              97,
              116,
              105,
              111,
              110,
              77,
              101,
              116,
              97,
              100,
              97,
              116,
              97,
            ]),
          ],
          8410: [
            Buffer.from([19, 100, 97, 116, 97, 115, 101, 116, 44, 117, 112, 100, 97, 116, 101, 95, 109, 97, 115, 107]),
          ],
          578365826: [
            Buffer.from([
              78,
              58,
              7,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              50,
              67,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              51,
              47,
              123,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              46,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              112,
              114,
              111,
              99,
              101,
              115,
              115,
              111,
              114,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              125,
            ]),
          ],
        },
      },
    },
    /** Import documents into a dataset. */
    importDocuments: {
      name: "ImportDocuments",
      requestType: ImportDocumentsRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              50,
              10,
              23,
              73,
              109,
              112,
              111,
              114,
              116,
              68,
              111,
              99,
              117,
              109,
              101,
              110,
              116,
              115,
              82,
              101,
              115,
              112,
              111,
              110,
              115,
              101,
              18,
              23,
              73,
              109,
              112,
              111,
              114,
              116,
              68,
              111,
              99,
              117,
              109,
              101,
              110,
              116,
              115,
              77,
              101,
              116,
              97,
              100,
              97,
              116,
              97,
            ]),
          ],
          8410: [Buffer.from([7, 100, 97, 116, 97, 115, 101, 116])],
          578365826: [
            Buffer.from([
              83,
              58,
              1,
              42,
              34,
              78,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              51,
              47,
              123,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              112,
              114,
              111,
              99,
              101,
              115,
              115,
              111,
              114,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              125,
              58,
              105,
              109,
              112,
              111,
              114,
              116,
              68,
              111,
              99,
              117,
              109,
              101,
              110,
              116,
              115,
            ]),
          ],
        },
      },
    },
    /** Returns relevant fields present in the requested document. */
    getDocument: {
      name: "GetDocument",
      requestType: GetDocumentRequest,
      requestStream: false,
      responseType: GetDocumentResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([7, 100, 97, 116, 97, 115, 101, 116])],
          578365826: [
            Buffer.from([
              76,
              18,
              74,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              51,
              47,
              123,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              112,
              114,
              111,
              99,
              101,
              115,
              115,
              111,
              114,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              125,
              58,
              103,
              101,
              116,
              68,
              111,
              99,
              117,
              109,
              101,
              110,
              116,
            ]),
          ],
        },
      },
    },
    /** Returns a list of documents present in the dataset. */
    listDocuments: {
      name: "ListDocuments",
      requestType: ListDocumentsRequest,
      requestStream: false,
      responseType: ListDocumentsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([7, 100, 97, 116, 97, 115, 101, 116])],
          578365826: [
            Buffer.from([
              81,
              58,
              1,
              42,
              34,
              76,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              51,
              47,
              123,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              112,
              114,
              111,
              99,
              101,
              115,
              115,
              111,
              114,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              125,
              58,
              108,
              105,
              115,
              116,
              68,
              111,
              99,
              117,
              109,
              101,
              110,
              116,
              115,
            ]),
          ],
        },
      },
    },
    /** Deletes a set of documents. */
    batchDeleteDocuments: {
      name: "BatchDeleteDocuments",
      requestType: BatchDeleteDocumentsRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              60,
              10,
              28,
              66,
              97,
              116,
              99,
              104,
              68,
              101,
              108,
              101,
              116,
              101,
              68,
              111,
              99,
              117,
              109,
              101,
              110,
              116,
              115,
              82,
              101,
              115,
              112,
              111,
              110,
              115,
              101,
              18,
              28,
              66,
              97,
              116,
              99,
              104,
              68,
              101,
              108,
              101,
              116,
              101,
              68,
              111,
              99,
              117,
              109,
              101,
              110,
              116,
              115,
              77,
              101,
              116,
              97,
              100,
              97,
              116,
              97,
            ]),
          ],
          8410: [Buffer.from([7, 100, 97, 116, 97, 115, 101, 116])],
          578365826: [
            Buffer.from([
              88,
              58,
              1,
              42,
              34,
              83,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              51,
              47,
              123,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              112,
              114,
              111,
              99,
              101,
              115,
              115,
              111,
              114,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              125,
              58,
              98,
              97,
              116,
              99,
              104,
              68,
              101,
              108,
              101,
              116,
              101,
              68,
              111,
              99,
              117,
              109,
              101,
              110,
              116,
              115,
            ]),
          ],
        },
      },
    },
    /** Gets the `DatasetSchema` of a `Dataset`. */
    getDatasetSchema: {
      name: "GetDatasetSchema",
      requestType: GetDatasetSchemaRequest,
      requestStream: false,
      responseType: DatasetSchema,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              75,
              18,
              73,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              51,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              112,
              114,
              111,
              99,
              101,
              115,
              115,
              111,
              114,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              83,
              99,
              104,
              101,
              109,
              97,
              125,
            ]),
          ],
        },
      },
    },
    /** Updates a `DatasetSchema`. */
    updateDatasetSchema: {
      name: "UpdateDatasetSchema",
      requestType: UpdateDatasetSchemaRequest,
      requestStream: false,
      responseType: DatasetSchema,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              26,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              95,
              115,
              99,
              104,
              101,
              109,
              97,
              44,
              117,
              112,
              100,
              97,
              116,
              101,
              95,
              109,
              97,
              115,
              107,
            ]),
          ],
          578365826: [
            Buffer.from([
              106,
              58,
              14,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              95,
              115,
              99,
              104,
              101,
              109,
              97,
              50,
              88,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              51,
              47,
              123,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              95,
              115,
              99,
              104,
              101,
              109,
              97,
              46,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              112,
              114,
              111,
              99,
              101,
              115,
              115,
              111,
              114,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              47,
              100,
              97,
              116,
              97,
              115,
              101,
              116,
              83,
              99,
              104,
              101,
              109,
              97,
              125,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface DocumentServiceImplementation<CallContextExt = {}> {
  /**
   * Updates metadata associated with a dataset.
   * Note that this method requires the
   * `documentai.googleapis.com/datasets.update` permission on the project,
   * which is highly privileged. A user or service account with this permission
   * can create new processors that can interact with any gcs bucket in your
   * project.
   */
  updateDataset(request: UpdateDatasetRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
  /** Import documents into a dataset. */
  importDocuments(
    request: ImportDocumentsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Operation>>;
  /** Returns relevant fields present in the requested document. */
  getDocument(
    request: GetDocumentRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<GetDocumentResponse>>;
  /** Returns a list of documents present in the dataset. */
  listDocuments(
    request: ListDocumentsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ListDocumentsResponse>>;
  /** Deletes a set of documents. */
  batchDeleteDocuments(
    request: BatchDeleteDocumentsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Operation>>;
  /** Gets the `DatasetSchema` of a `Dataset`. */
  getDatasetSchema(
    request: GetDatasetSchemaRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<DatasetSchema>>;
  /** Updates a `DatasetSchema`. */
  updateDatasetSchema(
    request: UpdateDatasetSchemaRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<DatasetSchema>>;
}

export interface DocumentServiceClient<CallOptionsExt = {}> {
  /**
   * Updates metadata associated with a dataset.
   * Note that this method requires the
   * `documentai.googleapis.com/datasets.update` permission on the project,
   * which is highly privileged. A user or service account with this permission
   * can create new processors that can interact with any gcs bucket in your
   * project.
   */
  updateDataset(request: DeepPartial<UpdateDatasetRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
  /** Import documents into a dataset. */
  importDocuments(
    request: DeepPartial<ImportDocumentsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Operation>;
  /** Returns relevant fields present in the requested document. */
  getDocument(
    request: DeepPartial<GetDocumentRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<GetDocumentResponse>;
  /** Returns a list of documents present in the dataset. */
  listDocuments(
    request: DeepPartial<ListDocumentsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ListDocumentsResponse>;
  /** Deletes a set of documents. */
  batchDeleteDocuments(
    request: DeepPartial<BatchDeleteDocumentsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Operation>;
  /** Gets the `DatasetSchema` of a `Dataset`. */
  getDatasetSchema(
    request: DeepPartial<GetDatasetSchemaRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<DatasetSchema>;
  /** Updates a `DatasetSchema`. */
  updateDatasetSchema(
    request: DeepPartial<UpdateDatasetSchemaRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<DatasetSchema>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
