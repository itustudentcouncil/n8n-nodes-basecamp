// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/notebooks/v1/execution.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Timestamp } from "../../../protobuf/timestamp.js";

export const protobufPackage = "google.cloud.notebooks.v1";

/** The description a notebook execution workload. */
export interface ExecutionTemplate {
  /**
   * Required. Scale tier of the hardware used for notebook execution.
   * DEPRECATED Will be discontinued. As right now only CUSTOM is supported.
   *
   * @deprecated
   */
  scaleTier: ExecutionTemplate_ScaleTier;
  /**
   * Specifies the type of virtual machine to use for your training
   * job's master worker. You must specify this field when `scaleTier` is set to
   * `CUSTOM`.
   *
   * You can use certain Compute Engine machine types directly in this field.
   * The following types are supported:
   *
   * - `n1-standard-4`
   * - `n1-standard-8`
   * - `n1-standard-16`
   * - `n1-standard-32`
   * - `n1-standard-64`
   * - `n1-standard-96`
   * - `n1-highmem-2`
   * - `n1-highmem-4`
   * - `n1-highmem-8`
   * - `n1-highmem-16`
   * - `n1-highmem-32`
   * - `n1-highmem-64`
   * - `n1-highmem-96`
   * - `n1-highcpu-16`
   * - `n1-highcpu-32`
   * - `n1-highcpu-64`
   * - `n1-highcpu-96`
   *
   * Alternatively, you can use the following legacy machine types:
   *
   * - `standard`
   * - `large_model`
   * - `complex_model_s`
   * - `complex_model_m`
   * - `complex_model_l`
   * - `standard_gpu`
   * - `complex_model_m_gpu`
   * - `complex_model_l_gpu`
   * - `standard_p100`
   * - `complex_model_m_p100`
   * - `standard_v100`
   * - `large_model_v100`
   * - `complex_model_m_v100`
   * - `complex_model_l_v100`
   *
   * Finally, if you want to use a TPU for training, specify `cloud_tpu` in this
   * field. Learn more about the [special configuration options for training
   * with
   * TPU](https://cloud.google.com/ai-platform/training/docs/using-tpus#configuring_a_custom_tpu_machine).
   */
  masterType: string;
  /**
   * Configuration (count and accelerator type) for hardware running notebook
   * execution.
   */
  acceleratorConfig:
    | ExecutionTemplate_SchedulerAcceleratorConfig
    | undefined;
  /**
   * Labels for execution.
   * If execution is scheduled, a field included will be 'nbs-scheduled'.
   * Otherwise, it is an immediate execution, and an included field will be
   * 'nbs-immediate'. Use fields to efficiently index between various types of
   * executions.
   */
  labels: { [key: string]: string };
  /**
   * Path to the notebook file to execute.
   * Must be in a Google Cloud Storage bucket.
   * Format: `gs://{bucket_name}/{folder}/{notebook_file_name}`
   * Ex: `gs://notebook_user/scheduled_notebooks/sentiment_notebook.ipynb`
   */
  inputNotebookFile: string;
  /**
   * Container Image URI to a DLVM
   * Example: 'gcr.io/deeplearning-platform-release/base-cu100'
   * More examples can be found at:
   * https://cloud.google.com/ai-platform/deep-learning-containers/docs/choosing-container
   */
  containerImageUri: string;
  /**
   * Path to the notebook folder to write to.
   * Must be in a Google Cloud Storage bucket path.
   * Format: `gs://{bucket_name}/{folder}`
   * Ex: `gs://notebook_user/scheduled_notebooks`
   */
  outputNotebookFolder: string;
  /**
   * Parameters to be overridden in the notebook during execution.
   * Ref https://papermill.readthedocs.io/en/latest/usage-parameterize.html on
   * how to specifying parameters in the input notebook and pass them here
   * in an YAML file.
   * Ex: `gs://notebook_user/scheduled_notebooks/sentiment_notebook_params.yaml`
   */
  paramsYamlFile: string;
  /** Parameters used within the 'input_notebook_file' notebook. */
  parameters: string;
  /**
   * The email address of a service account to use when running the execution.
   * You must have the `iam.serviceAccounts.actAs` permission for the specified
   * service account.
   */
  serviceAccount: string;
  /** The type of Job to be used on this execution. */
  jobType: ExecutionTemplate_JobType;
  /** Parameters used in Dataproc JobType executions. */
  dataprocParameters?:
    | ExecutionTemplate_DataprocParameters
    | undefined;
  /** Parameters used in Vertex AI JobType executions. */
  vertexAiParameters?:
    | ExecutionTemplate_VertexAIParameters
    | undefined;
  /**
   * Name of the kernel spec to use. This must be specified if the
   * kernel spec name on the execution target does not match the name in the
   * input notebook file.
   */
  kernelSpec: string;
  /**
   * The name of a Vertex AI [Tensorboard] resource to which this execution
   * will upload Tensorboard logs.
   * Format:
   * `projects/{project}/locations/{location}/tensorboards/{tensorboard}`
   */
  tensorboard: string;
}

/**
 * Required. Specifies the machine types, the number of replicas for workers
 * and parameter servers.
 */
export enum ExecutionTemplate_ScaleTier {
  /** SCALE_TIER_UNSPECIFIED - Unspecified Scale Tier. */
  SCALE_TIER_UNSPECIFIED = 0,
  /**
   * BASIC - A single worker instance. This tier is suitable for learning how to use
   * Cloud ML, and for experimenting with new models using small datasets.
   */
  BASIC = 1,
  /** STANDARD_1 - Many workers and a few parameter servers. */
  STANDARD_1 = 2,
  /** PREMIUM_1 - A large number of workers with many parameter servers. */
  PREMIUM_1 = 3,
  /** BASIC_GPU - A single worker instance with a K80 GPU. */
  BASIC_GPU = 4,
  /** BASIC_TPU - A single worker instance with a Cloud TPU. */
  BASIC_TPU = 5,
  /**
   * CUSTOM - The CUSTOM tier is not a set tier, but rather enables you to use your
   * own cluster specification. When you use this tier, set values to
   * configure your processing cluster according to these guidelines:
   *
   * *   You _must_ set `ExecutionTemplate.masterType` to specify the type
   *     of machine to use for your master node. This is the only required
   *     setting.
   */
  CUSTOM = 6,
  UNRECOGNIZED = -1,
}

export function executionTemplate_ScaleTierFromJSON(object: any): ExecutionTemplate_ScaleTier {
  switch (object) {
    case 0:
    case "SCALE_TIER_UNSPECIFIED":
      return ExecutionTemplate_ScaleTier.SCALE_TIER_UNSPECIFIED;
    case 1:
    case "BASIC":
      return ExecutionTemplate_ScaleTier.BASIC;
    case 2:
    case "STANDARD_1":
      return ExecutionTemplate_ScaleTier.STANDARD_1;
    case 3:
    case "PREMIUM_1":
      return ExecutionTemplate_ScaleTier.PREMIUM_1;
    case 4:
    case "BASIC_GPU":
      return ExecutionTemplate_ScaleTier.BASIC_GPU;
    case 5:
    case "BASIC_TPU":
      return ExecutionTemplate_ScaleTier.BASIC_TPU;
    case 6:
    case "CUSTOM":
      return ExecutionTemplate_ScaleTier.CUSTOM;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ExecutionTemplate_ScaleTier.UNRECOGNIZED;
  }
}

export function executionTemplate_ScaleTierToJSON(object: ExecutionTemplate_ScaleTier): string {
  switch (object) {
    case ExecutionTemplate_ScaleTier.SCALE_TIER_UNSPECIFIED:
      return "SCALE_TIER_UNSPECIFIED";
    case ExecutionTemplate_ScaleTier.BASIC:
      return "BASIC";
    case ExecutionTemplate_ScaleTier.STANDARD_1:
      return "STANDARD_1";
    case ExecutionTemplate_ScaleTier.PREMIUM_1:
      return "PREMIUM_1";
    case ExecutionTemplate_ScaleTier.BASIC_GPU:
      return "BASIC_GPU";
    case ExecutionTemplate_ScaleTier.BASIC_TPU:
      return "BASIC_TPU";
    case ExecutionTemplate_ScaleTier.CUSTOM:
      return "CUSTOM";
    case ExecutionTemplate_ScaleTier.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Hardware accelerator types for AI Platform Training jobs. */
export enum ExecutionTemplate_SchedulerAcceleratorType {
  /** SCHEDULER_ACCELERATOR_TYPE_UNSPECIFIED - Unspecified accelerator type. Default to no GPU. */
  SCHEDULER_ACCELERATOR_TYPE_UNSPECIFIED = 0,
  /** NVIDIA_TESLA_K80 - Nvidia Tesla K80 GPU. */
  NVIDIA_TESLA_K80 = 1,
  /** NVIDIA_TESLA_P100 - Nvidia Tesla P100 GPU. */
  NVIDIA_TESLA_P100 = 2,
  /** NVIDIA_TESLA_V100 - Nvidia Tesla V100 GPU. */
  NVIDIA_TESLA_V100 = 3,
  /** NVIDIA_TESLA_P4 - Nvidia Tesla P4 GPU. */
  NVIDIA_TESLA_P4 = 4,
  /** NVIDIA_TESLA_T4 - Nvidia Tesla T4 GPU. */
  NVIDIA_TESLA_T4 = 5,
  /** NVIDIA_TESLA_A100 - Nvidia Tesla A100 GPU. */
  NVIDIA_TESLA_A100 = 10,
  /** TPU_V2 - TPU v2. */
  TPU_V2 = 6,
  /** TPU_V3 - TPU v3. */
  TPU_V3 = 7,
  UNRECOGNIZED = -1,
}

export function executionTemplate_SchedulerAcceleratorTypeFromJSON(
  object: any,
): ExecutionTemplate_SchedulerAcceleratorType {
  switch (object) {
    case 0:
    case "SCHEDULER_ACCELERATOR_TYPE_UNSPECIFIED":
      return ExecutionTemplate_SchedulerAcceleratorType.SCHEDULER_ACCELERATOR_TYPE_UNSPECIFIED;
    case 1:
    case "NVIDIA_TESLA_K80":
      return ExecutionTemplate_SchedulerAcceleratorType.NVIDIA_TESLA_K80;
    case 2:
    case "NVIDIA_TESLA_P100":
      return ExecutionTemplate_SchedulerAcceleratorType.NVIDIA_TESLA_P100;
    case 3:
    case "NVIDIA_TESLA_V100":
      return ExecutionTemplate_SchedulerAcceleratorType.NVIDIA_TESLA_V100;
    case 4:
    case "NVIDIA_TESLA_P4":
      return ExecutionTemplate_SchedulerAcceleratorType.NVIDIA_TESLA_P4;
    case 5:
    case "NVIDIA_TESLA_T4":
      return ExecutionTemplate_SchedulerAcceleratorType.NVIDIA_TESLA_T4;
    case 10:
    case "NVIDIA_TESLA_A100":
      return ExecutionTemplate_SchedulerAcceleratorType.NVIDIA_TESLA_A100;
    case 6:
    case "TPU_V2":
      return ExecutionTemplate_SchedulerAcceleratorType.TPU_V2;
    case 7:
    case "TPU_V3":
      return ExecutionTemplate_SchedulerAcceleratorType.TPU_V3;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ExecutionTemplate_SchedulerAcceleratorType.UNRECOGNIZED;
  }
}

export function executionTemplate_SchedulerAcceleratorTypeToJSON(
  object: ExecutionTemplate_SchedulerAcceleratorType,
): string {
  switch (object) {
    case ExecutionTemplate_SchedulerAcceleratorType.SCHEDULER_ACCELERATOR_TYPE_UNSPECIFIED:
      return "SCHEDULER_ACCELERATOR_TYPE_UNSPECIFIED";
    case ExecutionTemplate_SchedulerAcceleratorType.NVIDIA_TESLA_K80:
      return "NVIDIA_TESLA_K80";
    case ExecutionTemplate_SchedulerAcceleratorType.NVIDIA_TESLA_P100:
      return "NVIDIA_TESLA_P100";
    case ExecutionTemplate_SchedulerAcceleratorType.NVIDIA_TESLA_V100:
      return "NVIDIA_TESLA_V100";
    case ExecutionTemplate_SchedulerAcceleratorType.NVIDIA_TESLA_P4:
      return "NVIDIA_TESLA_P4";
    case ExecutionTemplate_SchedulerAcceleratorType.NVIDIA_TESLA_T4:
      return "NVIDIA_TESLA_T4";
    case ExecutionTemplate_SchedulerAcceleratorType.NVIDIA_TESLA_A100:
      return "NVIDIA_TESLA_A100";
    case ExecutionTemplate_SchedulerAcceleratorType.TPU_V2:
      return "TPU_V2";
    case ExecutionTemplate_SchedulerAcceleratorType.TPU_V3:
      return "TPU_V3";
    case ExecutionTemplate_SchedulerAcceleratorType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The backend used for this execution. */
export enum ExecutionTemplate_JobType {
  /** JOB_TYPE_UNSPECIFIED - No type specified. */
  JOB_TYPE_UNSPECIFIED = 0,
  /**
   * VERTEX_AI - Custom Job in `aiplatform.googleapis.com`.
   * Default value for an execution.
   */
  VERTEX_AI = 1,
  /**
   * DATAPROC - Run execution on a cluster with Dataproc as a job.
   * https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs
   */
  DATAPROC = 2,
  UNRECOGNIZED = -1,
}

export function executionTemplate_JobTypeFromJSON(object: any): ExecutionTemplate_JobType {
  switch (object) {
    case 0:
    case "JOB_TYPE_UNSPECIFIED":
      return ExecutionTemplate_JobType.JOB_TYPE_UNSPECIFIED;
    case 1:
    case "VERTEX_AI":
      return ExecutionTemplate_JobType.VERTEX_AI;
    case 2:
    case "DATAPROC":
      return ExecutionTemplate_JobType.DATAPROC;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ExecutionTemplate_JobType.UNRECOGNIZED;
  }
}

export function executionTemplate_JobTypeToJSON(object: ExecutionTemplate_JobType): string {
  switch (object) {
    case ExecutionTemplate_JobType.JOB_TYPE_UNSPECIFIED:
      return "JOB_TYPE_UNSPECIFIED";
    case ExecutionTemplate_JobType.VERTEX_AI:
      return "VERTEX_AI";
    case ExecutionTemplate_JobType.DATAPROC:
      return "DATAPROC";
    case ExecutionTemplate_JobType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Definition of a hardware accelerator. Note that not all combinations
 * of `type` and `core_count` are valid. Check [GPUs on
 * Compute Engine](https://cloud.google.com/compute/docs/gpus) to find a valid
 * combination. TPUs are not supported.
 */
export interface ExecutionTemplate_SchedulerAcceleratorConfig {
  /** Type of this accelerator. */
  type: ExecutionTemplate_SchedulerAcceleratorType;
  /** Count of cores of this accelerator. */
  coreCount: Long;
}

/** Parameters used in Dataproc JobType executions. */
export interface ExecutionTemplate_DataprocParameters {
  /**
   * URI for cluster used to run Dataproc execution.
   * Format: `projects/{PROJECT_ID}/regions/{REGION}/clusters/{CLUSTER_NAME}`
   */
  cluster: string;
}

/** Parameters used in Vertex AI JobType executions. */
export interface ExecutionTemplate_VertexAIParameters {
  /**
   * The full name of the Compute Engine
   * [network](https://cloud.google.com/compute/docs/networks-and-firewalls#networks)
   * to which the Job should be peered. For example,
   * `projects/12345/global/networks/myVPC`.
   * [Format](https://cloud.google.com/compute/docs/reference/rest/v1/networks/insert)
   * is of the form `projects/{project}/global/networks/{network}`.
   * Where `{project}` is a project number, as in `12345`, and `{network}` is
   * a network name.
   *
   * Private services access must already be configured for the network. If
   * left unspecified, the job is not peered with any network.
   */
  network: string;
  /**
   * Environment variables.
   * At most 100 environment variables can be specified and unique.
   * Example: `GCP_BUCKET=gs://my-bucket/samples/`
   */
  env: { [key: string]: string };
}

export interface ExecutionTemplate_VertexAIParameters_EnvEntry {
  key: string;
  value: string;
}

export interface ExecutionTemplate_LabelsEntry {
  key: string;
  value: string;
}

/** The definition of a single executed notebook. */
export interface Execution {
  /** execute metadata including name, hardware spec, region, labels, etc. */
  executionTemplate:
    | ExecutionTemplate
    | undefined;
  /**
   * Output only. The resource name of the execute. Format:
   * `projects/{project_id}/locations/{location}/executions/{execution_id}`
   */
  name: string;
  /**
   * Output only. Name used for UI purposes.
   * Name can only contain alphanumeric characters and underscores '_'.
   */
  displayName: string;
  /** A brief description of this execution. */
  description: string;
  /** Output only. Time the Execution was instantiated. */
  createTime:
    | Date
    | undefined;
  /** Output only. Time the Execution was last updated. */
  updateTime:
    | Date
    | undefined;
  /** Output only. State of the underlying AI Platform job. */
  state: Execution_State;
  /** Output notebook file generated by this execution */
  outputNotebookFile: string;
  /** Output only. The URI of the external job used to execute the notebook. */
  jobUri: string;
}

/** Enum description of the state of the underlying AIP job. */
export enum Execution_State {
  /** STATE_UNSPECIFIED - The job state is unspecified. */
  STATE_UNSPECIFIED = 0,
  /** QUEUED - The job has been just created and processing has not yet begun. */
  QUEUED = 1,
  /** PREPARING - The service is preparing to execution the job. */
  PREPARING = 2,
  /** RUNNING - The job is in progress. */
  RUNNING = 3,
  /** SUCCEEDED - The job completed successfully. */
  SUCCEEDED = 4,
  /**
   * FAILED - The job failed.
   * `error_message` should contain the details of the failure.
   */
  FAILED = 5,
  /**
   * CANCELLING - The job is being cancelled.
   * `error_message` should describe the reason for the cancellation.
   */
  CANCELLING = 6,
  /**
   * CANCELLED - The job has been cancelled.
   * `error_message` should describe the reason for the cancellation.
   */
  CANCELLED = 7,
  /**
   * EXPIRED - The job has become expired (relevant to Vertex AI jobs)
   * https://cloud.google.com/vertex-ai/docs/reference/rest/v1/JobState
   */
  EXPIRED = 9,
  /** INITIALIZING - The Execution is being created. */
  INITIALIZING = 10,
  UNRECOGNIZED = -1,
}

export function execution_StateFromJSON(object: any): Execution_State {
  switch (object) {
    case 0:
    case "STATE_UNSPECIFIED":
      return Execution_State.STATE_UNSPECIFIED;
    case 1:
    case "QUEUED":
      return Execution_State.QUEUED;
    case 2:
    case "PREPARING":
      return Execution_State.PREPARING;
    case 3:
    case "RUNNING":
      return Execution_State.RUNNING;
    case 4:
    case "SUCCEEDED":
      return Execution_State.SUCCEEDED;
    case 5:
    case "FAILED":
      return Execution_State.FAILED;
    case 6:
    case "CANCELLING":
      return Execution_State.CANCELLING;
    case 7:
    case "CANCELLED":
      return Execution_State.CANCELLED;
    case 9:
    case "EXPIRED":
      return Execution_State.EXPIRED;
    case 10:
    case "INITIALIZING":
      return Execution_State.INITIALIZING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return Execution_State.UNRECOGNIZED;
  }
}

export function execution_StateToJSON(object: Execution_State): string {
  switch (object) {
    case Execution_State.STATE_UNSPECIFIED:
      return "STATE_UNSPECIFIED";
    case Execution_State.QUEUED:
      return "QUEUED";
    case Execution_State.PREPARING:
      return "PREPARING";
    case Execution_State.RUNNING:
      return "RUNNING";
    case Execution_State.SUCCEEDED:
      return "SUCCEEDED";
    case Execution_State.FAILED:
      return "FAILED";
    case Execution_State.CANCELLING:
      return "CANCELLING";
    case Execution_State.CANCELLED:
      return "CANCELLED";
    case Execution_State.EXPIRED:
      return "EXPIRED";
    case Execution_State.INITIALIZING:
      return "INITIALIZING";
    case Execution_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseExecutionTemplate(): ExecutionTemplate {
  return {
    scaleTier: 0,
    masterType: "",
    acceleratorConfig: undefined,
    labels: {},
    inputNotebookFile: "",
    containerImageUri: "",
    outputNotebookFolder: "",
    paramsYamlFile: "",
    parameters: "",
    serviceAccount: "",
    jobType: 0,
    dataprocParameters: undefined,
    vertexAiParameters: undefined,
    kernelSpec: "",
    tensorboard: "",
  };
}

export const ExecutionTemplate: MessageFns<ExecutionTemplate> = {
  encode(message: ExecutionTemplate, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.scaleTier !== 0) {
      writer.uint32(8).int32(message.scaleTier);
    }
    if (message.masterType !== "") {
      writer.uint32(18).string(message.masterType);
    }
    if (message.acceleratorConfig !== undefined) {
      ExecutionTemplate_SchedulerAcceleratorConfig.encode(message.acceleratorConfig, writer.uint32(26).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      ExecutionTemplate_LabelsEntry.encode({ key: key as any, value }, writer.uint32(34).fork()).join();
    });
    if (message.inputNotebookFile !== "") {
      writer.uint32(42).string(message.inputNotebookFile);
    }
    if (message.containerImageUri !== "") {
      writer.uint32(50).string(message.containerImageUri);
    }
    if (message.outputNotebookFolder !== "") {
      writer.uint32(58).string(message.outputNotebookFolder);
    }
    if (message.paramsYamlFile !== "") {
      writer.uint32(66).string(message.paramsYamlFile);
    }
    if (message.parameters !== "") {
      writer.uint32(74).string(message.parameters);
    }
    if (message.serviceAccount !== "") {
      writer.uint32(82).string(message.serviceAccount);
    }
    if (message.jobType !== 0) {
      writer.uint32(88).int32(message.jobType);
    }
    if (message.dataprocParameters !== undefined) {
      ExecutionTemplate_DataprocParameters.encode(message.dataprocParameters, writer.uint32(98).fork()).join();
    }
    if (message.vertexAiParameters !== undefined) {
      ExecutionTemplate_VertexAIParameters.encode(message.vertexAiParameters, writer.uint32(106).fork()).join();
    }
    if (message.kernelSpec !== "") {
      writer.uint32(114).string(message.kernelSpec);
    }
    if (message.tensorboard !== "") {
      writer.uint32(122).string(message.tensorboard);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecutionTemplate {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecutionTemplate();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.scaleTier = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.masterType = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.acceleratorConfig = ExecutionTemplate_SchedulerAcceleratorConfig.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          const entry4 = ExecutionTemplate_LabelsEntry.decode(reader, reader.uint32());
          if (entry4.value !== undefined) {
            message.labels[entry4.key] = entry4.value;
          }
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.inputNotebookFile = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.containerImageUri = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.outputNotebookFolder = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.paramsYamlFile = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.parameters = reader.string();
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.serviceAccount = reader.string();
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.jobType = reader.int32() as any;
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.dataprocParameters = ExecutionTemplate_DataprocParameters.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.vertexAiParameters = ExecutionTemplate_VertexAIParameters.decode(reader, reader.uint32());
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.kernelSpec = reader.string();
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.tensorboard = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecutionTemplate {
    return {
      scaleTier: isSet(object.scaleTier) ? executionTemplate_ScaleTierFromJSON(object.scaleTier) : 0,
      masterType: isSet(object.masterType) ? globalThis.String(object.masterType) : "",
      acceleratorConfig: isSet(object.acceleratorConfig)
        ? ExecutionTemplate_SchedulerAcceleratorConfig.fromJSON(object.acceleratorConfig)
        : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      inputNotebookFile: isSet(object.inputNotebookFile) ? globalThis.String(object.inputNotebookFile) : "",
      containerImageUri: isSet(object.containerImageUri) ? globalThis.String(object.containerImageUri) : "",
      outputNotebookFolder: isSet(object.outputNotebookFolder) ? globalThis.String(object.outputNotebookFolder) : "",
      paramsYamlFile: isSet(object.paramsYamlFile) ? globalThis.String(object.paramsYamlFile) : "",
      parameters: isSet(object.parameters) ? globalThis.String(object.parameters) : "",
      serviceAccount: isSet(object.serviceAccount) ? globalThis.String(object.serviceAccount) : "",
      jobType: isSet(object.jobType) ? executionTemplate_JobTypeFromJSON(object.jobType) : 0,
      dataprocParameters: isSet(object.dataprocParameters)
        ? ExecutionTemplate_DataprocParameters.fromJSON(object.dataprocParameters)
        : undefined,
      vertexAiParameters: isSet(object.vertexAiParameters)
        ? ExecutionTemplate_VertexAIParameters.fromJSON(object.vertexAiParameters)
        : undefined,
      kernelSpec: isSet(object.kernelSpec) ? globalThis.String(object.kernelSpec) : "",
      tensorboard: isSet(object.tensorboard) ? globalThis.String(object.tensorboard) : "",
    };
  },

  toJSON(message: ExecutionTemplate): unknown {
    const obj: any = {};
    if (message.scaleTier !== 0) {
      obj.scaleTier = executionTemplate_ScaleTierToJSON(message.scaleTier);
    }
    if (message.masterType !== "") {
      obj.masterType = message.masterType;
    }
    if (message.acceleratorConfig !== undefined) {
      obj.acceleratorConfig = ExecutionTemplate_SchedulerAcceleratorConfig.toJSON(message.acceleratorConfig);
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.inputNotebookFile !== "") {
      obj.inputNotebookFile = message.inputNotebookFile;
    }
    if (message.containerImageUri !== "") {
      obj.containerImageUri = message.containerImageUri;
    }
    if (message.outputNotebookFolder !== "") {
      obj.outputNotebookFolder = message.outputNotebookFolder;
    }
    if (message.paramsYamlFile !== "") {
      obj.paramsYamlFile = message.paramsYamlFile;
    }
    if (message.parameters !== "") {
      obj.parameters = message.parameters;
    }
    if (message.serviceAccount !== "") {
      obj.serviceAccount = message.serviceAccount;
    }
    if (message.jobType !== 0) {
      obj.jobType = executionTemplate_JobTypeToJSON(message.jobType);
    }
    if (message.dataprocParameters !== undefined) {
      obj.dataprocParameters = ExecutionTemplate_DataprocParameters.toJSON(message.dataprocParameters);
    }
    if (message.vertexAiParameters !== undefined) {
      obj.vertexAiParameters = ExecutionTemplate_VertexAIParameters.toJSON(message.vertexAiParameters);
    }
    if (message.kernelSpec !== "") {
      obj.kernelSpec = message.kernelSpec;
    }
    if (message.tensorboard !== "") {
      obj.tensorboard = message.tensorboard;
    }
    return obj;
  },

  create(base?: DeepPartial<ExecutionTemplate>): ExecutionTemplate {
    return ExecutionTemplate.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecutionTemplate>): ExecutionTemplate {
    const message = createBaseExecutionTemplate();
    message.scaleTier = object.scaleTier ?? 0;
    message.masterType = object.masterType ?? "";
    message.acceleratorConfig = (object.acceleratorConfig !== undefined && object.acceleratorConfig !== null)
      ? ExecutionTemplate_SchedulerAcceleratorConfig.fromPartial(object.acceleratorConfig)
      : undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.inputNotebookFile = object.inputNotebookFile ?? "";
    message.containerImageUri = object.containerImageUri ?? "";
    message.outputNotebookFolder = object.outputNotebookFolder ?? "";
    message.paramsYamlFile = object.paramsYamlFile ?? "";
    message.parameters = object.parameters ?? "";
    message.serviceAccount = object.serviceAccount ?? "";
    message.jobType = object.jobType ?? 0;
    message.dataprocParameters = (object.dataprocParameters !== undefined && object.dataprocParameters !== null)
      ? ExecutionTemplate_DataprocParameters.fromPartial(object.dataprocParameters)
      : undefined;
    message.vertexAiParameters = (object.vertexAiParameters !== undefined && object.vertexAiParameters !== null)
      ? ExecutionTemplate_VertexAIParameters.fromPartial(object.vertexAiParameters)
      : undefined;
    message.kernelSpec = object.kernelSpec ?? "";
    message.tensorboard = object.tensorboard ?? "";
    return message;
  },
};

function createBaseExecutionTemplate_SchedulerAcceleratorConfig(): ExecutionTemplate_SchedulerAcceleratorConfig {
  return { type: 0, coreCount: Long.ZERO };
}

export const ExecutionTemplate_SchedulerAcceleratorConfig: MessageFns<ExecutionTemplate_SchedulerAcceleratorConfig> = {
  encode(
    message: ExecutionTemplate_SchedulerAcceleratorConfig,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.type !== 0) {
      writer.uint32(8).int32(message.type);
    }
    if (!message.coreCount.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.coreCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecutionTemplate_SchedulerAcceleratorConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecutionTemplate_SchedulerAcceleratorConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.coreCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecutionTemplate_SchedulerAcceleratorConfig {
    return {
      type: isSet(object.type) ? executionTemplate_SchedulerAcceleratorTypeFromJSON(object.type) : 0,
      coreCount: isSet(object.coreCount) ? Long.fromValue(object.coreCount) : Long.ZERO,
    };
  },

  toJSON(message: ExecutionTemplate_SchedulerAcceleratorConfig): unknown {
    const obj: any = {};
    if (message.type !== 0) {
      obj.type = executionTemplate_SchedulerAcceleratorTypeToJSON(message.type);
    }
    if (!message.coreCount.equals(Long.ZERO)) {
      obj.coreCount = (message.coreCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(
    base?: DeepPartial<ExecutionTemplate_SchedulerAcceleratorConfig>,
  ): ExecutionTemplate_SchedulerAcceleratorConfig {
    return ExecutionTemplate_SchedulerAcceleratorConfig.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ExecutionTemplate_SchedulerAcceleratorConfig>,
  ): ExecutionTemplate_SchedulerAcceleratorConfig {
    const message = createBaseExecutionTemplate_SchedulerAcceleratorConfig();
    message.type = object.type ?? 0;
    message.coreCount = (object.coreCount !== undefined && object.coreCount !== null)
      ? Long.fromValue(object.coreCount)
      : Long.ZERO;
    return message;
  },
};

function createBaseExecutionTemplate_DataprocParameters(): ExecutionTemplate_DataprocParameters {
  return { cluster: "" };
}

export const ExecutionTemplate_DataprocParameters: MessageFns<ExecutionTemplate_DataprocParameters> = {
  encode(message: ExecutionTemplate_DataprocParameters, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.cluster !== "") {
      writer.uint32(10).string(message.cluster);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecutionTemplate_DataprocParameters {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecutionTemplate_DataprocParameters();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.cluster = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecutionTemplate_DataprocParameters {
    return { cluster: isSet(object.cluster) ? globalThis.String(object.cluster) : "" };
  },

  toJSON(message: ExecutionTemplate_DataprocParameters): unknown {
    const obj: any = {};
    if (message.cluster !== "") {
      obj.cluster = message.cluster;
    }
    return obj;
  },

  create(base?: DeepPartial<ExecutionTemplate_DataprocParameters>): ExecutionTemplate_DataprocParameters {
    return ExecutionTemplate_DataprocParameters.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecutionTemplate_DataprocParameters>): ExecutionTemplate_DataprocParameters {
    const message = createBaseExecutionTemplate_DataprocParameters();
    message.cluster = object.cluster ?? "";
    return message;
  },
};

function createBaseExecutionTemplate_VertexAIParameters(): ExecutionTemplate_VertexAIParameters {
  return { network: "", env: {} };
}

export const ExecutionTemplate_VertexAIParameters: MessageFns<ExecutionTemplate_VertexAIParameters> = {
  encode(message: ExecutionTemplate_VertexAIParameters, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.network !== "") {
      writer.uint32(10).string(message.network);
    }
    Object.entries(message.env).forEach(([key, value]) => {
      ExecutionTemplate_VertexAIParameters_EnvEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecutionTemplate_VertexAIParameters {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecutionTemplate_VertexAIParameters();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.network = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = ExecutionTemplate_VertexAIParameters_EnvEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.env[entry2.key] = entry2.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecutionTemplate_VertexAIParameters {
    return {
      network: isSet(object.network) ? globalThis.String(object.network) : "",
      env: isObject(object.env)
        ? Object.entries(object.env).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: ExecutionTemplate_VertexAIParameters): unknown {
    const obj: any = {};
    if (message.network !== "") {
      obj.network = message.network;
    }
    if (message.env) {
      const entries = Object.entries(message.env);
      if (entries.length > 0) {
        obj.env = {};
        entries.forEach(([k, v]) => {
          obj.env[k] = v;
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<ExecutionTemplate_VertexAIParameters>): ExecutionTemplate_VertexAIParameters {
    return ExecutionTemplate_VertexAIParameters.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecutionTemplate_VertexAIParameters>): ExecutionTemplate_VertexAIParameters {
    const message = createBaseExecutionTemplate_VertexAIParameters();
    message.network = object.network ?? "";
    message.env = Object.entries(object.env ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseExecutionTemplate_VertexAIParameters_EnvEntry(): ExecutionTemplate_VertexAIParameters_EnvEntry {
  return { key: "", value: "" };
}

export const ExecutionTemplate_VertexAIParameters_EnvEntry: MessageFns<ExecutionTemplate_VertexAIParameters_EnvEntry> =
  {
    encode(
      message: ExecutionTemplate_VertexAIParameters_EnvEntry,
      writer: BinaryWriter = new BinaryWriter(),
    ): BinaryWriter {
      if (message.key !== "") {
        writer.uint32(10).string(message.key);
      }
      if (message.value !== "") {
        writer.uint32(18).string(message.value);
      }
      return writer;
    },

    decode(input: BinaryReader | Uint8Array, length?: number): ExecutionTemplate_VertexAIParameters_EnvEntry {
      const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
      let end = length === undefined ? reader.len : reader.pos + length;
      const message = createBaseExecutionTemplate_VertexAIParameters_EnvEntry();
      while (reader.pos < end) {
        const tag = reader.uint32();
        switch (tag >>> 3) {
          case 1:
            if (tag !== 10) {
              break;
            }

            message.key = reader.string();
            continue;
          case 2:
            if (tag !== 18) {
              break;
            }

            message.value = reader.string();
            continue;
        }
        if ((tag & 7) === 4 || tag === 0) {
          break;
        }
        reader.skip(tag & 7);
      }
      return message;
    },

    fromJSON(object: any): ExecutionTemplate_VertexAIParameters_EnvEntry {
      return {
        key: isSet(object.key) ? globalThis.String(object.key) : "",
        value: isSet(object.value) ? globalThis.String(object.value) : "",
      };
    },

    toJSON(message: ExecutionTemplate_VertexAIParameters_EnvEntry): unknown {
      const obj: any = {};
      if (message.key !== "") {
        obj.key = message.key;
      }
      if (message.value !== "") {
        obj.value = message.value;
      }
      return obj;
    },

    create(
      base?: DeepPartial<ExecutionTemplate_VertexAIParameters_EnvEntry>,
    ): ExecutionTemplate_VertexAIParameters_EnvEntry {
      return ExecutionTemplate_VertexAIParameters_EnvEntry.fromPartial(base ?? {});
    },
    fromPartial(
      object: DeepPartial<ExecutionTemplate_VertexAIParameters_EnvEntry>,
    ): ExecutionTemplate_VertexAIParameters_EnvEntry {
      const message = createBaseExecutionTemplate_VertexAIParameters_EnvEntry();
      message.key = object.key ?? "";
      message.value = object.value ?? "";
      return message;
    },
  };

function createBaseExecutionTemplate_LabelsEntry(): ExecutionTemplate_LabelsEntry {
  return { key: "", value: "" };
}

export const ExecutionTemplate_LabelsEntry: MessageFns<ExecutionTemplate_LabelsEntry> = {
  encode(message: ExecutionTemplate_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecutionTemplate_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecutionTemplate_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecutionTemplate_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: ExecutionTemplate_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<ExecutionTemplate_LabelsEntry>): ExecutionTemplate_LabelsEntry {
    return ExecutionTemplate_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecutionTemplate_LabelsEntry>): ExecutionTemplate_LabelsEntry {
    const message = createBaseExecutionTemplate_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseExecution(): Execution {
  return {
    executionTemplate: undefined,
    name: "",
    displayName: "",
    description: "",
    createTime: undefined,
    updateTime: undefined,
    state: 0,
    outputNotebookFile: "",
    jobUri: "",
  };
}

export const Execution: MessageFns<Execution> = {
  encode(message: Execution, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.executionTemplate !== undefined) {
      ExecutionTemplate.encode(message.executionTemplate, writer.uint32(10).fork()).join();
    }
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    if (message.displayName !== "") {
      writer.uint32(26).string(message.displayName);
    }
    if (message.description !== "") {
      writer.uint32(34).string(message.description);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(42).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(50).fork()).join();
    }
    if (message.state !== 0) {
      writer.uint32(56).int32(message.state);
    }
    if (message.outputNotebookFile !== "") {
      writer.uint32(66).string(message.outputNotebookFile);
    }
    if (message.jobUri !== "") {
      writer.uint32(74).string(message.jobUri);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Execution {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecution();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.executionTemplate = ExecutionTemplate.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.description = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.outputNotebookFile = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.jobUri = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Execution {
    return {
      executionTemplate: isSet(object.executionTemplate)
        ? ExecutionTemplate.fromJSON(object.executionTemplate)
        : undefined,
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      state: isSet(object.state) ? execution_StateFromJSON(object.state) : 0,
      outputNotebookFile: isSet(object.outputNotebookFile) ? globalThis.String(object.outputNotebookFile) : "",
      jobUri: isSet(object.jobUri) ? globalThis.String(object.jobUri) : "",
    };
  },

  toJSON(message: Execution): unknown {
    const obj: any = {};
    if (message.executionTemplate !== undefined) {
      obj.executionTemplate = ExecutionTemplate.toJSON(message.executionTemplate);
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (message.state !== 0) {
      obj.state = execution_StateToJSON(message.state);
    }
    if (message.outputNotebookFile !== "") {
      obj.outputNotebookFile = message.outputNotebookFile;
    }
    if (message.jobUri !== "") {
      obj.jobUri = message.jobUri;
    }
    return obj;
  },

  create(base?: DeepPartial<Execution>): Execution {
    return Execution.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Execution>): Execution {
    const message = createBaseExecution();
    message.executionTemplate = (object.executionTemplate !== undefined && object.executionTemplate !== null)
      ? ExecutionTemplate.fromPartial(object.executionTemplate)
      : undefined;
    message.name = object.name ?? "";
    message.displayName = object.displayName ?? "";
    message.description = object.description ?? "";
    message.createTime = object.createTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.state = object.state ?? 0;
    message.outputNotebookFile = object.outputNotebookFile ?? "";
    message.jobUri = object.jobUri ?? "";
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
