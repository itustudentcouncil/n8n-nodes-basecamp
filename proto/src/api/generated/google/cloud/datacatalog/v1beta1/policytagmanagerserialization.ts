// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/datacatalog/v1beta1/policytagmanagerserialization.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import {
  Taxonomy,
  Taxonomy_PolicyType,
  taxonomy_PolicyTypeFromJSON,
  taxonomy_PolicyTypeToJSON,
} from "./policytagmanager.js";

export const protobufPackage = "google.cloud.datacatalog.v1beta1";

/**
 * Message capturing a taxonomy and its policy tag hierarchy as a nested proto.
 * Used for taxonomy import/export and mutation.
 */
export interface SerializedTaxonomy {
  /**
   * Required. Display name of the taxonomy. Max 200 bytes when encoded in
   * UTF-8.
   */
  displayName: string;
  /**
   * Description of the serialized taxonomy. The length of the
   * description is limited to 2000 bytes when encoded in UTF-8. If not set,
   * defaults to an empty description.
   */
  description: string;
  /** Top level policy tags associated with the taxonomy if any. */
  policyTags: SerializedPolicyTag[];
  /** A list of policy types that are activated for a taxonomy. */
  activatedPolicyTypes: Taxonomy_PolicyType[];
}

/** Message representing one policy tag when exported as a nested proto. */
export interface SerializedPolicyTag {
  /**
   * Resource name of the policy tag.
   *
   * This field will be ignored when calling ImportTaxonomies.
   */
  policyTag: string;
  /**
   * Required. Display name of the policy tag. Max 200 bytes when encoded in
   * UTF-8.
   */
  displayName: string;
  /**
   * Description of the serialized policy tag. The length of the
   * description is limited to 2000 bytes when encoded in UTF-8. If not set,
   * defaults to an empty description.
   */
  description: string;
  /** Children of the policy tag if any. */
  childPolicyTags: SerializedPolicyTag[];
}

/**
 * Request message for
 * [ImportTaxonomies][google.cloud.datacatalog.v1beta1.PolicyTagManagerSerialization.ImportTaxonomies].
 */
export interface ImportTaxonomiesRequest {
  /**
   * Required. Resource name of project that the imported taxonomies will belong
   * to.
   */
  parent: string;
  /** Inline source used for taxonomies to be imported. */
  inlineSource?: InlineSource | undefined;
}

/** Inline source used for taxonomies import. */
export interface InlineSource {
  /** Required. Taxonomies to be imported. */
  taxonomies: SerializedTaxonomy[];
}

/**
 * Response message for
 * [ImportTaxonomies][google.cloud.datacatalog.v1beta1.PolicyTagManagerSerialization.ImportTaxonomies].
 */
export interface ImportTaxonomiesResponse {
  /** Taxonomies that were imported. */
  taxonomies: Taxonomy[];
}

/**
 * Request message for
 * [ExportTaxonomies][google.cloud.datacatalog.v1beta1.PolicyTagManagerSerialization.ExportTaxonomies].
 */
export interface ExportTaxonomiesRequest {
  /**
   * Required. Resource name of the project that taxonomies to be exported
   * will share.
   */
  parent: string;
  /** Required. Resource names of the taxonomies to be exported. */
  taxonomies: string[];
  /** Export taxonomies as serialized taxonomies. */
  serializedTaxonomies?: boolean | undefined;
}

/**
 * Response message for
 * [ExportTaxonomies][google.cloud.datacatalog.v1beta1.PolicyTagManagerSerialization.ExportTaxonomies].
 */
export interface ExportTaxonomiesResponse {
  /** List of taxonomies and policy tags in a tree structure. */
  taxonomies: SerializedTaxonomy[];
}

function createBaseSerializedTaxonomy(): SerializedTaxonomy {
  return { displayName: "", description: "", policyTags: [], activatedPolicyTypes: [] };
}

export const SerializedTaxonomy: MessageFns<SerializedTaxonomy> = {
  encode(message: SerializedTaxonomy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.displayName !== "") {
      writer.uint32(10).string(message.displayName);
    }
    if (message.description !== "") {
      writer.uint32(18).string(message.description);
    }
    for (const v of message.policyTags) {
      SerializedPolicyTag.encode(v!, writer.uint32(26).fork()).join();
    }
    writer.uint32(34).fork();
    for (const v of message.activatedPolicyTypes) {
      writer.int32(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SerializedTaxonomy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSerializedTaxonomy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.description = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.policyTags.push(SerializedPolicyTag.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag === 32) {
            message.activatedPolicyTypes.push(reader.int32() as any);

            continue;
          }

          if (tag === 34) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.activatedPolicyTypes.push(reader.int32() as any);
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SerializedTaxonomy {
    return {
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      policyTags: globalThis.Array.isArray(object?.policyTags)
        ? object.policyTags.map((e: any) => SerializedPolicyTag.fromJSON(e))
        : [],
      activatedPolicyTypes: globalThis.Array.isArray(object?.activatedPolicyTypes)
        ? object.activatedPolicyTypes.map((e: any) => taxonomy_PolicyTypeFromJSON(e))
        : [],
    };
  },

  toJSON(message: SerializedTaxonomy): unknown {
    const obj: any = {};
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.policyTags?.length) {
      obj.policyTags = message.policyTags.map((e) => SerializedPolicyTag.toJSON(e));
    }
    if (message.activatedPolicyTypes?.length) {
      obj.activatedPolicyTypes = message.activatedPolicyTypes.map((e) => taxonomy_PolicyTypeToJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<SerializedTaxonomy>): SerializedTaxonomy {
    return SerializedTaxonomy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SerializedTaxonomy>): SerializedTaxonomy {
    const message = createBaseSerializedTaxonomy();
    message.displayName = object.displayName ?? "";
    message.description = object.description ?? "";
    message.policyTags = object.policyTags?.map((e) => SerializedPolicyTag.fromPartial(e)) || [];
    message.activatedPolicyTypes = object.activatedPolicyTypes?.map((e) => e) || [];
    return message;
  },
};

function createBaseSerializedPolicyTag(): SerializedPolicyTag {
  return { policyTag: "", displayName: "", description: "", childPolicyTags: [] };
}

export const SerializedPolicyTag: MessageFns<SerializedPolicyTag> = {
  encode(message: SerializedPolicyTag, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.policyTag !== "") {
      writer.uint32(10).string(message.policyTag);
    }
    if (message.displayName !== "") {
      writer.uint32(18).string(message.displayName);
    }
    if (message.description !== "") {
      writer.uint32(26).string(message.description);
    }
    for (const v of message.childPolicyTags) {
      SerializedPolicyTag.encode(v!, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SerializedPolicyTag {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSerializedPolicyTag();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.policyTag = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.description = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.childPolicyTags.push(SerializedPolicyTag.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SerializedPolicyTag {
    return {
      policyTag: isSet(object.policyTag) ? globalThis.String(object.policyTag) : "",
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      childPolicyTags: globalThis.Array.isArray(object?.childPolicyTags)
        ? object.childPolicyTags.map((e: any) => SerializedPolicyTag.fromJSON(e))
        : [],
    };
  },

  toJSON(message: SerializedPolicyTag): unknown {
    const obj: any = {};
    if (message.policyTag !== "") {
      obj.policyTag = message.policyTag;
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.childPolicyTags?.length) {
      obj.childPolicyTags = message.childPolicyTags.map((e) => SerializedPolicyTag.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<SerializedPolicyTag>): SerializedPolicyTag {
    return SerializedPolicyTag.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SerializedPolicyTag>): SerializedPolicyTag {
    const message = createBaseSerializedPolicyTag();
    message.policyTag = object.policyTag ?? "";
    message.displayName = object.displayName ?? "";
    message.description = object.description ?? "";
    message.childPolicyTags = object.childPolicyTags?.map((e) => SerializedPolicyTag.fromPartial(e)) || [];
    return message;
  },
};

function createBaseImportTaxonomiesRequest(): ImportTaxonomiesRequest {
  return { parent: "", inlineSource: undefined };
}

export const ImportTaxonomiesRequest: MessageFns<ImportTaxonomiesRequest> = {
  encode(message: ImportTaxonomiesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.inlineSource !== undefined) {
      InlineSource.encode(message.inlineSource, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportTaxonomiesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportTaxonomiesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inlineSource = InlineSource.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportTaxonomiesRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      inlineSource: isSet(object.inlineSource) ? InlineSource.fromJSON(object.inlineSource) : undefined,
    };
  },

  toJSON(message: ImportTaxonomiesRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.inlineSource !== undefined) {
      obj.inlineSource = InlineSource.toJSON(message.inlineSource);
    }
    return obj;
  },

  create(base?: DeepPartial<ImportTaxonomiesRequest>): ImportTaxonomiesRequest {
    return ImportTaxonomiesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportTaxonomiesRequest>): ImportTaxonomiesRequest {
    const message = createBaseImportTaxonomiesRequest();
    message.parent = object.parent ?? "";
    message.inlineSource = (object.inlineSource !== undefined && object.inlineSource !== null)
      ? InlineSource.fromPartial(object.inlineSource)
      : undefined;
    return message;
  },
};

function createBaseInlineSource(): InlineSource {
  return { taxonomies: [] };
}

export const InlineSource: MessageFns<InlineSource> = {
  encode(message: InlineSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.taxonomies) {
      SerializedTaxonomy.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InlineSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInlineSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.taxonomies.push(SerializedTaxonomy.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InlineSource {
    return {
      taxonomies: globalThis.Array.isArray(object?.taxonomies)
        ? object.taxonomies.map((e: any) => SerializedTaxonomy.fromJSON(e))
        : [],
    };
  },

  toJSON(message: InlineSource): unknown {
    const obj: any = {};
    if (message.taxonomies?.length) {
      obj.taxonomies = message.taxonomies.map((e) => SerializedTaxonomy.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<InlineSource>): InlineSource {
    return InlineSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InlineSource>): InlineSource {
    const message = createBaseInlineSource();
    message.taxonomies = object.taxonomies?.map((e) => SerializedTaxonomy.fromPartial(e)) || [];
    return message;
  },
};

function createBaseImportTaxonomiesResponse(): ImportTaxonomiesResponse {
  return { taxonomies: [] };
}

export const ImportTaxonomiesResponse: MessageFns<ImportTaxonomiesResponse> = {
  encode(message: ImportTaxonomiesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.taxonomies) {
      Taxonomy.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportTaxonomiesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportTaxonomiesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.taxonomies.push(Taxonomy.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportTaxonomiesResponse {
    return {
      taxonomies: globalThis.Array.isArray(object?.taxonomies)
        ? object.taxonomies.map((e: any) => Taxonomy.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ImportTaxonomiesResponse): unknown {
    const obj: any = {};
    if (message.taxonomies?.length) {
      obj.taxonomies = message.taxonomies.map((e) => Taxonomy.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ImportTaxonomiesResponse>): ImportTaxonomiesResponse {
    return ImportTaxonomiesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportTaxonomiesResponse>): ImportTaxonomiesResponse {
    const message = createBaseImportTaxonomiesResponse();
    message.taxonomies = object.taxonomies?.map((e) => Taxonomy.fromPartial(e)) || [];
    return message;
  },
};

function createBaseExportTaxonomiesRequest(): ExportTaxonomiesRequest {
  return { parent: "", taxonomies: [], serializedTaxonomies: undefined };
}

export const ExportTaxonomiesRequest: MessageFns<ExportTaxonomiesRequest> = {
  encode(message: ExportTaxonomiesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    for (const v of message.taxonomies) {
      writer.uint32(18).string(v!);
    }
    if (message.serializedTaxonomies !== undefined) {
      writer.uint32(24).bool(message.serializedTaxonomies);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportTaxonomiesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportTaxonomiesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.taxonomies.push(reader.string());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.serializedTaxonomies = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportTaxonomiesRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      taxonomies: globalThis.Array.isArray(object?.taxonomies)
        ? object.taxonomies.map((e: any) => globalThis.String(e))
        : [],
      serializedTaxonomies: isSet(object.serializedTaxonomies)
        ? globalThis.Boolean(object.serializedTaxonomies)
        : undefined,
    };
  },

  toJSON(message: ExportTaxonomiesRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.taxonomies?.length) {
      obj.taxonomies = message.taxonomies;
    }
    if (message.serializedTaxonomies !== undefined) {
      obj.serializedTaxonomies = message.serializedTaxonomies;
    }
    return obj;
  },

  create(base?: DeepPartial<ExportTaxonomiesRequest>): ExportTaxonomiesRequest {
    return ExportTaxonomiesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportTaxonomiesRequest>): ExportTaxonomiesRequest {
    const message = createBaseExportTaxonomiesRequest();
    message.parent = object.parent ?? "";
    message.taxonomies = object.taxonomies?.map((e) => e) || [];
    message.serializedTaxonomies = object.serializedTaxonomies ?? undefined;
    return message;
  },
};

function createBaseExportTaxonomiesResponse(): ExportTaxonomiesResponse {
  return { taxonomies: [] };
}

export const ExportTaxonomiesResponse: MessageFns<ExportTaxonomiesResponse> = {
  encode(message: ExportTaxonomiesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.taxonomies) {
      SerializedTaxonomy.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportTaxonomiesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportTaxonomiesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.taxonomies.push(SerializedTaxonomy.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportTaxonomiesResponse {
    return {
      taxonomies: globalThis.Array.isArray(object?.taxonomies)
        ? object.taxonomies.map((e: any) => SerializedTaxonomy.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ExportTaxonomiesResponse): unknown {
    const obj: any = {};
    if (message.taxonomies?.length) {
      obj.taxonomies = message.taxonomies.map((e) => SerializedTaxonomy.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ExportTaxonomiesResponse>): ExportTaxonomiesResponse {
    return ExportTaxonomiesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportTaxonomiesResponse>): ExportTaxonomiesResponse {
    const message = createBaseExportTaxonomiesResponse();
    message.taxonomies = object.taxonomies?.map((e) => SerializedTaxonomy.fromPartial(e)) || [];
    return message;
  },
};

/**
 * Policy tag manager serialization API service allows clients to manipulate
 * their taxonomies and policy tags data with serialized format.
 */
export type PolicyTagManagerSerializationDefinition = typeof PolicyTagManagerSerializationDefinition;
export const PolicyTagManagerSerializationDefinition = {
  name: "PolicyTagManagerSerialization",
  fullName: "google.cloud.datacatalog.v1beta1.PolicyTagManagerSerialization",
  methods: {
    /**
     * Imports all taxonomies and their policy tags to a project as new
     * taxonomies.
     *
     * This method provides a bulk taxonomy / policy tag creation using nested
     * proto structure.
     */
    importTaxonomies: {
      name: "ImportTaxonomies",
      requestType: ImportTaxonomiesRequest,
      requestStream: false,
      responseType: ImportTaxonomiesResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              63,
              58,
              1,
              42,
              34,
              58,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              47,
              116,
              97,
              120,
              111,
              110,
              111,
              109,
              105,
              101,
              115,
              58,
              105,
              109,
              112,
              111,
              114,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * Exports all taxonomies and their policy tags in a project.
     *
     * This method generates SerializedTaxonomy protos with nested policy tags
     * that can be used as an input for future ImportTaxonomies calls.
     */
    exportTaxonomies: {
      name: "ExportTaxonomies",
      requestType: ExportTaxonomiesRequest,
      requestStream: false,
      responseType: ExportTaxonomiesResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              60,
              18,
              58,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              47,
              116,
              97,
              120,
              111,
              110,
              111,
              109,
              105,
              101,
              115,
              58,
              101,
              120,
              112,
              111,
              114,
              116,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface PolicyTagManagerSerializationServiceImplementation<CallContextExt = {}> {
  /**
   * Imports all taxonomies and their policy tags to a project as new
   * taxonomies.
   *
   * This method provides a bulk taxonomy / policy tag creation using nested
   * proto structure.
   */
  importTaxonomies(
    request: ImportTaxonomiesRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ImportTaxonomiesResponse>>;
  /**
   * Exports all taxonomies and their policy tags in a project.
   *
   * This method generates SerializedTaxonomy protos with nested policy tags
   * that can be used as an input for future ImportTaxonomies calls.
   */
  exportTaxonomies(
    request: ExportTaxonomiesRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ExportTaxonomiesResponse>>;
}

export interface PolicyTagManagerSerializationClient<CallOptionsExt = {}> {
  /**
   * Imports all taxonomies and their policy tags to a project as new
   * taxonomies.
   *
   * This method provides a bulk taxonomy / policy tag creation using nested
   * proto structure.
   */
  importTaxonomies(
    request: DeepPartial<ImportTaxonomiesRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ImportTaxonomiesResponse>;
  /**
   * Exports all taxonomies and their policy tags in a project.
   *
   * This method generates SerializedTaxonomy protos with nested policy tags
   * that can be used as an input for future ImportTaxonomies calls.
   */
  exportTaxonomies(
    request: DeepPartial<ExportTaxonomiesRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ExportTaxonomiesResponse>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
