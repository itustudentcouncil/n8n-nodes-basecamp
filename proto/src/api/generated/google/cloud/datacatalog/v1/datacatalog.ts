// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/datacatalog/v1/datacatalog.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import {
  GetIamPolicyRequest,
  SetIamPolicyRequest,
  TestIamPermissionsRequest,
  TestIamPermissionsResponse,
} from "../../../iam/v1/iam_policy.js";
import { Policy } from "../../../iam/v1/policy.js";
import { Operation } from "../../../longrunning/operations.js";
import { Empty } from "../../../protobuf/empty.js";
import { FieldMask } from "../../../protobuf/field_mask.js";
import { Status } from "../../../rpc/status.js";
import { BigQueryConnectionSpec, BigQueryRoutineSpec } from "./bigquery.js";
import { IntegratedSystem, integratedSystemFromJSON, integratedSystemToJSON, PersonalDetails } from "./common.js";
import { DataSource } from "./data_source.js";
import { DataplexFilesetSpec, DataplexTableSpec } from "./dataplex_spec.js";
import { GcsFilesetSpec } from "./gcs_fileset_spec.js";
import { Schema } from "./schema.js";
import { SearchCatalogResult } from "./search.js";
import { BigQueryDateShardedSpec, BigQueryTableSpec } from "./table_spec.js";
import { Tag, TagTemplate, TagTemplateField } from "./tags.js";
import { SystemTimestamps } from "./timestamps.js";
import { UsageSignal } from "./usage.js";

export const protobufPackage = "google.cloud.datacatalog.v1";

/**
 * Metadata automatically ingested from Google Cloud resources like BigQuery
 * tables or Pub/Sub topics always uses enum values from `EntryType` as the type
 * of entry.
 *
 * Other sources of metadata like Hive or Oracle databases can identify the type
 * by either using one of the enum values from `EntryType` (for example,
 * `FILESET` for a Cloud Storage fileset) or specifying a custom value using
 * the [`Entry`](#resource:-entry) field `user_specified_type`. For more
 * information, see
 * [Surface files from Cloud Storage with fileset
 * entries](/data-catalog/docs/how-to/filesets) or [Create custom entries for
 * your data sources](/data-catalog/docs/how-to/custom-entries).
 */
export enum EntryType {
  /** ENTRY_TYPE_UNSPECIFIED - Default unknown type. */
  ENTRY_TYPE_UNSPECIFIED = 0,
  /**
   * TABLE - The entry type that has a GoogleSQL schema, including
   * logical views.
   */
  TABLE = 2,
  /**
   * MODEL - The type of models.
   *
   * For more information, see [Supported models in BigQuery
   * ML](/bigquery/docs/bqml-introduction#supported_models).
   */
  MODEL = 5,
  /** DATA_STREAM - An entry type for streaming entries. For example, a Pub/Sub topic. */
  DATA_STREAM = 3,
  /**
   * FILESET - An entry type for a set of files or objects. For example, a
   * Cloud Storage fileset.
   */
  FILESET = 4,
  /** CLUSTER - A group of servers that work together. For example, a Kafka cluster. */
  CLUSTER = 6,
  /** DATABASE - A database. */
  DATABASE = 7,
  /**
   * DATA_SOURCE_CONNECTION - Connection to a data source. For example, a BigQuery
   * connection.
   */
  DATA_SOURCE_CONNECTION = 8,
  /** ROUTINE - Routine, for example, a BigQuery routine. */
  ROUTINE = 9,
  /** LAKE - A Dataplex lake. */
  LAKE = 10,
  /** ZONE - A Dataplex zone. */
  ZONE = 11,
  /** SERVICE - A service, for example, a Dataproc Metastore service. */
  SERVICE = 14,
  /** DATABASE_SCHEMA - Schema within a relational database. */
  DATABASE_SCHEMA = 15,
  /** DASHBOARD - A Dashboard, for example from Looker. */
  DASHBOARD = 16,
  /**
   * EXPLORE - A Looker Explore.
   *
   * For more information, see [Looker Explore API]
   * (https://developers.looker.com/api/explorer/4.0/methods/LookmlModel/lookml_model_explore).
   */
  EXPLORE = 17,
  /**
   * LOOK - A Looker Look.
   *
   * For more information, see [Looker Look API]
   * (https://developers.looker.com/api/explorer/4.0/methods/Look).
   */
  LOOK = 18,
  UNRECOGNIZED = -1,
}

export function entryTypeFromJSON(object: any): EntryType {
  switch (object) {
    case 0:
    case "ENTRY_TYPE_UNSPECIFIED":
      return EntryType.ENTRY_TYPE_UNSPECIFIED;
    case 2:
    case "TABLE":
      return EntryType.TABLE;
    case 5:
    case "MODEL":
      return EntryType.MODEL;
    case 3:
    case "DATA_STREAM":
      return EntryType.DATA_STREAM;
    case 4:
    case "FILESET":
      return EntryType.FILESET;
    case 6:
    case "CLUSTER":
      return EntryType.CLUSTER;
    case 7:
    case "DATABASE":
      return EntryType.DATABASE;
    case 8:
    case "DATA_SOURCE_CONNECTION":
      return EntryType.DATA_SOURCE_CONNECTION;
    case 9:
    case "ROUTINE":
      return EntryType.ROUTINE;
    case 10:
    case "LAKE":
      return EntryType.LAKE;
    case 11:
    case "ZONE":
      return EntryType.ZONE;
    case 14:
    case "SERVICE":
      return EntryType.SERVICE;
    case 15:
    case "DATABASE_SCHEMA":
      return EntryType.DATABASE_SCHEMA;
    case 16:
    case "DASHBOARD":
      return EntryType.DASHBOARD;
    case 17:
    case "EXPLORE":
      return EntryType.EXPLORE;
    case 18:
    case "LOOK":
      return EntryType.LOOK;
    case -1:
    case "UNRECOGNIZED":
    default:
      return EntryType.UNRECOGNIZED;
  }
}

export function entryTypeToJSON(object: EntryType): string {
  switch (object) {
    case EntryType.ENTRY_TYPE_UNSPECIFIED:
      return "ENTRY_TYPE_UNSPECIFIED";
    case EntryType.TABLE:
      return "TABLE";
    case EntryType.MODEL:
      return "MODEL";
    case EntryType.DATA_STREAM:
      return "DATA_STREAM";
    case EntryType.FILESET:
      return "FILESET";
    case EntryType.CLUSTER:
      return "CLUSTER";
    case EntryType.DATABASE:
      return "DATABASE";
    case EntryType.DATA_SOURCE_CONNECTION:
      return "DATA_SOURCE_CONNECTION";
    case EntryType.ROUTINE:
      return "ROUTINE";
    case EntryType.LAKE:
      return "LAKE";
    case EntryType.ZONE:
      return "ZONE";
    case EntryType.SERVICE:
      return "SERVICE";
    case EntryType.DATABASE_SCHEMA:
      return "DATABASE_SCHEMA";
    case EntryType.DASHBOARD:
      return "DASHBOARD";
    case EntryType.EXPLORE:
      return "EXPLORE";
    case EntryType.LOOK:
      return "LOOK";
    case EntryType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Request message for
 * [SearchCatalog][google.cloud.datacatalog.v1.DataCatalog.SearchCatalog].
 */
export interface SearchCatalogRequest {
  /**
   * Required. The scope of this search request.
   *
   * The `scope` is invalid if `include_org_ids`, `include_project_ids` are
   * empty AND `include_gcp_public_datasets` is set to `false`. In this case,
   * the request returns an error.
   */
  scope:
    | SearchCatalogRequest_Scope
    | undefined;
  /**
   * Optional. The query string with a minimum of 3 characters and specific
   * syntax. For more information, see [Data Catalog search
   * syntax](https://cloud.google.com/data-catalog/docs/how-to/search-reference).
   *
   * An empty query string returns all data assets (in the specified scope)
   * that you have access to.
   *
   * A query string can be a simple `xyz` or qualified by predicates:
   *
   * * `name:x`
   * * `column:y`
   * * `description:z`
   */
  query: string;
  /**
   * Upper bound on the number of results you can get in a single response.
   *
   * Can't be negative or 0, defaults to 10 in this case.
   * The maximum number is 1000. If exceeded, throws an "invalid argument"
   * exception.
   */
  pageSize: number;
  /**
   * Optional. Pagination token that, if specified, returns the next page of
   * search results. If empty, returns the first page.
   *
   * This token is returned in the
   * [SearchCatalogResponse.next_page_token][google.cloud.datacatalog.v1.SearchCatalogResponse.next_page_token]
   * field of the response to a previous
   * [SearchCatalogRequest][google.cloud.datacatalog.v1.DataCatalog.SearchCatalog]
   * call.
   */
  pageToken: string;
  /**
   * Specifies the order of results.
   *
   * Currently supported case-sensitive values are:
   *
   * * `relevance` that can only be descending
   * * `last_modified_timestamp [asc|desc]` with descending (`desc`) as default
   * * `default` that can only be descending
   *
   * Search queries don't guarantee full recall. Results that match your query
   * might not be returned, even in subsequent result pages. Additionally,
   * returned (and not returned) results can vary if you repeat search queries.
   * If you are experiencing recall issues and you don't have to fetch the
   * results in any specific order, consider setting this parameter to
   * `default`.
   *
   * If this parameter is omitted, it defaults to the descending `relevance`.
   */
  orderBy: string;
  /**
   * Optional. If set, use searchAll permission granted on organizations from
   * `include_org_ids` and projects from `include_project_ids` instead of the
   * fine grained per resource permissions when filtering the search results.
   * The only allowed `order_by` criteria for admin_search mode is `default`.
   * Using this flags guarantees a full recall of the search results.
   */
  adminSearch: boolean;
}

/** The criteria that select the subspace used for query matching. */
export interface SearchCatalogRequest_Scope {
  /**
   * The list of organization IDs to search within.
   *
   * To find your organization ID, follow the steps from
   * [Creating and managing organizations]
   * (/resource-manager/docs/creating-managing-organization).
   */
  includeOrgIds: string[];
  /**
   * The list of project IDs to search within.
   *
   * For more information on the distinction between project names, IDs, and
   * numbers, see [Projects](/docs/overview/#projects).
   */
  includeProjectIds: string[];
  /**
   * If `true`, include Google Cloud public datasets in
   * search results. By default, they are excluded.
   *
   * See [Google Cloud Public Datasets](/public-datasets) for more
   * information.
   */
  includeGcpPublicDatasets: boolean;
  /**
   * Optional. The list of locations to search within. If empty, all locations
   * are searched.
   *
   * Returns an error if any location in the list isn't one of the [Supported
   * regions](https://cloud.google.com/data-catalog/docs/concepts/regions#supported_regions).
   *
   * If a location is unreachable, its name is returned in the
   * `SearchCatalogResponse.unreachable` field. To get additional information
   * on the error, repeat the search request and set the location name as the
   * value of this parameter.
   */
  restrictedLocations: string[];
  /**
   * Optional. If `true`, search only among starred entries.
   *
   * By default, all results are returned, starred or not.
   */
  starredOnly: boolean;
  /**
   * Optional. This field is deprecated. The search mechanism for public and
   * private tag templates is the same.
   *
   * @deprecated
   */
  includePublicTagTemplates: boolean;
}

/**
 * Response message for
 * [SearchCatalog][google.cloud.datacatalog.v1.DataCatalog.SearchCatalog].
 */
export interface SearchCatalogResponse {
  /** Search results. */
  results: SearchCatalogResult[];
  /** The approximate total number of entries matched by the query. */
  totalSize: number;
  /**
   * Pagination token that can be used in subsequent calls to retrieve the next
   * page of results.
   */
  nextPageToken: string;
  /**
   * Unreachable locations. Search results don't include data from those
   * locations.
   *
   * To get additional information on an error, repeat the search request and
   * restrict it to specific locations by setting the
   * `SearchCatalogRequest.scope.restricted_locations` parameter.
   */
  unreachable: string[];
}

/**
 * Request message for
 * [CreateEntryGroup][google.cloud.datacatalog.v1.DataCatalog.CreateEntryGroup].
 */
export interface CreateEntryGroupRequest {
  /**
   * Required. The names of the project and location that the new entry group
   * belongs to.
   *
   * Note: The entry group itself and its child resources might not be
   * stored in the location specified in its name.
   */
  parent: string;
  /**
   * Required. The ID of the entry group to create.
   *
   * The ID must contain only letters (a-z, A-Z), numbers (0-9),
   * underscores (_), and must start with a letter or underscore.
   * The maximum size is 64 bytes when encoded in UTF-8.
   */
  entryGroupId: string;
  /** The entry group to create. Defaults to empty. */
  entryGroup: EntryGroup | undefined;
}

/**
 * Request message for
 * [UpdateEntryGroup][google.cloud.datacatalog.v1.DataCatalog.UpdateEntryGroup].
 */
export interface UpdateEntryGroupRequest {
  /** Required. Updates for the entry group. The `name` field must be set. */
  entryGroup:
    | EntryGroup
    | undefined;
  /**
   * Names of fields whose values to overwrite on an entry group.
   *
   * If this parameter is absent or empty, all modifiable fields
   * are overwritten. If such fields are non-required and omitted in the
   * request body, their values are emptied.
   */
  updateMask: string[] | undefined;
}

/**
 * Request message for
 * [GetEntryGroup][google.cloud.datacatalog.v1.DataCatalog.GetEntryGroup].
 */
export interface GetEntryGroupRequest {
  /** Required. The name of the entry group to get. */
  name: string;
  /** The fields to return. If empty or omitted, all fields are returned. */
  readMask: string[] | undefined;
}

/**
 * Request message for
 * [DeleteEntryGroup][google.cloud.datacatalog.v1.DataCatalog.DeleteEntryGroup].
 */
export interface DeleteEntryGroupRequest {
  /** Required. The name of the entry group to delete. */
  name: string;
  /** Optional. If true, deletes all entries in the entry group. */
  force: boolean;
}

/**
 * Request message for
 * [ListEntryGroups][google.cloud.datacatalog.v1.DataCatalog.ListEntryGroups].
 */
export interface ListEntryGroupsRequest {
  /**
   * Required. The name of the location that contains the entry groups to list.
   *
   * Can be provided as a URL.
   */
  parent: string;
  /**
   * Optional. The maximum number of items to return.
   *
   * Default is 10. Maximum limit is 1000.
   * Throws an invalid argument if `page_size` is greater than 1000.
   */
  pageSize: number;
  /**
   * Optional. Pagination token that specifies the next page to return.
   * If empty, returns the first page.
   */
  pageToken: string;
}

/**
 * Response message for
 * [ListEntryGroups][google.cloud.datacatalog.v1.DataCatalog.ListEntryGroups].
 */
export interface ListEntryGroupsResponse {
  /** Entry group details. */
  entryGroups: EntryGroup[];
  /**
   * Pagination token to specify in the next call to retrieve the next page of
   * results. Empty if there are no more items.
   */
  nextPageToken: string;
}

/**
 * Request message for
 * [CreateEntry][google.cloud.datacatalog.v1.DataCatalog.CreateEntry].
 */
export interface CreateEntryRequest {
  /**
   * Required. The name of the entry group this entry belongs to.
   *
   * Note: The entry itself and its child resources might not be stored in
   * the location specified in its name.
   */
  parent: string;
  /**
   * Required. The ID of the entry to create.
   *
   * The ID must contain only letters (a-z, A-Z), numbers (0-9),
   * and underscores (_).
   * The maximum size is 64 bytes when encoded in UTF-8.
   */
  entryId: string;
  /** Required. The entry to create. */
  entry: Entry | undefined;
}

/**
 * Request message for
 * [UpdateEntry][google.cloud.datacatalog.v1.DataCatalog.UpdateEntry].
 */
export interface UpdateEntryRequest {
  /** Required. Updates for the entry. The `name` field must be set. */
  entry:
    | Entry
    | undefined;
  /**
   * Names of fields whose values to overwrite on an entry.
   *
   * If this parameter is absent or empty, all modifiable fields
   * are overwritten. If such fields are non-required and omitted in the
   * request body, their values are emptied.
   *
   * You can modify only the fields listed below.
   *
   * For entries with type `DATA_STREAM`:
   *
   * * `schema`
   *
   * For entries with type `FILESET`:
   *
   * * `schema`
   * * `display_name`
   * * `description`
   * * `gcs_fileset_spec`
   * * `gcs_fileset_spec.file_patterns`
   *
   * For entries with `user_specified_type`:
   *
   * * `schema`
   * * `display_name`
   * * `description`
   * * `user_specified_type`
   * * `user_specified_system`
   * * `linked_resource`
   * * `source_system_timestamps`
   */
  updateMask: string[] | undefined;
}

/**
 * Request message for
 * [DeleteEntry][google.cloud.datacatalog.v1.DataCatalog.DeleteEntry].
 */
export interface DeleteEntryRequest {
  /** Required. The name of the entry to delete. */
  name: string;
}

/**
 * Request message for
 * [GetEntry][google.cloud.datacatalog.v1.DataCatalog.GetEntry].
 */
export interface GetEntryRequest {
  /** Required. The name of the entry to get. */
  name: string;
}

/**
 * Request message for
 * [LookupEntry][google.cloud.datacatalog.v1.DataCatalog.LookupEntry].
 */
export interface LookupEntryRequest {
  /**
   * The full name of the Google Cloud Platform resource the Data Catalog
   * entry represents. For more information, see [Full Resource Name]
   * (https://cloud.google.com/apis/design/resource_names#full_resource_name).
   *
   * Full names are case-sensitive. For example:
   *
   *  * `//bigquery.googleapis.com/projects/{PROJECT_ID}/datasets/{DATASET_ID}/tables/{TABLE_ID}`
   *  * `//pubsub.googleapis.com/projects/{PROJECT_ID}/topics/{TOPIC_ID}`
   */
  linkedResource?:
    | string
    | undefined;
  /**
   * The SQL name of the entry. SQL names are case-sensitive.
   *
   * Examples:
   *
   * * `pubsub.topic.{PROJECT_ID}.{TOPIC_ID}`
   * * `pubsub.topic.{PROJECT_ID}.`\``{TOPIC.ID.SEPARATED.WITH.DOTS}`\`
   * * `bigquery.table.{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`
   * * `bigquery.dataset.{PROJECT_ID}.{DATASET_ID}`
   * * `datacatalog.entry.{PROJECT_ID}.{LOCATION_ID}.{ENTRY_GROUP_ID}.{ENTRY_ID}`
   *
   * Identifiers (`*_ID`) should comply with the
   * [Lexical structure in Standard SQL]
   * (https://cloud.google.com/bigquery/docs/reference/standard-sql/lexical).
   */
  sqlResource?:
    | string
    | undefined;
  /**
   * [Fully Qualified Name
   * (FQN)](https://cloud.google.com//data-catalog/docs/fully-qualified-names)
   * of the resource.
   *
   * FQNs take two forms:
   *
   * * For non-regionalized resources:
   *
   *   `{SYSTEM}:{PROJECT}.{PATH_TO_RESOURCE_SEPARATED_WITH_DOTS}`
   *
   * * For regionalized resources:
   *
   *   `{SYSTEM}:{PROJECT}.{LOCATION_ID}.{PATH_TO_RESOURCE_SEPARATED_WITH_DOTS}`
   *
   * Example for a DPMS table:
   *
   * `dataproc_metastore:{PROJECT_ID}.{LOCATION_ID}.{INSTANCE_ID}.{DATABASE_ID}.{TABLE_ID}`
   */
  fullyQualifiedName?:
    | string
    | undefined;
  /**
   * Project where the lookup should be performed. Required to lookup
   * entry that is not a part of `DPMS` or `DATAPLEX` `integrated_system`
   * using its `fully_qualified_name`. Ignored in other cases.
   */
  project: string;
  /**
   * Location where the lookup should be performed. Required to lookup
   * entry that is not a part of `DPMS` or `DATAPLEX` `integrated_system`
   * using its `fully_qualified_name`. Ignored in other cases.
   */
  location: string;
}

/**
 * Entry metadata.
 * A Data Catalog entry represents another resource in Google
 * Cloud Platform (such as a BigQuery dataset or a Pub/Sub topic) or
 * outside of it. You can use the `linked_resource` field
 * in the entry resource to refer to the original resource ID of the source
 * system.
 *
 * An entry resource contains resource details, for example, its schema.
 * Additionally, you can attach flexible metadata to an entry in the form of a
 * [Tag][google.cloud.datacatalog.v1.Tag].
 */
export interface Entry {
  /**
   * Output only. The resource name of an entry in URL format.
   *
   * Note: The entry itself and its child resources might not be
   * stored in the location specified in its name.
   */
  name: string;
  /**
   * The resource this metadata entry refers to.
   *
   * For Google Cloud Platform resources, `linked_resource` is the
   * [Full Resource Name]
   * (https://cloud.google.com/apis/design/resource_names#full_resource_name).
   * For example, the `linked_resource` for a table resource from BigQuery is:
   *
   * `//bigquery.googleapis.com/projects/{PROJECT_ID}/datasets/{DATASET_ID}/tables/{TABLE_ID}`
   *
   * Output only when the entry is one of the types in the `EntryType` enum.
   *
   * For entries with a `user_specified_type`, this field is optional and
   * defaults to an empty string.
   *
   * The resource string must contain only letters (a-z, A-Z), numbers (0-9),
   * underscores (_), periods (.), colons (:), slashes (/), dashes (-),
   * and hashes (#).
   * The maximum size is 200 bytes when encoded in UTF-8.
   */
  linkedResource: string;
  /**
   * [Fully Qualified Name
   * (FQN)](https://cloud.google.com//data-catalog/docs/fully-qualified-names)
   * of the resource. Set automatically for entries representing resources from
   * synced systems. Settable only during creation, and read-only later. Can
   * be used for search and lookup of the entries.
   */
  fullyQualifiedName: string;
  /**
   * The type of the entry.
   *
   * For details, see [`EntryType`](#entrytype).
   */
  type?:
    | EntryType
    | undefined;
  /**
   * Custom entry type that doesn't match any of the values allowed for input
   * and listed in the `EntryType` enum.
   *
   * When creating an entry, first check the type values in the enum.
   * If there are no appropriate types for the new entry,
   * provide a custom value, for example, `my_special_type`.
   *
   * The `user_specified_type` string has the following limitations:
   *
   * * Is case insensitive.
   * * Must begin with a letter or underscore.
   * * Can only contain letters, numbers, and underscores.
   * * Must be at least 1 character and at most 64 characters long.
   */
  userSpecifiedType?:
    | string
    | undefined;
  /**
   * Output only. Indicates the entry's source system that Data Catalog
   * integrates with, such as BigQuery, Pub/Sub, or Dataproc Metastore.
   */
  integratedSystem?:
    | IntegratedSystem
    | undefined;
  /**
   * Indicates the entry's source system that Data Catalog doesn't
   * automatically integrate with.
   *
   * The `user_specified_system` string has the following limitations:
   *
   * * Is case insensitive.
   * * Must begin with a letter or underscore.
   * * Can only contain letters, numbers, and underscores.
   * * Must be at least 1 character and at most 64 characters long.
   */
  userSpecifiedSystem?:
    | string
    | undefined;
  /**
   * Specification that applies to a relational database system. Only settable
   * when `user_specified_system` is equal to `SQL_DATABASE`
   */
  sqlDatabaseSystemSpec?:
    | SqlDatabaseSystemSpec
    | undefined;
  /**
   * Specification that applies to Looker sysstem. Only settable when
   * `user_specified_system` is equal to `LOOKER`
   */
  lookerSystemSpec?:
    | LookerSystemSpec
    | undefined;
  /**
   * Specification that applies to Cloud Bigtable system. Only settable when
   * `integrated_system` is equal to `CLOUD_BIGTABLE`
   */
  cloudBigtableSystemSpec?:
    | CloudBigtableSystemSpec
    | undefined;
  /**
   * Specification that applies to a Cloud Storage fileset. Valid only
   * for entries with the `FILESET` type.
   */
  gcsFilesetSpec?:
    | GcsFilesetSpec
    | undefined;
  /**
   * Output only. Specification that applies to a BigQuery table. Valid only
   * for entries with the `TABLE` type.
   */
  bigqueryTableSpec?:
    | BigQueryTableSpec
    | undefined;
  /**
   * Output only. Specification for a group of BigQuery tables with
   * the `[prefix]YYYYMMDD` name pattern.
   *
   * For more information, see [Introduction to partitioned tables]
   * (https://cloud.google.com/bigquery/docs/partitioned-tables#partitioning_versus_sharding).
   */
  bigqueryDateShardedSpec?:
    | BigQueryDateShardedSpec
    | undefined;
  /**
   * Specification that applies to a table resource. Valid only
   * for entries with the `TABLE` or `EXPLORE` type.
   */
  databaseTableSpec?:
    | DatabaseTableSpec
    | undefined;
  /**
   * Specification that applies to a data source connection. Valid only
   * for entries with the `DATA_SOURCE_CONNECTION` type.
   */
  dataSourceConnectionSpec?:
    | DataSourceConnectionSpec
    | undefined;
  /**
   * Specification that applies to a user-defined function or procedure. Valid
   * only for entries with the `ROUTINE` type.
   */
  routineSpec?:
    | RoutineSpec
    | undefined;
  /** Specification that applies to a dataset. */
  datasetSpec?:
    | DatasetSpec
    | undefined;
  /**
   * Specification that applies to a fileset resource. Valid only
   * for entries with the `FILESET` type.
   */
  filesetSpec?:
    | FilesetSpec
    | undefined;
  /** Specification that applies to a Service resource. */
  serviceSpec?:
    | ServiceSpec
    | undefined;
  /** Model specification. */
  modelSpec?:
    | ModelSpec
    | undefined;
  /**
   * Display name of an entry.
   *
   * The maximum size is 500 bytes when encoded in UTF-8.
   * Default value is an empty string.
   */
  displayName: string;
  /**
   * Entry description that can consist of several sentences or paragraphs
   * that describe entry contents.
   *
   * The description must not contain Unicode non-characters as well as C0
   * and C1 control codes except tabs (HT), new lines (LF), carriage returns
   * (CR), and page breaks (FF).
   * The maximum size is 2000 bytes when encoded in UTF-8.
   * Default value is an empty string.
   */
  description: string;
  /** Business Context of the entry. Not supported for BigQuery datasets */
  businessContext:
    | BusinessContext
    | undefined;
  /** Schema of the entry. An entry might not have any schema attached to it. */
  schema:
    | Schema
    | undefined;
  /**
   * Timestamps from the underlying resource, not from the Data Catalog
   * entry.
   *
   * Output only when the entry has a system listed in the `IntegratedSystem`
   * enum. For entries with `user_specified_system`, this field is optional
   * and defaults to an empty timestamp.
   */
  sourceSystemTimestamps:
    | SystemTimestamps
    | undefined;
  /** Resource usage statistics. */
  usageSignal:
    | UsageSignal
    | undefined;
  /**
   * Cloud labels attached to the entry.
   *
   * In Data Catalog, you can create and modify labels attached only to custom
   * entries. Synced entries have unmodifiable labels that come from the source
   * system.
   */
  labels: { [key: string]: string };
  /** Output only. Physical location of the entry. */
  dataSource:
    | DataSource
    | undefined;
  /**
   * Output only. Additional information related to the entry. Private to the
   * current user.
   */
  personalDetails: PersonalDetails | undefined;
}

export interface Entry_LabelsEntry {
  key: string;
  value: string;
}

/**
 * Specification that applies to a table resource. Valid only
 * for entries with the `TABLE` type.
 */
export interface DatabaseTableSpec {
  /** Type of this table. */
  type: DatabaseTableSpec_TableType;
  /**
   * Output only. Fields specific to a Dataplex table and present only in the
   * Dataplex table entries.
   */
  dataplexTable:
    | DataplexTableSpec
    | undefined;
  /**
   * Spec what aplies to tables that are actually views.
   * Not set for "real" tables.
   */
  databaseViewSpec: DatabaseTableSpec_DatabaseViewSpec | undefined;
}

/** Type of the table. */
export enum DatabaseTableSpec_TableType {
  /** TABLE_TYPE_UNSPECIFIED - Default unknown table type. */
  TABLE_TYPE_UNSPECIFIED = 0,
  /** NATIVE - Native table. */
  NATIVE = 1,
  /** EXTERNAL - External table. */
  EXTERNAL = 2,
  UNRECOGNIZED = -1,
}

export function databaseTableSpec_TableTypeFromJSON(object: any): DatabaseTableSpec_TableType {
  switch (object) {
    case 0:
    case "TABLE_TYPE_UNSPECIFIED":
      return DatabaseTableSpec_TableType.TABLE_TYPE_UNSPECIFIED;
    case 1:
    case "NATIVE":
      return DatabaseTableSpec_TableType.NATIVE;
    case 2:
    case "EXTERNAL":
      return DatabaseTableSpec_TableType.EXTERNAL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DatabaseTableSpec_TableType.UNRECOGNIZED;
  }
}

export function databaseTableSpec_TableTypeToJSON(object: DatabaseTableSpec_TableType): string {
  switch (object) {
    case DatabaseTableSpec_TableType.TABLE_TYPE_UNSPECIFIED:
      return "TABLE_TYPE_UNSPECIFIED";
    case DatabaseTableSpec_TableType.NATIVE:
      return "NATIVE";
    case DatabaseTableSpec_TableType.EXTERNAL:
      return "EXTERNAL";
    case DatabaseTableSpec_TableType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Specification that applies to database view. */
export interface DatabaseTableSpec_DatabaseViewSpec {
  /** Type of this view. */
  viewType: DatabaseTableSpec_DatabaseViewSpec_ViewType;
  /** Name of a singular table this view reflects one to one. */
  baseTable?:
    | string
    | undefined;
  /** SQL query used to generate this view. */
  sqlQuery?: string | undefined;
}

/** Concrete type of the view. */
export enum DatabaseTableSpec_DatabaseViewSpec_ViewType {
  /** VIEW_TYPE_UNSPECIFIED - Default unknown view type. */
  VIEW_TYPE_UNSPECIFIED = 0,
  /** STANDARD_VIEW - Standard view. */
  STANDARD_VIEW = 1,
  /** MATERIALIZED_VIEW - Materialized view. */
  MATERIALIZED_VIEW = 2,
  UNRECOGNIZED = -1,
}

export function databaseTableSpec_DatabaseViewSpec_ViewTypeFromJSON(
  object: any,
): DatabaseTableSpec_DatabaseViewSpec_ViewType {
  switch (object) {
    case 0:
    case "VIEW_TYPE_UNSPECIFIED":
      return DatabaseTableSpec_DatabaseViewSpec_ViewType.VIEW_TYPE_UNSPECIFIED;
    case 1:
    case "STANDARD_VIEW":
      return DatabaseTableSpec_DatabaseViewSpec_ViewType.STANDARD_VIEW;
    case 2:
    case "MATERIALIZED_VIEW":
      return DatabaseTableSpec_DatabaseViewSpec_ViewType.MATERIALIZED_VIEW;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DatabaseTableSpec_DatabaseViewSpec_ViewType.UNRECOGNIZED;
  }
}

export function databaseTableSpec_DatabaseViewSpec_ViewTypeToJSON(
  object: DatabaseTableSpec_DatabaseViewSpec_ViewType,
): string {
  switch (object) {
    case DatabaseTableSpec_DatabaseViewSpec_ViewType.VIEW_TYPE_UNSPECIFIED:
      return "VIEW_TYPE_UNSPECIFIED";
    case DatabaseTableSpec_DatabaseViewSpec_ViewType.STANDARD_VIEW:
      return "STANDARD_VIEW";
    case DatabaseTableSpec_DatabaseViewSpec_ViewType.MATERIALIZED_VIEW:
      return "MATERIALIZED_VIEW";
    case DatabaseTableSpec_DatabaseViewSpec_ViewType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Specification that applies to a fileset. Valid only for entries with the
 * 'FILESET' type.
 */
export interface FilesetSpec {
  /**
   * Fields specific to a Dataplex fileset and present only in the Dataplex
   * fileset entries.
   */
  dataplexFileset: DataplexFilesetSpec | undefined;
}

/**
 * Specification that applies to a data source connection. Valid only for
 * entries with the `DATA_SOURCE_CONNECTION` type.
 * Only one of internal specs can be set at the time, and cannot
 * be changed later.
 */
export interface DataSourceConnectionSpec {
  /** Output only. Fields specific to BigQuery connections. */
  bigqueryConnectionSpec: BigQueryConnectionSpec | undefined;
}

/**
 * Specification that applies to a routine. Valid only for
 * entries with the `ROUTINE` type.
 */
export interface RoutineSpec {
  /** The type of the routine. */
  routineType: RoutineSpec_RoutineType;
  /**
   * The language the routine is written in. The exact value depends on the
   * source system. For BigQuery routines, possible values are:
   *
   * * `SQL`
   * * `JAVASCRIPT`
   */
  language: string;
  /** Arguments of the routine. */
  routineArguments: RoutineSpec_Argument[];
  /**
   * Return type of the argument. The exact value depends on the source system
   * and the language.
   */
  returnType: string;
  /** The body of the routine. */
  definitionBody: string;
  /** Fields specific for BigQuery routines. */
  bigqueryRoutineSpec?: BigQueryRoutineSpec | undefined;
}

/** The fine-grained type of the routine. */
export enum RoutineSpec_RoutineType {
  /** ROUTINE_TYPE_UNSPECIFIED - Unspecified type. */
  ROUTINE_TYPE_UNSPECIFIED = 0,
  /** SCALAR_FUNCTION - Non-builtin permanent scalar function. */
  SCALAR_FUNCTION = 1,
  /** PROCEDURE - Stored procedure. */
  PROCEDURE = 2,
  UNRECOGNIZED = -1,
}

export function routineSpec_RoutineTypeFromJSON(object: any): RoutineSpec_RoutineType {
  switch (object) {
    case 0:
    case "ROUTINE_TYPE_UNSPECIFIED":
      return RoutineSpec_RoutineType.ROUTINE_TYPE_UNSPECIFIED;
    case 1:
    case "SCALAR_FUNCTION":
      return RoutineSpec_RoutineType.SCALAR_FUNCTION;
    case 2:
    case "PROCEDURE":
      return RoutineSpec_RoutineType.PROCEDURE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return RoutineSpec_RoutineType.UNRECOGNIZED;
  }
}

export function routineSpec_RoutineTypeToJSON(object: RoutineSpec_RoutineType): string {
  switch (object) {
    case RoutineSpec_RoutineType.ROUTINE_TYPE_UNSPECIFIED:
      return "ROUTINE_TYPE_UNSPECIFIED";
    case RoutineSpec_RoutineType.SCALAR_FUNCTION:
      return "SCALAR_FUNCTION";
    case RoutineSpec_RoutineType.PROCEDURE:
      return "PROCEDURE";
    case RoutineSpec_RoutineType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Input or output argument of a function or stored procedure. */
export interface RoutineSpec_Argument {
  /**
   * The name of the argument. A return argument of a function might not have
   * a name.
   */
  name: string;
  /** Specifies whether the argument is input or output. */
  mode: RoutineSpec_Argument_Mode;
  /**
   * Type of the argument. The exact value depends on the source system and
   * the language.
   */
  type: string;
}

/** The input or output mode of the argument. */
export enum RoutineSpec_Argument_Mode {
  /** MODE_UNSPECIFIED - Unspecified mode. */
  MODE_UNSPECIFIED = 0,
  /** IN - The argument is input-only. */
  IN = 1,
  /** OUT - The argument is output-only. */
  OUT = 2,
  /** INOUT - The argument is both an input and an output. */
  INOUT = 3,
  UNRECOGNIZED = -1,
}

export function routineSpec_Argument_ModeFromJSON(object: any): RoutineSpec_Argument_Mode {
  switch (object) {
    case 0:
    case "MODE_UNSPECIFIED":
      return RoutineSpec_Argument_Mode.MODE_UNSPECIFIED;
    case 1:
    case "IN":
      return RoutineSpec_Argument_Mode.IN;
    case 2:
    case "OUT":
      return RoutineSpec_Argument_Mode.OUT;
    case 3:
    case "INOUT":
      return RoutineSpec_Argument_Mode.INOUT;
    case -1:
    case "UNRECOGNIZED":
    default:
      return RoutineSpec_Argument_Mode.UNRECOGNIZED;
  }
}

export function routineSpec_Argument_ModeToJSON(object: RoutineSpec_Argument_Mode): string {
  switch (object) {
    case RoutineSpec_Argument_Mode.MODE_UNSPECIFIED:
      return "MODE_UNSPECIFIED";
    case RoutineSpec_Argument_Mode.IN:
      return "IN";
    case RoutineSpec_Argument_Mode.OUT:
      return "OUT";
    case RoutineSpec_Argument_Mode.INOUT:
      return "INOUT";
    case RoutineSpec_Argument_Mode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Specification that applies to a dataset. Valid only for
 * entries with the `DATASET` type.
 */
export interface DatasetSpec {
  /** Vertex AI Dataset specific fields */
  vertexDatasetSpec?: VertexDatasetSpec | undefined;
}

/**
 * Specification that applies to
 * entries that are part `SQL_DATABASE` system
 * (user_specified_type)
 */
export interface SqlDatabaseSystemSpec {
  /**
   * SQL Database Engine.
   * enum SqlEngine {
   *  UNDEFINED = 0;
   *  MY_SQL = 1;
   *  POSTGRE_SQL = 2;
   *  SQL_SERVER = 3;
   * }
   * Engine of the enclosing database instance.
   */
  sqlEngine: string;
  /** Version of the database engine. */
  databaseVersion: string;
  /**
   * Host of the SQL database
   * enum InstanceHost {
   *  UNDEFINED = 0;
   *  SELF_HOSTED = 1;
   *  CLOUD_SQL = 2;
   *  AMAZON_RDS = 3;
   *  AZURE_SQL = 4;
   * }
   * Host of the enclousing database instance.
   */
  instanceHost: string;
}

/**
 * Specification that applies to
 * entries that are part `LOOKER` system
 * (user_specified_type)
 */
export interface LookerSystemSpec {
  /**
   * ID of the parent Looker Instance. Empty if it does not exist.
   * Example value: `someinstance.looker.com`
   */
  parentInstanceId: string;
  /** Name of the parent Looker Instance. Empty if it does not exist. */
  parentInstanceDisplayName: string;
  /** ID of the parent Model. Empty if it does not exist. */
  parentModelId: string;
  /** Name of the parent Model. Empty if it does not exist. */
  parentModelDisplayName: string;
  /** ID of the parent View. Empty if it does not exist. */
  parentViewId: string;
  /** Name of the parent View. Empty if it does not exist. */
  parentViewDisplayName: string;
}

/**
 * Specification that applies to
 * all entries that are part of `CLOUD_BIGTABLE` system
 * (user_specified_type)
 */
export interface CloudBigtableSystemSpec {
  /**
   * Display name of the Instance. This is user specified and different from
   * the resource name.
   */
  instanceDisplayName: string;
}

/**
 * Specification that applies to Instance
 * entries that are part of `CLOUD_BIGTABLE` system.
 * (user_specified_type)
 */
export interface CloudBigtableInstanceSpec {
  /** The list of clusters for the Instance. */
  cloudBigtableClusterSpecs: CloudBigtableInstanceSpec_CloudBigtableClusterSpec[];
}

/** Spec that applies to clusters of an Instance of Cloud Bigtable. */
export interface CloudBigtableInstanceSpec_CloudBigtableClusterSpec {
  /** Name of the cluster. */
  displayName: string;
  /** Location of the cluster, typically a Cloud zone. */
  location: string;
  /** Type of the resource. For a cluster this would be "CLUSTER". */
  type: string;
  /** A link back to the parent resource, in this case Instance. */
  linkedResource: string;
}

/**
 * Specification that applies to a Service resource. Valid only
 * for entries with the `SERVICE` type.
 */
export interface ServiceSpec {
  /**
   * Specification that applies to Instance entries of `CLOUD_BIGTABLE`
   * system.
   */
  cloudBigtableInstanceSpec?: CloudBigtableInstanceSpec | undefined;
}

/** Detail description of the source information of a Vertex model. */
export interface VertexModelSourceInfo {
  /** Type of the model source. */
  sourceType: VertexModelSourceInfo_ModelSourceType;
  /**
   * If this Model is copy of another Model. If true then
   * [source_type][google.cloud.datacatalog.v1.VertexModelSourceInfo.source_type]
   * pertains to the original.
   */
  copy: boolean;
}

/** Source of the model. */
export enum VertexModelSourceInfo_ModelSourceType {
  /** MODEL_SOURCE_TYPE_UNSPECIFIED - Should not be used. */
  MODEL_SOURCE_TYPE_UNSPECIFIED = 0,
  /** AUTOML - The Model is uploaded by automl training pipeline. */
  AUTOML = 1,
  /** CUSTOM - The Model is uploaded by user or custom training pipeline. */
  CUSTOM = 2,
  /** BQML - The Model is registered and sync'ed from BigQuery ML. */
  BQML = 3,
  /** MODEL_GARDEN - The Model is saved or tuned from Model Garden. */
  MODEL_GARDEN = 4,
  UNRECOGNIZED = -1,
}

export function vertexModelSourceInfo_ModelSourceTypeFromJSON(object: any): VertexModelSourceInfo_ModelSourceType {
  switch (object) {
    case 0:
    case "MODEL_SOURCE_TYPE_UNSPECIFIED":
      return VertexModelSourceInfo_ModelSourceType.MODEL_SOURCE_TYPE_UNSPECIFIED;
    case 1:
    case "AUTOML":
      return VertexModelSourceInfo_ModelSourceType.AUTOML;
    case 2:
    case "CUSTOM":
      return VertexModelSourceInfo_ModelSourceType.CUSTOM;
    case 3:
    case "BQML":
      return VertexModelSourceInfo_ModelSourceType.BQML;
    case 4:
    case "MODEL_GARDEN":
      return VertexModelSourceInfo_ModelSourceType.MODEL_GARDEN;
    case -1:
    case "UNRECOGNIZED":
    default:
      return VertexModelSourceInfo_ModelSourceType.UNRECOGNIZED;
  }
}

export function vertexModelSourceInfo_ModelSourceTypeToJSON(object: VertexModelSourceInfo_ModelSourceType): string {
  switch (object) {
    case VertexModelSourceInfo_ModelSourceType.MODEL_SOURCE_TYPE_UNSPECIFIED:
      return "MODEL_SOURCE_TYPE_UNSPECIFIED";
    case VertexModelSourceInfo_ModelSourceType.AUTOML:
      return "AUTOML";
    case VertexModelSourceInfo_ModelSourceType.CUSTOM:
      return "CUSTOM";
    case VertexModelSourceInfo_ModelSourceType.BQML:
      return "BQML";
    case VertexModelSourceInfo_ModelSourceType.MODEL_GARDEN:
      return "MODEL_GARDEN";
    case VertexModelSourceInfo_ModelSourceType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Specification for vertex model resources. */
export interface VertexModelSpec {
  /** The version ID of the model. */
  versionId: string;
  /**
   * User provided version aliases so that a model version can be referenced via
   * alias
   */
  versionAliases: string[];
  /** The description of this version. */
  versionDescription: string;
  /** Source of a Vertex model. */
  vertexModelSourceInfo:
    | VertexModelSourceInfo
    | undefined;
  /**
   * URI of the Docker image to be used as the custom container for serving
   * predictions.
   */
  containerImageUri: string;
}

/** Specification for vertex dataset resources. */
export interface VertexDatasetSpec {
  /**
   * The number of DataItems in this Dataset. Only apply for non-structured
   * Dataset.
   */
  dataItemCount: Long;
  /** Type of the dataset. */
  dataType: VertexDatasetSpec_DataType;
}

/** Type of data stored in the dataset. */
export enum VertexDatasetSpec_DataType {
  /** DATA_TYPE_UNSPECIFIED - Should not be used. */
  DATA_TYPE_UNSPECIFIED = 0,
  /** TABLE - Structured data dataset. */
  TABLE = 1,
  /**
   * IMAGE - Image dataset which supports ImageClassification, ImageObjectDetection
   * and ImageSegmentation problems.
   */
  IMAGE = 2,
  /**
   * TEXT - Document dataset which supports TextClassification, TextExtraction and
   * TextSentiment problems.
   */
  TEXT = 3,
  /**
   * VIDEO - Video dataset which supports VideoClassification, VideoObjectTracking and
   * VideoActionRecognition problems.
   */
  VIDEO = 4,
  /** CONVERSATION - Conversation dataset which supports conversation problems. */
  CONVERSATION = 5,
  /** TIME_SERIES - TimeSeries dataset. */
  TIME_SERIES = 6,
  /** DOCUMENT - Document dataset which supports DocumentAnnotation problems. */
  DOCUMENT = 7,
  /** TEXT_TO_SPEECH - TextToSpeech dataset which supports TextToSpeech problems. */
  TEXT_TO_SPEECH = 8,
  /** TRANSLATION - Translation dataset which supports Translation problems. */
  TRANSLATION = 9,
  /** STORE_VISION - Store Vision dataset which is used for HITL integration. */
  STORE_VISION = 10,
  /**
   * ENTERPRISE_KNOWLEDGE_GRAPH - Enterprise Knowledge Graph dataset which is used for HITL labeling
   * integration.
   */
  ENTERPRISE_KNOWLEDGE_GRAPH = 11,
  /** TEXT_PROMPT - Text prompt dataset which supports Large Language Models. */
  TEXT_PROMPT = 12,
  UNRECOGNIZED = -1,
}

export function vertexDatasetSpec_DataTypeFromJSON(object: any): VertexDatasetSpec_DataType {
  switch (object) {
    case 0:
    case "DATA_TYPE_UNSPECIFIED":
      return VertexDatasetSpec_DataType.DATA_TYPE_UNSPECIFIED;
    case 1:
    case "TABLE":
      return VertexDatasetSpec_DataType.TABLE;
    case 2:
    case "IMAGE":
      return VertexDatasetSpec_DataType.IMAGE;
    case 3:
    case "TEXT":
      return VertexDatasetSpec_DataType.TEXT;
    case 4:
    case "VIDEO":
      return VertexDatasetSpec_DataType.VIDEO;
    case 5:
    case "CONVERSATION":
      return VertexDatasetSpec_DataType.CONVERSATION;
    case 6:
    case "TIME_SERIES":
      return VertexDatasetSpec_DataType.TIME_SERIES;
    case 7:
    case "DOCUMENT":
      return VertexDatasetSpec_DataType.DOCUMENT;
    case 8:
    case "TEXT_TO_SPEECH":
      return VertexDatasetSpec_DataType.TEXT_TO_SPEECH;
    case 9:
    case "TRANSLATION":
      return VertexDatasetSpec_DataType.TRANSLATION;
    case 10:
    case "STORE_VISION":
      return VertexDatasetSpec_DataType.STORE_VISION;
    case 11:
    case "ENTERPRISE_KNOWLEDGE_GRAPH":
      return VertexDatasetSpec_DataType.ENTERPRISE_KNOWLEDGE_GRAPH;
    case 12:
    case "TEXT_PROMPT":
      return VertexDatasetSpec_DataType.TEXT_PROMPT;
    case -1:
    case "UNRECOGNIZED":
    default:
      return VertexDatasetSpec_DataType.UNRECOGNIZED;
  }
}

export function vertexDatasetSpec_DataTypeToJSON(object: VertexDatasetSpec_DataType): string {
  switch (object) {
    case VertexDatasetSpec_DataType.DATA_TYPE_UNSPECIFIED:
      return "DATA_TYPE_UNSPECIFIED";
    case VertexDatasetSpec_DataType.TABLE:
      return "TABLE";
    case VertexDatasetSpec_DataType.IMAGE:
      return "IMAGE";
    case VertexDatasetSpec_DataType.TEXT:
      return "TEXT";
    case VertexDatasetSpec_DataType.VIDEO:
      return "VIDEO";
    case VertexDatasetSpec_DataType.CONVERSATION:
      return "CONVERSATION";
    case VertexDatasetSpec_DataType.TIME_SERIES:
      return "TIME_SERIES";
    case VertexDatasetSpec_DataType.DOCUMENT:
      return "DOCUMENT";
    case VertexDatasetSpec_DataType.TEXT_TO_SPEECH:
      return "TEXT_TO_SPEECH";
    case VertexDatasetSpec_DataType.TRANSLATION:
      return "TRANSLATION";
    case VertexDatasetSpec_DataType.STORE_VISION:
      return "STORE_VISION";
    case VertexDatasetSpec_DataType.ENTERPRISE_KNOWLEDGE_GRAPH:
      return "ENTERPRISE_KNOWLEDGE_GRAPH";
    case VertexDatasetSpec_DataType.TEXT_PROMPT:
      return "TEXT_PROMPT";
    case VertexDatasetSpec_DataType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Specification that applies to a model. Valid only for
 * entries with the `MODEL` type.
 */
export interface ModelSpec {
  /** Specification for vertex model resources. */
  vertexModelSpec?: VertexModelSpec | undefined;
}

/** Business Context of the entry. */
export interface BusinessContext {
  /** Entry overview fields for rich text descriptions of entries. */
  entryOverview:
    | EntryOverview
    | undefined;
  /** Contact people for the entry. */
  contacts: Contacts | undefined;
}

/** Entry overview fields for rich text descriptions of entries. */
export interface EntryOverview {
  /**
   * Entry overview with support for rich text.
   *
   * The overview must only contain Unicode characters, and should be
   * formatted using HTML.
   * The maximum length is 10 MiB as this value holds HTML descriptions
   * including encoded images. The maximum length of the text without images
   * is 100 KiB.
   */
  overview: string;
}

/** Contact people for the entry. */
export interface Contacts {
  /** The list of contact people for the entry. */
  people: Contacts_Person[];
}

/** A contact person for the entry. */
export interface Contacts_Person {
  /** Designation of the person, for example, Data Steward. */
  designation: string;
  /**
   * Email of the person in the format of `john.doe@xyz`,
   * `<john.doe@xyz>`, or `John Doe<john.doe@xyz>`.
   */
  email: string;
}

/**
 * Entry group metadata.
 *
 * An `EntryGroup` resource represents a logical grouping of zero or more
 * Data Catalog [Entry][google.cloud.datacatalog.v1.Entry] resources.
 */
export interface EntryGroup {
  /**
   * The resource name of the entry group in URL format.
   *
   * Note: The entry group itself and its child resources might not be
   * stored in the location specified in its name.
   */
  name: string;
  /**
   * A short name to identify the entry group, for example,
   * "analytics data - jan 2011". Default value is an empty string.
   */
  displayName: string;
  /**
   * Entry group description. Can consist of several sentences or
   * paragraphs that describe the entry group contents.
   * Default value is an empty string.
   */
  description: string;
  /** Output only. Timestamps of the entry group. Default value is empty. */
  dataCatalogTimestamps: SystemTimestamps | undefined;
}

/**
 * Request message for
 * [CreateTagTemplate][google.cloud.datacatalog.v1.DataCatalog.CreateTagTemplate].
 */
export interface CreateTagTemplateRequest {
  /**
   * Required. The name of the project and the template location
   * [region](https://cloud.google.com/data-catalog/docs/concepts/regions).
   */
  parent: string;
  /**
   * Required. The ID of the tag template to create.
   *
   * The ID must contain only lowercase letters (a-z), numbers (0-9),
   * or underscores (_), and must start with a letter or underscore.
   * The maximum size is 64 bytes when encoded in UTF-8.
   */
  tagTemplateId: string;
  /** Required. The tag template to create. */
  tagTemplate: TagTemplate | undefined;
}

/**
 * Request message for
 * [GetTagTemplate][google.cloud.datacatalog.v1.DataCatalog.GetTagTemplate].
 */
export interface GetTagTemplateRequest {
  /** Required. The name of the tag template to get. */
  name: string;
}

/**
 * Request message for
 * [UpdateTagTemplate][google.cloud.datacatalog.v1.DataCatalog.UpdateTagTemplate].
 */
export interface UpdateTagTemplateRequest {
  /** Required. The template to update. The `name` field must be set. */
  tagTemplate:
    | TagTemplate
    | undefined;
  /**
   * Names of fields whose values to overwrite on a tag template. Currently,
   * only `display_name` and `is_publicly_readable` can be overwritten.
   *
   * If this parameter is absent or empty, all modifiable fields
   * are overwritten. If such fields are non-required and omitted in the
   * request body, their values are emptied.
   *
   * Note: Updating the `is_publicly_readable` field may require up to 12
   * hours to take effect in search results.
   */
  updateMask: string[] | undefined;
}

/**
 * Request message for
 * [DeleteTagTemplate][google.cloud.datacatalog.v1.DataCatalog.DeleteTagTemplate].
 */
export interface DeleteTagTemplateRequest {
  /** Required. The name of the tag template to delete. */
  name: string;
  /**
   * Required. If true, deletes all tags that use this template.
   *
   * Currently, `true` is the only supported value.
   */
  force: boolean;
}

/**
 * Request message for
 * [CreateTag][google.cloud.datacatalog.v1.DataCatalog.CreateTag].
 */
export interface CreateTagRequest {
  /**
   * Required. The name of the resource to attach this tag to.
   *
   * Tags can be attached to entries or entry groups. An entry can have up to
   * 1000 attached tags.
   *
   * Note: The tag and its child resources might not be stored in
   * the location specified in its name.
   */
  parent: string;
  /** Required. The tag to create. */
  tag: Tag | undefined;
}

/**
 * Request message for
 * [UpdateTag][google.cloud.datacatalog.v1.DataCatalog.UpdateTag].
 */
export interface UpdateTagRequest {
  /** Required. The updated tag. The "name" field must be set. */
  tag:
    | Tag
    | undefined;
  /**
   * Names of fields whose values to overwrite on a tag. Currently, a tag has
   * the only modifiable field with the name `fields`.
   *
   * In general, if this parameter is absent or empty, all modifiable fields
   * are overwritten. If such fields are non-required and omitted in the
   * request body, their values are emptied.
   */
  updateMask: string[] | undefined;
}

/**
 * Request message for
 * [DeleteTag][google.cloud.datacatalog.v1.DataCatalog.DeleteTag].
 */
export interface DeleteTagRequest {
  /** Required. The name of the tag to delete. */
  name: string;
}

/**
 * Request message for
 * [CreateTagTemplateField][google.cloud.datacatalog.v1.DataCatalog.CreateTagTemplateField].
 */
export interface CreateTagTemplateFieldRequest {
  /**
   * Required. The name of the project and the template location
   * [region](https://cloud.google.com/data-catalog/docs/concepts/regions).
   */
  parent: string;
  /**
   * Required. The ID of the tag template field to create.
   *
   * Note: Adding a required field to an existing template is *not* allowed.
   *
   * Field IDs can contain letters (both uppercase and lowercase), numbers
   * (0-9), underscores (_) and dashes (-). Field IDs must be at least 1
   * character long and at most 128 characters long. Field IDs must also be
   * unique within their template.
   */
  tagTemplateFieldId: string;
  /** Required. The tag template field to create. */
  tagTemplateField: TagTemplateField | undefined;
}

/**
 * Request message for
 * [UpdateTagTemplateField][google.cloud.datacatalog.v1.DataCatalog.UpdateTagTemplateField].
 */
export interface UpdateTagTemplateFieldRequest {
  /** Required. The name of the tag template field. */
  name: string;
  /** Required. The template to update. */
  tagTemplateField:
    | TagTemplateField
    | undefined;
  /**
   * Optional. Names of fields whose values to overwrite on an individual field
   * of a tag template. The following fields are modifiable:
   *
   * * `display_name`
   * * `type.enum_type`
   * * `is_required`
   *
   * If this parameter is absent or empty, all modifiable fields
   * are overwritten. If such fields are non-required and omitted in the request
   * body, their values are emptied with one exception: when updating an enum
   * type, the provided values are merged with the existing values. Therefore,
   * enum values can only be added, existing enum values cannot be deleted or
   * renamed.
   *
   * Additionally, updating a template field from optional to required is
   * *not* allowed.
   */
  updateMask: string[] | undefined;
}

/**
 * Request message for
 * [RenameTagTemplateField][google.cloud.datacatalog.v1.DataCatalog.RenameTagTemplateField].
 */
export interface RenameTagTemplateFieldRequest {
  /** Required. The name of the tag template field. */
  name: string;
  /**
   * Required. The new ID of this tag template field. For example,
   * `my_new_field`.
   */
  newTagTemplateFieldId: string;
}

/**
 * Request message for
 * [RenameTagTemplateFieldEnumValue][google.cloud.datacatalog.v1.DataCatalog.RenameTagTemplateFieldEnumValue].
 */
export interface RenameTagTemplateFieldEnumValueRequest {
  /** Required. The name of the enum field value. */
  name: string;
  /**
   * Required. The new display name of the enum value. For example,
   * `my_new_enum_value`.
   */
  newEnumValueDisplayName: string;
}

/**
 * Request message for
 * [DeleteTagTemplateField][google.cloud.datacatalog.v1.DataCatalog.DeleteTagTemplateField].
 */
export interface DeleteTagTemplateFieldRequest {
  /** Required. The name of the tag template field to delete. */
  name: string;
  /**
   * Required. If true, deletes this field from any tags that use it.
   *
   * Currently, `true` is the only supported value.
   */
  force: boolean;
}

/**
 * Request message for
 * [ListTags][google.cloud.datacatalog.v1.DataCatalog.ListTags].
 */
export interface ListTagsRequest {
  /**
   * Required. The name of the Data Catalog resource to list the tags of.
   *
   * The resource can be an [Entry][google.cloud.datacatalog.v1.Entry]
   * or an [EntryGroup][google.cloud.datacatalog.v1.EntryGroup]
   * (without `/entries/{entries}` at the end).
   */
  parent: string;
  /** The maximum number of tags to return. Default is 10. Maximum limit is 1000. */
  pageSize: number;
  /**
   * Pagination token that specifies the next page to return. If empty, the
   * first page is returned.
   */
  pageToken: string;
}

/**
 * Response message for
 * [ListTags][google.cloud.datacatalog.v1.DataCatalog.ListTags].
 */
export interface ListTagsResponse {
  /** [Tag][google.cloud.datacatalog.v1.Tag] details. */
  tags: Tag[];
  /**
   * Pagination token of the next results page. Empty if there are
   * no more items in results.
   */
  nextPageToken: string;
}

/**
 * Request message for
 * [ReconcileTags][google.cloud.datacatalog.v1.DataCatalog.ReconcileTags].
 */
export interface ReconcileTagsRequest {
  /** Required. Name of [Entry][google.cloud.datacatalog.v1.Entry] to be tagged. */
  parent: string;
  /** Required. The name of the tag template, which is used for reconciliation. */
  tagTemplate: string;
  /**
   * If set to `true`, deletes entry tags related to a tag template
   * not listed in the tags source from an entry. If set to `false`,
   * unlisted tags are retained.
   */
  forceDeleteMissing: boolean;
  /**
   * A list of tags to apply to an entry. A tag can specify a
   * tag template, which must be the template specified in the
   * `ReconcileTagsRequest`.
   * The sole entry and each of its columns must be mentioned at most once.
   */
  tags: Tag[];
}

/**
 * [Long-running operation][google.longrunning.Operation]
 * response message returned by
 * [ReconcileTags][google.cloud.datacatalog.v1.DataCatalog.ReconcileTags].
 */
export interface ReconcileTagsResponse {
  /** Number of tags created in the request. */
  createdTagsCount: Long;
  /** Number of tags updated in the request. */
  updatedTagsCount: Long;
  /** Number of tags deleted in the request. */
  deletedTagsCount: Long;
}

/**
 * [Long-running operation][google.longrunning.Operation]
 * metadata message returned by the
 * [ReconcileTags][google.cloud.datacatalog.v1.DataCatalog.ReconcileTags].
 */
export interface ReconcileTagsMetadata {
  /** State of the reconciliation operation. */
  state: ReconcileTagsMetadata_ReconciliationState;
  /**
   * Maps the name of each tagged column (or empty string for a
   * sole entry) to tagging operation [status][google.rpc.Status].
   */
  errors: { [key: string]: Status };
}

/** Enum holding possible states of the reconciliation operation. */
export enum ReconcileTagsMetadata_ReconciliationState {
  /** RECONCILIATION_STATE_UNSPECIFIED - Default value. This value is unused. */
  RECONCILIATION_STATE_UNSPECIFIED = 0,
  /** RECONCILIATION_QUEUED - The reconciliation has been queued and awaits for execution. */
  RECONCILIATION_QUEUED = 1,
  /** RECONCILIATION_IN_PROGRESS - The reconciliation is in progress. */
  RECONCILIATION_IN_PROGRESS = 2,
  /** RECONCILIATION_DONE - The reconciliation has been finished. */
  RECONCILIATION_DONE = 3,
  UNRECOGNIZED = -1,
}

export function reconcileTagsMetadata_ReconciliationStateFromJSON(
  object: any,
): ReconcileTagsMetadata_ReconciliationState {
  switch (object) {
    case 0:
    case "RECONCILIATION_STATE_UNSPECIFIED":
      return ReconcileTagsMetadata_ReconciliationState.RECONCILIATION_STATE_UNSPECIFIED;
    case 1:
    case "RECONCILIATION_QUEUED":
      return ReconcileTagsMetadata_ReconciliationState.RECONCILIATION_QUEUED;
    case 2:
    case "RECONCILIATION_IN_PROGRESS":
      return ReconcileTagsMetadata_ReconciliationState.RECONCILIATION_IN_PROGRESS;
    case 3:
    case "RECONCILIATION_DONE":
      return ReconcileTagsMetadata_ReconciliationState.RECONCILIATION_DONE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ReconcileTagsMetadata_ReconciliationState.UNRECOGNIZED;
  }
}

export function reconcileTagsMetadata_ReconciliationStateToJSON(
  object: ReconcileTagsMetadata_ReconciliationState,
): string {
  switch (object) {
    case ReconcileTagsMetadata_ReconciliationState.RECONCILIATION_STATE_UNSPECIFIED:
      return "RECONCILIATION_STATE_UNSPECIFIED";
    case ReconcileTagsMetadata_ReconciliationState.RECONCILIATION_QUEUED:
      return "RECONCILIATION_QUEUED";
    case ReconcileTagsMetadata_ReconciliationState.RECONCILIATION_IN_PROGRESS:
      return "RECONCILIATION_IN_PROGRESS";
    case ReconcileTagsMetadata_ReconciliationState.RECONCILIATION_DONE:
      return "RECONCILIATION_DONE";
    case ReconcileTagsMetadata_ReconciliationState.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

export interface ReconcileTagsMetadata_ErrorsEntry {
  key: string;
  value: Status | undefined;
}

/**
 * Request message for
 * [ListEntries][google.cloud.datacatalog.v1.DataCatalog.ListEntries].
 */
export interface ListEntriesRequest {
  /**
   * Required. The name of the entry group that contains the entries to list.
   *
   * Can be provided in URL format.
   */
  parent: string;
  /**
   * The maximum number of items to return. Default is 10. Maximum limit is
   * 1000. Throws an invalid argument if `page_size` is more than 1000.
   */
  pageSize: number;
  /**
   * Pagination token that specifies the next page to return. If empty, the
   * first page is returned.
   */
  pageToken: string;
  /**
   * The fields to return for each entry. If empty or omitted, all
   * fields are returned.
   *
   * For example, to return a list of entries with only the `name` field,
   * set `read_mask` to only one path with the `name` value.
   */
  readMask: string[] | undefined;
}

/**
 * Response message for
 * [ListEntries][google.cloud.datacatalog.v1.DataCatalog.ListEntries].
 */
export interface ListEntriesResponse {
  /** Entry details. */
  entries: Entry[];
  /**
   * Pagination token of the next results page. Empty if there are no more items
   * in results.
   */
  nextPageToken: string;
}

/**
 * Request message for
 * [StarEntry][google.cloud.datacatalog.v1.DataCatalog.StarEntry].
 */
export interface StarEntryRequest {
  /** Required. The name of the entry to mark as starred. */
  name: string;
}

/**
 * Response message for
 * [StarEntry][google.cloud.datacatalog.v1.DataCatalog.StarEntry].
 * Empty for now
 */
export interface StarEntryResponse {
}

/**
 * Request message for
 * [UnstarEntry][google.cloud.datacatalog.v1.DataCatalog.UnstarEntry].
 */
export interface UnstarEntryRequest {
  /** Required. The name of the entry to mark as **not** starred. */
  name: string;
}

/**
 * Response message for
 * [UnstarEntry][google.cloud.datacatalog.v1.DataCatalog.UnstarEntry].
 * Empty for now
 */
export interface UnstarEntryResponse {
}

/**
 * Request message for
 * [ImportEntries][google.cloud.datacatalog.v1.DataCatalog.ImportEntries]
 * method.
 */
export interface ImportEntriesRequest {
  /** Required. Target entry group for ingested entries. */
  parent: string;
  /** Path to a Cloud Storage bucket that contains a dump ready for ingestion. */
  gcsBucketPath?:
    | string
    | undefined;
  /**
   * Optional. (Optional) Dataplex task job id, if specified will be used as
   * part of ImportEntries LRO ID
   */
  jobId: string;
}

/**
 * Response message for [long-running operation][google.longrunning.Operation]
 * returned by the
 * [ImportEntries][google.cloud.datacatalog.v1.DataCatalog.ImportEntries].
 */
export interface ImportEntriesResponse {
  /**
   * Cumulative number of entries created and entries updated as a result of
   * import operation.
   */
  upsertedEntriesCount?:
    | Long
    | undefined;
  /** Number of entries deleted as a result of import operation. */
  deletedEntriesCount?: Long | undefined;
}

/**
 * Metadata message for [long-running operation][google.longrunning.Operation]
 * returned by the
 * [ImportEntries][google.cloud.datacatalog.v1.DataCatalog.ImportEntries].
 */
export interface ImportEntriesMetadata {
  /** State of the import operation. */
  state: ImportEntriesMetadata_ImportState;
  /**
   * Partial errors that are encountered during the ImportEntries operation.
   * There is no guarantee that all the encountered errors are reported.
   * However, if no errors are reported, it means that no errors were
   * encountered.
   */
  errors: Status[];
}

/** Enum holding possible states of the import operation. */
export enum ImportEntriesMetadata_ImportState {
  /** IMPORT_STATE_UNSPECIFIED - Default value. This value is unused. */
  IMPORT_STATE_UNSPECIFIED = 0,
  /** IMPORT_QUEUED - The dump with entries has been queued for import. */
  IMPORT_QUEUED = 1,
  /** IMPORT_IN_PROGRESS - The import of entries is in progress. */
  IMPORT_IN_PROGRESS = 2,
  /** IMPORT_DONE - The import of entries has been finished. */
  IMPORT_DONE = 3,
  /** IMPORT_OBSOLETE - The import of entries has been abandoned in favor of a newer request. */
  IMPORT_OBSOLETE = 4,
  UNRECOGNIZED = -1,
}

export function importEntriesMetadata_ImportStateFromJSON(object: any): ImportEntriesMetadata_ImportState {
  switch (object) {
    case 0:
    case "IMPORT_STATE_UNSPECIFIED":
      return ImportEntriesMetadata_ImportState.IMPORT_STATE_UNSPECIFIED;
    case 1:
    case "IMPORT_QUEUED":
      return ImportEntriesMetadata_ImportState.IMPORT_QUEUED;
    case 2:
    case "IMPORT_IN_PROGRESS":
      return ImportEntriesMetadata_ImportState.IMPORT_IN_PROGRESS;
    case 3:
    case "IMPORT_DONE":
      return ImportEntriesMetadata_ImportState.IMPORT_DONE;
    case 4:
    case "IMPORT_OBSOLETE":
      return ImportEntriesMetadata_ImportState.IMPORT_OBSOLETE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ImportEntriesMetadata_ImportState.UNRECOGNIZED;
  }
}

export function importEntriesMetadata_ImportStateToJSON(object: ImportEntriesMetadata_ImportState): string {
  switch (object) {
    case ImportEntriesMetadata_ImportState.IMPORT_STATE_UNSPECIFIED:
      return "IMPORT_STATE_UNSPECIFIED";
    case ImportEntriesMetadata_ImportState.IMPORT_QUEUED:
      return "IMPORT_QUEUED";
    case ImportEntriesMetadata_ImportState.IMPORT_IN_PROGRESS:
      return "IMPORT_IN_PROGRESS";
    case ImportEntriesMetadata_ImportState.IMPORT_DONE:
      return "IMPORT_DONE";
    case ImportEntriesMetadata_ImportState.IMPORT_OBSOLETE:
      return "IMPORT_OBSOLETE";
    case ImportEntriesMetadata_ImportState.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Request message for
 * [ModifyEntryOverview][google.cloud.datacatalog.v1.DataCatalog.ModifyEntryOverview].
 */
export interface ModifyEntryOverviewRequest {
  /** Required. The full resource name of the entry. */
  name: string;
  /** Required. The new value for the Entry Overview. */
  entryOverview: EntryOverview | undefined;
}

/**
 * Request message for
 * [ModifyEntryContacts][google.cloud.datacatalog.v1.DataCatalog.ModifyEntryContacts].
 */
export interface ModifyEntryContactsRequest {
  /** Required. The full resource name of the entry. */
  name: string;
  /** Required. The new value for the Contacts. */
  contacts: Contacts | undefined;
}

function createBaseSearchCatalogRequest(): SearchCatalogRequest {
  return { scope: undefined, query: "", pageSize: 0, pageToken: "", orderBy: "", adminSearch: false };
}

export const SearchCatalogRequest: MessageFns<SearchCatalogRequest> = {
  encode(message: SearchCatalogRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.scope !== undefined) {
      SearchCatalogRequest_Scope.encode(message.scope, writer.uint32(50).fork()).join();
    }
    if (message.query !== "") {
      writer.uint32(10).string(message.query);
    }
    if (message.pageSize !== 0) {
      writer.uint32(16).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(26).string(message.pageToken);
    }
    if (message.orderBy !== "") {
      writer.uint32(42).string(message.orderBy);
    }
    if (message.adminSearch !== false) {
      writer.uint32(136).bool(message.adminSearch);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SearchCatalogRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSearchCatalogRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 6:
          if (tag !== 50) {
            break;
          }

          message.scope = SearchCatalogRequest_Scope.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.query = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pageToken = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.orderBy = reader.string();
          continue;
        case 17:
          if (tag !== 136) {
            break;
          }

          message.adminSearch = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SearchCatalogRequest {
    return {
      scope: isSet(object.scope) ? SearchCatalogRequest_Scope.fromJSON(object.scope) : undefined,
      query: isSet(object.query) ? globalThis.String(object.query) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
      orderBy: isSet(object.orderBy) ? globalThis.String(object.orderBy) : "",
      adminSearch: isSet(object.adminSearch) ? globalThis.Boolean(object.adminSearch) : false,
    };
  },

  toJSON(message: SearchCatalogRequest): unknown {
    const obj: any = {};
    if (message.scope !== undefined) {
      obj.scope = SearchCatalogRequest_Scope.toJSON(message.scope);
    }
    if (message.query !== "") {
      obj.query = message.query;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    if (message.orderBy !== "") {
      obj.orderBy = message.orderBy;
    }
    if (message.adminSearch !== false) {
      obj.adminSearch = message.adminSearch;
    }
    return obj;
  },

  create(base?: DeepPartial<SearchCatalogRequest>): SearchCatalogRequest {
    return SearchCatalogRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SearchCatalogRequest>): SearchCatalogRequest {
    const message = createBaseSearchCatalogRequest();
    message.scope = (object.scope !== undefined && object.scope !== null)
      ? SearchCatalogRequest_Scope.fromPartial(object.scope)
      : undefined;
    message.query = object.query ?? "";
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    message.orderBy = object.orderBy ?? "";
    message.adminSearch = object.adminSearch ?? false;
    return message;
  },
};

function createBaseSearchCatalogRequest_Scope(): SearchCatalogRequest_Scope {
  return {
    includeOrgIds: [],
    includeProjectIds: [],
    includeGcpPublicDatasets: false,
    restrictedLocations: [],
    starredOnly: false,
    includePublicTagTemplates: false,
  };
}

export const SearchCatalogRequest_Scope: MessageFns<SearchCatalogRequest_Scope> = {
  encode(message: SearchCatalogRequest_Scope, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.includeOrgIds) {
      writer.uint32(18).string(v!);
    }
    for (const v of message.includeProjectIds) {
      writer.uint32(26).string(v!);
    }
    if (message.includeGcpPublicDatasets !== false) {
      writer.uint32(56).bool(message.includeGcpPublicDatasets);
    }
    for (const v of message.restrictedLocations) {
      writer.uint32(130).string(v!);
    }
    if (message.starredOnly !== false) {
      writer.uint32(144).bool(message.starredOnly);
    }
    if (message.includePublicTagTemplates !== false) {
      writer.uint32(152).bool(message.includePublicTagTemplates);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SearchCatalogRequest_Scope {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSearchCatalogRequest_Scope();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.includeOrgIds.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.includeProjectIds.push(reader.string());
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.includeGcpPublicDatasets = reader.bool();
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.restrictedLocations.push(reader.string());
          continue;
        case 18:
          if (tag !== 144) {
            break;
          }

          message.starredOnly = reader.bool();
          continue;
        case 19:
          if (tag !== 152) {
            break;
          }

          message.includePublicTagTemplates = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SearchCatalogRequest_Scope {
    return {
      includeOrgIds: globalThis.Array.isArray(object?.includeOrgIds)
        ? object.includeOrgIds.map((e: any) => globalThis.String(e))
        : [],
      includeProjectIds: globalThis.Array.isArray(object?.includeProjectIds)
        ? object.includeProjectIds.map((e: any) => globalThis.String(e))
        : [],
      includeGcpPublicDatasets: isSet(object.includeGcpPublicDatasets)
        ? globalThis.Boolean(object.includeGcpPublicDatasets)
        : false,
      restrictedLocations: globalThis.Array.isArray(object?.restrictedLocations)
        ? object.restrictedLocations.map((e: any) => globalThis.String(e))
        : [],
      starredOnly: isSet(object.starredOnly) ? globalThis.Boolean(object.starredOnly) : false,
      includePublicTagTemplates: isSet(object.includePublicTagTemplates)
        ? globalThis.Boolean(object.includePublicTagTemplates)
        : false,
    };
  },

  toJSON(message: SearchCatalogRequest_Scope): unknown {
    const obj: any = {};
    if (message.includeOrgIds?.length) {
      obj.includeOrgIds = message.includeOrgIds;
    }
    if (message.includeProjectIds?.length) {
      obj.includeProjectIds = message.includeProjectIds;
    }
    if (message.includeGcpPublicDatasets !== false) {
      obj.includeGcpPublicDatasets = message.includeGcpPublicDatasets;
    }
    if (message.restrictedLocations?.length) {
      obj.restrictedLocations = message.restrictedLocations;
    }
    if (message.starredOnly !== false) {
      obj.starredOnly = message.starredOnly;
    }
    if (message.includePublicTagTemplates !== false) {
      obj.includePublicTagTemplates = message.includePublicTagTemplates;
    }
    return obj;
  },

  create(base?: DeepPartial<SearchCatalogRequest_Scope>): SearchCatalogRequest_Scope {
    return SearchCatalogRequest_Scope.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SearchCatalogRequest_Scope>): SearchCatalogRequest_Scope {
    const message = createBaseSearchCatalogRequest_Scope();
    message.includeOrgIds = object.includeOrgIds?.map((e) => e) || [];
    message.includeProjectIds = object.includeProjectIds?.map((e) => e) || [];
    message.includeGcpPublicDatasets = object.includeGcpPublicDatasets ?? false;
    message.restrictedLocations = object.restrictedLocations?.map((e) => e) || [];
    message.starredOnly = object.starredOnly ?? false;
    message.includePublicTagTemplates = object.includePublicTagTemplates ?? false;
    return message;
  },
};

function createBaseSearchCatalogResponse(): SearchCatalogResponse {
  return { results: [], totalSize: 0, nextPageToken: "", unreachable: [] };
}

export const SearchCatalogResponse: MessageFns<SearchCatalogResponse> = {
  encode(message: SearchCatalogResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.results) {
      SearchCatalogResult.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.totalSize !== 0) {
      writer.uint32(16).int32(message.totalSize);
    }
    if (message.nextPageToken !== "") {
      writer.uint32(26).string(message.nextPageToken);
    }
    for (const v of message.unreachable) {
      writer.uint32(50).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SearchCatalogResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSearchCatalogResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.results.push(SearchCatalogResult.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.totalSize = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.unreachable.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SearchCatalogResponse {
    return {
      results: globalThis.Array.isArray(object?.results)
        ? object.results.map((e: any) => SearchCatalogResult.fromJSON(e))
        : [],
      totalSize: isSet(object.totalSize) ? globalThis.Number(object.totalSize) : 0,
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
      unreachable: globalThis.Array.isArray(object?.unreachable)
        ? object.unreachable.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: SearchCatalogResponse): unknown {
    const obj: any = {};
    if (message.results?.length) {
      obj.results = message.results.map((e) => SearchCatalogResult.toJSON(e));
    }
    if (message.totalSize !== 0) {
      obj.totalSize = Math.round(message.totalSize);
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    if (message.unreachable?.length) {
      obj.unreachable = message.unreachable;
    }
    return obj;
  },

  create(base?: DeepPartial<SearchCatalogResponse>): SearchCatalogResponse {
    return SearchCatalogResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SearchCatalogResponse>): SearchCatalogResponse {
    const message = createBaseSearchCatalogResponse();
    message.results = object.results?.map((e) => SearchCatalogResult.fromPartial(e)) || [];
    message.totalSize = object.totalSize ?? 0;
    message.nextPageToken = object.nextPageToken ?? "";
    message.unreachable = object.unreachable?.map((e) => e) || [];
    return message;
  },
};

function createBaseCreateEntryGroupRequest(): CreateEntryGroupRequest {
  return { parent: "", entryGroupId: "", entryGroup: undefined };
}

export const CreateEntryGroupRequest: MessageFns<CreateEntryGroupRequest> = {
  encode(message: CreateEntryGroupRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.entryGroupId !== "") {
      writer.uint32(26).string(message.entryGroupId);
    }
    if (message.entryGroup !== undefined) {
      EntryGroup.encode(message.entryGroup, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateEntryGroupRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateEntryGroupRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.entryGroupId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.entryGroup = EntryGroup.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateEntryGroupRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      entryGroupId: isSet(object.entryGroupId) ? globalThis.String(object.entryGroupId) : "",
      entryGroup: isSet(object.entryGroup) ? EntryGroup.fromJSON(object.entryGroup) : undefined,
    };
  },

  toJSON(message: CreateEntryGroupRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.entryGroupId !== "") {
      obj.entryGroupId = message.entryGroupId;
    }
    if (message.entryGroup !== undefined) {
      obj.entryGroup = EntryGroup.toJSON(message.entryGroup);
    }
    return obj;
  },

  create(base?: DeepPartial<CreateEntryGroupRequest>): CreateEntryGroupRequest {
    return CreateEntryGroupRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateEntryGroupRequest>): CreateEntryGroupRequest {
    const message = createBaseCreateEntryGroupRequest();
    message.parent = object.parent ?? "";
    message.entryGroupId = object.entryGroupId ?? "";
    message.entryGroup = (object.entryGroup !== undefined && object.entryGroup !== null)
      ? EntryGroup.fromPartial(object.entryGroup)
      : undefined;
    return message;
  },
};

function createBaseUpdateEntryGroupRequest(): UpdateEntryGroupRequest {
  return { entryGroup: undefined, updateMask: undefined };
}

export const UpdateEntryGroupRequest: MessageFns<UpdateEntryGroupRequest> = {
  encode(message: UpdateEntryGroupRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.entryGroup !== undefined) {
      EntryGroup.encode(message.entryGroup, writer.uint32(10).fork()).join();
    }
    if (message.updateMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.updateMask), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateEntryGroupRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateEntryGroupRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.entryGroup = EntryGroup.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.updateMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateEntryGroupRequest {
    return {
      entryGroup: isSet(object.entryGroup) ? EntryGroup.fromJSON(object.entryGroup) : undefined,
      updateMask: isSet(object.updateMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.updateMask)) : undefined,
    };
  },

  toJSON(message: UpdateEntryGroupRequest): unknown {
    const obj: any = {};
    if (message.entryGroup !== undefined) {
      obj.entryGroup = EntryGroup.toJSON(message.entryGroup);
    }
    if (message.updateMask !== undefined) {
      obj.updateMask = FieldMask.toJSON(FieldMask.wrap(message.updateMask));
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateEntryGroupRequest>): UpdateEntryGroupRequest {
    return UpdateEntryGroupRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateEntryGroupRequest>): UpdateEntryGroupRequest {
    const message = createBaseUpdateEntryGroupRequest();
    message.entryGroup = (object.entryGroup !== undefined && object.entryGroup !== null)
      ? EntryGroup.fromPartial(object.entryGroup)
      : undefined;
    message.updateMask = object.updateMask ?? undefined;
    return message;
  },
};

function createBaseGetEntryGroupRequest(): GetEntryGroupRequest {
  return { name: "", readMask: undefined };
}

export const GetEntryGroupRequest: MessageFns<GetEntryGroupRequest> = {
  encode(message: GetEntryGroupRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.readMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.readMask), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetEntryGroupRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetEntryGroupRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.readMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetEntryGroupRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      readMask: isSet(object.readMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.readMask)) : undefined,
    };
  },

  toJSON(message: GetEntryGroupRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.readMask !== undefined) {
      obj.readMask = FieldMask.toJSON(FieldMask.wrap(message.readMask));
    }
    return obj;
  },

  create(base?: DeepPartial<GetEntryGroupRequest>): GetEntryGroupRequest {
    return GetEntryGroupRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetEntryGroupRequest>): GetEntryGroupRequest {
    const message = createBaseGetEntryGroupRequest();
    message.name = object.name ?? "";
    message.readMask = object.readMask ?? undefined;
    return message;
  },
};

function createBaseDeleteEntryGroupRequest(): DeleteEntryGroupRequest {
  return { name: "", force: false };
}

export const DeleteEntryGroupRequest: MessageFns<DeleteEntryGroupRequest> = {
  encode(message: DeleteEntryGroupRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.force !== false) {
      writer.uint32(16).bool(message.force);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeleteEntryGroupRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeleteEntryGroupRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.force = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeleteEntryGroupRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      force: isSet(object.force) ? globalThis.Boolean(object.force) : false,
    };
  },

  toJSON(message: DeleteEntryGroupRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.force !== false) {
      obj.force = message.force;
    }
    return obj;
  },

  create(base?: DeepPartial<DeleteEntryGroupRequest>): DeleteEntryGroupRequest {
    return DeleteEntryGroupRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeleteEntryGroupRequest>): DeleteEntryGroupRequest {
    const message = createBaseDeleteEntryGroupRequest();
    message.name = object.name ?? "";
    message.force = object.force ?? false;
    return message;
  },
};

function createBaseListEntryGroupsRequest(): ListEntryGroupsRequest {
  return { parent: "", pageSize: 0, pageToken: "" };
}

export const ListEntryGroupsRequest: MessageFns<ListEntryGroupsRequest> = {
  encode(message: ListEntryGroupsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.pageSize !== 0) {
      writer.uint32(16).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(26).string(message.pageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListEntryGroupsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListEntryGroupsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListEntryGroupsRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
    };
  },

  toJSON(message: ListEntryGroupsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListEntryGroupsRequest>): ListEntryGroupsRequest {
    return ListEntryGroupsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListEntryGroupsRequest>): ListEntryGroupsRequest {
    const message = createBaseListEntryGroupsRequest();
    message.parent = object.parent ?? "";
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    return message;
  },
};

function createBaseListEntryGroupsResponse(): ListEntryGroupsResponse {
  return { entryGroups: [], nextPageToken: "" };
}

export const ListEntryGroupsResponse: MessageFns<ListEntryGroupsResponse> = {
  encode(message: ListEntryGroupsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.entryGroups) {
      EntryGroup.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListEntryGroupsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListEntryGroupsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.entryGroups.push(EntryGroup.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListEntryGroupsResponse {
    return {
      entryGroups: globalThis.Array.isArray(object?.entryGroups)
        ? object.entryGroups.map((e: any) => EntryGroup.fromJSON(e))
        : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: ListEntryGroupsResponse): unknown {
    const obj: any = {};
    if (message.entryGroups?.length) {
      obj.entryGroups = message.entryGroups.map((e) => EntryGroup.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListEntryGroupsResponse>): ListEntryGroupsResponse {
    return ListEntryGroupsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListEntryGroupsResponse>): ListEntryGroupsResponse {
    const message = createBaseListEntryGroupsResponse();
    message.entryGroups = object.entryGroups?.map((e) => EntryGroup.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBaseCreateEntryRequest(): CreateEntryRequest {
  return { parent: "", entryId: "", entry: undefined };
}

export const CreateEntryRequest: MessageFns<CreateEntryRequest> = {
  encode(message: CreateEntryRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.entryId !== "") {
      writer.uint32(26).string(message.entryId);
    }
    if (message.entry !== undefined) {
      Entry.encode(message.entry, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateEntryRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateEntryRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.entryId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.entry = Entry.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateEntryRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      entryId: isSet(object.entryId) ? globalThis.String(object.entryId) : "",
      entry: isSet(object.entry) ? Entry.fromJSON(object.entry) : undefined,
    };
  },

  toJSON(message: CreateEntryRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.entryId !== "") {
      obj.entryId = message.entryId;
    }
    if (message.entry !== undefined) {
      obj.entry = Entry.toJSON(message.entry);
    }
    return obj;
  },

  create(base?: DeepPartial<CreateEntryRequest>): CreateEntryRequest {
    return CreateEntryRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateEntryRequest>): CreateEntryRequest {
    const message = createBaseCreateEntryRequest();
    message.parent = object.parent ?? "";
    message.entryId = object.entryId ?? "";
    message.entry = (object.entry !== undefined && object.entry !== null) ? Entry.fromPartial(object.entry) : undefined;
    return message;
  },
};

function createBaseUpdateEntryRequest(): UpdateEntryRequest {
  return { entry: undefined, updateMask: undefined };
}

export const UpdateEntryRequest: MessageFns<UpdateEntryRequest> = {
  encode(message: UpdateEntryRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.entry !== undefined) {
      Entry.encode(message.entry, writer.uint32(10).fork()).join();
    }
    if (message.updateMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.updateMask), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateEntryRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateEntryRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.entry = Entry.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.updateMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateEntryRequest {
    return {
      entry: isSet(object.entry) ? Entry.fromJSON(object.entry) : undefined,
      updateMask: isSet(object.updateMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.updateMask)) : undefined,
    };
  },

  toJSON(message: UpdateEntryRequest): unknown {
    const obj: any = {};
    if (message.entry !== undefined) {
      obj.entry = Entry.toJSON(message.entry);
    }
    if (message.updateMask !== undefined) {
      obj.updateMask = FieldMask.toJSON(FieldMask.wrap(message.updateMask));
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateEntryRequest>): UpdateEntryRequest {
    return UpdateEntryRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateEntryRequest>): UpdateEntryRequest {
    const message = createBaseUpdateEntryRequest();
    message.entry = (object.entry !== undefined && object.entry !== null) ? Entry.fromPartial(object.entry) : undefined;
    message.updateMask = object.updateMask ?? undefined;
    return message;
  },
};

function createBaseDeleteEntryRequest(): DeleteEntryRequest {
  return { name: "" };
}

export const DeleteEntryRequest: MessageFns<DeleteEntryRequest> = {
  encode(message: DeleteEntryRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeleteEntryRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeleteEntryRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeleteEntryRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: DeleteEntryRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<DeleteEntryRequest>): DeleteEntryRequest {
    return DeleteEntryRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeleteEntryRequest>): DeleteEntryRequest {
    const message = createBaseDeleteEntryRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseGetEntryRequest(): GetEntryRequest {
  return { name: "" };
}

export const GetEntryRequest: MessageFns<GetEntryRequest> = {
  encode(message: GetEntryRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetEntryRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetEntryRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetEntryRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: GetEntryRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<GetEntryRequest>): GetEntryRequest {
    return GetEntryRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetEntryRequest>): GetEntryRequest {
    const message = createBaseGetEntryRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseLookupEntryRequest(): LookupEntryRequest {
  return {
    linkedResource: undefined,
    sqlResource: undefined,
    fullyQualifiedName: undefined,
    project: "",
    location: "",
  };
}

export const LookupEntryRequest: MessageFns<LookupEntryRequest> = {
  encode(message: LookupEntryRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.linkedResource !== undefined) {
      writer.uint32(10).string(message.linkedResource);
    }
    if (message.sqlResource !== undefined) {
      writer.uint32(26).string(message.sqlResource);
    }
    if (message.fullyQualifiedName !== undefined) {
      writer.uint32(42).string(message.fullyQualifiedName);
    }
    if (message.project !== "") {
      writer.uint32(50).string(message.project);
    }
    if (message.location !== "") {
      writer.uint32(58).string(message.location);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LookupEntryRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLookupEntryRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.linkedResource = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.sqlResource = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.fullyQualifiedName = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.project = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.location = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LookupEntryRequest {
    return {
      linkedResource: isSet(object.linkedResource) ? globalThis.String(object.linkedResource) : undefined,
      sqlResource: isSet(object.sqlResource) ? globalThis.String(object.sqlResource) : undefined,
      fullyQualifiedName: isSet(object.fullyQualifiedName) ? globalThis.String(object.fullyQualifiedName) : undefined,
      project: isSet(object.project) ? globalThis.String(object.project) : "",
      location: isSet(object.location) ? globalThis.String(object.location) : "",
    };
  },

  toJSON(message: LookupEntryRequest): unknown {
    const obj: any = {};
    if (message.linkedResource !== undefined) {
      obj.linkedResource = message.linkedResource;
    }
    if (message.sqlResource !== undefined) {
      obj.sqlResource = message.sqlResource;
    }
    if (message.fullyQualifiedName !== undefined) {
      obj.fullyQualifiedName = message.fullyQualifiedName;
    }
    if (message.project !== "") {
      obj.project = message.project;
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    return obj;
  },

  create(base?: DeepPartial<LookupEntryRequest>): LookupEntryRequest {
    return LookupEntryRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LookupEntryRequest>): LookupEntryRequest {
    const message = createBaseLookupEntryRequest();
    message.linkedResource = object.linkedResource ?? undefined;
    message.sqlResource = object.sqlResource ?? undefined;
    message.fullyQualifiedName = object.fullyQualifiedName ?? undefined;
    message.project = object.project ?? "";
    message.location = object.location ?? "";
    return message;
  },
};

function createBaseEntry(): Entry {
  return {
    name: "",
    linkedResource: "",
    fullyQualifiedName: "",
    type: undefined,
    userSpecifiedType: undefined,
    integratedSystem: undefined,
    userSpecifiedSystem: undefined,
    sqlDatabaseSystemSpec: undefined,
    lookerSystemSpec: undefined,
    cloudBigtableSystemSpec: undefined,
    gcsFilesetSpec: undefined,
    bigqueryTableSpec: undefined,
    bigqueryDateShardedSpec: undefined,
    databaseTableSpec: undefined,
    dataSourceConnectionSpec: undefined,
    routineSpec: undefined,
    datasetSpec: undefined,
    filesetSpec: undefined,
    serviceSpec: undefined,
    modelSpec: undefined,
    displayName: "",
    description: "",
    businessContext: undefined,
    schema: undefined,
    sourceSystemTimestamps: undefined,
    usageSignal: undefined,
    labels: {},
    dataSource: undefined,
    personalDetails: undefined,
  };
}

export const Entry: MessageFns<Entry> = {
  encode(message: Entry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.linkedResource !== "") {
      writer.uint32(74).string(message.linkedResource);
    }
    if (message.fullyQualifiedName !== "") {
      writer.uint32(234).string(message.fullyQualifiedName);
    }
    if (message.type !== undefined) {
      writer.uint32(16).int32(message.type);
    }
    if (message.userSpecifiedType !== undefined) {
      writer.uint32(130).string(message.userSpecifiedType);
    }
    if (message.integratedSystem !== undefined) {
      writer.uint32(136).int32(message.integratedSystem);
    }
    if (message.userSpecifiedSystem !== undefined) {
      writer.uint32(146).string(message.userSpecifiedSystem);
    }
    if (message.sqlDatabaseSystemSpec !== undefined) {
      SqlDatabaseSystemSpec.encode(message.sqlDatabaseSystemSpec, writer.uint32(314).fork()).join();
    }
    if (message.lookerSystemSpec !== undefined) {
      LookerSystemSpec.encode(message.lookerSystemSpec, writer.uint32(322).fork()).join();
    }
    if (message.cloudBigtableSystemSpec !== undefined) {
      CloudBigtableSystemSpec.encode(message.cloudBigtableSystemSpec, writer.uint32(330).fork()).join();
    }
    if (message.gcsFilesetSpec !== undefined) {
      GcsFilesetSpec.encode(message.gcsFilesetSpec, writer.uint32(50).fork()).join();
    }
    if (message.bigqueryTableSpec !== undefined) {
      BigQueryTableSpec.encode(message.bigqueryTableSpec, writer.uint32(98).fork()).join();
    }
    if (message.bigqueryDateShardedSpec !== undefined) {
      BigQueryDateShardedSpec.encode(message.bigqueryDateShardedSpec, writer.uint32(122).fork()).join();
    }
    if (message.databaseTableSpec !== undefined) {
      DatabaseTableSpec.encode(message.databaseTableSpec, writer.uint32(194).fork()).join();
    }
    if (message.dataSourceConnectionSpec !== undefined) {
      DataSourceConnectionSpec.encode(message.dataSourceConnectionSpec, writer.uint32(218).fork()).join();
    }
    if (message.routineSpec !== undefined) {
      RoutineSpec.encode(message.routineSpec, writer.uint32(226).fork()).join();
    }
    if (message.datasetSpec !== undefined) {
      DatasetSpec.encode(message.datasetSpec, writer.uint32(258).fork()).join();
    }
    if (message.filesetSpec !== undefined) {
      FilesetSpec.encode(message.filesetSpec, writer.uint32(266).fork()).join();
    }
    if (message.serviceSpec !== undefined) {
      ServiceSpec.encode(message.serviceSpec, writer.uint32(338).fork()).join();
    }
    if (message.modelSpec !== undefined) {
      ModelSpec.encode(message.modelSpec, writer.uint32(346).fork()).join();
    }
    if (message.displayName !== "") {
      writer.uint32(26).string(message.displayName);
    }
    if (message.description !== "") {
      writer.uint32(34).string(message.description);
    }
    if (message.businessContext !== undefined) {
      BusinessContext.encode(message.businessContext, writer.uint32(298).fork()).join();
    }
    if (message.schema !== undefined) {
      Schema.encode(message.schema, writer.uint32(42).fork()).join();
    }
    if (message.sourceSystemTimestamps !== undefined) {
      SystemTimestamps.encode(message.sourceSystemTimestamps, writer.uint32(58).fork()).join();
    }
    if (message.usageSignal !== undefined) {
      UsageSignal.encode(message.usageSignal, writer.uint32(106).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      Entry_LabelsEntry.encode({ key: key as any, value }, writer.uint32(114).fork()).join();
    });
    if (message.dataSource !== undefined) {
      DataSource.encode(message.dataSource, writer.uint32(162).fork()).join();
    }
    if (message.personalDetails !== undefined) {
      PersonalDetails.encode(message.personalDetails, writer.uint32(210).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Entry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.linkedResource = reader.string();
          continue;
        case 29:
          if (tag !== 234) {
            break;
          }

          message.fullyQualifiedName = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.userSpecifiedType = reader.string();
          continue;
        case 17:
          if (tag !== 136) {
            break;
          }

          message.integratedSystem = reader.int32() as any;
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.userSpecifiedSystem = reader.string();
          continue;
        case 39:
          if (tag !== 314) {
            break;
          }

          message.sqlDatabaseSystemSpec = SqlDatabaseSystemSpec.decode(reader, reader.uint32());
          continue;
        case 40:
          if (tag !== 322) {
            break;
          }

          message.lookerSystemSpec = LookerSystemSpec.decode(reader, reader.uint32());
          continue;
        case 41:
          if (tag !== 330) {
            break;
          }

          message.cloudBigtableSystemSpec = CloudBigtableSystemSpec.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.gcsFilesetSpec = GcsFilesetSpec.decode(reader, reader.uint32());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.bigqueryTableSpec = BigQueryTableSpec.decode(reader, reader.uint32());
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.bigqueryDateShardedSpec = BigQueryDateShardedSpec.decode(reader, reader.uint32());
          continue;
        case 24:
          if (tag !== 194) {
            break;
          }

          message.databaseTableSpec = DatabaseTableSpec.decode(reader, reader.uint32());
          continue;
        case 27:
          if (tag !== 218) {
            break;
          }

          message.dataSourceConnectionSpec = DataSourceConnectionSpec.decode(reader, reader.uint32());
          continue;
        case 28:
          if (tag !== 226) {
            break;
          }

          message.routineSpec = RoutineSpec.decode(reader, reader.uint32());
          continue;
        case 32:
          if (tag !== 258) {
            break;
          }

          message.datasetSpec = DatasetSpec.decode(reader, reader.uint32());
          continue;
        case 33:
          if (tag !== 266) {
            break;
          }

          message.filesetSpec = FilesetSpec.decode(reader, reader.uint32());
          continue;
        case 42:
          if (tag !== 338) {
            break;
          }

          message.serviceSpec = ServiceSpec.decode(reader, reader.uint32());
          continue;
        case 43:
          if (tag !== 346) {
            break;
          }

          message.modelSpec = ModelSpec.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.description = reader.string();
          continue;
        case 37:
          if (tag !== 298) {
            break;
          }

          message.businessContext = BusinessContext.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.schema = Schema.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.sourceSystemTimestamps = SystemTimestamps.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.usageSignal = UsageSignal.decode(reader, reader.uint32());
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          const entry14 = Entry_LabelsEntry.decode(reader, reader.uint32());
          if (entry14.value !== undefined) {
            message.labels[entry14.key] = entry14.value;
          }
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.dataSource = DataSource.decode(reader, reader.uint32());
          continue;
        case 26:
          if (tag !== 210) {
            break;
          }

          message.personalDetails = PersonalDetails.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Entry {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      linkedResource: isSet(object.linkedResource) ? globalThis.String(object.linkedResource) : "",
      fullyQualifiedName: isSet(object.fullyQualifiedName) ? globalThis.String(object.fullyQualifiedName) : "",
      type: isSet(object.type) ? entryTypeFromJSON(object.type) : undefined,
      userSpecifiedType: isSet(object.userSpecifiedType) ? globalThis.String(object.userSpecifiedType) : undefined,
      integratedSystem: isSet(object.integratedSystem) ? integratedSystemFromJSON(object.integratedSystem) : undefined,
      userSpecifiedSystem: isSet(object.userSpecifiedSystem)
        ? globalThis.String(object.userSpecifiedSystem)
        : undefined,
      sqlDatabaseSystemSpec: isSet(object.sqlDatabaseSystemSpec)
        ? SqlDatabaseSystemSpec.fromJSON(object.sqlDatabaseSystemSpec)
        : undefined,
      lookerSystemSpec: isSet(object.lookerSystemSpec) ? LookerSystemSpec.fromJSON(object.lookerSystemSpec) : undefined,
      cloudBigtableSystemSpec: isSet(object.cloudBigtableSystemSpec)
        ? CloudBigtableSystemSpec.fromJSON(object.cloudBigtableSystemSpec)
        : undefined,
      gcsFilesetSpec: isSet(object.gcsFilesetSpec) ? GcsFilesetSpec.fromJSON(object.gcsFilesetSpec) : undefined,
      bigqueryTableSpec: isSet(object.bigqueryTableSpec)
        ? BigQueryTableSpec.fromJSON(object.bigqueryTableSpec)
        : undefined,
      bigqueryDateShardedSpec: isSet(object.bigqueryDateShardedSpec)
        ? BigQueryDateShardedSpec.fromJSON(object.bigqueryDateShardedSpec)
        : undefined,
      databaseTableSpec: isSet(object.databaseTableSpec)
        ? DatabaseTableSpec.fromJSON(object.databaseTableSpec)
        : undefined,
      dataSourceConnectionSpec: isSet(object.dataSourceConnectionSpec)
        ? DataSourceConnectionSpec.fromJSON(object.dataSourceConnectionSpec)
        : undefined,
      routineSpec: isSet(object.routineSpec) ? RoutineSpec.fromJSON(object.routineSpec) : undefined,
      datasetSpec: isSet(object.datasetSpec) ? DatasetSpec.fromJSON(object.datasetSpec) : undefined,
      filesetSpec: isSet(object.filesetSpec) ? FilesetSpec.fromJSON(object.filesetSpec) : undefined,
      serviceSpec: isSet(object.serviceSpec) ? ServiceSpec.fromJSON(object.serviceSpec) : undefined,
      modelSpec: isSet(object.modelSpec) ? ModelSpec.fromJSON(object.modelSpec) : undefined,
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      businessContext: isSet(object.businessContext) ? BusinessContext.fromJSON(object.businessContext) : undefined,
      schema: isSet(object.schema) ? Schema.fromJSON(object.schema) : undefined,
      sourceSystemTimestamps: isSet(object.sourceSystemTimestamps)
        ? SystemTimestamps.fromJSON(object.sourceSystemTimestamps)
        : undefined,
      usageSignal: isSet(object.usageSignal) ? UsageSignal.fromJSON(object.usageSignal) : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      dataSource: isSet(object.dataSource) ? DataSource.fromJSON(object.dataSource) : undefined,
      personalDetails: isSet(object.personalDetails) ? PersonalDetails.fromJSON(object.personalDetails) : undefined,
    };
  },

  toJSON(message: Entry): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.linkedResource !== "") {
      obj.linkedResource = message.linkedResource;
    }
    if (message.fullyQualifiedName !== "") {
      obj.fullyQualifiedName = message.fullyQualifiedName;
    }
    if (message.type !== undefined) {
      obj.type = entryTypeToJSON(message.type);
    }
    if (message.userSpecifiedType !== undefined) {
      obj.userSpecifiedType = message.userSpecifiedType;
    }
    if (message.integratedSystem !== undefined) {
      obj.integratedSystem = integratedSystemToJSON(message.integratedSystem);
    }
    if (message.userSpecifiedSystem !== undefined) {
      obj.userSpecifiedSystem = message.userSpecifiedSystem;
    }
    if (message.sqlDatabaseSystemSpec !== undefined) {
      obj.sqlDatabaseSystemSpec = SqlDatabaseSystemSpec.toJSON(message.sqlDatabaseSystemSpec);
    }
    if (message.lookerSystemSpec !== undefined) {
      obj.lookerSystemSpec = LookerSystemSpec.toJSON(message.lookerSystemSpec);
    }
    if (message.cloudBigtableSystemSpec !== undefined) {
      obj.cloudBigtableSystemSpec = CloudBigtableSystemSpec.toJSON(message.cloudBigtableSystemSpec);
    }
    if (message.gcsFilesetSpec !== undefined) {
      obj.gcsFilesetSpec = GcsFilesetSpec.toJSON(message.gcsFilesetSpec);
    }
    if (message.bigqueryTableSpec !== undefined) {
      obj.bigqueryTableSpec = BigQueryTableSpec.toJSON(message.bigqueryTableSpec);
    }
    if (message.bigqueryDateShardedSpec !== undefined) {
      obj.bigqueryDateShardedSpec = BigQueryDateShardedSpec.toJSON(message.bigqueryDateShardedSpec);
    }
    if (message.databaseTableSpec !== undefined) {
      obj.databaseTableSpec = DatabaseTableSpec.toJSON(message.databaseTableSpec);
    }
    if (message.dataSourceConnectionSpec !== undefined) {
      obj.dataSourceConnectionSpec = DataSourceConnectionSpec.toJSON(message.dataSourceConnectionSpec);
    }
    if (message.routineSpec !== undefined) {
      obj.routineSpec = RoutineSpec.toJSON(message.routineSpec);
    }
    if (message.datasetSpec !== undefined) {
      obj.datasetSpec = DatasetSpec.toJSON(message.datasetSpec);
    }
    if (message.filesetSpec !== undefined) {
      obj.filesetSpec = FilesetSpec.toJSON(message.filesetSpec);
    }
    if (message.serviceSpec !== undefined) {
      obj.serviceSpec = ServiceSpec.toJSON(message.serviceSpec);
    }
    if (message.modelSpec !== undefined) {
      obj.modelSpec = ModelSpec.toJSON(message.modelSpec);
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.businessContext !== undefined) {
      obj.businessContext = BusinessContext.toJSON(message.businessContext);
    }
    if (message.schema !== undefined) {
      obj.schema = Schema.toJSON(message.schema);
    }
    if (message.sourceSystemTimestamps !== undefined) {
      obj.sourceSystemTimestamps = SystemTimestamps.toJSON(message.sourceSystemTimestamps);
    }
    if (message.usageSignal !== undefined) {
      obj.usageSignal = UsageSignal.toJSON(message.usageSignal);
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.dataSource !== undefined) {
      obj.dataSource = DataSource.toJSON(message.dataSource);
    }
    if (message.personalDetails !== undefined) {
      obj.personalDetails = PersonalDetails.toJSON(message.personalDetails);
    }
    return obj;
  },

  create(base?: DeepPartial<Entry>): Entry {
    return Entry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Entry>): Entry {
    const message = createBaseEntry();
    message.name = object.name ?? "";
    message.linkedResource = object.linkedResource ?? "";
    message.fullyQualifiedName = object.fullyQualifiedName ?? "";
    message.type = object.type ?? undefined;
    message.userSpecifiedType = object.userSpecifiedType ?? undefined;
    message.integratedSystem = object.integratedSystem ?? undefined;
    message.userSpecifiedSystem = object.userSpecifiedSystem ?? undefined;
    message.sqlDatabaseSystemSpec =
      (object.sqlDatabaseSystemSpec !== undefined && object.sqlDatabaseSystemSpec !== null)
        ? SqlDatabaseSystemSpec.fromPartial(object.sqlDatabaseSystemSpec)
        : undefined;
    message.lookerSystemSpec = (object.lookerSystemSpec !== undefined && object.lookerSystemSpec !== null)
      ? LookerSystemSpec.fromPartial(object.lookerSystemSpec)
      : undefined;
    message.cloudBigtableSystemSpec =
      (object.cloudBigtableSystemSpec !== undefined && object.cloudBigtableSystemSpec !== null)
        ? CloudBigtableSystemSpec.fromPartial(object.cloudBigtableSystemSpec)
        : undefined;
    message.gcsFilesetSpec = (object.gcsFilesetSpec !== undefined && object.gcsFilesetSpec !== null)
      ? GcsFilesetSpec.fromPartial(object.gcsFilesetSpec)
      : undefined;
    message.bigqueryTableSpec = (object.bigqueryTableSpec !== undefined && object.bigqueryTableSpec !== null)
      ? BigQueryTableSpec.fromPartial(object.bigqueryTableSpec)
      : undefined;
    message.bigqueryDateShardedSpec =
      (object.bigqueryDateShardedSpec !== undefined && object.bigqueryDateShardedSpec !== null)
        ? BigQueryDateShardedSpec.fromPartial(object.bigqueryDateShardedSpec)
        : undefined;
    message.databaseTableSpec = (object.databaseTableSpec !== undefined && object.databaseTableSpec !== null)
      ? DatabaseTableSpec.fromPartial(object.databaseTableSpec)
      : undefined;
    message.dataSourceConnectionSpec =
      (object.dataSourceConnectionSpec !== undefined && object.dataSourceConnectionSpec !== null)
        ? DataSourceConnectionSpec.fromPartial(object.dataSourceConnectionSpec)
        : undefined;
    message.routineSpec = (object.routineSpec !== undefined && object.routineSpec !== null)
      ? RoutineSpec.fromPartial(object.routineSpec)
      : undefined;
    message.datasetSpec = (object.datasetSpec !== undefined && object.datasetSpec !== null)
      ? DatasetSpec.fromPartial(object.datasetSpec)
      : undefined;
    message.filesetSpec = (object.filesetSpec !== undefined && object.filesetSpec !== null)
      ? FilesetSpec.fromPartial(object.filesetSpec)
      : undefined;
    message.serviceSpec = (object.serviceSpec !== undefined && object.serviceSpec !== null)
      ? ServiceSpec.fromPartial(object.serviceSpec)
      : undefined;
    message.modelSpec = (object.modelSpec !== undefined && object.modelSpec !== null)
      ? ModelSpec.fromPartial(object.modelSpec)
      : undefined;
    message.displayName = object.displayName ?? "";
    message.description = object.description ?? "";
    message.businessContext = (object.businessContext !== undefined && object.businessContext !== null)
      ? BusinessContext.fromPartial(object.businessContext)
      : undefined;
    message.schema = (object.schema !== undefined && object.schema !== null)
      ? Schema.fromPartial(object.schema)
      : undefined;
    message.sourceSystemTimestamps =
      (object.sourceSystemTimestamps !== undefined && object.sourceSystemTimestamps !== null)
        ? SystemTimestamps.fromPartial(object.sourceSystemTimestamps)
        : undefined;
    message.usageSignal = (object.usageSignal !== undefined && object.usageSignal !== null)
      ? UsageSignal.fromPartial(object.usageSignal)
      : undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.dataSource = (object.dataSource !== undefined && object.dataSource !== null)
      ? DataSource.fromPartial(object.dataSource)
      : undefined;
    message.personalDetails = (object.personalDetails !== undefined && object.personalDetails !== null)
      ? PersonalDetails.fromPartial(object.personalDetails)
      : undefined;
    return message;
  },
};

function createBaseEntry_LabelsEntry(): Entry_LabelsEntry {
  return { key: "", value: "" };
}

export const Entry_LabelsEntry: MessageFns<Entry_LabelsEntry> = {
  encode(message: Entry_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Entry_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEntry_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Entry_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Entry_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Entry_LabelsEntry>): Entry_LabelsEntry {
    return Entry_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Entry_LabelsEntry>): Entry_LabelsEntry {
    const message = createBaseEntry_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseDatabaseTableSpec(): DatabaseTableSpec {
  return { type: 0, dataplexTable: undefined, databaseViewSpec: undefined };
}

export const DatabaseTableSpec: MessageFns<DatabaseTableSpec> = {
  encode(message: DatabaseTableSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.type !== 0) {
      writer.uint32(8).int32(message.type);
    }
    if (message.dataplexTable !== undefined) {
      DataplexTableSpec.encode(message.dataplexTable, writer.uint32(18).fork()).join();
    }
    if (message.databaseViewSpec !== undefined) {
      DatabaseTableSpec_DatabaseViewSpec.encode(message.databaseViewSpec, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatabaseTableSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatabaseTableSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.dataplexTable = DataplexTableSpec.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.databaseViewSpec = DatabaseTableSpec_DatabaseViewSpec.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatabaseTableSpec {
    return {
      type: isSet(object.type) ? databaseTableSpec_TableTypeFromJSON(object.type) : 0,
      dataplexTable: isSet(object.dataplexTable) ? DataplexTableSpec.fromJSON(object.dataplexTable) : undefined,
      databaseViewSpec: isSet(object.databaseViewSpec)
        ? DatabaseTableSpec_DatabaseViewSpec.fromJSON(object.databaseViewSpec)
        : undefined,
    };
  },

  toJSON(message: DatabaseTableSpec): unknown {
    const obj: any = {};
    if (message.type !== 0) {
      obj.type = databaseTableSpec_TableTypeToJSON(message.type);
    }
    if (message.dataplexTable !== undefined) {
      obj.dataplexTable = DataplexTableSpec.toJSON(message.dataplexTable);
    }
    if (message.databaseViewSpec !== undefined) {
      obj.databaseViewSpec = DatabaseTableSpec_DatabaseViewSpec.toJSON(message.databaseViewSpec);
    }
    return obj;
  },

  create(base?: DeepPartial<DatabaseTableSpec>): DatabaseTableSpec {
    return DatabaseTableSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatabaseTableSpec>): DatabaseTableSpec {
    const message = createBaseDatabaseTableSpec();
    message.type = object.type ?? 0;
    message.dataplexTable = (object.dataplexTable !== undefined && object.dataplexTable !== null)
      ? DataplexTableSpec.fromPartial(object.dataplexTable)
      : undefined;
    message.databaseViewSpec = (object.databaseViewSpec !== undefined && object.databaseViewSpec !== null)
      ? DatabaseTableSpec_DatabaseViewSpec.fromPartial(object.databaseViewSpec)
      : undefined;
    return message;
  },
};

function createBaseDatabaseTableSpec_DatabaseViewSpec(): DatabaseTableSpec_DatabaseViewSpec {
  return { viewType: 0, baseTable: undefined, sqlQuery: undefined };
}

export const DatabaseTableSpec_DatabaseViewSpec: MessageFns<DatabaseTableSpec_DatabaseViewSpec> = {
  encode(message: DatabaseTableSpec_DatabaseViewSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.viewType !== 0) {
      writer.uint32(8).int32(message.viewType);
    }
    if (message.baseTable !== undefined) {
      writer.uint32(18).string(message.baseTable);
    }
    if (message.sqlQuery !== undefined) {
      writer.uint32(26).string(message.sqlQuery);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatabaseTableSpec_DatabaseViewSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatabaseTableSpec_DatabaseViewSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.viewType = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.baseTable = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.sqlQuery = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatabaseTableSpec_DatabaseViewSpec {
    return {
      viewType: isSet(object.viewType) ? databaseTableSpec_DatabaseViewSpec_ViewTypeFromJSON(object.viewType) : 0,
      baseTable: isSet(object.baseTable) ? globalThis.String(object.baseTable) : undefined,
      sqlQuery: isSet(object.sqlQuery) ? globalThis.String(object.sqlQuery) : undefined,
    };
  },

  toJSON(message: DatabaseTableSpec_DatabaseViewSpec): unknown {
    const obj: any = {};
    if (message.viewType !== 0) {
      obj.viewType = databaseTableSpec_DatabaseViewSpec_ViewTypeToJSON(message.viewType);
    }
    if (message.baseTable !== undefined) {
      obj.baseTable = message.baseTable;
    }
    if (message.sqlQuery !== undefined) {
      obj.sqlQuery = message.sqlQuery;
    }
    return obj;
  },

  create(base?: DeepPartial<DatabaseTableSpec_DatabaseViewSpec>): DatabaseTableSpec_DatabaseViewSpec {
    return DatabaseTableSpec_DatabaseViewSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatabaseTableSpec_DatabaseViewSpec>): DatabaseTableSpec_DatabaseViewSpec {
    const message = createBaseDatabaseTableSpec_DatabaseViewSpec();
    message.viewType = object.viewType ?? 0;
    message.baseTable = object.baseTable ?? undefined;
    message.sqlQuery = object.sqlQuery ?? undefined;
    return message;
  },
};

function createBaseFilesetSpec(): FilesetSpec {
  return { dataplexFileset: undefined };
}

export const FilesetSpec: MessageFns<FilesetSpec> = {
  encode(message: FilesetSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dataplexFileset !== undefined) {
      DataplexFilesetSpec.encode(message.dataplexFileset, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FilesetSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFilesetSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dataplexFileset = DataplexFilesetSpec.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FilesetSpec {
    return {
      dataplexFileset: isSet(object.dataplexFileset) ? DataplexFilesetSpec.fromJSON(object.dataplexFileset) : undefined,
    };
  },

  toJSON(message: FilesetSpec): unknown {
    const obj: any = {};
    if (message.dataplexFileset !== undefined) {
      obj.dataplexFileset = DataplexFilesetSpec.toJSON(message.dataplexFileset);
    }
    return obj;
  },

  create(base?: DeepPartial<FilesetSpec>): FilesetSpec {
    return FilesetSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<FilesetSpec>): FilesetSpec {
    const message = createBaseFilesetSpec();
    message.dataplexFileset = (object.dataplexFileset !== undefined && object.dataplexFileset !== null)
      ? DataplexFilesetSpec.fromPartial(object.dataplexFileset)
      : undefined;
    return message;
  },
};

function createBaseDataSourceConnectionSpec(): DataSourceConnectionSpec {
  return { bigqueryConnectionSpec: undefined };
}

export const DataSourceConnectionSpec: MessageFns<DataSourceConnectionSpec> = {
  encode(message: DataSourceConnectionSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.bigqueryConnectionSpec !== undefined) {
      BigQueryConnectionSpec.encode(message.bigqueryConnectionSpec, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DataSourceConnectionSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDataSourceConnectionSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.bigqueryConnectionSpec = BigQueryConnectionSpec.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DataSourceConnectionSpec {
    return {
      bigqueryConnectionSpec: isSet(object.bigqueryConnectionSpec)
        ? BigQueryConnectionSpec.fromJSON(object.bigqueryConnectionSpec)
        : undefined,
    };
  },

  toJSON(message: DataSourceConnectionSpec): unknown {
    const obj: any = {};
    if (message.bigqueryConnectionSpec !== undefined) {
      obj.bigqueryConnectionSpec = BigQueryConnectionSpec.toJSON(message.bigqueryConnectionSpec);
    }
    return obj;
  },

  create(base?: DeepPartial<DataSourceConnectionSpec>): DataSourceConnectionSpec {
    return DataSourceConnectionSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DataSourceConnectionSpec>): DataSourceConnectionSpec {
    const message = createBaseDataSourceConnectionSpec();
    message.bigqueryConnectionSpec =
      (object.bigqueryConnectionSpec !== undefined && object.bigqueryConnectionSpec !== null)
        ? BigQueryConnectionSpec.fromPartial(object.bigqueryConnectionSpec)
        : undefined;
    return message;
  },
};

function createBaseRoutineSpec(): RoutineSpec {
  return {
    routineType: 0,
    language: "",
    routineArguments: [],
    returnType: "",
    definitionBody: "",
    bigqueryRoutineSpec: undefined,
  };
}

export const RoutineSpec: MessageFns<RoutineSpec> = {
  encode(message: RoutineSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.routineType !== 0) {
      writer.uint32(8).int32(message.routineType);
    }
    if (message.language !== "") {
      writer.uint32(18).string(message.language);
    }
    for (const v of message.routineArguments) {
      RoutineSpec_Argument.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.returnType !== "") {
      writer.uint32(34).string(message.returnType);
    }
    if (message.definitionBody !== "") {
      writer.uint32(42).string(message.definitionBody);
    }
    if (message.bigqueryRoutineSpec !== undefined) {
      BigQueryRoutineSpec.encode(message.bigqueryRoutineSpec, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RoutineSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRoutineSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.routineType = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.language = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.routineArguments.push(RoutineSpec_Argument.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.returnType = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.definitionBody = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.bigqueryRoutineSpec = BigQueryRoutineSpec.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RoutineSpec {
    return {
      routineType: isSet(object.routineType) ? routineSpec_RoutineTypeFromJSON(object.routineType) : 0,
      language: isSet(object.language) ? globalThis.String(object.language) : "",
      routineArguments: globalThis.Array.isArray(object?.routineArguments)
        ? object.routineArguments.map((e: any) => RoutineSpec_Argument.fromJSON(e))
        : [],
      returnType: isSet(object.returnType) ? globalThis.String(object.returnType) : "",
      definitionBody: isSet(object.definitionBody) ? globalThis.String(object.definitionBody) : "",
      bigqueryRoutineSpec: isSet(object.bigqueryRoutineSpec)
        ? BigQueryRoutineSpec.fromJSON(object.bigqueryRoutineSpec)
        : undefined,
    };
  },

  toJSON(message: RoutineSpec): unknown {
    const obj: any = {};
    if (message.routineType !== 0) {
      obj.routineType = routineSpec_RoutineTypeToJSON(message.routineType);
    }
    if (message.language !== "") {
      obj.language = message.language;
    }
    if (message.routineArguments?.length) {
      obj.routineArguments = message.routineArguments.map((e) => RoutineSpec_Argument.toJSON(e));
    }
    if (message.returnType !== "") {
      obj.returnType = message.returnType;
    }
    if (message.definitionBody !== "") {
      obj.definitionBody = message.definitionBody;
    }
    if (message.bigqueryRoutineSpec !== undefined) {
      obj.bigqueryRoutineSpec = BigQueryRoutineSpec.toJSON(message.bigqueryRoutineSpec);
    }
    return obj;
  },

  create(base?: DeepPartial<RoutineSpec>): RoutineSpec {
    return RoutineSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RoutineSpec>): RoutineSpec {
    const message = createBaseRoutineSpec();
    message.routineType = object.routineType ?? 0;
    message.language = object.language ?? "";
    message.routineArguments = object.routineArguments?.map((e) => RoutineSpec_Argument.fromPartial(e)) || [];
    message.returnType = object.returnType ?? "";
    message.definitionBody = object.definitionBody ?? "";
    message.bigqueryRoutineSpec = (object.bigqueryRoutineSpec !== undefined && object.bigqueryRoutineSpec !== null)
      ? BigQueryRoutineSpec.fromPartial(object.bigqueryRoutineSpec)
      : undefined;
    return message;
  },
};

function createBaseRoutineSpec_Argument(): RoutineSpec_Argument {
  return { name: "", mode: 0, type: "" };
}

export const RoutineSpec_Argument: MessageFns<RoutineSpec_Argument> = {
  encode(message: RoutineSpec_Argument, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.mode !== 0) {
      writer.uint32(16).int32(message.mode);
    }
    if (message.type !== "") {
      writer.uint32(26).string(message.type);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RoutineSpec_Argument {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRoutineSpec_Argument();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.mode = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.type = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RoutineSpec_Argument {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      mode: isSet(object.mode) ? routineSpec_Argument_ModeFromJSON(object.mode) : 0,
      type: isSet(object.type) ? globalThis.String(object.type) : "",
    };
  },

  toJSON(message: RoutineSpec_Argument): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.mode !== 0) {
      obj.mode = routineSpec_Argument_ModeToJSON(message.mode);
    }
    if (message.type !== "") {
      obj.type = message.type;
    }
    return obj;
  },

  create(base?: DeepPartial<RoutineSpec_Argument>): RoutineSpec_Argument {
    return RoutineSpec_Argument.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RoutineSpec_Argument>): RoutineSpec_Argument {
    const message = createBaseRoutineSpec_Argument();
    message.name = object.name ?? "";
    message.mode = object.mode ?? 0;
    message.type = object.type ?? "";
    return message;
  },
};

function createBaseDatasetSpec(): DatasetSpec {
  return { vertexDatasetSpec: undefined };
}

export const DatasetSpec: MessageFns<DatasetSpec> = {
  encode(message: DatasetSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.vertexDatasetSpec !== undefined) {
      VertexDatasetSpec.encode(message.vertexDatasetSpec, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatasetSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatasetSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.vertexDatasetSpec = VertexDatasetSpec.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatasetSpec {
    return {
      vertexDatasetSpec: isSet(object.vertexDatasetSpec)
        ? VertexDatasetSpec.fromJSON(object.vertexDatasetSpec)
        : undefined,
    };
  },

  toJSON(message: DatasetSpec): unknown {
    const obj: any = {};
    if (message.vertexDatasetSpec !== undefined) {
      obj.vertexDatasetSpec = VertexDatasetSpec.toJSON(message.vertexDatasetSpec);
    }
    return obj;
  },

  create(base?: DeepPartial<DatasetSpec>): DatasetSpec {
    return DatasetSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatasetSpec>): DatasetSpec {
    const message = createBaseDatasetSpec();
    message.vertexDatasetSpec = (object.vertexDatasetSpec !== undefined && object.vertexDatasetSpec !== null)
      ? VertexDatasetSpec.fromPartial(object.vertexDatasetSpec)
      : undefined;
    return message;
  },
};

function createBaseSqlDatabaseSystemSpec(): SqlDatabaseSystemSpec {
  return { sqlEngine: "", databaseVersion: "", instanceHost: "" };
}

export const SqlDatabaseSystemSpec: MessageFns<SqlDatabaseSystemSpec> = {
  encode(message: SqlDatabaseSystemSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.sqlEngine !== "") {
      writer.uint32(10).string(message.sqlEngine);
    }
    if (message.databaseVersion !== "") {
      writer.uint32(18).string(message.databaseVersion);
    }
    if (message.instanceHost !== "") {
      writer.uint32(26).string(message.instanceHost);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SqlDatabaseSystemSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSqlDatabaseSystemSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.sqlEngine = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.databaseVersion = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.instanceHost = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SqlDatabaseSystemSpec {
    return {
      sqlEngine: isSet(object.sqlEngine) ? globalThis.String(object.sqlEngine) : "",
      databaseVersion: isSet(object.databaseVersion) ? globalThis.String(object.databaseVersion) : "",
      instanceHost: isSet(object.instanceHost) ? globalThis.String(object.instanceHost) : "",
    };
  },

  toJSON(message: SqlDatabaseSystemSpec): unknown {
    const obj: any = {};
    if (message.sqlEngine !== "") {
      obj.sqlEngine = message.sqlEngine;
    }
    if (message.databaseVersion !== "") {
      obj.databaseVersion = message.databaseVersion;
    }
    if (message.instanceHost !== "") {
      obj.instanceHost = message.instanceHost;
    }
    return obj;
  },

  create(base?: DeepPartial<SqlDatabaseSystemSpec>): SqlDatabaseSystemSpec {
    return SqlDatabaseSystemSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SqlDatabaseSystemSpec>): SqlDatabaseSystemSpec {
    const message = createBaseSqlDatabaseSystemSpec();
    message.sqlEngine = object.sqlEngine ?? "";
    message.databaseVersion = object.databaseVersion ?? "";
    message.instanceHost = object.instanceHost ?? "";
    return message;
  },
};

function createBaseLookerSystemSpec(): LookerSystemSpec {
  return {
    parentInstanceId: "",
    parentInstanceDisplayName: "",
    parentModelId: "",
    parentModelDisplayName: "",
    parentViewId: "",
    parentViewDisplayName: "",
  };
}

export const LookerSystemSpec: MessageFns<LookerSystemSpec> = {
  encode(message: LookerSystemSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parentInstanceId !== "") {
      writer.uint32(10).string(message.parentInstanceId);
    }
    if (message.parentInstanceDisplayName !== "") {
      writer.uint32(18).string(message.parentInstanceDisplayName);
    }
    if (message.parentModelId !== "") {
      writer.uint32(26).string(message.parentModelId);
    }
    if (message.parentModelDisplayName !== "") {
      writer.uint32(34).string(message.parentModelDisplayName);
    }
    if (message.parentViewId !== "") {
      writer.uint32(42).string(message.parentViewId);
    }
    if (message.parentViewDisplayName !== "") {
      writer.uint32(50).string(message.parentViewDisplayName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): LookerSystemSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLookerSystemSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parentInstanceId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.parentInstanceDisplayName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.parentModelId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.parentModelDisplayName = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.parentViewId = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.parentViewDisplayName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LookerSystemSpec {
    return {
      parentInstanceId: isSet(object.parentInstanceId) ? globalThis.String(object.parentInstanceId) : "",
      parentInstanceDisplayName: isSet(object.parentInstanceDisplayName)
        ? globalThis.String(object.parentInstanceDisplayName)
        : "",
      parentModelId: isSet(object.parentModelId) ? globalThis.String(object.parentModelId) : "",
      parentModelDisplayName: isSet(object.parentModelDisplayName)
        ? globalThis.String(object.parentModelDisplayName)
        : "",
      parentViewId: isSet(object.parentViewId) ? globalThis.String(object.parentViewId) : "",
      parentViewDisplayName: isSet(object.parentViewDisplayName) ? globalThis.String(object.parentViewDisplayName) : "",
    };
  },

  toJSON(message: LookerSystemSpec): unknown {
    const obj: any = {};
    if (message.parentInstanceId !== "") {
      obj.parentInstanceId = message.parentInstanceId;
    }
    if (message.parentInstanceDisplayName !== "") {
      obj.parentInstanceDisplayName = message.parentInstanceDisplayName;
    }
    if (message.parentModelId !== "") {
      obj.parentModelId = message.parentModelId;
    }
    if (message.parentModelDisplayName !== "") {
      obj.parentModelDisplayName = message.parentModelDisplayName;
    }
    if (message.parentViewId !== "") {
      obj.parentViewId = message.parentViewId;
    }
    if (message.parentViewDisplayName !== "") {
      obj.parentViewDisplayName = message.parentViewDisplayName;
    }
    return obj;
  },

  create(base?: DeepPartial<LookerSystemSpec>): LookerSystemSpec {
    return LookerSystemSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<LookerSystemSpec>): LookerSystemSpec {
    const message = createBaseLookerSystemSpec();
    message.parentInstanceId = object.parentInstanceId ?? "";
    message.parentInstanceDisplayName = object.parentInstanceDisplayName ?? "";
    message.parentModelId = object.parentModelId ?? "";
    message.parentModelDisplayName = object.parentModelDisplayName ?? "";
    message.parentViewId = object.parentViewId ?? "";
    message.parentViewDisplayName = object.parentViewDisplayName ?? "";
    return message;
  },
};

function createBaseCloudBigtableSystemSpec(): CloudBigtableSystemSpec {
  return { instanceDisplayName: "" };
}

export const CloudBigtableSystemSpec: MessageFns<CloudBigtableSystemSpec> = {
  encode(message: CloudBigtableSystemSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.instanceDisplayName !== "") {
      writer.uint32(10).string(message.instanceDisplayName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CloudBigtableSystemSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCloudBigtableSystemSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.instanceDisplayName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CloudBigtableSystemSpec {
    return {
      instanceDisplayName: isSet(object.instanceDisplayName) ? globalThis.String(object.instanceDisplayName) : "",
    };
  },

  toJSON(message: CloudBigtableSystemSpec): unknown {
    const obj: any = {};
    if (message.instanceDisplayName !== "") {
      obj.instanceDisplayName = message.instanceDisplayName;
    }
    return obj;
  },

  create(base?: DeepPartial<CloudBigtableSystemSpec>): CloudBigtableSystemSpec {
    return CloudBigtableSystemSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CloudBigtableSystemSpec>): CloudBigtableSystemSpec {
    const message = createBaseCloudBigtableSystemSpec();
    message.instanceDisplayName = object.instanceDisplayName ?? "";
    return message;
  },
};

function createBaseCloudBigtableInstanceSpec(): CloudBigtableInstanceSpec {
  return { cloudBigtableClusterSpecs: [] };
}

export const CloudBigtableInstanceSpec: MessageFns<CloudBigtableInstanceSpec> = {
  encode(message: CloudBigtableInstanceSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.cloudBigtableClusterSpecs) {
      CloudBigtableInstanceSpec_CloudBigtableClusterSpec.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CloudBigtableInstanceSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCloudBigtableInstanceSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.cloudBigtableClusterSpecs.push(
            CloudBigtableInstanceSpec_CloudBigtableClusterSpec.decode(reader, reader.uint32()),
          );
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CloudBigtableInstanceSpec {
    return {
      cloudBigtableClusterSpecs: globalThis.Array.isArray(object?.cloudBigtableClusterSpecs)
        ? object.cloudBigtableClusterSpecs.map((e: any) =>
          CloudBigtableInstanceSpec_CloudBigtableClusterSpec.fromJSON(e)
        )
        : [],
    };
  },

  toJSON(message: CloudBigtableInstanceSpec): unknown {
    const obj: any = {};
    if (message.cloudBigtableClusterSpecs?.length) {
      obj.cloudBigtableClusterSpecs = message.cloudBigtableClusterSpecs.map((e) =>
        CloudBigtableInstanceSpec_CloudBigtableClusterSpec.toJSON(e)
      );
    }
    return obj;
  },

  create(base?: DeepPartial<CloudBigtableInstanceSpec>): CloudBigtableInstanceSpec {
    return CloudBigtableInstanceSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CloudBigtableInstanceSpec>): CloudBigtableInstanceSpec {
    const message = createBaseCloudBigtableInstanceSpec();
    message.cloudBigtableClusterSpecs =
      object.cloudBigtableClusterSpecs?.map((e) => CloudBigtableInstanceSpec_CloudBigtableClusterSpec.fromPartial(e)) ||
      [];
    return message;
  },
};

function createBaseCloudBigtableInstanceSpec_CloudBigtableClusterSpec(): CloudBigtableInstanceSpec_CloudBigtableClusterSpec {
  return { displayName: "", location: "", type: "", linkedResource: "" };
}

export const CloudBigtableInstanceSpec_CloudBigtableClusterSpec: MessageFns<
  CloudBigtableInstanceSpec_CloudBigtableClusterSpec
> = {
  encode(
    message: CloudBigtableInstanceSpec_CloudBigtableClusterSpec,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.displayName !== "") {
      writer.uint32(10).string(message.displayName);
    }
    if (message.location !== "") {
      writer.uint32(18).string(message.location);
    }
    if (message.type !== "") {
      writer.uint32(26).string(message.type);
    }
    if (message.linkedResource !== "") {
      writer.uint32(34).string(message.linkedResource);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CloudBigtableInstanceSpec_CloudBigtableClusterSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCloudBigtableInstanceSpec_CloudBigtableClusterSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.location = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.type = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.linkedResource = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CloudBigtableInstanceSpec_CloudBigtableClusterSpec {
    return {
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      location: isSet(object.location) ? globalThis.String(object.location) : "",
      type: isSet(object.type) ? globalThis.String(object.type) : "",
      linkedResource: isSet(object.linkedResource) ? globalThis.String(object.linkedResource) : "",
    };
  },

  toJSON(message: CloudBigtableInstanceSpec_CloudBigtableClusterSpec): unknown {
    const obj: any = {};
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.location !== "") {
      obj.location = message.location;
    }
    if (message.type !== "") {
      obj.type = message.type;
    }
    if (message.linkedResource !== "") {
      obj.linkedResource = message.linkedResource;
    }
    return obj;
  },

  create(
    base?: DeepPartial<CloudBigtableInstanceSpec_CloudBigtableClusterSpec>,
  ): CloudBigtableInstanceSpec_CloudBigtableClusterSpec {
    return CloudBigtableInstanceSpec_CloudBigtableClusterSpec.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<CloudBigtableInstanceSpec_CloudBigtableClusterSpec>,
  ): CloudBigtableInstanceSpec_CloudBigtableClusterSpec {
    const message = createBaseCloudBigtableInstanceSpec_CloudBigtableClusterSpec();
    message.displayName = object.displayName ?? "";
    message.location = object.location ?? "";
    message.type = object.type ?? "";
    message.linkedResource = object.linkedResource ?? "";
    return message;
  },
};

function createBaseServiceSpec(): ServiceSpec {
  return { cloudBigtableInstanceSpec: undefined };
}

export const ServiceSpec: MessageFns<ServiceSpec> = {
  encode(message: ServiceSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.cloudBigtableInstanceSpec !== undefined) {
      CloudBigtableInstanceSpec.encode(message.cloudBigtableInstanceSpec, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ServiceSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseServiceSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.cloudBigtableInstanceSpec = CloudBigtableInstanceSpec.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ServiceSpec {
    return {
      cloudBigtableInstanceSpec: isSet(object.cloudBigtableInstanceSpec)
        ? CloudBigtableInstanceSpec.fromJSON(object.cloudBigtableInstanceSpec)
        : undefined,
    };
  },

  toJSON(message: ServiceSpec): unknown {
    const obj: any = {};
    if (message.cloudBigtableInstanceSpec !== undefined) {
      obj.cloudBigtableInstanceSpec = CloudBigtableInstanceSpec.toJSON(message.cloudBigtableInstanceSpec);
    }
    return obj;
  },

  create(base?: DeepPartial<ServiceSpec>): ServiceSpec {
    return ServiceSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ServiceSpec>): ServiceSpec {
    const message = createBaseServiceSpec();
    message.cloudBigtableInstanceSpec =
      (object.cloudBigtableInstanceSpec !== undefined && object.cloudBigtableInstanceSpec !== null)
        ? CloudBigtableInstanceSpec.fromPartial(object.cloudBigtableInstanceSpec)
        : undefined;
    return message;
  },
};

function createBaseVertexModelSourceInfo(): VertexModelSourceInfo {
  return { sourceType: 0, copy: false };
}

export const VertexModelSourceInfo: MessageFns<VertexModelSourceInfo> = {
  encode(message: VertexModelSourceInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.sourceType !== 0) {
      writer.uint32(8).int32(message.sourceType);
    }
    if (message.copy !== false) {
      writer.uint32(16).bool(message.copy);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VertexModelSourceInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVertexModelSourceInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.sourceType = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.copy = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VertexModelSourceInfo {
    return {
      sourceType: isSet(object.sourceType) ? vertexModelSourceInfo_ModelSourceTypeFromJSON(object.sourceType) : 0,
      copy: isSet(object.copy) ? globalThis.Boolean(object.copy) : false,
    };
  },

  toJSON(message: VertexModelSourceInfo): unknown {
    const obj: any = {};
    if (message.sourceType !== 0) {
      obj.sourceType = vertexModelSourceInfo_ModelSourceTypeToJSON(message.sourceType);
    }
    if (message.copy !== false) {
      obj.copy = message.copy;
    }
    return obj;
  },

  create(base?: DeepPartial<VertexModelSourceInfo>): VertexModelSourceInfo {
    return VertexModelSourceInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VertexModelSourceInfo>): VertexModelSourceInfo {
    const message = createBaseVertexModelSourceInfo();
    message.sourceType = object.sourceType ?? 0;
    message.copy = object.copy ?? false;
    return message;
  },
};

function createBaseVertexModelSpec(): VertexModelSpec {
  return {
    versionId: "",
    versionAliases: [],
    versionDescription: "",
    vertexModelSourceInfo: undefined,
    containerImageUri: "",
  };
}

export const VertexModelSpec: MessageFns<VertexModelSpec> = {
  encode(message: VertexModelSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.versionId !== "") {
      writer.uint32(10).string(message.versionId);
    }
    for (const v of message.versionAliases) {
      writer.uint32(18).string(v!);
    }
    if (message.versionDescription !== "") {
      writer.uint32(26).string(message.versionDescription);
    }
    if (message.vertexModelSourceInfo !== undefined) {
      VertexModelSourceInfo.encode(message.vertexModelSourceInfo, writer.uint32(34).fork()).join();
    }
    if (message.containerImageUri !== "") {
      writer.uint32(42).string(message.containerImageUri);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VertexModelSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVertexModelSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.versionId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.versionAliases.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.versionDescription = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.vertexModelSourceInfo = VertexModelSourceInfo.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.containerImageUri = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VertexModelSpec {
    return {
      versionId: isSet(object.versionId) ? globalThis.String(object.versionId) : "",
      versionAliases: globalThis.Array.isArray(object?.versionAliases)
        ? object.versionAliases.map((e: any) => globalThis.String(e))
        : [],
      versionDescription: isSet(object.versionDescription) ? globalThis.String(object.versionDescription) : "",
      vertexModelSourceInfo: isSet(object.vertexModelSourceInfo)
        ? VertexModelSourceInfo.fromJSON(object.vertexModelSourceInfo)
        : undefined,
      containerImageUri: isSet(object.containerImageUri) ? globalThis.String(object.containerImageUri) : "",
    };
  },

  toJSON(message: VertexModelSpec): unknown {
    const obj: any = {};
    if (message.versionId !== "") {
      obj.versionId = message.versionId;
    }
    if (message.versionAliases?.length) {
      obj.versionAliases = message.versionAliases;
    }
    if (message.versionDescription !== "") {
      obj.versionDescription = message.versionDescription;
    }
    if (message.vertexModelSourceInfo !== undefined) {
      obj.vertexModelSourceInfo = VertexModelSourceInfo.toJSON(message.vertexModelSourceInfo);
    }
    if (message.containerImageUri !== "") {
      obj.containerImageUri = message.containerImageUri;
    }
    return obj;
  },

  create(base?: DeepPartial<VertexModelSpec>): VertexModelSpec {
    return VertexModelSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VertexModelSpec>): VertexModelSpec {
    const message = createBaseVertexModelSpec();
    message.versionId = object.versionId ?? "";
    message.versionAliases = object.versionAliases?.map((e) => e) || [];
    message.versionDescription = object.versionDescription ?? "";
    message.vertexModelSourceInfo =
      (object.vertexModelSourceInfo !== undefined && object.vertexModelSourceInfo !== null)
        ? VertexModelSourceInfo.fromPartial(object.vertexModelSourceInfo)
        : undefined;
    message.containerImageUri = object.containerImageUri ?? "";
    return message;
  },
};

function createBaseVertexDatasetSpec(): VertexDatasetSpec {
  return { dataItemCount: Long.ZERO, dataType: 0 };
}

export const VertexDatasetSpec: MessageFns<VertexDatasetSpec> = {
  encode(message: VertexDatasetSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.dataItemCount.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.dataItemCount.toString());
    }
    if (message.dataType !== 0) {
      writer.uint32(16).int32(message.dataType);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VertexDatasetSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVertexDatasetSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.dataItemCount = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.dataType = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VertexDatasetSpec {
    return {
      dataItemCount: isSet(object.dataItemCount) ? Long.fromValue(object.dataItemCount) : Long.ZERO,
      dataType: isSet(object.dataType) ? vertexDatasetSpec_DataTypeFromJSON(object.dataType) : 0,
    };
  },

  toJSON(message: VertexDatasetSpec): unknown {
    const obj: any = {};
    if (!message.dataItemCount.equals(Long.ZERO)) {
      obj.dataItemCount = (message.dataItemCount || Long.ZERO).toString();
    }
    if (message.dataType !== 0) {
      obj.dataType = vertexDatasetSpec_DataTypeToJSON(message.dataType);
    }
    return obj;
  },

  create(base?: DeepPartial<VertexDatasetSpec>): VertexDatasetSpec {
    return VertexDatasetSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VertexDatasetSpec>): VertexDatasetSpec {
    const message = createBaseVertexDatasetSpec();
    message.dataItemCount = (object.dataItemCount !== undefined && object.dataItemCount !== null)
      ? Long.fromValue(object.dataItemCount)
      : Long.ZERO;
    message.dataType = object.dataType ?? 0;
    return message;
  },
};

function createBaseModelSpec(): ModelSpec {
  return { vertexModelSpec: undefined };
}

export const ModelSpec: MessageFns<ModelSpec> = {
  encode(message: ModelSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.vertexModelSpec !== undefined) {
      VertexModelSpec.encode(message.vertexModelSpec, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ModelSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseModelSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.vertexModelSpec = VertexModelSpec.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ModelSpec {
    return {
      vertexModelSpec: isSet(object.vertexModelSpec) ? VertexModelSpec.fromJSON(object.vertexModelSpec) : undefined,
    };
  },

  toJSON(message: ModelSpec): unknown {
    const obj: any = {};
    if (message.vertexModelSpec !== undefined) {
      obj.vertexModelSpec = VertexModelSpec.toJSON(message.vertexModelSpec);
    }
    return obj;
  },

  create(base?: DeepPartial<ModelSpec>): ModelSpec {
    return ModelSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ModelSpec>): ModelSpec {
    const message = createBaseModelSpec();
    message.vertexModelSpec = (object.vertexModelSpec !== undefined && object.vertexModelSpec !== null)
      ? VertexModelSpec.fromPartial(object.vertexModelSpec)
      : undefined;
    return message;
  },
};

function createBaseBusinessContext(): BusinessContext {
  return { entryOverview: undefined, contacts: undefined };
}

export const BusinessContext: MessageFns<BusinessContext> = {
  encode(message: BusinessContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.entryOverview !== undefined) {
      EntryOverview.encode(message.entryOverview, writer.uint32(10).fork()).join();
    }
    if (message.contacts !== undefined) {
      Contacts.encode(message.contacts, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BusinessContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBusinessContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.entryOverview = EntryOverview.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.contacts = Contacts.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BusinessContext {
    return {
      entryOverview: isSet(object.entryOverview) ? EntryOverview.fromJSON(object.entryOverview) : undefined,
      contacts: isSet(object.contacts) ? Contacts.fromJSON(object.contacts) : undefined,
    };
  },

  toJSON(message: BusinessContext): unknown {
    const obj: any = {};
    if (message.entryOverview !== undefined) {
      obj.entryOverview = EntryOverview.toJSON(message.entryOverview);
    }
    if (message.contacts !== undefined) {
      obj.contacts = Contacts.toJSON(message.contacts);
    }
    return obj;
  },

  create(base?: DeepPartial<BusinessContext>): BusinessContext {
    return BusinessContext.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BusinessContext>): BusinessContext {
    const message = createBaseBusinessContext();
    message.entryOverview = (object.entryOverview !== undefined && object.entryOverview !== null)
      ? EntryOverview.fromPartial(object.entryOverview)
      : undefined;
    message.contacts = (object.contacts !== undefined && object.contacts !== null)
      ? Contacts.fromPartial(object.contacts)
      : undefined;
    return message;
  },
};

function createBaseEntryOverview(): EntryOverview {
  return { overview: "" };
}

export const EntryOverview: MessageFns<EntryOverview> = {
  encode(message: EntryOverview, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.overview !== "") {
      writer.uint32(10).string(message.overview);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EntryOverview {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEntryOverview();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.overview = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EntryOverview {
    return { overview: isSet(object.overview) ? globalThis.String(object.overview) : "" };
  },

  toJSON(message: EntryOverview): unknown {
    const obj: any = {};
    if (message.overview !== "") {
      obj.overview = message.overview;
    }
    return obj;
  },

  create(base?: DeepPartial<EntryOverview>): EntryOverview {
    return EntryOverview.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EntryOverview>): EntryOverview {
    const message = createBaseEntryOverview();
    message.overview = object.overview ?? "";
    return message;
  },
};

function createBaseContacts(): Contacts {
  return { people: [] };
}

export const Contacts: MessageFns<Contacts> = {
  encode(message: Contacts, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.people) {
      Contacts_Person.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Contacts {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseContacts();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.people.push(Contacts_Person.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Contacts {
    return {
      people: globalThis.Array.isArray(object?.people)
        ? object.people.map((e: any) => Contacts_Person.fromJSON(e))
        : [],
    };
  },

  toJSON(message: Contacts): unknown {
    const obj: any = {};
    if (message.people?.length) {
      obj.people = message.people.map((e) => Contacts_Person.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<Contacts>): Contacts {
    return Contacts.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Contacts>): Contacts {
    const message = createBaseContacts();
    message.people = object.people?.map((e) => Contacts_Person.fromPartial(e)) || [];
    return message;
  },
};

function createBaseContacts_Person(): Contacts_Person {
  return { designation: "", email: "" };
}

export const Contacts_Person: MessageFns<Contacts_Person> = {
  encode(message: Contacts_Person, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.designation !== "") {
      writer.uint32(10).string(message.designation);
    }
    if (message.email !== "") {
      writer.uint32(18).string(message.email);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Contacts_Person {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseContacts_Person();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.designation = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.email = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Contacts_Person {
    return {
      designation: isSet(object.designation) ? globalThis.String(object.designation) : "",
      email: isSet(object.email) ? globalThis.String(object.email) : "",
    };
  },

  toJSON(message: Contacts_Person): unknown {
    const obj: any = {};
    if (message.designation !== "") {
      obj.designation = message.designation;
    }
    if (message.email !== "") {
      obj.email = message.email;
    }
    return obj;
  },

  create(base?: DeepPartial<Contacts_Person>): Contacts_Person {
    return Contacts_Person.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Contacts_Person>): Contacts_Person {
    const message = createBaseContacts_Person();
    message.designation = object.designation ?? "";
    message.email = object.email ?? "";
    return message;
  },
};

function createBaseEntryGroup(): EntryGroup {
  return { name: "", displayName: "", description: "", dataCatalogTimestamps: undefined };
}

export const EntryGroup: MessageFns<EntryGroup> = {
  encode(message: EntryGroup, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.displayName !== "") {
      writer.uint32(18).string(message.displayName);
    }
    if (message.description !== "") {
      writer.uint32(26).string(message.description);
    }
    if (message.dataCatalogTimestamps !== undefined) {
      SystemTimestamps.encode(message.dataCatalogTimestamps, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EntryGroup {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEntryGroup();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.description = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.dataCatalogTimestamps = SystemTimestamps.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EntryGroup {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      dataCatalogTimestamps: isSet(object.dataCatalogTimestamps)
        ? SystemTimestamps.fromJSON(object.dataCatalogTimestamps)
        : undefined,
    };
  },

  toJSON(message: EntryGroup): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.dataCatalogTimestamps !== undefined) {
      obj.dataCatalogTimestamps = SystemTimestamps.toJSON(message.dataCatalogTimestamps);
    }
    return obj;
  },

  create(base?: DeepPartial<EntryGroup>): EntryGroup {
    return EntryGroup.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EntryGroup>): EntryGroup {
    const message = createBaseEntryGroup();
    message.name = object.name ?? "";
    message.displayName = object.displayName ?? "";
    message.description = object.description ?? "";
    message.dataCatalogTimestamps =
      (object.dataCatalogTimestamps !== undefined && object.dataCatalogTimestamps !== null)
        ? SystemTimestamps.fromPartial(object.dataCatalogTimestamps)
        : undefined;
    return message;
  },
};

function createBaseCreateTagTemplateRequest(): CreateTagTemplateRequest {
  return { parent: "", tagTemplateId: "", tagTemplate: undefined };
}

export const CreateTagTemplateRequest: MessageFns<CreateTagTemplateRequest> = {
  encode(message: CreateTagTemplateRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.tagTemplateId !== "") {
      writer.uint32(26).string(message.tagTemplateId);
    }
    if (message.tagTemplate !== undefined) {
      TagTemplate.encode(message.tagTemplate, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateTagTemplateRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateTagTemplateRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.tagTemplateId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.tagTemplate = TagTemplate.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateTagTemplateRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      tagTemplateId: isSet(object.tagTemplateId) ? globalThis.String(object.tagTemplateId) : "",
      tagTemplate: isSet(object.tagTemplate) ? TagTemplate.fromJSON(object.tagTemplate) : undefined,
    };
  },

  toJSON(message: CreateTagTemplateRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.tagTemplateId !== "") {
      obj.tagTemplateId = message.tagTemplateId;
    }
    if (message.tagTemplate !== undefined) {
      obj.tagTemplate = TagTemplate.toJSON(message.tagTemplate);
    }
    return obj;
  },

  create(base?: DeepPartial<CreateTagTemplateRequest>): CreateTagTemplateRequest {
    return CreateTagTemplateRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateTagTemplateRequest>): CreateTagTemplateRequest {
    const message = createBaseCreateTagTemplateRequest();
    message.parent = object.parent ?? "";
    message.tagTemplateId = object.tagTemplateId ?? "";
    message.tagTemplate = (object.tagTemplate !== undefined && object.tagTemplate !== null)
      ? TagTemplate.fromPartial(object.tagTemplate)
      : undefined;
    return message;
  },
};

function createBaseGetTagTemplateRequest(): GetTagTemplateRequest {
  return { name: "" };
}

export const GetTagTemplateRequest: MessageFns<GetTagTemplateRequest> = {
  encode(message: GetTagTemplateRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetTagTemplateRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetTagTemplateRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetTagTemplateRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: GetTagTemplateRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<GetTagTemplateRequest>): GetTagTemplateRequest {
    return GetTagTemplateRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetTagTemplateRequest>): GetTagTemplateRequest {
    const message = createBaseGetTagTemplateRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseUpdateTagTemplateRequest(): UpdateTagTemplateRequest {
  return { tagTemplate: undefined, updateMask: undefined };
}

export const UpdateTagTemplateRequest: MessageFns<UpdateTagTemplateRequest> = {
  encode(message: UpdateTagTemplateRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.tagTemplate !== undefined) {
      TagTemplate.encode(message.tagTemplate, writer.uint32(10).fork()).join();
    }
    if (message.updateMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.updateMask), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateTagTemplateRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateTagTemplateRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.tagTemplate = TagTemplate.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.updateMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateTagTemplateRequest {
    return {
      tagTemplate: isSet(object.tagTemplate) ? TagTemplate.fromJSON(object.tagTemplate) : undefined,
      updateMask: isSet(object.updateMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.updateMask)) : undefined,
    };
  },

  toJSON(message: UpdateTagTemplateRequest): unknown {
    const obj: any = {};
    if (message.tagTemplate !== undefined) {
      obj.tagTemplate = TagTemplate.toJSON(message.tagTemplate);
    }
    if (message.updateMask !== undefined) {
      obj.updateMask = FieldMask.toJSON(FieldMask.wrap(message.updateMask));
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateTagTemplateRequest>): UpdateTagTemplateRequest {
    return UpdateTagTemplateRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateTagTemplateRequest>): UpdateTagTemplateRequest {
    const message = createBaseUpdateTagTemplateRequest();
    message.tagTemplate = (object.tagTemplate !== undefined && object.tagTemplate !== null)
      ? TagTemplate.fromPartial(object.tagTemplate)
      : undefined;
    message.updateMask = object.updateMask ?? undefined;
    return message;
  },
};

function createBaseDeleteTagTemplateRequest(): DeleteTagTemplateRequest {
  return { name: "", force: false };
}

export const DeleteTagTemplateRequest: MessageFns<DeleteTagTemplateRequest> = {
  encode(message: DeleteTagTemplateRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.force !== false) {
      writer.uint32(16).bool(message.force);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeleteTagTemplateRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeleteTagTemplateRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.force = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeleteTagTemplateRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      force: isSet(object.force) ? globalThis.Boolean(object.force) : false,
    };
  },

  toJSON(message: DeleteTagTemplateRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.force !== false) {
      obj.force = message.force;
    }
    return obj;
  },

  create(base?: DeepPartial<DeleteTagTemplateRequest>): DeleteTagTemplateRequest {
    return DeleteTagTemplateRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeleteTagTemplateRequest>): DeleteTagTemplateRequest {
    const message = createBaseDeleteTagTemplateRequest();
    message.name = object.name ?? "";
    message.force = object.force ?? false;
    return message;
  },
};

function createBaseCreateTagRequest(): CreateTagRequest {
  return { parent: "", tag: undefined };
}

export const CreateTagRequest: MessageFns<CreateTagRequest> = {
  encode(message: CreateTagRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.tag !== undefined) {
      Tag.encode(message.tag, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateTagRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateTagRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.tag = Tag.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateTagRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      tag: isSet(object.tag) ? Tag.fromJSON(object.tag) : undefined,
    };
  },

  toJSON(message: CreateTagRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.tag !== undefined) {
      obj.tag = Tag.toJSON(message.tag);
    }
    return obj;
  },

  create(base?: DeepPartial<CreateTagRequest>): CreateTagRequest {
    return CreateTagRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateTagRequest>): CreateTagRequest {
    const message = createBaseCreateTagRequest();
    message.parent = object.parent ?? "";
    message.tag = (object.tag !== undefined && object.tag !== null) ? Tag.fromPartial(object.tag) : undefined;
    return message;
  },
};

function createBaseUpdateTagRequest(): UpdateTagRequest {
  return { tag: undefined, updateMask: undefined };
}

export const UpdateTagRequest: MessageFns<UpdateTagRequest> = {
  encode(message: UpdateTagRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.tag !== undefined) {
      Tag.encode(message.tag, writer.uint32(10).fork()).join();
    }
    if (message.updateMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.updateMask), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateTagRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateTagRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.tag = Tag.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.updateMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateTagRequest {
    return {
      tag: isSet(object.tag) ? Tag.fromJSON(object.tag) : undefined,
      updateMask: isSet(object.updateMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.updateMask)) : undefined,
    };
  },

  toJSON(message: UpdateTagRequest): unknown {
    const obj: any = {};
    if (message.tag !== undefined) {
      obj.tag = Tag.toJSON(message.tag);
    }
    if (message.updateMask !== undefined) {
      obj.updateMask = FieldMask.toJSON(FieldMask.wrap(message.updateMask));
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateTagRequest>): UpdateTagRequest {
    return UpdateTagRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateTagRequest>): UpdateTagRequest {
    const message = createBaseUpdateTagRequest();
    message.tag = (object.tag !== undefined && object.tag !== null) ? Tag.fromPartial(object.tag) : undefined;
    message.updateMask = object.updateMask ?? undefined;
    return message;
  },
};

function createBaseDeleteTagRequest(): DeleteTagRequest {
  return { name: "" };
}

export const DeleteTagRequest: MessageFns<DeleteTagRequest> = {
  encode(message: DeleteTagRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeleteTagRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeleteTagRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeleteTagRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: DeleteTagRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<DeleteTagRequest>): DeleteTagRequest {
    return DeleteTagRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeleteTagRequest>): DeleteTagRequest {
    const message = createBaseDeleteTagRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseCreateTagTemplateFieldRequest(): CreateTagTemplateFieldRequest {
  return { parent: "", tagTemplateFieldId: "", tagTemplateField: undefined };
}

export const CreateTagTemplateFieldRequest: MessageFns<CreateTagTemplateFieldRequest> = {
  encode(message: CreateTagTemplateFieldRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.tagTemplateFieldId !== "") {
      writer.uint32(18).string(message.tagTemplateFieldId);
    }
    if (message.tagTemplateField !== undefined) {
      TagTemplateField.encode(message.tagTemplateField, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateTagTemplateFieldRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateTagTemplateFieldRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.tagTemplateFieldId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.tagTemplateField = TagTemplateField.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateTagTemplateFieldRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      tagTemplateFieldId: isSet(object.tagTemplateFieldId) ? globalThis.String(object.tagTemplateFieldId) : "",
      tagTemplateField: isSet(object.tagTemplateField) ? TagTemplateField.fromJSON(object.tagTemplateField) : undefined,
    };
  },

  toJSON(message: CreateTagTemplateFieldRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.tagTemplateFieldId !== "") {
      obj.tagTemplateFieldId = message.tagTemplateFieldId;
    }
    if (message.tagTemplateField !== undefined) {
      obj.tagTemplateField = TagTemplateField.toJSON(message.tagTemplateField);
    }
    return obj;
  },

  create(base?: DeepPartial<CreateTagTemplateFieldRequest>): CreateTagTemplateFieldRequest {
    return CreateTagTemplateFieldRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateTagTemplateFieldRequest>): CreateTagTemplateFieldRequest {
    const message = createBaseCreateTagTemplateFieldRequest();
    message.parent = object.parent ?? "";
    message.tagTemplateFieldId = object.tagTemplateFieldId ?? "";
    message.tagTemplateField = (object.tagTemplateField !== undefined && object.tagTemplateField !== null)
      ? TagTemplateField.fromPartial(object.tagTemplateField)
      : undefined;
    return message;
  },
};

function createBaseUpdateTagTemplateFieldRequest(): UpdateTagTemplateFieldRequest {
  return { name: "", tagTemplateField: undefined, updateMask: undefined };
}

export const UpdateTagTemplateFieldRequest: MessageFns<UpdateTagTemplateFieldRequest> = {
  encode(message: UpdateTagTemplateFieldRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.tagTemplateField !== undefined) {
      TagTemplateField.encode(message.tagTemplateField, writer.uint32(18).fork()).join();
    }
    if (message.updateMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.updateMask), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateTagTemplateFieldRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateTagTemplateFieldRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.tagTemplateField = TagTemplateField.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.updateMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateTagTemplateFieldRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      tagTemplateField: isSet(object.tagTemplateField) ? TagTemplateField.fromJSON(object.tagTemplateField) : undefined,
      updateMask: isSet(object.updateMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.updateMask)) : undefined,
    };
  },

  toJSON(message: UpdateTagTemplateFieldRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.tagTemplateField !== undefined) {
      obj.tagTemplateField = TagTemplateField.toJSON(message.tagTemplateField);
    }
    if (message.updateMask !== undefined) {
      obj.updateMask = FieldMask.toJSON(FieldMask.wrap(message.updateMask));
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateTagTemplateFieldRequest>): UpdateTagTemplateFieldRequest {
    return UpdateTagTemplateFieldRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateTagTemplateFieldRequest>): UpdateTagTemplateFieldRequest {
    const message = createBaseUpdateTagTemplateFieldRequest();
    message.name = object.name ?? "";
    message.tagTemplateField = (object.tagTemplateField !== undefined && object.tagTemplateField !== null)
      ? TagTemplateField.fromPartial(object.tagTemplateField)
      : undefined;
    message.updateMask = object.updateMask ?? undefined;
    return message;
  },
};

function createBaseRenameTagTemplateFieldRequest(): RenameTagTemplateFieldRequest {
  return { name: "", newTagTemplateFieldId: "" };
}

export const RenameTagTemplateFieldRequest: MessageFns<RenameTagTemplateFieldRequest> = {
  encode(message: RenameTagTemplateFieldRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.newTagTemplateFieldId !== "") {
      writer.uint32(18).string(message.newTagTemplateFieldId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RenameTagTemplateFieldRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRenameTagTemplateFieldRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.newTagTemplateFieldId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RenameTagTemplateFieldRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      newTagTemplateFieldId: isSet(object.newTagTemplateFieldId) ? globalThis.String(object.newTagTemplateFieldId) : "",
    };
  },

  toJSON(message: RenameTagTemplateFieldRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.newTagTemplateFieldId !== "") {
      obj.newTagTemplateFieldId = message.newTagTemplateFieldId;
    }
    return obj;
  },

  create(base?: DeepPartial<RenameTagTemplateFieldRequest>): RenameTagTemplateFieldRequest {
    return RenameTagTemplateFieldRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RenameTagTemplateFieldRequest>): RenameTagTemplateFieldRequest {
    const message = createBaseRenameTagTemplateFieldRequest();
    message.name = object.name ?? "";
    message.newTagTemplateFieldId = object.newTagTemplateFieldId ?? "";
    return message;
  },
};

function createBaseRenameTagTemplateFieldEnumValueRequest(): RenameTagTemplateFieldEnumValueRequest {
  return { name: "", newEnumValueDisplayName: "" };
}

export const RenameTagTemplateFieldEnumValueRequest: MessageFns<RenameTagTemplateFieldEnumValueRequest> = {
  encode(message: RenameTagTemplateFieldEnumValueRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.newEnumValueDisplayName !== "") {
      writer.uint32(18).string(message.newEnumValueDisplayName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RenameTagTemplateFieldEnumValueRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRenameTagTemplateFieldEnumValueRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.newEnumValueDisplayName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RenameTagTemplateFieldEnumValueRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      newEnumValueDisplayName: isSet(object.newEnumValueDisplayName)
        ? globalThis.String(object.newEnumValueDisplayName)
        : "",
    };
  },

  toJSON(message: RenameTagTemplateFieldEnumValueRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.newEnumValueDisplayName !== "") {
      obj.newEnumValueDisplayName = message.newEnumValueDisplayName;
    }
    return obj;
  },

  create(base?: DeepPartial<RenameTagTemplateFieldEnumValueRequest>): RenameTagTemplateFieldEnumValueRequest {
    return RenameTagTemplateFieldEnumValueRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RenameTagTemplateFieldEnumValueRequest>): RenameTagTemplateFieldEnumValueRequest {
    const message = createBaseRenameTagTemplateFieldEnumValueRequest();
    message.name = object.name ?? "";
    message.newEnumValueDisplayName = object.newEnumValueDisplayName ?? "";
    return message;
  },
};

function createBaseDeleteTagTemplateFieldRequest(): DeleteTagTemplateFieldRequest {
  return { name: "", force: false };
}

export const DeleteTagTemplateFieldRequest: MessageFns<DeleteTagTemplateFieldRequest> = {
  encode(message: DeleteTagTemplateFieldRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.force !== false) {
      writer.uint32(16).bool(message.force);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeleteTagTemplateFieldRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeleteTagTemplateFieldRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.force = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeleteTagTemplateFieldRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      force: isSet(object.force) ? globalThis.Boolean(object.force) : false,
    };
  },

  toJSON(message: DeleteTagTemplateFieldRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.force !== false) {
      obj.force = message.force;
    }
    return obj;
  },

  create(base?: DeepPartial<DeleteTagTemplateFieldRequest>): DeleteTagTemplateFieldRequest {
    return DeleteTagTemplateFieldRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeleteTagTemplateFieldRequest>): DeleteTagTemplateFieldRequest {
    const message = createBaseDeleteTagTemplateFieldRequest();
    message.name = object.name ?? "";
    message.force = object.force ?? false;
    return message;
  },
};

function createBaseListTagsRequest(): ListTagsRequest {
  return { parent: "", pageSize: 0, pageToken: "" };
}

export const ListTagsRequest: MessageFns<ListTagsRequest> = {
  encode(message: ListTagsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.pageSize !== 0) {
      writer.uint32(16).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(26).string(message.pageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListTagsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListTagsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListTagsRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
    };
  },

  toJSON(message: ListTagsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListTagsRequest>): ListTagsRequest {
    return ListTagsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListTagsRequest>): ListTagsRequest {
    const message = createBaseListTagsRequest();
    message.parent = object.parent ?? "";
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    return message;
  },
};

function createBaseListTagsResponse(): ListTagsResponse {
  return { tags: [], nextPageToken: "" };
}

export const ListTagsResponse: MessageFns<ListTagsResponse> = {
  encode(message: ListTagsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.tags) {
      Tag.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListTagsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListTagsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.tags.push(Tag.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListTagsResponse {
    return {
      tags: globalThis.Array.isArray(object?.tags) ? object.tags.map((e: any) => Tag.fromJSON(e)) : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: ListTagsResponse): unknown {
    const obj: any = {};
    if (message.tags?.length) {
      obj.tags = message.tags.map((e) => Tag.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListTagsResponse>): ListTagsResponse {
    return ListTagsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListTagsResponse>): ListTagsResponse {
    const message = createBaseListTagsResponse();
    message.tags = object.tags?.map((e) => Tag.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBaseReconcileTagsRequest(): ReconcileTagsRequest {
  return { parent: "", tagTemplate: "", forceDeleteMissing: false, tags: [] };
}

export const ReconcileTagsRequest: MessageFns<ReconcileTagsRequest> = {
  encode(message: ReconcileTagsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.tagTemplate !== "") {
      writer.uint32(18).string(message.tagTemplate);
    }
    if (message.forceDeleteMissing !== false) {
      writer.uint32(24).bool(message.forceDeleteMissing);
    }
    for (const v of message.tags) {
      Tag.encode(v!, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReconcileTagsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReconcileTagsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.tagTemplate = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.forceDeleteMissing = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.tags.push(Tag.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReconcileTagsRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      tagTemplate: isSet(object.tagTemplate) ? globalThis.String(object.tagTemplate) : "",
      forceDeleteMissing: isSet(object.forceDeleteMissing) ? globalThis.Boolean(object.forceDeleteMissing) : false,
      tags: globalThis.Array.isArray(object?.tags) ? object.tags.map((e: any) => Tag.fromJSON(e)) : [],
    };
  },

  toJSON(message: ReconcileTagsRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.tagTemplate !== "") {
      obj.tagTemplate = message.tagTemplate;
    }
    if (message.forceDeleteMissing !== false) {
      obj.forceDeleteMissing = message.forceDeleteMissing;
    }
    if (message.tags?.length) {
      obj.tags = message.tags.map((e) => Tag.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ReconcileTagsRequest>): ReconcileTagsRequest {
    return ReconcileTagsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReconcileTagsRequest>): ReconcileTagsRequest {
    const message = createBaseReconcileTagsRequest();
    message.parent = object.parent ?? "";
    message.tagTemplate = object.tagTemplate ?? "";
    message.forceDeleteMissing = object.forceDeleteMissing ?? false;
    message.tags = object.tags?.map((e) => Tag.fromPartial(e)) || [];
    return message;
  },
};

function createBaseReconcileTagsResponse(): ReconcileTagsResponse {
  return { createdTagsCount: Long.ZERO, updatedTagsCount: Long.ZERO, deletedTagsCount: Long.ZERO };
}

export const ReconcileTagsResponse: MessageFns<ReconcileTagsResponse> = {
  encode(message: ReconcileTagsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.createdTagsCount.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.createdTagsCount.toString());
    }
    if (!message.updatedTagsCount.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.updatedTagsCount.toString());
    }
    if (!message.deletedTagsCount.equals(Long.ZERO)) {
      writer.uint32(24).int64(message.deletedTagsCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReconcileTagsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReconcileTagsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.createdTagsCount = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.updatedTagsCount = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.deletedTagsCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReconcileTagsResponse {
    return {
      createdTagsCount: isSet(object.createdTagsCount) ? Long.fromValue(object.createdTagsCount) : Long.ZERO,
      updatedTagsCount: isSet(object.updatedTagsCount) ? Long.fromValue(object.updatedTagsCount) : Long.ZERO,
      deletedTagsCount: isSet(object.deletedTagsCount) ? Long.fromValue(object.deletedTagsCount) : Long.ZERO,
    };
  },

  toJSON(message: ReconcileTagsResponse): unknown {
    const obj: any = {};
    if (!message.createdTagsCount.equals(Long.ZERO)) {
      obj.createdTagsCount = (message.createdTagsCount || Long.ZERO).toString();
    }
    if (!message.updatedTagsCount.equals(Long.ZERO)) {
      obj.updatedTagsCount = (message.updatedTagsCount || Long.ZERO).toString();
    }
    if (!message.deletedTagsCount.equals(Long.ZERO)) {
      obj.deletedTagsCount = (message.deletedTagsCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ReconcileTagsResponse>): ReconcileTagsResponse {
    return ReconcileTagsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReconcileTagsResponse>): ReconcileTagsResponse {
    const message = createBaseReconcileTagsResponse();
    message.createdTagsCount = (object.createdTagsCount !== undefined && object.createdTagsCount !== null)
      ? Long.fromValue(object.createdTagsCount)
      : Long.ZERO;
    message.updatedTagsCount = (object.updatedTagsCount !== undefined && object.updatedTagsCount !== null)
      ? Long.fromValue(object.updatedTagsCount)
      : Long.ZERO;
    message.deletedTagsCount = (object.deletedTagsCount !== undefined && object.deletedTagsCount !== null)
      ? Long.fromValue(object.deletedTagsCount)
      : Long.ZERO;
    return message;
  },
};

function createBaseReconcileTagsMetadata(): ReconcileTagsMetadata {
  return { state: 0, errors: {} };
}

export const ReconcileTagsMetadata: MessageFns<ReconcileTagsMetadata> = {
  encode(message: ReconcileTagsMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.state !== 0) {
      writer.uint32(8).int32(message.state);
    }
    Object.entries(message.errors).forEach(([key, value]) => {
      ReconcileTagsMetadata_ErrorsEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReconcileTagsMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReconcileTagsMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = ReconcileTagsMetadata_ErrorsEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.errors[entry2.key] = entry2.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReconcileTagsMetadata {
    return {
      state: isSet(object.state) ? reconcileTagsMetadata_ReconciliationStateFromJSON(object.state) : 0,
      errors: isObject(object.errors)
        ? Object.entries(object.errors).reduce<{ [key: string]: Status }>((acc, [key, value]) => {
          acc[key] = Status.fromJSON(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: ReconcileTagsMetadata): unknown {
    const obj: any = {};
    if (message.state !== 0) {
      obj.state = reconcileTagsMetadata_ReconciliationStateToJSON(message.state);
    }
    if (message.errors) {
      const entries = Object.entries(message.errors);
      if (entries.length > 0) {
        obj.errors = {};
        entries.forEach(([k, v]) => {
          obj.errors[k] = Status.toJSON(v);
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<ReconcileTagsMetadata>): ReconcileTagsMetadata {
    return ReconcileTagsMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReconcileTagsMetadata>): ReconcileTagsMetadata {
    const message = createBaseReconcileTagsMetadata();
    message.state = object.state ?? 0;
    message.errors = Object.entries(object.errors ?? {}).reduce<{ [key: string]: Status }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = Status.fromPartial(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseReconcileTagsMetadata_ErrorsEntry(): ReconcileTagsMetadata_ErrorsEntry {
  return { key: "", value: undefined };
}

export const ReconcileTagsMetadata_ErrorsEntry: MessageFns<ReconcileTagsMetadata_ErrorsEntry> = {
  encode(message: ReconcileTagsMetadata_ErrorsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      Status.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReconcileTagsMetadata_ErrorsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReconcileTagsMetadata_ErrorsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = Status.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReconcileTagsMetadata_ErrorsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? Status.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: ReconcileTagsMetadata_ErrorsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = Status.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<ReconcileTagsMetadata_ErrorsEntry>): ReconcileTagsMetadata_ErrorsEntry {
    return ReconcileTagsMetadata_ErrorsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReconcileTagsMetadata_ErrorsEntry>): ReconcileTagsMetadata_ErrorsEntry {
    const message = createBaseReconcileTagsMetadata_ErrorsEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? Status.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBaseListEntriesRequest(): ListEntriesRequest {
  return { parent: "", pageSize: 0, pageToken: "", readMask: undefined };
}

export const ListEntriesRequest: MessageFns<ListEntriesRequest> = {
  encode(message: ListEntriesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.pageSize !== 0) {
      writer.uint32(16).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(26).string(message.pageToken);
    }
    if (message.readMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.readMask), writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListEntriesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListEntriesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pageToken = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.readMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListEntriesRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
      readMask: isSet(object.readMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.readMask)) : undefined,
    };
  },

  toJSON(message: ListEntriesRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    if (message.readMask !== undefined) {
      obj.readMask = FieldMask.toJSON(FieldMask.wrap(message.readMask));
    }
    return obj;
  },

  create(base?: DeepPartial<ListEntriesRequest>): ListEntriesRequest {
    return ListEntriesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListEntriesRequest>): ListEntriesRequest {
    const message = createBaseListEntriesRequest();
    message.parent = object.parent ?? "";
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    message.readMask = object.readMask ?? undefined;
    return message;
  },
};

function createBaseListEntriesResponse(): ListEntriesResponse {
  return { entries: [], nextPageToken: "" };
}

export const ListEntriesResponse: MessageFns<ListEntriesResponse> = {
  encode(message: ListEntriesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.entries) {
      Entry.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListEntriesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListEntriesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.entries.push(Entry.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListEntriesResponse {
    return {
      entries: globalThis.Array.isArray(object?.entries) ? object.entries.map((e: any) => Entry.fromJSON(e)) : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: ListEntriesResponse): unknown {
    const obj: any = {};
    if (message.entries?.length) {
      obj.entries = message.entries.map((e) => Entry.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListEntriesResponse>): ListEntriesResponse {
    return ListEntriesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListEntriesResponse>): ListEntriesResponse {
    const message = createBaseListEntriesResponse();
    message.entries = object.entries?.map((e) => Entry.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBaseStarEntryRequest(): StarEntryRequest {
  return { name: "" };
}

export const StarEntryRequest: MessageFns<StarEntryRequest> = {
  encode(message: StarEntryRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StarEntryRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStarEntryRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StarEntryRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: StarEntryRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<StarEntryRequest>): StarEntryRequest {
    return StarEntryRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StarEntryRequest>): StarEntryRequest {
    const message = createBaseStarEntryRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseStarEntryResponse(): StarEntryResponse {
  return {};
}

export const StarEntryResponse: MessageFns<StarEntryResponse> = {
  encode(_: StarEntryResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StarEntryResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStarEntryResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): StarEntryResponse {
    return {};
  },

  toJSON(_: StarEntryResponse): unknown {
    const obj: any = {};
    return obj;
  },

  create(base?: DeepPartial<StarEntryResponse>): StarEntryResponse {
    return StarEntryResponse.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<StarEntryResponse>): StarEntryResponse {
    const message = createBaseStarEntryResponse();
    return message;
  },
};

function createBaseUnstarEntryRequest(): UnstarEntryRequest {
  return { name: "" };
}

export const UnstarEntryRequest: MessageFns<UnstarEntryRequest> = {
  encode(message: UnstarEntryRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UnstarEntryRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUnstarEntryRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UnstarEntryRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: UnstarEntryRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<UnstarEntryRequest>): UnstarEntryRequest {
    return UnstarEntryRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UnstarEntryRequest>): UnstarEntryRequest {
    const message = createBaseUnstarEntryRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseUnstarEntryResponse(): UnstarEntryResponse {
  return {};
}

export const UnstarEntryResponse: MessageFns<UnstarEntryResponse> = {
  encode(_: UnstarEntryResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UnstarEntryResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUnstarEntryResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): UnstarEntryResponse {
    return {};
  },

  toJSON(_: UnstarEntryResponse): unknown {
    const obj: any = {};
    return obj;
  },

  create(base?: DeepPartial<UnstarEntryResponse>): UnstarEntryResponse {
    return UnstarEntryResponse.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<UnstarEntryResponse>): UnstarEntryResponse {
    const message = createBaseUnstarEntryResponse();
    return message;
  },
};

function createBaseImportEntriesRequest(): ImportEntriesRequest {
  return { parent: "", gcsBucketPath: undefined, jobId: "" };
}

export const ImportEntriesRequest: MessageFns<ImportEntriesRequest> = {
  encode(message: ImportEntriesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.gcsBucketPath !== undefined) {
      writer.uint32(18).string(message.gcsBucketPath);
    }
    if (message.jobId !== "") {
      writer.uint32(26).string(message.jobId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportEntriesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportEntriesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.gcsBucketPath = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.jobId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportEntriesRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      gcsBucketPath: isSet(object.gcsBucketPath) ? globalThis.String(object.gcsBucketPath) : undefined,
      jobId: isSet(object.jobId) ? globalThis.String(object.jobId) : "",
    };
  },

  toJSON(message: ImportEntriesRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.gcsBucketPath !== undefined) {
      obj.gcsBucketPath = message.gcsBucketPath;
    }
    if (message.jobId !== "") {
      obj.jobId = message.jobId;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportEntriesRequest>): ImportEntriesRequest {
    return ImportEntriesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportEntriesRequest>): ImportEntriesRequest {
    const message = createBaseImportEntriesRequest();
    message.parent = object.parent ?? "";
    message.gcsBucketPath = object.gcsBucketPath ?? undefined;
    message.jobId = object.jobId ?? "";
    return message;
  },
};

function createBaseImportEntriesResponse(): ImportEntriesResponse {
  return { upsertedEntriesCount: undefined, deletedEntriesCount: undefined };
}

export const ImportEntriesResponse: MessageFns<ImportEntriesResponse> = {
  encode(message: ImportEntriesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.upsertedEntriesCount !== undefined) {
      writer.uint32(40).int64(message.upsertedEntriesCount.toString());
    }
    if (message.deletedEntriesCount !== undefined) {
      writer.uint32(48).int64(message.deletedEntriesCount.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportEntriesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportEntriesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 5:
          if (tag !== 40) {
            break;
          }

          message.upsertedEntriesCount = Long.fromString(reader.int64().toString());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.deletedEntriesCount = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportEntriesResponse {
    return {
      upsertedEntriesCount: isSet(object.upsertedEntriesCount)
        ? Long.fromValue(object.upsertedEntriesCount)
        : undefined,
      deletedEntriesCount: isSet(object.deletedEntriesCount) ? Long.fromValue(object.deletedEntriesCount) : undefined,
    };
  },

  toJSON(message: ImportEntriesResponse): unknown {
    const obj: any = {};
    if (message.upsertedEntriesCount !== undefined) {
      obj.upsertedEntriesCount = (message.upsertedEntriesCount || Long.ZERO).toString();
    }
    if (message.deletedEntriesCount !== undefined) {
      obj.deletedEntriesCount = (message.deletedEntriesCount || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ImportEntriesResponse>): ImportEntriesResponse {
    return ImportEntriesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportEntriesResponse>): ImportEntriesResponse {
    const message = createBaseImportEntriesResponse();
    message.upsertedEntriesCount = (object.upsertedEntriesCount !== undefined && object.upsertedEntriesCount !== null)
      ? Long.fromValue(object.upsertedEntriesCount)
      : undefined;
    message.deletedEntriesCount = (object.deletedEntriesCount !== undefined && object.deletedEntriesCount !== null)
      ? Long.fromValue(object.deletedEntriesCount)
      : undefined;
    return message;
  },
};

function createBaseImportEntriesMetadata(): ImportEntriesMetadata {
  return { state: 0, errors: [] };
}

export const ImportEntriesMetadata: MessageFns<ImportEntriesMetadata> = {
  encode(message: ImportEntriesMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.state !== 0) {
      writer.uint32(8).int32(message.state);
    }
    for (const v of message.errors) {
      Status.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportEntriesMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportEntriesMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.errors.push(Status.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportEntriesMetadata {
    return {
      state: isSet(object.state) ? importEntriesMetadata_ImportStateFromJSON(object.state) : 0,
      errors: globalThis.Array.isArray(object?.errors) ? object.errors.map((e: any) => Status.fromJSON(e)) : [],
    };
  },

  toJSON(message: ImportEntriesMetadata): unknown {
    const obj: any = {};
    if (message.state !== 0) {
      obj.state = importEntriesMetadata_ImportStateToJSON(message.state);
    }
    if (message.errors?.length) {
      obj.errors = message.errors.map((e) => Status.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ImportEntriesMetadata>): ImportEntriesMetadata {
    return ImportEntriesMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportEntriesMetadata>): ImportEntriesMetadata {
    const message = createBaseImportEntriesMetadata();
    message.state = object.state ?? 0;
    message.errors = object.errors?.map((e) => Status.fromPartial(e)) || [];
    return message;
  },
};

function createBaseModifyEntryOverviewRequest(): ModifyEntryOverviewRequest {
  return { name: "", entryOverview: undefined };
}

export const ModifyEntryOverviewRequest: MessageFns<ModifyEntryOverviewRequest> = {
  encode(message: ModifyEntryOverviewRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.entryOverview !== undefined) {
      EntryOverview.encode(message.entryOverview, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ModifyEntryOverviewRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseModifyEntryOverviewRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.entryOverview = EntryOverview.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ModifyEntryOverviewRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      entryOverview: isSet(object.entryOverview) ? EntryOverview.fromJSON(object.entryOverview) : undefined,
    };
  },

  toJSON(message: ModifyEntryOverviewRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.entryOverview !== undefined) {
      obj.entryOverview = EntryOverview.toJSON(message.entryOverview);
    }
    return obj;
  },

  create(base?: DeepPartial<ModifyEntryOverviewRequest>): ModifyEntryOverviewRequest {
    return ModifyEntryOverviewRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ModifyEntryOverviewRequest>): ModifyEntryOverviewRequest {
    const message = createBaseModifyEntryOverviewRequest();
    message.name = object.name ?? "";
    message.entryOverview = (object.entryOverview !== undefined && object.entryOverview !== null)
      ? EntryOverview.fromPartial(object.entryOverview)
      : undefined;
    return message;
  },
};

function createBaseModifyEntryContactsRequest(): ModifyEntryContactsRequest {
  return { name: "", contacts: undefined };
}

export const ModifyEntryContactsRequest: MessageFns<ModifyEntryContactsRequest> = {
  encode(message: ModifyEntryContactsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.contacts !== undefined) {
      Contacts.encode(message.contacts, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ModifyEntryContactsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseModifyEntryContactsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.contacts = Contacts.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ModifyEntryContactsRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      contacts: isSet(object.contacts) ? Contacts.fromJSON(object.contacts) : undefined,
    };
  },

  toJSON(message: ModifyEntryContactsRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.contacts !== undefined) {
      obj.contacts = Contacts.toJSON(message.contacts);
    }
    return obj;
  },

  create(base?: DeepPartial<ModifyEntryContactsRequest>): ModifyEntryContactsRequest {
    return ModifyEntryContactsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ModifyEntryContactsRequest>): ModifyEntryContactsRequest {
    const message = createBaseModifyEntryContactsRequest();
    message.name = object.name ?? "";
    message.contacts = (object.contacts !== undefined && object.contacts !== null)
      ? Contacts.fromPartial(object.contacts)
      : undefined;
    return message;
  },
};

/**
 * Data Catalog API service allows you to discover, understand, and manage
 * your data.
 */
export type DataCatalogDefinition = typeof DataCatalogDefinition;
export const DataCatalogDefinition = {
  name: "DataCatalog",
  fullName: "google.cloud.datacatalog.v1.DataCatalog",
  methods: {
    /**
     * Searches Data Catalog for multiple resources like entries and tags that
     * match a query.
     *
     * This is a [Custom Method]
     * (https://cloud.google.com/apis/design/custom_methods) that doesn't return
     * all information on a resource, only its ID and high level fields. To get
     * more information, you can subsequently call specific get methods.
     *
     * Note: Data Catalog search queries don't guarantee full recall. Results
     * that match your query might not be returned, even in subsequent
     * result pages. Additionally, returned (and not returned) results can vary
     * if you repeat search queries.
     *
     * For more information, see [Data Catalog search syntax]
     * (https://cloud.google.com/data-catalog/docs/how-to/search-reference).
     */
    searchCatalog: {
      name: "SearchCatalog",
      requestType: SearchCatalogRequest,
      requestStream: false,
      responseType: SearchCatalogResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([11, 115, 99, 111, 112, 101, 44, 113, 117, 101, 114, 121])],
          578365826: [
            Buffer.from([
              23,
              58,
              1,
              42,
              34,
              18,
              47,
              118,
              49,
              47,
              99,
              97,
              116,
              97,
              108,
              111,
              103,
              58,
              115,
              101,
              97,
              114,
              99,
              104,
            ]),
          ],
        },
      },
    },
    /**
     * Creates an entry group.
     *
     * An entry group contains logically related entries together with [Cloud
     * Identity and Access Management](/data-catalog/docs/concepts/iam) policies.
     * These policies specify users who can create, edit, and view entries
     * within entry groups.
     *
     * Data Catalog automatically creates entry groups with names that start with
     * the `@` symbol for the following resources:
     *
     * * BigQuery entries (`@bigquery`)
     * * Pub/Sub topics (`@pubsub`)
     * * Dataproc Metastore services (`@dataproc_metastore_{SERVICE_NAME_HASH}`)
     *
     * You can create your own entry groups for Cloud Storage fileset entries
     * and custom entries together with the corresponding IAM policies.
     * User-created entry groups can't contain the `@` symbol, it is reserved
     * for automatically created groups.
     *
     * Entry groups, like entries, can be searched.
     *
     * A maximum of 10,000 entry groups may be created per organization across all
     * locations.
     *
     * You must enable the Data Catalog API in the project identified by
     * the `parent` parameter. For more information, see [Data Catalog resource
     * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
     */
    createEntryGroup: {
      name: "CreateEntryGroup",
      requestType: CreateEntryGroupRequest,
      requestStream: false,
      responseType: EntryGroup,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              33,
              112,
              97,
              114,
              101,
              110,
              116,
              44,
              101,
              110,
              116,
              114,
              121,
              95,
              103,
              114,
              111,
              117,
              112,
              95,
              105,
              100,
              44,
              101,
              110,
              116,
              114,
              121,
              95,
              103,
              114,
              111,
              117,
              112,
            ]),
          ],
          578365826: [
            Buffer.from([
              62,
              58,
              11,
              101,
              110,
              116,
              114,
              121,
              95,
              103,
              114,
              111,
              117,
              112,
              34,
              47,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
            ]),
          ],
        },
      },
    },
    /** Gets an entry group. */
    getEntryGroup: {
      name: "GetEntryGroup",
      requestType: GetEntryGroupRequest,
      requestStream: false,
      responseType: EntryGroup,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([4, 110, 97, 109, 101]),
            Buffer.from([14, 110, 97, 109, 101, 44, 114, 101, 97, 100, 95, 109, 97, 115, 107]),
          ],
          578365826: [
            Buffer.from([
              49,
              18,
              47,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Updates an entry group.
     *
     * You must enable the Data Catalog API in the project identified by
     * the `entry_group.name` parameter. For more information, see [Data Catalog
     * resource
     * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
     */
    updateEntryGroup: {
      name: "UpdateEntryGroup",
      requestType: UpdateEntryGroupRequest,
      requestStream: false,
      responseType: EntryGroup,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([11, 101, 110, 116, 114, 121, 95, 103, 114, 111, 117, 112]),
            Buffer.from([
              23,
              101,
              110,
              116,
              114,
              121,
              95,
              103,
              114,
              111,
              117,
              112,
              44,
              117,
              112,
              100,
              97,
              116,
              101,
              95,
              109,
              97,
              115,
              107,
            ]),
          ],
          578365826: [
            Buffer.from([
              74,
              58,
              11,
              101,
              110,
              116,
              114,
              121,
              95,
              103,
              114,
              111,
              117,
              112,
              50,
              59,
              47,
              118,
              49,
              47,
              123,
              101,
              110,
              116,
              114,
              121,
              95,
              103,
              114,
              111,
              117,
              112,
              46,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Deletes an entry group.
     *
     * You must enable the Data Catalog API in the project
     * identified by the `name` parameter. For more information, see [Data Catalog
     * resource
     * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
     */
    deleteEntryGroup: {
      name: "DeleteEntryGroup",
      requestType: DeleteEntryGroupRequest,
      requestStream: false,
      responseType: Empty,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              49,
              42,
              47,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /** Lists entry groups. */
    listEntryGroups: {
      name: "ListEntryGroups",
      requestType: ListEntryGroupsRequest,
      requestStream: false,
      responseType: ListEntryGroupsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([6, 112, 97, 114, 101, 110, 116])],
          578365826: [
            Buffer.from([
              49,
              18,
              47,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Creates an entry.
     *
     * You can create entries only with 'FILESET', 'CLUSTER', 'DATA_STREAM',
     * or custom types. Data Catalog automatically creates entries with other
     * types during metadata ingestion from integrated systems.
     *
     * You must enable the Data Catalog API in the project identified by
     * the `parent` parameter. For more information, see [Data Catalog resource
     * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
     *
     * An entry group can have a maximum of 100,000 entries.
     */
    createEntry: {
      name: "CreateEntry",
      requestType: CreateEntryRequest,
      requestStream: false,
      responseType: Entry,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              21,
              112,
              97,
              114,
              101,
              110,
              116,
              44,
              101,
              110,
              116,
              114,
              121,
              95,
              105,
              100,
              44,
              101,
              110,
              116,
              114,
              121,
            ]),
          ],
          578365826: [
            Buffer.from([
              66,
              58,
              5,
              101,
              110,
              116,
              114,
              121,
              34,
              57,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              125,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Updates an existing entry.
     *
     * You must enable the Data Catalog API in the project identified by
     * the `entry.name` parameter. For more information, see [Data Catalog
     * resource
     * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
     */
    updateEntry: {
      name: "UpdateEntry",
      requestType: UpdateEntryRequest,
      requestStream: false,
      responseType: Entry,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([5, 101, 110, 116, 114, 121]),
            Buffer.from([17, 101, 110, 116, 114, 121, 44, 117, 112, 100, 97, 116, 101, 95, 109, 97, 115, 107]),
          ],
          578365826: [
            Buffer.from([
              72,
              58,
              5,
              101,
              110,
              116,
              114,
              121,
              50,
              63,
              47,
              118,
              49,
              47,
              123,
              101,
              110,
              116,
              114,
              121,
              46,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Deletes an existing entry.
     *
     * You can delete only the entries created by the
     * [CreateEntry][google.cloud.datacatalog.v1.DataCatalog.CreateEntry]
     * method.
     *
     * You must enable the Data Catalog API in the project identified by
     * the `name` parameter. For more information, see [Data Catalog
     * resource
     * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
     */
    deleteEntry: {
      name: "DeleteEntry",
      requestType: DeleteEntryRequest,
      requestStream: false,
      responseType: Empty,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              59,
              42,
              57,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /** Gets an entry. */
    getEntry: {
      name: "GetEntry",
      requestType: GetEntryRequest,
      requestStream: false,
      responseType: Entry,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              59,
              18,
              57,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Gets an entry by its target resource name.
     *
     * The resource name comes from the source Google Cloud Platform service.
     */
    lookupEntry: {
      name: "LookupEntry",
      requestType: LookupEntryRequest,
      requestStream: false,
      responseType: Entry,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              20,
              18,
              18,
              47,
              118,
              49,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
              58,
              108,
              111,
              111,
              107,
              117,
              112,
            ]),
          ],
        },
      },
    },
    /**
     * Lists entries.
     *
     * Note: Currently, this method can list only custom entries.
     * To get a list of both custom and automatically created entries, use
     * [SearchCatalog][google.cloud.datacatalog.v1.DataCatalog.SearchCatalog].
     */
    listEntries: {
      name: "ListEntries",
      requestType: ListEntriesRequest,
      requestStream: false,
      responseType: ListEntriesResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([6, 112, 97, 114, 101, 110, 116])],
          578365826: [
            Buffer.from([
              59,
              18,
              57,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              125,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Modifies entry overview, part of the business context of an
     * [Entry][google.cloud.datacatalog.v1.Entry].
     *
     * To call this method, you must have the `datacatalog.entries.updateOverview`
     * IAM permission on the corresponding project.
     */
    modifyEntryOverview: {
      name: "ModifyEntryOverview",
      requestType: ModifyEntryOverviewRequest,
      requestStream: false,
      responseType: EntryOverview,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              82,
              58,
              1,
              42,
              34,
              77,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
              47,
              42,
              125,
              58,
              109,
              111,
              100,
              105,
              102,
              121,
              69,
              110,
              116,
              114,
              121,
              79,
              118,
              101,
              114,
              118,
              105,
              101,
              119,
            ]),
          ],
        },
      },
    },
    /**
     * Modifies contacts, part of the business context of an
     * [Entry][google.cloud.datacatalog.v1.Entry].
     *
     * To call this method, you must have the `datacatalog.entries.updateContacts`
     * IAM permission on the corresponding project.
     */
    modifyEntryContacts: {
      name: "ModifyEntryContacts",
      requestType: ModifyEntryContactsRequest,
      requestStream: false,
      responseType: Contacts,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              82,
              58,
              1,
              42,
              34,
              77,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
              47,
              42,
              125,
              58,
              109,
              111,
              100,
              105,
              102,
              121,
              69,
              110,
              116,
              114,
              121,
              67,
              111,
              110,
              116,
              97,
              99,
              116,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Creates a tag template.
     *
     * You must enable the Data Catalog API in the project identified by the
     * `parent` parameter.
     * For more information, see [Data Catalog resource project]
     * (https://cloud.google.com/data-catalog/docs/concepts/resource-project).
     */
    createTagTemplate: {
      name: "CreateTagTemplate",
      requestType: CreateTagTemplateRequest,
      requestStream: false,
      responseType: TagTemplate,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              35,
              112,
              97,
              114,
              101,
              110,
              116,
              44,
              116,
              97,
              103,
              95,
              116,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              95,
              105,
              100,
              44,
              116,
              97,
              103,
              95,
              116,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
            ]),
          ],
          578365826: [
            Buffer.from([
              64,
              58,
              12,
              116,
              97,
              103,
              95,
              116,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              34,
              48,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              47,
              116,
              97,
              103,
              84,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              115,
            ]),
          ],
        },
      },
    },
    /** Gets a tag template. */
    getTagTemplate: {
      name: "GetTagTemplate",
      requestType: GetTagTemplateRequest,
      requestStream: false,
      responseType: TagTemplate,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              50,
              18,
              48,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              116,
              97,
              103,
              84,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Updates a tag template.
     *
     * You can't update template fields with this method. These fields are
     * separate resources with their own create, update, and delete methods.
     *
     * You must enable the Data Catalog API in the project identified by
     * the `tag_template.name` parameter. For more information, see [Data Catalog
     * resource
     * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
     */
    updateTagTemplate: {
      name: "UpdateTagTemplate",
      requestType: UpdateTagTemplateRequest,
      requestStream: false,
      responseType: TagTemplate,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([12, 116, 97, 103, 95, 116, 101, 109, 112, 108, 97, 116, 101]),
            Buffer.from([
              24,
              116,
              97,
              103,
              95,
              116,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              44,
              117,
              112,
              100,
              97,
              116,
              101,
              95,
              109,
              97,
              115,
              107,
            ]),
          ],
          578365826: [
            Buffer.from([
              77,
              58,
              12,
              116,
              97,
              103,
              95,
              116,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              50,
              61,
              47,
              118,
              49,
              47,
              123,
              116,
              97,
              103,
              95,
              116,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              46,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              116,
              97,
              103,
              84,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Deletes a tag template and all tags that use it.
     *
     * You must enable the Data Catalog API in the project identified by
     * the `name` parameter. For more information, see [Data Catalog resource
     * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
     */
    deleteTagTemplate: {
      name: "DeleteTagTemplate",
      requestType: DeleteTagTemplateRequest,
      requestStream: false,
      responseType: Empty,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([10, 110, 97, 109, 101, 44, 102, 111, 114, 99, 101])],
          578365826: [
            Buffer.from([
              50,
              42,
              48,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              116,
              97,
              103,
              84,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Creates a field in a tag template.
     *
     * You must enable the Data Catalog API in the project identified by
     * the `parent` parameter. For more information, see [Data Catalog resource
     * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
     */
    createTagTemplateField: {
      name: "CreateTagTemplateField",
      requestType: CreateTagTemplateFieldRequest,
      requestStream: false,
      responseType: TagTemplateField,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              47,
              112,
              97,
              114,
              101,
              110,
              116,
              44,
              116,
              97,
              103,
              95,
              116,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              95,
              102,
              105,
              101,
              108,
              100,
              95,
              105,
              100,
              44,
              116,
              97,
              103,
              95,
              116,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              95,
              102,
              105,
              101,
              108,
              100,
            ]),
          ],
          578365826: [
            Buffer.from([
              79,
              58,
              18,
              116,
              97,
              103,
              95,
              116,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              95,
              102,
              105,
              101,
              108,
              100,
              34,
              57,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              116,
              97,
              103,
              84,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              115,
              47,
              42,
              125,
              47,
              102,
              105,
              101,
              108,
              100,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Updates a field in a tag template.
     *
     * You can't update the field type with this method.
     *
     * You must enable the Data Catalog API in the project
     * identified by the `name` parameter. For more information, see [Data Catalog
     * resource
     * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
     */
    updateTagTemplateField: {
      name: "UpdateTagTemplateField",
      requestType: UpdateTagTemplateFieldRequest,
      requestStream: false,
      responseType: TagTemplateField,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              23,
              110,
              97,
              109,
              101,
              44,
              116,
              97,
              103,
              95,
              116,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              95,
              102,
              105,
              101,
              108,
              100,
            ]),
            Buffer.from([
              35,
              110,
              97,
              109,
              101,
              44,
              116,
              97,
              103,
              95,
              116,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              95,
              102,
              105,
              101,
              108,
              100,
              44,
              117,
              112,
              100,
              97,
              116,
              101,
              95,
              109,
              97,
              115,
              107,
            ]),
          ],
          578365826: [
            Buffer.from([
              79,
              58,
              18,
              116,
              97,
              103,
              95,
              116,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              95,
              102,
              105,
              101,
              108,
              100,
              50,
              57,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              116,
              97,
              103,
              84,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              115,
              47,
              42,
              47,
              102,
              105,
              101,
              108,
              100,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Renames a field in a tag template.
     *
     * You must enable the Data Catalog API in the project identified by the
     * `name` parameter. For more information, see [Data Catalog resource project]
     * (https://cloud.google.com/data-catalog/docs/concepts/resource-project).
     */
    renameTagTemplateField: {
      name: "RenameTagTemplateField",
      requestType: RenameTagTemplateFieldRequest,
      requestStream: false,
      responseType: TagTemplateField,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              30,
              110,
              97,
              109,
              101,
              44,
              110,
              101,
              119,
              95,
              116,
              97,
              103,
              95,
              116,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              95,
              102,
              105,
              101,
              108,
              100,
              95,
              105,
              100,
            ]),
          ],
          578365826: [
            Buffer.from([
              69,
              58,
              1,
              42,
              34,
              64,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              116,
              97,
              103,
              84,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              115,
              47,
              42,
              47,
              102,
              105,
              101,
              108,
              100,
              115,
              47,
              42,
              125,
              58,
              114,
              101,
              110,
              97,
              109,
              101,
            ]),
          ],
        },
      },
    },
    /**
     * Renames an enum value in a tag template.
     *
     * Within a single enum field, enum values must be unique.
     */
    renameTagTemplateFieldEnumValue: {
      name: "RenameTagTemplateFieldEnumValue",
      requestType: RenameTagTemplateFieldEnumValueRequest,
      requestStream: false,
      responseType: TagTemplateField,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              32,
              110,
              97,
              109,
              101,
              44,
              110,
              101,
              119,
              95,
              101,
              110,
              117,
              109,
              95,
              118,
              97,
              108,
              117,
              101,
              95,
              100,
              105,
              115,
              112,
              108,
              97,
              121,
              95,
              110,
              97,
              109,
              101,
            ]),
          ],
          578365826: [
            Buffer.from([
              82,
              58,
              1,
              42,
              34,
              77,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              116,
              97,
              103,
              84,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              115,
              47,
              42,
              47,
              102,
              105,
              101,
              108,
              100,
              115,
              47,
              42,
              47,
              101,
              110,
              117,
              109,
              86,
              97,
              108,
              117,
              101,
              115,
              47,
              42,
              125,
              58,
              114,
              101,
              110,
              97,
              109,
              101,
            ]),
          ],
        },
      },
    },
    /**
     * Deletes a field in a tag template and all uses of this field from the tags
     * based on this template.
     *
     * You must enable the Data Catalog API in the project identified by
     * the `name` parameter. For more information, see [Data Catalog resource
     * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
     */
    deleteTagTemplateField: {
      name: "DeleteTagTemplateField",
      requestType: DeleteTagTemplateFieldRequest,
      requestStream: false,
      responseType: Empty,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([10, 110, 97, 109, 101, 44, 102, 111, 114, 99, 101])],
          578365826: [
            Buffer.from([
              59,
              42,
              57,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              116,
              97,
              103,
              84,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              115,
              47,
              42,
              47,
              102,
              105,
              101,
              108,
              100,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Creates a tag and assigns it to:
     *
     * * An [Entry][google.cloud.datacatalog.v1.Entry] if the method name is
     *   `projects.locations.entryGroups.entries.tags.create`.
     * * Or [EntryGroup][google.cloud.datacatalog.v1.EntryGroup]if the method
     *   name is `projects.locations.entryGroups.tags.create`.
     *
     * Note: The project identified by the `parent` parameter for the [tag]
     * (https://cloud.google.com/data-catalog/docs/reference/rest/v1/projects.locations.entryGroups.entries.tags/create#path-parameters)
     * and the [tag template]
     * (https://cloud.google.com/data-catalog/docs/reference/rest/v1/projects.locations.tagTemplates/create#path-parameters)
     * used to create the tag must be in the same organization.
     */
    createTag: {
      name: "CreateTag",
      requestType: CreateTagRequest,
      requestStream: false,
      responseType: Tag,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([10, 112, 97, 114, 101, 110, 116, 44, 116, 97, 103])],
          578365826: [
            Buffer.from([
              134,
              1,
              58,
              3,
              116,
              97,
              103,
              90,
              61,
              58,
              3,
              116,
              97,
              103,
              34,
              54,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              125,
              47,
              116,
              97,
              103,
              115,
              34,
              64,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
              47,
              42,
              125,
              47,
              116,
              97,
              103,
              115,
            ]),
          ],
        },
      },
    },
    /** Updates an existing tag. */
    updateTag: {
      name: "UpdateTag",
      requestType: UpdateTagRequest,
      requestStream: false,
      responseType: Tag,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([3, 116, 97, 103]),
            Buffer.from([15, 116, 97, 103, 44, 117, 112, 100, 97, 116, 101, 95, 109, 97, 115, 107]),
          ],
          578365826: [
            Buffer.from([
              142,
              1,
              58,
              3,
              116,
              97,
              103,
              90,
              65,
              58,
              3,
              116,
              97,
              103,
              50,
              58,
              47,
              118,
              49,
              47,
              123,
              116,
              97,
              103,
              46,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              47,
              116,
              97,
              103,
              115,
              47,
              42,
              125,
              50,
              68,
              47,
              118,
              49,
              47,
              123,
              116,
              97,
              103,
              46,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
              47,
              42,
              47,
              116,
              97,
              103,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /** Deletes a tag. */
    deleteTag: {
      name: "DeleteTag",
      requestType: DeleteTagRequest,
      requestStream: false,
      responseType: Empty,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              124,
              90,
              56,
              42,
              54,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              47,
              116,
              97,
              103,
              115,
              47,
              42,
              125,
              42,
              64,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
              47,
              42,
              47,
              116,
              97,
              103,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Lists tags assigned to an [Entry][google.cloud.datacatalog.v1.Entry].
     * The [columns][google.cloud.datacatalog.v1.Tag.column] in the response are
     * lowercased.
     */
    listTags: {
      name: "ListTags",
      requestType: ListTagsRequest,
      requestStream: false,
      responseType: ListTagsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([6, 112, 97, 114, 101, 110, 116])],
          578365826: [
            Buffer.from([
              124,
              90,
              56,
              18,
              54,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              125,
              47,
              116,
              97,
              103,
              115,
              18,
              64,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
              47,
              42,
              125,
              47,
              116,
              97,
              103,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * `ReconcileTags` creates or updates a list of tags on the entry.
     * If the
     * [ReconcileTagsRequest.force_delete_missing][google.cloud.datacatalog.v1.ReconcileTagsRequest.force_delete_missing]
     * parameter is set, the operation deletes tags not included in the input tag
     * list.
     *
     * `ReconcileTags` returns a [long-running operation]
     * [google.longrunning.Operation] resource that can be queried with
     * [Operations.GetOperation][google.longrunning.Operations.GetOperation]
     * to return [ReconcileTagsMetadata]
     * [google.cloud.datacatalog.v1.ReconcileTagsMetadata] and
     * a [ReconcileTagsResponse]
     * [google.cloud.datacatalog.v1.ReconcileTagsResponse] message.
     */
    reconcileTags: {
      name: "ReconcileTags",
      requestType: ReconcileTagsRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              46,
              10,
              21,
              82,
              101,
              99,
              111,
              110,
              99,
              105,
              108,
              101,
              84,
              97,
              103,
              115,
              82,
              101,
              115,
              112,
              111,
              110,
              115,
              101,
              18,
              21,
              82,
              101,
              99,
              111,
              110,
              99,
              105,
              108,
              101,
              84,
              97,
              103,
              115,
              77,
              101,
              116,
              97,
              100,
              97,
              116,
              97,
            ]),
          ],
          578365826: [
            Buffer.from([
              79,
              58,
              1,
              42,
              34,
              74,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
              47,
              42,
              125,
              47,
              116,
              97,
              103,
              115,
              58,
              114,
              101,
              99,
              111,
              110,
              99,
              105,
              108,
              101,
            ]),
          ],
        },
      },
    },
    /**
     * Marks an [Entry][google.cloud.datacatalog.v1.Entry] as starred by
     * the current user. Starring information is private to each user.
     */
    starEntry: {
      name: "StarEntry",
      requestType: StarEntryRequest,
      requestStream: false,
      responseType: StarEntryResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              67,
              58,
              1,
              42,
              34,
              62,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
              47,
              42,
              125,
              58,
              115,
              116,
              97,
              114,
            ]),
          ],
        },
      },
    },
    /**
     * Marks an [Entry][google.cloud.datacatalog.v1.Entry] as NOT starred by
     * the current user. Starring information is private to each user.
     */
    unstarEntry: {
      name: "UnstarEntry",
      requestType: UnstarEntryRequest,
      requestStream: false,
      responseType: UnstarEntryResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              69,
              58,
              1,
              42,
              34,
              64,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
              47,
              42,
              125,
              58,
              117,
              110,
              115,
              116,
              97,
              114,
            ]),
          ],
        },
      },
    },
    /**
     * Sets an access control policy for a resource. Replaces any existing
     * policy.
     *
     * Supported resources are:
     *
     * - Tag templates
     * - Entry groups
     *
     * Note: This method sets policies only within Data Catalog and can't be
     * used to manage policies in BigQuery, Pub/Sub, Dataproc Metastore, and any
     * external Google Cloud Platform resources synced with the Data Catalog.
     *
     * To call this method, you must have the following Google IAM permissions:
     *
     * - `datacatalog.tagTemplates.setIamPolicy` to set policies on tag
     *   templates.
     * - `datacatalog.entryGroups.setIamPolicy` to set policies on entry groups.
     */
    setIamPolicy: {
      name: "SetIamPolicy",
      requestType: SetIamPolicyRequest,
      requestStream: false,
      responseType: Policy,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([15, 114, 101, 115, 111, 117, 114, 99, 101, 44, 112, 111, 108, 105, 99, 121])],
          578365826: [
            Buffer.from([
              141,
              1,
              58,
              1,
              42,
              90,
              69,
              58,
              1,
              42,
              34,
              64,
              47,
              118,
              49,
              47,
              123,
              114,
              101,
              115,
              111,
              117,
              114,
              99,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              125,
              58,
              115,
              101,
              116,
              73,
              97,
              109,
              80,
              111,
              108,
              105,
              99,
              121,
              34,
              65,
              47,
              118,
              49,
              47,
              123,
              114,
              101,
              115,
              111,
              117,
              114,
              99,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              116,
              97,
              103,
              84,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              115,
              47,
              42,
              125,
              58,
              115,
              101,
              116,
              73,
              97,
              109,
              80,
              111,
              108,
              105,
              99,
              121,
            ]),
          ],
        },
      },
    },
    /**
     * Gets the access control policy for a resource.
     *
     * May return:
     *
     * * A`NOT_FOUND` error if the resource doesn't exist or you don't have the
     *   permission to view it.
     * * An empty policy if the resource exists but doesn't have a set policy.
     *
     * Supported resources are:
     *
     * - Tag templates
     * - Entry groups
     *
     * Note: This method doesn't get policies from Google Cloud Platform
     * resources ingested into Data Catalog.
     *
     * To call this method, you must have the following Google IAM permissions:
     *
     * - `datacatalog.tagTemplates.getIamPolicy` to get policies on tag
     *   templates.
     * - `datacatalog.entryGroups.getIamPolicy` to get policies on entry groups.
     */
    getIamPolicy: {
      name: "GetIamPolicy",
      requestType: GetIamPolicyRequest,
      requestStream: false,
      responseType: Policy,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([8, 114, 101, 115, 111, 117, 114, 99, 101])],
          578365826: [
            Buffer.from([
              222,
              1,
              58,
              1,
              42,
              90,
              69,
              58,
              1,
              42,
              34,
              64,
              47,
              118,
              49,
              47,
              123,
              114,
              101,
              115,
              111,
              117,
              114,
              99,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              125,
              58,
              103,
              101,
              116,
              73,
              97,
              109,
              80,
              111,
              108,
              105,
              99,
              121,
              90,
              79,
              58,
              1,
              42,
              34,
              74,
              47,
              118,
              49,
              47,
              123,
              114,
              101,
              115,
              111,
              117,
              114,
              99,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
              47,
              42,
              125,
              58,
              103,
              101,
              116,
              73,
              97,
              109,
              80,
              111,
              108,
              105,
              99,
              121,
              34,
              65,
              47,
              118,
              49,
              47,
              123,
              114,
              101,
              115,
              111,
              117,
              114,
              99,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              116,
              97,
              103,
              84,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              115,
              47,
              42,
              125,
              58,
              103,
              101,
              116,
              73,
              97,
              109,
              80,
              111,
              108,
              105,
              99,
              121,
            ]),
          ],
        },
      },
    },
    /**
     * Gets your permissions on a resource.
     *
     * Returns an empty set of permissions if the resource doesn't exist.
     *
     * Supported resources are:
     *
     * - Tag templates
     * - Entry groups
     *
     * Note: This method gets policies only within Data Catalog and can't be
     * used to get policies from BigQuery, Pub/Sub, Dataproc Metastore, and any
     * external Google Cloud Platform resources ingested into Data Catalog.
     *
     * No Google IAM permissions are required to call this method.
     */
    testIamPermissions: {
      name: "TestIamPermissions",
      requestType: TestIamPermissionsRequest,
      requestStream: false,
      responseType: TestIamPermissionsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              240,
              1,
              58,
              1,
              42,
              90,
              75,
              58,
              1,
              42,
              34,
              70,
              47,
              118,
              49,
              47,
              123,
              114,
              101,
              115,
              111,
              117,
              114,
              99,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              125,
              58,
              116,
              101,
              115,
              116,
              73,
              97,
              109,
              80,
              101,
              114,
              109,
              105,
              115,
              115,
              105,
              111,
              110,
              115,
              90,
              85,
              58,
              1,
              42,
              34,
              80,
              47,
              118,
              49,
              47,
              123,
              114,
              101,
              115,
              111,
              117,
              114,
              99,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
              47,
              42,
              125,
              58,
              116,
              101,
              115,
              116,
              73,
              97,
              109,
              80,
              101,
              114,
              109,
              105,
              115,
              115,
              105,
              111,
              110,
              115,
              34,
              71,
              47,
              118,
              49,
              47,
              123,
              114,
              101,
              115,
              111,
              117,
              114,
              99,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              116,
              97,
              103,
              84,
              101,
              109,
              112,
              108,
              97,
              116,
              101,
              115,
              47,
              42,
              125,
              58,
              116,
              101,
              115,
              116,
              73,
              97,
              109,
              80,
              101,
              114,
              109,
              105,
              115,
              115,
              105,
              111,
              110,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Imports entries from a source, such as data previously dumped into a
     * Cloud Storage bucket, into Data Catalog. Import of entries
     * is a sync operation that reconciles the state of the third-party system
     * with the Data Catalog.
     *
     * `ImportEntries` accepts source data snapshots of a third-party system.
     * Snapshot should be delivered as a .wire or base65-encoded .txt file
     * containing a sequence of Protocol Buffer messages of
     * [DumpItem][google.cloud.datacatalog.v1.DumpItem] type.
     *
     * `ImportEntries` returns a [long-running operation]
     * [google.longrunning.Operation] resource that can be queried with
     * [Operations.GetOperation][google.longrunning.Operations.GetOperation]
     * to return
     * [ImportEntriesMetadata][google.cloud.datacatalog.v1.ImportEntriesMetadata]
     * and an
     * [ImportEntriesResponse][google.cloud.datacatalog.v1.ImportEntriesResponse]
     * message.
     */
    importEntries: {
      name: "ImportEntries",
      requestType: ImportEntriesRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          8394: [
            Buffer.from([
              46,
              10,
              21,
              73,
              109,
              112,
              111,
              114,
              116,
              69,
              110,
              116,
              114,
              105,
              101,
              115,
              82,
              101,
              115,
              112,
              111,
              110,
              115,
              101,
              18,
              21,
              73,
              109,
              112,
              111,
              114,
              116,
              69,
              110,
              116,
              114,
              105,
              101,
              115,
              77,
              101,
              116,
              97,
              100,
              97,
              116,
              97,
            ]),
          ],
          578365826: [
            Buffer.from([
              69,
              58,
              1,
              42,
              34,
              64,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              116,
              114,
              121,
              71,
              114,
              111,
              117,
              112,
              115,
              47,
              42,
              125,
              47,
              101,
              110,
              116,
              114,
              105,
              101,
              115,
              58,
              105,
              109,
              112,
              111,
              114,
              116,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface DataCatalogServiceImplementation<CallContextExt = {}> {
  /**
   * Searches Data Catalog for multiple resources like entries and tags that
   * match a query.
   *
   * This is a [Custom Method]
   * (https://cloud.google.com/apis/design/custom_methods) that doesn't return
   * all information on a resource, only its ID and high level fields. To get
   * more information, you can subsequently call specific get methods.
   *
   * Note: Data Catalog search queries don't guarantee full recall. Results
   * that match your query might not be returned, even in subsequent
   * result pages. Additionally, returned (and not returned) results can vary
   * if you repeat search queries.
   *
   * For more information, see [Data Catalog search syntax]
   * (https://cloud.google.com/data-catalog/docs/how-to/search-reference).
   */
  searchCatalog(
    request: SearchCatalogRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<SearchCatalogResponse>>;
  /**
   * Creates an entry group.
   *
   * An entry group contains logically related entries together with [Cloud
   * Identity and Access Management](/data-catalog/docs/concepts/iam) policies.
   * These policies specify users who can create, edit, and view entries
   * within entry groups.
   *
   * Data Catalog automatically creates entry groups with names that start with
   * the `@` symbol for the following resources:
   *
   * * BigQuery entries (`@bigquery`)
   * * Pub/Sub topics (`@pubsub`)
   * * Dataproc Metastore services (`@dataproc_metastore_{SERVICE_NAME_HASH}`)
   *
   * You can create your own entry groups for Cloud Storage fileset entries
   * and custom entries together with the corresponding IAM policies.
   * User-created entry groups can't contain the `@` symbol, it is reserved
   * for automatically created groups.
   *
   * Entry groups, like entries, can be searched.
   *
   * A maximum of 10,000 entry groups may be created per organization across all
   * locations.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `parent` parameter. For more information, see [Data Catalog resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  createEntryGroup(
    request: CreateEntryGroupRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<EntryGroup>>;
  /** Gets an entry group. */
  getEntryGroup(request: GetEntryGroupRequest, context: CallContext & CallContextExt): Promise<DeepPartial<EntryGroup>>;
  /**
   * Updates an entry group.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `entry_group.name` parameter. For more information, see [Data Catalog
   * resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  updateEntryGroup(
    request: UpdateEntryGroupRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<EntryGroup>>;
  /**
   * Deletes an entry group.
   *
   * You must enable the Data Catalog API in the project
   * identified by the `name` parameter. For more information, see [Data Catalog
   * resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  deleteEntryGroup(
    request: DeleteEntryGroupRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Empty>>;
  /** Lists entry groups. */
  listEntryGroups(
    request: ListEntryGroupsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ListEntryGroupsResponse>>;
  /**
   * Creates an entry.
   *
   * You can create entries only with 'FILESET', 'CLUSTER', 'DATA_STREAM',
   * or custom types. Data Catalog automatically creates entries with other
   * types during metadata ingestion from integrated systems.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `parent` parameter. For more information, see [Data Catalog resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   *
   * An entry group can have a maximum of 100,000 entries.
   */
  createEntry(request: CreateEntryRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Entry>>;
  /**
   * Updates an existing entry.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `entry.name` parameter. For more information, see [Data Catalog
   * resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  updateEntry(request: UpdateEntryRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Entry>>;
  /**
   * Deletes an existing entry.
   *
   * You can delete only the entries created by the
   * [CreateEntry][google.cloud.datacatalog.v1.DataCatalog.CreateEntry]
   * method.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `name` parameter. For more information, see [Data Catalog
   * resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  deleteEntry(request: DeleteEntryRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Empty>>;
  /** Gets an entry. */
  getEntry(request: GetEntryRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Entry>>;
  /**
   * Gets an entry by its target resource name.
   *
   * The resource name comes from the source Google Cloud Platform service.
   */
  lookupEntry(request: LookupEntryRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Entry>>;
  /**
   * Lists entries.
   *
   * Note: Currently, this method can list only custom entries.
   * To get a list of both custom and automatically created entries, use
   * [SearchCatalog][google.cloud.datacatalog.v1.DataCatalog.SearchCatalog].
   */
  listEntries(
    request: ListEntriesRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ListEntriesResponse>>;
  /**
   * Modifies entry overview, part of the business context of an
   * [Entry][google.cloud.datacatalog.v1.Entry].
   *
   * To call this method, you must have the `datacatalog.entries.updateOverview`
   * IAM permission on the corresponding project.
   */
  modifyEntryOverview(
    request: ModifyEntryOverviewRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<EntryOverview>>;
  /**
   * Modifies contacts, part of the business context of an
   * [Entry][google.cloud.datacatalog.v1.Entry].
   *
   * To call this method, you must have the `datacatalog.entries.updateContacts`
   * IAM permission on the corresponding project.
   */
  modifyEntryContacts(
    request: ModifyEntryContactsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Contacts>>;
  /**
   * Creates a tag template.
   *
   * You must enable the Data Catalog API in the project identified by the
   * `parent` parameter.
   * For more information, see [Data Catalog resource project]
   * (https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  createTagTemplate(
    request: CreateTagTemplateRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<TagTemplate>>;
  /** Gets a tag template. */
  getTagTemplate(
    request: GetTagTemplateRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<TagTemplate>>;
  /**
   * Updates a tag template.
   *
   * You can't update template fields with this method. These fields are
   * separate resources with their own create, update, and delete methods.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `tag_template.name` parameter. For more information, see [Data Catalog
   * resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  updateTagTemplate(
    request: UpdateTagTemplateRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<TagTemplate>>;
  /**
   * Deletes a tag template and all tags that use it.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `name` parameter. For more information, see [Data Catalog resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  deleteTagTemplate(
    request: DeleteTagTemplateRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Empty>>;
  /**
   * Creates a field in a tag template.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `parent` parameter. For more information, see [Data Catalog resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  createTagTemplateField(
    request: CreateTagTemplateFieldRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<TagTemplateField>>;
  /**
   * Updates a field in a tag template.
   *
   * You can't update the field type with this method.
   *
   * You must enable the Data Catalog API in the project
   * identified by the `name` parameter. For more information, see [Data Catalog
   * resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  updateTagTemplateField(
    request: UpdateTagTemplateFieldRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<TagTemplateField>>;
  /**
   * Renames a field in a tag template.
   *
   * You must enable the Data Catalog API in the project identified by the
   * `name` parameter. For more information, see [Data Catalog resource project]
   * (https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  renameTagTemplateField(
    request: RenameTagTemplateFieldRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<TagTemplateField>>;
  /**
   * Renames an enum value in a tag template.
   *
   * Within a single enum field, enum values must be unique.
   */
  renameTagTemplateFieldEnumValue(
    request: RenameTagTemplateFieldEnumValueRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<TagTemplateField>>;
  /**
   * Deletes a field in a tag template and all uses of this field from the tags
   * based on this template.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `name` parameter. For more information, see [Data Catalog resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  deleteTagTemplateField(
    request: DeleteTagTemplateFieldRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Empty>>;
  /**
   * Creates a tag and assigns it to:
   *
   * * An [Entry][google.cloud.datacatalog.v1.Entry] if the method name is
   *   `projects.locations.entryGroups.entries.tags.create`.
   * * Or [EntryGroup][google.cloud.datacatalog.v1.EntryGroup]if the method
   *   name is `projects.locations.entryGroups.tags.create`.
   *
   * Note: The project identified by the `parent` parameter for the [tag]
   * (https://cloud.google.com/data-catalog/docs/reference/rest/v1/projects.locations.entryGroups.entries.tags/create#path-parameters)
   * and the [tag template]
   * (https://cloud.google.com/data-catalog/docs/reference/rest/v1/projects.locations.tagTemplates/create#path-parameters)
   * used to create the tag must be in the same organization.
   */
  createTag(request: CreateTagRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Tag>>;
  /** Updates an existing tag. */
  updateTag(request: UpdateTagRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Tag>>;
  /** Deletes a tag. */
  deleteTag(request: DeleteTagRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Empty>>;
  /**
   * Lists tags assigned to an [Entry][google.cloud.datacatalog.v1.Entry].
   * The [columns][google.cloud.datacatalog.v1.Tag.column] in the response are
   * lowercased.
   */
  listTags(request: ListTagsRequest, context: CallContext & CallContextExt): Promise<DeepPartial<ListTagsResponse>>;
  /**
   * `ReconcileTags` creates or updates a list of tags on the entry.
   * If the
   * [ReconcileTagsRequest.force_delete_missing][google.cloud.datacatalog.v1.ReconcileTagsRequest.force_delete_missing]
   * parameter is set, the operation deletes tags not included in the input tag
   * list.
   *
   * `ReconcileTags` returns a [long-running operation]
   * [google.longrunning.Operation] resource that can be queried with
   * [Operations.GetOperation][google.longrunning.Operations.GetOperation]
   * to return [ReconcileTagsMetadata]
   * [google.cloud.datacatalog.v1.ReconcileTagsMetadata] and
   * a [ReconcileTagsResponse]
   * [google.cloud.datacatalog.v1.ReconcileTagsResponse] message.
   */
  reconcileTags(request: ReconcileTagsRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
  /**
   * Marks an [Entry][google.cloud.datacatalog.v1.Entry] as starred by
   * the current user. Starring information is private to each user.
   */
  starEntry(request: StarEntryRequest, context: CallContext & CallContextExt): Promise<DeepPartial<StarEntryResponse>>;
  /**
   * Marks an [Entry][google.cloud.datacatalog.v1.Entry] as NOT starred by
   * the current user. Starring information is private to each user.
   */
  unstarEntry(
    request: UnstarEntryRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<UnstarEntryResponse>>;
  /**
   * Sets an access control policy for a resource. Replaces any existing
   * policy.
   *
   * Supported resources are:
   *
   * - Tag templates
   * - Entry groups
   *
   * Note: This method sets policies only within Data Catalog and can't be
   * used to manage policies in BigQuery, Pub/Sub, Dataproc Metastore, and any
   * external Google Cloud Platform resources synced with the Data Catalog.
   *
   * To call this method, you must have the following Google IAM permissions:
   *
   * - `datacatalog.tagTemplates.setIamPolicy` to set policies on tag
   *   templates.
   * - `datacatalog.entryGroups.setIamPolicy` to set policies on entry groups.
   */
  setIamPolicy(request: SetIamPolicyRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Policy>>;
  /**
   * Gets the access control policy for a resource.
   *
   * May return:
   *
   * * A`NOT_FOUND` error if the resource doesn't exist or you don't have the
   *   permission to view it.
   * * An empty policy if the resource exists but doesn't have a set policy.
   *
   * Supported resources are:
   *
   * - Tag templates
   * - Entry groups
   *
   * Note: This method doesn't get policies from Google Cloud Platform
   * resources ingested into Data Catalog.
   *
   * To call this method, you must have the following Google IAM permissions:
   *
   * - `datacatalog.tagTemplates.getIamPolicy` to get policies on tag
   *   templates.
   * - `datacatalog.entryGroups.getIamPolicy` to get policies on entry groups.
   */
  getIamPolicy(request: GetIamPolicyRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Policy>>;
  /**
   * Gets your permissions on a resource.
   *
   * Returns an empty set of permissions if the resource doesn't exist.
   *
   * Supported resources are:
   *
   * - Tag templates
   * - Entry groups
   *
   * Note: This method gets policies only within Data Catalog and can't be
   * used to get policies from BigQuery, Pub/Sub, Dataproc Metastore, and any
   * external Google Cloud Platform resources ingested into Data Catalog.
   *
   * No Google IAM permissions are required to call this method.
   */
  testIamPermissions(
    request: TestIamPermissionsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<TestIamPermissionsResponse>>;
  /**
   * Imports entries from a source, such as data previously dumped into a
   * Cloud Storage bucket, into Data Catalog. Import of entries
   * is a sync operation that reconciles the state of the third-party system
   * with the Data Catalog.
   *
   * `ImportEntries` accepts source data snapshots of a third-party system.
   * Snapshot should be delivered as a .wire or base65-encoded .txt file
   * containing a sequence of Protocol Buffer messages of
   * [DumpItem][google.cloud.datacatalog.v1.DumpItem] type.
   *
   * `ImportEntries` returns a [long-running operation]
   * [google.longrunning.Operation] resource that can be queried with
   * [Operations.GetOperation][google.longrunning.Operations.GetOperation]
   * to return
   * [ImportEntriesMetadata][google.cloud.datacatalog.v1.ImportEntriesMetadata]
   * and an
   * [ImportEntriesResponse][google.cloud.datacatalog.v1.ImportEntriesResponse]
   * message.
   */
  importEntries(request: ImportEntriesRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Operation>>;
}

export interface DataCatalogClient<CallOptionsExt = {}> {
  /**
   * Searches Data Catalog for multiple resources like entries and tags that
   * match a query.
   *
   * This is a [Custom Method]
   * (https://cloud.google.com/apis/design/custom_methods) that doesn't return
   * all information on a resource, only its ID and high level fields. To get
   * more information, you can subsequently call specific get methods.
   *
   * Note: Data Catalog search queries don't guarantee full recall. Results
   * that match your query might not be returned, even in subsequent
   * result pages. Additionally, returned (and not returned) results can vary
   * if you repeat search queries.
   *
   * For more information, see [Data Catalog search syntax]
   * (https://cloud.google.com/data-catalog/docs/how-to/search-reference).
   */
  searchCatalog(
    request: DeepPartial<SearchCatalogRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<SearchCatalogResponse>;
  /**
   * Creates an entry group.
   *
   * An entry group contains logically related entries together with [Cloud
   * Identity and Access Management](/data-catalog/docs/concepts/iam) policies.
   * These policies specify users who can create, edit, and view entries
   * within entry groups.
   *
   * Data Catalog automatically creates entry groups with names that start with
   * the `@` symbol for the following resources:
   *
   * * BigQuery entries (`@bigquery`)
   * * Pub/Sub topics (`@pubsub`)
   * * Dataproc Metastore services (`@dataproc_metastore_{SERVICE_NAME_HASH}`)
   *
   * You can create your own entry groups for Cloud Storage fileset entries
   * and custom entries together with the corresponding IAM policies.
   * User-created entry groups can't contain the `@` symbol, it is reserved
   * for automatically created groups.
   *
   * Entry groups, like entries, can be searched.
   *
   * A maximum of 10,000 entry groups may be created per organization across all
   * locations.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `parent` parameter. For more information, see [Data Catalog resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  createEntryGroup(
    request: DeepPartial<CreateEntryGroupRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<EntryGroup>;
  /** Gets an entry group. */
  getEntryGroup(
    request: DeepPartial<GetEntryGroupRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<EntryGroup>;
  /**
   * Updates an entry group.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `entry_group.name` parameter. For more information, see [Data Catalog
   * resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  updateEntryGroup(
    request: DeepPartial<UpdateEntryGroupRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<EntryGroup>;
  /**
   * Deletes an entry group.
   *
   * You must enable the Data Catalog API in the project
   * identified by the `name` parameter. For more information, see [Data Catalog
   * resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  deleteEntryGroup(
    request: DeepPartial<DeleteEntryGroupRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Empty>;
  /** Lists entry groups. */
  listEntryGroups(
    request: DeepPartial<ListEntryGroupsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ListEntryGroupsResponse>;
  /**
   * Creates an entry.
   *
   * You can create entries only with 'FILESET', 'CLUSTER', 'DATA_STREAM',
   * or custom types. Data Catalog automatically creates entries with other
   * types during metadata ingestion from integrated systems.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `parent` parameter. For more information, see [Data Catalog resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   *
   * An entry group can have a maximum of 100,000 entries.
   */
  createEntry(request: DeepPartial<CreateEntryRequest>, options?: CallOptions & CallOptionsExt): Promise<Entry>;
  /**
   * Updates an existing entry.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `entry.name` parameter. For more information, see [Data Catalog
   * resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  updateEntry(request: DeepPartial<UpdateEntryRequest>, options?: CallOptions & CallOptionsExt): Promise<Entry>;
  /**
   * Deletes an existing entry.
   *
   * You can delete only the entries created by the
   * [CreateEntry][google.cloud.datacatalog.v1.DataCatalog.CreateEntry]
   * method.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `name` parameter. For more information, see [Data Catalog
   * resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  deleteEntry(request: DeepPartial<DeleteEntryRequest>, options?: CallOptions & CallOptionsExt): Promise<Empty>;
  /** Gets an entry. */
  getEntry(request: DeepPartial<GetEntryRequest>, options?: CallOptions & CallOptionsExt): Promise<Entry>;
  /**
   * Gets an entry by its target resource name.
   *
   * The resource name comes from the source Google Cloud Platform service.
   */
  lookupEntry(request: DeepPartial<LookupEntryRequest>, options?: CallOptions & CallOptionsExt): Promise<Entry>;
  /**
   * Lists entries.
   *
   * Note: Currently, this method can list only custom entries.
   * To get a list of both custom and automatically created entries, use
   * [SearchCatalog][google.cloud.datacatalog.v1.DataCatalog.SearchCatalog].
   */
  listEntries(
    request: DeepPartial<ListEntriesRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ListEntriesResponse>;
  /**
   * Modifies entry overview, part of the business context of an
   * [Entry][google.cloud.datacatalog.v1.Entry].
   *
   * To call this method, you must have the `datacatalog.entries.updateOverview`
   * IAM permission on the corresponding project.
   */
  modifyEntryOverview(
    request: DeepPartial<ModifyEntryOverviewRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<EntryOverview>;
  /**
   * Modifies contacts, part of the business context of an
   * [Entry][google.cloud.datacatalog.v1.Entry].
   *
   * To call this method, you must have the `datacatalog.entries.updateContacts`
   * IAM permission on the corresponding project.
   */
  modifyEntryContacts(
    request: DeepPartial<ModifyEntryContactsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Contacts>;
  /**
   * Creates a tag template.
   *
   * You must enable the Data Catalog API in the project identified by the
   * `parent` parameter.
   * For more information, see [Data Catalog resource project]
   * (https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  createTagTemplate(
    request: DeepPartial<CreateTagTemplateRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<TagTemplate>;
  /** Gets a tag template. */
  getTagTemplate(
    request: DeepPartial<GetTagTemplateRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<TagTemplate>;
  /**
   * Updates a tag template.
   *
   * You can't update template fields with this method. These fields are
   * separate resources with their own create, update, and delete methods.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `tag_template.name` parameter. For more information, see [Data Catalog
   * resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  updateTagTemplate(
    request: DeepPartial<UpdateTagTemplateRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<TagTemplate>;
  /**
   * Deletes a tag template and all tags that use it.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `name` parameter. For more information, see [Data Catalog resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  deleteTagTemplate(
    request: DeepPartial<DeleteTagTemplateRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Empty>;
  /**
   * Creates a field in a tag template.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `parent` parameter. For more information, see [Data Catalog resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  createTagTemplateField(
    request: DeepPartial<CreateTagTemplateFieldRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<TagTemplateField>;
  /**
   * Updates a field in a tag template.
   *
   * You can't update the field type with this method.
   *
   * You must enable the Data Catalog API in the project
   * identified by the `name` parameter. For more information, see [Data Catalog
   * resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  updateTagTemplateField(
    request: DeepPartial<UpdateTagTemplateFieldRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<TagTemplateField>;
  /**
   * Renames a field in a tag template.
   *
   * You must enable the Data Catalog API in the project identified by the
   * `name` parameter. For more information, see [Data Catalog resource project]
   * (https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  renameTagTemplateField(
    request: DeepPartial<RenameTagTemplateFieldRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<TagTemplateField>;
  /**
   * Renames an enum value in a tag template.
   *
   * Within a single enum field, enum values must be unique.
   */
  renameTagTemplateFieldEnumValue(
    request: DeepPartial<RenameTagTemplateFieldEnumValueRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<TagTemplateField>;
  /**
   * Deletes a field in a tag template and all uses of this field from the tags
   * based on this template.
   *
   * You must enable the Data Catalog API in the project identified by
   * the `name` parameter. For more information, see [Data Catalog resource
   * project](https://cloud.google.com/data-catalog/docs/concepts/resource-project).
   */
  deleteTagTemplateField(
    request: DeepPartial<DeleteTagTemplateFieldRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Empty>;
  /**
   * Creates a tag and assigns it to:
   *
   * * An [Entry][google.cloud.datacatalog.v1.Entry] if the method name is
   *   `projects.locations.entryGroups.entries.tags.create`.
   * * Or [EntryGroup][google.cloud.datacatalog.v1.EntryGroup]if the method
   *   name is `projects.locations.entryGroups.tags.create`.
   *
   * Note: The project identified by the `parent` parameter for the [tag]
   * (https://cloud.google.com/data-catalog/docs/reference/rest/v1/projects.locations.entryGroups.entries.tags/create#path-parameters)
   * and the [tag template]
   * (https://cloud.google.com/data-catalog/docs/reference/rest/v1/projects.locations.tagTemplates/create#path-parameters)
   * used to create the tag must be in the same organization.
   */
  createTag(request: DeepPartial<CreateTagRequest>, options?: CallOptions & CallOptionsExt): Promise<Tag>;
  /** Updates an existing tag. */
  updateTag(request: DeepPartial<UpdateTagRequest>, options?: CallOptions & CallOptionsExt): Promise<Tag>;
  /** Deletes a tag. */
  deleteTag(request: DeepPartial<DeleteTagRequest>, options?: CallOptions & CallOptionsExt): Promise<Empty>;
  /**
   * Lists tags assigned to an [Entry][google.cloud.datacatalog.v1.Entry].
   * The [columns][google.cloud.datacatalog.v1.Tag.column] in the response are
   * lowercased.
   */
  listTags(request: DeepPartial<ListTagsRequest>, options?: CallOptions & CallOptionsExt): Promise<ListTagsResponse>;
  /**
   * `ReconcileTags` creates or updates a list of tags on the entry.
   * If the
   * [ReconcileTagsRequest.force_delete_missing][google.cloud.datacatalog.v1.ReconcileTagsRequest.force_delete_missing]
   * parameter is set, the operation deletes tags not included in the input tag
   * list.
   *
   * `ReconcileTags` returns a [long-running operation]
   * [google.longrunning.Operation] resource that can be queried with
   * [Operations.GetOperation][google.longrunning.Operations.GetOperation]
   * to return [ReconcileTagsMetadata]
   * [google.cloud.datacatalog.v1.ReconcileTagsMetadata] and
   * a [ReconcileTagsResponse]
   * [google.cloud.datacatalog.v1.ReconcileTagsResponse] message.
   */
  reconcileTags(request: DeepPartial<ReconcileTagsRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
  /**
   * Marks an [Entry][google.cloud.datacatalog.v1.Entry] as starred by
   * the current user. Starring information is private to each user.
   */
  starEntry(request: DeepPartial<StarEntryRequest>, options?: CallOptions & CallOptionsExt): Promise<StarEntryResponse>;
  /**
   * Marks an [Entry][google.cloud.datacatalog.v1.Entry] as NOT starred by
   * the current user. Starring information is private to each user.
   */
  unstarEntry(
    request: DeepPartial<UnstarEntryRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<UnstarEntryResponse>;
  /**
   * Sets an access control policy for a resource. Replaces any existing
   * policy.
   *
   * Supported resources are:
   *
   * - Tag templates
   * - Entry groups
   *
   * Note: This method sets policies only within Data Catalog and can't be
   * used to manage policies in BigQuery, Pub/Sub, Dataproc Metastore, and any
   * external Google Cloud Platform resources synced with the Data Catalog.
   *
   * To call this method, you must have the following Google IAM permissions:
   *
   * - `datacatalog.tagTemplates.setIamPolicy` to set policies on tag
   *   templates.
   * - `datacatalog.entryGroups.setIamPolicy` to set policies on entry groups.
   */
  setIamPolicy(request: DeepPartial<SetIamPolicyRequest>, options?: CallOptions & CallOptionsExt): Promise<Policy>;
  /**
   * Gets the access control policy for a resource.
   *
   * May return:
   *
   * * A`NOT_FOUND` error if the resource doesn't exist or you don't have the
   *   permission to view it.
   * * An empty policy if the resource exists but doesn't have a set policy.
   *
   * Supported resources are:
   *
   * - Tag templates
   * - Entry groups
   *
   * Note: This method doesn't get policies from Google Cloud Platform
   * resources ingested into Data Catalog.
   *
   * To call this method, you must have the following Google IAM permissions:
   *
   * - `datacatalog.tagTemplates.getIamPolicy` to get policies on tag
   *   templates.
   * - `datacatalog.entryGroups.getIamPolicy` to get policies on entry groups.
   */
  getIamPolicy(request: DeepPartial<GetIamPolicyRequest>, options?: CallOptions & CallOptionsExt): Promise<Policy>;
  /**
   * Gets your permissions on a resource.
   *
   * Returns an empty set of permissions if the resource doesn't exist.
   *
   * Supported resources are:
   *
   * - Tag templates
   * - Entry groups
   *
   * Note: This method gets policies only within Data Catalog and can't be
   * used to get policies from BigQuery, Pub/Sub, Dataproc Metastore, and any
   * external Google Cloud Platform resources ingested into Data Catalog.
   *
   * No Google IAM permissions are required to call this method.
   */
  testIamPermissions(
    request: DeepPartial<TestIamPermissionsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<TestIamPermissionsResponse>;
  /**
   * Imports entries from a source, such as data previously dumped into a
   * Cloud Storage bucket, into Data Catalog. Import of entries
   * is a sync operation that reconciles the state of the third-party system
   * with the Data Catalog.
   *
   * `ImportEntries` accepts source data snapshots of a third-party system.
   * Snapshot should be delivered as a .wire or base65-encoded .txt file
   * containing a sequence of Protocol Buffer messages of
   * [DumpItem][google.cloud.datacatalog.v1.DumpItem] type.
   *
   * `ImportEntries` returns a [long-running operation]
   * [google.longrunning.Operation] resource that can be queried with
   * [Operations.GetOperation][google.longrunning.Operations.GetOperation]
   * to return
   * [ImportEntriesMetadata][google.cloud.datacatalog.v1.ImportEntriesMetadata]
   * and an
   * [ImportEntriesResponse][google.cloud.datacatalog.v1.ImportEntriesResponse]
   * message.
   */
  importEntries(request: DeepPartial<ImportEntriesRequest>, options?: CallOptions & CallOptionsExt): Promise<Operation>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
