// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/datacatalog/v1/policytagmanagerserialization.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import {
  Taxonomy,
  Taxonomy_PolicyType,
  taxonomy_PolicyTypeFromJSON,
  taxonomy_PolicyTypeToJSON,
} from "./policytagmanager.js";

export const protobufPackage = "google.cloud.datacatalog.v1";

/**
 * A nested protocol buffer that represents a taxonomy and the hierarchy of its
 * policy tags. Used for taxonomy replacement, import, and
 * export.
 */
export interface SerializedTaxonomy {
  /**
   * Required. Display name of the taxonomy. At most 200 bytes when encoded in
   * UTF-8.
   */
  displayName: string;
  /**
   * Description of the serialized taxonomy. At most 2000 bytes when
   * encoded in UTF-8. If not set, defaults to an empty description.
   */
  description: string;
  /** Top level policy tags associated with the taxonomy, if any. */
  policyTags: SerializedPolicyTag[];
  /** A list of policy types that are activated per taxonomy. */
  activatedPolicyTypes: Taxonomy_PolicyType[];
}

/**
 * A nested protocol buffer that represents a policy tag and all its
 * descendants.
 */
export interface SerializedPolicyTag {
  /**
   * Resource name of the policy tag.
   *
   * This field is ignored when calling `ImportTaxonomies`.
   */
  policyTag: string;
  /**
   * Required. Display name of the policy tag. At most 200 bytes when encoded
   * in UTF-8.
   */
  displayName: string;
  /**
   * Description of the serialized policy tag. At most
   * 2000 bytes when encoded in UTF-8. If not set, defaults to an
   * empty description.
   */
  description: string;
  /** Children of the policy tag, if any. */
  childPolicyTags: SerializedPolicyTag[];
}

/**
 * Request message for
 * [ReplaceTaxonomy][google.cloud.datacatalog.v1.PolicyTagManagerSerialization.ReplaceTaxonomy].
 */
export interface ReplaceTaxonomyRequest {
  /** Required. Resource name of the taxonomy to update. */
  name: string;
  /** Required. Taxonomy to update along with its child policy tags. */
  serializedTaxonomy: SerializedTaxonomy | undefined;
}

/**
 * Request message for
 * [ImportTaxonomies][google.cloud.datacatalog.v1.PolicyTagManagerSerialization.ImportTaxonomies].
 */
export interface ImportTaxonomiesRequest {
  /**
   * Required. Resource name of project that the imported taxonomies will belong
   * to.
   */
  parent: string;
  /** Inline source taxonomy to import. */
  inlineSource?:
    | InlineSource
    | undefined;
  /** Cross-regional source taxonomy to import. */
  crossRegionalSource?: CrossRegionalSource | undefined;
}

/** Inline source containing taxonomies to import. */
export interface InlineSource {
  /** Required. Taxonomies to import. */
  taxonomies: SerializedTaxonomy[];
}

/**
 * Cross-regional source used to import an existing taxonomy into a different
 * region.
 */
export interface CrossRegionalSource {
  /** Required. The resource name of the source taxonomy to import. */
  taxonomy: string;
}

/**
 * Response message for
 * [ImportTaxonomies][google.cloud.datacatalog.v1.PolicyTagManagerSerialization.ImportTaxonomies].
 */
export interface ImportTaxonomiesResponse {
  /** Imported taxonomies. */
  taxonomies: Taxonomy[];
}

/**
 * Request message for
 * [ExportTaxonomies][google.cloud.datacatalog.v1.PolicyTagManagerSerialization.ExportTaxonomies].
 */
export interface ExportTaxonomiesRequest {
  /**
   * Required. Resource name of the project that the exported taxonomies belong
   * to.
   */
  parent: string;
  /** Required. Resource names of the taxonomies to export. */
  taxonomies: string[];
  /**
   * Serialized export taxonomies that contain all the policy
   * tags as nested protocol buffers.
   */
  serializedTaxonomies?: boolean | undefined;
}

/**
 * Response message for
 * [ExportTaxonomies][google.cloud.datacatalog.v1.PolicyTagManagerSerialization.ExportTaxonomies].
 */
export interface ExportTaxonomiesResponse {
  /** List of taxonomies and policy tags as nested protocol buffers. */
  taxonomies: SerializedTaxonomy[];
}

function createBaseSerializedTaxonomy(): SerializedTaxonomy {
  return { displayName: "", description: "", policyTags: [], activatedPolicyTypes: [] };
}

export const SerializedTaxonomy: MessageFns<SerializedTaxonomy> = {
  encode(message: SerializedTaxonomy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.displayName !== "") {
      writer.uint32(10).string(message.displayName);
    }
    if (message.description !== "") {
      writer.uint32(18).string(message.description);
    }
    for (const v of message.policyTags) {
      SerializedPolicyTag.encode(v!, writer.uint32(26).fork()).join();
    }
    writer.uint32(34).fork();
    for (const v of message.activatedPolicyTypes) {
      writer.int32(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SerializedTaxonomy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSerializedTaxonomy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.description = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.policyTags.push(SerializedPolicyTag.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag === 32) {
            message.activatedPolicyTypes.push(reader.int32() as any);

            continue;
          }

          if (tag === 34) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.activatedPolicyTypes.push(reader.int32() as any);
            }

            continue;
          }

          break;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SerializedTaxonomy {
    return {
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      policyTags: globalThis.Array.isArray(object?.policyTags)
        ? object.policyTags.map((e: any) => SerializedPolicyTag.fromJSON(e))
        : [],
      activatedPolicyTypes: globalThis.Array.isArray(object?.activatedPolicyTypes)
        ? object.activatedPolicyTypes.map((e: any) => taxonomy_PolicyTypeFromJSON(e))
        : [],
    };
  },

  toJSON(message: SerializedTaxonomy): unknown {
    const obj: any = {};
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.policyTags?.length) {
      obj.policyTags = message.policyTags.map((e) => SerializedPolicyTag.toJSON(e));
    }
    if (message.activatedPolicyTypes?.length) {
      obj.activatedPolicyTypes = message.activatedPolicyTypes.map((e) => taxonomy_PolicyTypeToJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<SerializedTaxonomy>): SerializedTaxonomy {
    return SerializedTaxonomy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SerializedTaxonomy>): SerializedTaxonomy {
    const message = createBaseSerializedTaxonomy();
    message.displayName = object.displayName ?? "";
    message.description = object.description ?? "";
    message.policyTags = object.policyTags?.map((e) => SerializedPolicyTag.fromPartial(e)) || [];
    message.activatedPolicyTypes = object.activatedPolicyTypes?.map((e) => e) || [];
    return message;
  },
};

function createBaseSerializedPolicyTag(): SerializedPolicyTag {
  return { policyTag: "", displayName: "", description: "", childPolicyTags: [] };
}

export const SerializedPolicyTag: MessageFns<SerializedPolicyTag> = {
  encode(message: SerializedPolicyTag, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.policyTag !== "") {
      writer.uint32(10).string(message.policyTag);
    }
    if (message.displayName !== "") {
      writer.uint32(18).string(message.displayName);
    }
    if (message.description !== "") {
      writer.uint32(26).string(message.description);
    }
    for (const v of message.childPolicyTags) {
      SerializedPolicyTag.encode(v!, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SerializedPolicyTag {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSerializedPolicyTag();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.policyTag = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.description = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.childPolicyTags.push(SerializedPolicyTag.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SerializedPolicyTag {
    return {
      policyTag: isSet(object.policyTag) ? globalThis.String(object.policyTag) : "",
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      childPolicyTags: globalThis.Array.isArray(object?.childPolicyTags)
        ? object.childPolicyTags.map((e: any) => SerializedPolicyTag.fromJSON(e))
        : [],
    };
  },

  toJSON(message: SerializedPolicyTag): unknown {
    const obj: any = {};
    if (message.policyTag !== "") {
      obj.policyTag = message.policyTag;
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.childPolicyTags?.length) {
      obj.childPolicyTags = message.childPolicyTags.map((e) => SerializedPolicyTag.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<SerializedPolicyTag>): SerializedPolicyTag {
    return SerializedPolicyTag.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SerializedPolicyTag>): SerializedPolicyTag {
    const message = createBaseSerializedPolicyTag();
    message.policyTag = object.policyTag ?? "";
    message.displayName = object.displayName ?? "";
    message.description = object.description ?? "";
    message.childPolicyTags = object.childPolicyTags?.map((e) => SerializedPolicyTag.fromPartial(e)) || [];
    return message;
  },
};

function createBaseReplaceTaxonomyRequest(): ReplaceTaxonomyRequest {
  return { name: "", serializedTaxonomy: undefined };
}

export const ReplaceTaxonomyRequest: MessageFns<ReplaceTaxonomyRequest> = {
  encode(message: ReplaceTaxonomyRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.serializedTaxonomy !== undefined) {
      SerializedTaxonomy.encode(message.serializedTaxonomy, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReplaceTaxonomyRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReplaceTaxonomyRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.serializedTaxonomy = SerializedTaxonomy.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReplaceTaxonomyRequest {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      serializedTaxonomy: isSet(object.serializedTaxonomy)
        ? SerializedTaxonomy.fromJSON(object.serializedTaxonomy)
        : undefined,
    };
  },

  toJSON(message: ReplaceTaxonomyRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.serializedTaxonomy !== undefined) {
      obj.serializedTaxonomy = SerializedTaxonomy.toJSON(message.serializedTaxonomy);
    }
    return obj;
  },

  create(base?: DeepPartial<ReplaceTaxonomyRequest>): ReplaceTaxonomyRequest {
    return ReplaceTaxonomyRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReplaceTaxonomyRequest>): ReplaceTaxonomyRequest {
    const message = createBaseReplaceTaxonomyRequest();
    message.name = object.name ?? "";
    message.serializedTaxonomy = (object.serializedTaxonomy !== undefined && object.serializedTaxonomy !== null)
      ? SerializedTaxonomy.fromPartial(object.serializedTaxonomy)
      : undefined;
    return message;
  },
};

function createBaseImportTaxonomiesRequest(): ImportTaxonomiesRequest {
  return { parent: "", inlineSource: undefined, crossRegionalSource: undefined };
}

export const ImportTaxonomiesRequest: MessageFns<ImportTaxonomiesRequest> = {
  encode(message: ImportTaxonomiesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    if (message.inlineSource !== undefined) {
      InlineSource.encode(message.inlineSource, writer.uint32(18).fork()).join();
    }
    if (message.crossRegionalSource !== undefined) {
      CrossRegionalSource.encode(message.crossRegionalSource, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportTaxonomiesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportTaxonomiesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.inlineSource = InlineSource.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.crossRegionalSource = CrossRegionalSource.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportTaxonomiesRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      inlineSource: isSet(object.inlineSource) ? InlineSource.fromJSON(object.inlineSource) : undefined,
      crossRegionalSource: isSet(object.crossRegionalSource)
        ? CrossRegionalSource.fromJSON(object.crossRegionalSource)
        : undefined,
    };
  },

  toJSON(message: ImportTaxonomiesRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.inlineSource !== undefined) {
      obj.inlineSource = InlineSource.toJSON(message.inlineSource);
    }
    if (message.crossRegionalSource !== undefined) {
      obj.crossRegionalSource = CrossRegionalSource.toJSON(message.crossRegionalSource);
    }
    return obj;
  },

  create(base?: DeepPartial<ImportTaxonomiesRequest>): ImportTaxonomiesRequest {
    return ImportTaxonomiesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportTaxonomiesRequest>): ImportTaxonomiesRequest {
    const message = createBaseImportTaxonomiesRequest();
    message.parent = object.parent ?? "";
    message.inlineSource = (object.inlineSource !== undefined && object.inlineSource !== null)
      ? InlineSource.fromPartial(object.inlineSource)
      : undefined;
    message.crossRegionalSource = (object.crossRegionalSource !== undefined && object.crossRegionalSource !== null)
      ? CrossRegionalSource.fromPartial(object.crossRegionalSource)
      : undefined;
    return message;
  },
};

function createBaseInlineSource(): InlineSource {
  return { taxonomies: [] };
}

export const InlineSource: MessageFns<InlineSource> = {
  encode(message: InlineSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.taxonomies) {
      SerializedTaxonomy.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InlineSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInlineSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.taxonomies.push(SerializedTaxonomy.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InlineSource {
    return {
      taxonomies: globalThis.Array.isArray(object?.taxonomies)
        ? object.taxonomies.map((e: any) => SerializedTaxonomy.fromJSON(e))
        : [],
    };
  },

  toJSON(message: InlineSource): unknown {
    const obj: any = {};
    if (message.taxonomies?.length) {
      obj.taxonomies = message.taxonomies.map((e) => SerializedTaxonomy.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<InlineSource>): InlineSource {
    return InlineSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InlineSource>): InlineSource {
    const message = createBaseInlineSource();
    message.taxonomies = object.taxonomies?.map((e) => SerializedTaxonomy.fromPartial(e)) || [];
    return message;
  },
};

function createBaseCrossRegionalSource(): CrossRegionalSource {
  return { taxonomy: "" };
}

export const CrossRegionalSource: MessageFns<CrossRegionalSource> = {
  encode(message: CrossRegionalSource, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.taxonomy !== "") {
      writer.uint32(10).string(message.taxonomy);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CrossRegionalSource {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCrossRegionalSource();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.taxonomy = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CrossRegionalSource {
    return { taxonomy: isSet(object.taxonomy) ? globalThis.String(object.taxonomy) : "" };
  },

  toJSON(message: CrossRegionalSource): unknown {
    const obj: any = {};
    if (message.taxonomy !== "") {
      obj.taxonomy = message.taxonomy;
    }
    return obj;
  },

  create(base?: DeepPartial<CrossRegionalSource>): CrossRegionalSource {
    return CrossRegionalSource.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CrossRegionalSource>): CrossRegionalSource {
    const message = createBaseCrossRegionalSource();
    message.taxonomy = object.taxonomy ?? "";
    return message;
  },
};

function createBaseImportTaxonomiesResponse(): ImportTaxonomiesResponse {
  return { taxonomies: [] };
}

export const ImportTaxonomiesResponse: MessageFns<ImportTaxonomiesResponse> = {
  encode(message: ImportTaxonomiesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.taxonomies) {
      Taxonomy.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportTaxonomiesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportTaxonomiesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.taxonomies.push(Taxonomy.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportTaxonomiesResponse {
    return {
      taxonomies: globalThis.Array.isArray(object?.taxonomies)
        ? object.taxonomies.map((e: any) => Taxonomy.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ImportTaxonomiesResponse): unknown {
    const obj: any = {};
    if (message.taxonomies?.length) {
      obj.taxonomies = message.taxonomies.map((e) => Taxonomy.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ImportTaxonomiesResponse>): ImportTaxonomiesResponse {
    return ImportTaxonomiesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportTaxonomiesResponse>): ImportTaxonomiesResponse {
    const message = createBaseImportTaxonomiesResponse();
    message.taxonomies = object.taxonomies?.map((e) => Taxonomy.fromPartial(e)) || [];
    return message;
  },
};

function createBaseExportTaxonomiesRequest(): ExportTaxonomiesRequest {
  return { parent: "", taxonomies: [], serializedTaxonomies: undefined };
}

export const ExportTaxonomiesRequest: MessageFns<ExportTaxonomiesRequest> = {
  encode(message: ExportTaxonomiesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== "") {
      writer.uint32(10).string(message.parent);
    }
    for (const v of message.taxonomies) {
      writer.uint32(18).string(v!);
    }
    if (message.serializedTaxonomies !== undefined) {
      writer.uint32(24).bool(message.serializedTaxonomies);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportTaxonomiesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportTaxonomiesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.taxonomies.push(reader.string());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.serializedTaxonomies = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportTaxonomiesRequest {
    return {
      parent: isSet(object.parent) ? globalThis.String(object.parent) : "",
      taxonomies: globalThis.Array.isArray(object?.taxonomies)
        ? object.taxonomies.map((e: any) => globalThis.String(e))
        : [],
      serializedTaxonomies: isSet(object.serializedTaxonomies)
        ? globalThis.Boolean(object.serializedTaxonomies)
        : undefined,
    };
  },

  toJSON(message: ExportTaxonomiesRequest): unknown {
    const obj: any = {};
    if (message.parent !== "") {
      obj.parent = message.parent;
    }
    if (message.taxonomies?.length) {
      obj.taxonomies = message.taxonomies;
    }
    if (message.serializedTaxonomies !== undefined) {
      obj.serializedTaxonomies = message.serializedTaxonomies;
    }
    return obj;
  },

  create(base?: DeepPartial<ExportTaxonomiesRequest>): ExportTaxonomiesRequest {
    return ExportTaxonomiesRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportTaxonomiesRequest>): ExportTaxonomiesRequest {
    const message = createBaseExportTaxonomiesRequest();
    message.parent = object.parent ?? "";
    message.taxonomies = object.taxonomies?.map((e) => e) || [];
    message.serializedTaxonomies = object.serializedTaxonomies ?? undefined;
    return message;
  },
};

function createBaseExportTaxonomiesResponse(): ExportTaxonomiesResponse {
  return { taxonomies: [] };
}

export const ExportTaxonomiesResponse: MessageFns<ExportTaxonomiesResponse> = {
  encode(message: ExportTaxonomiesResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.taxonomies) {
      SerializedTaxonomy.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportTaxonomiesResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportTaxonomiesResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.taxonomies.push(SerializedTaxonomy.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportTaxonomiesResponse {
    return {
      taxonomies: globalThis.Array.isArray(object?.taxonomies)
        ? object.taxonomies.map((e: any) => SerializedTaxonomy.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ExportTaxonomiesResponse): unknown {
    const obj: any = {};
    if (message.taxonomies?.length) {
      obj.taxonomies = message.taxonomies.map((e) => SerializedTaxonomy.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ExportTaxonomiesResponse>): ExportTaxonomiesResponse {
    return ExportTaxonomiesResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportTaxonomiesResponse>): ExportTaxonomiesResponse {
    const message = createBaseExportTaxonomiesResponse();
    message.taxonomies = object.taxonomies?.map((e) => SerializedTaxonomy.fromPartial(e)) || [];
    return message;
  },
};

/**
 * Policy Tag Manager Serialization API service allows you to manipulate
 * your policy tags and taxonomies in a serialized format.
 *
 * Taxonomy is a hierarchical group of policy tags.
 */
export type PolicyTagManagerSerializationDefinition = typeof PolicyTagManagerSerializationDefinition;
export const PolicyTagManagerSerializationDefinition = {
  name: "PolicyTagManagerSerialization",
  fullName: "google.cloud.datacatalog.v1.PolicyTagManagerSerialization",
  methods: {
    /**
     * Replaces (updates) a taxonomy and all its policy tags.
     *
     * The taxonomy and its entire hierarchy of policy tags must be
     * represented literally by `SerializedTaxonomy` and the nested
     * `SerializedPolicyTag` messages.
     *
     * This operation automatically does the following:
     *
     * - Deletes the existing policy tags that are missing from the
     *   `SerializedPolicyTag`.
     * - Creates policy tags that don't have resource names. They are considered
     *   new.
     * - Updates policy tags with valid resources names accordingly.
     */
    replaceTaxonomy: {
      name: "ReplaceTaxonomy",
      requestType: ReplaceTaxonomyRequest,
      requestStream: false,
      responseType: Taxonomy,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              59,
              58,
              1,
              42,
              34,
              54,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              116,
              97,
              120,
              111,
              110,
              111,
              109,
              105,
              101,
              115,
              47,
              42,
              125,
              58,
              114,
              101,
              112,
              108,
              97,
              99,
              101,
            ]),
          ],
        },
      },
    },
    /**
     * Creates new taxonomies (including their policy tags) in a given project
     * by importing from inlined or cross-regional sources.
     *
     * For a cross-regional source, new taxonomies are created by copying
     * from a source in another region.
     *
     * For an inlined source, taxonomies and policy tags are created in bulk using
     * nested protocol buffer structures.
     */
    importTaxonomies: {
      name: "ImportTaxonomies",
      requestType: ImportTaxonomiesRequest,
      requestStream: false,
      responseType: ImportTaxonomiesResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              58,
              58,
              1,
              42,
              34,
              53,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              47,
              116,
              97,
              120,
              111,
              110,
              111,
              109,
              105,
              101,
              115,
              58,
              105,
              109,
              112,
              111,
              114,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * Exports taxonomies in the requested type and returns them,
     * including their policy tags. The requested taxonomies must belong to the
     * same project.
     *
     * This method generates `SerializedTaxonomy` protocol buffers with nested
     * policy tags that can be used as input for `ImportTaxonomies` calls.
     */
    exportTaxonomies: {
      name: "ExportTaxonomies",
      requestType: ExportTaxonomiesRequest,
      requestStream: false,
      responseType: ExportTaxonomiesResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              55,
              18,
              53,
              47,
              118,
              49,
              47,
              123,
              112,
              97,
              114,
              101,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              47,
              116,
              97,
              120,
              111,
              110,
              111,
              109,
              105,
              101,
              115,
              58,
              101,
              120,
              112,
              111,
              114,
              116,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface PolicyTagManagerSerializationServiceImplementation<CallContextExt = {}> {
  /**
   * Replaces (updates) a taxonomy and all its policy tags.
   *
   * The taxonomy and its entire hierarchy of policy tags must be
   * represented literally by `SerializedTaxonomy` and the nested
   * `SerializedPolicyTag` messages.
   *
   * This operation automatically does the following:
   *
   * - Deletes the existing policy tags that are missing from the
   *   `SerializedPolicyTag`.
   * - Creates policy tags that don't have resource names. They are considered
   *   new.
   * - Updates policy tags with valid resources names accordingly.
   */
  replaceTaxonomy(
    request: ReplaceTaxonomyRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Taxonomy>>;
  /**
   * Creates new taxonomies (including their policy tags) in a given project
   * by importing from inlined or cross-regional sources.
   *
   * For a cross-regional source, new taxonomies are created by copying
   * from a source in another region.
   *
   * For an inlined source, taxonomies and policy tags are created in bulk using
   * nested protocol buffer structures.
   */
  importTaxonomies(
    request: ImportTaxonomiesRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ImportTaxonomiesResponse>>;
  /**
   * Exports taxonomies in the requested type and returns them,
   * including their policy tags. The requested taxonomies must belong to the
   * same project.
   *
   * This method generates `SerializedTaxonomy` protocol buffers with nested
   * policy tags that can be used as input for `ImportTaxonomies` calls.
   */
  exportTaxonomies(
    request: ExportTaxonomiesRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ExportTaxonomiesResponse>>;
}

export interface PolicyTagManagerSerializationClient<CallOptionsExt = {}> {
  /**
   * Replaces (updates) a taxonomy and all its policy tags.
   *
   * The taxonomy and its entire hierarchy of policy tags must be
   * represented literally by `SerializedTaxonomy` and the nested
   * `SerializedPolicyTag` messages.
   *
   * This operation automatically does the following:
   *
   * - Deletes the existing policy tags that are missing from the
   *   `SerializedPolicyTag`.
   * - Creates policy tags that don't have resource names. They are considered
   *   new.
   * - Updates policy tags with valid resources names accordingly.
   */
  replaceTaxonomy(
    request: DeepPartial<ReplaceTaxonomyRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Taxonomy>;
  /**
   * Creates new taxonomies (including their policy tags) in a given project
   * by importing from inlined or cross-regional sources.
   *
   * For a cross-regional source, new taxonomies are created by copying
   * from a source in another region.
   *
   * For an inlined source, taxonomies and policy tags are created in bulk using
   * nested protocol buffer structures.
   */
  importTaxonomies(
    request: DeepPartial<ImportTaxonomiesRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ImportTaxonomiesResponse>;
  /**
   * Exports taxonomies in the requested type and returns them,
   * including their policy tags. The requested taxonomies must belong to the
   * same project.
   *
   * This method generates `SerializedTaxonomy` protocol buffers with nested
   * policy tags that can be used as input for `ImportTaxonomies` calls.
   */
  exportTaxonomies(
    request: DeepPartial<ExportTaxonomiesRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ExportTaxonomiesResponse>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
