// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/datacatalog/v1/bigquery.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";

export const protobufPackage = "google.cloud.datacatalog.v1";

/** Specification for the BigQuery connection. */
export interface BigQueryConnectionSpec {
  /** The type of the BigQuery connection. */
  connectionType: BigQueryConnectionSpec_ConnectionType;
  /** Specification for the BigQuery connection to a Cloud SQL instance. */
  cloudSql?:
    | CloudSqlBigQueryConnectionSpec
    | undefined;
  /**
   * True if there are credentials attached to the BigQuery connection; false
   * otherwise.
   */
  hasCredential: boolean;
}

/** The type of the BigQuery connection. */
export enum BigQueryConnectionSpec_ConnectionType {
  /** CONNECTION_TYPE_UNSPECIFIED - Unspecified type. */
  CONNECTION_TYPE_UNSPECIFIED = 0,
  /** CLOUD_SQL - Cloud SQL connection. */
  CLOUD_SQL = 1,
  UNRECOGNIZED = -1,
}

export function bigQueryConnectionSpec_ConnectionTypeFromJSON(object: any): BigQueryConnectionSpec_ConnectionType {
  switch (object) {
    case 0:
    case "CONNECTION_TYPE_UNSPECIFIED":
      return BigQueryConnectionSpec_ConnectionType.CONNECTION_TYPE_UNSPECIFIED;
    case 1:
    case "CLOUD_SQL":
      return BigQueryConnectionSpec_ConnectionType.CLOUD_SQL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return BigQueryConnectionSpec_ConnectionType.UNRECOGNIZED;
  }
}

export function bigQueryConnectionSpec_ConnectionTypeToJSON(object: BigQueryConnectionSpec_ConnectionType): string {
  switch (object) {
    case BigQueryConnectionSpec_ConnectionType.CONNECTION_TYPE_UNSPECIFIED:
      return "CONNECTION_TYPE_UNSPECIFIED";
    case BigQueryConnectionSpec_ConnectionType.CLOUD_SQL:
      return "CLOUD_SQL";
    case BigQueryConnectionSpec_ConnectionType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Specification for the BigQuery connection to a Cloud SQL instance. */
export interface CloudSqlBigQueryConnectionSpec {
  /** Cloud SQL instance ID in the format of `project:location:instance`. */
  instanceId: string;
  /** Database name. */
  database: string;
  /** Type of the Cloud SQL database. */
  type: CloudSqlBigQueryConnectionSpec_DatabaseType;
}

/** Supported Cloud SQL database types. */
export enum CloudSqlBigQueryConnectionSpec_DatabaseType {
  /** DATABASE_TYPE_UNSPECIFIED - Unspecified database type. */
  DATABASE_TYPE_UNSPECIFIED = 0,
  /** POSTGRES - Cloud SQL for PostgreSQL. */
  POSTGRES = 1,
  /** MYSQL - Cloud SQL for MySQL. */
  MYSQL = 2,
  UNRECOGNIZED = -1,
}

export function cloudSqlBigQueryConnectionSpec_DatabaseTypeFromJSON(
  object: any,
): CloudSqlBigQueryConnectionSpec_DatabaseType {
  switch (object) {
    case 0:
    case "DATABASE_TYPE_UNSPECIFIED":
      return CloudSqlBigQueryConnectionSpec_DatabaseType.DATABASE_TYPE_UNSPECIFIED;
    case 1:
    case "POSTGRES":
      return CloudSqlBigQueryConnectionSpec_DatabaseType.POSTGRES;
    case 2:
    case "MYSQL":
      return CloudSqlBigQueryConnectionSpec_DatabaseType.MYSQL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return CloudSqlBigQueryConnectionSpec_DatabaseType.UNRECOGNIZED;
  }
}

export function cloudSqlBigQueryConnectionSpec_DatabaseTypeToJSON(
  object: CloudSqlBigQueryConnectionSpec_DatabaseType,
): string {
  switch (object) {
    case CloudSqlBigQueryConnectionSpec_DatabaseType.DATABASE_TYPE_UNSPECIFIED:
      return "DATABASE_TYPE_UNSPECIFIED";
    case CloudSqlBigQueryConnectionSpec_DatabaseType.POSTGRES:
      return "POSTGRES";
    case CloudSqlBigQueryConnectionSpec_DatabaseType.MYSQL:
      return "MYSQL";
    case CloudSqlBigQueryConnectionSpec_DatabaseType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Fields specific for BigQuery routines. */
export interface BigQueryRoutineSpec {
  /** Paths of the imported libraries. */
  importedLibraries: string[];
}

function createBaseBigQueryConnectionSpec(): BigQueryConnectionSpec {
  return { connectionType: 0, cloudSql: undefined, hasCredential: false };
}

export const BigQueryConnectionSpec: MessageFns<BigQueryConnectionSpec> = {
  encode(message: BigQueryConnectionSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.connectionType !== 0) {
      writer.uint32(8).int32(message.connectionType);
    }
    if (message.cloudSql !== undefined) {
      CloudSqlBigQueryConnectionSpec.encode(message.cloudSql, writer.uint32(18).fork()).join();
    }
    if (message.hasCredential !== false) {
      writer.uint32(24).bool(message.hasCredential);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigQueryConnectionSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigQueryConnectionSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.connectionType = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.cloudSql = CloudSqlBigQueryConnectionSpec.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.hasCredential = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigQueryConnectionSpec {
    return {
      connectionType: isSet(object.connectionType)
        ? bigQueryConnectionSpec_ConnectionTypeFromJSON(object.connectionType)
        : 0,
      cloudSql: isSet(object.cloudSql) ? CloudSqlBigQueryConnectionSpec.fromJSON(object.cloudSql) : undefined,
      hasCredential: isSet(object.hasCredential) ? globalThis.Boolean(object.hasCredential) : false,
    };
  },

  toJSON(message: BigQueryConnectionSpec): unknown {
    const obj: any = {};
    if (message.connectionType !== 0) {
      obj.connectionType = bigQueryConnectionSpec_ConnectionTypeToJSON(message.connectionType);
    }
    if (message.cloudSql !== undefined) {
      obj.cloudSql = CloudSqlBigQueryConnectionSpec.toJSON(message.cloudSql);
    }
    if (message.hasCredential !== false) {
      obj.hasCredential = message.hasCredential;
    }
    return obj;
  },

  create(base?: DeepPartial<BigQueryConnectionSpec>): BigQueryConnectionSpec {
    return BigQueryConnectionSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigQueryConnectionSpec>): BigQueryConnectionSpec {
    const message = createBaseBigQueryConnectionSpec();
    message.connectionType = object.connectionType ?? 0;
    message.cloudSql = (object.cloudSql !== undefined && object.cloudSql !== null)
      ? CloudSqlBigQueryConnectionSpec.fromPartial(object.cloudSql)
      : undefined;
    message.hasCredential = object.hasCredential ?? false;
    return message;
  },
};

function createBaseCloudSqlBigQueryConnectionSpec(): CloudSqlBigQueryConnectionSpec {
  return { instanceId: "", database: "", type: 0 };
}

export const CloudSqlBigQueryConnectionSpec: MessageFns<CloudSqlBigQueryConnectionSpec> = {
  encode(message: CloudSqlBigQueryConnectionSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.instanceId !== "") {
      writer.uint32(10).string(message.instanceId);
    }
    if (message.database !== "") {
      writer.uint32(18).string(message.database);
    }
    if (message.type !== 0) {
      writer.uint32(24).int32(message.type);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CloudSqlBigQueryConnectionSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCloudSqlBigQueryConnectionSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.instanceId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.database = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CloudSqlBigQueryConnectionSpec {
    return {
      instanceId: isSet(object.instanceId) ? globalThis.String(object.instanceId) : "",
      database: isSet(object.database) ? globalThis.String(object.database) : "",
      type: isSet(object.type) ? cloudSqlBigQueryConnectionSpec_DatabaseTypeFromJSON(object.type) : 0,
    };
  },

  toJSON(message: CloudSqlBigQueryConnectionSpec): unknown {
    const obj: any = {};
    if (message.instanceId !== "") {
      obj.instanceId = message.instanceId;
    }
    if (message.database !== "") {
      obj.database = message.database;
    }
    if (message.type !== 0) {
      obj.type = cloudSqlBigQueryConnectionSpec_DatabaseTypeToJSON(message.type);
    }
    return obj;
  },

  create(base?: DeepPartial<CloudSqlBigQueryConnectionSpec>): CloudSqlBigQueryConnectionSpec {
    return CloudSqlBigQueryConnectionSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CloudSqlBigQueryConnectionSpec>): CloudSqlBigQueryConnectionSpec {
    const message = createBaseCloudSqlBigQueryConnectionSpec();
    message.instanceId = object.instanceId ?? "";
    message.database = object.database ?? "";
    message.type = object.type ?? 0;
    return message;
  },
};

function createBaseBigQueryRoutineSpec(): BigQueryRoutineSpec {
  return { importedLibraries: [] };
}

export const BigQueryRoutineSpec: MessageFns<BigQueryRoutineSpec> = {
  encode(message: BigQueryRoutineSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.importedLibraries) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BigQueryRoutineSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBigQueryRoutineSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.importedLibraries.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BigQueryRoutineSpec {
    return {
      importedLibraries: globalThis.Array.isArray(object?.importedLibraries)
        ? object.importedLibraries.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: BigQueryRoutineSpec): unknown {
    const obj: any = {};
    if (message.importedLibraries?.length) {
      obj.importedLibraries = message.importedLibraries;
    }
    return obj;
  },

  create(base?: DeepPartial<BigQueryRoutineSpec>): BigQueryRoutineSpec {
    return BigQueryRoutineSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BigQueryRoutineSpec>): BigQueryRoutineSpec {
    const message = createBaseBigQueryRoutineSpec();
    message.importedLibraries = object.importedLibraries?.map((e) => e) || [];
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
