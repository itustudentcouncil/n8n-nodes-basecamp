// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/dialogflow/cx/v3/audio_config.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Duration } from "../../../../protobuf/duration.js";

export const protobufPackage = "google.cloud.dialogflow.cx.v3";

/**
 * Audio encoding of the audio content sent in the conversational query request.
 * Refer to the
 * [Cloud Speech API
 * documentation](https://cloud.google.com/speech-to-text/docs/basics) for more
 * details.
 */
export enum AudioEncoding {
  /** AUDIO_ENCODING_UNSPECIFIED - Not specified. */
  AUDIO_ENCODING_UNSPECIFIED = 0,
  /** AUDIO_ENCODING_LINEAR_16 - Uncompressed 16-bit signed little-endian samples (Linear PCM). */
  AUDIO_ENCODING_LINEAR_16 = 1,
  /**
   * AUDIO_ENCODING_FLAC - [`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
   * Codec) is the recommended encoding because it is lossless (therefore
   * recognition is not compromised) and requires only about half the
   * bandwidth of `LINEAR16`. `FLAC` stream encoding supports 16-bit and
   * 24-bit samples, however, not all fields in `STREAMINFO` are supported.
   */
  AUDIO_ENCODING_FLAC = 2,
  /** AUDIO_ENCODING_MULAW - 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law. */
  AUDIO_ENCODING_MULAW = 3,
  /** AUDIO_ENCODING_AMR - Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000. */
  AUDIO_ENCODING_AMR = 4,
  /** AUDIO_ENCODING_AMR_WB - Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000. */
  AUDIO_ENCODING_AMR_WB = 5,
  /**
   * AUDIO_ENCODING_OGG_OPUS - Opus encoded audio frames in Ogg container
   * ([OggOpus](https://wiki.xiph.org/OggOpus)).
   * `sample_rate_hertz` must be 16000.
   */
  AUDIO_ENCODING_OGG_OPUS = 6,
  /**
   * AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE - Although the use of lossy encodings is not recommended, if a very low
   * bitrate encoding is required, `OGG_OPUS` is highly preferred over
   * Speex encoding. The [Speex](https://speex.org/) encoding supported by
   * Dialogflow API has a header byte in each block, as in MIME type
   * `audio/x-speex-with-header-byte`.
   * It is a variant of the RTP Speex encoding defined in
   * [RFC 5574](https://tools.ietf.org/html/rfc5574).
   * The stream is a sequence of blocks, one block per RTP packet. Each block
   * starts with a byte containing the length of the block, in bytes, followed
   * by one or more frames of Speex data, padded to an integral number of
   * bytes (octets) as specified in RFC 5574. In other words, each RTP header
   * is replaced with a single byte containing the block length. Only Speex
   * wideband is supported. `sample_rate_hertz` must be 16000.
   */
  AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE = 7,
  UNRECOGNIZED = -1,
}

export function audioEncodingFromJSON(object: any): AudioEncoding {
  switch (object) {
    case 0:
    case "AUDIO_ENCODING_UNSPECIFIED":
      return AudioEncoding.AUDIO_ENCODING_UNSPECIFIED;
    case 1:
    case "AUDIO_ENCODING_LINEAR_16":
      return AudioEncoding.AUDIO_ENCODING_LINEAR_16;
    case 2:
    case "AUDIO_ENCODING_FLAC":
      return AudioEncoding.AUDIO_ENCODING_FLAC;
    case 3:
    case "AUDIO_ENCODING_MULAW":
      return AudioEncoding.AUDIO_ENCODING_MULAW;
    case 4:
    case "AUDIO_ENCODING_AMR":
      return AudioEncoding.AUDIO_ENCODING_AMR;
    case 5:
    case "AUDIO_ENCODING_AMR_WB":
      return AudioEncoding.AUDIO_ENCODING_AMR_WB;
    case 6:
    case "AUDIO_ENCODING_OGG_OPUS":
      return AudioEncoding.AUDIO_ENCODING_OGG_OPUS;
    case 7:
    case "AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE":
      return AudioEncoding.AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AudioEncoding.UNRECOGNIZED;
  }
}

export function audioEncodingToJSON(object: AudioEncoding): string {
  switch (object) {
    case AudioEncoding.AUDIO_ENCODING_UNSPECIFIED:
      return "AUDIO_ENCODING_UNSPECIFIED";
    case AudioEncoding.AUDIO_ENCODING_LINEAR_16:
      return "AUDIO_ENCODING_LINEAR_16";
    case AudioEncoding.AUDIO_ENCODING_FLAC:
      return "AUDIO_ENCODING_FLAC";
    case AudioEncoding.AUDIO_ENCODING_MULAW:
      return "AUDIO_ENCODING_MULAW";
    case AudioEncoding.AUDIO_ENCODING_AMR:
      return "AUDIO_ENCODING_AMR";
    case AudioEncoding.AUDIO_ENCODING_AMR_WB:
      return "AUDIO_ENCODING_AMR_WB";
    case AudioEncoding.AUDIO_ENCODING_OGG_OPUS:
      return "AUDIO_ENCODING_OGG_OPUS";
    case AudioEncoding.AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE:
      return "AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE";
    case AudioEncoding.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Variant of the specified [Speech
 * model][google.cloud.dialogflow.cx.v3.InputAudioConfig.model] to use.
 *
 * See the [Cloud Speech
 * documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
 * for which models have different variants. For example, the "phone_call" model
 * has both a standard and an enhanced variant. When you use an enhanced model,
 * you will generally receive higher quality results than for a standard model.
 */
export enum SpeechModelVariant {
  /**
   * SPEECH_MODEL_VARIANT_UNSPECIFIED - No model variant specified. In this case Dialogflow defaults to
   * USE_BEST_AVAILABLE.
   */
  SPEECH_MODEL_VARIANT_UNSPECIFIED = 0,
  /**
   * USE_BEST_AVAILABLE - Use the best available variant of the [Speech
   * model][InputAudioConfig.model] that the caller is eligible for.
   */
  USE_BEST_AVAILABLE = 1,
  /**
   * USE_STANDARD - Use standard model variant even if an enhanced model is available.  See the
   * [Cloud Speech
   * documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
   * for details about enhanced models.
   */
  USE_STANDARD = 2,
  /**
   * USE_ENHANCED - Use an enhanced model variant:
   *
   * * If an enhanced variant does not exist for the given
   *   [model][google.cloud.dialogflow.cx.v3.InputAudioConfig.model] and request
   *   language, Dialogflow falls back to the standard variant.
   *
   *   The [Cloud Speech
   *   documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
   *   describes which models have enhanced variants.
   */
  USE_ENHANCED = 3,
  UNRECOGNIZED = -1,
}

export function speechModelVariantFromJSON(object: any): SpeechModelVariant {
  switch (object) {
    case 0:
    case "SPEECH_MODEL_VARIANT_UNSPECIFIED":
      return SpeechModelVariant.SPEECH_MODEL_VARIANT_UNSPECIFIED;
    case 1:
    case "USE_BEST_AVAILABLE":
      return SpeechModelVariant.USE_BEST_AVAILABLE;
    case 2:
    case "USE_STANDARD":
      return SpeechModelVariant.USE_STANDARD;
    case 3:
    case "USE_ENHANCED":
      return SpeechModelVariant.USE_ENHANCED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SpeechModelVariant.UNRECOGNIZED;
  }
}

export function speechModelVariantToJSON(object: SpeechModelVariant): string {
  switch (object) {
    case SpeechModelVariant.SPEECH_MODEL_VARIANT_UNSPECIFIED:
      return "SPEECH_MODEL_VARIANT_UNSPECIFIED";
    case SpeechModelVariant.USE_BEST_AVAILABLE:
      return "USE_BEST_AVAILABLE";
    case SpeechModelVariant.USE_STANDARD:
      return "USE_STANDARD";
    case SpeechModelVariant.USE_ENHANCED:
      return "USE_ENHANCED";
    case SpeechModelVariant.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Gender of the voice as described in
 * [SSML voice element](https://www.w3.org/TR/speech-synthesis11/#edef_voice).
 */
export enum SsmlVoiceGender {
  /**
   * SSML_VOICE_GENDER_UNSPECIFIED - An unspecified gender, which means that the client doesn't care which
   * gender the selected voice will have.
   */
  SSML_VOICE_GENDER_UNSPECIFIED = 0,
  /** SSML_VOICE_GENDER_MALE - A male voice. */
  SSML_VOICE_GENDER_MALE = 1,
  /** SSML_VOICE_GENDER_FEMALE - A female voice. */
  SSML_VOICE_GENDER_FEMALE = 2,
  /** SSML_VOICE_GENDER_NEUTRAL - A gender-neutral voice. */
  SSML_VOICE_GENDER_NEUTRAL = 3,
  UNRECOGNIZED = -1,
}

export function ssmlVoiceGenderFromJSON(object: any): SsmlVoiceGender {
  switch (object) {
    case 0:
    case "SSML_VOICE_GENDER_UNSPECIFIED":
      return SsmlVoiceGender.SSML_VOICE_GENDER_UNSPECIFIED;
    case 1:
    case "SSML_VOICE_GENDER_MALE":
      return SsmlVoiceGender.SSML_VOICE_GENDER_MALE;
    case 2:
    case "SSML_VOICE_GENDER_FEMALE":
      return SsmlVoiceGender.SSML_VOICE_GENDER_FEMALE;
    case 3:
    case "SSML_VOICE_GENDER_NEUTRAL":
      return SsmlVoiceGender.SSML_VOICE_GENDER_NEUTRAL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return SsmlVoiceGender.UNRECOGNIZED;
  }
}

export function ssmlVoiceGenderToJSON(object: SsmlVoiceGender): string {
  switch (object) {
    case SsmlVoiceGender.SSML_VOICE_GENDER_UNSPECIFIED:
      return "SSML_VOICE_GENDER_UNSPECIFIED";
    case SsmlVoiceGender.SSML_VOICE_GENDER_MALE:
      return "SSML_VOICE_GENDER_MALE";
    case SsmlVoiceGender.SSML_VOICE_GENDER_FEMALE:
      return "SSML_VOICE_GENDER_FEMALE";
    case SsmlVoiceGender.SSML_VOICE_GENDER_NEUTRAL:
      return "SSML_VOICE_GENDER_NEUTRAL";
    case SsmlVoiceGender.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Audio encoding of the output audio format in Text-To-Speech. */
export enum OutputAudioEncoding {
  /** OUTPUT_AUDIO_ENCODING_UNSPECIFIED - Not specified. */
  OUTPUT_AUDIO_ENCODING_UNSPECIFIED = 0,
  /**
   * OUTPUT_AUDIO_ENCODING_LINEAR_16 - Uncompressed 16-bit signed little-endian samples (Linear PCM).
   * Audio content returned as LINEAR16 also contains a WAV header.
   */
  OUTPUT_AUDIO_ENCODING_LINEAR_16 = 1,
  /** OUTPUT_AUDIO_ENCODING_MP3 - MP3 audio at 32kbps. */
  OUTPUT_AUDIO_ENCODING_MP3 = 2,
  /** OUTPUT_AUDIO_ENCODING_MP3_64_KBPS - MP3 audio at 64kbps. */
  OUTPUT_AUDIO_ENCODING_MP3_64_KBPS = 4,
  /**
   * OUTPUT_AUDIO_ENCODING_OGG_OPUS - Opus encoded audio wrapped in an ogg container. The result will be a
   * file which can be played natively on Android, and in browsers (at least
   * Chrome and Firefox). The quality of the encoding is considerably higher
   * than MP3 while using approximately the same bitrate.
   */
  OUTPUT_AUDIO_ENCODING_OGG_OPUS = 3,
  /** OUTPUT_AUDIO_ENCODING_MULAW - 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law. */
  OUTPUT_AUDIO_ENCODING_MULAW = 5,
  UNRECOGNIZED = -1,
}

export function outputAudioEncodingFromJSON(object: any): OutputAudioEncoding {
  switch (object) {
    case 0:
    case "OUTPUT_AUDIO_ENCODING_UNSPECIFIED":
      return OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_UNSPECIFIED;
    case 1:
    case "OUTPUT_AUDIO_ENCODING_LINEAR_16":
      return OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_LINEAR_16;
    case 2:
    case "OUTPUT_AUDIO_ENCODING_MP3":
      return OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_MP3;
    case 4:
    case "OUTPUT_AUDIO_ENCODING_MP3_64_KBPS":
      return OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_MP3_64_KBPS;
    case 3:
    case "OUTPUT_AUDIO_ENCODING_OGG_OPUS":
      return OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_OGG_OPUS;
    case 5:
    case "OUTPUT_AUDIO_ENCODING_MULAW":
      return OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_MULAW;
    case -1:
    case "UNRECOGNIZED":
    default:
      return OutputAudioEncoding.UNRECOGNIZED;
  }
}

export function outputAudioEncodingToJSON(object: OutputAudioEncoding): string {
  switch (object) {
    case OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_UNSPECIFIED:
      return "OUTPUT_AUDIO_ENCODING_UNSPECIFIED";
    case OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_LINEAR_16:
      return "OUTPUT_AUDIO_ENCODING_LINEAR_16";
    case OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_MP3:
      return "OUTPUT_AUDIO_ENCODING_MP3";
    case OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_MP3_64_KBPS:
      return "OUTPUT_AUDIO_ENCODING_MP3_64_KBPS";
    case OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_OGG_OPUS:
      return "OUTPUT_AUDIO_ENCODING_OGG_OPUS";
    case OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_MULAW:
      return "OUTPUT_AUDIO_ENCODING_MULAW";
    case OutputAudioEncoding.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Information for a word recognized by the speech recognizer. */
export interface SpeechWordInfo {
  /** The word this info is for. */
  word: string;
  /**
   * Time offset relative to the beginning of the audio that corresponds to the
   * start of the spoken word. This is an experimental feature and the accuracy
   * of the time offset can vary.
   */
  startOffset:
    | Duration
    | undefined;
  /**
   * Time offset relative to the beginning of the audio that corresponds to the
   * end of the spoken word. This is an experimental feature and the accuracy of
   * the time offset can vary.
   */
  endOffset:
    | Duration
    | undefined;
  /**
   * The Speech confidence between 0.0 and 1.0 for this word. A higher number
   * indicates an estimated greater likelihood that the recognized word is
   * correct. The default of 0.0 is a sentinel value indicating that confidence
   * was not set.
   *
   * This field is not guaranteed to be fully stable over time for the same
   * audio input. Users should also not rely on it to always be provided.
   */
  confidence: number;
}

/**
 * Configuration of the barge-in behavior. Barge-in instructs the API to return
 * a detected utterance at a proper time while the client is playing back the
 * response audio from a previous request. When the client sees the
 * utterance, it should stop the playback and immediately get ready for
 * receiving the responses for the current request.
 *
 * The barge-in handling requires the client to start streaming audio input
 * as soon as it starts playing back the audio from the previous response. The
 * playback is modeled into two phases:
 *
 * * No barge-in phase: which goes first and during which speech detection
 *   should not be carried out.
 *
 * * Barge-in phase: which follows the no barge-in phase and during which
 *   the API starts speech detection and may inform the client that an utterance
 *   has been detected. Note that no-speech event is not expected in this
 *   phase.
 *
 * The client provides this configuration in terms of the durations of those
 * two phases. The durations are measured in terms of the audio length from the
 * the start of the input audio.
 *
 * No-speech event is a response with END_OF_UTTERANCE without any transcript
 * following up.
 */
export interface BargeInConfig {
  /**
   * Duration that is not eligible for barge-in at the beginning of the input
   * audio.
   */
  noBargeInDuration:
    | Duration
    | undefined;
  /** Total duration for the playback at the beginning of the input audio. */
  totalDuration: Duration | undefined;
}

/** Instructs the speech recognizer on how to process the audio content. */
export interface InputAudioConfig {
  /** Required. Audio encoding of the audio content to process. */
  audioEncoding: AudioEncoding;
  /**
   * Sample rate (in Hertz) of the audio content sent in the query.
   * Refer to
   * [Cloud Speech API
   * documentation](https://cloud.google.com/speech-to-text/docs/basics) for
   * more details.
   */
  sampleRateHertz: number;
  /**
   * Optional. If `true`, Dialogflow returns
   * [SpeechWordInfo][google.cloud.dialogflow.cx.v3.SpeechWordInfo] in
   * [StreamingRecognitionResult][google.cloud.dialogflow.cx.v3.StreamingRecognitionResult]
   * with information about the recognized speech words, e.g. start and end time
   * offsets. If false or unspecified, Speech doesn't return any word-level
   * information.
   */
  enableWordInfo: boolean;
  /**
   * Optional. A list of strings containing words and phrases that the speech
   * recognizer should recognize with higher likelihood.
   *
   * See [the Cloud Speech
   * documentation](https://cloud.google.com/speech-to-text/docs/basics#phrase-hints)
   * for more details.
   */
  phraseHints: string[];
  /**
   * Optional. Which Speech model to select for the given request.
   * For more information, see
   * [Speech
   * models](https://cloud.google.com/dialogflow/cx/docs/concept/speech-models).
   */
  model: string;
  /**
   * Optional. Which variant of the [Speech
   * model][google.cloud.dialogflow.cx.v3.InputAudioConfig.model] to use.
   */
  modelVariant: SpeechModelVariant;
  /**
   * Optional. If `false` (default), recognition does not cease until the
   * client closes the stream.
   * If `true`, the recognizer will detect a single spoken utterance in input
   * audio. Recognition ceases when it detects the audio's voice has
   * stopped or paused. In this case, once a detected intent is received, the
   * client should close the stream and start a new request with a new stream as
   * needed.
   * Note: This setting is relevant only for streaming methods.
   */
  singleUtterance: boolean;
  /** Configuration of barge-in behavior during the streaming of input audio. */
  bargeInConfig:
    | BargeInConfig
    | undefined;
  /**
   * If `true`, the request will opt out for STT conformer model migration.
   * This field will be deprecated once force migration takes place in June
   * 2024. Please refer to [Dialogflow CX Speech model
   * migration](https://cloud.google.com/dialogflow/cx/docs/concept/speech-model-migration).
   */
  optOutConformerModelMigration: boolean;
}

/** Description of which voice to use for speech synthesis. */
export interface VoiceSelectionParams {
  /**
   * Optional. The name of the voice. If not set, the service will choose a
   * voice based on the other parameters such as language_code and
   * [ssml_gender][google.cloud.dialogflow.cx.v3.VoiceSelectionParams.ssml_gender].
   *
   * For the list of available voices, please refer to [Supported voices and
   * languages](https://cloud.google.com/text-to-speech/docs/voices).
   */
  name: string;
  /**
   * Optional. The preferred gender of the voice. If not set, the service will
   * choose a voice based on the other parameters such as language_code and
   * [name][google.cloud.dialogflow.cx.v3.VoiceSelectionParams.name]. Note that
   * this is only a preference, not requirement. If a voice of the appropriate
   * gender is not available, the synthesizer substitutes a voice with a
   * different gender rather than failing the request.
   */
  ssmlGender: SsmlVoiceGender;
}

/** Configuration of how speech should be synthesized. */
export interface SynthesizeSpeechConfig {
  /**
   * Optional. Speaking rate/speed, in the range [0.25, 4.0]. 1.0 is the normal
   * native speed supported by the specific voice. 2.0 is twice as fast, and
   * 0.5 is half as fast. If unset(0.0), defaults to the native 1.0 speed. Any
   * other values < 0.25 or > 4.0 will return an error.
   */
  speakingRate: number;
  /**
   * Optional. Speaking pitch, in the range [-20.0, 20.0]. 20 means increase 20
   * semitones from the original pitch. -20 means decrease 20 semitones from the
   * original pitch.
   */
  pitch: number;
  /**
   * Optional. Volume gain (in dB) of the normal native volume supported by the
   * specific voice, in the range [-96.0, 16.0]. If unset, or set to a value of
   * 0.0 (dB), will play at normal native signal amplitude. A value of -6.0 (dB)
   * will play at approximately half the amplitude of the normal native signal
   * amplitude. A value of +6.0 (dB) will play at approximately twice the
   * amplitude of the normal native signal amplitude. We strongly recommend not
   * to exceed +10 (dB) as there's usually no effective increase in loudness for
   * any value greater than that.
   */
  volumeGainDb: number;
  /**
   * Optional. An identifier which selects 'audio effects' profiles that are
   * applied on (post synthesized) text to speech. Effects are applied on top of
   * each other in the order they are given.
   */
  effectsProfileId: string[];
  /** Optional. The desired voice of the synthesized audio. */
  voice: VoiceSelectionParams | undefined;
}

/** Instructs the speech synthesizer how to generate the output audio content. */
export interface OutputAudioConfig {
  /** Required. Audio encoding of the synthesized audio content. */
  audioEncoding: OutputAudioEncoding;
  /**
   * Optional. The synthesis sample rate (in hertz) for this audio. If not
   * provided, then the synthesizer will use the default sample rate based on
   * the audio encoding. If this is different from the voice's natural sample
   * rate, then the synthesizer will honor this request by converting to the
   * desired sample rate (which might result in worse audio quality).
   */
  sampleRateHertz: number;
  /**
   * Optional. Configuration of how speech should be synthesized.
   * If not specified,
   * [Agent.text_to_speech_settings][google.cloud.dialogflow.cx.v3.Agent.text_to_speech_settings]
   * is applied.
   */
  synthesizeSpeechConfig: SynthesizeSpeechConfig | undefined;
}

/** Settings related to speech synthesizing. */
export interface TextToSpeechSettings {
  /**
   * Configuration of how speech should be synthesized, mapping from language
   * (https://cloud.google.com/dialogflow/cx/docs/reference/language) to
   * SynthesizeSpeechConfig.
   *
   * These settings affect:
   *
   *  - The [phone
   *  gateway](https://cloud.google.com/dialogflow/cx/docs/concept/integration/phone-gateway)
   *    synthesize configuration set via
   *    [Agent.text_to_speech_settings][google.cloud.dialogflow.cx.v3.Agent.text_to_speech_settings].
   *
   *  - How speech is synthesized when invoking
   *  [session][google.cloud.dialogflow.cx.v3.Sessions] APIs.
   *    [Agent.text_to_speech_settings][google.cloud.dialogflow.cx.v3.Agent.text_to_speech_settings]
   *    only applies if
   *    [OutputAudioConfig.synthesize_speech_config][google.cloud.dialogflow.cx.v3.OutputAudioConfig.synthesize_speech_config]
   *    is not specified.
   */
  synthesizeSpeechConfigs: { [key: string]: SynthesizeSpeechConfig };
}

export interface TextToSpeechSettings_SynthesizeSpeechConfigsEntry {
  key: string;
  value: SynthesizeSpeechConfig | undefined;
}

function createBaseSpeechWordInfo(): SpeechWordInfo {
  return { word: "", startOffset: undefined, endOffset: undefined, confidence: 0 };
}

export const SpeechWordInfo: MessageFns<SpeechWordInfo> = {
  encode(message: SpeechWordInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.word !== "") {
      writer.uint32(26).string(message.word);
    }
    if (message.startOffset !== undefined) {
      Duration.encode(message.startOffset, writer.uint32(10).fork()).join();
    }
    if (message.endOffset !== undefined) {
      Duration.encode(message.endOffset, writer.uint32(18).fork()).join();
    }
    if (message.confidence !== 0) {
      writer.uint32(37).float(message.confidence);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechWordInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechWordInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 26) {
            break;
          }

          message.word = reader.string();
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.startOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.endOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.confidence = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechWordInfo {
    return {
      word: isSet(object.word) ? globalThis.String(object.word) : "",
      startOffset: isSet(object.startOffset) ? Duration.fromJSON(object.startOffset) : undefined,
      endOffset: isSet(object.endOffset) ? Duration.fromJSON(object.endOffset) : undefined,
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
    };
  },

  toJSON(message: SpeechWordInfo): unknown {
    const obj: any = {};
    if (message.word !== "") {
      obj.word = message.word;
    }
    if (message.startOffset !== undefined) {
      obj.startOffset = Duration.toJSON(message.startOffset);
    }
    if (message.endOffset !== undefined) {
      obj.endOffset = Duration.toJSON(message.endOffset);
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    return obj;
  },

  create(base?: DeepPartial<SpeechWordInfo>): SpeechWordInfo {
    return SpeechWordInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SpeechWordInfo>): SpeechWordInfo {
    const message = createBaseSpeechWordInfo();
    message.word = object.word ?? "";
    message.startOffset = (object.startOffset !== undefined && object.startOffset !== null)
      ? Duration.fromPartial(object.startOffset)
      : undefined;
    message.endOffset = (object.endOffset !== undefined && object.endOffset !== null)
      ? Duration.fromPartial(object.endOffset)
      : undefined;
    message.confidence = object.confidence ?? 0;
    return message;
  },
};

function createBaseBargeInConfig(): BargeInConfig {
  return { noBargeInDuration: undefined, totalDuration: undefined };
}

export const BargeInConfig: MessageFns<BargeInConfig> = {
  encode(message: BargeInConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.noBargeInDuration !== undefined) {
      Duration.encode(message.noBargeInDuration, writer.uint32(10).fork()).join();
    }
    if (message.totalDuration !== undefined) {
      Duration.encode(message.totalDuration, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BargeInConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBargeInConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.noBargeInDuration = Duration.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.totalDuration = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BargeInConfig {
    return {
      noBargeInDuration: isSet(object.noBargeInDuration) ? Duration.fromJSON(object.noBargeInDuration) : undefined,
      totalDuration: isSet(object.totalDuration) ? Duration.fromJSON(object.totalDuration) : undefined,
    };
  },

  toJSON(message: BargeInConfig): unknown {
    const obj: any = {};
    if (message.noBargeInDuration !== undefined) {
      obj.noBargeInDuration = Duration.toJSON(message.noBargeInDuration);
    }
    if (message.totalDuration !== undefined) {
      obj.totalDuration = Duration.toJSON(message.totalDuration);
    }
    return obj;
  },

  create(base?: DeepPartial<BargeInConfig>): BargeInConfig {
    return BargeInConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BargeInConfig>): BargeInConfig {
    const message = createBaseBargeInConfig();
    message.noBargeInDuration = (object.noBargeInDuration !== undefined && object.noBargeInDuration !== null)
      ? Duration.fromPartial(object.noBargeInDuration)
      : undefined;
    message.totalDuration = (object.totalDuration !== undefined && object.totalDuration !== null)
      ? Duration.fromPartial(object.totalDuration)
      : undefined;
    return message;
  },
};

function createBaseInputAudioConfig(): InputAudioConfig {
  return {
    audioEncoding: 0,
    sampleRateHertz: 0,
    enableWordInfo: false,
    phraseHints: [],
    model: "",
    modelVariant: 0,
    singleUtterance: false,
    bargeInConfig: undefined,
    optOutConformerModelMigration: false,
  };
}

export const InputAudioConfig: MessageFns<InputAudioConfig> = {
  encode(message: InputAudioConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.audioEncoding !== 0) {
      writer.uint32(8).int32(message.audioEncoding);
    }
    if (message.sampleRateHertz !== 0) {
      writer.uint32(16).int32(message.sampleRateHertz);
    }
    if (message.enableWordInfo !== false) {
      writer.uint32(104).bool(message.enableWordInfo);
    }
    for (const v of message.phraseHints) {
      writer.uint32(34).string(v!);
    }
    if (message.model !== "") {
      writer.uint32(58).string(message.model);
    }
    if (message.modelVariant !== 0) {
      writer.uint32(80).int32(message.modelVariant);
    }
    if (message.singleUtterance !== false) {
      writer.uint32(64).bool(message.singleUtterance);
    }
    if (message.bargeInConfig !== undefined) {
      BargeInConfig.encode(message.bargeInConfig, writer.uint32(122).fork()).join();
    }
    if (message.optOutConformerModelMigration !== false) {
      writer.uint32(208).bool(message.optOutConformerModelMigration);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): InputAudioConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInputAudioConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.audioEncoding = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.sampleRateHertz = reader.int32();
          continue;
        case 13:
          if (tag !== 104) {
            break;
          }

          message.enableWordInfo = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.phraseHints.push(reader.string());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.model = reader.string();
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.modelVariant = reader.int32() as any;
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.singleUtterance = reader.bool();
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.bargeInConfig = BargeInConfig.decode(reader, reader.uint32());
          continue;
        case 26:
          if (tag !== 208) {
            break;
          }

          message.optOutConformerModelMigration = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InputAudioConfig {
    return {
      audioEncoding: isSet(object.audioEncoding) ? audioEncodingFromJSON(object.audioEncoding) : 0,
      sampleRateHertz: isSet(object.sampleRateHertz) ? globalThis.Number(object.sampleRateHertz) : 0,
      enableWordInfo: isSet(object.enableWordInfo) ? globalThis.Boolean(object.enableWordInfo) : false,
      phraseHints: globalThis.Array.isArray(object?.phraseHints)
        ? object.phraseHints.map((e: any) => globalThis.String(e))
        : [],
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      modelVariant: isSet(object.modelVariant) ? speechModelVariantFromJSON(object.modelVariant) : 0,
      singleUtterance: isSet(object.singleUtterance) ? globalThis.Boolean(object.singleUtterance) : false,
      bargeInConfig: isSet(object.bargeInConfig) ? BargeInConfig.fromJSON(object.bargeInConfig) : undefined,
      optOutConformerModelMigration: isSet(object.optOutConformerModelMigration)
        ? globalThis.Boolean(object.optOutConformerModelMigration)
        : false,
    };
  },

  toJSON(message: InputAudioConfig): unknown {
    const obj: any = {};
    if (message.audioEncoding !== 0) {
      obj.audioEncoding = audioEncodingToJSON(message.audioEncoding);
    }
    if (message.sampleRateHertz !== 0) {
      obj.sampleRateHertz = Math.round(message.sampleRateHertz);
    }
    if (message.enableWordInfo !== false) {
      obj.enableWordInfo = message.enableWordInfo;
    }
    if (message.phraseHints?.length) {
      obj.phraseHints = message.phraseHints;
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.modelVariant !== 0) {
      obj.modelVariant = speechModelVariantToJSON(message.modelVariant);
    }
    if (message.singleUtterance !== false) {
      obj.singleUtterance = message.singleUtterance;
    }
    if (message.bargeInConfig !== undefined) {
      obj.bargeInConfig = BargeInConfig.toJSON(message.bargeInConfig);
    }
    if (message.optOutConformerModelMigration !== false) {
      obj.optOutConformerModelMigration = message.optOutConformerModelMigration;
    }
    return obj;
  },

  create(base?: DeepPartial<InputAudioConfig>): InputAudioConfig {
    return InputAudioConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<InputAudioConfig>): InputAudioConfig {
    const message = createBaseInputAudioConfig();
    message.audioEncoding = object.audioEncoding ?? 0;
    message.sampleRateHertz = object.sampleRateHertz ?? 0;
    message.enableWordInfo = object.enableWordInfo ?? false;
    message.phraseHints = object.phraseHints?.map((e) => e) || [];
    message.model = object.model ?? "";
    message.modelVariant = object.modelVariant ?? 0;
    message.singleUtterance = object.singleUtterance ?? false;
    message.bargeInConfig = (object.bargeInConfig !== undefined && object.bargeInConfig !== null)
      ? BargeInConfig.fromPartial(object.bargeInConfig)
      : undefined;
    message.optOutConformerModelMigration = object.optOutConformerModelMigration ?? false;
    return message;
  },
};

function createBaseVoiceSelectionParams(): VoiceSelectionParams {
  return { name: "", ssmlGender: 0 };
}

export const VoiceSelectionParams: MessageFns<VoiceSelectionParams> = {
  encode(message: VoiceSelectionParams, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.ssmlGender !== 0) {
      writer.uint32(16).int32(message.ssmlGender);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VoiceSelectionParams {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVoiceSelectionParams();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.ssmlGender = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VoiceSelectionParams {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      ssmlGender: isSet(object.ssmlGender) ? ssmlVoiceGenderFromJSON(object.ssmlGender) : 0,
    };
  },

  toJSON(message: VoiceSelectionParams): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.ssmlGender !== 0) {
      obj.ssmlGender = ssmlVoiceGenderToJSON(message.ssmlGender);
    }
    return obj;
  },

  create(base?: DeepPartial<VoiceSelectionParams>): VoiceSelectionParams {
    return VoiceSelectionParams.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VoiceSelectionParams>): VoiceSelectionParams {
    const message = createBaseVoiceSelectionParams();
    message.name = object.name ?? "";
    message.ssmlGender = object.ssmlGender ?? 0;
    return message;
  },
};

function createBaseSynthesizeSpeechConfig(): SynthesizeSpeechConfig {
  return { speakingRate: 0, pitch: 0, volumeGainDb: 0, effectsProfileId: [], voice: undefined };
}

export const SynthesizeSpeechConfig: MessageFns<SynthesizeSpeechConfig> = {
  encode(message: SynthesizeSpeechConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.speakingRate !== 0) {
      writer.uint32(9).double(message.speakingRate);
    }
    if (message.pitch !== 0) {
      writer.uint32(17).double(message.pitch);
    }
    if (message.volumeGainDb !== 0) {
      writer.uint32(25).double(message.volumeGainDb);
    }
    for (const v of message.effectsProfileId) {
      writer.uint32(42).string(v!);
    }
    if (message.voice !== undefined) {
      VoiceSelectionParams.encode(message.voice, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SynthesizeSpeechConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSynthesizeSpeechConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 9) {
            break;
          }

          message.speakingRate = reader.double();
          continue;
        case 2:
          if (tag !== 17) {
            break;
          }

          message.pitch = reader.double();
          continue;
        case 3:
          if (tag !== 25) {
            break;
          }

          message.volumeGainDb = reader.double();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.effectsProfileId.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.voice = VoiceSelectionParams.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SynthesizeSpeechConfig {
    return {
      speakingRate: isSet(object.speakingRate) ? globalThis.Number(object.speakingRate) : 0,
      pitch: isSet(object.pitch) ? globalThis.Number(object.pitch) : 0,
      volumeGainDb: isSet(object.volumeGainDb) ? globalThis.Number(object.volumeGainDb) : 0,
      effectsProfileId: globalThis.Array.isArray(object?.effectsProfileId)
        ? object.effectsProfileId.map((e: any) => globalThis.String(e))
        : [],
      voice: isSet(object.voice) ? VoiceSelectionParams.fromJSON(object.voice) : undefined,
    };
  },

  toJSON(message: SynthesizeSpeechConfig): unknown {
    const obj: any = {};
    if (message.speakingRate !== 0) {
      obj.speakingRate = message.speakingRate;
    }
    if (message.pitch !== 0) {
      obj.pitch = message.pitch;
    }
    if (message.volumeGainDb !== 0) {
      obj.volumeGainDb = message.volumeGainDb;
    }
    if (message.effectsProfileId?.length) {
      obj.effectsProfileId = message.effectsProfileId;
    }
    if (message.voice !== undefined) {
      obj.voice = VoiceSelectionParams.toJSON(message.voice);
    }
    return obj;
  },

  create(base?: DeepPartial<SynthesizeSpeechConfig>): SynthesizeSpeechConfig {
    return SynthesizeSpeechConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SynthesizeSpeechConfig>): SynthesizeSpeechConfig {
    const message = createBaseSynthesizeSpeechConfig();
    message.speakingRate = object.speakingRate ?? 0;
    message.pitch = object.pitch ?? 0;
    message.volumeGainDb = object.volumeGainDb ?? 0;
    message.effectsProfileId = object.effectsProfileId?.map((e) => e) || [];
    message.voice = (object.voice !== undefined && object.voice !== null)
      ? VoiceSelectionParams.fromPartial(object.voice)
      : undefined;
    return message;
  },
};

function createBaseOutputAudioConfig(): OutputAudioConfig {
  return { audioEncoding: 0, sampleRateHertz: 0, synthesizeSpeechConfig: undefined };
}

export const OutputAudioConfig: MessageFns<OutputAudioConfig> = {
  encode(message: OutputAudioConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.audioEncoding !== 0) {
      writer.uint32(8).int32(message.audioEncoding);
    }
    if (message.sampleRateHertz !== 0) {
      writer.uint32(16).int32(message.sampleRateHertz);
    }
    if (message.synthesizeSpeechConfig !== undefined) {
      SynthesizeSpeechConfig.encode(message.synthesizeSpeechConfig, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OutputAudioConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOutputAudioConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.audioEncoding = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.sampleRateHertz = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.synthesizeSpeechConfig = SynthesizeSpeechConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): OutputAudioConfig {
    return {
      audioEncoding: isSet(object.audioEncoding) ? outputAudioEncodingFromJSON(object.audioEncoding) : 0,
      sampleRateHertz: isSet(object.sampleRateHertz) ? globalThis.Number(object.sampleRateHertz) : 0,
      synthesizeSpeechConfig: isSet(object.synthesizeSpeechConfig)
        ? SynthesizeSpeechConfig.fromJSON(object.synthesizeSpeechConfig)
        : undefined,
    };
  },

  toJSON(message: OutputAudioConfig): unknown {
    const obj: any = {};
    if (message.audioEncoding !== 0) {
      obj.audioEncoding = outputAudioEncodingToJSON(message.audioEncoding);
    }
    if (message.sampleRateHertz !== 0) {
      obj.sampleRateHertz = Math.round(message.sampleRateHertz);
    }
    if (message.synthesizeSpeechConfig !== undefined) {
      obj.synthesizeSpeechConfig = SynthesizeSpeechConfig.toJSON(message.synthesizeSpeechConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<OutputAudioConfig>): OutputAudioConfig {
    return OutputAudioConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OutputAudioConfig>): OutputAudioConfig {
    const message = createBaseOutputAudioConfig();
    message.audioEncoding = object.audioEncoding ?? 0;
    message.sampleRateHertz = object.sampleRateHertz ?? 0;
    message.synthesizeSpeechConfig =
      (object.synthesizeSpeechConfig !== undefined && object.synthesizeSpeechConfig !== null)
        ? SynthesizeSpeechConfig.fromPartial(object.synthesizeSpeechConfig)
        : undefined;
    return message;
  },
};

function createBaseTextToSpeechSettings(): TextToSpeechSettings {
  return { synthesizeSpeechConfigs: {} };
}

export const TextToSpeechSettings: MessageFns<TextToSpeechSettings> = {
  encode(message: TextToSpeechSettings, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.synthesizeSpeechConfigs).forEach(([key, value]) => {
      TextToSpeechSettings_SynthesizeSpeechConfigsEntry.encode({ key: key as any, value }, writer.uint32(10).fork())
        .join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TextToSpeechSettings {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextToSpeechSettings();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          const entry1 = TextToSpeechSettings_SynthesizeSpeechConfigsEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.synthesizeSpeechConfigs[entry1.key] = entry1.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextToSpeechSettings {
    return {
      synthesizeSpeechConfigs: isObject(object.synthesizeSpeechConfigs)
        ? Object.entries(object.synthesizeSpeechConfigs).reduce<{ [key: string]: SynthesizeSpeechConfig }>(
          (acc, [key, value]) => {
            acc[key] = SynthesizeSpeechConfig.fromJSON(value);
            return acc;
          },
          {},
        )
        : {},
    };
  },

  toJSON(message: TextToSpeechSettings): unknown {
    const obj: any = {};
    if (message.synthesizeSpeechConfigs) {
      const entries = Object.entries(message.synthesizeSpeechConfigs);
      if (entries.length > 0) {
        obj.synthesizeSpeechConfigs = {};
        entries.forEach(([k, v]) => {
          obj.synthesizeSpeechConfigs[k] = SynthesizeSpeechConfig.toJSON(v);
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<TextToSpeechSettings>): TextToSpeechSettings {
    return TextToSpeechSettings.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TextToSpeechSettings>): TextToSpeechSettings {
    const message = createBaseTextToSpeechSettings();
    message.synthesizeSpeechConfigs = Object.entries(object.synthesizeSpeechConfigs ?? {}).reduce<
      { [key: string]: SynthesizeSpeechConfig }
    >((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = SynthesizeSpeechConfig.fromPartial(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseTextToSpeechSettings_SynthesizeSpeechConfigsEntry(): TextToSpeechSettings_SynthesizeSpeechConfigsEntry {
  return { key: "", value: undefined };
}

export const TextToSpeechSettings_SynthesizeSpeechConfigsEntry: MessageFns<
  TextToSpeechSettings_SynthesizeSpeechConfigsEntry
> = {
  encode(
    message: TextToSpeechSettings_SynthesizeSpeechConfigsEntry,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      SynthesizeSpeechConfig.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TextToSpeechSettings_SynthesizeSpeechConfigsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextToSpeechSettings_SynthesizeSpeechConfigsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = SynthesizeSpeechConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextToSpeechSettings_SynthesizeSpeechConfigsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? SynthesizeSpeechConfig.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: TextToSpeechSettings_SynthesizeSpeechConfigsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = SynthesizeSpeechConfig.toJSON(message.value);
    }
    return obj;
  },

  create(
    base?: DeepPartial<TextToSpeechSettings_SynthesizeSpeechConfigsEntry>,
  ): TextToSpeechSettings_SynthesizeSpeechConfigsEntry {
    return TextToSpeechSettings_SynthesizeSpeechConfigsEntry.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<TextToSpeechSettings_SynthesizeSpeechConfigsEntry>,
  ): TextToSpeechSettings_SynthesizeSpeechConfigsEntry {
    const message = createBaseTextToSpeechSettings_SynthesizeSpeechConfigsEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? SynthesizeSpeechConfig.fromPartial(object.value)
      : undefined;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
