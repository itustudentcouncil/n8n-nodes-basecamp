// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/dialogflow/v2beta1/session.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Duration } from "../../../protobuf/duration.js";
import { FieldMask } from "../../../protobuf/field_mask.js";
import { Struct } from "../../../protobuf/struct.js";
import { Status } from "../../../rpc/status.js";
import { LatLng } from "../../../type/latlng.js";
import { SubAgent } from "./agent.js";
import { InputAudioConfig, OutputAudioConfig, SpeechWordInfo, TelephonyDtmfEvents } from "./audio_config.js";
import { Context } from "./context.js";
import { Intent, Intent_Message } from "./intent.js";
import { SessionEntityType } from "./session_entity_type.js";

export const protobufPackage = "google.cloud.dialogflow.v2beta1";

/** The request to detect user's intent. */
export interface DetectIntentRequest {
  /**
   * Required. The name of the session this query is sent to. Supported formats:
   * - `projects/<Project ID>/agent/sessions/<Session ID>,
   * - `projects/<Project ID>/locations/<Location ID>/agent/sessions/<Session
   *   ID>`,
   * - `projects/<Project ID>/agent/environments/<Environment ID>/users/<User
   *   ID>/sessions/<Session ID>`,
   * - `projects/<Project ID>/locations/<Location
   *   ID>/agent/environments/<Environment ID>/users/<User ID>/sessions/<Session
   *   ID>`,
   *
   * If `Location ID` is not specified we assume default 'us' location. If
   * `Environment ID` is not specified, we assume default 'draft' environment
   * (`Environment ID` might be referred to as environment name at some places).
   * If `User ID` is not specified, we are using "-". It's up to the API caller
   * to choose an appropriate `Session ID` and `User Id`. They can be a random
   * number or some type of user and session identifiers (preferably hashed).
   * The length of the `Session ID` and `User ID` must not exceed 36 characters.
   * For more information, see the [API interactions
   * guide](https://cloud.google.com/dialogflow/docs/api-overview).
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/es/docs/agents-versions).
   */
  session: string;
  /** The parameters of this query. */
  queryParams:
    | QueryParameters
    | undefined;
  /**
   * Required. The input specification. It can be set to:
   *
   * 1. an audio config which instructs the speech recognizer how to process
   * the speech audio,
   *
   * 2. a conversational query in the form of text, or
   *
   * 3. an event that specifies which intent to trigger.
   */
  queryInput:
    | QueryInput
    | undefined;
  /**
   * Instructs the speech synthesizer how to generate the output
   * audio. If this field is not set and agent-level speech synthesizer is not
   * configured, no output audio is generated.
   */
  outputAudioConfig:
    | OutputAudioConfig
    | undefined;
  /**
   * Mask for
   * [output_audio_config][google.cloud.dialogflow.v2beta1.DetectIntentRequest.output_audio_config]
   * indicating which settings in this request-level config should override
   * speech synthesizer settings defined at agent-level.
   *
   * If unspecified or empty,
   * [output_audio_config][google.cloud.dialogflow.v2beta1.DetectIntentRequest.output_audio_config]
   * replaces the agent-level config in its entirety.
   */
  outputAudioConfigMask:
    | string[]
    | undefined;
  /**
   * The natural language speech audio to be processed. This field
   * should be populated iff `query_input` is set to an input audio config.
   * A single request can contain up to 1 minute of speech audio data.
   */
  inputAudio: Buffer;
}

/** The message returned from the DetectIntent method. */
export interface DetectIntentResponse {
  /**
   * The unique identifier of the response. It can be used to
   * locate a response in the training example set or for reporting issues.
   */
  responseId: string;
  /**
   * The selected results of the conversational query or event processing.
   * See `alternative_query_results` for additional potential results.
   */
  queryResult:
    | QueryResult
    | undefined;
  /**
   * If Knowledge Connectors are enabled, there could be more than one result
   * returned for a given query or event, and this field will contain all
   * results except for the top one, which is captured in query_result. The
   * alternative results are ordered by decreasing
   * `QueryResult.intent_detection_confidence`. If Knowledge Connectors are
   * disabled, this field will be empty until multiple responses for regular
   * intents are supported, at which point those additional results will be
   * surfaced here.
   */
  alternativeQueryResults: QueryResult[];
  /** Specifies the status of the webhook request. */
  webhookStatus:
    | Status
    | undefined;
  /**
   * The audio data bytes encoded as specified in the request.
   * Note: The output audio is generated based on the values of default platform
   * text responses found in the `query_result.fulfillment_messages` field. If
   * multiple default text responses exist, they will be concatenated when
   * generating audio. If no default platform text responses exist, the
   * generated audio content will be empty.
   *
   * In some scenarios, multiple output audio fields may be present in the
   * response structure. In these cases, only the top-most-level audio output
   * has content.
   */
  outputAudio: Buffer;
  /** The config used by the speech synthesizer to generate the output audio. */
  outputAudioConfig: OutputAudioConfig | undefined;
}

/** Represents the parameters of the conversational query. */
export interface QueryParameters {
  /**
   * The time zone of this conversational query from the
   * [time zone database](https://www.iana.org/time-zones), e.g.,
   * America/New_York, Europe/Paris. If not provided, the time zone specified in
   * agent settings is used.
   */
  timeZone: string;
  /** The geo location of this conversational query. */
  geoLocation:
    | LatLng
    | undefined;
  /**
   * The collection of contexts to be activated before this query is
   * executed.
   */
  contexts: Context[];
  /**
   * Specifies whether to delete all contexts in the current session
   * before the new ones are activated.
   */
  resetContexts: boolean;
  /**
   * Additional session entity types to replace or extend developer
   * entity types with. The entity synonyms apply to all languages and persist
   * for the session of this query.
   */
  sessionEntityTypes: SessionEntityType[];
  /**
   * This field can be used to pass custom data to your webhook.
   * Arbitrary JSON objects are supported.
   * If supplied, the value is used to populate the
   * `WebhookRequest.original_detect_intent_request.payload`
   * field sent to your webhook.
   */
  payload:
    | { [key: string]: any }
    | undefined;
  /**
   * KnowledgeBases to get alternative results from. If not set, the
   * KnowledgeBases enabled in the agent (through UI) will be used.
   * Format:  `projects/<Project ID>/knowledgeBases/<Knowledge Base ID>`.
   */
  knowledgeBaseNames: string[];
  /**
   * Configures the type of sentiment analysis to perform. If not
   * provided, sentiment analysis is not performed.
   * Note: Sentiment Analysis is only currently available for Essentials Edition
   * agents.
   */
  sentimentAnalysisRequestConfig:
    | SentimentAnalysisRequestConfig
    | undefined;
  /**
   * For mega agent query, directly specify which sub agents to query.
   * If any specified sub agent is not linked to the mega agent, an error will
   * be returned. If empty, Dialogflow will decide which sub agents to query.
   * If specified for a non-mega-agent query, will be silently ignored.
   */
  subAgents: SubAgent[];
  /**
   * This field can be used to pass HTTP headers for a webhook
   * call. These headers will be sent to webhook along with the headers that
   * have been configured through Dialogflow web console. The headers defined
   * within this field will overwrite the headers configured through Dialogflow
   * console if there is a conflict. Header names are case-insensitive.
   * Google's specified headers are not allowed. Including: "Host",
   * "Content-Length", "Connection", "From", "User-Agent", "Accept-Encoding",
   * "If-Modified-Since", "If-None-Match", "X-Forwarded-For", etc.
   */
  webhookHeaders: { [key: string]: string };
  /**
   * The platform of the virtual agent response messages.
   *
   * If not empty, only emits messages from this platform in the response.
   * Valid values are the enum names of
   * [platform][google.cloud.dialogflow.v2beta1.Intent.Message.platform].
   */
  platform: string;
}

export interface QueryParameters_WebhookHeadersEntry {
  key: string;
  value: string;
}

/**
 * Represents the query input. It can contain either:
 *
 * 1. An audio config which instructs the speech recognizer how to process the
 * speech audio.
 *
 * 2. A conversational query in the form of text.
 *
 * 3. An event that specifies which intent to trigger.
 */
export interface QueryInput {
  /** Instructs the speech recognizer how to process the speech audio. */
  audioConfig?:
    | InputAudioConfig
    | undefined;
  /** The natural language text to be processed. */
  text?:
    | TextInput
    | undefined;
  /** The event to be processed. */
  event?:
    | EventInput
    | undefined;
  /** The DTMF digits used to invoke intent and fill in parameter value. */
  dtmf?: TelephonyDtmfEvents | undefined;
}

/** Represents the result of conversational query or event processing. */
export interface QueryResult {
  /**
   * The original conversational query text:
   *
   * - If natural language text was provided as input, `query_text` contains
   *   a copy of the input.
   * - If natural language speech audio was provided as input, `query_text`
   *   contains the speech recognition result. If speech recognizer produced
   *   multiple alternatives, a particular one is picked.
   * - If automatic spell correction is enabled, `query_text` will contain the
   *   corrected user input.
   */
  queryText: string;
  /**
   * The language that was triggered during intent detection.
   * See [Language
   * Support](https://cloud.google.com/dialogflow/docs/reference/language)
   * for a list of the currently supported language codes.
   */
  languageCode: string;
  /**
   * The Speech recognition confidence between 0.0 and 1.0. A higher number
   * indicates an estimated greater likelihood that the recognized words are
   * correct. The default of 0.0 is a sentinel value indicating that confidence
   * was not set.
   *
   * This field is not guaranteed to be accurate or set. In particular this
   * field isn't set for StreamingDetectIntent since the streaming endpoint has
   * separate confidence estimates per portion of the audio in
   * StreamingRecognitionResult.
   */
  speechRecognitionConfidence: number;
  /** The action name from the matched intent. */
  action: string;
  /**
   * The collection of extracted parameters.
   *
   * Depending on your protocol or client library language, this is a
   * map, associative array, symbol table, dictionary, or JSON object
   * composed of a collection of (MapKey, MapValue) pairs:
   *
   * * MapKey type: string
   * * MapKey value: parameter name
   * * MapValue type: If parameter's entity type is a composite entity then use
   * map, otherwise, depending on the parameter value type, it could be one of
   * string, number, boolean, null, list or map.
   * * MapValue value: If parameter's entity type is a composite entity then use
   * map from composite entity property names to property values, otherwise,
   * use parameter value.
   */
  parameters:
    | { [key: string]: any }
    | undefined;
  /**
   * This field is set to:
   *
   * - `false` if the matched intent has required parameters and not all of
   *    the required parameter values have been collected.
   * - `true` if all required parameter values have been collected, or if the
   *    matched intent doesn't contain any required parameters.
   */
  allRequiredParamsPresent: boolean;
  /**
   * Indicates whether the conversational query triggers a cancellation for slot
   * filling. For more information, see the [cancel slot filling
   * documentation](https://cloud.google.com/dialogflow/es/docs/intents-actions-parameters#cancel).
   */
  cancelsSlotFilling: boolean;
  /**
   * The text to be pronounced to the user or shown on the screen.
   * Note: This is a legacy field, `fulfillment_messages` should be preferred.
   */
  fulfillmentText: string;
  /** The collection of rich messages to present to the user. */
  fulfillmentMessages: Intent_Message[];
  /**
   * If the query was fulfilled by a webhook call, this field is set to the
   * value of the `source` field returned in the webhook response.
   */
  webhookSource: string;
  /**
   * If the query was fulfilled by a webhook call, this field is set to the
   * value of the `payload` field returned in the webhook response.
   */
  webhookPayload:
    | { [key: string]: any }
    | undefined;
  /**
   * The collection of output contexts. If applicable,
   * `output_contexts.parameters` contains entries with name
   * `<parameter name>.original` containing the original parameter values
   * before the query.
   */
  outputContexts: Context[];
  /**
   * The intent that matched the conversational query. Some, not
   * all fields are filled in this message, including but not limited to:
   * `name`, `display_name`, `end_interaction` and `is_fallback`.
   */
  intent:
    | Intent
    | undefined;
  /**
   * The intent detection confidence. Values range from 0.0
   * (completely uncertain) to 1.0 (completely certain).
   * This value is for informational purpose only and is only used to
   * help match the best intent within the classification threshold.
   * This value may change for the same end-user expression at any time due to a
   * model retraining or change in implementation.
   * If there are `multiple knowledge_answers` messages, this value is set to
   * the greatest `knowledgeAnswers.match_confidence` value in the list.
   */
  intentDetectionConfidence: number;
  /**
   * Free-form diagnostic information for the associated detect intent request.
   * The fields of this data can change without notice, so you should not write
   * code that depends on its structure.
   * The data may contain:
   *
   * - webhook call latency
   * - webhook errors
   */
  diagnosticInfo:
    | { [key: string]: any }
    | undefined;
  /**
   * The sentiment analysis result, which depends on the
   * `sentiment_analysis_request_config` specified in the request.
   */
  sentimentAnalysisResult:
    | SentimentAnalysisResult
    | undefined;
  /**
   * The result from Knowledge Connector (if any), ordered by decreasing
   * `KnowledgeAnswers.match_confidence`.
   */
  knowledgeAnswers: KnowledgeAnswers | undefined;
}

/** Represents the result of querying a Knowledge base. */
export interface KnowledgeAnswers {
  /** A list of answers from Knowledge Connector. */
  answers: KnowledgeAnswers_Answer[];
}

/** An answer from Knowledge Connector. */
export interface KnowledgeAnswers_Answer {
  /**
   * Indicates which Knowledge Document this answer was extracted from.
   * Format: `projects/<Project ID>/knowledgeBases/<Knowledge Base
   * ID>/documents/<Document ID>`.
   */
  source: string;
  /**
   * The corresponding FAQ question if the answer was extracted from a FAQ
   * Document, empty otherwise.
   */
  faqQuestion: string;
  /**
   * The piece of text from the `source` knowledge base document that answers
   * this conversational query.
   */
  answer: string;
  /**
   * The system's confidence level that this knowledge answer is a good match
   * for this conversational query.
   * NOTE: The confidence level for a given `<query, answer>` pair may change
   * without notice, as it depends on models that are constantly being
   * improved. However, it will change less frequently than the confidence
   * score below, and should be preferred for referencing the quality of an
   * answer.
   */
  matchConfidenceLevel: KnowledgeAnswers_Answer_MatchConfidenceLevel;
  /**
   * The system's confidence score that this Knowledge answer is a good match
   * for this conversational query.
   * The range is from 0.0 (completely uncertain) to 1.0 (completely certain).
   * Note: The confidence score is likely to vary somewhat (possibly even for
   * identical requests), as the underlying model is under constant
   * improvement. It may be deprecated in the future. We recommend using
   * `match_confidence_level` which should be generally more stable.
   */
  matchConfidence: number;
}

/**
 * Represents the system's confidence that this knowledge answer is a good
 * match for this conversational query.
 */
export enum KnowledgeAnswers_Answer_MatchConfidenceLevel {
  /** MATCH_CONFIDENCE_LEVEL_UNSPECIFIED - Not specified. */
  MATCH_CONFIDENCE_LEVEL_UNSPECIFIED = 0,
  /** LOW - Indicates that the confidence is low. */
  LOW = 1,
  /** MEDIUM - Indicates our confidence is medium. */
  MEDIUM = 2,
  /** HIGH - Indicates our confidence is high. */
  HIGH = 3,
  UNRECOGNIZED = -1,
}

export function knowledgeAnswers_Answer_MatchConfidenceLevelFromJSON(
  object: any,
): KnowledgeAnswers_Answer_MatchConfidenceLevel {
  switch (object) {
    case 0:
    case "MATCH_CONFIDENCE_LEVEL_UNSPECIFIED":
      return KnowledgeAnswers_Answer_MatchConfidenceLevel.MATCH_CONFIDENCE_LEVEL_UNSPECIFIED;
    case 1:
    case "LOW":
      return KnowledgeAnswers_Answer_MatchConfidenceLevel.LOW;
    case 2:
    case "MEDIUM":
      return KnowledgeAnswers_Answer_MatchConfidenceLevel.MEDIUM;
    case 3:
    case "HIGH":
      return KnowledgeAnswers_Answer_MatchConfidenceLevel.HIGH;
    case -1:
    case "UNRECOGNIZED":
    default:
      return KnowledgeAnswers_Answer_MatchConfidenceLevel.UNRECOGNIZED;
  }
}

export function knowledgeAnswers_Answer_MatchConfidenceLevelToJSON(
  object: KnowledgeAnswers_Answer_MatchConfidenceLevel,
): string {
  switch (object) {
    case KnowledgeAnswers_Answer_MatchConfidenceLevel.MATCH_CONFIDENCE_LEVEL_UNSPECIFIED:
      return "MATCH_CONFIDENCE_LEVEL_UNSPECIFIED";
    case KnowledgeAnswers_Answer_MatchConfidenceLevel.LOW:
      return "LOW";
    case KnowledgeAnswers_Answer_MatchConfidenceLevel.MEDIUM:
      return "MEDIUM";
    case KnowledgeAnswers_Answer_MatchConfidenceLevel.HIGH:
      return "HIGH";
    case KnowledgeAnswers_Answer_MatchConfidenceLevel.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * The top-level message sent by the client to the
 * [Sessions.StreamingDetectIntent][google.cloud.dialogflow.v2beta1.Sessions.StreamingDetectIntent]
 * method.
 *
 * Multiple request messages should be sent in order:
 *
 * 1.  The first message must contain
 * [session][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.session],
 *     [query_input][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.query_input]
 *     plus optionally
 *     [query_params][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.query_params].
 *     If the client wants to receive an audio response, it should also contain
 *     [output_audio_config][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.output_audio_config].
 *     The message must not contain
 *     [input_audio][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.input_audio].
 * 2.  If
 * [query_input][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.query_input]
 * was set to
 *     [query_input.audio_config][google.cloud.dialogflow.v2beta1.InputAudioConfig],
 *     all subsequent messages must contain
 *     [input_audio][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.input_audio]
 *     to continue with Speech recognition. If you decide to rather detect an
 *     intent from text input after you already started Speech recognition,
 *     please send a message with
 *     [query_input.text][google.cloud.dialogflow.v2beta1.QueryInput.text].
 *
 *     However, note that:
 *
 *     * Dialogflow will bill you for the audio duration so far.
 *     * Dialogflow discards all Speech recognition results in favor of the
 *       input text.
 *     * Dialogflow will use the language code from the first message.
 *
 * After you sent all input, you must half-close or abort the request stream.
 */
export interface StreamingDetectIntentRequest {
  /**
   * Required. The name of the session the query is sent to.
   * Supported formats:
   * - `projects/<Project ID>/agent/sessions/<Session ID>,
   * - `projects/<Project ID>/locations/<Location ID>/agent/sessions/<Session
   *   ID>`,
   * - `projects/<Project ID>/agent/environments/<Environment ID>/users/<User
   *   ID>/sessions/<Session ID>`,
   * - `projects/<Project ID>/locations/<Location
   *   ID>/agent/environments/<Environment ID>/users/<User ID>/sessions/<Session
   *   ID>`,
   *
   * If `Location ID` is not specified we assume default 'us' location. If
   * `Environment ID` is not specified, we assume default 'draft' environment.
   * If `User ID` is not specified, we are using "-". It's up to the API caller
   * to choose an appropriate `Session ID` and `User Id`. They can be a random
   * number or some type of user and session identifiers (preferably hashed).
   * The length of the `Session ID` and `User ID` must not exceed 36 characters.
   *
   * For more information, see the [API interactions
   * guide](https://cloud.google.com/dialogflow/docs/api-overview).
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/es/docs/agents-versions).
   */
  session: string;
  /** The parameters of this query. */
  queryParams:
    | QueryParameters
    | undefined;
  /**
   * Required. The input specification. It can be set to:
   *
   * 1. an audio config which instructs the speech recognizer how to process
   * the speech audio,
   *
   * 2. a conversational query in the form of text, or
   *
   * 3. an event that specifies which intent to trigger.
   */
  queryInput:
    | QueryInput
    | undefined;
  /**
   * DEPRECATED. Please use
   * [InputAudioConfig.single_utterance][google.cloud.dialogflow.v2beta1.InputAudioConfig.single_utterance]
   * instead. If `false` (default), recognition does not cease until the client
   * closes the stream. If `true`, the recognizer will detect a single spoken
   * utterance in input audio. Recognition ceases when it detects the audio's
   * voice has stopped or paused. In this case, once a detected intent is
   * received, the client should close the stream and start a new request with a
   * new stream as needed. This setting is ignored when `query_input` is a piece
   * of text or an event.
   *
   * @deprecated
   */
  singleUtterance: boolean;
  /**
   * Instructs the speech synthesizer how to generate the output
   * audio. If this field is not set and agent-level speech synthesizer is not
   * configured, no output audio is generated.
   */
  outputAudioConfig:
    | OutputAudioConfig
    | undefined;
  /**
   * Mask for
   * [output_audio_config][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.output_audio_config]
   * indicating which settings in this request-level config should override
   * speech synthesizer settings defined at agent-level.
   *
   * If unspecified or empty,
   * [output_audio_config][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.output_audio_config]
   * replaces the agent-level config in its entirety.
   */
  outputAudioConfigMask:
    | string[]
    | undefined;
  /**
   * The input audio content to be recognized. Must be sent if
   * `query_input` was set to a streaming input audio config. The complete audio
   * over all streaming messages must not exceed 1 minute.
   */
  inputAudio: Buffer;
  /** If true, `StreamingDetectIntentResponse.debugging_info` will get populated. */
  enableDebuggingInfo: boolean;
}

/**
 * Cloud conversation info for easier debugging.
 * It will get populated in `StreamingDetectIntentResponse` or
 * `StreamingAnalyzeContentResponse` when the flag `enable_debugging_info` is
 * set to true in corresponding requests.
 */
export interface CloudConversationDebuggingInfo {
  /** Number of input audio data chunks in streaming requests. */
  audioDataChunks: number;
  /**
   * Time offset of the end of speech utterance relative to the
   * beginning of the first audio chunk.
   */
  resultEndTimeOffset:
    | Duration
    | undefined;
  /** Duration of first audio chunk. */
  firstAudioDuration:
    | Duration
    | undefined;
  /** Whether client used single utterance mode. */
  singleUtterance: boolean;
  /**
   * Time offsets of the speech partial results relative to the beginning of
   * the stream.
   */
  speechPartialResultsEndTimes: Duration[];
  /**
   * Time offsets of the speech final results (is_final=true) relative to the
   * beginning of the stream.
   */
  speechFinalResultsEndTimes: Duration[];
  /** Total number of partial responses. */
  partialResponses: number;
  /**
   * Time offset of Speaker ID stream close time relative to the Speech stream
   * close time in milliseconds. Only meaningful for conversations involving
   * passive verification.
   */
  speakerIdPassiveLatencyMsOffset: number;
  /** Whether a barge-in event is triggered in this request. */
  bargeinEventTriggered: boolean;
  /** Whether speech uses single utterance mode. */
  speechSingleUtterance: boolean;
  /**
   * Time offsets of the DTMF partial results relative to the beginning of
   * the stream.
   */
  dtmfPartialResultsTimes: Duration[];
  /**
   * Time offsets of the DTMF final results relative to the beginning of
   * the stream.
   */
  dtmfFinalResultsTimes: Duration[];
  /**
   * Time offset of the end-of-single-utterance signal relative to the
   * beginning of the stream.
   */
  singleUtteranceEndTimeOffset:
    | Duration
    | undefined;
  /** No speech timeout settings for the stream. */
  noSpeechTimeout:
    | Duration
    | undefined;
  /** Speech endpointing timeout settings for the stream. */
  endpointingTimeout:
    | Duration
    | undefined;
  /** Whether the streaming terminates with an injected text query. */
  isInputText: boolean;
  /** Client half close time in terms of input audio duration. */
  clientHalfCloseTimeOffset:
    | Duration
    | undefined;
  /** Client half close time in terms of API streaming duration. */
  clientHalfCloseStreamingTimeOffset: Duration | undefined;
}

/**
 * The top-level message returned from the
 * `StreamingDetectIntent` method.
 *
 * Multiple response messages can be returned in order:
 *
 * 1.  If the `StreamingDetectIntentRequest.input_audio` field was
 *     set, the `recognition_result` field is populated for one
 *     or more messages.
 *     See the
 *     [StreamingRecognitionResult][google.cloud.dialogflow.v2beta1.StreamingRecognitionResult]
 *     message for details about the result message sequence.
 *
 * 2.  The next message contains `response_id`, `query_result`,
 *     `alternative_query_results` and optionally `webhook_status` if a WebHook
 *     was called.
 *
 * 3.  If `output_audio_config` was specified in the request or agent-level
 *     speech synthesizer is configured, all subsequent messages contain
 *     `output_audio` and `output_audio_config`.
 */
export interface StreamingDetectIntentResponse {
  /**
   * The unique identifier of the response. It can be used to
   * locate a response in the training example set or for reporting issues.
   */
  responseId: string;
  /** The result of speech recognition. */
  recognitionResult:
    | StreamingRecognitionResult
    | undefined;
  /**
   * The selected results of the conversational query or event processing.
   * See `alternative_query_results` for additional potential results.
   */
  queryResult:
    | QueryResult
    | undefined;
  /**
   * If Knowledge Connectors are enabled, there could be more than one result
   * returned for a given query or event, and this field will contain all
   * results except for the top one, which is captured in query_result. The
   * alternative results are ordered by decreasing
   * `QueryResult.intent_detection_confidence`. If Knowledge Connectors are
   * disabled, this field will be empty until multiple responses for regular
   * intents are supported, at which point those additional results will be
   * surfaced here.
   */
  alternativeQueryResults: QueryResult[];
  /** Specifies the status of the webhook request. */
  webhookStatus:
    | Status
    | undefined;
  /**
   * The audio data bytes encoded as specified in the request.
   * Note: The output audio is generated based on the values of default platform
   * text responses found in the `query_result.fulfillment_messages` field. If
   * multiple default text responses exist, they will be concatenated when
   * generating audio. If no default platform text responses exist, the
   * generated audio content will be empty.
   *
   * In some scenarios, multiple output audio fields may be present in the
   * response structure. In these cases, only the top-most-level audio output
   * has content.
   */
  outputAudio: Buffer;
  /** The config used by the speech synthesizer to generate the output audio. */
  outputAudioConfig:
    | OutputAudioConfig
    | undefined;
  /**
   * Debugging info that would get populated when
   * `StreamingDetectIntentRequest.enable_debugging_info` is set to true.
   */
  debuggingInfo: CloudConversationDebuggingInfo | undefined;
}

/**
 * Contains a speech recognition result corresponding to a portion of the audio
 * that is currently being processed or an indication that this is the end
 * of the single requested utterance.
 *
 * While end-user audio is being processed, Dialogflow sends a series of
 * results. Each result may contain a `transcript` value. A transcript
 * represents a portion of the utterance. While the recognizer is processing
 * audio, transcript values may be interim values or finalized values.
 * Once a transcript is finalized, the `is_final` value is set to true and
 * processing continues for the next transcript.
 *
 * If `StreamingDetectIntentRequest.query_input.audio_config.single_utterance`
 * was true, and the recognizer has completed processing audio,
 * the `message_type` value is set to `END_OF_SINGLE_UTTERANCE and the
 * following (last) result contains the last finalized transcript.
 *
 * The complete end-user utterance is determined by concatenating the
 * finalized transcript values received for the series of results.
 *
 * In the following example, single utterance is enabled. In the case where
 * single utterance is not enabled, result 7 would not occur.
 *
 * ```
 * Num | transcript              | message_type            | is_final
 * --- | ----------------------- | ----------------------- | --------
 * 1   | "tube"                  | TRANSCRIPT              | false
 * 2   | "to be a"               | TRANSCRIPT              | false
 * 3   | "to be"                 | TRANSCRIPT              | false
 * 4   | "to be or not to be"    | TRANSCRIPT              | true
 * 5   | "that's"                | TRANSCRIPT              | false
 * 6   | "that is                | TRANSCRIPT              | false
 * 7   | unset                   | END_OF_SINGLE_UTTERANCE | unset
 * 8   | " that is the question" | TRANSCRIPT              | true
 * ```
 * Concatenating the finalized transcripts with `is_final` set to true,
 * the complete utterance becomes "to be or not to be that is the question".
 */
export interface StreamingRecognitionResult {
  /** Type of the result message. */
  messageType: StreamingRecognitionResult_MessageType;
  /**
   * Transcript text representing the words that the user spoke.
   * Populated if and only if `message_type` = `TRANSCRIPT`.
   */
  transcript: string;
  /**
   * If `false`, the `StreamingRecognitionResult` represents an
   * interim result that may change. If `true`, the recognizer will not return
   * any further hypotheses about this piece of the audio. May only be populated
   * for `message_type` = `TRANSCRIPT`.
   */
  isFinal: boolean;
  /**
   * The Speech confidence between 0.0 and 1.0 for the current portion of audio.
   * A higher number indicates an estimated greater likelihood that the
   * recognized words are correct. The default of 0.0 is a sentinel value
   * indicating that confidence was not set.
   *
   * This field is typically only provided if `is_final` is true and you should
   * not rely on it being accurate or even set.
   */
  confidence: number;
  /**
   * An estimate of the likelihood that the speech recognizer will
   * not change its guess about this interim recognition result:
   *
   * * If the value is unspecified or 0.0, Dialogflow didn't compute the
   *   stability. In particular, Dialogflow will only provide stability for
   *   `TRANSCRIPT` results with `is_final = false`.
   * * Otherwise, the value is in (0.0, 1.0] where 0.0 means completely
   *   unstable and 1.0 means completely stable.
   */
  stability: number;
  /**
   * Word-specific information for the words recognized by Speech in
   * [transcript][google.cloud.dialogflow.v2beta1.StreamingRecognitionResult.transcript].
   * Populated if and only if `message_type` = `TRANSCRIPT` and
   * [InputAudioConfig.enable_word_info] is set.
   */
  speechWordInfo: SpeechWordInfo[];
  /**
   * Time offset of the end of this Speech recognition result relative to the
   * beginning of the audio. Only populated for `message_type` = `TRANSCRIPT`.
   */
  speechEndOffset:
    | Duration
    | undefined;
  /** Detected language code for the transcript. */
  languageCode: string;
  /** DTMF digits. Populated if and only if `message_type` = `DTMF_DIGITS`. */
  dtmfDigits: TelephonyDtmfEvents | undefined;
}

/** Type of the response message. */
export enum StreamingRecognitionResult_MessageType {
  /** MESSAGE_TYPE_UNSPECIFIED - Not specified. Should never be used. */
  MESSAGE_TYPE_UNSPECIFIED = 0,
  /** TRANSCRIPT - Message contains a (possibly partial) transcript. */
  TRANSCRIPT = 1,
  /** DTMF_DIGITS - Message contains DTMF digits. */
  DTMF_DIGITS = 3,
  /**
   * END_OF_SINGLE_UTTERANCE - This event indicates that the server has detected the end of the user's
   * speech utterance and expects no additional speech. Therefore, the server
   * will not process additional audio (although it may subsequently return
   * additional results). The client should stop sending additional audio
   * data, half-close the gRPC connection, and wait for any additional results
   * until the server closes the gRPC connection. This message is only sent if
   * `single_utterance` was set to `true`, and is not used otherwise.
   */
  END_OF_SINGLE_UTTERANCE = 2,
  /**
   * PARTIAL_DTMF_DIGITS - Message contains DTMF digits. Before a message with DTMF_DIGITS is sent,
   * a message with PARTIAL_DTMF_DIGITS may be sent with DTMF digits collected
   * up to the time of sending, which represents an intermediate result.
   */
  PARTIAL_DTMF_DIGITS = 4,
  UNRECOGNIZED = -1,
}

export function streamingRecognitionResult_MessageTypeFromJSON(object: any): StreamingRecognitionResult_MessageType {
  switch (object) {
    case 0:
    case "MESSAGE_TYPE_UNSPECIFIED":
      return StreamingRecognitionResult_MessageType.MESSAGE_TYPE_UNSPECIFIED;
    case 1:
    case "TRANSCRIPT":
      return StreamingRecognitionResult_MessageType.TRANSCRIPT;
    case 3:
    case "DTMF_DIGITS":
      return StreamingRecognitionResult_MessageType.DTMF_DIGITS;
    case 2:
    case "END_OF_SINGLE_UTTERANCE":
      return StreamingRecognitionResult_MessageType.END_OF_SINGLE_UTTERANCE;
    case 4:
    case "PARTIAL_DTMF_DIGITS":
      return StreamingRecognitionResult_MessageType.PARTIAL_DTMF_DIGITS;
    case -1:
    case "UNRECOGNIZED":
    default:
      return StreamingRecognitionResult_MessageType.UNRECOGNIZED;
  }
}

export function streamingRecognitionResult_MessageTypeToJSON(object: StreamingRecognitionResult_MessageType): string {
  switch (object) {
    case StreamingRecognitionResult_MessageType.MESSAGE_TYPE_UNSPECIFIED:
      return "MESSAGE_TYPE_UNSPECIFIED";
    case StreamingRecognitionResult_MessageType.TRANSCRIPT:
      return "TRANSCRIPT";
    case StreamingRecognitionResult_MessageType.DTMF_DIGITS:
      return "DTMF_DIGITS";
    case StreamingRecognitionResult_MessageType.END_OF_SINGLE_UTTERANCE:
      return "END_OF_SINGLE_UTTERANCE";
    case StreamingRecognitionResult_MessageType.PARTIAL_DTMF_DIGITS:
      return "PARTIAL_DTMF_DIGITS";
    case StreamingRecognitionResult_MessageType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Represents the natural language text to be processed. */
export interface TextInput {
  /**
   * Required. The UTF-8 encoded natural language text to be processed.
   * Text length must not exceed 256 characters for virtual agent interactions.
   */
  text: string;
  /**
   * Required. The language of this conversational query. See [Language
   * Support](https://cloud.google.com/dialogflow/docs/reference/language)
   * for a list of the currently supported language codes. Note that queries in
   * the same session do not necessarily need to specify the same language.
   */
  languageCode: string;
}

/**
 * Events allow for matching intents by event name instead of the natural
 * language input. For instance, input `<event: { name: "welcome_event",
 * parameters: { name: "Sam" } }>` can trigger a personalized welcome response.
 * The parameter `name` may be used by the agent in the response:
 * `"Hello #welcome_event.name! What can I do for you today?"`.
 */
export interface EventInput {
  /** Required. The unique identifier of the event. */
  name: string;
  /**
   * The collection of parameters associated with the event.
   *
   * Depending on your protocol or client library language, this is a
   * map, associative array, symbol table, dictionary, or JSON object
   * composed of a collection of (MapKey, MapValue) pairs:
   *
   * * MapKey type: string
   * * MapKey value: parameter name
   * * MapValue type: If parameter's entity type is a composite entity then use
   * map, otherwise, depending on the parameter value type, it could be one of
   * string, number, boolean, null, list or map.
   * * MapValue value: If parameter's entity type is a composite entity then use
   * map from composite entity property names to property values, otherwise,
   * use parameter value.
   */
  parameters:
    | { [key: string]: any }
    | undefined;
  /**
   * Required. The language of this query. See [Language
   * Support](https://cloud.google.com/dialogflow/docs/reference/language)
   * for a list of the currently supported language codes. Note that queries in
   * the same session do not necessarily need to specify the same language.
   *
   * This field is ignored when used in the context of a
   * [WebhookResponse.followup_event_input][google.cloud.dialogflow.v2beta1.WebhookResponse.followup_event_input]
   * field, because the language was already defined in the originating detect
   * intent request.
   */
  languageCode: string;
}

/** Configures the types of sentiment analysis to perform. */
export interface SentimentAnalysisRequestConfig {
  /**
   * Instructs the service to perform sentiment analysis on
   * `query_text`. If not provided, sentiment analysis is not performed on
   * `query_text`.
   */
  analyzeQueryTextSentiment: boolean;
}

/**
 * The result of sentiment analysis. Sentiment analysis inspects user input
 * and identifies the prevailing subjective opinion, especially to determine a
 * user's attitude as positive, negative, or neutral.
 * For [Participants.DetectIntent][], it needs to be configured in
 * [DetectIntentRequest.query_params][google.cloud.dialogflow.v2beta1.DetectIntentRequest.query_params].
 * For [Participants.StreamingDetectIntent][], it needs to be configured in
 * [StreamingDetectIntentRequest.query_params][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.query_params].
 * And for
 * [Participants.AnalyzeContent][google.cloud.dialogflow.v2beta1.Participants.AnalyzeContent]
 * and
 * [Participants.StreamingAnalyzeContent][google.cloud.dialogflow.v2beta1.Participants.StreamingAnalyzeContent],
 * it needs to be configured in
 * [ConversationProfile.human_agent_assistant_config][google.cloud.dialogflow.v2beta1.ConversationProfile.human_agent_assistant_config]
 */
export interface SentimentAnalysisResult {
  /** The sentiment analysis result for `query_text`. */
  queryTextSentiment: Sentiment | undefined;
}

/**
 * The sentiment, such as positive/negative feeling or association, for a unit
 * of analysis, such as the query text. See:
 * https://cloud.google.com/natural-language/docs/basics#interpreting_sentiment_analysis_values
 * for how to interpret the result.
 */
export interface Sentiment {
  /**
   * Sentiment score between -1.0 (negative sentiment) and 1.0 (positive
   * sentiment).
   */
  score: number;
  /**
   * A non-negative number in the [0, +inf) range, which represents the absolute
   * magnitude of sentiment, regardless of score (positive or negative).
   */
  magnitude: number;
}

function createBaseDetectIntentRequest(): DetectIntentRequest {
  return {
    session: "",
    queryParams: undefined,
    queryInput: undefined,
    outputAudioConfig: undefined,
    outputAudioConfigMask: undefined,
    inputAudio: Buffer.alloc(0),
  };
}

export const DetectIntentRequest: MessageFns<DetectIntentRequest> = {
  encode(message: DetectIntentRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.session !== "") {
      writer.uint32(10).string(message.session);
    }
    if (message.queryParams !== undefined) {
      QueryParameters.encode(message.queryParams, writer.uint32(18).fork()).join();
    }
    if (message.queryInput !== undefined) {
      QueryInput.encode(message.queryInput, writer.uint32(26).fork()).join();
    }
    if (message.outputAudioConfig !== undefined) {
      OutputAudioConfig.encode(message.outputAudioConfig, writer.uint32(34).fork()).join();
    }
    if (message.outputAudioConfigMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.outputAudioConfigMask), writer.uint32(58).fork()).join();
    }
    if (message.inputAudio.length !== 0) {
      writer.uint32(42).bytes(message.inputAudio);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DetectIntentRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDetectIntentRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.session = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.queryParams = QueryParameters.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.queryInput = QueryInput.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.outputAudioConfig = OutputAudioConfig.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.outputAudioConfigMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.inputAudio = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DetectIntentRequest {
    return {
      session: isSet(object.session) ? globalThis.String(object.session) : "",
      queryParams: isSet(object.queryParams) ? QueryParameters.fromJSON(object.queryParams) : undefined,
      queryInput: isSet(object.queryInput) ? QueryInput.fromJSON(object.queryInput) : undefined,
      outputAudioConfig: isSet(object.outputAudioConfig)
        ? OutputAudioConfig.fromJSON(object.outputAudioConfig)
        : undefined,
      outputAudioConfigMask: isSet(object.outputAudioConfigMask)
        ? FieldMask.unwrap(FieldMask.fromJSON(object.outputAudioConfigMask))
        : undefined,
      inputAudio: isSet(object.inputAudio) ? Buffer.from(bytesFromBase64(object.inputAudio)) : Buffer.alloc(0),
    };
  },

  toJSON(message: DetectIntentRequest): unknown {
    const obj: any = {};
    if (message.session !== "") {
      obj.session = message.session;
    }
    if (message.queryParams !== undefined) {
      obj.queryParams = QueryParameters.toJSON(message.queryParams);
    }
    if (message.queryInput !== undefined) {
      obj.queryInput = QueryInput.toJSON(message.queryInput);
    }
    if (message.outputAudioConfig !== undefined) {
      obj.outputAudioConfig = OutputAudioConfig.toJSON(message.outputAudioConfig);
    }
    if (message.outputAudioConfigMask !== undefined) {
      obj.outputAudioConfigMask = FieldMask.toJSON(FieldMask.wrap(message.outputAudioConfigMask));
    }
    if (message.inputAudio.length !== 0) {
      obj.inputAudio = base64FromBytes(message.inputAudio);
    }
    return obj;
  },

  create(base?: DeepPartial<DetectIntentRequest>): DetectIntentRequest {
    return DetectIntentRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DetectIntentRequest>): DetectIntentRequest {
    const message = createBaseDetectIntentRequest();
    message.session = object.session ?? "";
    message.queryParams = (object.queryParams !== undefined && object.queryParams !== null)
      ? QueryParameters.fromPartial(object.queryParams)
      : undefined;
    message.queryInput = (object.queryInput !== undefined && object.queryInput !== null)
      ? QueryInput.fromPartial(object.queryInput)
      : undefined;
    message.outputAudioConfig = (object.outputAudioConfig !== undefined && object.outputAudioConfig !== null)
      ? OutputAudioConfig.fromPartial(object.outputAudioConfig)
      : undefined;
    message.outputAudioConfigMask = object.outputAudioConfigMask ?? undefined;
    message.inputAudio = object.inputAudio ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseDetectIntentResponse(): DetectIntentResponse {
  return {
    responseId: "",
    queryResult: undefined,
    alternativeQueryResults: [],
    webhookStatus: undefined,
    outputAudio: Buffer.alloc(0),
    outputAudioConfig: undefined,
  };
}

export const DetectIntentResponse: MessageFns<DetectIntentResponse> = {
  encode(message: DetectIntentResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.responseId !== "") {
      writer.uint32(10).string(message.responseId);
    }
    if (message.queryResult !== undefined) {
      QueryResult.encode(message.queryResult, writer.uint32(18).fork()).join();
    }
    for (const v of message.alternativeQueryResults) {
      QueryResult.encode(v!, writer.uint32(42).fork()).join();
    }
    if (message.webhookStatus !== undefined) {
      Status.encode(message.webhookStatus, writer.uint32(26).fork()).join();
    }
    if (message.outputAudio.length !== 0) {
      writer.uint32(34).bytes(message.outputAudio);
    }
    if (message.outputAudioConfig !== undefined) {
      OutputAudioConfig.encode(message.outputAudioConfig, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DetectIntentResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDetectIntentResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.responseId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.queryResult = QueryResult.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.alternativeQueryResults.push(QueryResult.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.webhookStatus = Status.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.outputAudio = Buffer.from(reader.bytes());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.outputAudioConfig = OutputAudioConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DetectIntentResponse {
    return {
      responseId: isSet(object.responseId) ? globalThis.String(object.responseId) : "",
      queryResult: isSet(object.queryResult) ? QueryResult.fromJSON(object.queryResult) : undefined,
      alternativeQueryResults: globalThis.Array.isArray(object?.alternativeQueryResults)
        ? object.alternativeQueryResults.map((e: any) => QueryResult.fromJSON(e))
        : [],
      webhookStatus: isSet(object.webhookStatus) ? Status.fromJSON(object.webhookStatus) : undefined,
      outputAudio: isSet(object.outputAudio) ? Buffer.from(bytesFromBase64(object.outputAudio)) : Buffer.alloc(0),
      outputAudioConfig: isSet(object.outputAudioConfig)
        ? OutputAudioConfig.fromJSON(object.outputAudioConfig)
        : undefined,
    };
  },

  toJSON(message: DetectIntentResponse): unknown {
    const obj: any = {};
    if (message.responseId !== "") {
      obj.responseId = message.responseId;
    }
    if (message.queryResult !== undefined) {
      obj.queryResult = QueryResult.toJSON(message.queryResult);
    }
    if (message.alternativeQueryResults?.length) {
      obj.alternativeQueryResults = message.alternativeQueryResults.map((e) => QueryResult.toJSON(e));
    }
    if (message.webhookStatus !== undefined) {
      obj.webhookStatus = Status.toJSON(message.webhookStatus);
    }
    if (message.outputAudio.length !== 0) {
      obj.outputAudio = base64FromBytes(message.outputAudio);
    }
    if (message.outputAudioConfig !== undefined) {
      obj.outputAudioConfig = OutputAudioConfig.toJSON(message.outputAudioConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<DetectIntentResponse>): DetectIntentResponse {
    return DetectIntentResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DetectIntentResponse>): DetectIntentResponse {
    const message = createBaseDetectIntentResponse();
    message.responseId = object.responseId ?? "";
    message.queryResult = (object.queryResult !== undefined && object.queryResult !== null)
      ? QueryResult.fromPartial(object.queryResult)
      : undefined;
    message.alternativeQueryResults = object.alternativeQueryResults?.map((e) => QueryResult.fromPartial(e)) || [];
    message.webhookStatus = (object.webhookStatus !== undefined && object.webhookStatus !== null)
      ? Status.fromPartial(object.webhookStatus)
      : undefined;
    message.outputAudio = object.outputAudio ?? Buffer.alloc(0);
    message.outputAudioConfig = (object.outputAudioConfig !== undefined && object.outputAudioConfig !== null)
      ? OutputAudioConfig.fromPartial(object.outputAudioConfig)
      : undefined;
    return message;
  },
};

function createBaseQueryParameters(): QueryParameters {
  return {
    timeZone: "",
    geoLocation: undefined,
    contexts: [],
    resetContexts: false,
    sessionEntityTypes: [],
    payload: undefined,
    knowledgeBaseNames: [],
    sentimentAnalysisRequestConfig: undefined,
    subAgents: [],
    webhookHeaders: {},
    platform: "",
  };
}

export const QueryParameters: MessageFns<QueryParameters> = {
  encode(message: QueryParameters, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.timeZone !== "") {
      writer.uint32(10).string(message.timeZone);
    }
    if (message.geoLocation !== undefined) {
      LatLng.encode(message.geoLocation, writer.uint32(18).fork()).join();
    }
    for (const v of message.contexts) {
      Context.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.resetContexts !== false) {
      writer.uint32(32).bool(message.resetContexts);
    }
    for (const v of message.sessionEntityTypes) {
      SessionEntityType.encode(v!, writer.uint32(42).fork()).join();
    }
    if (message.payload !== undefined) {
      Struct.encode(Struct.wrap(message.payload), writer.uint32(50).fork()).join();
    }
    for (const v of message.knowledgeBaseNames) {
      writer.uint32(98).string(v!);
    }
    if (message.sentimentAnalysisRequestConfig !== undefined) {
      SentimentAnalysisRequestConfig.encode(message.sentimentAnalysisRequestConfig, writer.uint32(82).fork()).join();
    }
    for (const v of message.subAgents) {
      SubAgent.encode(v!, writer.uint32(106).fork()).join();
    }
    Object.entries(message.webhookHeaders).forEach(([key, value]) => {
      QueryParameters_WebhookHeadersEntry.encode({ key: key as any, value }, writer.uint32(114).fork()).join();
    });
    if (message.platform !== "") {
      writer.uint32(146).string(message.platform);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryParameters {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryParameters();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.timeZone = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.geoLocation = LatLng.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.contexts.push(Context.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.resetContexts = reader.bool();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.sessionEntityTypes.push(SessionEntityType.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.payload = Struct.unwrap(Struct.decode(reader, reader.uint32()));
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.knowledgeBaseNames.push(reader.string());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.sentimentAnalysisRequestConfig = SentimentAnalysisRequestConfig.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.subAgents.push(SubAgent.decode(reader, reader.uint32()));
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          const entry14 = QueryParameters_WebhookHeadersEntry.decode(reader, reader.uint32());
          if (entry14.value !== undefined) {
            message.webhookHeaders[entry14.key] = entry14.value;
          }
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.platform = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryParameters {
    return {
      timeZone: isSet(object.timeZone) ? globalThis.String(object.timeZone) : "",
      geoLocation: isSet(object.geoLocation) ? LatLng.fromJSON(object.geoLocation) : undefined,
      contexts: globalThis.Array.isArray(object?.contexts) ? object.contexts.map((e: any) => Context.fromJSON(e)) : [],
      resetContexts: isSet(object.resetContexts) ? globalThis.Boolean(object.resetContexts) : false,
      sessionEntityTypes: globalThis.Array.isArray(object?.sessionEntityTypes)
        ? object.sessionEntityTypes.map((e: any) => SessionEntityType.fromJSON(e))
        : [],
      payload: isObject(object.payload) ? object.payload : undefined,
      knowledgeBaseNames: globalThis.Array.isArray(object?.knowledgeBaseNames)
        ? object.knowledgeBaseNames.map((e: any) => globalThis.String(e))
        : [],
      sentimentAnalysisRequestConfig: isSet(object.sentimentAnalysisRequestConfig)
        ? SentimentAnalysisRequestConfig.fromJSON(object.sentimentAnalysisRequestConfig)
        : undefined,
      subAgents: globalThis.Array.isArray(object?.subAgents)
        ? object.subAgents.map((e: any) => SubAgent.fromJSON(e))
        : [],
      webhookHeaders: isObject(object.webhookHeaders)
        ? Object.entries(object.webhookHeaders).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      platform: isSet(object.platform) ? globalThis.String(object.platform) : "",
    };
  },

  toJSON(message: QueryParameters): unknown {
    const obj: any = {};
    if (message.timeZone !== "") {
      obj.timeZone = message.timeZone;
    }
    if (message.geoLocation !== undefined) {
      obj.geoLocation = LatLng.toJSON(message.geoLocation);
    }
    if (message.contexts?.length) {
      obj.contexts = message.contexts.map((e) => Context.toJSON(e));
    }
    if (message.resetContexts !== false) {
      obj.resetContexts = message.resetContexts;
    }
    if (message.sessionEntityTypes?.length) {
      obj.sessionEntityTypes = message.sessionEntityTypes.map((e) => SessionEntityType.toJSON(e));
    }
    if (message.payload !== undefined) {
      obj.payload = message.payload;
    }
    if (message.knowledgeBaseNames?.length) {
      obj.knowledgeBaseNames = message.knowledgeBaseNames;
    }
    if (message.sentimentAnalysisRequestConfig !== undefined) {
      obj.sentimentAnalysisRequestConfig = SentimentAnalysisRequestConfig.toJSON(
        message.sentimentAnalysisRequestConfig,
      );
    }
    if (message.subAgents?.length) {
      obj.subAgents = message.subAgents.map((e) => SubAgent.toJSON(e));
    }
    if (message.webhookHeaders) {
      const entries = Object.entries(message.webhookHeaders);
      if (entries.length > 0) {
        obj.webhookHeaders = {};
        entries.forEach(([k, v]) => {
          obj.webhookHeaders[k] = v;
        });
      }
    }
    if (message.platform !== "") {
      obj.platform = message.platform;
    }
    return obj;
  },

  create(base?: DeepPartial<QueryParameters>): QueryParameters {
    return QueryParameters.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<QueryParameters>): QueryParameters {
    const message = createBaseQueryParameters();
    message.timeZone = object.timeZone ?? "";
    message.geoLocation = (object.geoLocation !== undefined && object.geoLocation !== null)
      ? LatLng.fromPartial(object.geoLocation)
      : undefined;
    message.contexts = object.contexts?.map((e) => Context.fromPartial(e)) || [];
    message.resetContexts = object.resetContexts ?? false;
    message.sessionEntityTypes = object.sessionEntityTypes?.map((e) => SessionEntityType.fromPartial(e)) || [];
    message.payload = object.payload ?? undefined;
    message.knowledgeBaseNames = object.knowledgeBaseNames?.map((e) => e) || [];
    message.sentimentAnalysisRequestConfig =
      (object.sentimentAnalysisRequestConfig !== undefined && object.sentimentAnalysisRequestConfig !== null)
        ? SentimentAnalysisRequestConfig.fromPartial(object.sentimentAnalysisRequestConfig)
        : undefined;
    message.subAgents = object.subAgents?.map((e) => SubAgent.fromPartial(e)) || [];
    message.webhookHeaders = Object.entries(object.webhookHeaders ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.platform = object.platform ?? "";
    return message;
  },
};

function createBaseQueryParameters_WebhookHeadersEntry(): QueryParameters_WebhookHeadersEntry {
  return { key: "", value: "" };
}

export const QueryParameters_WebhookHeadersEntry: MessageFns<QueryParameters_WebhookHeadersEntry> = {
  encode(message: QueryParameters_WebhookHeadersEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryParameters_WebhookHeadersEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryParameters_WebhookHeadersEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryParameters_WebhookHeadersEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: QueryParameters_WebhookHeadersEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<QueryParameters_WebhookHeadersEntry>): QueryParameters_WebhookHeadersEntry {
    return QueryParameters_WebhookHeadersEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<QueryParameters_WebhookHeadersEntry>): QueryParameters_WebhookHeadersEntry {
    const message = createBaseQueryParameters_WebhookHeadersEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseQueryInput(): QueryInput {
  return { audioConfig: undefined, text: undefined, event: undefined, dtmf: undefined };
}

export const QueryInput: MessageFns<QueryInput> = {
  encode(message: QueryInput, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.audioConfig !== undefined) {
      InputAudioConfig.encode(message.audioConfig, writer.uint32(10).fork()).join();
    }
    if (message.text !== undefined) {
      TextInput.encode(message.text, writer.uint32(18).fork()).join();
    }
    if (message.event !== undefined) {
      EventInput.encode(message.event, writer.uint32(26).fork()).join();
    }
    if (message.dtmf !== undefined) {
      TelephonyDtmfEvents.encode(message.dtmf, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryInput {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryInput();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.audioConfig = InputAudioConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.text = TextInput.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.event = EventInput.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.dtmf = TelephonyDtmfEvents.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryInput {
    return {
      audioConfig: isSet(object.audioConfig) ? InputAudioConfig.fromJSON(object.audioConfig) : undefined,
      text: isSet(object.text) ? TextInput.fromJSON(object.text) : undefined,
      event: isSet(object.event) ? EventInput.fromJSON(object.event) : undefined,
      dtmf: isSet(object.dtmf) ? TelephonyDtmfEvents.fromJSON(object.dtmf) : undefined,
    };
  },

  toJSON(message: QueryInput): unknown {
    const obj: any = {};
    if (message.audioConfig !== undefined) {
      obj.audioConfig = InputAudioConfig.toJSON(message.audioConfig);
    }
    if (message.text !== undefined) {
      obj.text = TextInput.toJSON(message.text);
    }
    if (message.event !== undefined) {
      obj.event = EventInput.toJSON(message.event);
    }
    if (message.dtmf !== undefined) {
      obj.dtmf = TelephonyDtmfEvents.toJSON(message.dtmf);
    }
    return obj;
  },

  create(base?: DeepPartial<QueryInput>): QueryInput {
    return QueryInput.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<QueryInput>): QueryInput {
    const message = createBaseQueryInput();
    message.audioConfig = (object.audioConfig !== undefined && object.audioConfig !== null)
      ? InputAudioConfig.fromPartial(object.audioConfig)
      : undefined;
    message.text = (object.text !== undefined && object.text !== null) ? TextInput.fromPartial(object.text) : undefined;
    message.event = (object.event !== undefined && object.event !== null)
      ? EventInput.fromPartial(object.event)
      : undefined;
    message.dtmf = (object.dtmf !== undefined && object.dtmf !== null)
      ? TelephonyDtmfEvents.fromPartial(object.dtmf)
      : undefined;
    return message;
  },
};

function createBaseQueryResult(): QueryResult {
  return {
    queryText: "",
    languageCode: "",
    speechRecognitionConfidence: 0,
    action: "",
    parameters: undefined,
    allRequiredParamsPresent: false,
    cancelsSlotFilling: false,
    fulfillmentText: "",
    fulfillmentMessages: [],
    webhookSource: "",
    webhookPayload: undefined,
    outputContexts: [],
    intent: undefined,
    intentDetectionConfidence: 0,
    diagnosticInfo: undefined,
    sentimentAnalysisResult: undefined,
    knowledgeAnswers: undefined,
  };
}

export const QueryResult: MessageFns<QueryResult> = {
  encode(message: QueryResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.queryText !== "") {
      writer.uint32(10).string(message.queryText);
    }
    if (message.languageCode !== "") {
      writer.uint32(122).string(message.languageCode);
    }
    if (message.speechRecognitionConfidence !== 0) {
      writer.uint32(21).float(message.speechRecognitionConfidence);
    }
    if (message.action !== "") {
      writer.uint32(26).string(message.action);
    }
    if (message.parameters !== undefined) {
      Struct.encode(Struct.wrap(message.parameters), writer.uint32(34).fork()).join();
    }
    if (message.allRequiredParamsPresent !== false) {
      writer.uint32(40).bool(message.allRequiredParamsPresent);
    }
    if (message.cancelsSlotFilling !== false) {
      writer.uint32(168).bool(message.cancelsSlotFilling);
    }
    if (message.fulfillmentText !== "") {
      writer.uint32(50).string(message.fulfillmentText);
    }
    for (const v of message.fulfillmentMessages) {
      Intent_Message.encode(v!, writer.uint32(58).fork()).join();
    }
    if (message.webhookSource !== "") {
      writer.uint32(66).string(message.webhookSource);
    }
    if (message.webhookPayload !== undefined) {
      Struct.encode(Struct.wrap(message.webhookPayload), writer.uint32(74).fork()).join();
    }
    for (const v of message.outputContexts) {
      Context.encode(v!, writer.uint32(82).fork()).join();
    }
    if (message.intent !== undefined) {
      Intent.encode(message.intent, writer.uint32(90).fork()).join();
    }
    if (message.intentDetectionConfidence !== 0) {
      writer.uint32(101).float(message.intentDetectionConfidence);
    }
    if (message.diagnosticInfo !== undefined) {
      Struct.encode(Struct.wrap(message.diagnosticInfo), writer.uint32(114).fork()).join();
    }
    if (message.sentimentAnalysisResult !== undefined) {
      SentimentAnalysisResult.encode(message.sentimentAnalysisResult, writer.uint32(138).fork()).join();
    }
    if (message.knowledgeAnswers !== undefined) {
      KnowledgeAnswers.encode(message.knowledgeAnswers, writer.uint32(146).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): QueryResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseQueryResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.queryText = reader.string();
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.languageCode = reader.string();
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.speechRecognitionConfidence = reader.float();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.action = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.parameters = Struct.unwrap(Struct.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.allRequiredParamsPresent = reader.bool();
          continue;
        case 21:
          if (tag !== 168) {
            break;
          }

          message.cancelsSlotFilling = reader.bool();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.fulfillmentText = reader.string();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.fulfillmentMessages.push(Intent_Message.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.webhookSource = reader.string();
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.webhookPayload = Struct.unwrap(Struct.decode(reader, reader.uint32()));
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.outputContexts.push(Context.decode(reader, reader.uint32()));
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.intent = Intent.decode(reader, reader.uint32());
          continue;
        case 12:
          if (tag !== 101) {
            break;
          }

          message.intentDetectionConfidence = reader.float();
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.diagnosticInfo = Struct.unwrap(Struct.decode(reader, reader.uint32()));
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.sentimentAnalysisResult = SentimentAnalysisResult.decode(reader, reader.uint32());
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.knowledgeAnswers = KnowledgeAnswers.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): QueryResult {
    return {
      queryText: isSet(object.queryText) ? globalThis.String(object.queryText) : "",
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
      speechRecognitionConfidence: isSet(object.speechRecognitionConfidence)
        ? globalThis.Number(object.speechRecognitionConfidence)
        : 0,
      action: isSet(object.action) ? globalThis.String(object.action) : "",
      parameters: isObject(object.parameters) ? object.parameters : undefined,
      allRequiredParamsPresent: isSet(object.allRequiredParamsPresent)
        ? globalThis.Boolean(object.allRequiredParamsPresent)
        : false,
      cancelsSlotFilling: isSet(object.cancelsSlotFilling) ? globalThis.Boolean(object.cancelsSlotFilling) : false,
      fulfillmentText: isSet(object.fulfillmentText) ? globalThis.String(object.fulfillmentText) : "",
      fulfillmentMessages: globalThis.Array.isArray(object?.fulfillmentMessages)
        ? object.fulfillmentMessages.map((e: any) => Intent_Message.fromJSON(e))
        : [],
      webhookSource: isSet(object.webhookSource) ? globalThis.String(object.webhookSource) : "",
      webhookPayload: isObject(object.webhookPayload) ? object.webhookPayload : undefined,
      outputContexts: globalThis.Array.isArray(object?.outputContexts)
        ? object.outputContexts.map((e: any) => Context.fromJSON(e))
        : [],
      intent: isSet(object.intent) ? Intent.fromJSON(object.intent) : undefined,
      intentDetectionConfidence: isSet(object.intentDetectionConfidence)
        ? globalThis.Number(object.intentDetectionConfidence)
        : 0,
      diagnosticInfo: isObject(object.diagnosticInfo) ? object.diagnosticInfo : undefined,
      sentimentAnalysisResult: isSet(object.sentimentAnalysisResult)
        ? SentimentAnalysisResult.fromJSON(object.sentimentAnalysisResult)
        : undefined,
      knowledgeAnswers: isSet(object.knowledgeAnswers) ? KnowledgeAnswers.fromJSON(object.knowledgeAnswers) : undefined,
    };
  },

  toJSON(message: QueryResult): unknown {
    const obj: any = {};
    if (message.queryText !== "") {
      obj.queryText = message.queryText;
    }
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    if (message.speechRecognitionConfidence !== 0) {
      obj.speechRecognitionConfidence = message.speechRecognitionConfidence;
    }
    if (message.action !== "") {
      obj.action = message.action;
    }
    if (message.parameters !== undefined) {
      obj.parameters = message.parameters;
    }
    if (message.allRequiredParamsPresent !== false) {
      obj.allRequiredParamsPresent = message.allRequiredParamsPresent;
    }
    if (message.cancelsSlotFilling !== false) {
      obj.cancelsSlotFilling = message.cancelsSlotFilling;
    }
    if (message.fulfillmentText !== "") {
      obj.fulfillmentText = message.fulfillmentText;
    }
    if (message.fulfillmentMessages?.length) {
      obj.fulfillmentMessages = message.fulfillmentMessages.map((e) => Intent_Message.toJSON(e));
    }
    if (message.webhookSource !== "") {
      obj.webhookSource = message.webhookSource;
    }
    if (message.webhookPayload !== undefined) {
      obj.webhookPayload = message.webhookPayload;
    }
    if (message.outputContexts?.length) {
      obj.outputContexts = message.outputContexts.map((e) => Context.toJSON(e));
    }
    if (message.intent !== undefined) {
      obj.intent = Intent.toJSON(message.intent);
    }
    if (message.intentDetectionConfidence !== 0) {
      obj.intentDetectionConfidence = message.intentDetectionConfidence;
    }
    if (message.diagnosticInfo !== undefined) {
      obj.diagnosticInfo = message.diagnosticInfo;
    }
    if (message.sentimentAnalysisResult !== undefined) {
      obj.sentimentAnalysisResult = SentimentAnalysisResult.toJSON(message.sentimentAnalysisResult);
    }
    if (message.knowledgeAnswers !== undefined) {
      obj.knowledgeAnswers = KnowledgeAnswers.toJSON(message.knowledgeAnswers);
    }
    return obj;
  },

  create(base?: DeepPartial<QueryResult>): QueryResult {
    return QueryResult.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<QueryResult>): QueryResult {
    const message = createBaseQueryResult();
    message.queryText = object.queryText ?? "";
    message.languageCode = object.languageCode ?? "";
    message.speechRecognitionConfidence = object.speechRecognitionConfidence ?? 0;
    message.action = object.action ?? "";
    message.parameters = object.parameters ?? undefined;
    message.allRequiredParamsPresent = object.allRequiredParamsPresent ?? false;
    message.cancelsSlotFilling = object.cancelsSlotFilling ?? false;
    message.fulfillmentText = object.fulfillmentText ?? "";
    message.fulfillmentMessages = object.fulfillmentMessages?.map((e) => Intent_Message.fromPartial(e)) || [];
    message.webhookSource = object.webhookSource ?? "";
    message.webhookPayload = object.webhookPayload ?? undefined;
    message.outputContexts = object.outputContexts?.map((e) => Context.fromPartial(e)) || [];
    message.intent = (object.intent !== undefined && object.intent !== null)
      ? Intent.fromPartial(object.intent)
      : undefined;
    message.intentDetectionConfidence = object.intentDetectionConfidence ?? 0;
    message.diagnosticInfo = object.diagnosticInfo ?? undefined;
    message.sentimentAnalysisResult =
      (object.sentimentAnalysisResult !== undefined && object.sentimentAnalysisResult !== null)
        ? SentimentAnalysisResult.fromPartial(object.sentimentAnalysisResult)
        : undefined;
    message.knowledgeAnswers = (object.knowledgeAnswers !== undefined && object.knowledgeAnswers !== null)
      ? KnowledgeAnswers.fromPartial(object.knowledgeAnswers)
      : undefined;
    return message;
  },
};

function createBaseKnowledgeAnswers(): KnowledgeAnswers {
  return { answers: [] };
}

export const KnowledgeAnswers: MessageFns<KnowledgeAnswers> = {
  encode(message: KnowledgeAnswers, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.answers) {
      KnowledgeAnswers_Answer.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KnowledgeAnswers {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKnowledgeAnswers();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.answers.push(KnowledgeAnswers_Answer.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KnowledgeAnswers {
    return {
      answers: globalThis.Array.isArray(object?.answers)
        ? object.answers.map((e: any) => KnowledgeAnswers_Answer.fromJSON(e))
        : [],
    };
  },

  toJSON(message: KnowledgeAnswers): unknown {
    const obj: any = {};
    if (message.answers?.length) {
      obj.answers = message.answers.map((e) => KnowledgeAnswers_Answer.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<KnowledgeAnswers>): KnowledgeAnswers {
    return KnowledgeAnswers.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<KnowledgeAnswers>): KnowledgeAnswers {
    const message = createBaseKnowledgeAnswers();
    message.answers = object.answers?.map((e) => KnowledgeAnswers_Answer.fromPartial(e)) || [];
    return message;
  },
};

function createBaseKnowledgeAnswers_Answer(): KnowledgeAnswers_Answer {
  return { source: "", faqQuestion: "", answer: "", matchConfidenceLevel: 0, matchConfidence: 0 };
}

export const KnowledgeAnswers_Answer: MessageFns<KnowledgeAnswers_Answer> = {
  encode(message: KnowledgeAnswers_Answer, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.source !== "") {
      writer.uint32(10).string(message.source);
    }
    if (message.faqQuestion !== "") {
      writer.uint32(18).string(message.faqQuestion);
    }
    if (message.answer !== "") {
      writer.uint32(26).string(message.answer);
    }
    if (message.matchConfidenceLevel !== 0) {
      writer.uint32(32).int32(message.matchConfidenceLevel);
    }
    if (message.matchConfidence !== 0) {
      writer.uint32(45).float(message.matchConfidence);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KnowledgeAnswers_Answer {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKnowledgeAnswers_Answer();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.source = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.faqQuestion = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.answer = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.matchConfidenceLevel = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 45) {
            break;
          }

          message.matchConfidence = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KnowledgeAnswers_Answer {
    return {
      source: isSet(object.source) ? globalThis.String(object.source) : "",
      faqQuestion: isSet(object.faqQuestion) ? globalThis.String(object.faqQuestion) : "",
      answer: isSet(object.answer) ? globalThis.String(object.answer) : "",
      matchConfidenceLevel: isSet(object.matchConfidenceLevel)
        ? knowledgeAnswers_Answer_MatchConfidenceLevelFromJSON(object.matchConfidenceLevel)
        : 0,
      matchConfidence: isSet(object.matchConfidence) ? globalThis.Number(object.matchConfidence) : 0,
    };
  },

  toJSON(message: KnowledgeAnswers_Answer): unknown {
    const obj: any = {};
    if (message.source !== "") {
      obj.source = message.source;
    }
    if (message.faqQuestion !== "") {
      obj.faqQuestion = message.faqQuestion;
    }
    if (message.answer !== "") {
      obj.answer = message.answer;
    }
    if (message.matchConfidenceLevel !== 0) {
      obj.matchConfidenceLevel = knowledgeAnswers_Answer_MatchConfidenceLevelToJSON(message.matchConfidenceLevel);
    }
    if (message.matchConfidence !== 0) {
      obj.matchConfidence = message.matchConfidence;
    }
    return obj;
  },

  create(base?: DeepPartial<KnowledgeAnswers_Answer>): KnowledgeAnswers_Answer {
    return KnowledgeAnswers_Answer.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<KnowledgeAnswers_Answer>): KnowledgeAnswers_Answer {
    const message = createBaseKnowledgeAnswers_Answer();
    message.source = object.source ?? "";
    message.faqQuestion = object.faqQuestion ?? "";
    message.answer = object.answer ?? "";
    message.matchConfidenceLevel = object.matchConfidenceLevel ?? 0;
    message.matchConfidence = object.matchConfidence ?? 0;
    return message;
  },
};

function createBaseStreamingDetectIntentRequest(): StreamingDetectIntentRequest {
  return {
    session: "",
    queryParams: undefined,
    queryInput: undefined,
    singleUtterance: false,
    outputAudioConfig: undefined,
    outputAudioConfigMask: undefined,
    inputAudio: Buffer.alloc(0),
    enableDebuggingInfo: false,
  };
}

export const StreamingDetectIntentRequest: MessageFns<StreamingDetectIntentRequest> = {
  encode(message: StreamingDetectIntentRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.session !== "") {
      writer.uint32(10).string(message.session);
    }
    if (message.queryParams !== undefined) {
      QueryParameters.encode(message.queryParams, writer.uint32(18).fork()).join();
    }
    if (message.queryInput !== undefined) {
      QueryInput.encode(message.queryInput, writer.uint32(26).fork()).join();
    }
    if (message.singleUtterance !== false) {
      writer.uint32(32).bool(message.singleUtterance);
    }
    if (message.outputAudioConfig !== undefined) {
      OutputAudioConfig.encode(message.outputAudioConfig, writer.uint32(42).fork()).join();
    }
    if (message.outputAudioConfigMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.outputAudioConfigMask), writer.uint32(58).fork()).join();
    }
    if (message.inputAudio.length !== 0) {
      writer.uint32(50).bytes(message.inputAudio);
    }
    if (message.enableDebuggingInfo !== false) {
      writer.uint32(64).bool(message.enableDebuggingInfo);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingDetectIntentRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingDetectIntentRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.session = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.queryParams = QueryParameters.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.queryInput = QueryInput.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.singleUtterance = reader.bool();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.outputAudioConfig = OutputAudioConfig.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.outputAudioConfigMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.inputAudio = Buffer.from(reader.bytes());
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.enableDebuggingInfo = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingDetectIntentRequest {
    return {
      session: isSet(object.session) ? globalThis.String(object.session) : "",
      queryParams: isSet(object.queryParams) ? QueryParameters.fromJSON(object.queryParams) : undefined,
      queryInput: isSet(object.queryInput) ? QueryInput.fromJSON(object.queryInput) : undefined,
      singleUtterance: isSet(object.singleUtterance) ? globalThis.Boolean(object.singleUtterance) : false,
      outputAudioConfig: isSet(object.outputAudioConfig)
        ? OutputAudioConfig.fromJSON(object.outputAudioConfig)
        : undefined,
      outputAudioConfigMask: isSet(object.outputAudioConfigMask)
        ? FieldMask.unwrap(FieldMask.fromJSON(object.outputAudioConfigMask))
        : undefined,
      inputAudio: isSet(object.inputAudio) ? Buffer.from(bytesFromBase64(object.inputAudio)) : Buffer.alloc(0),
      enableDebuggingInfo: isSet(object.enableDebuggingInfo) ? globalThis.Boolean(object.enableDebuggingInfo) : false,
    };
  },

  toJSON(message: StreamingDetectIntentRequest): unknown {
    const obj: any = {};
    if (message.session !== "") {
      obj.session = message.session;
    }
    if (message.queryParams !== undefined) {
      obj.queryParams = QueryParameters.toJSON(message.queryParams);
    }
    if (message.queryInput !== undefined) {
      obj.queryInput = QueryInput.toJSON(message.queryInput);
    }
    if (message.singleUtterance !== false) {
      obj.singleUtterance = message.singleUtterance;
    }
    if (message.outputAudioConfig !== undefined) {
      obj.outputAudioConfig = OutputAudioConfig.toJSON(message.outputAudioConfig);
    }
    if (message.outputAudioConfigMask !== undefined) {
      obj.outputAudioConfigMask = FieldMask.toJSON(FieldMask.wrap(message.outputAudioConfigMask));
    }
    if (message.inputAudio.length !== 0) {
      obj.inputAudio = base64FromBytes(message.inputAudio);
    }
    if (message.enableDebuggingInfo !== false) {
      obj.enableDebuggingInfo = message.enableDebuggingInfo;
    }
    return obj;
  },

  create(base?: DeepPartial<StreamingDetectIntentRequest>): StreamingDetectIntentRequest {
    return StreamingDetectIntentRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamingDetectIntentRequest>): StreamingDetectIntentRequest {
    const message = createBaseStreamingDetectIntentRequest();
    message.session = object.session ?? "";
    message.queryParams = (object.queryParams !== undefined && object.queryParams !== null)
      ? QueryParameters.fromPartial(object.queryParams)
      : undefined;
    message.queryInput = (object.queryInput !== undefined && object.queryInput !== null)
      ? QueryInput.fromPartial(object.queryInput)
      : undefined;
    message.singleUtterance = object.singleUtterance ?? false;
    message.outputAudioConfig = (object.outputAudioConfig !== undefined && object.outputAudioConfig !== null)
      ? OutputAudioConfig.fromPartial(object.outputAudioConfig)
      : undefined;
    message.outputAudioConfigMask = object.outputAudioConfigMask ?? undefined;
    message.inputAudio = object.inputAudio ?? Buffer.alloc(0);
    message.enableDebuggingInfo = object.enableDebuggingInfo ?? false;
    return message;
  },
};

function createBaseCloudConversationDebuggingInfo(): CloudConversationDebuggingInfo {
  return {
    audioDataChunks: 0,
    resultEndTimeOffset: undefined,
    firstAudioDuration: undefined,
    singleUtterance: false,
    speechPartialResultsEndTimes: [],
    speechFinalResultsEndTimes: [],
    partialResponses: 0,
    speakerIdPassiveLatencyMsOffset: 0,
    bargeinEventTriggered: false,
    speechSingleUtterance: false,
    dtmfPartialResultsTimes: [],
    dtmfFinalResultsTimes: [],
    singleUtteranceEndTimeOffset: undefined,
    noSpeechTimeout: undefined,
    endpointingTimeout: undefined,
    isInputText: false,
    clientHalfCloseTimeOffset: undefined,
    clientHalfCloseStreamingTimeOffset: undefined,
  };
}

export const CloudConversationDebuggingInfo: MessageFns<CloudConversationDebuggingInfo> = {
  encode(message: CloudConversationDebuggingInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.audioDataChunks !== 0) {
      writer.uint32(8).int32(message.audioDataChunks);
    }
    if (message.resultEndTimeOffset !== undefined) {
      Duration.encode(message.resultEndTimeOffset, writer.uint32(18).fork()).join();
    }
    if (message.firstAudioDuration !== undefined) {
      Duration.encode(message.firstAudioDuration, writer.uint32(26).fork()).join();
    }
    if (message.singleUtterance !== false) {
      writer.uint32(40).bool(message.singleUtterance);
    }
    for (const v of message.speechPartialResultsEndTimes) {
      Duration.encode(v!, writer.uint32(50).fork()).join();
    }
    for (const v of message.speechFinalResultsEndTimes) {
      Duration.encode(v!, writer.uint32(58).fork()).join();
    }
    if (message.partialResponses !== 0) {
      writer.uint32(64).int32(message.partialResponses);
    }
    if (message.speakerIdPassiveLatencyMsOffset !== 0) {
      writer.uint32(72).int32(message.speakerIdPassiveLatencyMsOffset);
    }
    if (message.bargeinEventTriggered !== false) {
      writer.uint32(80).bool(message.bargeinEventTriggered);
    }
    if (message.speechSingleUtterance !== false) {
      writer.uint32(88).bool(message.speechSingleUtterance);
    }
    for (const v of message.dtmfPartialResultsTimes) {
      Duration.encode(v!, writer.uint32(98).fork()).join();
    }
    for (const v of message.dtmfFinalResultsTimes) {
      Duration.encode(v!, writer.uint32(106).fork()).join();
    }
    if (message.singleUtteranceEndTimeOffset !== undefined) {
      Duration.encode(message.singleUtteranceEndTimeOffset, writer.uint32(114).fork()).join();
    }
    if (message.noSpeechTimeout !== undefined) {
      Duration.encode(message.noSpeechTimeout, writer.uint32(122).fork()).join();
    }
    if (message.endpointingTimeout !== undefined) {
      Duration.encode(message.endpointingTimeout, writer.uint32(154).fork()).join();
    }
    if (message.isInputText !== false) {
      writer.uint32(128).bool(message.isInputText);
    }
    if (message.clientHalfCloseTimeOffset !== undefined) {
      Duration.encode(message.clientHalfCloseTimeOffset, writer.uint32(138).fork()).join();
    }
    if (message.clientHalfCloseStreamingTimeOffset !== undefined) {
      Duration.encode(message.clientHalfCloseStreamingTimeOffset, writer.uint32(146).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CloudConversationDebuggingInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCloudConversationDebuggingInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.audioDataChunks = reader.int32();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.resultEndTimeOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.firstAudioDuration = Duration.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.singleUtterance = reader.bool();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.speechPartialResultsEndTimes.push(Duration.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.speechFinalResultsEndTimes.push(Duration.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.partialResponses = reader.int32();
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.speakerIdPassiveLatencyMsOffset = reader.int32();
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.bargeinEventTriggered = reader.bool();
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.speechSingleUtterance = reader.bool();
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.dtmfPartialResultsTimes.push(Duration.decode(reader, reader.uint32()));
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.dtmfFinalResultsTimes.push(Duration.decode(reader, reader.uint32()));
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.singleUtteranceEndTimeOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.noSpeechTimeout = Duration.decode(reader, reader.uint32());
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.endpointingTimeout = Duration.decode(reader, reader.uint32());
          continue;
        case 16:
          if (tag !== 128) {
            break;
          }

          message.isInputText = reader.bool();
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.clientHalfCloseTimeOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.clientHalfCloseStreamingTimeOffset = Duration.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CloudConversationDebuggingInfo {
    return {
      audioDataChunks: isSet(object.audioDataChunks) ? globalThis.Number(object.audioDataChunks) : 0,
      resultEndTimeOffset: isSet(object.resultEndTimeOffset)
        ? Duration.fromJSON(object.resultEndTimeOffset)
        : undefined,
      firstAudioDuration: isSet(object.firstAudioDuration) ? Duration.fromJSON(object.firstAudioDuration) : undefined,
      singleUtterance: isSet(object.singleUtterance) ? globalThis.Boolean(object.singleUtterance) : false,
      speechPartialResultsEndTimes: globalThis.Array.isArray(object?.speechPartialResultsEndTimes)
        ? object.speechPartialResultsEndTimes.map((e: any) => Duration.fromJSON(e))
        : [],
      speechFinalResultsEndTimes: globalThis.Array.isArray(object?.speechFinalResultsEndTimes)
        ? object.speechFinalResultsEndTimes.map((e: any) => Duration.fromJSON(e))
        : [],
      partialResponses: isSet(object.partialResponses) ? globalThis.Number(object.partialResponses) : 0,
      speakerIdPassiveLatencyMsOffset: isSet(object.speakerIdPassiveLatencyMsOffset)
        ? globalThis.Number(object.speakerIdPassiveLatencyMsOffset)
        : 0,
      bargeinEventTriggered: isSet(object.bargeinEventTriggered)
        ? globalThis.Boolean(object.bargeinEventTriggered)
        : false,
      speechSingleUtterance: isSet(object.speechSingleUtterance)
        ? globalThis.Boolean(object.speechSingleUtterance)
        : false,
      dtmfPartialResultsTimes: globalThis.Array.isArray(object?.dtmfPartialResultsTimes)
        ? object.dtmfPartialResultsTimes.map((e: any) => Duration.fromJSON(e))
        : [],
      dtmfFinalResultsTimes: globalThis.Array.isArray(object?.dtmfFinalResultsTimes)
        ? object.dtmfFinalResultsTimes.map((e: any) => Duration.fromJSON(e))
        : [],
      singleUtteranceEndTimeOffset: isSet(object.singleUtteranceEndTimeOffset)
        ? Duration.fromJSON(object.singleUtteranceEndTimeOffset)
        : undefined,
      noSpeechTimeout: isSet(object.noSpeechTimeout) ? Duration.fromJSON(object.noSpeechTimeout) : undefined,
      endpointingTimeout: isSet(object.endpointingTimeout) ? Duration.fromJSON(object.endpointingTimeout) : undefined,
      isInputText: isSet(object.isInputText) ? globalThis.Boolean(object.isInputText) : false,
      clientHalfCloseTimeOffset: isSet(object.clientHalfCloseTimeOffset)
        ? Duration.fromJSON(object.clientHalfCloseTimeOffset)
        : undefined,
      clientHalfCloseStreamingTimeOffset: isSet(object.clientHalfCloseStreamingTimeOffset)
        ? Duration.fromJSON(object.clientHalfCloseStreamingTimeOffset)
        : undefined,
    };
  },

  toJSON(message: CloudConversationDebuggingInfo): unknown {
    const obj: any = {};
    if (message.audioDataChunks !== 0) {
      obj.audioDataChunks = Math.round(message.audioDataChunks);
    }
    if (message.resultEndTimeOffset !== undefined) {
      obj.resultEndTimeOffset = Duration.toJSON(message.resultEndTimeOffset);
    }
    if (message.firstAudioDuration !== undefined) {
      obj.firstAudioDuration = Duration.toJSON(message.firstAudioDuration);
    }
    if (message.singleUtterance !== false) {
      obj.singleUtterance = message.singleUtterance;
    }
    if (message.speechPartialResultsEndTimes?.length) {
      obj.speechPartialResultsEndTimes = message.speechPartialResultsEndTimes.map((e) => Duration.toJSON(e));
    }
    if (message.speechFinalResultsEndTimes?.length) {
      obj.speechFinalResultsEndTimes = message.speechFinalResultsEndTimes.map((e) => Duration.toJSON(e));
    }
    if (message.partialResponses !== 0) {
      obj.partialResponses = Math.round(message.partialResponses);
    }
    if (message.speakerIdPassiveLatencyMsOffset !== 0) {
      obj.speakerIdPassiveLatencyMsOffset = Math.round(message.speakerIdPassiveLatencyMsOffset);
    }
    if (message.bargeinEventTriggered !== false) {
      obj.bargeinEventTriggered = message.bargeinEventTriggered;
    }
    if (message.speechSingleUtterance !== false) {
      obj.speechSingleUtterance = message.speechSingleUtterance;
    }
    if (message.dtmfPartialResultsTimes?.length) {
      obj.dtmfPartialResultsTimes = message.dtmfPartialResultsTimes.map((e) => Duration.toJSON(e));
    }
    if (message.dtmfFinalResultsTimes?.length) {
      obj.dtmfFinalResultsTimes = message.dtmfFinalResultsTimes.map((e) => Duration.toJSON(e));
    }
    if (message.singleUtteranceEndTimeOffset !== undefined) {
      obj.singleUtteranceEndTimeOffset = Duration.toJSON(message.singleUtteranceEndTimeOffset);
    }
    if (message.noSpeechTimeout !== undefined) {
      obj.noSpeechTimeout = Duration.toJSON(message.noSpeechTimeout);
    }
    if (message.endpointingTimeout !== undefined) {
      obj.endpointingTimeout = Duration.toJSON(message.endpointingTimeout);
    }
    if (message.isInputText !== false) {
      obj.isInputText = message.isInputText;
    }
    if (message.clientHalfCloseTimeOffset !== undefined) {
      obj.clientHalfCloseTimeOffset = Duration.toJSON(message.clientHalfCloseTimeOffset);
    }
    if (message.clientHalfCloseStreamingTimeOffset !== undefined) {
      obj.clientHalfCloseStreamingTimeOffset = Duration.toJSON(message.clientHalfCloseStreamingTimeOffset);
    }
    return obj;
  },

  create(base?: DeepPartial<CloudConversationDebuggingInfo>): CloudConversationDebuggingInfo {
    return CloudConversationDebuggingInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CloudConversationDebuggingInfo>): CloudConversationDebuggingInfo {
    const message = createBaseCloudConversationDebuggingInfo();
    message.audioDataChunks = object.audioDataChunks ?? 0;
    message.resultEndTimeOffset = (object.resultEndTimeOffset !== undefined && object.resultEndTimeOffset !== null)
      ? Duration.fromPartial(object.resultEndTimeOffset)
      : undefined;
    message.firstAudioDuration = (object.firstAudioDuration !== undefined && object.firstAudioDuration !== null)
      ? Duration.fromPartial(object.firstAudioDuration)
      : undefined;
    message.singleUtterance = object.singleUtterance ?? false;
    message.speechPartialResultsEndTimes = object.speechPartialResultsEndTimes?.map((e) => Duration.fromPartial(e)) ||
      [];
    message.speechFinalResultsEndTimes = object.speechFinalResultsEndTimes?.map((e) => Duration.fromPartial(e)) || [];
    message.partialResponses = object.partialResponses ?? 0;
    message.speakerIdPassiveLatencyMsOffset = object.speakerIdPassiveLatencyMsOffset ?? 0;
    message.bargeinEventTriggered = object.bargeinEventTriggered ?? false;
    message.speechSingleUtterance = object.speechSingleUtterance ?? false;
    message.dtmfPartialResultsTimes = object.dtmfPartialResultsTimes?.map((e) => Duration.fromPartial(e)) || [];
    message.dtmfFinalResultsTimes = object.dtmfFinalResultsTimes?.map((e) => Duration.fromPartial(e)) || [];
    message.singleUtteranceEndTimeOffset =
      (object.singleUtteranceEndTimeOffset !== undefined && object.singleUtteranceEndTimeOffset !== null)
        ? Duration.fromPartial(object.singleUtteranceEndTimeOffset)
        : undefined;
    message.noSpeechTimeout = (object.noSpeechTimeout !== undefined && object.noSpeechTimeout !== null)
      ? Duration.fromPartial(object.noSpeechTimeout)
      : undefined;
    message.endpointingTimeout = (object.endpointingTimeout !== undefined && object.endpointingTimeout !== null)
      ? Duration.fromPartial(object.endpointingTimeout)
      : undefined;
    message.isInputText = object.isInputText ?? false;
    message.clientHalfCloseTimeOffset =
      (object.clientHalfCloseTimeOffset !== undefined && object.clientHalfCloseTimeOffset !== null)
        ? Duration.fromPartial(object.clientHalfCloseTimeOffset)
        : undefined;
    message.clientHalfCloseStreamingTimeOffset =
      (object.clientHalfCloseStreamingTimeOffset !== undefined && object.clientHalfCloseStreamingTimeOffset !== null)
        ? Duration.fromPartial(object.clientHalfCloseStreamingTimeOffset)
        : undefined;
    return message;
  },
};

function createBaseStreamingDetectIntentResponse(): StreamingDetectIntentResponse {
  return {
    responseId: "",
    recognitionResult: undefined,
    queryResult: undefined,
    alternativeQueryResults: [],
    webhookStatus: undefined,
    outputAudio: Buffer.alloc(0),
    outputAudioConfig: undefined,
    debuggingInfo: undefined,
  };
}

export const StreamingDetectIntentResponse: MessageFns<StreamingDetectIntentResponse> = {
  encode(message: StreamingDetectIntentResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.responseId !== "") {
      writer.uint32(10).string(message.responseId);
    }
    if (message.recognitionResult !== undefined) {
      StreamingRecognitionResult.encode(message.recognitionResult, writer.uint32(18).fork()).join();
    }
    if (message.queryResult !== undefined) {
      QueryResult.encode(message.queryResult, writer.uint32(26).fork()).join();
    }
    for (const v of message.alternativeQueryResults) {
      QueryResult.encode(v!, writer.uint32(58).fork()).join();
    }
    if (message.webhookStatus !== undefined) {
      Status.encode(message.webhookStatus, writer.uint32(34).fork()).join();
    }
    if (message.outputAudio.length !== 0) {
      writer.uint32(42).bytes(message.outputAudio);
    }
    if (message.outputAudioConfig !== undefined) {
      OutputAudioConfig.encode(message.outputAudioConfig, writer.uint32(50).fork()).join();
    }
    if (message.debuggingInfo !== undefined) {
      CloudConversationDebuggingInfo.encode(message.debuggingInfo, writer.uint32(66).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingDetectIntentResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingDetectIntentResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.responseId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.recognitionResult = StreamingRecognitionResult.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.queryResult = QueryResult.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.alternativeQueryResults.push(QueryResult.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.webhookStatus = Status.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.outputAudio = Buffer.from(reader.bytes());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.outputAudioConfig = OutputAudioConfig.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.debuggingInfo = CloudConversationDebuggingInfo.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingDetectIntentResponse {
    return {
      responseId: isSet(object.responseId) ? globalThis.String(object.responseId) : "",
      recognitionResult: isSet(object.recognitionResult)
        ? StreamingRecognitionResult.fromJSON(object.recognitionResult)
        : undefined,
      queryResult: isSet(object.queryResult) ? QueryResult.fromJSON(object.queryResult) : undefined,
      alternativeQueryResults: globalThis.Array.isArray(object?.alternativeQueryResults)
        ? object.alternativeQueryResults.map((e: any) => QueryResult.fromJSON(e))
        : [],
      webhookStatus: isSet(object.webhookStatus) ? Status.fromJSON(object.webhookStatus) : undefined,
      outputAudio: isSet(object.outputAudio) ? Buffer.from(bytesFromBase64(object.outputAudio)) : Buffer.alloc(0),
      outputAudioConfig: isSet(object.outputAudioConfig)
        ? OutputAudioConfig.fromJSON(object.outputAudioConfig)
        : undefined,
      debuggingInfo: isSet(object.debuggingInfo)
        ? CloudConversationDebuggingInfo.fromJSON(object.debuggingInfo)
        : undefined,
    };
  },

  toJSON(message: StreamingDetectIntentResponse): unknown {
    const obj: any = {};
    if (message.responseId !== "") {
      obj.responseId = message.responseId;
    }
    if (message.recognitionResult !== undefined) {
      obj.recognitionResult = StreamingRecognitionResult.toJSON(message.recognitionResult);
    }
    if (message.queryResult !== undefined) {
      obj.queryResult = QueryResult.toJSON(message.queryResult);
    }
    if (message.alternativeQueryResults?.length) {
      obj.alternativeQueryResults = message.alternativeQueryResults.map((e) => QueryResult.toJSON(e));
    }
    if (message.webhookStatus !== undefined) {
      obj.webhookStatus = Status.toJSON(message.webhookStatus);
    }
    if (message.outputAudio.length !== 0) {
      obj.outputAudio = base64FromBytes(message.outputAudio);
    }
    if (message.outputAudioConfig !== undefined) {
      obj.outputAudioConfig = OutputAudioConfig.toJSON(message.outputAudioConfig);
    }
    if (message.debuggingInfo !== undefined) {
      obj.debuggingInfo = CloudConversationDebuggingInfo.toJSON(message.debuggingInfo);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamingDetectIntentResponse>): StreamingDetectIntentResponse {
    return StreamingDetectIntentResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamingDetectIntentResponse>): StreamingDetectIntentResponse {
    const message = createBaseStreamingDetectIntentResponse();
    message.responseId = object.responseId ?? "";
    message.recognitionResult = (object.recognitionResult !== undefined && object.recognitionResult !== null)
      ? StreamingRecognitionResult.fromPartial(object.recognitionResult)
      : undefined;
    message.queryResult = (object.queryResult !== undefined && object.queryResult !== null)
      ? QueryResult.fromPartial(object.queryResult)
      : undefined;
    message.alternativeQueryResults = object.alternativeQueryResults?.map((e) => QueryResult.fromPartial(e)) || [];
    message.webhookStatus = (object.webhookStatus !== undefined && object.webhookStatus !== null)
      ? Status.fromPartial(object.webhookStatus)
      : undefined;
    message.outputAudio = object.outputAudio ?? Buffer.alloc(0);
    message.outputAudioConfig = (object.outputAudioConfig !== undefined && object.outputAudioConfig !== null)
      ? OutputAudioConfig.fromPartial(object.outputAudioConfig)
      : undefined;
    message.debuggingInfo = (object.debuggingInfo !== undefined && object.debuggingInfo !== null)
      ? CloudConversationDebuggingInfo.fromPartial(object.debuggingInfo)
      : undefined;
    return message;
  },
};

function createBaseStreamingRecognitionResult(): StreamingRecognitionResult {
  return {
    messageType: 0,
    transcript: "",
    isFinal: false,
    confidence: 0,
    stability: 0,
    speechWordInfo: [],
    speechEndOffset: undefined,
    languageCode: "",
    dtmfDigits: undefined,
  };
}

export const StreamingRecognitionResult: MessageFns<StreamingRecognitionResult> = {
  encode(message: StreamingRecognitionResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.messageType !== 0) {
      writer.uint32(8).int32(message.messageType);
    }
    if (message.transcript !== "") {
      writer.uint32(18).string(message.transcript);
    }
    if (message.isFinal !== false) {
      writer.uint32(24).bool(message.isFinal);
    }
    if (message.confidence !== 0) {
      writer.uint32(37).float(message.confidence);
    }
    if (message.stability !== 0) {
      writer.uint32(53).float(message.stability);
    }
    for (const v of message.speechWordInfo) {
      SpeechWordInfo.encode(v!, writer.uint32(58).fork()).join();
    }
    if (message.speechEndOffset !== undefined) {
      Duration.encode(message.speechEndOffset, writer.uint32(66).fork()).join();
    }
    if (message.languageCode !== "") {
      writer.uint32(82).string(message.languageCode);
    }
    if (message.dtmfDigits !== undefined) {
      TelephonyDtmfEvents.encode(message.dtmfDigits, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingRecognitionResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingRecognitionResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.messageType = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.transcript = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.isFinal = reader.bool();
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.confidence = reader.float();
          continue;
        case 6:
          if (tag !== 53) {
            break;
          }

          message.stability = reader.float();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.speechWordInfo.push(SpeechWordInfo.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.speechEndOffset = Duration.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.languageCode = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.dtmfDigits = TelephonyDtmfEvents.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingRecognitionResult {
    return {
      messageType: isSet(object.messageType) ? streamingRecognitionResult_MessageTypeFromJSON(object.messageType) : 0,
      transcript: isSet(object.transcript) ? globalThis.String(object.transcript) : "",
      isFinal: isSet(object.isFinal) ? globalThis.Boolean(object.isFinal) : false,
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
      stability: isSet(object.stability) ? globalThis.Number(object.stability) : 0,
      speechWordInfo: globalThis.Array.isArray(object?.speechWordInfo)
        ? object.speechWordInfo.map((e: any) => SpeechWordInfo.fromJSON(e))
        : [],
      speechEndOffset: isSet(object.speechEndOffset) ? Duration.fromJSON(object.speechEndOffset) : undefined,
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
      dtmfDigits: isSet(object.dtmfDigits) ? TelephonyDtmfEvents.fromJSON(object.dtmfDigits) : undefined,
    };
  },

  toJSON(message: StreamingRecognitionResult): unknown {
    const obj: any = {};
    if (message.messageType !== 0) {
      obj.messageType = streamingRecognitionResult_MessageTypeToJSON(message.messageType);
    }
    if (message.transcript !== "") {
      obj.transcript = message.transcript;
    }
    if (message.isFinal !== false) {
      obj.isFinal = message.isFinal;
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    if (message.stability !== 0) {
      obj.stability = message.stability;
    }
    if (message.speechWordInfo?.length) {
      obj.speechWordInfo = message.speechWordInfo.map((e) => SpeechWordInfo.toJSON(e));
    }
    if (message.speechEndOffset !== undefined) {
      obj.speechEndOffset = Duration.toJSON(message.speechEndOffset);
    }
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    if (message.dtmfDigits !== undefined) {
      obj.dtmfDigits = TelephonyDtmfEvents.toJSON(message.dtmfDigits);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamingRecognitionResult>): StreamingRecognitionResult {
    return StreamingRecognitionResult.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamingRecognitionResult>): StreamingRecognitionResult {
    const message = createBaseStreamingRecognitionResult();
    message.messageType = object.messageType ?? 0;
    message.transcript = object.transcript ?? "";
    message.isFinal = object.isFinal ?? false;
    message.confidence = object.confidence ?? 0;
    message.stability = object.stability ?? 0;
    message.speechWordInfo = object.speechWordInfo?.map((e) => SpeechWordInfo.fromPartial(e)) || [];
    message.speechEndOffset = (object.speechEndOffset !== undefined && object.speechEndOffset !== null)
      ? Duration.fromPartial(object.speechEndOffset)
      : undefined;
    message.languageCode = object.languageCode ?? "";
    message.dtmfDigits = (object.dtmfDigits !== undefined && object.dtmfDigits !== null)
      ? TelephonyDtmfEvents.fromPartial(object.dtmfDigits)
      : undefined;
    return message;
  },
};

function createBaseTextInput(): TextInput {
  return { text: "", languageCode: "" };
}

export const TextInput: MessageFns<TextInput> = {
  encode(message: TextInput, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.text !== "") {
      writer.uint32(10).string(message.text);
    }
    if (message.languageCode !== "") {
      writer.uint32(18).string(message.languageCode);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TextInput {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextInput();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.text = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.languageCode = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextInput {
    return {
      text: isSet(object.text) ? globalThis.String(object.text) : "",
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
    };
  },

  toJSON(message: TextInput): unknown {
    const obj: any = {};
    if (message.text !== "") {
      obj.text = message.text;
    }
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    return obj;
  },

  create(base?: DeepPartial<TextInput>): TextInput {
    return TextInput.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TextInput>): TextInput {
    const message = createBaseTextInput();
    message.text = object.text ?? "";
    message.languageCode = object.languageCode ?? "";
    return message;
  },
};

function createBaseEventInput(): EventInput {
  return { name: "", parameters: undefined, languageCode: "" };
}

export const EventInput: MessageFns<EventInput> = {
  encode(message: EventInput, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.parameters !== undefined) {
      Struct.encode(Struct.wrap(message.parameters), writer.uint32(18).fork()).join();
    }
    if (message.languageCode !== "") {
      writer.uint32(26).string(message.languageCode);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EventInput {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEventInput();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.parameters = Struct.unwrap(Struct.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.languageCode = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EventInput {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      parameters: isObject(object.parameters) ? object.parameters : undefined,
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
    };
  },

  toJSON(message: EventInput): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.parameters !== undefined) {
      obj.parameters = message.parameters;
    }
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    return obj;
  },

  create(base?: DeepPartial<EventInput>): EventInput {
    return EventInput.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<EventInput>): EventInput {
    const message = createBaseEventInput();
    message.name = object.name ?? "";
    message.parameters = object.parameters ?? undefined;
    message.languageCode = object.languageCode ?? "";
    return message;
  },
};

function createBaseSentimentAnalysisRequestConfig(): SentimentAnalysisRequestConfig {
  return { analyzeQueryTextSentiment: false };
}

export const SentimentAnalysisRequestConfig: MessageFns<SentimentAnalysisRequestConfig> = {
  encode(message: SentimentAnalysisRequestConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.analyzeQueryTextSentiment !== false) {
      writer.uint32(8).bool(message.analyzeQueryTextSentiment);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SentimentAnalysisRequestConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSentimentAnalysisRequestConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.analyzeQueryTextSentiment = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SentimentAnalysisRequestConfig {
    return {
      analyzeQueryTextSentiment: isSet(object.analyzeQueryTextSentiment)
        ? globalThis.Boolean(object.analyzeQueryTextSentiment)
        : false,
    };
  },

  toJSON(message: SentimentAnalysisRequestConfig): unknown {
    const obj: any = {};
    if (message.analyzeQueryTextSentiment !== false) {
      obj.analyzeQueryTextSentiment = message.analyzeQueryTextSentiment;
    }
    return obj;
  },

  create(base?: DeepPartial<SentimentAnalysisRequestConfig>): SentimentAnalysisRequestConfig {
    return SentimentAnalysisRequestConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SentimentAnalysisRequestConfig>): SentimentAnalysisRequestConfig {
    const message = createBaseSentimentAnalysisRequestConfig();
    message.analyzeQueryTextSentiment = object.analyzeQueryTextSentiment ?? false;
    return message;
  },
};

function createBaseSentimentAnalysisResult(): SentimentAnalysisResult {
  return { queryTextSentiment: undefined };
}

export const SentimentAnalysisResult: MessageFns<SentimentAnalysisResult> = {
  encode(message: SentimentAnalysisResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.queryTextSentiment !== undefined) {
      Sentiment.encode(message.queryTextSentiment, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SentimentAnalysisResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSentimentAnalysisResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.queryTextSentiment = Sentiment.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SentimentAnalysisResult {
    return {
      queryTextSentiment: isSet(object.queryTextSentiment) ? Sentiment.fromJSON(object.queryTextSentiment) : undefined,
    };
  },

  toJSON(message: SentimentAnalysisResult): unknown {
    const obj: any = {};
    if (message.queryTextSentiment !== undefined) {
      obj.queryTextSentiment = Sentiment.toJSON(message.queryTextSentiment);
    }
    return obj;
  },

  create(base?: DeepPartial<SentimentAnalysisResult>): SentimentAnalysisResult {
    return SentimentAnalysisResult.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SentimentAnalysisResult>): SentimentAnalysisResult {
    const message = createBaseSentimentAnalysisResult();
    message.queryTextSentiment = (object.queryTextSentiment !== undefined && object.queryTextSentiment !== null)
      ? Sentiment.fromPartial(object.queryTextSentiment)
      : undefined;
    return message;
  },
};

function createBaseSentiment(): Sentiment {
  return { score: 0, magnitude: 0 };
}

export const Sentiment: MessageFns<Sentiment> = {
  encode(message: Sentiment, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.score !== 0) {
      writer.uint32(13).float(message.score);
    }
    if (message.magnitude !== 0) {
      writer.uint32(21).float(message.magnitude);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Sentiment {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSentiment();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 13) {
            break;
          }

          message.score = reader.float();
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.magnitude = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Sentiment {
    return {
      score: isSet(object.score) ? globalThis.Number(object.score) : 0,
      magnitude: isSet(object.magnitude) ? globalThis.Number(object.magnitude) : 0,
    };
  },

  toJSON(message: Sentiment): unknown {
    const obj: any = {};
    if (message.score !== 0) {
      obj.score = message.score;
    }
    if (message.magnitude !== 0) {
      obj.magnitude = message.magnitude;
    }
    return obj;
  },

  create(base?: DeepPartial<Sentiment>): Sentiment {
    return Sentiment.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Sentiment>): Sentiment {
    const message = createBaseSentiment();
    message.score = object.score ?? 0;
    message.magnitude = object.magnitude ?? 0;
    return message;
  },
};

/**
 * A service used for session interactions.
 *
 * For more information, see the [API interactions
 * guide](https://cloud.google.com/dialogflow/docs/api-overview).
 */
export type SessionsDefinition = typeof SessionsDefinition;
export const SessionsDefinition = {
  name: "Sessions",
  fullName: "google.cloud.dialogflow.v2beta1.Sessions",
  methods: {
    /**
     * Processes a natural language query and returns structured, actionable data
     * as a result. This method is not idempotent, because it may cause contexts
     * and session entity types to be updated, which in turn might affect
     * results of future queries.
     *
     * If you might use
     * [Agent Assist](https://cloud.google.com/dialogflow/docs/#aa)
     * or other CCAI products now or in the future, consider using
     * [AnalyzeContent][google.cloud.dialogflow.v2beta1.Participants.AnalyzeContent]
     * instead of `DetectIntent`. `AnalyzeContent` has additional
     * functionality for Agent Assist and other CCAI products.
     *
     * Note: Always use agent versions for production traffic.
     * See [Versions and
     * environments](https://cloud.google.com/dialogflow/es/docs/agents-versions).
     */
    detectIntent: {
      name: "DetectIntent",
      requestType: DetectIntentRequest,
      requestStream: false,
      responseType: DetectIntentResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              19,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              44,
              113,
              117,
              101,
              114,
              121,
              95,
              105,
              110,
              112,
              117,
              116,
            ]),
          ],
          578365826: [
            Buffer.from([
              204,
              2,
              58,
              1,
              42,
              90,
              87,
              58,
              1,
              42,
              34,
              82,
              47,
              118,
              50,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              97,
              103,
              101,
              110,
              116,
              47,
              101,
              110,
              118,
              105,
              114,
              111,
              110,
              109,
              101,
              110,
              116,
              115,
              47,
              42,
              47,
              117,
              115,
              101,
              114,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              58,
              100,
              101,
              116,
              101,
              99,
              116,
              73,
              110,
              116,
              101,
              110,
              116,
              90,
              76,
              58,
              1,
              42,
              34,
              71,
              47,
              118,
              50,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              97,
              103,
              101,
              110,
              116,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              58,
              100,
              101,
              116,
              101,
              99,
              116,
              73,
              110,
              116,
              101,
              110,
              116,
              90,
              99,
              58,
              1,
              42,
              34,
              94,
              47,
              118,
              50,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              97,
              103,
              101,
              110,
              116,
              47,
              101,
              110,
              118,
              105,
              114,
              111,
              110,
              109,
              101,
              110,
              116,
              115,
              47,
              42,
              47,
              117,
              115,
              101,
              114,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              58,
              100,
              101,
              116,
              101,
              99,
              116,
              73,
              110,
              116,
              101,
              110,
              116,
              34,
              59,
              47,
              118,
              50,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              97,
              103,
              101,
              110,
              116,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              58,
              100,
              101,
              116,
              101,
              99,
              116,
              73,
              110,
              116,
              101,
              110,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * Processes a natural language query in audio format in a streaming fashion
     * and returns structured, actionable data as a result. This method is only
     * available via the gRPC API (not REST).
     *
     * If you might use
     * [Agent Assist](https://cloud.google.com/dialogflow/docs/#aa)
     * or other CCAI products now or in the future, consider using
     * [StreamingAnalyzeContent][google.cloud.dialogflow.v2beta1.Participants.StreamingAnalyzeContent]
     * instead of `StreamingDetectIntent`. `StreamingAnalyzeContent` has
     * additional functionality for Agent Assist and other CCAI products.
     *
     * Note: Always use agent versions for production traffic.
     * See [Versions and
     * environments](https://cloud.google.com/dialogflow/es/docs/agents-versions).
     */
    streamingDetectIntent: {
      name: "StreamingDetectIntent",
      requestType: StreamingDetectIntentRequest,
      requestStream: true,
      responseType: StreamingDetectIntentResponse,
      responseStream: true,
      options: {},
    },
  },
} as const;

export interface SessionsServiceImplementation<CallContextExt = {}> {
  /**
   * Processes a natural language query and returns structured, actionable data
   * as a result. This method is not idempotent, because it may cause contexts
   * and session entity types to be updated, which in turn might affect
   * results of future queries.
   *
   * If you might use
   * [Agent Assist](https://cloud.google.com/dialogflow/docs/#aa)
   * or other CCAI products now or in the future, consider using
   * [AnalyzeContent][google.cloud.dialogflow.v2beta1.Participants.AnalyzeContent]
   * instead of `DetectIntent`. `AnalyzeContent` has additional
   * functionality for Agent Assist and other CCAI products.
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/es/docs/agents-versions).
   */
  detectIntent(
    request: DetectIntentRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<DetectIntentResponse>>;
  /**
   * Processes a natural language query in audio format in a streaming fashion
   * and returns structured, actionable data as a result. This method is only
   * available via the gRPC API (not REST).
   *
   * If you might use
   * [Agent Assist](https://cloud.google.com/dialogflow/docs/#aa)
   * or other CCAI products now or in the future, consider using
   * [StreamingAnalyzeContent][google.cloud.dialogflow.v2beta1.Participants.StreamingAnalyzeContent]
   * instead of `StreamingDetectIntent`. `StreamingAnalyzeContent` has
   * additional functionality for Agent Assist and other CCAI products.
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/es/docs/agents-versions).
   */
  streamingDetectIntent(
    request: AsyncIterable<StreamingDetectIntentRequest>,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<StreamingDetectIntentResponse>>;
}

export interface SessionsClient<CallOptionsExt = {}> {
  /**
   * Processes a natural language query and returns structured, actionable data
   * as a result. This method is not idempotent, because it may cause contexts
   * and session entity types to be updated, which in turn might affect
   * results of future queries.
   *
   * If you might use
   * [Agent Assist](https://cloud.google.com/dialogflow/docs/#aa)
   * or other CCAI products now or in the future, consider using
   * [AnalyzeContent][google.cloud.dialogflow.v2beta1.Participants.AnalyzeContent]
   * instead of `DetectIntent`. `AnalyzeContent` has additional
   * functionality for Agent Assist and other CCAI products.
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/es/docs/agents-versions).
   */
  detectIntent(
    request: DeepPartial<DetectIntentRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<DetectIntentResponse>;
  /**
   * Processes a natural language query in audio format in a streaming fashion
   * and returns structured, actionable data as a result. This method is only
   * available via the gRPC API (not REST).
   *
   * If you might use
   * [Agent Assist](https://cloud.google.com/dialogflow/docs/#aa)
   * or other CCAI products now or in the future, consider using
   * [StreamingAnalyzeContent][google.cloud.dialogflow.v2beta1.Participants.StreamingAnalyzeContent]
   * instead of `StreamingDetectIntent`. `StreamingAnalyzeContent` has
   * additional functionality for Agent Assist and other CCAI products.
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/es/docs/agents-versions).
   */
  streamingDetectIntent(
    request: AsyncIterable<DeepPartial<StreamingDetectIntentRequest>>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<StreamingDetectIntentResponse>;
}

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export type ServerStreamingMethodResult<Response> = { [Symbol.asyncIterator](): AsyncIterator<Response, void> };

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
