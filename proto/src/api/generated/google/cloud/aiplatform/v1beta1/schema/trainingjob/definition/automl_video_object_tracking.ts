// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/aiplatform/v1beta1/schema/trainingjob/definition/automl_video_object_tracking.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";

export const protobufPackage = "google.cloud.aiplatform.v1beta1.schema.trainingjob.definition";

/** A TrainingJob that trains and uploads an AutoML Video ObjectTracking Model. */
export interface AutoMlVideoObjectTracking {
  /** The input parameters of this TrainingJob. */
  inputs: AutoMlVideoObjectTrackingInputs | undefined;
}

export interface AutoMlVideoObjectTrackingInputs {
  modelType: AutoMlVideoObjectTrackingInputs_ModelType;
}

export enum AutoMlVideoObjectTrackingInputs_ModelType {
  /** MODEL_TYPE_UNSPECIFIED - Should not be set. */
  MODEL_TYPE_UNSPECIFIED = 0,
  /**
   * CLOUD - A model best tailored to be used within Google Cloud, and which c annot
   * be exported. Default.
   */
  CLOUD = 1,
  /**
   * MOBILE_VERSATILE_1 - A model that, in addition to being available within Google Cloud, can
   * also be exported (see ModelService.ExportModel) as a TensorFlow or
   * TensorFlow Lite model and used on a mobile or edge device afterwards.
   */
  MOBILE_VERSATILE_1 = 2,
  /**
   * MOBILE_CORAL_VERSATILE_1 - A versatile model that is meant to be exported (see
   * ModelService.ExportModel) and used on a Google Coral device.
   */
  MOBILE_CORAL_VERSATILE_1 = 3,
  /**
   * MOBILE_CORAL_LOW_LATENCY_1 - A model that trades off quality for low latency, to be exported (see
   * ModelService.ExportModel) and used on a Google Coral device.
   */
  MOBILE_CORAL_LOW_LATENCY_1 = 4,
  /**
   * MOBILE_JETSON_VERSATILE_1 - A versatile model that is meant to be exported (see
   * ModelService.ExportModel) and used on an NVIDIA Jetson device.
   */
  MOBILE_JETSON_VERSATILE_1 = 5,
  /**
   * MOBILE_JETSON_LOW_LATENCY_1 - A model that trades off quality for low latency, to be exported (see
   * ModelService.ExportModel) and used on an NVIDIA Jetson device.
   */
  MOBILE_JETSON_LOW_LATENCY_1 = 6,
  UNRECOGNIZED = -1,
}

export function autoMlVideoObjectTrackingInputs_ModelTypeFromJSON(
  object: any,
): AutoMlVideoObjectTrackingInputs_ModelType {
  switch (object) {
    case 0:
    case "MODEL_TYPE_UNSPECIFIED":
      return AutoMlVideoObjectTrackingInputs_ModelType.MODEL_TYPE_UNSPECIFIED;
    case 1:
    case "CLOUD":
      return AutoMlVideoObjectTrackingInputs_ModelType.CLOUD;
    case 2:
    case "MOBILE_VERSATILE_1":
      return AutoMlVideoObjectTrackingInputs_ModelType.MOBILE_VERSATILE_1;
    case 3:
    case "MOBILE_CORAL_VERSATILE_1":
      return AutoMlVideoObjectTrackingInputs_ModelType.MOBILE_CORAL_VERSATILE_1;
    case 4:
    case "MOBILE_CORAL_LOW_LATENCY_1":
      return AutoMlVideoObjectTrackingInputs_ModelType.MOBILE_CORAL_LOW_LATENCY_1;
    case 5:
    case "MOBILE_JETSON_VERSATILE_1":
      return AutoMlVideoObjectTrackingInputs_ModelType.MOBILE_JETSON_VERSATILE_1;
    case 6:
    case "MOBILE_JETSON_LOW_LATENCY_1":
      return AutoMlVideoObjectTrackingInputs_ModelType.MOBILE_JETSON_LOW_LATENCY_1;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AutoMlVideoObjectTrackingInputs_ModelType.UNRECOGNIZED;
  }
}

export function autoMlVideoObjectTrackingInputs_ModelTypeToJSON(
  object: AutoMlVideoObjectTrackingInputs_ModelType,
): string {
  switch (object) {
    case AutoMlVideoObjectTrackingInputs_ModelType.MODEL_TYPE_UNSPECIFIED:
      return "MODEL_TYPE_UNSPECIFIED";
    case AutoMlVideoObjectTrackingInputs_ModelType.CLOUD:
      return "CLOUD";
    case AutoMlVideoObjectTrackingInputs_ModelType.MOBILE_VERSATILE_1:
      return "MOBILE_VERSATILE_1";
    case AutoMlVideoObjectTrackingInputs_ModelType.MOBILE_CORAL_VERSATILE_1:
      return "MOBILE_CORAL_VERSATILE_1";
    case AutoMlVideoObjectTrackingInputs_ModelType.MOBILE_CORAL_LOW_LATENCY_1:
      return "MOBILE_CORAL_LOW_LATENCY_1";
    case AutoMlVideoObjectTrackingInputs_ModelType.MOBILE_JETSON_VERSATILE_1:
      return "MOBILE_JETSON_VERSATILE_1";
    case AutoMlVideoObjectTrackingInputs_ModelType.MOBILE_JETSON_LOW_LATENCY_1:
      return "MOBILE_JETSON_LOW_LATENCY_1";
    case AutoMlVideoObjectTrackingInputs_ModelType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseAutoMlVideoObjectTracking(): AutoMlVideoObjectTracking {
  return { inputs: undefined };
}

export const AutoMlVideoObjectTracking: MessageFns<AutoMlVideoObjectTracking> = {
  encode(message: AutoMlVideoObjectTracking, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.inputs !== undefined) {
      AutoMlVideoObjectTrackingInputs.encode(message.inputs, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutoMlVideoObjectTracking {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutoMlVideoObjectTracking();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.inputs = AutoMlVideoObjectTrackingInputs.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutoMlVideoObjectTracking {
    return { inputs: isSet(object.inputs) ? AutoMlVideoObjectTrackingInputs.fromJSON(object.inputs) : undefined };
  },

  toJSON(message: AutoMlVideoObjectTracking): unknown {
    const obj: any = {};
    if (message.inputs !== undefined) {
      obj.inputs = AutoMlVideoObjectTrackingInputs.toJSON(message.inputs);
    }
    return obj;
  },

  create(base?: DeepPartial<AutoMlVideoObjectTracking>): AutoMlVideoObjectTracking {
    return AutoMlVideoObjectTracking.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutoMlVideoObjectTracking>): AutoMlVideoObjectTracking {
    const message = createBaseAutoMlVideoObjectTracking();
    message.inputs = (object.inputs !== undefined && object.inputs !== null)
      ? AutoMlVideoObjectTrackingInputs.fromPartial(object.inputs)
      : undefined;
    return message;
  },
};

function createBaseAutoMlVideoObjectTrackingInputs(): AutoMlVideoObjectTrackingInputs {
  return { modelType: 0 };
}

export const AutoMlVideoObjectTrackingInputs: MessageFns<AutoMlVideoObjectTrackingInputs> = {
  encode(message: AutoMlVideoObjectTrackingInputs, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.modelType !== 0) {
      writer.uint32(8).int32(message.modelType);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AutoMlVideoObjectTrackingInputs {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAutoMlVideoObjectTrackingInputs();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.modelType = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AutoMlVideoObjectTrackingInputs {
    return {
      modelType: isSet(object.modelType) ? autoMlVideoObjectTrackingInputs_ModelTypeFromJSON(object.modelType) : 0,
    };
  },

  toJSON(message: AutoMlVideoObjectTrackingInputs): unknown {
    const obj: any = {};
    if (message.modelType !== 0) {
      obj.modelType = autoMlVideoObjectTrackingInputs_ModelTypeToJSON(message.modelType);
    }
    return obj;
  },

  create(base?: DeepPartial<AutoMlVideoObjectTrackingInputs>): AutoMlVideoObjectTrackingInputs {
    return AutoMlVideoObjectTrackingInputs.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AutoMlVideoObjectTrackingInputs>): AutoMlVideoObjectTrackingInputs {
    const message = createBaseAutoMlVideoObjectTrackingInputs();
    message.modelType = object.modelType ?? 0;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
