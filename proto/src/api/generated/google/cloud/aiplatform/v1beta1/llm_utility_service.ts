// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/aiplatform/v1beta1/llm_utility_service.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Value } from "../../../protobuf/struct.js";
import { Content } from "./content.js";

export const protobufPackage = "google.cloud.aiplatform.v1beta1";

/** Request message for ComputeTokens RPC call. */
export interface ComputeTokensRequest {
  /**
   * Required. The name of the Endpoint requested to get lists of tokens and
   * token ids.
   */
  endpoint: string;
  /**
   * Optional. The instances that are the input to token computing API call.
   * Schema is identical to the prediction schema of the text model, even for
   * the non-text models, like chat models, or Codey models.
   */
  instances: any[];
  /**
   * Optional. The name of the publisher model requested to serve the
   * prediction. Format:
   * projects/{project}/locations/{location}/publishers/* /models/*
   */
  model: string;
  /** Optional. Input content. */
  contents: Content[];
}

/** Tokens info with a list of tokens and the corresponding list of token ids. */
export interface TokensInfo {
  /** A list of tokens from the input. */
  tokens: Buffer[];
  /** A list of token ids from the input. */
  tokenIds: Long[];
  /** Optional. Optional fields for the role from the corresponding Content. */
  role: string;
}

/** Response message for ComputeTokens RPC call. */
export interface ComputeTokensResponse {
  /**
   * Lists of tokens info from the input. A ComputeTokensRequest could have
   * multiple instances with a prompt in each instance. We also need to return
   * lists of tokens info for the request with multiple instances.
   */
  tokensInfo: TokensInfo[];
}

function createBaseComputeTokensRequest(): ComputeTokensRequest {
  return { endpoint: "", instances: [], model: "", contents: [] };
}

export const ComputeTokensRequest: MessageFns<ComputeTokensRequest> = {
  encode(message: ComputeTokensRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.endpoint !== "") {
      writer.uint32(10).string(message.endpoint);
    }
    for (const v of message.instances) {
      Value.encode(Value.wrap(v!), writer.uint32(18).fork()).join();
    }
    if (message.model !== "") {
      writer.uint32(26).string(message.model);
    }
    for (const v of message.contents) {
      Content.encode(v!, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ComputeTokensRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseComputeTokensRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.endpoint = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.instances.push(Value.unwrap(Value.decode(reader, reader.uint32())));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.model = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.contents.push(Content.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ComputeTokensRequest {
    return {
      endpoint: isSet(object.endpoint) ? globalThis.String(object.endpoint) : "",
      instances: globalThis.Array.isArray(object?.instances) ? [...object.instances] : [],
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      contents: globalThis.Array.isArray(object?.contents) ? object.contents.map((e: any) => Content.fromJSON(e)) : [],
    };
  },

  toJSON(message: ComputeTokensRequest): unknown {
    const obj: any = {};
    if (message.endpoint !== "") {
      obj.endpoint = message.endpoint;
    }
    if (message.instances?.length) {
      obj.instances = message.instances;
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.contents?.length) {
      obj.contents = message.contents.map((e) => Content.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ComputeTokensRequest>): ComputeTokensRequest {
    return ComputeTokensRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ComputeTokensRequest>): ComputeTokensRequest {
    const message = createBaseComputeTokensRequest();
    message.endpoint = object.endpoint ?? "";
    message.instances = object.instances?.map((e) => e) || [];
    message.model = object.model ?? "";
    message.contents = object.contents?.map((e) => Content.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTokensInfo(): TokensInfo {
  return { tokens: [], tokenIds: [], role: "" };
}

export const TokensInfo: MessageFns<TokensInfo> = {
  encode(message: TokensInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.tokens) {
      writer.uint32(10).bytes(v!);
    }
    writer.uint32(18).fork();
    for (const v of message.tokenIds) {
      writer.int64(v.toString());
    }
    writer.join();
    if (message.role !== "") {
      writer.uint32(26).string(message.role);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TokensInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTokensInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.tokens.push(Buffer.from(reader.bytes()));
          continue;
        case 2:
          if (tag === 16) {
            message.tokenIds.push(Long.fromString(reader.int64().toString()));

            continue;
          }

          if (tag === 18) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.tokenIds.push(Long.fromString(reader.int64().toString()));
            }

            continue;
          }

          break;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.role = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TokensInfo {
    return {
      tokens: globalThis.Array.isArray(object?.tokens)
        ? object.tokens.map((e: any) => Buffer.from(bytesFromBase64(e)))
        : [],
      tokenIds: globalThis.Array.isArray(object?.tokenIds) ? object.tokenIds.map((e: any) => Long.fromValue(e)) : [],
      role: isSet(object.role) ? globalThis.String(object.role) : "",
    };
  },

  toJSON(message: TokensInfo): unknown {
    const obj: any = {};
    if (message.tokens?.length) {
      obj.tokens = message.tokens.map((e) => base64FromBytes(e));
    }
    if (message.tokenIds?.length) {
      obj.tokenIds = message.tokenIds.map((e) => (e || Long.ZERO).toString());
    }
    if (message.role !== "") {
      obj.role = message.role;
    }
    return obj;
  },

  create(base?: DeepPartial<TokensInfo>): TokensInfo {
    return TokensInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<TokensInfo>): TokensInfo {
    const message = createBaseTokensInfo();
    message.tokens = object.tokens?.map((e) => e) || [];
    message.tokenIds = object.tokenIds?.map((e) => Long.fromValue(e)) || [];
    message.role = object.role ?? "";
    return message;
  },
};

function createBaseComputeTokensResponse(): ComputeTokensResponse {
  return { tokensInfo: [] };
}

export const ComputeTokensResponse: MessageFns<ComputeTokensResponse> = {
  encode(message: ComputeTokensResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.tokensInfo) {
      TokensInfo.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ComputeTokensResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseComputeTokensResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.tokensInfo.push(TokensInfo.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ComputeTokensResponse {
    return {
      tokensInfo: globalThis.Array.isArray(object?.tokensInfo)
        ? object.tokensInfo.map((e: any) => TokensInfo.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ComputeTokensResponse): unknown {
    const obj: any = {};
    if (message.tokensInfo?.length) {
      obj.tokensInfo = message.tokensInfo.map((e) => TokensInfo.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<ComputeTokensResponse>): ComputeTokensResponse {
    return ComputeTokensResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ComputeTokensResponse>): ComputeTokensResponse {
    const message = createBaseComputeTokensResponse();
    message.tokensInfo = object.tokensInfo?.map((e) => TokensInfo.fromPartial(e)) || [];
    return message;
  },
};

/** Service for LLM related utility functions. */
export type LlmUtilityServiceDefinition = typeof LlmUtilityServiceDefinition;
export const LlmUtilityServiceDefinition = {
  name: "LlmUtilityService",
  fullName: "google.cloud.aiplatform.v1beta1.LlmUtilityService",
  methods: {
    /** Return a list of tokens based on the input text. */
    computeTokens: {
      name: "ComputeTokens",
      requestType: ComputeTokensRequest,
      requestStream: false,
      responseType: ComputeTokensResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([18, 101, 110, 100, 112, 111, 105, 110, 116, 44, 105, 110, 115, 116, 97, 110, 99, 101, 115]),
          ],
          578365826: [
            Buffer.from([
              144,
              2,
              58,
              1,
              42,
              90,
              83,
              58,
              1,
              42,
              34,
              78,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              112,
              117,
              98,
              108,
              105,
              115,
              104,
              101,
              114,
              115,
              47,
              42,
              47,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              99,
              111,
              109,
              112,
              117,
              116,
              101,
              84,
              111,
              107,
              101,
              110,
              115,
              90,
              50,
              58,
              1,
              42,
              34,
              45,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              61,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              115,
              47,
              42,
              125,
              58,
              99,
              111,
              109,
              112,
              117,
              116,
              101,
              84,
              111,
              107,
              101,
              110,
              115,
              90,
              60,
              58,
              1,
              42,
              34,
              55,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              61,
              112,
              117,
              98,
              108,
              105,
              115,
              104,
              101,
              114,
              115,
              47,
              42,
              47,
              109,
              111,
              100,
              101,
              108,
              115,
              47,
              42,
              125,
              58,
              99,
              111,
              109,
              112,
              117,
              116,
              101,
              84,
              111,
              107,
              101,
              110,
              115,
              34,
              68,
              47,
              118,
              49,
              98,
              101,
              116,
              97,
              49,
              47,
              123,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              108,
              111,
              99,
              97,
              116,
              105,
              111,
              110,
              115,
              47,
              42,
              47,
              101,
              110,
              100,
              112,
              111,
              105,
              110,
              116,
              115,
              47,
              42,
              125,
              58,
              99,
              111,
              109,
              112,
              117,
              116,
              101,
              84,
              111,
              107,
              101,
              110,
              115,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface LlmUtilityServiceImplementation<CallContextExt = {}> {
  /** Return a list of tokens based on the input text. */
  computeTokens(
    request: ComputeTokensRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ComputeTokensResponse>>;
}

export interface LlmUtilityServiceClient<CallOptionsExt = {}> {
  /** Return a list of tokens based on the input text. */
  computeTokens(
    request: DeepPartial<ComputeTokensRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ComputeTokensResponse>;
}

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
