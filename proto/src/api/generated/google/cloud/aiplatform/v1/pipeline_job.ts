// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/aiplatform/v1/pipeline_job.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Struct, Value as Value1 } from "../../../protobuf/struct.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { Status } from "../../../rpc/status.js";
import { Artifact } from "./artifact.js";
import { Context } from "./context.js";
import { EncryptionSpec } from "./encryption_spec.js";
import { Execution } from "./execution.js";
import {
  PipelineFailurePolicy,
  pipelineFailurePolicyFromJSON,
  pipelineFailurePolicyToJSON,
} from "./pipeline_failure_policy.js";
import { PipelineState, pipelineStateFromJSON, pipelineStateToJSON } from "./pipeline_state.js";
import { Value } from "./value.js";

export const protobufPackage = "google.cloud.aiplatform.v1";

/** An instance of a machine learning PipelineJob. */
export interface PipelineJob {
  /** Output only. The resource name of the PipelineJob. */
  name: string;
  /**
   * The display name of the Pipeline.
   * The name can be up to 128 characters long and can consist of any UTF-8
   * characters.
   */
  displayName: string;
  /** Output only. Pipeline creation time. */
  createTime:
    | Date
    | undefined;
  /** Output only. Pipeline start time. */
  startTime:
    | Date
    | undefined;
  /** Output only. Pipeline end time. */
  endTime:
    | Date
    | undefined;
  /** Output only. Timestamp when this PipelineJob was most recently updated. */
  updateTime:
    | Date
    | undefined;
  /** The spec of the pipeline. */
  pipelineSpec:
    | { [key: string]: any }
    | undefined;
  /** Output only. The detailed state of the job. */
  state: PipelineState;
  /** Output only. The details of pipeline run. Not available in the list view. */
  jobDetail:
    | PipelineJobDetail
    | undefined;
  /**
   * Output only. The error that occurred during pipeline execution.
   * Only populated when the pipeline's state is FAILED or CANCELLED.
   */
  error:
    | Status
    | undefined;
  /**
   * The labels with user-defined metadata to organize PipelineJob.
   *
   * Label keys and values can be no longer than 64 characters
   * (Unicode codepoints), can only contain lowercase letters, numeric
   * characters, underscores and dashes. International characters are allowed.
   *
   * See https://goo.gl/xmQnxf for more information and examples of labels.
   *
   * Note there is some reserved label key for Vertex AI Pipelines.
   * - `vertex-ai-pipelines-run-billing-id`, user set value will get overrided.
   */
  labels: { [key: string]: string };
  /** Runtime config of the pipeline. */
  runtimeConfig:
    | PipelineJob_RuntimeConfig
    | undefined;
  /**
   * Customer-managed encryption key spec for a pipelineJob. If set, this
   * PipelineJob and all of its sub-resources will be secured by this key.
   */
  encryptionSpec:
    | EncryptionSpec
    | undefined;
  /**
   * The service account that the pipeline workload runs as.
   * If not specified, the Compute Engine default service account in the project
   * will be used.
   * See
   * https://cloud.google.com/compute/docs/access/service-accounts#default_service_account
   *
   * Users starting the pipeline must have the `iam.serviceAccounts.actAs`
   * permission on this service account.
   */
  serviceAccount: string;
  /**
   * The full name of the Compute Engine
   * [network](/compute/docs/networks-and-firewalls#networks) to which the
   * Pipeline Job's workload should be peered. For example,
   * `projects/12345/global/networks/myVPC`.
   * [Format](/compute/docs/reference/rest/v1/networks/insert)
   * is of the form `projects/{project}/global/networks/{network}`.
   * Where {project} is a project number, as in `12345`, and {network} is a
   * network name.
   *
   * Private services access must already be configured for the network.
   * Pipeline job will apply the network configuration to the Google Cloud
   * resources being launched, if applied, such as Vertex AI
   * Training or Dataflow job. If left unspecified, the workload is not peered
   * with any network.
   */
  network: string;
  /**
   * A list of names for the reserved ip ranges under the VPC network
   * that can be used for this Pipeline Job's workload.
   *
   * If set, we will deploy the Pipeline Job's workload within the provided ip
   * ranges. Otherwise, the job will be deployed to any ip ranges under the
   * provided VPC network.
   *
   * Example: ['vertex-ai-ip-range'].
   */
  reservedIpRanges: string[];
  /**
   * A template uri from where the
   * [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec],
   * if empty, will be downloaded. Currently, only uri from Vertex Template
   * Registry & Gallery is supported. Reference to
   * https://cloud.google.com/vertex-ai/docs/pipelines/create-pipeline-template.
   */
  templateUri: string;
  /**
   * Output only. Pipeline template metadata. Will fill up fields if
   * [PipelineJob.template_uri][google.cloud.aiplatform.v1.PipelineJob.template_uri]
   * is from supported template registry.
   */
  templateMetadata:
    | PipelineTemplateMetadata
    | undefined;
  /**
   * Output only. The schedule resource name.
   * Only returned if the Pipeline is created by Schedule API.
   */
  scheduleName: string;
  /** Optional. Whether to do component level validations before job creation. */
  preflightValidations: boolean;
}

/** The runtime config of a PipelineJob. */
export interface PipelineJob_RuntimeConfig {
  /**
   * Deprecated. Use
   * [RuntimeConfig.parameter_values][google.cloud.aiplatform.v1.PipelineJob.RuntimeConfig.parameter_values]
   * instead. The runtime parameters of the PipelineJob. The parameters will
   * be passed into
   * [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec]
   * to replace the placeholders at runtime. This field is used by pipelines
   * built using `PipelineJob.pipeline_spec.schema_version` 2.0.0 or lower,
   * such as pipelines built using Kubeflow Pipelines SDK 1.8 or lower.
   *
   * @deprecated
   */
  parameters: { [key: string]: Value };
  /**
   * Required. A path in a Cloud Storage bucket, which will be treated as the
   * root output directory of the pipeline. It is used by the system to
   * generate the paths of output artifacts. The artifact paths are generated
   * with a sub-path pattern `{job_id}/{task_id}/{output_key}` under the
   * specified output directory. The service account specified in this
   * pipeline must have the `storage.objects.get` and `storage.objects.create`
   * permissions for this bucket.
   */
  gcsOutputDirectory: string;
  /**
   * The runtime parameters of the PipelineJob. The parameters will be
   * passed into
   * [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec]
   * to replace the placeholders at runtime. This field is used by pipelines
   * built using `PipelineJob.pipeline_spec.schema_version` 2.1.0, such as
   * pipelines built using Kubeflow Pipelines SDK 1.9 or higher and the v2
   * DSL.
   */
  parameterValues: { [key: string]: any | undefined };
  /**
   * Represents the failure policy of a pipeline. Currently, the default of a
   * pipeline is that the pipeline will continue to run until no more tasks
   * can be executed, also known as PIPELINE_FAILURE_POLICY_FAIL_SLOW.
   * However, if a pipeline is set to PIPELINE_FAILURE_POLICY_FAIL_FAST, it
   * will stop scheduling any new tasks when a task has failed. Any scheduled
   * tasks will continue to completion.
   */
  failurePolicy: PipelineFailurePolicy;
  /**
   * The runtime artifacts of the PipelineJob. The key will be the input
   * artifact name and the value would be one of the InputArtifact.
   */
  inputArtifacts: { [key: string]: PipelineJob_RuntimeConfig_InputArtifact };
}

/** The type of an input artifact. */
export interface PipelineJob_RuntimeConfig_InputArtifact {
  /**
   * Artifact resource id from MLMD. Which is the last portion of an
   * artifact resource name:
   * `projects/{project}/locations/{location}/metadataStores/default/artifacts/{artifact_id}`.
   * The artifact must stay within the same project, location and default
   * metadatastore as the pipeline.
   */
  artifactId?: string | undefined;
}

export interface PipelineJob_RuntimeConfig_ParametersEntry {
  key: string;
  value: Value | undefined;
}

export interface PipelineJob_RuntimeConfig_ParameterValuesEntry {
  key: string;
  value: any | undefined;
}

export interface PipelineJob_RuntimeConfig_InputArtifactsEntry {
  key: string;
  value: PipelineJob_RuntimeConfig_InputArtifact | undefined;
}

export interface PipelineJob_LabelsEntry {
  key: string;
  value: string;
}

/**
 * Pipeline template metadata if
 * [PipelineJob.template_uri][google.cloud.aiplatform.v1.PipelineJob.template_uri]
 * is from supported template registry. Currently, the only supported registry
 * is Artifact Registry.
 */
export interface PipelineTemplateMetadata {
  /**
   * The version_name in artifact registry.
   *
   * Will always be presented in output if the
   * [PipelineJob.template_uri][google.cloud.aiplatform.v1.PipelineJob.template_uri]
   * is from supported template registry.
   *
   * Format is "sha256:abcdef123456...".
   */
  version: string;
}

/** The runtime detail of PipelineJob. */
export interface PipelineJobDetail {
  /** Output only. The context of the pipeline. */
  pipelineContext:
    | Context
    | undefined;
  /** Output only. The context of the current pipeline run. */
  pipelineRunContext:
    | Context
    | undefined;
  /** Output only. The runtime details of the tasks under the pipeline. */
  taskDetails: PipelineTaskDetail[];
}

/** The runtime detail of a task execution. */
export interface PipelineTaskDetail {
  /** Output only. The system generated ID of the task. */
  taskId: Long;
  /**
   * Output only. The id of the parent task if the task is within a component
   * scope. Empty if the task is at the root level.
   */
  parentTaskId: Long;
  /**
   * Output only. The user specified name of the task that is defined in
   * [pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec].
   */
  taskName: string;
  /** Output only. Task create time. */
  createTime:
    | Date
    | undefined;
  /** Output only. Task start time. */
  startTime:
    | Date
    | undefined;
  /** Output only. Task end time. */
  endTime:
    | Date
    | undefined;
  /** Output only. The detailed execution info. */
  executorDetail:
    | PipelineTaskExecutorDetail
    | undefined;
  /** Output only. State of the task. */
  state: PipelineTaskDetail_State;
  /** Output only. The execution metadata of the task. */
  execution:
    | Execution
    | undefined;
  /**
   * Output only. The error that occurred during task execution.
   * Only populated when the task's state is FAILED or CANCELLED.
   */
  error:
    | Status
    | undefined;
  /**
   * Output only. A list of task status. This field keeps a record of task
   * status evolving over time.
   */
  pipelineTaskStatus: PipelineTaskDetail_PipelineTaskStatus[];
  /** Output only. The runtime input artifacts of the task. */
  inputs: { [key: string]: PipelineTaskDetail_ArtifactList };
  /** Output only. The runtime output artifacts of the task. */
  outputs: { [key: string]: PipelineTaskDetail_ArtifactList };
}

/** Specifies state of TaskExecution */
export enum PipelineTaskDetail_State {
  /** STATE_UNSPECIFIED - Unspecified. */
  STATE_UNSPECIFIED = 0,
  /** PENDING - Specifies pending state for the task. */
  PENDING = 1,
  /** RUNNING - Specifies task is being executed. */
  RUNNING = 2,
  /** SUCCEEDED - Specifies task completed successfully. */
  SUCCEEDED = 3,
  /** CANCEL_PENDING - Specifies Task cancel is in pending state. */
  CANCEL_PENDING = 4,
  /** CANCELLING - Specifies task is being cancelled. */
  CANCELLING = 5,
  /** CANCELLED - Specifies task was cancelled. */
  CANCELLED = 6,
  /** FAILED - Specifies task failed. */
  FAILED = 7,
  /** SKIPPED - Specifies task was skipped due to cache hit. */
  SKIPPED = 8,
  /**
   * NOT_TRIGGERED - Specifies that the task was not triggered because the task's trigger
   * policy is not satisfied. The trigger policy is specified in the
   * `condition` field of
   * [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec].
   */
  NOT_TRIGGERED = 9,
  UNRECOGNIZED = -1,
}

export function pipelineTaskDetail_StateFromJSON(object: any): PipelineTaskDetail_State {
  switch (object) {
    case 0:
    case "STATE_UNSPECIFIED":
      return PipelineTaskDetail_State.STATE_UNSPECIFIED;
    case 1:
    case "PENDING":
      return PipelineTaskDetail_State.PENDING;
    case 2:
    case "RUNNING":
      return PipelineTaskDetail_State.RUNNING;
    case 3:
    case "SUCCEEDED":
      return PipelineTaskDetail_State.SUCCEEDED;
    case 4:
    case "CANCEL_PENDING":
      return PipelineTaskDetail_State.CANCEL_PENDING;
    case 5:
    case "CANCELLING":
      return PipelineTaskDetail_State.CANCELLING;
    case 6:
    case "CANCELLED":
      return PipelineTaskDetail_State.CANCELLED;
    case 7:
    case "FAILED":
      return PipelineTaskDetail_State.FAILED;
    case 8:
    case "SKIPPED":
      return PipelineTaskDetail_State.SKIPPED;
    case 9:
    case "NOT_TRIGGERED":
      return PipelineTaskDetail_State.NOT_TRIGGERED;
    case -1:
    case "UNRECOGNIZED":
    default:
      return PipelineTaskDetail_State.UNRECOGNIZED;
  }
}

export function pipelineTaskDetail_StateToJSON(object: PipelineTaskDetail_State): string {
  switch (object) {
    case PipelineTaskDetail_State.STATE_UNSPECIFIED:
      return "STATE_UNSPECIFIED";
    case PipelineTaskDetail_State.PENDING:
      return "PENDING";
    case PipelineTaskDetail_State.RUNNING:
      return "RUNNING";
    case PipelineTaskDetail_State.SUCCEEDED:
      return "SUCCEEDED";
    case PipelineTaskDetail_State.CANCEL_PENDING:
      return "CANCEL_PENDING";
    case PipelineTaskDetail_State.CANCELLING:
      return "CANCELLING";
    case PipelineTaskDetail_State.CANCELLED:
      return "CANCELLED";
    case PipelineTaskDetail_State.FAILED:
      return "FAILED";
    case PipelineTaskDetail_State.SKIPPED:
      return "SKIPPED";
    case PipelineTaskDetail_State.NOT_TRIGGERED:
      return "NOT_TRIGGERED";
    case PipelineTaskDetail_State.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** A single record of the task status. */
export interface PipelineTaskDetail_PipelineTaskStatus {
  /** Output only. Update time of this status. */
  updateTime:
    | Date
    | undefined;
  /** Output only. The state of the task. */
  state: PipelineTaskDetail_State;
  /**
   * Output only. The error that occurred during the state. May be set when
   * the state is any of the non-final state (PENDING/RUNNING/CANCELLING) or
   * FAILED state. If the state is FAILED, the error here is final and not
   * going to be retried. If the state is a non-final state, the error
   * indicates a system-error being retried.
   */
  error: Status | undefined;
}

/** A list of artifact metadata. */
export interface PipelineTaskDetail_ArtifactList {
  /** Output only. A list of artifact metadata. */
  artifacts: Artifact[];
}

export interface PipelineTaskDetail_InputsEntry {
  key: string;
  value: PipelineTaskDetail_ArtifactList | undefined;
}

export interface PipelineTaskDetail_OutputsEntry {
  key: string;
  value: PipelineTaskDetail_ArtifactList | undefined;
}

/** The runtime detail of a pipeline executor. */
export interface PipelineTaskExecutorDetail {
  /** Output only. The detailed info for a container executor. */
  containerDetail?:
    | PipelineTaskExecutorDetail_ContainerDetail
    | undefined;
  /** Output only. The detailed info for a custom job executor. */
  customJobDetail?: PipelineTaskExecutorDetail_CustomJobDetail | undefined;
}

/**
 * The detail of a container execution. It contains the job names of the
 * lifecycle of a container execution.
 */
export interface PipelineTaskExecutorDetail_ContainerDetail {
  /**
   * Output only. The name of the
   * [CustomJob][google.cloud.aiplatform.v1.CustomJob] for the main container
   * execution.
   */
  mainJob: string;
  /**
   * Output only. The name of the
   * [CustomJob][google.cloud.aiplatform.v1.CustomJob] for the
   * pre-caching-check container execution. This job will be available if the
   * [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec]
   * specifies the `pre_caching_check` hook in the lifecycle events.
   */
  preCachingCheckJob: string;
  /**
   * Output only. The names of the previously failed
   * [CustomJob][google.cloud.aiplatform.v1.CustomJob] for the main container
   * executions. The list includes the all attempts in chronological order.
   */
  failedMainJobs: string[];
  /**
   * Output only. The names of the previously failed
   * [CustomJob][google.cloud.aiplatform.v1.CustomJob] for the
   * pre-caching-check container executions. This job will be available if the
   * [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec]
   * specifies the `pre_caching_check` hook in the lifecycle events. The list
   * includes the all attempts in chronological order.
   */
  failedPreCachingCheckJobs: string[];
}

/** The detailed info for a custom job executor. */
export interface PipelineTaskExecutorDetail_CustomJobDetail {
  /**
   * Output only. The name of the
   * [CustomJob][google.cloud.aiplatform.v1.CustomJob].
   */
  job: string;
  /**
   * Output only. The names of the previously failed
   * [CustomJob][google.cloud.aiplatform.v1.CustomJob]. The list includes the
   * all attempts in chronological order.
   */
  failedJobs: string[];
}

function createBasePipelineJob(): PipelineJob {
  return {
    name: "",
    displayName: "",
    createTime: undefined,
    startTime: undefined,
    endTime: undefined,
    updateTime: undefined,
    pipelineSpec: undefined,
    state: 0,
    jobDetail: undefined,
    error: undefined,
    labels: {},
    runtimeConfig: undefined,
    encryptionSpec: undefined,
    serviceAccount: "",
    network: "",
    reservedIpRanges: [],
    templateUri: "",
    templateMetadata: undefined,
    scheduleName: "",
    preflightValidations: false,
  };
}

export const PipelineJob: MessageFns<PipelineJob> = {
  encode(message: PipelineJob, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.displayName !== "") {
      writer.uint32(18).string(message.displayName);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(26).fork()).join();
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(34).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(42).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(50).fork()).join();
    }
    if (message.pipelineSpec !== undefined) {
      Struct.encode(Struct.wrap(message.pipelineSpec), writer.uint32(58).fork()).join();
    }
    if (message.state !== 0) {
      writer.uint32(64).int32(message.state);
    }
    if (message.jobDetail !== undefined) {
      PipelineJobDetail.encode(message.jobDetail, writer.uint32(74).fork()).join();
    }
    if (message.error !== undefined) {
      Status.encode(message.error, writer.uint32(82).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      PipelineJob_LabelsEntry.encode({ key: key as any, value }, writer.uint32(90).fork()).join();
    });
    if (message.runtimeConfig !== undefined) {
      PipelineJob_RuntimeConfig.encode(message.runtimeConfig, writer.uint32(98).fork()).join();
    }
    if (message.encryptionSpec !== undefined) {
      EncryptionSpec.encode(message.encryptionSpec, writer.uint32(130).fork()).join();
    }
    if (message.serviceAccount !== "") {
      writer.uint32(138).string(message.serviceAccount);
    }
    if (message.network !== "") {
      writer.uint32(146).string(message.network);
    }
    for (const v of message.reservedIpRanges) {
      writer.uint32(202).string(v!);
    }
    if (message.templateUri !== "") {
      writer.uint32(154).string(message.templateUri);
    }
    if (message.templateMetadata !== undefined) {
      PipelineTemplateMetadata.encode(message.templateMetadata, writer.uint32(162).fork()).join();
    }
    if (message.scheduleName !== "") {
      writer.uint32(178).string(message.scheduleName);
    }
    if (message.preflightValidations !== false) {
      writer.uint32(208).bool(message.preflightValidations);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineJob {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.pipelineSpec = Struct.unwrap(Struct.decode(reader, reader.uint32()));
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.jobDetail = PipelineJobDetail.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.error = Status.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          const entry11 = PipelineJob_LabelsEntry.decode(reader, reader.uint32());
          if (entry11.value !== undefined) {
            message.labels[entry11.key] = entry11.value;
          }
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.runtimeConfig = PipelineJob_RuntimeConfig.decode(reader, reader.uint32());
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.encryptionSpec = EncryptionSpec.decode(reader, reader.uint32());
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.serviceAccount = reader.string();
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.network = reader.string();
          continue;
        case 25:
          if (tag !== 202) {
            break;
          }

          message.reservedIpRanges.push(reader.string());
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.templateUri = reader.string();
          continue;
        case 20:
          if (tag !== 162) {
            break;
          }

          message.templateMetadata = PipelineTemplateMetadata.decode(reader, reader.uint32());
          continue;
        case 22:
          if (tag !== 178) {
            break;
          }

          message.scheduleName = reader.string();
          continue;
        case 26:
          if (tag !== 208) {
            break;
          }

          message.preflightValidations = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineJob {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      pipelineSpec: isObject(object.pipelineSpec) ? object.pipelineSpec : undefined,
      state: isSet(object.state) ? pipelineStateFromJSON(object.state) : 0,
      jobDetail: isSet(object.jobDetail) ? PipelineJobDetail.fromJSON(object.jobDetail) : undefined,
      error: isSet(object.error) ? Status.fromJSON(object.error) : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      runtimeConfig: isSet(object.runtimeConfig) ? PipelineJob_RuntimeConfig.fromJSON(object.runtimeConfig) : undefined,
      encryptionSpec: isSet(object.encryptionSpec) ? EncryptionSpec.fromJSON(object.encryptionSpec) : undefined,
      serviceAccount: isSet(object.serviceAccount) ? globalThis.String(object.serviceAccount) : "",
      network: isSet(object.network) ? globalThis.String(object.network) : "",
      reservedIpRanges: globalThis.Array.isArray(object?.reservedIpRanges)
        ? object.reservedIpRanges.map((e: any) => globalThis.String(e))
        : [],
      templateUri: isSet(object.templateUri) ? globalThis.String(object.templateUri) : "",
      templateMetadata: isSet(object.templateMetadata)
        ? PipelineTemplateMetadata.fromJSON(object.templateMetadata)
        : undefined,
      scheduleName: isSet(object.scheduleName) ? globalThis.String(object.scheduleName) : "",
      preflightValidations: isSet(object.preflightValidations)
        ? globalThis.Boolean(object.preflightValidations)
        : false,
    };
  },

  toJSON(message: PipelineJob): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (message.pipelineSpec !== undefined) {
      obj.pipelineSpec = message.pipelineSpec;
    }
    if (message.state !== 0) {
      obj.state = pipelineStateToJSON(message.state);
    }
    if (message.jobDetail !== undefined) {
      obj.jobDetail = PipelineJobDetail.toJSON(message.jobDetail);
    }
    if (message.error !== undefined) {
      obj.error = Status.toJSON(message.error);
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.runtimeConfig !== undefined) {
      obj.runtimeConfig = PipelineJob_RuntimeConfig.toJSON(message.runtimeConfig);
    }
    if (message.encryptionSpec !== undefined) {
      obj.encryptionSpec = EncryptionSpec.toJSON(message.encryptionSpec);
    }
    if (message.serviceAccount !== "") {
      obj.serviceAccount = message.serviceAccount;
    }
    if (message.network !== "") {
      obj.network = message.network;
    }
    if (message.reservedIpRanges?.length) {
      obj.reservedIpRanges = message.reservedIpRanges;
    }
    if (message.templateUri !== "") {
      obj.templateUri = message.templateUri;
    }
    if (message.templateMetadata !== undefined) {
      obj.templateMetadata = PipelineTemplateMetadata.toJSON(message.templateMetadata);
    }
    if (message.scheduleName !== "") {
      obj.scheduleName = message.scheduleName;
    }
    if (message.preflightValidations !== false) {
      obj.preflightValidations = message.preflightValidations;
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineJob>): PipelineJob {
    return PipelineJob.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineJob>): PipelineJob {
    const message = createBasePipelineJob();
    message.name = object.name ?? "";
    message.displayName = object.displayName ?? "";
    message.createTime = object.createTime ?? undefined;
    message.startTime = object.startTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.pipelineSpec = object.pipelineSpec ?? undefined;
    message.state = object.state ?? 0;
    message.jobDetail = (object.jobDetail !== undefined && object.jobDetail !== null)
      ? PipelineJobDetail.fromPartial(object.jobDetail)
      : undefined;
    message.error = (object.error !== undefined && object.error !== null)
      ? Status.fromPartial(object.error)
      : undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.runtimeConfig = (object.runtimeConfig !== undefined && object.runtimeConfig !== null)
      ? PipelineJob_RuntimeConfig.fromPartial(object.runtimeConfig)
      : undefined;
    message.encryptionSpec = (object.encryptionSpec !== undefined && object.encryptionSpec !== null)
      ? EncryptionSpec.fromPartial(object.encryptionSpec)
      : undefined;
    message.serviceAccount = object.serviceAccount ?? "";
    message.network = object.network ?? "";
    message.reservedIpRanges = object.reservedIpRanges?.map((e) => e) || [];
    message.templateUri = object.templateUri ?? "";
    message.templateMetadata = (object.templateMetadata !== undefined && object.templateMetadata !== null)
      ? PipelineTemplateMetadata.fromPartial(object.templateMetadata)
      : undefined;
    message.scheduleName = object.scheduleName ?? "";
    message.preflightValidations = object.preflightValidations ?? false;
    return message;
  },
};

function createBasePipelineJob_RuntimeConfig(): PipelineJob_RuntimeConfig {
  return { parameters: {}, gcsOutputDirectory: "", parameterValues: {}, failurePolicy: 0, inputArtifacts: {} };
}

export const PipelineJob_RuntimeConfig: MessageFns<PipelineJob_RuntimeConfig> = {
  encode(message: PipelineJob_RuntimeConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.parameters).forEach(([key, value]) => {
      PipelineJob_RuntimeConfig_ParametersEntry.encode({ key: key as any, value }, writer.uint32(10).fork()).join();
    });
    if (message.gcsOutputDirectory !== "") {
      writer.uint32(18).string(message.gcsOutputDirectory);
    }
    Object.entries(message.parameterValues).forEach(([key, value]) => {
      if (value !== undefined) {
        PipelineJob_RuntimeConfig_ParameterValuesEntry.encode({ key: key as any, value }, writer.uint32(26).fork())
          .join();
      }
    });
    if (message.failurePolicy !== 0) {
      writer.uint32(32).int32(message.failurePolicy);
    }
    Object.entries(message.inputArtifacts).forEach(([key, value]) => {
      PipelineJob_RuntimeConfig_InputArtifactsEntry.encode({ key: key as any, value }, writer.uint32(42).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineJob_RuntimeConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineJob_RuntimeConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          const entry1 = PipelineJob_RuntimeConfig_ParametersEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.parameters[entry1.key] = entry1.value;
          }
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.gcsOutputDirectory = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          const entry3 = PipelineJob_RuntimeConfig_ParameterValuesEntry.decode(reader, reader.uint32());
          if (entry3.value !== undefined) {
            message.parameterValues[entry3.key] = entry3.value;
          }
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.failurePolicy = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          const entry5 = PipelineJob_RuntimeConfig_InputArtifactsEntry.decode(reader, reader.uint32());
          if (entry5.value !== undefined) {
            message.inputArtifacts[entry5.key] = entry5.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineJob_RuntimeConfig {
    return {
      parameters: isObject(object.parameters)
        ? Object.entries(object.parameters).reduce<{ [key: string]: Value }>((acc, [key, value]) => {
          acc[key] = Value.fromJSON(value);
          return acc;
        }, {})
        : {},
      gcsOutputDirectory: isSet(object.gcsOutputDirectory) ? globalThis.String(object.gcsOutputDirectory) : "",
      parameterValues: isObject(object.parameterValues)
        ? Object.entries(object.parameterValues).reduce<{ [key: string]: any | undefined }>((acc, [key, value]) => {
          acc[key] = value as any | undefined;
          return acc;
        }, {})
        : {},
      failurePolicy: isSet(object.failurePolicy) ? pipelineFailurePolicyFromJSON(object.failurePolicy) : 0,
      inputArtifacts: isObject(object.inputArtifacts)
        ? Object.entries(object.inputArtifacts).reduce<{ [key: string]: PipelineJob_RuntimeConfig_InputArtifact }>(
          (acc, [key, value]) => {
            acc[key] = PipelineJob_RuntimeConfig_InputArtifact.fromJSON(value);
            return acc;
          },
          {},
        )
        : {},
    };
  },

  toJSON(message: PipelineJob_RuntimeConfig): unknown {
    const obj: any = {};
    if (message.parameters) {
      const entries = Object.entries(message.parameters);
      if (entries.length > 0) {
        obj.parameters = {};
        entries.forEach(([k, v]) => {
          obj.parameters[k] = Value.toJSON(v);
        });
      }
    }
    if (message.gcsOutputDirectory !== "") {
      obj.gcsOutputDirectory = message.gcsOutputDirectory;
    }
    if (message.parameterValues) {
      const entries = Object.entries(message.parameterValues);
      if (entries.length > 0) {
        obj.parameterValues = {};
        entries.forEach(([k, v]) => {
          obj.parameterValues[k] = v;
        });
      }
    }
    if (message.failurePolicy !== 0) {
      obj.failurePolicy = pipelineFailurePolicyToJSON(message.failurePolicy);
    }
    if (message.inputArtifacts) {
      const entries = Object.entries(message.inputArtifacts);
      if (entries.length > 0) {
        obj.inputArtifacts = {};
        entries.forEach(([k, v]) => {
          obj.inputArtifacts[k] = PipelineJob_RuntimeConfig_InputArtifact.toJSON(v);
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineJob_RuntimeConfig>): PipelineJob_RuntimeConfig {
    return PipelineJob_RuntimeConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineJob_RuntimeConfig>): PipelineJob_RuntimeConfig {
    const message = createBasePipelineJob_RuntimeConfig();
    message.parameters = Object.entries(object.parameters ?? {}).reduce<{ [key: string]: Value }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = Value.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    message.gcsOutputDirectory = object.gcsOutputDirectory ?? "";
    message.parameterValues = Object.entries(object.parameterValues ?? {}).reduce<{ [key: string]: any | undefined }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = value;
        }
        return acc;
      },
      {},
    );
    message.failurePolicy = object.failurePolicy ?? 0;
    message.inputArtifacts = Object.entries(object.inputArtifacts ?? {}).reduce<
      { [key: string]: PipelineJob_RuntimeConfig_InputArtifact }
    >((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = PipelineJob_RuntimeConfig_InputArtifact.fromPartial(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBasePipelineJob_RuntimeConfig_InputArtifact(): PipelineJob_RuntimeConfig_InputArtifact {
  return { artifactId: undefined };
}

export const PipelineJob_RuntimeConfig_InputArtifact: MessageFns<PipelineJob_RuntimeConfig_InputArtifact> = {
  encode(message: PipelineJob_RuntimeConfig_InputArtifact, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.artifactId !== undefined) {
      writer.uint32(10).string(message.artifactId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineJob_RuntimeConfig_InputArtifact {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineJob_RuntimeConfig_InputArtifact();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.artifactId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineJob_RuntimeConfig_InputArtifact {
    return { artifactId: isSet(object.artifactId) ? globalThis.String(object.artifactId) : undefined };
  },

  toJSON(message: PipelineJob_RuntimeConfig_InputArtifact): unknown {
    const obj: any = {};
    if (message.artifactId !== undefined) {
      obj.artifactId = message.artifactId;
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineJob_RuntimeConfig_InputArtifact>): PipelineJob_RuntimeConfig_InputArtifact {
    return PipelineJob_RuntimeConfig_InputArtifact.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineJob_RuntimeConfig_InputArtifact>): PipelineJob_RuntimeConfig_InputArtifact {
    const message = createBasePipelineJob_RuntimeConfig_InputArtifact();
    message.artifactId = object.artifactId ?? undefined;
    return message;
  },
};

function createBasePipelineJob_RuntimeConfig_ParametersEntry(): PipelineJob_RuntimeConfig_ParametersEntry {
  return { key: "", value: undefined };
}

export const PipelineJob_RuntimeConfig_ParametersEntry: MessageFns<PipelineJob_RuntimeConfig_ParametersEntry> = {
  encode(message: PipelineJob_RuntimeConfig_ParametersEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      Value.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineJob_RuntimeConfig_ParametersEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineJob_RuntimeConfig_ParametersEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = Value.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineJob_RuntimeConfig_ParametersEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? Value.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: PipelineJob_RuntimeConfig_ParametersEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = Value.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineJob_RuntimeConfig_ParametersEntry>): PipelineJob_RuntimeConfig_ParametersEntry {
    return PipelineJob_RuntimeConfig_ParametersEntry.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<PipelineJob_RuntimeConfig_ParametersEntry>,
  ): PipelineJob_RuntimeConfig_ParametersEntry {
    const message = createBasePipelineJob_RuntimeConfig_ParametersEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null) ? Value.fromPartial(object.value) : undefined;
    return message;
  },
};

function createBasePipelineJob_RuntimeConfig_ParameterValuesEntry(): PipelineJob_RuntimeConfig_ParameterValuesEntry {
  return { key: "", value: undefined };
}

export const PipelineJob_RuntimeConfig_ParameterValuesEntry: MessageFns<
  PipelineJob_RuntimeConfig_ParameterValuesEntry
> = {
  encode(
    message: PipelineJob_RuntimeConfig_ParameterValuesEntry,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      Value1.encode(Value1.wrap(message.value), writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineJob_RuntimeConfig_ParameterValuesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineJob_RuntimeConfig_ParameterValuesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = Value1.unwrap(Value1.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineJob_RuntimeConfig_ParameterValuesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object?.value) ? object.value : undefined,
    };
  },

  toJSON(message: PipelineJob_RuntimeConfig_ParameterValuesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = message.value;
    }
    return obj;
  },

  create(
    base?: DeepPartial<PipelineJob_RuntimeConfig_ParameterValuesEntry>,
  ): PipelineJob_RuntimeConfig_ParameterValuesEntry {
    return PipelineJob_RuntimeConfig_ParameterValuesEntry.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<PipelineJob_RuntimeConfig_ParameterValuesEntry>,
  ): PipelineJob_RuntimeConfig_ParameterValuesEntry {
    const message = createBasePipelineJob_RuntimeConfig_ParameterValuesEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? undefined;
    return message;
  },
};

function createBasePipelineJob_RuntimeConfig_InputArtifactsEntry(): PipelineJob_RuntimeConfig_InputArtifactsEntry {
  return { key: "", value: undefined };
}

export const PipelineJob_RuntimeConfig_InputArtifactsEntry: MessageFns<PipelineJob_RuntimeConfig_InputArtifactsEntry> =
  {
    encode(
      message: PipelineJob_RuntimeConfig_InputArtifactsEntry,
      writer: BinaryWriter = new BinaryWriter(),
    ): BinaryWriter {
      if (message.key !== "") {
        writer.uint32(10).string(message.key);
      }
      if (message.value !== undefined) {
        PipelineJob_RuntimeConfig_InputArtifact.encode(message.value, writer.uint32(18).fork()).join();
      }
      return writer;
    },

    decode(input: BinaryReader | Uint8Array, length?: number): PipelineJob_RuntimeConfig_InputArtifactsEntry {
      const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
      let end = length === undefined ? reader.len : reader.pos + length;
      const message = createBasePipelineJob_RuntimeConfig_InputArtifactsEntry();
      while (reader.pos < end) {
        const tag = reader.uint32();
        switch (tag >>> 3) {
          case 1:
            if (tag !== 10) {
              break;
            }

            message.key = reader.string();
            continue;
          case 2:
            if (tag !== 18) {
              break;
            }

            message.value = PipelineJob_RuntimeConfig_InputArtifact.decode(reader, reader.uint32());
            continue;
        }
        if ((tag & 7) === 4 || tag === 0) {
          break;
        }
        reader.skip(tag & 7);
      }
      return message;
    },

    fromJSON(object: any): PipelineJob_RuntimeConfig_InputArtifactsEntry {
      return {
        key: isSet(object.key) ? globalThis.String(object.key) : "",
        value: isSet(object.value) ? PipelineJob_RuntimeConfig_InputArtifact.fromJSON(object.value) : undefined,
      };
    },

    toJSON(message: PipelineJob_RuntimeConfig_InputArtifactsEntry): unknown {
      const obj: any = {};
      if (message.key !== "") {
        obj.key = message.key;
      }
      if (message.value !== undefined) {
        obj.value = PipelineJob_RuntimeConfig_InputArtifact.toJSON(message.value);
      }
      return obj;
    },

    create(
      base?: DeepPartial<PipelineJob_RuntimeConfig_InputArtifactsEntry>,
    ): PipelineJob_RuntimeConfig_InputArtifactsEntry {
      return PipelineJob_RuntimeConfig_InputArtifactsEntry.fromPartial(base ?? {});
    },
    fromPartial(
      object: DeepPartial<PipelineJob_RuntimeConfig_InputArtifactsEntry>,
    ): PipelineJob_RuntimeConfig_InputArtifactsEntry {
      const message = createBasePipelineJob_RuntimeConfig_InputArtifactsEntry();
      message.key = object.key ?? "";
      message.value = (object.value !== undefined && object.value !== null)
        ? PipelineJob_RuntimeConfig_InputArtifact.fromPartial(object.value)
        : undefined;
      return message;
    },
  };

function createBasePipelineJob_LabelsEntry(): PipelineJob_LabelsEntry {
  return { key: "", value: "" };
}

export const PipelineJob_LabelsEntry: MessageFns<PipelineJob_LabelsEntry> = {
  encode(message: PipelineJob_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineJob_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineJob_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineJob_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: PipelineJob_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineJob_LabelsEntry>): PipelineJob_LabelsEntry {
    return PipelineJob_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineJob_LabelsEntry>): PipelineJob_LabelsEntry {
    const message = createBasePipelineJob_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBasePipelineTemplateMetadata(): PipelineTemplateMetadata {
  return { version: "" };
}

export const PipelineTemplateMetadata: MessageFns<PipelineTemplateMetadata> = {
  encode(message: PipelineTemplateMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.version !== "") {
      writer.uint32(26).string(message.version);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineTemplateMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineTemplateMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 26) {
            break;
          }

          message.version = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineTemplateMetadata {
    return { version: isSet(object.version) ? globalThis.String(object.version) : "" };
  },

  toJSON(message: PipelineTemplateMetadata): unknown {
    const obj: any = {};
    if (message.version !== "") {
      obj.version = message.version;
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineTemplateMetadata>): PipelineTemplateMetadata {
    return PipelineTemplateMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineTemplateMetadata>): PipelineTemplateMetadata {
    const message = createBasePipelineTemplateMetadata();
    message.version = object.version ?? "";
    return message;
  },
};

function createBasePipelineJobDetail(): PipelineJobDetail {
  return { pipelineContext: undefined, pipelineRunContext: undefined, taskDetails: [] };
}

export const PipelineJobDetail: MessageFns<PipelineJobDetail> = {
  encode(message: PipelineJobDetail, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.pipelineContext !== undefined) {
      Context.encode(message.pipelineContext, writer.uint32(10).fork()).join();
    }
    if (message.pipelineRunContext !== undefined) {
      Context.encode(message.pipelineRunContext, writer.uint32(18).fork()).join();
    }
    for (const v of message.taskDetails) {
      PipelineTaskDetail.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineJobDetail {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineJobDetail();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.pipelineContext = Context.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.pipelineRunContext = Context.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.taskDetails.push(PipelineTaskDetail.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineJobDetail {
    return {
      pipelineContext: isSet(object.pipelineContext) ? Context.fromJSON(object.pipelineContext) : undefined,
      pipelineRunContext: isSet(object.pipelineRunContext) ? Context.fromJSON(object.pipelineRunContext) : undefined,
      taskDetails: globalThis.Array.isArray(object?.taskDetails)
        ? object.taskDetails.map((e: any) => PipelineTaskDetail.fromJSON(e))
        : [],
    };
  },

  toJSON(message: PipelineJobDetail): unknown {
    const obj: any = {};
    if (message.pipelineContext !== undefined) {
      obj.pipelineContext = Context.toJSON(message.pipelineContext);
    }
    if (message.pipelineRunContext !== undefined) {
      obj.pipelineRunContext = Context.toJSON(message.pipelineRunContext);
    }
    if (message.taskDetails?.length) {
      obj.taskDetails = message.taskDetails.map((e) => PipelineTaskDetail.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineJobDetail>): PipelineJobDetail {
    return PipelineJobDetail.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineJobDetail>): PipelineJobDetail {
    const message = createBasePipelineJobDetail();
    message.pipelineContext = (object.pipelineContext !== undefined && object.pipelineContext !== null)
      ? Context.fromPartial(object.pipelineContext)
      : undefined;
    message.pipelineRunContext = (object.pipelineRunContext !== undefined && object.pipelineRunContext !== null)
      ? Context.fromPartial(object.pipelineRunContext)
      : undefined;
    message.taskDetails = object.taskDetails?.map((e) => PipelineTaskDetail.fromPartial(e)) || [];
    return message;
  },
};

function createBasePipelineTaskDetail(): PipelineTaskDetail {
  return {
    taskId: Long.ZERO,
    parentTaskId: Long.ZERO,
    taskName: "",
    createTime: undefined,
    startTime: undefined,
    endTime: undefined,
    executorDetail: undefined,
    state: 0,
    execution: undefined,
    error: undefined,
    pipelineTaskStatus: [],
    inputs: {},
    outputs: {},
  };
}

export const PipelineTaskDetail: MessageFns<PipelineTaskDetail> = {
  encode(message: PipelineTaskDetail, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.taskId.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.taskId.toString());
    }
    if (!message.parentTaskId.equals(Long.ZERO)) {
      writer.uint32(96).int64(message.parentTaskId.toString());
    }
    if (message.taskName !== "") {
      writer.uint32(18).string(message.taskName);
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(26).fork()).join();
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(34).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(42).fork()).join();
    }
    if (message.executorDetail !== undefined) {
      PipelineTaskExecutorDetail.encode(message.executorDetail, writer.uint32(50).fork()).join();
    }
    if (message.state !== 0) {
      writer.uint32(56).int32(message.state);
    }
    if (message.execution !== undefined) {
      Execution.encode(message.execution, writer.uint32(66).fork()).join();
    }
    if (message.error !== undefined) {
      Status.encode(message.error, writer.uint32(74).fork()).join();
    }
    for (const v of message.pipelineTaskStatus) {
      PipelineTaskDetail_PipelineTaskStatus.encode(v!, writer.uint32(106).fork()).join();
    }
    Object.entries(message.inputs).forEach(([key, value]) => {
      PipelineTaskDetail_InputsEntry.encode({ key: key as any, value }, writer.uint32(82).fork()).join();
    });
    Object.entries(message.outputs).forEach(([key, value]) => {
      PipelineTaskDetail_OutputsEntry.encode({ key: key as any, value }, writer.uint32(90).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineTaskDetail {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineTaskDetail();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.taskId = Long.fromString(reader.int64().toString());
          continue;
        case 12:
          if (tag !== 96) {
            break;
          }

          message.parentTaskId = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.taskName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.executorDetail = PipelineTaskExecutorDetail.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.execution = Execution.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.error = Status.decode(reader, reader.uint32());
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.pipelineTaskStatus.push(PipelineTaskDetail_PipelineTaskStatus.decode(reader, reader.uint32()));
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          const entry10 = PipelineTaskDetail_InputsEntry.decode(reader, reader.uint32());
          if (entry10.value !== undefined) {
            message.inputs[entry10.key] = entry10.value;
          }
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          const entry11 = PipelineTaskDetail_OutputsEntry.decode(reader, reader.uint32());
          if (entry11.value !== undefined) {
            message.outputs[entry11.key] = entry11.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineTaskDetail {
    return {
      taskId: isSet(object.taskId) ? Long.fromValue(object.taskId) : Long.ZERO,
      parentTaskId: isSet(object.parentTaskId) ? Long.fromValue(object.parentTaskId) : Long.ZERO,
      taskName: isSet(object.taskName) ? globalThis.String(object.taskName) : "",
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      executorDetail: isSet(object.executorDetail)
        ? PipelineTaskExecutorDetail.fromJSON(object.executorDetail)
        : undefined,
      state: isSet(object.state) ? pipelineTaskDetail_StateFromJSON(object.state) : 0,
      execution: isSet(object.execution) ? Execution.fromJSON(object.execution) : undefined,
      error: isSet(object.error) ? Status.fromJSON(object.error) : undefined,
      pipelineTaskStatus: globalThis.Array.isArray(object?.pipelineTaskStatus)
        ? object.pipelineTaskStatus.map((e: any) => PipelineTaskDetail_PipelineTaskStatus.fromJSON(e))
        : [],
      inputs: isObject(object.inputs)
        ? Object.entries(object.inputs).reduce<{ [key: string]: PipelineTaskDetail_ArtifactList }>(
          (acc, [key, value]) => {
            acc[key] = PipelineTaskDetail_ArtifactList.fromJSON(value);
            return acc;
          },
          {},
        )
        : {},
      outputs: isObject(object.outputs)
        ? Object.entries(object.outputs).reduce<{ [key: string]: PipelineTaskDetail_ArtifactList }>(
          (acc, [key, value]) => {
            acc[key] = PipelineTaskDetail_ArtifactList.fromJSON(value);
            return acc;
          },
          {},
        )
        : {},
    };
  },

  toJSON(message: PipelineTaskDetail): unknown {
    const obj: any = {};
    if (!message.taskId.equals(Long.ZERO)) {
      obj.taskId = (message.taskId || Long.ZERO).toString();
    }
    if (!message.parentTaskId.equals(Long.ZERO)) {
      obj.parentTaskId = (message.parentTaskId || Long.ZERO).toString();
    }
    if (message.taskName !== "") {
      obj.taskName = message.taskName;
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (message.executorDetail !== undefined) {
      obj.executorDetail = PipelineTaskExecutorDetail.toJSON(message.executorDetail);
    }
    if (message.state !== 0) {
      obj.state = pipelineTaskDetail_StateToJSON(message.state);
    }
    if (message.execution !== undefined) {
      obj.execution = Execution.toJSON(message.execution);
    }
    if (message.error !== undefined) {
      obj.error = Status.toJSON(message.error);
    }
    if (message.pipelineTaskStatus?.length) {
      obj.pipelineTaskStatus = message.pipelineTaskStatus.map((e) => PipelineTaskDetail_PipelineTaskStatus.toJSON(e));
    }
    if (message.inputs) {
      const entries = Object.entries(message.inputs);
      if (entries.length > 0) {
        obj.inputs = {};
        entries.forEach(([k, v]) => {
          obj.inputs[k] = PipelineTaskDetail_ArtifactList.toJSON(v);
        });
      }
    }
    if (message.outputs) {
      const entries = Object.entries(message.outputs);
      if (entries.length > 0) {
        obj.outputs = {};
        entries.forEach(([k, v]) => {
          obj.outputs[k] = PipelineTaskDetail_ArtifactList.toJSON(v);
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineTaskDetail>): PipelineTaskDetail {
    return PipelineTaskDetail.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineTaskDetail>): PipelineTaskDetail {
    const message = createBasePipelineTaskDetail();
    message.taskId = (object.taskId !== undefined && object.taskId !== null)
      ? Long.fromValue(object.taskId)
      : Long.ZERO;
    message.parentTaskId = (object.parentTaskId !== undefined && object.parentTaskId !== null)
      ? Long.fromValue(object.parentTaskId)
      : Long.ZERO;
    message.taskName = object.taskName ?? "";
    message.createTime = object.createTime ?? undefined;
    message.startTime = object.startTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    message.executorDetail = (object.executorDetail !== undefined && object.executorDetail !== null)
      ? PipelineTaskExecutorDetail.fromPartial(object.executorDetail)
      : undefined;
    message.state = object.state ?? 0;
    message.execution = (object.execution !== undefined && object.execution !== null)
      ? Execution.fromPartial(object.execution)
      : undefined;
    message.error = (object.error !== undefined && object.error !== null)
      ? Status.fromPartial(object.error)
      : undefined;
    message.pipelineTaskStatus =
      object.pipelineTaskStatus?.map((e) => PipelineTaskDetail_PipelineTaskStatus.fromPartial(e)) || [];
    message.inputs = Object.entries(object.inputs ?? {}).reduce<{ [key: string]: PipelineTaskDetail_ArtifactList }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = PipelineTaskDetail_ArtifactList.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    message.outputs = Object.entries(object.outputs ?? {}).reduce<{ [key: string]: PipelineTaskDetail_ArtifactList }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = PipelineTaskDetail_ArtifactList.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBasePipelineTaskDetail_PipelineTaskStatus(): PipelineTaskDetail_PipelineTaskStatus {
  return { updateTime: undefined, state: 0, error: undefined };
}

export const PipelineTaskDetail_PipelineTaskStatus: MessageFns<PipelineTaskDetail_PipelineTaskStatus> = {
  encode(message: PipelineTaskDetail_PipelineTaskStatus, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(10).fork()).join();
    }
    if (message.state !== 0) {
      writer.uint32(16).int32(message.state);
    }
    if (message.error !== undefined) {
      Status.encode(message.error, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineTaskDetail_PipelineTaskStatus {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineTaskDetail_PipelineTaskStatus();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.error = Status.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineTaskDetail_PipelineTaskStatus {
    return {
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      state: isSet(object.state) ? pipelineTaskDetail_StateFromJSON(object.state) : 0,
      error: isSet(object.error) ? Status.fromJSON(object.error) : undefined,
    };
  },

  toJSON(message: PipelineTaskDetail_PipelineTaskStatus): unknown {
    const obj: any = {};
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (message.state !== 0) {
      obj.state = pipelineTaskDetail_StateToJSON(message.state);
    }
    if (message.error !== undefined) {
      obj.error = Status.toJSON(message.error);
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineTaskDetail_PipelineTaskStatus>): PipelineTaskDetail_PipelineTaskStatus {
    return PipelineTaskDetail_PipelineTaskStatus.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineTaskDetail_PipelineTaskStatus>): PipelineTaskDetail_PipelineTaskStatus {
    const message = createBasePipelineTaskDetail_PipelineTaskStatus();
    message.updateTime = object.updateTime ?? undefined;
    message.state = object.state ?? 0;
    message.error = (object.error !== undefined && object.error !== null)
      ? Status.fromPartial(object.error)
      : undefined;
    return message;
  },
};

function createBasePipelineTaskDetail_ArtifactList(): PipelineTaskDetail_ArtifactList {
  return { artifacts: [] };
}

export const PipelineTaskDetail_ArtifactList: MessageFns<PipelineTaskDetail_ArtifactList> = {
  encode(message: PipelineTaskDetail_ArtifactList, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.artifacts) {
      Artifact.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineTaskDetail_ArtifactList {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineTaskDetail_ArtifactList();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.artifacts.push(Artifact.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineTaskDetail_ArtifactList {
    return {
      artifacts: globalThis.Array.isArray(object?.artifacts)
        ? object.artifacts.map((e: any) => Artifact.fromJSON(e))
        : [],
    };
  },

  toJSON(message: PipelineTaskDetail_ArtifactList): unknown {
    const obj: any = {};
    if (message.artifacts?.length) {
      obj.artifacts = message.artifacts.map((e) => Artifact.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineTaskDetail_ArtifactList>): PipelineTaskDetail_ArtifactList {
    return PipelineTaskDetail_ArtifactList.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineTaskDetail_ArtifactList>): PipelineTaskDetail_ArtifactList {
    const message = createBasePipelineTaskDetail_ArtifactList();
    message.artifacts = object.artifacts?.map((e) => Artifact.fromPartial(e)) || [];
    return message;
  },
};

function createBasePipelineTaskDetail_InputsEntry(): PipelineTaskDetail_InputsEntry {
  return { key: "", value: undefined };
}

export const PipelineTaskDetail_InputsEntry: MessageFns<PipelineTaskDetail_InputsEntry> = {
  encode(message: PipelineTaskDetail_InputsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      PipelineTaskDetail_ArtifactList.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineTaskDetail_InputsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineTaskDetail_InputsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = PipelineTaskDetail_ArtifactList.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineTaskDetail_InputsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? PipelineTaskDetail_ArtifactList.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: PipelineTaskDetail_InputsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = PipelineTaskDetail_ArtifactList.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineTaskDetail_InputsEntry>): PipelineTaskDetail_InputsEntry {
    return PipelineTaskDetail_InputsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineTaskDetail_InputsEntry>): PipelineTaskDetail_InputsEntry {
    const message = createBasePipelineTaskDetail_InputsEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? PipelineTaskDetail_ArtifactList.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBasePipelineTaskDetail_OutputsEntry(): PipelineTaskDetail_OutputsEntry {
  return { key: "", value: undefined };
}

export const PipelineTaskDetail_OutputsEntry: MessageFns<PipelineTaskDetail_OutputsEntry> = {
  encode(message: PipelineTaskDetail_OutputsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      PipelineTaskDetail_ArtifactList.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineTaskDetail_OutputsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineTaskDetail_OutputsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = PipelineTaskDetail_ArtifactList.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineTaskDetail_OutputsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? PipelineTaskDetail_ArtifactList.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: PipelineTaskDetail_OutputsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = PipelineTaskDetail_ArtifactList.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineTaskDetail_OutputsEntry>): PipelineTaskDetail_OutputsEntry {
    return PipelineTaskDetail_OutputsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineTaskDetail_OutputsEntry>): PipelineTaskDetail_OutputsEntry {
    const message = createBasePipelineTaskDetail_OutputsEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null)
      ? PipelineTaskDetail_ArtifactList.fromPartial(object.value)
      : undefined;
    return message;
  },
};

function createBasePipelineTaskExecutorDetail(): PipelineTaskExecutorDetail {
  return { containerDetail: undefined, customJobDetail: undefined };
}

export const PipelineTaskExecutorDetail: MessageFns<PipelineTaskExecutorDetail> = {
  encode(message: PipelineTaskExecutorDetail, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.containerDetail !== undefined) {
      PipelineTaskExecutorDetail_ContainerDetail.encode(message.containerDetail, writer.uint32(10).fork()).join();
    }
    if (message.customJobDetail !== undefined) {
      PipelineTaskExecutorDetail_CustomJobDetail.encode(message.customJobDetail, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineTaskExecutorDetail {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineTaskExecutorDetail();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.containerDetail = PipelineTaskExecutorDetail_ContainerDetail.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.customJobDetail = PipelineTaskExecutorDetail_CustomJobDetail.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineTaskExecutorDetail {
    return {
      containerDetail: isSet(object.containerDetail)
        ? PipelineTaskExecutorDetail_ContainerDetail.fromJSON(object.containerDetail)
        : undefined,
      customJobDetail: isSet(object.customJobDetail)
        ? PipelineTaskExecutorDetail_CustomJobDetail.fromJSON(object.customJobDetail)
        : undefined,
    };
  },

  toJSON(message: PipelineTaskExecutorDetail): unknown {
    const obj: any = {};
    if (message.containerDetail !== undefined) {
      obj.containerDetail = PipelineTaskExecutorDetail_ContainerDetail.toJSON(message.containerDetail);
    }
    if (message.customJobDetail !== undefined) {
      obj.customJobDetail = PipelineTaskExecutorDetail_CustomJobDetail.toJSON(message.customJobDetail);
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineTaskExecutorDetail>): PipelineTaskExecutorDetail {
    return PipelineTaskExecutorDetail.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PipelineTaskExecutorDetail>): PipelineTaskExecutorDetail {
    const message = createBasePipelineTaskExecutorDetail();
    message.containerDetail = (object.containerDetail !== undefined && object.containerDetail !== null)
      ? PipelineTaskExecutorDetail_ContainerDetail.fromPartial(object.containerDetail)
      : undefined;
    message.customJobDetail = (object.customJobDetail !== undefined && object.customJobDetail !== null)
      ? PipelineTaskExecutorDetail_CustomJobDetail.fromPartial(object.customJobDetail)
      : undefined;
    return message;
  },
};

function createBasePipelineTaskExecutorDetail_ContainerDetail(): PipelineTaskExecutorDetail_ContainerDetail {
  return { mainJob: "", preCachingCheckJob: "", failedMainJobs: [], failedPreCachingCheckJobs: [] };
}

export const PipelineTaskExecutorDetail_ContainerDetail: MessageFns<PipelineTaskExecutorDetail_ContainerDetail> = {
  encode(message: PipelineTaskExecutorDetail_ContainerDetail, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.mainJob !== "") {
      writer.uint32(10).string(message.mainJob);
    }
    if (message.preCachingCheckJob !== "") {
      writer.uint32(18).string(message.preCachingCheckJob);
    }
    for (const v of message.failedMainJobs) {
      writer.uint32(26).string(v!);
    }
    for (const v of message.failedPreCachingCheckJobs) {
      writer.uint32(34).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineTaskExecutorDetail_ContainerDetail {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineTaskExecutorDetail_ContainerDetail();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.mainJob = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.preCachingCheckJob = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.failedMainJobs.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.failedPreCachingCheckJobs.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineTaskExecutorDetail_ContainerDetail {
    return {
      mainJob: isSet(object.mainJob) ? globalThis.String(object.mainJob) : "",
      preCachingCheckJob: isSet(object.preCachingCheckJob) ? globalThis.String(object.preCachingCheckJob) : "",
      failedMainJobs: globalThis.Array.isArray(object?.failedMainJobs)
        ? object.failedMainJobs.map((e: any) => globalThis.String(e))
        : [],
      failedPreCachingCheckJobs: globalThis.Array.isArray(object?.failedPreCachingCheckJobs)
        ? object.failedPreCachingCheckJobs.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: PipelineTaskExecutorDetail_ContainerDetail): unknown {
    const obj: any = {};
    if (message.mainJob !== "") {
      obj.mainJob = message.mainJob;
    }
    if (message.preCachingCheckJob !== "") {
      obj.preCachingCheckJob = message.preCachingCheckJob;
    }
    if (message.failedMainJobs?.length) {
      obj.failedMainJobs = message.failedMainJobs;
    }
    if (message.failedPreCachingCheckJobs?.length) {
      obj.failedPreCachingCheckJobs = message.failedPreCachingCheckJobs;
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineTaskExecutorDetail_ContainerDetail>): PipelineTaskExecutorDetail_ContainerDetail {
    return PipelineTaskExecutorDetail_ContainerDetail.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<PipelineTaskExecutorDetail_ContainerDetail>,
  ): PipelineTaskExecutorDetail_ContainerDetail {
    const message = createBasePipelineTaskExecutorDetail_ContainerDetail();
    message.mainJob = object.mainJob ?? "";
    message.preCachingCheckJob = object.preCachingCheckJob ?? "";
    message.failedMainJobs = object.failedMainJobs?.map((e) => e) || [];
    message.failedPreCachingCheckJobs = object.failedPreCachingCheckJobs?.map((e) => e) || [];
    return message;
  },
};

function createBasePipelineTaskExecutorDetail_CustomJobDetail(): PipelineTaskExecutorDetail_CustomJobDetail {
  return { job: "", failedJobs: [] };
}

export const PipelineTaskExecutorDetail_CustomJobDetail: MessageFns<PipelineTaskExecutorDetail_CustomJobDetail> = {
  encode(message: PipelineTaskExecutorDetail_CustomJobDetail, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.job !== "") {
      writer.uint32(10).string(message.job);
    }
    for (const v of message.failedJobs) {
      writer.uint32(26).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PipelineTaskExecutorDetail_CustomJobDetail {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePipelineTaskExecutorDetail_CustomJobDetail();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.job = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.failedJobs.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PipelineTaskExecutorDetail_CustomJobDetail {
    return {
      job: isSet(object.job) ? globalThis.String(object.job) : "",
      failedJobs: globalThis.Array.isArray(object?.failedJobs)
        ? object.failedJobs.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: PipelineTaskExecutorDetail_CustomJobDetail): unknown {
    const obj: any = {};
    if (message.job !== "") {
      obj.job = message.job;
    }
    if (message.failedJobs?.length) {
      obj.failedJobs = message.failedJobs;
    }
    return obj;
  },

  create(base?: DeepPartial<PipelineTaskExecutorDetail_CustomJobDetail>): PipelineTaskExecutorDetail_CustomJobDetail {
    return PipelineTaskExecutorDetail_CustomJobDetail.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<PipelineTaskExecutorDetail_CustomJobDetail>,
  ): PipelineTaskExecutorDetail_CustomJobDetail {
    const message = createBasePipelineTaskExecutorDetail_CustomJobDetail();
    message.job = object.job ?? "";
    message.failedJobs = object.failedJobs?.map((e) => e) || [];
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
