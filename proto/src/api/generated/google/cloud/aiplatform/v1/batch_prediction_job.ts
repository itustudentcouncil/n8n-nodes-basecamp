// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/aiplatform/v1/batch_prediction_job.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { Value } from "../../../protobuf/struct.js";
import { Timestamp } from "../../../protobuf/timestamp.js";
import { Status } from "../../../rpc/status.js";
import { CompletionStats } from "./completion_stats.js";
import { EncryptionSpec } from "./encryption_spec.js";
import { ExplanationSpec } from "./explanation.js";
import { BigQueryDestination, BigQuerySource, GcsDestination, GcsSource } from "./io.js";
import { JobState, jobStateFromJSON, jobStateToJSON } from "./job_state.js";
import { BatchDedicatedResources, ResourcesConsumed } from "./machine_resources.js";
import { ManualBatchTuningParameters } from "./manual_batch_tuning_parameters.js";
import { UnmanagedContainerModel } from "./unmanaged_container_model.js";

export const protobufPackage = "google.cloud.aiplatform.v1";

/**
 * A job that uses a
 * [Model][google.cloud.aiplatform.v1.BatchPredictionJob.model] to produce
 * predictions on multiple [input
 * instances][google.cloud.aiplatform.v1.BatchPredictionJob.input_config]. If
 * predictions for significant portion of the instances fail, the job may finish
 * without attempting predictions for all remaining instances.
 */
export interface BatchPredictionJob {
  /** Output only. Resource name of the BatchPredictionJob. */
  name: string;
  /** Required. The user-defined name of this BatchPredictionJob. */
  displayName: string;
  /**
   * The name of the Model resource that produces the predictions via this job,
   * must share the same ancestor Location.
   * Starting this job has no impact on any existing deployments of the Model
   * and their resources.
   * Exactly one of model and unmanaged_container_model must be set.
   *
   * The model resource name may contain version id or version alias to specify
   * the version.
   *  Example: `projects/{project}/locations/{location}/models/{model}@2`
   *              or
   *            `projects/{project}/locations/{location}/models/{model}@golden`
   * if no version is specified, the default version will be deployed.
   *
   * The model resource could also be a publisher model.
   *  Example: `publishers/{publisher}/models/{model}`
   *              or
   *           `projects/{project}/locations/{location}/publishers/{publisher}/models/{model}`
   */
  model: string;
  /**
   * Output only. The version ID of the Model that produces the predictions via
   * this job.
   */
  modelVersionId: string;
  /**
   * Contains model information necessary to perform batch prediction without
   * requiring uploading to model registry.
   * Exactly one of model and unmanaged_container_model must be set.
   */
  unmanagedContainerModel:
    | UnmanagedContainerModel
    | undefined;
  /**
   * Required. Input configuration of the instances on which predictions are
   * performed. The schema of any single instance may be specified via the
   * [Model's][google.cloud.aiplatform.v1.BatchPredictionJob.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
   * [instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri].
   */
  inputConfig:
    | BatchPredictionJob_InputConfig
    | undefined;
  /**
   * Configuration for how to convert batch prediction input instances to the
   * prediction instances that are sent to the Model.
   */
  instanceConfig:
    | BatchPredictionJob_InstanceConfig
    | undefined;
  /**
   * The parameters that govern the predictions. The schema of the parameters
   * may be specified via the
   * [Model's][google.cloud.aiplatform.v1.BatchPredictionJob.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
   * [parameters_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri].
   */
  modelParameters:
    | any
    | undefined;
  /**
   * Required. The Configuration specifying where output predictions should
   * be written.
   * The schema of any single prediction may be specified as a concatenation
   * of [Model's][google.cloud.aiplatform.v1.BatchPredictionJob.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
   * [instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
   * and
   * [prediction_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.prediction_schema_uri].
   */
  outputConfig:
    | BatchPredictionJob_OutputConfig
    | undefined;
  /**
   * The config of resources used by the Model during the batch prediction. If
   * the Model
   * [supports][google.cloud.aiplatform.v1.Model.supported_deployment_resources_types]
   * DEDICATED_RESOURCES this config may be provided (and the job will use these
   * resources), if the Model doesn't support AUTOMATIC_RESOURCES, this config
   * must be provided.
   */
  dedicatedResources:
    | BatchDedicatedResources
    | undefined;
  /**
   * The service account that the DeployedModel's container runs as. If not
   * specified, a system generated one will be used, which
   * has minimal permissions and the custom container, if used, may not have
   * enough permission to access other Google Cloud resources.
   *
   * Users deploying the Model must have the `iam.serviceAccounts.actAs`
   * permission on this service account.
   */
  serviceAccount: string;
  /**
   * Immutable. Parameters configuring the batch behavior. Currently only
   * applicable when
   * [dedicated_resources][google.cloud.aiplatform.v1.BatchPredictionJob.dedicated_resources]
   * are used (in other cases Vertex AI does the tuning itself).
   */
  manualBatchTuningParameters:
    | ManualBatchTuningParameters
    | undefined;
  /**
   * Generate explanation with the batch prediction results.
   *
   * When set to `true`, the batch prediction output changes based on the
   * `predictions_format` field of the
   * [BatchPredictionJob.output_config][google.cloud.aiplatform.v1.BatchPredictionJob.output_config]
   * object:
   *
   *  * `bigquery`: output includes a column named `explanation`. The value
   *    is a struct that conforms to the
   *    [Explanation][google.cloud.aiplatform.v1.Explanation] object.
   *  * `jsonl`: The JSON objects on each line include an additional entry
   *    keyed `explanation`. The value of the entry is a JSON object that
   *    conforms to the [Explanation][google.cloud.aiplatform.v1.Explanation]
   *    object.
   *  * `csv`: Generating explanations for CSV format is not supported.
   *
   * If this field is set to true, either the
   * [Model.explanation_spec][google.cloud.aiplatform.v1.Model.explanation_spec]
   * or
   * [explanation_spec][google.cloud.aiplatform.v1.BatchPredictionJob.explanation_spec]
   * must be populated.
   */
  generateExplanation: boolean;
  /**
   * Explanation configuration for this BatchPredictionJob. Can be
   * specified only if
   * [generate_explanation][google.cloud.aiplatform.v1.BatchPredictionJob.generate_explanation]
   * is set to `true`.
   *
   * This value overrides the value of
   * [Model.explanation_spec][google.cloud.aiplatform.v1.Model.explanation_spec].
   * All fields of
   * [explanation_spec][google.cloud.aiplatform.v1.BatchPredictionJob.explanation_spec]
   * are optional in the request. If a field of the
   * [explanation_spec][google.cloud.aiplatform.v1.BatchPredictionJob.explanation_spec]
   * object is not populated, the corresponding field of the
   * [Model.explanation_spec][google.cloud.aiplatform.v1.Model.explanation_spec]
   * object is inherited.
   */
  explanationSpec:
    | ExplanationSpec
    | undefined;
  /** Output only. Information further describing the output of this job. */
  outputInfo:
    | BatchPredictionJob_OutputInfo
    | undefined;
  /** Output only. The detailed state of the job. */
  state: JobState;
  /**
   * Output only. Only populated when the job's state is JOB_STATE_FAILED or
   * JOB_STATE_CANCELLED.
   */
  error:
    | Status
    | undefined;
  /**
   * Output only. Partial failures encountered.
   * For example, single files that can't be read.
   * This field never exceeds 20 entries.
   * Status details fields contain standard Google Cloud error details.
   */
  partialFailures: Status[];
  /**
   * Output only. Information about resources that had been consumed by this
   * job. Provided in real time at best effort basis, as well as a final value
   * once the job completes.
   *
   * Note: This field currently may be not populated for batch predictions that
   * use AutoML Models.
   */
  resourcesConsumed:
    | ResourcesConsumed
    | undefined;
  /** Output only. Statistics on completed and failed prediction instances. */
  completionStats:
    | CompletionStats
    | undefined;
  /** Output only. Time when the BatchPredictionJob was created. */
  createTime:
    | Date
    | undefined;
  /**
   * Output only. Time when the BatchPredictionJob for the first time entered
   * the `JOB_STATE_RUNNING` state.
   */
  startTime:
    | Date
    | undefined;
  /**
   * Output only. Time when the BatchPredictionJob entered any of the following
   * states: `JOB_STATE_SUCCEEDED`, `JOB_STATE_FAILED`, `JOB_STATE_CANCELLED`.
   */
  endTime:
    | Date
    | undefined;
  /** Output only. Time when the BatchPredictionJob was most recently updated. */
  updateTime:
    | Date
    | undefined;
  /**
   * The labels with user-defined metadata to organize BatchPredictionJobs.
   *
   * Label keys and values can be no longer than 64 characters
   * (Unicode codepoints), can only contain lowercase letters, numeric
   * characters, underscores and dashes. International characters are allowed.
   *
   * See https://goo.gl/xmQnxf for more information and examples of labels.
   */
  labels: { [key: string]: string };
  /**
   * Customer-managed encryption key options for a BatchPredictionJob. If this
   * is set, then all resources created by the BatchPredictionJob will be
   * encrypted with the provided encryption key.
   */
  encryptionSpec:
    | EncryptionSpec
    | undefined;
  /**
   * For custom-trained Models and AutoML Tabular Models, the container of the
   * DeployedModel instances will send `stderr` and `stdout` streams to
   * Cloud Logging by default. Please note that the logs incur cost,
   * which are subject to [Cloud Logging
   * pricing](https://cloud.google.com/logging/pricing).
   *
   * User can disable container logging by setting this flag to true.
   */
  disableContainerLogging: boolean;
  /** Output only. Reserved for future use. */
  satisfiesPzs: boolean;
  /** Output only. Reserved for future use. */
  satisfiesPzi: boolean;
}

/**
 * Configures the input to
 * [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob]. See
 * [Model.supported_input_storage_formats][google.cloud.aiplatform.v1.Model.supported_input_storage_formats]
 * for Model's supported input formats, and how instances should be expressed
 * via any of them.
 */
export interface BatchPredictionJob_InputConfig {
  /** The Cloud Storage location for the input instances. */
  gcsSource?:
    | GcsSource
    | undefined;
  /**
   * The BigQuery location of the input table.
   * The schema of the table should be in the format described by the given
   * context OpenAPI Schema, if one is provided. The table may contain
   * additional columns that are not described by the schema, and they will
   * be ignored.
   */
  bigquerySource?:
    | BigQuerySource
    | undefined;
  /**
   * Required. The format in which instances are given, must be one of the
   * [Model's][google.cloud.aiplatform.v1.BatchPredictionJob.model]
   * [supported_input_storage_formats][google.cloud.aiplatform.v1.Model.supported_input_storage_formats].
   */
  instancesFormat: string;
}

/**
 * Configuration defining how to transform batch prediction input instances to
 * the instances that the Model accepts.
 */
export interface BatchPredictionJob_InstanceConfig {
  /**
   * The format of the instance that the Model accepts. Vertex AI will
   * convert compatible
   * [batch prediction input instance
   * formats][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.instances_format]
   * to the specified format.
   *
   * Supported values are:
   *
   * * `object`: Each input is converted to JSON object format.
   *     * For `bigquery`, each row is converted to an object.
   *     * For `jsonl`, each line of the JSONL input must be an object.
   *     * Does not apply to `csv`, `file-list`, `tf-record`, or
   *       `tf-record-gzip`.
   *
   * * `array`: Each input is converted to JSON array format.
   *     * For `bigquery`, each row is converted to an array. The order
   *       of columns is determined by the BigQuery column order, unless
   *       [included_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.included_fields]
   *       is populated.
   *       [included_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.included_fields]
   *       must be populated for specifying field orders.
   *     * For `jsonl`, if each line of the JSONL input is an object,
   *       [included_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.included_fields]
   *       must be populated for specifying field orders.
   *     * Does not apply to `csv`, `file-list`, `tf-record`, or
   *       `tf-record-gzip`.
   *
   * If not specified, Vertex AI converts the batch prediction input as
   * follows:
   *
   *  * For `bigquery` and `csv`, the behavior is the same as `array`. The
   *    order of columns is the same as defined in the file or table, unless
   *    [included_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.included_fields]
   *    is populated.
   *  * For `jsonl`, the prediction instance format is determined by
   *    each line of the input.
   *  * For `tf-record`/`tf-record-gzip`, each record will be converted to
   *    an object in the format of `{"b64": <value>}`, where `<value>` is
   *    the Base64-encoded string of the content of the record.
   *  * For `file-list`, each file in the list will be converted to an
   *    object in the format of `{"b64": <value>}`, where `<value>` is
   *    the Base64-encoded string of the content of the file.
   */
  instanceType: string;
  /**
   * The name of the field that is considered as a key.
   *
   * The values identified by the key field is not included in the transformed
   * instances that is sent to the Model. This is similar to
   * specifying this name of the field in
   * [excluded_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.excluded_fields].
   * In addition, the batch prediction output will not include the instances.
   * Instead the output will only include the value of the key field, in a
   * field named `key` in the output:
   *
   *  * For `jsonl` output format, the output will have a `key` field
   *    instead of the `instance` field.
   *  * For `csv`/`bigquery` output format, the output will have have a `key`
   *    column instead of the instance feature columns.
   *
   * The input must be JSONL with objects at each line, CSV, BigQuery
   * or TfRecord.
   */
  keyField: string;
  /**
   * Fields that will be included in the prediction instance that is
   * sent to the Model.
   *
   * If
   * [instance_type][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.instance_type]
   * is `array`, the order of field names in included_fields also determines
   * the order of the values in the array.
   *
   * When included_fields is populated,
   * [excluded_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.excluded_fields]
   * must be empty.
   *
   * The input must be JSONL with objects at each line, BigQuery
   * or TfRecord.
   */
  includedFields: string[];
  /**
   * Fields that will be excluded in the prediction instance that is
   * sent to the Model.
   *
   * Excluded will be attached to the batch prediction output if
   * [key_field][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.key_field]
   * is not specified.
   *
   * When excluded_fields is populated,
   * [included_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.included_fields]
   * must be empty.
   *
   * The input must be JSONL with objects at each line, BigQuery
   * or TfRecord.
   */
  excludedFields: string[];
}

/**
 * Configures the output of
 * [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob]. See
 * [Model.supported_output_storage_formats][google.cloud.aiplatform.v1.Model.supported_output_storage_formats]
 * for supported output formats, and how predictions are expressed via any of
 * them.
 */
export interface BatchPredictionJob_OutputConfig {
  /**
   * The Cloud Storage location of the directory where the output is
   * to be written to. In the given directory a new directory is created.
   * Its name is `prediction-<model-display-name>-<job-create-time>`,
   * where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format.
   * Inside of it files `predictions_0001.<extension>`,
   * `predictions_0002.<extension>`, ..., `predictions_N.<extension>`
   * are created where `<extension>` depends on chosen
   * [predictions_format][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.predictions_format],
   * and N may equal 0001 and depends on the total number of successfully
   * predicted instances. If the Model has both
   * [instance][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
   * and
   * [prediction][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri]
   * schemata defined then each such file contains predictions as per the
   * [predictions_format][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.predictions_format].
   * If prediction for any instance failed (partially or completely), then
   * an additional `errors_0001.<extension>`, `errors_0002.<extension>`,...,
   * `errors_N.<extension>` files are created (N depends on total number
   * of failed predictions). These files contain the failed instances,
   * as per their schema, followed by an additional `error` field which as
   * value has [google.rpc.Status][google.rpc.Status]
   * containing only `code` and `message` fields.
   */
  gcsDestination?:
    | GcsDestination
    | undefined;
  /**
   * The BigQuery project or dataset location where the output is to be
   * written to. If project is provided, a new dataset is created with name
   * `prediction_<model-display-name>_<job-create-time>`
   * where <model-display-name> is made
   * BigQuery-dataset-name compatible (for example, most special characters
   * become underscores), and timestamp is in
   * YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset
   * two tables will be created, `predictions`, and `errors`.
   * If the Model has both
   * [instance][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
   * and
   * [prediction][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri]
   * schemata defined then the tables have columns as follows: The
   * `predictions` table contains instances for which the prediction
   * succeeded, it has columns as per a concatenation of the Model's
   * instance and prediction schemata. The `errors` table contains rows for
   * which the prediction has failed, it has instance columns, as per the
   * instance schema, followed by a single "errors" column, which as values
   * has [google.rpc.Status][google.rpc.Status]
   * represented as a STRUCT, and containing only `code` and `message`.
   */
  bigqueryDestination?:
    | BigQueryDestination
    | undefined;
  /**
   * Required. The format in which Vertex AI gives the predictions, must be
   * one of the [Model's][google.cloud.aiplatform.v1.BatchPredictionJob.model]
   * [supported_output_storage_formats][google.cloud.aiplatform.v1.Model.supported_output_storage_formats].
   */
  predictionsFormat: string;
}

/**
 * Further describes this job's output.
 * Supplements
 * [output_config][google.cloud.aiplatform.v1.BatchPredictionJob.output_config].
 */
export interface BatchPredictionJob_OutputInfo {
  /**
   * Output only. The full path of the Cloud Storage directory created, into
   * which the prediction output is written.
   */
  gcsOutputDirectory?:
    | string
    | undefined;
  /**
   * Output only. The path of the BigQuery dataset created, in
   * `bq://projectId.bqDatasetId`
   * format, into which the prediction output is written.
   */
  bigqueryOutputDataset?:
    | string
    | undefined;
  /**
   * Output only. The name of the BigQuery table created, in
   * `predictions_<timestamp>`
   * format, into which the prediction output is written.
   * Can be used by UI to generate the BigQuery output path, for example.
   */
  bigqueryOutputTable: string;
}

export interface BatchPredictionJob_LabelsEntry {
  key: string;
  value: string;
}

function createBaseBatchPredictionJob(): BatchPredictionJob {
  return {
    name: "",
    displayName: "",
    model: "",
    modelVersionId: "",
    unmanagedContainerModel: undefined,
    inputConfig: undefined,
    instanceConfig: undefined,
    modelParameters: undefined,
    outputConfig: undefined,
    dedicatedResources: undefined,
    serviceAccount: "",
    manualBatchTuningParameters: undefined,
    generateExplanation: false,
    explanationSpec: undefined,
    outputInfo: undefined,
    state: 0,
    error: undefined,
    partialFailures: [],
    resourcesConsumed: undefined,
    completionStats: undefined,
    createTime: undefined,
    startTime: undefined,
    endTime: undefined,
    updateTime: undefined,
    labels: {},
    encryptionSpec: undefined,
    disableContainerLogging: false,
    satisfiesPzs: false,
    satisfiesPzi: false,
  };
}

export const BatchPredictionJob: MessageFns<BatchPredictionJob> = {
  encode(message: BatchPredictionJob, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.displayName !== "") {
      writer.uint32(18).string(message.displayName);
    }
    if (message.model !== "") {
      writer.uint32(26).string(message.model);
    }
    if (message.modelVersionId !== "") {
      writer.uint32(242).string(message.modelVersionId);
    }
    if (message.unmanagedContainerModel !== undefined) {
      UnmanagedContainerModel.encode(message.unmanagedContainerModel, writer.uint32(226).fork()).join();
    }
    if (message.inputConfig !== undefined) {
      BatchPredictionJob_InputConfig.encode(message.inputConfig, writer.uint32(34).fork()).join();
    }
    if (message.instanceConfig !== undefined) {
      BatchPredictionJob_InstanceConfig.encode(message.instanceConfig, writer.uint32(218).fork()).join();
    }
    if (message.modelParameters !== undefined) {
      Value.encode(Value.wrap(message.modelParameters), writer.uint32(42).fork()).join();
    }
    if (message.outputConfig !== undefined) {
      BatchPredictionJob_OutputConfig.encode(message.outputConfig, writer.uint32(50).fork()).join();
    }
    if (message.dedicatedResources !== undefined) {
      BatchDedicatedResources.encode(message.dedicatedResources, writer.uint32(58).fork()).join();
    }
    if (message.serviceAccount !== "") {
      writer.uint32(234).string(message.serviceAccount);
    }
    if (message.manualBatchTuningParameters !== undefined) {
      ManualBatchTuningParameters.encode(message.manualBatchTuningParameters, writer.uint32(66).fork()).join();
    }
    if (message.generateExplanation !== false) {
      writer.uint32(184).bool(message.generateExplanation);
    }
    if (message.explanationSpec !== undefined) {
      ExplanationSpec.encode(message.explanationSpec, writer.uint32(202).fork()).join();
    }
    if (message.outputInfo !== undefined) {
      BatchPredictionJob_OutputInfo.encode(message.outputInfo, writer.uint32(74).fork()).join();
    }
    if (message.state !== 0) {
      writer.uint32(80).int32(message.state);
    }
    if (message.error !== undefined) {
      Status.encode(message.error, writer.uint32(90).fork()).join();
    }
    for (const v of message.partialFailures) {
      Status.encode(v!, writer.uint32(98).fork()).join();
    }
    if (message.resourcesConsumed !== undefined) {
      ResourcesConsumed.encode(message.resourcesConsumed, writer.uint32(106).fork()).join();
    }
    if (message.completionStats !== undefined) {
      CompletionStats.encode(message.completionStats, writer.uint32(114).fork()).join();
    }
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(122).fork()).join();
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(toTimestamp(message.startTime), writer.uint32(130).fork()).join();
    }
    if (message.endTime !== undefined) {
      Timestamp.encode(toTimestamp(message.endTime), writer.uint32(138).fork()).join();
    }
    if (message.updateTime !== undefined) {
      Timestamp.encode(toTimestamp(message.updateTime), writer.uint32(146).fork()).join();
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      BatchPredictionJob_LabelsEntry.encode({ key: key as any, value }, writer.uint32(154).fork()).join();
    });
    if (message.encryptionSpec !== undefined) {
      EncryptionSpec.encode(message.encryptionSpec, writer.uint32(194).fork()).join();
    }
    if (message.disableContainerLogging !== false) {
      writer.uint32(272).bool(message.disableContainerLogging);
    }
    if (message.satisfiesPzs !== false) {
      writer.uint32(288).bool(message.satisfiesPzs);
    }
    if (message.satisfiesPzi !== false) {
      writer.uint32(296).bool(message.satisfiesPzi);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchPredictionJob {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchPredictionJob();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.model = reader.string();
          continue;
        case 30:
          if (tag !== 242) {
            break;
          }

          message.modelVersionId = reader.string();
          continue;
        case 28:
          if (tag !== 226) {
            break;
          }

          message.unmanagedContainerModel = UnmanagedContainerModel.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.inputConfig = BatchPredictionJob_InputConfig.decode(reader, reader.uint32());
          continue;
        case 27:
          if (tag !== 218) {
            break;
          }

          message.instanceConfig = BatchPredictionJob_InstanceConfig.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.modelParameters = Value.unwrap(Value.decode(reader, reader.uint32()));
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.outputConfig = BatchPredictionJob_OutputConfig.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.dedicatedResources = BatchDedicatedResources.decode(reader, reader.uint32());
          continue;
        case 29:
          if (tag !== 234) {
            break;
          }

          message.serviceAccount = reader.string();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.manualBatchTuningParameters = ManualBatchTuningParameters.decode(reader, reader.uint32());
          continue;
        case 23:
          if (tag !== 184) {
            break;
          }

          message.generateExplanation = reader.bool();
          continue;
        case 25:
          if (tag !== 202) {
            break;
          }

          message.explanationSpec = ExplanationSpec.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.outputInfo = BatchPredictionJob_OutputInfo.decode(reader, reader.uint32());
          continue;
        case 10:
          if (tag !== 80) {
            break;
          }

          message.state = reader.int32() as any;
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.error = Status.decode(reader, reader.uint32());
          continue;
        case 12:
          if (tag !== 98) {
            break;
          }

          message.partialFailures.push(Status.decode(reader, reader.uint32()));
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.resourcesConsumed = ResourcesConsumed.decode(reader, reader.uint32());
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.completionStats = CompletionStats.decode(reader, reader.uint32());
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 16:
          if (tag !== 130) {
            break;
          }

          message.startTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 17:
          if (tag !== 138) {
            break;
          }

          message.endTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 18:
          if (tag !== 146) {
            break;
          }

          message.updateTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          const entry19 = BatchPredictionJob_LabelsEntry.decode(reader, reader.uint32());
          if (entry19.value !== undefined) {
            message.labels[entry19.key] = entry19.value;
          }
          continue;
        case 24:
          if (tag !== 194) {
            break;
          }

          message.encryptionSpec = EncryptionSpec.decode(reader, reader.uint32());
          continue;
        case 34:
          if (tag !== 272) {
            break;
          }

          message.disableContainerLogging = reader.bool();
          continue;
        case 36:
          if (tag !== 288) {
            break;
          }

          message.satisfiesPzs = reader.bool();
          continue;
        case 37:
          if (tag !== 296) {
            break;
          }

          message.satisfiesPzi = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchPredictionJob {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      modelVersionId: isSet(object.modelVersionId) ? globalThis.String(object.modelVersionId) : "",
      unmanagedContainerModel: isSet(object.unmanagedContainerModel)
        ? UnmanagedContainerModel.fromJSON(object.unmanagedContainerModel)
        : undefined,
      inputConfig: isSet(object.inputConfig) ? BatchPredictionJob_InputConfig.fromJSON(object.inputConfig) : undefined,
      instanceConfig: isSet(object.instanceConfig)
        ? BatchPredictionJob_InstanceConfig.fromJSON(object.instanceConfig)
        : undefined,
      modelParameters: isSet(object?.modelParameters) ? object.modelParameters : undefined,
      outputConfig: isSet(object.outputConfig)
        ? BatchPredictionJob_OutputConfig.fromJSON(object.outputConfig)
        : undefined,
      dedicatedResources: isSet(object.dedicatedResources)
        ? BatchDedicatedResources.fromJSON(object.dedicatedResources)
        : undefined,
      serviceAccount: isSet(object.serviceAccount) ? globalThis.String(object.serviceAccount) : "",
      manualBatchTuningParameters: isSet(object.manualBatchTuningParameters)
        ? ManualBatchTuningParameters.fromJSON(object.manualBatchTuningParameters)
        : undefined,
      generateExplanation: isSet(object.generateExplanation) ? globalThis.Boolean(object.generateExplanation) : false,
      explanationSpec: isSet(object.explanationSpec) ? ExplanationSpec.fromJSON(object.explanationSpec) : undefined,
      outputInfo: isSet(object.outputInfo) ? BatchPredictionJob_OutputInfo.fromJSON(object.outputInfo) : undefined,
      state: isSet(object.state) ? jobStateFromJSON(object.state) : 0,
      error: isSet(object.error) ? Status.fromJSON(object.error) : undefined,
      partialFailures: globalThis.Array.isArray(object?.partialFailures)
        ? object.partialFailures.map((e: any) => Status.fromJSON(e))
        : [],
      resourcesConsumed: isSet(object.resourcesConsumed)
        ? ResourcesConsumed.fromJSON(object.resourcesConsumed)
        : undefined,
      completionStats: isSet(object.completionStats) ? CompletionStats.fromJSON(object.completionStats) : undefined,
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      startTime: isSet(object.startTime) ? fromJsonTimestamp(object.startTime) : undefined,
      endTime: isSet(object.endTime) ? fromJsonTimestamp(object.endTime) : undefined,
      updateTime: isSet(object.updateTime) ? fromJsonTimestamp(object.updateTime) : undefined,
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      encryptionSpec: isSet(object.encryptionSpec) ? EncryptionSpec.fromJSON(object.encryptionSpec) : undefined,
      disableContainerLogging: isSet(object.disableContainerLogging)
        ? globalThis.Boolean(object.disableContainerLogging)
        : false,
      satisfiesPzs: isSet(object.satisfiesPzs) ? globalThis.Boolean(object.satisfiesPzs) : false,
      satisfiesPzi: isSet(object.satisfiesPzi) ? globalThis.Boolean(object.satisfiesPzi) : false,
    };
  },

  toJSON(message: BatchPredictionJob): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.modelVersionId !== "") {
      obj.modelVersionId = message.modelVersionId;
    }
    if (message.unmanagedContainerModel !== undefined) {
      obj.unmanagedContainerModel = UnmanagedContainerModel.toJSON(message.unmanagedContainerModel);
    }
    if (message.inputConfig !== undefined) {
      obj.inputConfig = BatchPredictionJob_InputConfig.toJSON(message.inputConfig);
    }
    if (message.instanceConfig !== undefined) {
      obj.instanceConfig = BatchPredictionJob_InstanceConfig.toJSON(message.instanceConfig);
    }
    if (message.modelParameters !== undefined) {
      obj.modelParameters = message.modelParameters;
    }
    if (message.outputConfig !== undefined) {
      obj.outputConfig = BatchPredictionJob_OutputConfig.toJSON(message.outputConfig);
    }
    if (message.dedicatedResources !== undefined) {
      obj.dedicatedResources = BatchDedicatedResources.toJSON(message.dedicatedResources);
    }
    if (message.serviceAccount !== "") {
      obj.serviceAccount = message.serviceAccount;
    }
    if (message.manualBatchTuningParameters !== undefined) {
      obj.manualBatchTuningParameters = ManualBatchTuningParameters.toJSON(message.manualBatchTuningParameters);
    }
    if (message.generateExplanation !== false) {
      obj.generateExplanation = message.generateExplanation;
    }
    if (message.explanationSpec !== undefined) {
      obj.explanationSpec = ExplanationSpec.toJSON(message.explanationSpec);
    }
    if (message.outputInfo !== undefined) {
      obj.outputInfo = BatchPredictionJob_OutputInfo.toJSON(message.outputInfo);
    }
    if (message.state !== 0) {
      obj.state = jobStateToJSON(message.state);
    }
    if (message.error !== undefined) {
      obj.error = Status.toJSON(message.error);
    }
    if (message.partialFailures?.length) {
      obj.partialFailures = message.partialFailures.map((e) => Status.toJSON(e));
    }
    if (message.resourcesConsumed !== undefined) {
      obj.resourcesConsumed = ResourcesConsumed.toJSON(message.resourcesConsumed);
    }
    if (message.completionStats !== undefined) {
      obj.completionStats = CompletionStats.toJSON(message.completionStats);
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.endTime !== undefined) {
      obj.endTime = message.endTime.toISOString();
    }
    if (message.updateTime !== undefined) {
      obj.updateTime = message.updateTime.toISOString();
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.encryptionSpec !== undefined) {
      obj.encryptionSpec = EncryptionSpec.toJSON(message.encryptionSpec);
    }
    if (message.disableContainerLogging !== false) {
      obj.disableContainerLogging = message.disableContainerLogging;
    }
    if (message.satisfiesPzs !== false) {
      obj.satisfiesPzs = message.satisfiesPzs;
    }
    if (message.satisfiesPzi !== false) {
      obj.satisfiesPzi = message.satisfiesPzi;
    }
    return obj;
  },

  create(base?: DeepPartial<BatchPredictionJob>): BatchPredictionJob {
    return BatchPredictionJob.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchPredictionJob>): BatchPredictionJob {
    const message = createBaseBatchPredictionJob();
    message.name = object.name ?? "";
    message.displayName = object.displayName ?? "";
    message.model = object.model ?? "";
    message.modelVersionId = object.modelVersionId ?? "";
    message.unmanagedContainerModel =
      (object.unmanagedContainerModel !== undefined && object.unmanagedContainerModel !== null)
        ? UnmanagedContainerModel.fromPartial(object.unmanagedContainerModel)
        : undefined;
    message.inputConfig = (object.inputConfig !== undefined && object.inputConfig !== null)
      ? BatchPredictionJob_InputConfig.fromPartial(object.inputConfig)
      : undefined;
    message.instanceConfig = (object.instanceConfig !== undefined && object.instanceConfig !== null)
      ? BatchPredictionJob_InstanceConfig.fromPartial(object.instanceConfig)
      : undefined;
    message.modelParameters = object.modelParameters ?? undefined;
    message.outputConfig = (object.outputConfig !== undefined && object.outputConfig !== null)
      ? BatchPredictionJob_OutputConfig.fromPartial(object.outputConfig)
      : undefined;
    message.dedicatedResources = (object.dedicatedResources !== undefined && object.dedicatedResources !== null)
      ? BatchDedicatedResources.fromPartial(object.dedicatedResources)
      : undefined;
    message.serviceAccount = object.serviceAccount ?? "";
    message.manualBatchTuningParameters =
      (object.manualBatchTuningParameters !== undefined && object.manualBatchTuningParameters !== null)
        ? ManualBatchTuningParameters.fromPartial(object.manualBatchTuningParameters)
        : undefined;
    message.generateExplanation = object.generateExplanation ?? false;
    message.explanationSpec = (object.explanationSpec !== undefined && object.explanationSpec !== null)
      ? ExplanationSpec.fromPartial(object.explanationSpec)
      : undefined;
    message.outputInfo = (object.outputInfo !== undefined && object.outputInfo !== null)
      ? BatchPredictionJob_OutputInfo.fromPartial(object.outputInfo)
      : undefined;
    message.state = object.state ?? 0;
    message.error = (object.error !== undefined && object.error !== null)
      ? Status.fromPartial(object.error)
      : undefined;
    message.partialFailures = object.partialFailures?.map((e) => Status.fromPartial(e)) || [];
    message.resourcesConsumed = (object.resourcesConsumed !== undefined && object.resourcesConsumed !== null)
      ? ResourcesConsumed.fromPartial(object.resourcesConsumed)
      : undefined;
    message.completionStats = (object.completionStats !== undefined && object.completionStats !== null)
      ? CompletionStats.fromPartial(object.completionStats)
      : undefined;
    message.createTime = object.createTime ?? undefined;
    message.startTime = object.startTime ?? undefined;
    message.endTime = object.endTime ?? undefined;
    message.updateTime = object.updateTime ?? undefined;
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.encryptionSpec = (object.encryptionSpec !== undefined && object.encryptionSpec !== null)
      ? EncryptionSpec.fromPartial(object.encryptionSpec)
      : undefined;
    message.disableContainerLogging = object.disableContainerLogging ?? false;
    message.satisfiesPzs = object.satisfiesPzs ?? false;
    message.satisfiesPzi = object.satisfiesPzi ?? false;
    return message;
  },
};

function createBaseBatchPredictionJob_InputConfig(): BatchPredictionJob_InputConfig {
  return { gcsSource: undefined, bigquerySource: undefined, instancesFormat: "" };
}

export const BatchPredictionJob_InputConfig: MessageFns<BatchPredictionJob_InputConfig> = {
  encode(message: BatchPredictionJob_InputConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcsSource !== undefined) {
      GcsSource.encode(message.gcsSource, writer.uint32(18).fork()).join();
    }
    if (message.bigquerySource !== undefined) {
      BigQuerySource.encode(message.bigquerySource, writer.uint32(26).fork()).join();
    }
    if (message.instancesFormat !== "") {
      writer.uint32(10).string(message.instancesFormat);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchPredictionJob_InputConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchPredictionJob_InputConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.gcsSource = GcsSource.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.bigquerySource = BigQuerySource.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.instancesFormat = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchPredictionJob_InputConfig {
    return {
      gcsSource: isSet(object.gcsSource) ? GcsSource.fromJSON(object.gcsSource) : undefined,
      bigquerySource: isSet(object.bigquerySource) ? BigQuerySource.fromJSON(object.bigquerySource) : undefined,
      instancesFormat: isSet(object.instancesFormat) ? globalThis.String(object.instancesFormat) : "",
    };
  },

  toJSON(message: BatchPredictionJob_InputConfig): unknown {
    const obj: any = {};
    if (message.gcsSource !== undefined) {
      obj.gcsSource = GcsSource.toJSON(message.gcsSource);
    }
    if (message.bigquerySource !== undefined) {
      obj.bigquerySource = BigQuerySource.toJSON(message.bigquerySource);
    }
    if (message.instancesFormat !== "") {
      obj.instancesFormat = message.instancesFormat;
    }
    return obj;
  },

  create(base?: DeepPartial<BatchPredictionJob_InputConfig>): BatchPredictionJob_InputConfig {
    return BatchPredictionJob_InputConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchPredictionJob_InputConfig>): BatchPredictionJob_InputConfig {
    const message = createBaseBatchPredictionJob_InputConfig();
    message.gcsSource = (object.gcsSource !== undefined && object.gcsSource !== null)
      ? GcsSource.fromPartial(object.gcsSource)
      : undefined;
    message.bigquerySource = (object.bigquerySource !== undefined && object.bigquerySource !== null)
      ? BigQuerySource.fromPartial(object.bigquerySource)
      : undefined;
    message.instancesFormat = object.instancesFormat ?? "";
    return message;
  },
};

function createBaseBatchPredictionJob_InstanceConfig(): BatchPredictionJob_InstanceConfig {
  return { instanceType: "", keyField: "", includedFields: [], excludedFields: [] };
}

export const BatchPredictionJob_InstanceConfig: MessageFns<BatchPredictionJob_InstanceConfig> = {
  encode(message: BatchPredictionJob_InstanceConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.instanceType !== "") {
      writer.uint32(10).string(message.instanceType);
    }
    if (message.keyField !== "") {
      writer.uint32(18).string(message.keyField);
    }
    for (const v of message.includedFields) {
      writer.uint32(26).string(v!);
    }
    for (const v of message.excludedFields) {
      writer.uint32(34).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchPredictionJob_InstanceConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchPredictionJob_InstanceConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.instanceType = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.keyField = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.includedFields.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.excludedFields.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchPredictionJob_InstanceConfig {
    return {
      instanceType: isSet(object.instanceType) ? globalThis.String(object.instanceType) : "",
      keyField: isSet(object.keyField) ? globalThis.String(object.keyField) : "",
      includedFields: globalThis.Array.isArray(object?.includedFields)
        ? object.includedFields.map((e: any) => globalThis.String(e))
        : [],
      excludedFields: globalThis.Array.isArray(object?.excludedFields)
        ? object.excludedFields.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: BatchPredictionJob_InstanceConfig): unknown {
    const obj: any = {};
    if (message.instanceType !== "") {
      obj.instanceType = message.instanceType;
    }
    if (message.keyField !== "") {
      obj.keyField = message.keyField;
    }
    if (message.includedFields?.length) {
      obj.includedFields = message.includedFields;
    }
    if (message.excludedFields?.length) {
      obj.excludedFields = message.excludedFields;
    }
    return obj;
  },

  create(base?: DeepPartial<BatchPredictionJob_InstanceConfig>): BatchPredictionJob_InstanceConfig {
    return BatchPredictionJob_InstanceConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchPredictionJob_InstanceConfig>): BatchPredictionJob_InstanceConfig {
    const message = createBaseBatchPredictionJob_InstanceConfig();
    message.instanceType = object.instanceType ?? "";
    message.keyField = object.keyField ?? "";
    message.includedFields = object.includedFields?.map((e) => e) || [];
    message.excludedFields = object.excludedFields?.map((e) => e) || [];
    return message;
  },
};

function createBaseBatchPredictionJob_OutputConfig(): BatchPredictionJob_OutputConfig {
  return { gcsDestination: undefined, bigqueryDestination: undefined, predictionsFormat: "" };
}

export const BatchPredictionJob_OutputConfig: MessageFns<BatchPredictionJob_OutputConfig> = {
  encode(message: BatchPredictionJob_OutputConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcsDestination !== undefined) {
      GcsDestination.encode(message.gcsDestination, writer.uint32(18).fork()).join();
    }
    if (message.bigqueryDestination !== undefined) {
      BigQueryDestination.encode(message.bigqueryDestination, writer.uint32(26).fork()).join();
    }
    if (message.predictionsFormat !== "") {
      writer.uint32(10).string(message.predictionsFormat);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchPredictionJob_OutputConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchPredictionJob_OutputConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 2:
          if (tag !== 18) {
            break;
          }

          message.gcsDestination = GcsDestination.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.bigqueryDestination = BigQueryDestination.decode(reader, reader.uint32());
          continue;
        case 1:
          if (tag !== 10) {
            break;
          }

          message.predictionsFormat = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchPredictionJob_OutputConfig {
    return {
      gcsDestination: isSet(object.gcsDestination) ? GcsDestination.fromJSON(object.gcsDestination) : undefined,
      bigqueryDestination: isSet(object.bigqueryDestination)
        ? BigQueryDestination.fromJSON(object.bigqueryDestination)
        : undefined,
      predictionsFormat: isSet(object.predictionsFormat) ? globalThis.String(object.predictionsFormat) : "",
    };
  },

  toJSON(message: BatchPredictionJob_OutputConfig): unknown {
    const obj: any = {};
    if (message.gcsDestination !== undefined) {
      obj.gcsDestination = GcsDestination.toJSON(message.gcsDestination);
    }
    if (message.bigqueryDestination !== undefined) {
      obj.bigqueryDestination = BigQueryDestination.toJSON(message.bigqueryDestination);
    }
    if (message.predictionsFormat !== "") {
      obj.predictionsFormat = message.predictionsFormat;
    }
    return obj;
  },

  create(base?: DeepPartial<BatchPredictionJob_OutputConfig>): BatchPredictionJob_OutputConfig {
    return BatchPredictionJob_OutputConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchPredictionJob_OutputConfig>): BatchPredictionJob_OutputConfig {
    const message = createBaseBatchPredictionJob_OutputConfig();
    message.gcsDestination = (object.gcsDestination !== undefined && object.gcsDestination !== null)
      ? GcsDestination.fromPartial(object.gcsDestination)
      : undefined;
    message.bigqueryDestination = (object.bigqueryDestination !== undefined && object.bigqueryDestination !== null)
      ? BigQueryDestination.fromPartial(object.bigqueryDestination)
      : undefined;
    message.predictionsFormat = object.predictionsFormat ?? "";
    return message;
  },
};

function createBaseBatchPredictionJob_OutputInfo(): BatchPredictionJob_OutputInfo {
  return { gcsOutputDirectory: undefined, bigqueryOutputDataset: undefined, bigqueryOutputTable: "" };
}

export const BatchPredictionJob_OutputInfo: MessageFns<BatchPredictionJob_OutputInfo> = {
  encode(message: BatchPredictionJob_OutputInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.gcsOutputDirectory !== undefined) {
      writer.uint32(10).string(message.gcsOutputDirectory);
    }
    if (message.bigqueryOutputDataset !== undefined) {
      writer.uint32(18).string(message.bigqueryOutputDataset);
    }
    if (message.bigqueryOutputTable !== "") {
      writer.uint32(34).string(message.bigqueryOutputTable);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchPredictionJob_OutputInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchPredictionJob_OutputInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.gcsOutputDirectory = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.bigqueryOutputDataset = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.bigqueryOutputTable = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchPredictionJob_OutputInfo {
    return {
      gcsOutputDirectory: isSet(object.gcsOutputDirectory) ? globalThis.String(object.gcsOutputDirectory) : undefined,
      bigqueryOutputDataset: isSet(object.bigqueryOutputDataset)
        ? globalThis.String(object.bigqueryOutputDataset)
        : undefined,
      bigqueryOutputTable: isSet(object.bigqueryOutputTable) ? globalThis.String(object.bigqueryOutputTable) : "",
    };
  },

  toJSON(message: BatchPredictionJob_OutputInfo): unknown {
    const obj: any = {};
    if (message.gcsOutputDirectory !== undefined) {
      obj.gcsOutputDirectory = message.gcsOutputDirectory;
    }
    if (message.bigqueryOutputDataset !== undefined) {
      obj.bigqueryOutputDataset = message.bigqueryOutputDataset;
    }
    if (message.bigqueryOutputTable !== "") {
      obj.bigqueryOutputTable = message.bigqueryOutputTable;
    }
    return obj;
  },

  create(base?: DeepPartial<BatchPredictionJob_OutputInfo>): BatchPredictionJob_OutputInfo {
    return BatchPredictionJob_OutputInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchPredictionJob_OutputInfo>): BatchPredictionJob_OutputInfo {
    const message = createBaseBatchPredictionJob_OutputInfo();
    message.gcsOutputDirectory = object.gcsOutputDirectory ?? undefined;
    message.bigqueryOutputDataset = object.bigqueryOutputDataset ?? undefined;
    message.bigqueryOutputTable = object.bigqueryOutputTable ?? "";
    return message;
  },
};

function createBaseBatchPredictionJob_LabelsEntry(): BatchPredictionJob_LabelsEntry {
  return { key: "", value: "" };
}

export const BatchPredictionJob_LabelsEntry: MessageFns<BatchPredictionJob_LabelsEntry> = {
  encode(message: BatchPredictionJob_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchPredictionJob_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchPredictionJob_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchPredictionJob_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: BatchPredictionJob_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<BatchPredictionJob_LabelsEntry>): BatchPredictionJob_LabelsEntry {
    return BatchPredictionJob_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchPredictionJob_LabelsEntry>): BatchPredictionJob_LabelsEntry {
    const message = createBaseBatchPredictionJob_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
