// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/cloud/aiplatform/v1/schema/predict/params/video_classification.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";

export const protobufPackage = "google.cloud.aiplatform.v1.schema.predict.params";

/** Prediction model parameters for Video Classification. */
export interface VideoClassificationPredictionParams {
  /**
   * The Model only returns predictions with at least this confidence score.
   * Default value is 0.0
   */
  confidenceThreshold: number;
  /**
   * The Model only returns up to that many top, by confidence score,
   * predictions per instance. If this number is very high, the Model may return
   * fewer predictions. Default value is 10,000.
   */
  maxPredictions: number;
  /**
   * Set to true to request segment-level classification. Vertex AI returns
   * labels and their confidence scores for the entire time segment of the
   * video that user specified in the input instance.
   * Default value is true
   */
  segmentClassification: boolean;
  /**
   * Set to true to request shot-level classification. Vertex AI determines
   * the boundaries for each camera shot in the entire time segment of the
   * video that user specified in the input instance. Vertex AI then
   * returns labels and their confidence scores for each detected shot, along
   * with the start and end time of the shot.
   * WARNING: Model evaluation is not done for this classification type,
   * the quality of it depends on the training data, but there are no metrics
   * provided to describe that quality.
   * Default value is false
   */
  shotClassification: boolean;
  /**
   * Set to true to request classification for a video at one-second intervals.
   * Vertex AI returns labels and their confidence scores for each second of
   * the entire time segment of the video that user specified in the input
   * WARNING: Model evaluation is not done for this classification type, the
   * quality of it depends on the training data, but there are no metrics
   * provided to describe that quality. Default value is false
   */
  oneSecIntervalClassification: boolean;
}

function createBaseVideoClassificationPredictionParams(): VideoClassificationPredictionParams {
  return {
    confidenceThreshold: 0,
    maxPredictions: 0,
    segmentClassification: false,
    shotClassification: false,
    oneSecIntervalClassification: false,
  };
}

export const VideoClassificationPredictionParams: MessageFns<VideoClassificationPredictionParams> = {
  encode(message: VideoClassificationPredictionParams, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.confidenceThreshold !== 0) {
      writer.uint32(13).float(message.confidenceThreshold);
    }
    if (message.maxPredictions !== 0) {
      writer.uint32(16).int32(message.maxPredictions);
    }
    if (message.segmentClassification !== false) {
      writer.uint32(24).bool(message.segmentClassification);
    }
    if (message.shotClassification !== false) {
      writer.uint32(32).bool(message.shotClassification);
    }
    if (message.oneSecIntervalClassification !== false) {
      writer.uint32(40).bool(message.oneSecIntervalClassification);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): VideoClassificationPredictionParams {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseVideoClassificationPredictionParams();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 13) {
            break;
          }

          message.confidenceThreshold = reader.float();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.maxPredictions = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.segmentClassification = reader.bool();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.shotClassification = reader.bool();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.oneSecIntervalClassification = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): VideoClassificationPredictionParams {
    return {
      confidenceThreshold: isSet(object.confidenceThreshold) ? globalThis.Number(object.confidenceThreshold) : 0,
      maxPredictions: isSet(object.maxPredictions) ? globalThis.Number(object.maxPredictions) : 0,
      segmentClassification: isSet(object.segmentClassification)
        ? globalThis.Boolean(object.segmentClassification)
        : false,
      shotClassification: isSet(object.shotClassification) ? globalThis.Boolean(object.shotClassification) : false,
      oneSecIntervalClassification: isSet(object.oneSecIntervalClassification)
        ? globalThis.Boolean(object.oneSecIntervalClassification)
        : false,
    };
  },

  toJSON(message: VideoClassificationPredictionParams): unknown {
    const obj: any = {};
    if (message.confidenceThreshold !== 0) {
      obj.confidenceThreshold = message.confidenceThreshold;
    }
    if (message.maxPredictions !== 0) {
      obj.maxPredictions = Math.round(message.maxPredictions);
    }
    if (message.segmentClassification !== false) {
      obj.segmentClassification = message.segmentClassification;
    }
    if (message.shotClassification !== false) {
      obj.shotClassification = message.shotClassification;
    }
    if (message.oneSecIntervalClassification !== false) {
      obj.oneSecIntervalClassification = message.oneSecIntervalClassification;
    }
    return obj;
  },

  create(base?: DeepPartial<VideoClassificationPredictionParams>): VideoClassificationPredictionParams {
    return VideoClassificationPredictionParams.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<VideoClassificationPredictionParams>): VideoClassificationPredictionParams {
    const message = createBaseVideoClassificationPredictionParams();
    message.confidenceThreshold = object.confidenceThreshold ?? 0;
    message.maxPredictions = object.maxPredictions ?? 0;
    message.segmentClassification = object.segmentClassification ?? false;
    message.shotClassification = object.shotClassification ?? false;
    message.oneSecIntervalClassification = object.oneSecIntervalClassification ?? false;
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
