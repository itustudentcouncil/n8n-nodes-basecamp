// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/genomics/v1/reads.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Operation } from "../../longrunning/operations.js";
import { Empty } from "../../protobuf/empty.js";
import { FieldMask } from "../../protobuf/field_mask.js";
import { Range } from "./range.js";
import { Read } from "./readalignment.js";
import { ReadGroupSet } from "./readgroupset.js";

export const protobufPackage = "google.genomics.v1";

/** The read group set search request. */
export interface SearchReadGroupSetsRequest {
  /**
   * Restricts this query to read group sets within the given datasets. At least
   * one ID must be provided.
   */
  datasetIds: string[];
  /**
   * Only return read group sets for which a substring of the name matches this
   * string.
   */
  name: string;
  /**
   * The continuation token, which is used to page through large result sets.
   * To get the next page of results, set this parameter to the value of
   * `nextPageToken` from the previous response.
   */
  pageToken: string;
  /**
   * The maximum number of results to return in a single page. If unspecified,
   * defaults to 256. The maximum value is 1024.
   */
  pageSize: number;
}

/** The read group set search response. */
export interface SearchReadGroupSetsResponse {
  /** The list of matching read group sets. */
  readGroupSets: ReadGroupSet[];
  /**
   * The continuation token, which is used to page through large result sets.
   * Provide this value in a subsequent request to return the next page of
   * results. This field will be empty if there aren't any additional results.
   */
  nextPageToken: string;
}

/** The read group set import request. */
export interface ImportReadGroupSetsRequest {
  /**
   * Required. The ID of the dataset these read group sets will belong to. The
   * caller must have WRITE permissions to this dataset.
   */
  datasetId: string;
  /**
   * The reference set to which the imported read group sets are aligned to, if
   * any. The reference names of this reference set must be a superset of those
   * found in the imported file headers. If no reference set id is provided, a
   * best effort is made to associate with a matching reference set.
   */
  referenceSetId: string;
  /**
   * A list of URIs pointing at [BAM
   * files](https://samtools.github.io/hts-specs/SAMv1.pdf)
   * in Google Cloud Storage.
   * Those URIs can include wildcards (*), but do not add or remove
   * matching files before import has completed.
   *
   * Note that Google Cloud Storage object listing is only eventually
   * consistent: files added may be not be immediately visible to
   * everyone. Thus, if using a wildcard it is preferable not to start
   * the import immediately after the files are created.
   */
  sourceUris: string[];
  /**
   * The partition strategy describes how read groups are partitioned into read
   * group sets.
   */
  partitionStrategy: ImportReadGroupSetsRequest_PartitionStrategy;
}

export enum ImportReadGroupSetsRequest_PartitionStrategy {
  PARTITION_STRATEGY_UNSPECIFIED = 0,
  /**
   * PER_FILE_PER_SAMPLE - In most cases, this strategy yields one read group set per file. This is
   * the default behavior.
   *
   * Allocate one read group set per file per sample. For BAM files, read
   * groups are considered to share a sample if they have identical sample
   * names. Furthermore, all reads for each file which do not belong to a read
   * group, if any, will be grouped into a single read group set per-file.
   */
  PER_FILE_PER_SAMPLE = 1,
  /**
   * MERGE_ALL - Includes all read groups in all imported files into a single read group
   * set. Requires that the headers for all imported files are equivalent. All
   * reads which do not belong to a read group, if any, will be grouped into a
   * separate read group set.
   */
  MERGE_ALL = 2,
  UNRECOGNIZED = -1,
}

export function importReadGroupSetsRequest_PartitionStrategyFromJSON(
  object: any,
): ImportReadGroupSetsRequest_PartitionStrategy {
  switch (object) {
    case 0:
    case "PARTITION_STRATEGY_UNSPECIFIED":
      return ImportReadGroupSetsRequest_PartitionStrategy.PARTITION_STRATEGY_UNSPECIFIED;
    case 1:
    case "PER_FILE_PER_SAMPLE":
      return ImportReadGroupSetsRequest_PartitionStrategy.PER_FILE_PER_SAMPLE;
    case 2:
    case "MERGE_ALL":
      return ImportReadGroupSetsRequest_PartitionStrategy.MERGE_ALL;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ImportReadGroupSetsRequest_PartitionStrategy.UNRECOGNIZED;
  }
}

export function importReadGroupSetsRequest_PartitionStrategyToJSON(
  object: ImportReadGroupSetsRequest_PartitionStrategy,
): string {
  switch (object) {
    case ImportReadGroupSetsRequest_PartitionStrategy.PARTITION_STRATEGY_UNSPECIFIED:
      return "PARTITION_STRATEGY_UNSPECIFIED";
    case ImportReadGroupSetsRequest_PartitionStrategy.PER_FILE_PER_SAMPLE:
      return "PER_FILE_PER_SAMPLE";
    case ImportReadGroupSetsRequest_PartitionStrategy.MERGE_ALL:
      return "MERGE_ALL";
    case ImportReadGroupSetsRequest_PartitionStrategy.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** The read group set import response. */
export interface ImportReadGroupSetsResponse {
  /** IDs of the read group sets that were created. */
  readGroupSetIds: string[];
}

/** The read group set export request. */
export interface ExportReadGroupSetRequest {
  /**
   * Required. The Google Cloud project ID that owns this
   * export. The caller must have WRITE access to this project.
   */
  projectId: string;
  /**
   * Required. A Google Cloud Storage URI for the exported BAM file.
   * The currently authenticated user must have write access to the new file.
   * An error will be returned if the URI already contains data.
   */
  exportUri: string;
  /**
   * Required. The ID of the read group set to export. The caller must have
   * READ access to this read group set.
   */
  readGroupSetId: string;
  /**
   * The reference names to export. If this is not specified, all reference
   * sequences, including unmapped reads, are exported.
   * Use `*` to export only unmapped reads.
   */
  referenceNames: string[];
}

export interface UpdateReadGroupSetRequest {
  /**
   * The ID of the read group set to be updated. The caller must have WRITE
   * permissions to the dataset associated with this read group set.
   */
  readGroupSetId: string;
  /**
   * The new read group set data. See `updateMask` for details on mutability of
   * fields.
   */
  readGroupSet:
    | ReadGroupSet
    | undefined;
  /**
   * An optional mask specifying which fields to update. Supported fields:
   *
   * * [name][google.genomics.v1.ReadGroupSet.name].
   * * [referenceSetId][google.genomics.v1.ReadGroupSet.reference_set_id].
   *
   * Leaving `updateMask` unset is equivalent to specifying all mutable
   * fields.
   */
  updateMask: string[] | undefined;
}

export interface DeleteReadGroupSetRequest {
  /**
   * The ID of the read group set to be deleted. The caller must have WRITE
   * permissions to the dataset associated with this read group set.
   */
  readGroupSetId: string;
}

export interface GetReadGroupSetRequest {
  /** The ID of the read group set. */
  readGroupSetId: string;
}

export interface ListCoverageBucketsRequest {
  /** Required. The ID of the read group set over which coverage is requested. */
  readGroupSetId: string;
  /**
   * The name of the reference to query, within the reference set associated
   * with this query. Optional.
   */
  referenceName: string;
  /**
   * The start position of the range on the reference, 0-based inclusive. If
   * specified, `referenceName` must also be specified. Defaults to 0.
   */
  start: Long;
  /**
   * The end position of the range on the reference, 0-based exclusive. If
   * specified, `referenceName` must also be specified. If unset or 0, defaults
   * to the length of the reference.
   */
  end: Long;
  /**
   * The desired width of each reported coverage bucket in base pairs. This
   * will be rounded down to the nearest precomputed bucket width; the value
   * of which is returned as `bucketWidth` in the response. Defaults
   * to infinity (each bucket spans an entire reference sequence) or the length
   * of the target range, if specified. The smallest precomputed
   * `bucketWidth` is currently 2048 base pairs; this is subject to
   * change.
   */
  targetBucketWidth: Long;
  /**
   * The continuation token, which is used to page through large result sets.
   * To get the next page of results, set this parameter to the value of
   * `nextPageToken` from the previous response.
   */
  pageToken: string;
  /**
   * The maximum number of results to return in a single page. If unspecified,
   * defaults to 1024. The maximum value is 2048.
   */
  pageSize: number;
}

/**
 * A bucket over which read coverage has been precomputed. A bucket corresponds
 * to a specific range of the reference sequence.
 */
export interface CoverageBucket {
  /** The genomic coordinate range spanned by this bucket. */
  range:
    | Range
    | undefined;
  /**
   * The average number of reads which are aligned to each individual
   * reference base in this bucket.
   */
  meanCoverage: number;
}

export interface ListCoverageBucketsResponse {
  /**
   * The length of each coverage bucket in base pairs. Note that buckets at the
   * end of a reference sequence may be shorter. This value is omitted if the
   * bucket width is infinity (the default behaviour, with no range or
   * `targetBucketWidth`).
   */
  bucketWidth: Long;
  /**
   * The coverage buckets. The list of buckets is sparse; a bucket with 0
   * overlapping reads is not returned. A bucket never crosses more than one
   * reference sequence. Each bucket has width `bucketWidth`, unless
   * its end is the end of the reference sequence.
   */
  coverageBuckets: CoverageBucket[];
  /**
   * The continuation token, which is used to page through large result sets.
   * Provide this value in a subsequent request to return the next page of
   * results. This field will be empty if there aren't any additional results.
   */
  nextPageToken: string;
}

/** The read search request. */
export interface SearchReadsRequest {
  /**
   * The IDs of the read groups sets within which to search for reads. All
   * specified read group sets must be aligned against a common set of reference
   * sequences; this defines the genomic coordinates for the query. Must specify
   * one of `readGroupSetIds` or `readGroupIds`.
   */
  readGroupSetIds: string[];
  /**
   * The IDs of the read groups within which to search for reads. All specified
   * read groups must belong to the same read group sets. Must specify one of
   * `readGroupSetIds` or `readGroupIds`.
   */
  readGroupIds: string[];
  /**
   * The reference sequence name, for example `chr1`, `1`, or `chrX`. If set to
   * `*`, only unmapped reads are returned. If unspecified, all reads (mapped
   * and unmapped) are returned.
   */
  referenceName: string;
  /**
   * The start position of the range on the reference, 0-based inclusive. If
   * specified, `referenceName` must also be specified.
   */
  start: Long;
  /**
   * The end position of the range on the reference, 0-based exclusive. If
   * specified, `referenceName` must also be specified.
   */
  end: Long;
  /**
   * The continuation token, which is used to page through large result sets.
   * To get the next page of results, set this parameter to the value of
   * `nextPageToken` from the previous response.
   */
  pageToken: string;
  /**
   * The maximum number of results to return in a single page. If unspecified,
   * defaults to 256. The maximum value is 2048.
   */
  pageSize: number;
}

/** The read search response. */
export interface SearchReadsResponse {
  /**
   * The list of matching alignments sorted by mapped genomic coordinate,
   * if any, ascending in position within the same reference. Unmapped reads,
   * which have no position, are returned contiguously and are sorted in
   * ascending lexicographic order by fragment name.
   */
  alignments: Read[];
  /**
   * The continuation token, which is used to page through large result sets.
   * Provide this value in a subsequent request to return the next page of
   * results. This field will be empty if there aren't any additional results.
   */
  nextPageToken: string;
}

/** The stream reads request. */
export interface StreamReadsRequest {
  /**
   * The Google Cloud project ID which will be billed
   * for this access. The caller must have WRITE access to this project.
   * Required.
   */
  projectId: string;
  /** The ID of the read group set from which to stream reads. */
  readGroupSetId: string;
  /**
   * The reference sequence name, for example `chr1`,
   * `1`, or `chrX`. If set to *, only unmapped reads are
   * returned.
   */
  referenceName: string;
  /**
   * The start position of the range on the reference, 0-based inclusive. If
   * specified, `referenceName` must also be specified.
   */
  start: Long;
  /**
   * The end position of the range on the reference, 0-based exclusive. If
   * specified, `referenceName` must also be specified.
   */
  end: Long;
  /**
   * Restricts results to a shard containing approximately `1/totalShards`
   * of the normal response payload for this query. Results from a sharded
   * request are disjoint from those returned by all queries which differ only
   * in their shard parameter. A shard may yield 0 results; this is especially
   * likely for large values of `totalShards`.
   *
   * Valid values are `[0, totalShards)`.
   */
  shard: number;
  /**
   * Specifying `totalShards` causes a disjoint subset of the normal response
   * payload to be returned for each query with a unique `shard` parameter
   * specified. A best effort is made to yield equally sized shards. Sharding
   * can be used to distribute processing amongst workers, where each worker is
   * assigned a unique `shard` number and all workers specify the same
   * `totalShards` number. The union of reads returned for all sharded queries
   * `[0, totalShards)` is equal to those returned by a single unsharded query.
   *
   * Queries for different values of `totalShards` with common divisors will
   * share shard boundaries. For example, streaming `shard` 2 of 5
   * `totalShards` yields the same results as streaming `shard`s 4 and 5 of 10
   * `totalShards`. This property can be leveraged for adaptive retries.
   */
  totalShards: number;
}

export interface StreamReadsResponse {
  alignments: Read[];
}

function createBaseSearchReadGroupSetsRequest(): SearchReadGroupSetsRequest {
  return { datasetIds: [], name: "", pageToken: "", pageSize: 0 };
}

export const SearchReadGroupSetsRequest: MessageFns<SearchReadGroupSetsRequest> = {
  encode(message: SearchReadGroupSetsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.datasetIds) {
      writer.uint32(10).string(v!);
    }
    if (message.name !== "") {
      writer.uint32(26).string(message.name);
    }
    if (message.pageToken !== "") {
      writer.uint32(18).string(message.pageToken);
    }
    if (message.pageSize !== 0) {
      writer.uint32(32).int32(message.pageSize);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SearchReadGroupSetsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSearchReadGroupSetsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.datasetIds.push(reader.string());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.pageToken = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SearchReadGroupSetsRequest {
    return {
      datasetIds: globalThis.Array.isArray(object?.datasetIds)
        ? object.datasetIds.map((e: any) => globalThis.String(e))
        : [],
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
    };
  },

  toJSON(message: SearchReadGroupSetsRequest): unknown {
    const obj: any = {};
    if (message.datasetIds?.length) {
      obj.datasetIds = message.datasetIds;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    return obj;
  },

  create(base?: DeepPartial<SearchReadGroupSetsRequest>): SearchReadGroupSetsRequest {
    return SearchReadGroupSetsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SearchReadGroupSetsRequest>): SearchReadGroupSetsRequest {
    const message = createBaseSearchReadGroupSetsRequest();
    message.datasetIds = object.datasetIds?.map((e) => e) || [];
    message.name = object.name ?? "";
    message.pageToken = object.pageToken ?? "";
    message.pageSize = object.pageSize ?? 0;
    return message;
  },
};

function createBaseSearchReadGroupSetsResponse(): SearchReadGroupSetsResponse {
  return { readGroupSets: [], nextPageToken: "" };
}

export const SearchReadGroupSetsResponse: MessageFns<SearchReadGroupSetsResponse> = {
  encode(message: SearchReadGroupSetsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.readGroupSets) {
      ReadGroupSet.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SearchReadGroupSetsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSearchReadGroupSetsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.readGroupSets.push(ReadGroupSet.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SearchReadGroupSetsResponse {
    return {
      readGroupSets: globalThis.Array.isArray(object?.readGroupSets)
        ? object.readGroupSets.map((e: any) => ReadGroupSet.fromJSON(e))
        : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: SearchReadGroupSetsResponse): unknown {
    const obj: any = {};
    if (message.readGroupSets?.length) {
      obj.readGroupSets = message.readGroupSets.map((e) => ReadGroupSet.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<SearchReadGroupSetsResponse>): SearchReadGroupSetsResponse {
    return SearchReadGroupSetsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SearchReadGroupSetsResponse>): SearchReadGroupSetsResponse {
    const message = createBaseSearchReadGroupSetsResponse();
    message.readGroupSets = object.readGroupSets?.map((e) => ReadGroupSet.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBaseImportReadGroupSetsRequest(): ImportReadGroupSetsRequest {
  return { datasetId: "", referenceSetId: "", sourceUris: [], partitionStrategy: 0 };
}

export const ImportReadGroupSetsRequest: MessageFns<ImportReadGroupSetsRequest> = {
  encode(message: ImportReadGroupSetsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.datasetId !== "") {
      writer.uint32(10).string(message.datasetId);
    }
    if (message.referenceSetId !== "") {
      writer.uint32(34).string(message.referenceSetId);
    }
    for (const v of message.sourceUris) {
      writer.uint32(18).string(v!);
    }
    if (message.partitionStrategy !== 0) {
      writer.uint32(40).int32(message.partitionStrategy);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportReadGroupSetsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportReadGroupSetsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.referenceSetId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.sourceUris.push(reader.string());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.partitionStrategy = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportReadGroupSetsRequest {
    return {
      datasetId: isSet(object.datasetId) ? globalThis.String(object.datasetId) : "",
      referenceSetId: isSet(object.referenceSetId) ? globalThis.String(object.referenceSetId) : "",
      sourceUris: globalThis.Array.isArray(object?.sourceUris)
        ? object.sourceUris.map((e: any) => globalThis.String(e))
        : [],
      partitionStrategy: isSet(object.partitionStrategy)
        ? importReadGroupSetsRequest_PartitionStrategyFromJSON(object.partitionStrategy)
        : 0,
    };
  },

  toJSON(message: ImportReadGroupSetsRequest): unknown {
    const obj: any = {};
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    if (message.referenceSetId !== "") {
      obj.referenceSetId = message.referenceSetId;
    }
    if (message.sourceUris?.length) {
      obj.sourceUris = message.sourceUris;
    }
    if (message.partitionStrategy !== 0) {
      obj.partitionStrategy = importReadGroupSetsRequest_PartitionStrategyToJSON(message.partitionStrategy);
    }
    return obj;
  },

  create(base?: DeepPartial<ImportReadGroupSetsRequest>): ImportReadGroupSetsRequest {
    return ImportReadGroupSetsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportReadGroupSetsRequest>): ImportReadGroupSetsRequest {
    const message = createBaseImportReadGroupSetsRequest();
    message.datasetId = object.datasetId ?? "";
    message.referenceSetId = object.referenceSetId ?? "";
    message.sourceUris = object.sourceUris?.map((e) => e) || [];
    message.partitionStrategy = object.partitionStrategy ?? 0;
    return message;
  },
};

function createBaseImportReadGroupSetsResponse(): ImportReadGroupSetsResponse {
  return { readGroupSetIds: [] };
}

export const ImportReadGroupSetsResponse: MessageFns<ImportReadGroupSetsResponse> = {
  encode(message: ImportReadGroupSetsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.readGroupSetIds) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ImportReadGroupSetsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseImportReadGroupSetsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.readGroupSetIds.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ImportReadGroupSetsResponse {
    return {
      readGroupSetIds: globalThis.Array.isArray(object?.readGroupSetIds)
        ? object.readGroupSetIds.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: ImportReadGroupSetsResponse): unknown {
    const obj: any = {};
    if (message.readGroupSetIds?.length) {
      obj.readGroupSetIds = message.readGroupSetIds;
    }
    return obj;
  },

  create(base?: DeepPartial<ImportReadGroupSetsResponse>): ImportReadGroupSetsResponse {
    return ImportReadGroupSetsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ImportReadGroupSetsResponse>): ImportReadGroupSetsResponse {
    const message = createBaseImportReadGroupSetsResponse();
    message.readGroupSetIds = object.readGroupSetIds?.map((e) => e) || [];
    return message;
  },
};

function createBaseExportReadGroupSetRequest(): ExportReadGroupSetRequest {
  return { projectId: "", exportUri: "", readGroupSetId: "", referenceNames: [] };
}

export const ExportReadGroupSetRequest: MessageFns<ExportReadGroupSetRequest> = {
  encode(message: ExportReadGroupSetRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.exportUri !== "") {
      writer.uint32(18).string(message.exportUri);
    }
    if (message.readGroupSetId !== "") {
      writer.uint32(26).string(message.readGroupSetId);
    }
    for (const v of message.referenceNames) {
      writer.uint32(34).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExportReadGroupSetRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExportReadGroupSetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.exportUri = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.readGroupSetId = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.referenceNames.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExportReadGroupSetRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      exportUri: isSet(object.exportUri) ? globalThis.String(object.exportUri) : "",
      readGroupSetId: isSet(object.readGroupSetId) ? globalThis.String(object.readGroupSetId) : "",
      referenceNames: globalThis.Array.isArray(object?.referenceNames)
        ? object.referenceNames.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: ExportReadGroupSetRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.exportUri !== "") {
      obj.exportUri = message.exportUri;
    }
    if (message.readGroupSetId !== "") {
      obj.readGroupSetId = message.readGroupSetId;
    }
    if (message.referenceNames?.length) {
      obj.referenceNames = message.referenceNames;
    }
    return obj;
  },

  create(base?: DeepPartial<ExportReadGroupSetRequest>): ExportReadGroupSetRequest {
    return ExportReadGroupSetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExportReadGroupSetRequest>): ExportReadGroupSetRequest {
    const message = createBaseExportReadGroupSetRequest();
    message.projectId = object.projectId ?? "";
    message.exportUri = object.exportUri ?? "";
    message.readGroupSetId = object.readGroupSetId ?? "";
    message.referenceNames = object.referenceNames?.map((e) => e) || [];
    return message;
  },
};

function createBaseUpdateReadGroupSetRequest(): UpdateReadGroupSetRequest {
  return { readGroupSetId: "", readGroupSet: undefined, updateMask: undefined };
}

export const UpdateReadGroupSetRequest: MessageFns<UpdateReadGroupSetRequest> = {
  encode(message: UpdateReadGroupSetRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.readGroupSetId !== "") {
      writer.uint32(10).string(message.readGroupSetId);
    }
    if (message.readGroupSet !== undefined) {
      ReadGroupSet.encode(message.readGroupSet, writer.uint32(18).fork()).join();
    }
    if (message.updateMask !== undefined) {
      FieldMask.encode(FieldMask.wrap(message.updateMask), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpdateReadGroupSetRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpdateReadGroupSetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.readGroupSetId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.readGroupSet = ReadGroupSet.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.updateMask = FieldMask.unwrap(FieldMask.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpdateReadGroupSetRequest {
    return {
      readGroupSetId: isSet(object.readGroupSetId) ? globalThis.String(object.readGroupSetId) : "",
      readGroupSet: isSet(object.readGroupSet) ? ReadGroupSet.fromJSON(object.readGroupSet) : undefined,
      updateMask: isSet(object.updateMask) ? FieldMask.unwrap(FieldMask.fromJSON(object.updateMask)) : undefined,
    };
  },

  toJSON(message: UpdateReadGroupSetRequest): unknown {
    const obj: any = {};
    if (message.readGroupSetId !== "") {
      obj.readGroupSetId = message.readGroupSetId;
    }
    if (message.readGroupSet !== undefined) {
      obj.readGroupSet = ReadGroupSet.toJSON(message.readGroupSet);
    }
    if (message.updateMask !== undefined) {
      obj.updateMask = FieldMask.toJSON(FieldMask.wrap(message.updateMask));
    }
    return obj;
  },

  create(base?: DeepPartial<UpdateReadGroupSetRequest>): UpdateReadGroupSetRequest {
    return UpdateReadGroupSetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpdateReadGroupSetRequest>): UpdateReadGroupSetRequest {
    const message = createBaseUpdateReadGroupSetRequest();
    message.readGroupSetId = object.readGroupSetId ?? "";
    message.readGroupSet = (object.readGroupSet !== undefined && object.readGroupSet !== null)
      ? ReadGroupSet.fromPartial(object.readGroupSet)
      : undefined;
    message.updateMask = object.updateMask ?? undefined;
    return message;
  },
};

function createBaseDeleteReadGroupSetRequest(): DeleteReadGroupSetRequest {
  return { readGroupSetId: "" };
}

export const DeleteReadGroupSetRequest: MessageFns<DeleteReadGroupSetRequest> = {
  encode(message: DeleteReadGroupSetRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.readGroupSetId !== "") {
      writer.uint32(10).string(message.readGroupSetId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeleteReadGroupSetRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeleteReadGroupSetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.readGroupSetId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeleteReadGroupSetRequest {
    return { readGroupSetId: isSet(object.readGroupSetId) ? globalThis.String(object.readGroupSetId) : "" };
  },

  toJSON(message: DeleteReadGroupSetRequest): unknown {
    const obj: any = {};
    if (message.readGroupSetId !== "") {
      obj.readGroupSetId = message.readGroupSetId;
    }
    return obj;
  },

  create(base?: DeepPartial<DeleteReadGroupSetRequest>): DeleteReadGroupSetRequest {
    return DeleteReadGroupSetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeleteReadGroupSetRequest>): DeleteReadGroupSetRequest {
    const message = createBaseDeleteReadGroupSetRequest();
    message.readGroupSetId = object.readGroupSetId ?? "";
    return message;
  },
};

function createBaseGetReadGroupSetRequest(): GetReadGroupSetRequest {
  return { readGroupSetId: "" };
}

export const GetReadGroupSetRequest: MessageFns<GetReadGroupSetRequest> = {
  encode(message: GetReadGroupSetRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.readGroupSetId !== "") {
      writer.uint32(10).string(message.readGroupSetId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetReadGroupSetRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetReadGroupSetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.readGroupSetId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetReadGroupSetRequest {
    return { readGroupSetId: isSet(object.readGroupSetId) ? globalThis.String(object.readGroupSetId) : "" };
  },

  toJSON(message: GetReadGroupSetRequest): unknown {
    const obj: any = {};
    if (message.readGroupSetId !== "") {
      obj.readGroupSetId = message.readGroupSetId;
    }
    return obj;
  },

  create(base?: DeepPartial<GetReadGroupSetRequest>): GetReadGroupSetRequest {
    return GetReadGroupSetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetReadGroupSetRequest>): GetReadGroupSetRequest {
    const message = createBaseGetReadGroupSetRequest();
    message.readGroupSetId = object.readGroupSetId ?? "";
    return message;
  },
};

function createBaseListCoverageBucketsRequest(): ListCoverageBucketsRequest {
  return {
    readGroupSetId: "",
    referenceName: "",
    start: Long.ZERO,
    end: Long.ZERO,
    targetBucketWidth: Long.ZERO,
    pageToken: "",
    pageSize: 0,
  };
}

export const ListCoverageBucketsRequest: MessageFns<ListCoverageBucketsRequest> = {
  encode(message: ListCoverageBucketsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.readGroupSetId !== "") {
      writer.uint32(10).string(message.readGroupSetId);
    }
    if (message.referenceName !== "") {
      writer.uint32(26).string(message.referenceName);
    }
    if (!message.start.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.start.toString());
    }
    if (!message.end.equals(Long.ZERO)) {
      writer.uint32(40).int64(message.end.toString());
    }
    if (!message.targetBucketWidth.equals(Long.ZERO)) {
      writer.uint32(48).int64(message.targetBucketWidth.toString());
    }
    if (message.pageToken !== "") {
      writer.uint32(58).string(message.pageToken);
    }
    if (message.pageSize !== 0) {
      writer.uint32(64).int32(message.pageSize);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListCoverageBucketsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListCoverageBucketsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.readGroupSetId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.referenceName = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.start = Long.fromString(reader.int64().toString());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.end = Long.fromString(reader.int64().toString());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.targetBucketWidth = Long.fromString(reader.int64().toString());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.pageToken = reader.string();
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListCoverageBucketsRequest {
    return {
      readGroupSetId: isSet(object.readGroupSetId) ? globalThis.String(object.readGroupSetId) : "",
      referenceName: isSet(object.referenceName) ? globalThis.String(object.referenceName) : "",
      start: isSet(object.start) ? Long.fromValue(object.start) : Long.ZERO,
      end: isSet(object.end) ? Long.fromValue(object.end) : Long.ZERO,
      targetBucketWidth: isSet(object.targetBucketWidth) ? Long.fromValue(object.targetBucketWidth) : Long.ZERO,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
    };
  },

  toJSON(message: ListCoverageBucketsRequest): unknown {
    const obj: any = {};
    if (message.readGroupSetId !== "") {
      obj.readGroupSetId = message.readGroupSetId;
    }
    if (message.referenceName !== "") {
      obj.referenceName = message.referenceName;
    }
    if (!message.start.equals(Long.ZERO)) {
      obj.start = (message.start || Long.ZERO).toString();
    }
    if (!message.end.equals(Long.ZERO)) {
      obj.end = (message.end || Long.ZERO).toString();
    }
    if (!message.targetBucketWidth.equals(Long.ZERO)) {
      obj.targetBucketWidth = (message.targetBucketWidth || Long.ZERO).toString();
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    return obj;
  },

  create(base?: DeepPartial<ListCoverageBucketsRequest>): ListCoverageBucketsRequest {
    return ListCoverageBucketsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListCoverageBucketsRequest>): ListCoverageBucketsRequest {
    const message = createBaseListCoverageBucketsRequest();
    message.readGroupSetId = object.readGroupSetId ?? "";
    message.referenceName = object.referenceName ?? "";
    message.start = (object.start !== undefined && object.start !== null) ? Long.fromValue(object.start) : Long.ZERO;
    message.end = (object.end !== undefined && object.end !== null) ? Long.fromValue(object.end) : Long.ZERO;
    message.targetBucketWidth = (object.targetBucketWidth !== undefined && object.targetBucketWidth !== null)
      ? Long.fromValue(object.targetBucketWidth)
      : Long.ZERO;
    message.pageToken = object.pageToken ?? "";
    message.pageSize = object.pageSize ?? 0;
    return message;
  },
};

function createBaseCoverageBucket(): CoverageBucket {
  return { range: undefined, meanCoverage: 0 };
}

export const CoverageBucket: MessageFns<CoverageBucket> = {
  encode(message: CoverageBucket, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.range !== undefined) {
      Range.encode(message.range, writer.uint32(10).fork()).join();
    }
    if (message.meanCoverage !== 0) {
      writer.uint32(21).float(message.meanCoverage);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CoverageBucket {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCoverageBucket();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.range = Range.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.meanCoverage = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CoverageBucket {
    return {
      range: isSet(object.range) ? Range.fromJSON(object.range) : undefined,
      meanCoverage: isSet(object.meanCoverage) ? globalThis.Number(object.meanCoverage) : 0,
    };
  },

  toJSON(message: CoverageBucket): unknown {
    const obj: any = {};
    if (message.range !== undefined) {
      obj.range = Range.toJSON(message.range);
    }
    if (message.meanCoverage !== 0) {
      obj.meanCoverage = message.meanCoverage;
    }
    return obj;
  },

  create(base?: DeepPartial<CoverageBucket>): CoverageBucket {
    return CoverageBucket.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CoverageBucket>): CoverageBucket {
    const message = createBaseCoverageBucket();
    message.range = (object.range !== undefined && object.range !== null) ? Range.fromPartial(object.range) : undefined;
    message.meanCoverage = object.meanCoverage ?? 0;
    return message;
  },
};

function createBaseListCoverageBucketsResponse(): ListCoverageBucketsResponse {
  return { bucketWidth: Long.ZERO, coverageBuckets: [], nextPageToken: "" };
}

export const ListCoverageBucketsResponse: MessageFns<ListCoverageBucketsResponse> = {
  encode(message: ListCoverageBucketsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.bucketWidth.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.bucketWidth.toString());
    }
    for (const v of message.coverageBuckets) {
      CoverageBucket.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(26).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListCoverageBucketsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListCoverageBucketsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.bucketWidth = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.coverageBuckets.push(CoverageBucket.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListCoverageBucketsResponse {
    return {
      bucketWidth: isSet(object.bucketWidth) ? Long.fromValue(object.bucketWidth) : Long.ZERO,
      coverageBuckets: globalThis.Array.isArray(object?.coverageBuckets)
        ? object.coverageBuckets.map((e: any) => CoverageBucket.fromJSON(e))
        : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: ListCoverageBucketsResponse): unknown {
    const obj: any = {};
    if (!message.bucketWidth.equals(Long.ZERO)) {
      obj.bucketWidth = (message.bucketWidth || Long.ZERO).toString();
    }
    if (message.coverageBuckets?.length) {
      obj.coverageBuckets = message.coverageBuckets.map((e) => CoverageBucket.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListCoverageBucketsResponse>): ListCoverageBucketsResponse {
    return ListCoverageBucketsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListCoverageBucketsResponse>): ListCoverageBucketsResponse {
    const message = createBaseListCoverageBucketsResponse();
    message.bucketWidth = (object.bucketWidth !== undefined && object.bucketWidth !== null)
      ? Long.fromValue(object.bucketWidth)
      : Long.ZERO;
    message.coverageBuckets = object.coverageBuckets?.map((e) => CoverageBucket.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBaseSearchReadsRequest(): SearchReadsRequest {
  return {
    readGroupSetIds: [],
    readGroupIds: [],
    referenceName: "",
    start: Long.ZERO,
    end: Long.ZERO,
    pageToken: "",
    pageSize: 0,
  };
}

export const SearchReadsRequest: MessageFns<SearchReadsRequest> = {
  encode(message: SearchReadsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.readGroupSetIds) {
      writer.uint32(10).string(v!);
    }
    for (const v of message.readGroupIds) {
      writer.uint32(42).string(v!);
    }
    if (message.referenceName !== "") {
      writer.uint32(58).string(message.referenceName);
    }
    if (!message.start.equals(Long.ZERO)) {
      writer.uint32(64).int64(message.start.toString());
    }
    if (!message.end.equals(Long.ZERO)) {
      writer.uint32(72).int64(message.end.toString());
    }
    if (message.pageToken !== "") {
      writer.uint32(26).string(message.pageToken);
    }
    if (message.pageSize !== 0) {
      writer.uint32(32).int32(message.pageSize);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SearchReadsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSearchReadsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.readGroupSetIds.push(reader.string());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.readGroupIds.push(reader.string());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.referenceName = reader.string();
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.start = Long.fromString(reader.int64().toString());
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.end = Long.fromString(reader.int64().toString());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pageToken = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SearchReadsRequest {
    return {
      readGroupSetIds: globalThis.Array.isArray(object?.readGroupSetIds)
        ? object.readGroupSetIds.map((e: any) => globalThis.String(e))
        : [],
      readGroupIds: globalThis.Array.isArray(object?.readGroupIds)
        ? object.readGroupIds.map((e: any) => globalThis.String(e))
        : [],
      referenceName: isSet(object.referenceName) ? globalThis.String(object.referenceName) : "",
      start: isSet(object.start) ? Long.fromValue(object.start) : Long.ZERO,
      end: isSet(object.end) ? Long.fromValue(object.end) : Long.ZERO,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
    };
  },

  toJSON(message: SearchReadsRequest): unknown {
    const obj: any = {};
    if (message.readGroupSetIds?.length) {
      obj.readGroupSetIds = message.readGroupSetIds;
    }
    if (message.readGroupIds?.length) {
      obj.readGroupIds = message.readGroupIds;
    }
    if (message.referenceName !== "") {
      obj.referenceName = message.referenceName;
    }
    if (!message.start.equals(Long.ZERO)) {
      obj.start = (message.start || Long.ZERO).toString();
    }
    if (!message.end.equals(Long.ZERO)) {
      obj.end = (message.end || Long.ZERO).toString();
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    return obj;
  },

  create(base?: DeepPartial<SearchReadsRequest>): SearchReadsRequest {
    return SearchReadsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SearchReadsRequest>): SearchReadsRequest {
    const message = createBaseSearchReadsRequest();
    message.readGroupSetIds = object.readGroupSetIds?.map((e) => e) || [];
    message.readGroupIds = object.readGroupIds?.map((e) => e) || [];
    message.referenceName = object.referenceName ?? "";
    message.start = (object.start !== undefined && object.start !== null) ? Long.fromValue(object.start) : Long.ZERO;
    message.end = (object.end !== undefined && object.end !== null) ? Long.fromValue(object.end) : Long.ZERO;
    message.pageToken = object.pageToken ?? "";
    message.pageSize = object.pageSize ?? 0;
    return message;
  },
};

function createBaseSearchReadsResponse(): SearchReadsResponse {
  return { alignments: [], nextPageToken: "" };
}

export const SearchReadsResponse: MessageFns<SearchReadsResponse> = {
  encode(message: SearchReadsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.alignments) {
      Read.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SearchReadsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSearchReadsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.alignments.push(Read.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SearchReadsResponse {
    return {
      alignments: globalThis.Array.isArray(object?.alignments)
        ? object.alignments.map((e: any) => Read.fromJSON(e))
        : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: SearchReadsResponse): unknown {
    const obj: any = {};
    if (message.alignments?.length) {
      obj.alignments = message.alignments.map((e) => Read.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<SearchReadsResponse>): SearchReadsResponse {
    return SearchReadsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SearchReadsResponse>): SearchReadsResponse {
    const message = createBaseSearchReadsResponse();
    message.alignments = object.alignments?.map((e) => Read.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBaseStreamReadsRequest(): StreamReadsRequest {
  return {
    projectId: "",
    readGroupSetId: "",
    referenceName: "",
    start: Long.ZERO,
    end: Long.ZERO,
    shard: 0,
    totalShards: 0,
  };
}

export const StreamReadsRequest: MessageFns<StreamReadsRequest> = {
  encode(message: StreamReadsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.projectId !== "") {
      writer.uint32(10).string(message.projectId);
    }
    if (message.readGroupSetId !== "") {
      writer.uint32(18).string(message.readGroupSetId);
    }
    if (message.referenceName !== "") {
      writer.uint32(26).string(message.referenceName);
    }
    if (!message.start.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.start.toString());
    }
    if (!message.end.equals(Long.ZERO)) {
      writer.uint32(40).int64(message.end.toString());
    }
    if (message.shard !== 0) {
      writer.uint32(48).int32(message.shard);
    }
    if (message.totalShards !== 0) {
      writer.uint32(56).int32(message.totalShards);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamReadsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamReadsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.projectId = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.readGroupSetId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.referenceName = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.start = Long.fromString(reader.int64().toString());
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.end = Long.fromString(reader.int64().toString());
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.shard = reader.int32();
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.totalShards = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamReadsRequest {
    return {
      projectId: isSet(object.projectId) ? globalThis.String(object.projectId) : "",
      readGroupSetId: isSet(object.readGroupSetId) ? globalThis.String(object.readGroupSetId) : "",
      referenceName: isSet(object.referenceName) ? globalThis.String(object.referenceName) : "",
      start: isSet(object.start) ? Long.fromValue(object.start) : Long.ZERO,
      end: isSet(object.end) ? Long.fromValue(object.end) : Long.ZERO,
      shard: isSet(object.shard) ? globalThis.Number(object.shard) : 0,
      totalShards: isSet(object.totalShards) ? globalThis.Number(object.totalShards) : 0,
    };
  },

  toJSON(message: StreamReadsRequest): unknown {
    const obj: any = {};
    if (message.projectId !== "") {
      obj.projectId = message.projectId;
    }
    if (message.readGroupSetId !== "") {
      obj.readGroupSetId = message.readGroupSetId;
    }
    if (message.referenceName !== "") {
      obj.referenceName = message.referenceName;
    }
    if (!message.start.equals(Long.ZERO)) {
      obj.start = (message.start || Long.ZERO).toString();
    }
    if (!message.end.equals(Long.ZERO)) {
      obj.end = (message.end || Long.ZERO).toString();
    }
    if (message.shard !== 0) {
      obj.shard = Math.round(message.shard);
    }
    if (message.totalShards !== 0) {
      obj.totalShards = Math.round(message.totalShards);
    }
    return obj;
  },

  create(base?: DeepPartial<StreamReadsRequest>): StreamReadsRequest {
    return StreamReadsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamReadsRequest>): StreamReadsRequest {
    const message = createBaseStreamReadsRequest();
    message.projectId = object.projectId ?? "";
    message.readGroupSetId = object.readGroupSetId ?? "";
    message.referenceName = object.referenceName ?? "";
    message.start = (object.start !== undefined && object.start !== null) ? Long.fromValue(object.start) : Long.ZERO;
    message.end = (object.end !== undefined && object.end !== null) ? Long.fromValue(object.end) : Long.ZERO;
    message.shard = object.shard ?? 0;
    message.totalShards = object.totalShards ?? 0;
    return message;
  },
};

function createBaseStreamReadsResponse(): StreamReadsResponse {
  return { alignments: [] };
}

export const StreamReadsResponse: MessageFns<StreamReadsResponse> = {
  encode(message: StreamReadsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.alignments) {
      Read.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamReadsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamReadsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.alignments.push(Read.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamReadsResponse {
    return {
      alignments: globalThis.Array.isArray(object?.alignments)
        ? object.alignments.map((e: any) => Read.fromJSON(e))
        : [],
    };
  },

  toJSON(message: StreamReadsResponse): unknown {
    const obj: any = {};
    if (message.alignments?.length) {
      obj.alignments = message.alignments.map((e) => Read.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<StreamReadsResponse>): StreamReadsResponse {
    return StreamReadsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<StreamReadsResponse>): StreamReadsResponse {
    const message = createBaseStreamReadsResponse();
    message.alignments = object.alignments?.map((e) => Read.fromPartial(e)) || [];
    return message;
  },
};

export type StreamingReadServiceDefinition = typeof StreamingReadServiceDefinition;
export const StreamingReadServiceDefinition = {
  name: "StreamingReadService",
  fullName: "google.genomics.v1.StreamingReadService",
  methods: {
    /**
     * Returns a stream of all the reads matching the search request, ordered
     * by reference name, position, and ID.
     */
    streamReads: {
      name: "StreamReads",
      requestType: StreamReadsRequest,
      requestStream: false,
      responseType: StreamReadsResponse,
      responseStream: true,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              21,
              58,
              1,
              42,
              34,
              16,
              47,
              118,
              49,
              47,
              114,
              101,
              97,
              100,
              115,
              58,
              115,
              116,
              114,
              101,
              97,
              109,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface StreamingReadServiceImplementation<CallContextExt = {}> {
  /**
   * Returns a stream of all the reads matching the search request, ordered
   * by reference name, position, and ID.
   */
  streamReads(
    request: StreamReadsRequest,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<StreamReadsResponse>>;
}

export interface StreamingReadServiceClient<CallOptionsExt = {}> {
  /**
   * Returns a stream of all the reads matching the search request, ordered
   * by reference name, position, and ID.
   */
  streamReads(
    request: DeepPartial<StreamReadsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<StreamReadsResponse>;
}

/** The Readstore. A data store for DNA sequencing Reads. */
export type ReadServiceV1Definition = typeof ReadServiceV1Definition;
export const ReadServiceV1Definition = {
  name: "ReadServiceV1",
  fullName: "google.genomics.v1.ReadServiceV1",
  methods: {
    /**
     * Creates read group sets by asynchronously importing the provided
     * information.
     *
     * For the definitions of read group sets and other genomics resources, see
     * [Fundamentals of Google
     * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
     *
     * The caller must have WRITE permissions to the dataset.
     *
     * ## Notes on [BAM](https://samtools.github.io/hts-specs/SAMv1.pdf) import
     *
     * - Tags will be converted to strings - tag types are not preserved
     * - Comments (`@CO`) in the input file header will not be preserved
     * - Original header order of references (`@SQ`) will not be preserved
     * - Any reverse stranded unmapped reads will be reverse complemented, and
     * their qualities (also the "BQ" and "OQ" tags, if any) will be reversed
     * - Unmapped reads will be stripped of positional information (reference name
     * and position)
     */
    importReadGroupSets: {
      name: "ImportReadGroupSets",
      requestType: ImportReadGroupSetsRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              29,
              58,
              1,
              42,
              34,
              24,
              47,
              118,
              49,
              47,
              114,
              101,
              97,
              100,
              103,
              114,
              111,
              117,
              112,
              115,
              101,
              116,
              115,
              58,
              105,
              109,
              112,
              111,
              114,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * Exports a read group set to a BAM file in Google Cloud Storage.
     *
     * For the definitions of read group sets and other genomics resources, see
     * [Fundamentals of Google
     * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
     *
     * Note that currently there may be some differences between exported BAM
     * files and the original BAM file at the time of import. See
     * [ImportReadGroupSets][google.genomics.v1.ReadServiceV1.ImportReadGroupSets]
     * for caveats.
     */
    exportReadGroupSet: {
      name: "ExportReadGroupSet",
      requestType: ExportReadGroupSetRequest,
      requestStream: false,
      responseType: Operation,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              49,
              58,
              1,
              42,
              34,
              44,
              47,
              118,
              49,
              47,
              114,
              101,
              97,
              100,
              103,
              114,
              111,
              117,
              112,
              115,
              101,
              116,
              115,
              47,
              123,
              114,
              101,
              97,
              100,
              95,
              103,
              114,
              111,
              117,
              112,
              95,
              115,
              101,
              116,
              95,
              105,
              100,
              125,
              58,
              101,
              120,
              112,
              111,
              114,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * Searches for read group sets matching the criteria.
     *
     * For the definitions of read group sets and other genomics resources, see
     * [Fundamentals of Google
     * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
     *
     * Implements
     * [GlobalAllianceApi.searchReadGroupSets](https://github.com/ga4gh/schemas/blob/v0.5.1/src/main/resources/avro/readmethods.avdl#L135).
     */
    searchReadGroupSets: {
      name: "SearchReadGroupSets",
      requestType: SearchReadGroupSetsRequest,
      requestStream: false,
      responseType: SearchReadGroupSetsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              29,
              58,
              1,
              42,
              34,
              24,
              47,
              118,
              49,
              47,
              114,
              101,
              97,
              100,
              103,
              114,
              111,
              117,
              112,
              115,
              101,
              116,
              115,
              47,
              115,
              101,
              97,
              114,
              99,
              104,
            ]),
          ],
        },
      },
    },
    /**
     * Updates a read group set.
     *
     * For the definitions of read group sets and other genomics resources, see
     * [Fundamentals of Google
     * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
     *
     * This method supports patch semantics.
     */
    updateReadGroupSet: {
      name: "UpdateReadGroupSet",
      requestType: UpdateReadGroupSetRequest,
      requestStream: false,
      responseType: ReadGroupSet,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              55,
              58,
              14,
              114,
              101,
              97,
              100,
              95,
              103,
              114,
              111,
              117,
              112,
              95,
              115,
              101,
              116,
              50,
              37,
              47,
              118,
              49,
              47,
              114,
              101,
              97,
              100,
              103,
              114,
              111,
              117,
              112,
              115,
              101,
              116,
              115,
              47,
              123,
              114,
              101,
              97,
              100,
              95,
              103,
              114,
              111,
              117,
              112,
              95,
              115,
              101,
              116,
              95,
              105,
              100,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Deletes a read group set.
     *
     * For the definitions of read group sets and other genomics resources, see
     * [Fundamentals of Google
     * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
     */
    deleteReadGroupSet: {
      name: "DeleteReadGroupSet",
      requestType: DeleteReadGroupSetRequest,
      requestStream: false,
      responseType: Empty,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              39,
              42,
              37,
              47,
              118,
              49,
              47,
              114,
              101,
              97,
              100,
              103,
              114,
              111,
              117,
              112,
              115,
              101,
              116,
              115,
              47,
              123,
              114,
              101,
              97,
              100,
              95,
              103,
              114,
              111,
              117,
              112,
              95,
              115,
              101,
              116,
              95,
              105,
              100,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Gets a read group set by ID.
     *
     * For the definitions of read group sets and other genomics resources, see
     * [Fundamentals of Google
     * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
     */
    getReadGroupSet: {
      name: "GetReadGroupSet",
      requestType: GetReadGroupSetRequest,
      requestStream: false,
      responseType: ReadGroupSet,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              39,
              18,
              37,
              47,
              118,
              49,
              47,
              114,
              101,
              97,
              100,
              103,
              114,
              111,
              117,
              112,
              115,
              101,
              116,
              115,
              47,
              123,
              114,
              101,
              97,
              100,
              95,
              103,
              114,
              111,
              117,
              112,
              95,
              115,
              101,
              116,
              95,
              105,
              100,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Lists fixed width coverage buckets for a read group set, each of which
     * correspond to a range of a reference sequence. Each bucket summarizes
     * coverage information across its corresponding genomic range.
     *
     * For the definitions of read group sets and other genomics resources, see
     * [Fundamentals of Google
     * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
     *
     * Coverage is defined as the number of reads which are aligned to a given
     * base in the reference sequence. Coverage buckets are available at several
     * precomputed bucket widths, enabling retrieval of various coverage 'zoom
     * levels'. The caller must have READ permissions for the target read group
     * set.
     */
    listCoverageBuckets: {
      name: "ListCoverageBuckets",
      requestType: ListCoverageBucketsRequest,
      requestStream: false,
      responseType: ListCoverageBucketsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              55,
              18,
              53,
              47,
              118,
              49,
              47,
              114,
              101,
              97,
              100,
              103,
              114,
              111,
              117,
              112,
              115,
              101,
              116,
              115,
              47,
              123,
              114,
              101,
              97,
              100,
              95,
              103,
              114,
              111,
              117,
              112,
              95,
              115,
              101,
              116,
              95,
              105,
              100,
              125,
              47,
              99,
              111,
              118,
              101,
              114,
              97,
              103,
              101,
              98,
              117,
              99,
              107,
              101,
              116,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Gets a list of reads for one or more read group sets.
     *
     * For the definitions of read group sets and other genomics resources, see
     * [Fundamentals of Google
     * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
     *
     * Reads search operates over a genomic coordinate space of reference sequence
     * & position defined over the reference sequences to which the requested
     * read group sets are aligned.
     *
     * If a target positional range is specified, search returns all reads whose
     * alignment to the reference genome overlap the range. A query which
     * specifies only read group set IDs yields all reads in those read group
     * sets, including unmapped reads.
     *
     * All reads returned (including reads on subsequent pages) are ordered by
     * genomic coordinate (by reference sequence, then position). Reads with
     * equivalent genomic coordinates are returned in an unspecified order. This
     * order is consistent, such that two queries for the same content (regardless
     * of page size) yield reads in the same order across their respective streams
     * of paginated responses.
     *
     * Implements
     * [GlobalAllianceApi.searchReads](https://github.com/ga4gh/schemas/blob/v0.5.1/src/main/resources/avro/readmethods.avdl#L85).
     */
    searchReads: {
      name: "SearchReads",
      requestType: SearchReadsRequest,
      requestStream: false,
      responseType: SearchReadsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              21,
              58,
              1,
              42,
              34,
              16,
              47,
              118,
              49,
              47,
              114,
              101,
              97,
              100,
              115,
              47,
              115,
              101,
              97,
              114,
              99,
              104,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface ReadServiceV1ServiceImplementation<CallContextExt = {}> {
  /**
   * Creates read group sets by asynchronously importing the provided
   * information.
   *
   * For the definitions of read group sets and other genomics resources, see
   * [Fundamentals of Google
   * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
   *
   * The caller must have WRITE permissions to the dataset.
   *
   * ## Notes on [BAM](https://samtools.github.io/hts-specs/SAMv1.pdf) import
   *
   * - Tags will be converted to strings - tag types are not preserved
   * - Comments (`@CO`) in the input file header will not be preserved
   * - Original header order of references (`@SQ`) will not be preserved
   * - Any reverse stranded unmapped reads will be reverse complemented, and
   * their qualities (also the "BQ" and "OQ" tags, if any) will be reversed
   * - Unmapped reads will be stripped of positional information (reference name
   * and position)
   */
  importReadGroupSets(
    request: ImportReadGroupSetsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Operation>>;
  /**
   * Exports a read group set to a BAM file in Google Cloud Storage.
   *
   * For the definitions of read group sets and other genomics resources, see
   * [Fundamentals of Google
   * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
   *
   * Note that currently there may be some differences between exported BAM
   * files and the original BAM file at the time of import. See
   * [ImportReadGroupSets][google.genomics.v1.ReadServiceV1.ImportReadGroupSets]
   * for caveats.
   */
  exportReadGroupSet(
    request: ExportReadGroupSetRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Operation>>;
  /**
   * Searches for read group sets matching the criteria.
   *
   * For the definitions of read group sets and other genomics resources, see
   * [Fundamentals of Google
   * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
   *
   * Implements
   * [GlobalAllianceApi.searchReadGroupSets](https://github.com/ga4gh/schemas/blob/v0.5.1/src/main/resources/avro/readmethods.avdl#L135).
   */
  searchReadGroupSets(
    request: SearchReadGroupSetsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<SearchReadGroupSetsResponse>>;
  /**
   * Updates a read group set.
   *
   * For the definitions of read group sets and other genomics resources, see
   * [Fundamentals of Google
   * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
   *
   * This method supports patch semantics.
   */
  updateReadGroupSet(
    request: UpdateReadGroupSetRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ReadGroupSet>>;
  /**
   * Deletes a read group set.
   *
   * For the definitions of read group sets and other genomics resources, see
   * [Fundamentals of Google
   * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
   */
  deleteReadGroupSet(
    request: DeleteReadGroupSetRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Empty>>;
  /**
   * Gets a read group set by ID.
   *
   * For the definitions of read group sets and other genomics resources, see
   * [Fundamentals of Google
   * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
   */
  getReadGroupSet(
    request: GetReadGroupSetRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ReadGroupSet>>;
  /**
   * Lists fixed width coverage buckets for a read group set, each of which
   * correspond to a range of a reference sequence. Each bucket summarizes
   * coverage information across its corresponding genomic range.
   *
   * For the definitions of read group sets and other genomics resources, see
   * [Fundamentals of Google
   * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
   *
   * Coverage is defined as the number of reads which are aligned to a given
   * base in the reference sequence. Coverage buckets are available at several
   * precomputed bucket widths, enabling retrieval of various coverage 'zoom
   * levels'. The caller must have READ permissions for the target read group
   * set.
   */
  listCoverageBuckets(
    request: ListCoverageBucketsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ListCoverageBucketsResponse>>;
  /**
   * Gets a list of reads for one or more read group sets.
   *
   * For the definitions of read group sets and other genomics resources, see
   * [Fundamentals of Google
   * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
   *
   * Reads search operates over a genomic coordinate space of reference sequence
   * & position defined over the reference sequences to which the requested
   * read group sets are aligned.
   *
   * If a target positional range is specified, search returns all reads whose
   * alignment to the reference genome overlap the range. A query which
   * specifies only read group set IDs yields all reads in those read group
   * sets, including unmapped reads.
   *
   * All reads returned (including reads on subsequent pages) are ordered by
   * genomic coordinate (by reference sequence, then position). Reads with
   * equivalent genomic coordinates are returned in an unspecified order. This
   * order is consistent, such that two queries for the same content (regardless
   * of page size) yield reads in the same order across their respective streams
   * of paginated responses.
   *
   * Implements
   * [GlobalAllianceApi.searchReads](https://github.com/ga4gh/schemas/blob/v0.5.1/src/main/resources/avro/readmethods.avdl#L85).
   */
  searchReads(
    request: SearchReadsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<SearchReadsResponse>>;
}

export interface ReadServiceV1Client<CallOptionsExt = {}> {
  /**
   * Creates read group sets by asynchronously importing the provided
   * information.
   *
   * For the definitions of read group sets and other genomics resources, see
   * [Fundamentals of Google
   * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
   *
   * The caller must have WRITE permissions to the dataset.
   *
   * ## Notes on [BAM](https://samtools.github.io/hts-specs/SAMv1.pdf) import
   *
   * - Tags will be converted to strings - tag types are not preserved
   * - Comments (`@CO`) in the input file header will not be preserved
   * - Original header order of references (`@SQ`) will not be preserved
   * - Any reverse stranded unmapped reads will be reverse complemented, and
   * their qualities (also the "BQ" and "OQ" tags, if any) will be reversed
   * - Unmapped reads will be stripped of positional information (reference name
   * and position)
   */
  importReadGroupSets(
    request: DeepPartial<ImportReadGroupSetsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Operation>;
  /**
   * Exports a read group set to a BAM file in Google Cloud Storage.
   *
   * For the definitions of read group sets and other genomics resources, see
   * [Fundamentals of Google
   * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
   *
   * Note that currently there may be some differences between exported BAM
   * files and the original BAM file at the time of import. See
   * [ImportReadGroupSets][google.genomics.v1.ReadServiceV1.ImportReadGroupSets]
   * for caveats.
   */
  exportReadGroupSet(
    request: DeepPartial<ExportReadGroupSetRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Operation>;
  /**
   * Searches for read group sets matching the criteria.
   *
   * For the definitions of read group sets and other genomics resources, see
   * [Fundamentals of Google
   * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
   *
   * Implements
   * [GlobalAllianceApi.searchReadGroupSets](https://github.com/ga4gh/schemas/blob/v0.5.1/src/main/resources/avro/readmethods.avdl#L135).
   */
  searchReadGroupSets(
    request: DeepPartial<SearchReadGroupSetsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<SearchReadGroupSetsResponse>;
  /**
   * Updates a read group set.
   *
   * For the definitions of read group sets and other genomics resources, see
   * [Fundamentals of Google
   * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
   *
   * This method supports patch semantics.
   */
  updateReadGroupSet(
    request: DeepPartial<UpdateReadGroupSetRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ReadGroupSet>;
  /**
   * Deletes a read group set.
   *
   * For the definitions of read group sets and other genomics resources, see
   * [Fundamentals of Google
   * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
   */
  deleteReadGroupSet(
    request: DeepPartial<DeleteReadGroupSetRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Empty>;
  /**
   * Gets a read group set by ID.
   *
   * For the definitions of read group sets and other genomics resources, see
   * [Fundamentals of Google
   * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
   */
  getReadGroupSet(
    request: DeepPartial<GetReadGroupSetRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ReadGroupSet>;
  /**
   * Lists fixed width coverage buckets for a read group set, each of which
   * correspond to a range of a reference sequence. Each bucket summarizes
   * coverage information across its corresponding genomic range.
   *
   * For the definitions of read group sets and other genomics resources, see
   * [Fundamentals of Google
   * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
   *
   * Coverage is defined as the number of reads which are aligned to a given
   * base in the reference sequence. Coverage buckets are available at several
   * precomputed bucket widths, enabling retrieval of various coverage 'zoom
   * levels'. The caller must have READ permissions for the target read group
   * set.
   */
  listCoverageBuckets(
    request: DeepPartial<ListCoverageBucketsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ListCoverageBucketsResponse>;
  /**
   * Gets a list of reads for one or more read group sets.
   *
   * For the definitions of read group sets and other genomics resources, see
   * [Fundamentals of Google
   * Genomics](https://cloud.google.com/genomics/fundamentals-of-google-genomics)
   *
   * Reads search operates over a genomic coordinate space of reference sequence
   * & position defined over the reference sequences to which the requested
   * read group sets are aligned.
   *
   * If a target positional range is specified, search returns all reads whose
   * alignment to the reference genome overlap the range. A query which
   * specifies only read group set IDs yields all reads in those read group
   * sets, including unmapped reads.
   *
   * All reads returned (including reads on subsequent pages) are ordered by
   * genomic coordinate (by reference sequence, then position). Reads with
   * equivalent genomic coordinates are returned in an unspecified order. This
   * order is consistent, such that two queries for the same content (regardless
   * of page size) yield reads in the same order across their respective streams
   * of paginated responses.
   *
   * Implements
   * [GlobalAllianceApi.searchReads](https://github.com/ga4gh/schemas/blob/v0.5.1/src/main/resources/avro/readmethods.avdl#L85).
   */
  searchReads(
    request: DeepPartial<SearchReadsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<SearchReadsResponse>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export type ServerStreamingMethodResult<Response> = { [Symbol.asyncIterator](): AsyncIterator<Response, void> };

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
