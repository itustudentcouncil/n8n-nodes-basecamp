// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/spanner/v1/spanner.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Duration } from "../../protobuf/duration.js";
import { Empty } from "../../protobuf/empty.js";
import { Struct } from "../../protobuf/struct.js";
import { Timestamp } from "../../protobuf/timestamp.js";
import { Status } from "../../rpc/status.js";
import { CommitResponse } from "./commit_response.js";
import { KeySet } from "./keys.js";
import { Mutation } from "./mutation.js";
import { PartialResultSet, ResultSet } from "./result_set.js";
import { Transaction, TransactionOptions, TransactionSelector } from "./transaction.js";
import { Type } from "./type.js";

export const protobufPackage = "google.spanner.v1";

/** The request for [CreateSession][google.spanner.v1.Spanner.CreateSession]. */
export interface CreateSessionRequest {
  /** Required. The database in which the new session is created. */
  database: string;
  /** Required. The session to create. */
  session: Session | undefined;
}

/**
 * The request for
 * [BatchCreateSessions][google.spanner.v1.Spanner.BatchCreateSessions].
 */
export interface BatchCreateSessionsRequest {
  /** Required. The database in which the new sessions are created. */
  database: string;
  /** Parameters to be applied to each created session. */
  sessionTemplate:
    | Session
    | undefined;
  /**
   * Required. The number of sessions to be created in this batch call.
   * The API may return fewer than the requested number of sessions. If a
   * specific number of sessions are desired, the client can make additional
   * calls to BatchCreateSessions (adjusting
   * [session_count][google.spanner.v1.BatchCreateSessionsRequest.session_count]
   * as necessary).
   */
  sessionCount: number;
}

/**
 * The response for
 * [BatchCreateSessions][google.spanner.v1.Spanner.BatchCreateSessions].
 */
export interface BatchCreateSessionsResponse {
  /** The freshly created sessions. */
  session: Session[];
}

/** A session in the Cloud Spanner API. */
export interface Session {
  /** Output only. The name of the session. This is always system-assigned. */
  name: string;
  /**
   * The labels for the session.
   *
   *  * Label keys must be between 1 and 63 characters long and must conform to
   *    the following regular expression: `[a-z]([-a-z0-9]*[a-z0-9])?`.
   *  * Label values must be between 0 and 63 characters long and must conform
   *    to the regular expression `([a-z]([-a-z0-9]*[a-z0-9])?)?`.
   *  * No more than 64 labels can be associated with a given session.
   *
   * See https://goo.gl/xmQnxf for more information on and examples of labels.
   */
  labels: { [key: string]: string };
  /** Output only. The timestamp when the session is created. */
  createTime:
    | Date
    | undefined;
  /**
   * Output only. The approximate timestamp when the session is last used. It is
   * typically earlier than the actual last use time.
   */
  approximateLastUseTime:
    | Date
    | undefined;
  /** The database role which created this session. */
  creatorRole: string;
  /**
   * Optional. If true, specifies a multiplexed session. A multiplexed session
   * may be used for multiple, concurrent read-only operations but can not be
   * used for read-write transactions, partitioned reads, or partitioned
   * queries. Multiplexed sessions can be created via
   * [CreateSession][google.spanner.v1.Spanner.CreateSession] but not via
   * [BatchCreateSessions][google.spanner.v1.Spanner.BatchCreateSessions].
   * Multiplexed sessions may not be deleted nor listed.
   */
  multiplexed: boolean;
}

export interface Session_LabelsEntry {
  key: string;
  value: string;
}

/** The request for [GetSession][google.spanner.v1.Spanner.GetSession]. */
export interface GetSessionRequest {
  /** Required. The name of the session to retrieve. */
  name: string;
}

/** The request for [ListSessions][google.spanner.v1.Spanner.ListSessions]. */
export interface ListSessionsRequest {
  /** Required. The database in which to list sessions. */
  database: string;
  /**
   * Number of sessions to be returned in the response. If 0 or less, defaults
   * to the server's maximum allowed page size.
   */
  pageSize: number;
  /**
   * If non-empty, `page_token` should contain a
   * [next_page_token][google.spanner.v1.ListSessionsResponse.next_page_token]
   * from a previous
   * [ListSessionsResponse][google.spanner.v1.ListSessionsResponse].
   */
  pageToken: string;
  /**
   * An expression for filtering the results of the request. Filter rules are
   * case insensitive. The fields eligible for filtering are:
   *
   *   * `labels.key` where key is the name of a label
   *
   * Some examples of using filters are:
   *
   *   * `labels.env:*` --> The session has the label "env".
   *   * `labels.env:dev` --> The session has the label "env" and the value of
   *                        the label contains the string "dev".
   */
  filter: string;
}

/** The response for [ListSessions][google.spanner.v1.Spanner.ListSessions]. */
export interface ListSessionsResponse {
  /** The list of requested sessions. */
  sessions: Session[];
  /**
   * `next_page_token` can be sent in a subsequent
   * [ListSessions][google.spanner.v1.Spanner.ListSessions] call to fetch more
   * of the matching sessions.
   */
  nextPageToken: string;
}

/** The request for [DeleteSession][google.spanner.v1.Spanner.DeleteSession]. */
export interface DeleteSessionRequest {
  /** Required. The name of the session to delete. */
  name: string;
}

/** Common request options for various APIs. */
export interface RequestOptions {
  /** Priority for the request. */
  priority: RequestOptions_Priority;
  /**
   * A per-request tag which can be applied to queries or reads, used for
   * statistics collection.
   * Both request_tag and transaction_tag can be specified for a read or query
   * that belongs to a transaction.
   * This field is ignored for requests where it's not applicable (e.g.
   * CommitRequest).
   * Legal characters for `request_tag` values are all printable characters
   * (ASCII 32 - 126) and the length of a request_tag is limited to 50
   * characters. Values that exceed this limit are truncated.
   * Any leading underscore (_) characters will be removed from the string.
   */
  requestTag: string;
  /**
   * A tag used for statistics collection about this transaction.
   * Both request_tag and transaction_tag can be specified for a read or query
   * that belongs to a transaction.
   * The value of transaction_tag should be the same for all requests belonging
   * to the same transaction.
   * If this request doesn't belong to any transaction, transaction_tag will be
   * ignored.
   * Legal characters for `transaction_tag` values are all printable characters
   * (ASCII 32 - 126) and the length of a transaction_tag is limited to 50
   * characters. Values that exceed this limit are truncated.
   * Any leading underscore (_) characters will be removed from the string.
   */
  transactionTag: string;
}

/**
 * The relative priority for requests. Note that priority is not applicable
 * for [BeginTransaction][google.spanner.v1.Spanner.BeginTransaction].
 *
 * The priority acts as a hint to the Cloud Spanner scheduler and does not
 * guarantee priority or order of execution. For example:
 *
 * * Some parts of a write operation always execute at `PRIORITY_HIGH`,
 *   regardless of the specified priority. This may cause you to see an
 *   increase in high priority workload even when executing a low priority
 *   request. This can also potentially cause a priority inversion where a
 *   lower priority request will be fulfilled ahead of a higher priority
 *   request.
 * * If a transaction contains multiple operations with different priorities,
 *   Cloud Spanner does not guarantee to process the higher priority
 *   operations first. There may be other constraints to satisfy, such as
 *   order of operations.
 */
export enum RequestOptions_Priority {
  /** PRIORITY_UNSPECIFIED - `PRIORITY_UNSPECIFIED` is equivalent to `PRIORITY_HIGH`. */
  PRIORITY_UNSPECIFIED = 0,
  /** PRIORITY_LOW - This specifies that the request is low priority. */
  PRIORITY_LOW = 1,
  /** PRIORITY_MEDIUM - This specifies that the request is medium priority. */
  PRIORITY_MEDIUM = 2,
  /** PRIORITY_HIGH - This specifies that the request is high priority. */
  PRIORITY_HIGH = 3,
  UNRECOGNIZED = -1,
}

export function requestOptions_PriorityFromJSON(object: any): RequestOptions_Priority {
  switch (object) {
    case 0:
    case "PRIORITY_UNSPECIFIED":
      return RequestOptions_Priority.PRIORITY_UNSPECIFIED;
    case 1:
    case "PRIORITY_LOW":
      return RequestOptions_Priority.PRIORITY_LOW;
    case 2:
    case "PRIORITY_MEDIUM":
      return RequestOptions_Priority.PRIORITY_MEDIUM;
    case 3:
    case "PRIORITY_HIGH":
      return RequestOptions_Priority.PRIORITY_HIGH;
    case -1:
    case "UNRECOGNIZED":
    default:
      return RequestOptions_Priority.UNRECOGNIZED;
  }
}

export function requestOptions_PriorityToJSON(object: RequestOptions_Priority): string {
  switch (object) {
    case RequestOptions_Priority.PRIORITY_UNSPECIFIED:
      return "PRIORITY_UNSPECIFIED";
    case RequestOptions_Priority.PRIORITY_LOW:
      return "PRIORITY_LOW";
    case RequestOptions_Priority.PRIORITY_MEDIUM:
      return "PRIORITY_MEDIUM";
    case RequestOptions_Priority.PRIORITY_HIGH:
      return "PRIORITY_HIGH";
    case RequestOptions_Priority.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * The DirectedReadOptions can be used to indicate which replicas or regions
 * should be used for non-transactional reads or queries.
 *
 * DirectedReadOptions may only be specified for a read-only transaction,
 * otherwise the API will return an `INVALID_ARGUMENT` error.
 */
export interface DirectedReadOptions {
  /**
   * Include_replicas indicates the order of replicas (as they appear in
   * this list) to process the request. If auto_failover_disabled is set to
   * true and all replicas are exhausted without finding a healthy replica,
   * Spanner will wait for a replica in the list to become available, requests
   * may fail due to `DEADLINE_EXCEEDED` errors.
   */
  includeReplicas?:
    | DirectedReadOptions_IncludeReplicas
    | undefined;
  /**
   * Exclude_replicas indicates that specified replicas should be excluded
   * from serving requests. Spanner will not route requests to the replicas
   * in this list.
   */
  excludeReplicas?: DirectedReadOptions_ExcludeReplicas | undefined;
}

/**
 * The directed read replica selector.
 * Callers must provide one or more of the following fields for replica
 * selection:
 *
 *   * `location` - The location must be one of the regions within the
 *      multi-region configuration of your database.
 *   * `type` - The type of the replica.
 *
 * Some examples of using replica_selectors are:
 *
 *   * `location:us-east1` --> The "us-east1" replica(s) of any available type
 *                             will be used to process the request.
 *   * `type:READ_ONLY`    --> The "READ_ONLY" type replica(s) in nearest
 *                             available location will be used to process the
 *                             request.
 *   * `location:us-east1 type:READ_ONLY` --> The "READ_ONLY" type replica(s)
 *                          in location "us-east1" will be used to process
 *                          the request.
 */
export interface DirectedReadOptions_ReplicaSelection {
  /** The location or region of the serving requests, e.g. "us-east1". */
  location: string;
  /** The type of replica. */
  type: DirectedReadOptions_ReplicaSelection_Type;
}

/** Indicates the type of replica. */
export enum DirectedReadOptions_ReplicaSelection_Type {
  /** TYPE_UNSPECIFIED - Not specified. */
  TYPE_UNSPECIFIED = 0,
  /** READ_WRITE - Read-write replicas support both reads and writes. */
  READ_WRITE = 1,
  /** READ_ONLY - Read-only replicas only support reads (not writes). */
  READ_ONLY = 2,
  UNRECOGNIZED = -1,
}

export function directedReadOptions_ReplicaSelection_TypeFromJSON(
  object: any,
): DirectedReadOptions_ReplicaSelection_Type {
  switch (object) {
    case 0:
    case "TYPE_UNSPECIFIED":
      return DirectedReadOptions_ReplicaSelection_Type.TYPE_UNSPECIFIED;
    case 1:
    case "READ_WRITE":
      return DirectedReadOptions_ReplicaSelection_Type.READ_WRITE;
    case 2:
    case "READ_ONLY":
      return DirectedReadOptions_ReplicaSelection_Type.READ_ONLY;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DirectedReadOptions_ReplicaSelection_Type.UNRECOGNIZED;
  }
}

export function directedReadOptions_ReplicaSelection_TypeToJSON(
  object: DirectedReadOptions_ReplicaSelection_Type,
): string {
  switch (object) {
    case DirectedReadOptions_ReplicaSelection_Type.TYPE_UNSPECIFIED:
      return "TYPE_UNSPECIFIED";
    case DirectedReadOptions_ReplicaSelection_Type.READ_WRITE:
      return "READ_WRITE";
    case DirectedReadOptions_ReplicaSelection_Type.READ_ONLY:
      return "READ_ONLY";
    case DirectedReadOptions_ReplicaSelection_Type.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * An IncludeReplicas contains a repeated set of ReplicaSelection which
 * indicates the order in which replicas should be considered.
 */
export interface DirectedReadOptions_IncludeReplicas {
  /** The directed read replica selector. */
  replicaSelections: DirectedReadOptions_ReplicaSelection[];
  /**
   * If true, Spanner will not route requests to a replica outside the
   * include_replicas list when all of the specified replicas are unavailable
   * or unhealthy. Default value is `false`.
   */
  autoFailoverDisabled: boolean;
}

/**
 * An ExcludeReplicas contains a repeated set of ReplicaSelection that should
 * be excluded from serving requests.
 */
export interface DirectedReadOptions_ExcludeReplicas {
  /** The directed read replica selector. */
  replicaSelections: DirectedReadOptions_ReplicaSelection[];
}

/**
 * The request for [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql] and
 * [ExecuteStreamingSql][google.spanner.v1.Spanner.ExecuteStreamingSql].
 */
export interface ExecuteSqlRequest {
  /** Required. The session in which the SQL query should be performed. */
  session: string;
  /**
   * The transaction to use.
   *
   * For queries, if none is provided, the default is a temporary read-only
   * transaction with strong concurrency.
   *
   * Standard DML statements require a read-write transaction. To protect
   * against replays, single-use transactions are not supported.  The caller
   * must either supply an existing transaction ID or begin a new transaction.
   *
   * Partitioned DML requires an existing Partitioned DML transaction ID.
   */
  transaction:
    | TransactionSelector
    | undefined;
  /** Required. The SQL string. */
  sql: string;
  /**
   * Parameter names and values that bind to placeholders in the SQL string.
   *
   * A parameter placeholder consists of the `@` character followed by the
   * parameter name (for example, `@firstName`). Parameter names must conform
   * to the naming requirements of identifiers as specified at
   * https://cloud.google.com/spanner/docs/lexical#identifiers.
   *
   * Parameters can appear anywhere that a literal value is expected.  The same
   * parameter name can be used more than once, for example:
   *
   * `"WHERE id > @msg_id AND id < @msg_id + 100"`
   *
   * It is an error to execute a SQL statement with unbound parameters.
   */
  params:
    | { [key: string]: any }
    | undefined;
  /**
   * It is not always possible for Cloud Spanner to infer the right SQL type
   * from a JSON value.  For example, values of type `BYTES` and values
   * of type `STRING` both appear in
   * [params][google.spanner.v1.ExecuteSqlRequest.params] as JSON strings.
   *
   * In these cases, `param_types` can be used to specify the exact
   * SQL type for some or all of the SQL statement parameters. See the
   * definition of [Type][google.spanner.v1.Type] for more information
   * about SQL types.
   */
  paramTypes: { [key: string]: Type };
  /**
   * If this request is resuming a previously interrupted SQL statement
   * execution, `resume_token` should be copied from the last
   * [PartialResultSet][google.spanner.v1.PartialResultSet] yielded before the
   * interruption. Doing this enables the new SQL statement execution to resume
   * where the last one left off. The rest of the request parameters must
   * exactly match the request that yielded this token.
   */
  resumeToken: Buffer;
  /**
   * Used to control the amount of debugging information returned in
   * [ResultSetStats][google.spanner.v1.ResultSetStats]. If
   * [partition_token][google.spanner.v1.ExecuteSqlRequest.partition_token] is
   * set, [query_mode][google.spanner.v1.ExecuteSqlRequest.query_mode] can only
   * be set to
   * [QueryMode.NORMAL][google.spanner.v1.ExecuteSqlRequest.QueryMode.NORMAL].
   */
  queryMode: ExecuteSqlRequest_QueryMode;
  /**
   * If present, results will be restricted to the specified partition
   * previously created using PartitionQuery().  There must be an exact
   * match for the values of fields common to this message and the
   * PartitionQueryRequest message used to create this partition_token.
   */
  partitionToken: Buffer;
  /**
   * A per-transaction sequence number used to identify this request. This field
   * makes each request idempotent such that if the request is received multiple
   * times, at most one will succeed.
   *
   * The sequence number must be monotonically increasing within the
   * transaction. If a request arrives for the first time with an out-of-order
   * sequence number, the transaction may be aborted. Replays of previously
   * handled requests will yield the same response as the first execution.
   *
   * Required for DML statements. Ignored for queries.
   */
  seqno: Long;
  /** Query optimizer configuration to use for the given query. */
  queryOptions:
    | ExecuteSqlRequest_QueryOptions
    | undefined;
  /** Common options for this request. */
  requestOptions:
    | RequestOptions
    | undefined;
  /** Directed read options for this request. */
  directedReadOptions:
    | DirectedReadOptions
    | undefined;
  /**
   * If this is for a partitioned query and this field is set to `true`, the
   * request is executed with Spanner Data Boost independent compute resources.
   *
   * If the field is set to `true` but the request does not set
   * `partition_token`, the API returns an `INVALID_ARGUMENT` error.
   */
  dataBoostEnabled: boolean;
}

/** Mode in which the statement must be processed. */
export enum ExecuteSqlRequest_QueryMode {
  /** NORMAL - The default mode. Only the statement results are returned. */
  NORMAL = 0,
  /**
   * PLAN - This mode returns only the query plan, without any results or
   * execution statistics information.
   */
  PLAN = 1,
  /**
   * PROFILE - This mode returns both the query plan and the execution statistics along
   * with the results.
   */
  PROFILE = 2,
  UNRECOGNIZED = -1,
}

export function executeSqlRequest_QueryModeFromJSON(object: any): ExecuteSqlRequest_QueryMode {
  switch (object) {
    case 0:
    case "NORMAL":
      return ExecuteSqlRequest_QueryMode.NORMAL;
    case 1:
    case "PLAN":
      return ExecuteSqlRequest_QueryMode.PLAN;
    case 2:
    case "PROFILE":
      return ExecuteSqlRequest_QueryMode.PROFILE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ExecuteSqlRequest_QueryMode.UNRECOGNIZED;
  }
}

export function executeSqlRequest_QueryModeToJSON(object: ExecuteSqlRequest_QueryMode): string {
  switch (object) {
    case ExecuteSqlRequest_QueryMode.NORMAL:
      return "NORMAL";
    case ExecuteSqlRequest_QueryMode.PLAN:
      return "PLAN";
    case ExecuteSqlRequest_QueryMode.PROFILE:
      return "PROFILE";
    case ExecuteSqlRequest_QueryMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Query optimizer configuration. */
export interface ExecuteSqlRequest_QueryOptions {
  /**
   * An option to control the selection of optimizer version.
   *
   * This parameter allows individual queries to pick different query
   * optimizer versions.
   *
   * Specifying `latest` as a value instructs Cloud Spanner to use the
   * latest supported query optimizer version. If not specified, Cloud Spanner
   * uses the optimizer version set at the database level options. Any other
   * positive integer (from the list of supported optimizer versions)
   * overrides the default optimizer version for query execution.
   *
   * The list of supported optimizer versions can be queried from
   * SPANNER_SYS.SUPPORTED_OPTIMIZER_VERSIONS.
   *
   * Executing a SQL statement with an invalid optimizer version fails with
   * an `INVALID_ARGUMENT` error.
   *
   * See
   * https://cloud.google.com/spanner/docs/query-optimizer/manage-query-optimizer
   * for more information on managing the query optimizer.
   *
   * The `optimizer_version` statement hint has precedence over this setting.
   */
  optimizerVersion: string;
  /**
   * An option to control the selection of optimizer statistics package.
   *
   * This parameter allows individual queries to use a different query
   * optimizer statistics package.
   *
   * Specifying `latest` as a value instructs Cloud Spanner to use the latest
   * generated statistics package. If not specified, Cloud Spanner uses
   * the statistics package set at the database level options, or the latest
   * package if the database option is not set.
   *
   * The statistics package requested by the query has to be exempt from
   * garbage collection. This can be achieved with the following DDL
   * statement:
   *
   * ```
   * ALTER STATISTICS <package_name> SET OPTIONS (allow_gc=false)
   * ```
   *
   * The list of available statistics packages can be queried from
   * `INFORMATION_SCHEMA.SPANNER_STATISTICS`.
   *
   * Executing a SQL statement with an invalid optimizer statistics package
   * or with a statistics package that allows garbage collection fails with
   * an `INVALID_ARGUMENT` error.
   */
  optimizerStatisticsPackage: string;
}

export interface ExecuteSqlRequest_ParamTypesEntry {
  key: string;
  value: Type | undefined;
}

/** The request for [ExecuteBatchDml][google.spanner.v1.Spanner.ExecuteBatchDml]. */
export interface ExecuteBatchDmlRequest {
  /** Required. The session in which the DML statements should be performed. */
  session: string;
  /**
   * Required. The transaction to use. Must be a read-write transaction.
   *
   * To protect against replays, single-use transactions are not supported. The
   * caller must either supply an existing transaction ID or begin a new
   * transaction.
   */
  transaction:
    | TransactionSelector
    | undefined;
  /**
   * Required. The list of statements to execute in this batch. Statements are
   * executed serially, such that the effects of statement `i` are visible to
   * statement `i+1`. Each statement must be a DML statement. Execution stops at
   * the first failed statement; the remaining statements are not executed.
   *
   * Callers must provide at least one statement.
   */
  statements: ExecuteBatchDmlRequest_Statement[];
  /**
   * Required. A per-transaction sequence number used to identify this request.
   * This field makes each request idempotent such that if the request is
   * received multiple times, at most one will succeed.
   *
   * The sequence number must be monotonically increasing within the
   * transaction. If a request arrives for the first time with an out-of-order
   * sequence number, the transaction may be aborted. Replays of previously
   * handled requests will yield the same response as the first execution.
   */
  seqno: Long;
  /** Common options for this request. */
  requestOptions: RequestOptions | undefined;
}

/** A single DML statement. */
export interface ExecuteBatchDmlRequest_Statement {
  /** Required. The DML string. */
  sql: string;
  /**
   * Parameter names and values that bind to placeholders in the DML string.
   *
   * A parameter placeholder consists of the `@` character followed by the
   * parameter name (for example, `@firstName`). Parameter names can contain
   * letters, numbers, and underscores.
   *
   * Parameters can appear anywhere that a literal value is expected.  The
   * same parameter name can be used more than once, for example:
   *
   * `"WHERE id > @msg_id AND id < @msg_id + 100"`
   *
   * It is an error to execute a SQL statement with unbound parameters.
   */
  params:
    | { [key: string]: any }
    | undefined;
  /**
   * It is not always possible for Cloud Spanner to infer the right SQL type
   * from a JSON value.  For example, values of type `BYTES` and values
   * of type `STRING` both appear in
   * [params][google.spanner.v1.ExecuteBatchDmlRequest.Statement.params] as
   * JSON strings.
   *
   * In these cases, `param_types` can be used to specify the exact
   * SQL type for some or all of the SQL statement parameters. See the
   * definition of [Type][google.spanner.v1.Type] for more information
   * about SQL types.
   */
  paramTypes: { [key: string]: Type };
}

export interface ExecuteBatchDmlRequest_Statement_ParamTypesEntry {
  key: string;
  value: Type | undefined;
}

/**
 * The response for
 * [ExecuteBatchDml][google.spanner.v1.Spanner.ExecuteBatchDml]. Contains a list
 * of [ResultSet][google.spanner.v1.ResultSet] messages, one for each DML
 * statement that has successfully executed, in the same order as the statements
 * in the request. If a statement fails, the status in the response body
 * identifies the cause of the failure.
 *
 * To check for DML statements that failed, use the following approach:
 *
 * 1. Check the status in the response message. The
 * [google.rpc.Code][google.rpc.Code] enum
 *    value `OK` indicates that all statements were executed successfully.
 * 2. If the status was not `OK`, check the number of result sets in the
 *    response. If the response contains `N`
 *    [ResultSet][google.spanner.v1.ResultSet] messages, then statement `N+1` in
 *    the request failed.
 *
 * Example 1:
 *
 * * Request: 5 DML statements, all executed successfully.
 * * Response: 5 [ResultSet][google.spanner.v1.ResultSet] messages, with the
 * status `OK`.
 *
 * Example 2:
 *
 * * Request: 5 DML statements. The third statement has a syntax error.
 * * Response: 2 [ResultSet][google.spanner.v1.ResultSet] messages, and a syntax
 * error (`INVALID_ARGUMENT`)
 *   status. The number of [ResultSet][google.spanner.v1.ResultSet] messages
 *   indicates that the third statement failed, and the fourth and fifth
 *   statements were not executed.
 */
export interface ExecuteBatchDmlResponse {
  /**
   * One [ResultSet][google.spanner.v1.ResultSet] for each statement in the
   * request that ran successfully, in the same order as the statements in the
   * request. Each [ResultSet][google.spanner.v1.ResultSet] does not contain any
   * rows. The [ResultSetStats][google.spanner.v1.ResultSetStats] in each
   * [ResultSet][google.spanner.v1.ResultSet] contain the number of rows
   * modified by the statement.
   *
   * Only the first [ResultSet][google.spanner.v1.ResultSet] in the response
   * contains valid [ResultSetMetadata][google.spanner.v1.ResultSetMetadata].
   */
  resultSets: ResultSet[];
  /**
   * If all DML statements are executed successfully, the status is `OK`.
   * Otherwise, the error status of the first failed statement.
   */
  status: Status | undefined;
}

/**
 * Options for a PartitionQueryRequest and
 * PartitionReadRequest.
 */
export interface PartitionOptions {
  /**
   * *Note:** This hint is currently ignored by PartitionQuery and
   * PartitionRead requests.
   *
   * The desired data size for each partition generated.  The default for this
   * option is currently 1 GiB.  This is only a hint. The actual size of each
   * partition may be smaller or larger than this size request.
   */
  partitionSizeBytes: Long;
  /**
   * *Note:** This hint is currently ignored by PartitionQuery and
   * PartitionRead requests.
   *
   * The desired maximum number of partitions to return.  For example, this may
   * be set to the number of workers available.  The default for this option
   * is currently 10,000. The maximum value is currently 200,000.  This is only
   * a hint.  The actual number of partitions returned may be smaller or larger
   * than this maximum count request.
   */
  maxPartitions: Long;
}

/** The request for [PartitionQuery][google.spanner.v1.Spanner.PartitionQuery] */
export interface PartitionQueryRequest {
  /** Required. The session used to create the partitions. */
  session: string;
  /**
   * Read only snapshot transactions are supported, read/write and single use
   * transactions are not.
   */
  transaction:
    | TransactionSelector
    | undefined;
  /**
   * Required. The query request to generate partitions for. The request will
   * fail if the query is not root partitionable. For a query to be root
   * partitionable, it needs to satisfy a few conditions. For example, if the
   * query execution plan contains a distributed union operator, then it must be
   * the first operator in the plan. For more information about other
   * conditions, see [Read data in
   * parallel](https://cloud.google.com/spanner/docs/reads#read_data_in_parallel).
   *
   * The query request must not contain DML commands, such as INSERT, UPDATE, or
   * DELETE. Use
   * [ExecuteStreamingSql][google.spanner.v1.Spanner.ExecuteStreamingSql] with a
   * PartitionedDml transaction for large, partition-friendly DML operations.
   */
  sql: string;
  /**
   * Parameter names and values that bind to placeholders in the SQL string.
   *
   * A parameter placeholder consists of the `@` character followed by the
   * parameter name (for example, `@firstName`). Parameter names can contain
   * letters, numbers, and underscores.
   *
   * Parameters can appear anywhere that a literal value is expected.  The same
   * parameter name can be used more than once, for example:
   *
   * `"WHERE id > @msg_id AND id < @msg_id + 100"`
   *
   * It is an error to execute a SQL statement with unbound parameters.
   */
  params:
    | { [key: string]: any }
    | undefined;
  /**
   * It is not always possible for Cloud Spanner to infer the right SQL type
   * from a JSON value.  For example, values of type `BYTES` and values
   * of type `STRING` both appear in
   * [params][google.spanner.v1.PartitionQueryRequest.params] as JSON strings.
   *
   * In these cases, `param_types` can be used to specify the exact
   * SQL type for some or all of the SQL query parameters. See the
   * definition of [Type][google.spanner.v1.Type] for more information
   * about SQL types.
   */
  paramTypes: { [key: string]: Type };
  /** Additional options that affect how many partitions are created. */
  partitionOptions: PartitionOptions | undefined;
}

export interface PartitionQueryRequest_ParamTypesEntry {
  key: string;
  value: Type | undefined;
}

/** The request for [PartitionRead][google.spanner.v1.Spanner.PartitionRead] */
export interface PartitionReadRequest {
  /** Required. The session used to create the partitions. */
  session: string;
  /**
   * Read only snapshot transactions are supported, read/write and single use
   * transactions are not.
   */
  transaction:
    | TransactionSelector
    | undefined;
  /** Required. The name of the table in the database to be read. */
  table: string;
  /**
   * If non-empty, the name of an index on
   * [table][google.spanner.v1.PartitionReadRequest.table]. This index is used
   * instead of the table primary key when interpreting
   * [key_set][google.spanner.v1.PartitionReadRequest.key_set] and sorting
   * result rows. See [key_set][google.spanner.v1.PartitionReadRequest.key_set]
   * for further information.
   */
  index: string;
  /**
   * The columns of [table][google.spanner.v1.PartitionReadRequest.table] to be
   * returned for each row matching this request.
   */
  columns: string[];
  /**
   * Required. `key_set` identifies the rows to be yielded. `key_set` names the
   * primary keys of the rows in
   * [table][google.spanner.v1.PartitionReadRequest.table] to be yielded, unless
   * [index][google.spanner.v1.PartitionReadRequest.index] is present. If
   * [index][google.spanner.v1.PartitionReadRequest.index] is present, then
   * [key_set][google.spanner.v1.PartitionReadRequest.key_set] instead names
   * index keys in [index][google.spanner.v1.PartitionReadRequest.index].
   *
   * It is not an error for the `key_set` to name rows that do not
   * exist in the database. Read yields nothing for nonexistent rows.
   */
  keySet:
    | KeySet
    | undefined;
  /** Additional options that affect how many partitions are created. */
  partitionOptions: PartitionOptions | undefined;
}

/**
 * Information returned for each partition returned in a
 * PartitionResponse.
 */
export interface Partition {
  /**
   * This token can be passed to Read, StreamingRead, ExecuteSql, or
   * ExecuteStreamingSql requests to restrict the results to those identified by
   * this partition token.
   */
  partitionToken: Buffer;
}

/**
 * The response for [PartitionQuery][google.spanner.v1.Spanner.PartitionQuery]
 * or [PartitionRead][google.spanner.v1.Spanner.PartitionRead]
 */
export interface PartitionResponse {
  /** Partitions created by this request. */
  partitions: Partition[];
  /** Transaction created by this request. */
  transaction: Transaction | undefined;
}

/**
 * The request for [Read][google.spanner.v1.Spanner.Read] and
 * [StreamingRead][google.spanner.v1.Spanner.StreamingRead].
 */
export interface ReadRequest {
  /** Required. The session in which the read should be performed. */
  session: string;
  /**
   * The transaction to use. If none is provided, the default is a
   * temporary read-only transaction with strong concurrency.
   */
  transaction:
    | TransactionSelector
    | undefined;
  /** Required. The name of the table in the database to be read. */
  table: string;
  /**
   * If non-empty, the name of an index on
   * [table][google.spanner.v1.ReadRequest.table]. This index is used instead of
   * the table primary key when interpreting
   * [key_set][google.spanner.v1.ReadRequest.key_set] and sorting result rows.
   * See [key_set][google.spanner.v1.ReadRequest.key_set] for further
   * information.
   */
  index: string;
  /**
   * Required. The columns of [table][google.spanner.v1.ReadRequest.table] to be
   * returned for each row matching this request.
   */
  columns: string[];
  /**
   * Required. `key_set` identifies the rows to be yielded. `key_set` names the
   * primary keys of the rows in [table][google.spanner.v1.ReadRequest.table] to
   * be yielded, unless [index][google.spanner.v1.ReadRequest.index] is present.
   * If [index][google.spanner.v1.ReadRequest.index] is present, then
   * [key_set][google.spanner.v1.ReadRequest.key_set] instead names index keys
   * in [index][google.spanner.v1.ReadRequest.index].
   *
   * If the [partition_token][google.spanner.v1.ReadRequest.partition_token]
   * field is empty, rows are yielded in table primary key order (if
   * [index][google.spanner.v1.ReadRequest.index] is empty) or index key order
   * (if [index][google.spanner.v1.ReadRequest.index] is non-empty).  If the
   * [partition_token][google.spanner.v1.ReadRequest.partition_token] field is
   * not empty, rows will be yielded in an unspecified order.
   *
   * It is not an error for the `key_set` to name rows that do not
   * exist in the database. Read yields nothing for nonexistent rows.
   */
  keySet:
    | KeySet
    | undefined;
  /**
   * If greater than zero, only the first `limit` rows are yielded. If `limit`
   * is zero, the default is no limit. A limit cannot be specified if
   * `partition_token` is set.
   */
  limit: Long;
  /**
   * If this request is resuming a previously interrupted read,
   * `resume_token` should be copied from the last
   * [PartialResultSet][google.spanner.v1.PartialResultSet] yielded before the
   * interruption. Doing this enables the new read to resume where the last read
   * left off. The rest of the request parameters must exactly match the request
   * that yielded this token.
   */
  resumeToken: Buffer;
  /**
   * If present, results will be restricted to the specified partition
   * previously created using PartitionRead().    There must be an exact
   * match for the values of fields common to this message and the
   * PartitionReadRequest message used to create this partition_token.
   */
  partitionToken: Buffer;
  /** Common options for this request. */
  requestOptions:
    | RequestOptions
    | undefined;
  /** Directed read options for this request. */
  directedReadOptions:
    | DirectedReadOptions
    | undefined;
  /**
   * If this is for a partitioned read and this field is set to `true`, the
   * request is executed with Spanner Data Boost independent compute resources.
   *
   * If the field is set to `true` but the request does not set
   * `partition_token`, the API returns an `INVALID_ARGUMENT` error.
   */
  dataBoostEnabled: boolean;
  /**
   * Optional. Order for the returned rows.
   *
   * By default, Spanner will return result rows in primary key order except for
   * PartitionRead requests. For applications that do not require rows to be
   * returned in primary key (`ORDER_BY_PRIMARY_KEY`) order, setting
   * `ORDER_BY_NO_ORDER` option allows Spanner to optimize row retrieval,
   * resulting in lower latencies in certain cases (e.g. bulk point lookups).
   */
  orderBy: ReadRequest_OrderBy;
  /**
   * Optional. Lock Hint for the request, it can only be used with read-write
   * transactions.
   */
  lockHint: ReadRequest_LockHint;
}

/** An option to control the order in which rows are returned from a read. */
export enum ReadRequest_OrderBy {
  /**
   * ORDER_BY_UNSPECIFIED - Default value.
   *
   * ORDER_BY_UNSPECIFIED is equivalent to ORDER_BY_PRIMARY_KEY.
   */
  ORDER_BY_UNSPECIFIED = 0,
  /**
   * ORDER_BY_PRIMARY_KEY - Read rows are returned in primary key order.
   *
   * In the event that this option is used in conjunction with the
   * `partition_token` field, the API will return an `INVALID_ARGUMENT` error.
   */
  ORDER_BY_PRIMARY_KEY = 1,
  /** ORDER_BY_NO_ORDER - Read rows are returned in any order. */
  ORDER_BY_NO_ORDER = 2,
  UNRECOGNIZED = -1,
}

export function readRequest_OrderByFromJSON(object: any): ReadRequest_OrderBy {
  switch (object) {
    case 0:
    case "ORDER_BY_UNSPECIFIED":
      return ReadRequest_OrderBy.ORDER_BY_UNSPECIFIED;
    case 1:
    case "ORDER_BY_PRIMARY_KEY":
      return ReadRequest_OrderBy.ORDER_BY_PRIMARY_KEY;
    case 2:
    case "ORDER_BY_NO_ORDER":
      return ReadRequest_OrderBy.ORDER_BY_NO_ORDER;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ReadRequest_OrderBy.UNRECOGNIZED;
  }
}

export function readRequest_OrderByToJSON(object: ReadRequest_OrderBy): string {
  switch (object) {
    case ReadRequest_OrderBy.ORDER_BY_UNSPECIFIED:
      return "ORDER_BY_UNSPECIFIED";
    case ReadRequest_OrderBy.ORDER_BY_PRIMARY_KEY:
      return "ORDER_BY_PRIMARY_KEY";
    case ReadRequest_OrderBy.ORDER_BY_NO_ORDER:
      return "ORDER_BY_NO_ORDER";
    case ReadRequest_OrderBy.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** A lock hint mechanism for reads done within a transaction. */
export enum ReadRequest_LockHint {
  /**
   * LOCK_HINT_UNSPECIFIED - Default value.
   *
   * LOCK_HINT_UNSPECIFIED is equivalent to LOCK_HINT_SHARED.
   */
  LOCK_HINT_UNSPECIFIED = 0,
  /**
   * LOCK_HINT_SHARED - Acquire shared locks.
   *
   * By default when you perform a read as part of a read-write transaction,
   * Spanner acquires shared read locks, which allows other reads to still
   * access the data until your transaction is ready to commit. When your
   * transaction is committing and writes are being applied, the transaction
   * attempts to upgrade to an exclusive lock for any data you are writing.
   * For more information about locks, see [Lock
   * modes](https://cloud.google.com/spanner/docs/introspection/lock-statistics#explain-lock-modes).
   */
  LOCK_HINT_SHARED = 1,
  /**
   * LOCK_HINT_EXCLUSIVE - Acquire exclusive locks.
   *
   * Requesting exclusive locks is beneficial if you observe high write
   * contention, which means you notice that multiple transactions are
   * concurrently trying to read and write to the same data, resulting in a
   * large number of aborts. This problem occurs when two transactions
   * initially acquire shared locks and then both try to upgrade to exclusive
   * locks at the same time. In this situation both transactions are waiting
   * for the other to give up their lock, resulting in a deadlocked situation.
   * Spanner is able to detect this occurring and force one of the
   * transactions to abort. However, this is a slow and expensive operation
   * and results in lower performance. In this case it makes sense to acquire
   * exclusive locks at the start of the transaction because then when
   * multiple transactions try to act on the same data, they automatically get
   * serialized. Each transaction waits its turn to acquire the lock and
   * avoids getting into deadlock situations.
   *
   * Because the exclusive lock hint is just a hint, it should not be
   * considered equivalent to a mutex. In other words, you should not use
   * Spanner exclusive locks as a mutual exclusion mechanism for the execution
   * of code outside of Spanner.
   *
   * **Note:** Request exclusive locks judiciously because they block others
   * from reading that data for the entire transaction, rather than just when
   * the writes are being performed. Unless you observe high write contention,
   * you should use the default of shared read locks so you don't prematurely
   * block other clients from reading the data that you're writing to.
   */
  LOCK_HINT_EXCLUSIVE = 2,
  UNRECOGNIZED = -1,
}

export function readRequest_LockHintFromJSON(object: any): ReadRequest_LockHint {
  switch (object) {
    case 0:
    case "LOCK_HINT_UNSPECIFIED":
      return ReadRequest_LockHint.LOCK_HINT_UNSPECIFIED;
    case 1:
    case "LOCK_HINT_SHARED":
      return ReadRequest_LockHint.LOCK_HINT_SHARED;
    case 2:
    case "LOCK_HINT_EXCLUSIVE":
      return ReadRequest_LockHint.LOCK_HINT_EXCLUSIVE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ReadRequest_LockHint.UNRECOGNIZED;
  }
}

export function readRequest_LockHintToJSON(object: ReadRequest_LockHint): string {
  switch (object) {
    case ReadRequest_LockHint.LOCK_HINT_UNSPECIFIED:
      return "LOCK_HINT_UNSPECIFIED";
    case ReadRequest_LockHint.LOCK_HINT_SHARED:
      return "LOCK_HINT_SHARED";
    case ReadRequest_LockHint.LOCK_HINT_EXCLUSIVE:
      return "LOCK_HINT_EXCLUSIVE";
    case ReadRequest_LockHint.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * The request for
 * [BeginTransaction][google.spanner.v1.Spanner.BeginTransaction].
 */
export interface BeginTransactionRequest {
  /** Required. The session in which the transaction runs. */
  session: string;
  /** Required. Options for the new transaction. */
  options:
    | TransactionOptions
    | undefined;
  /**
   * Common options for this request.
   * Priority is ignored for this request. Setting the priority in this
   * request_options struct will not do anything. To set the priority for a
   * transaction, set it on the reads and writes that are part of this
   * transaction instead.
   */
  requestOptions: RequestOptions | undefined;
}

/** The request for [Commit][google.spanner.v1.Spanner.Commit]. */
export interface CommitRequest {
  /** Required. The session in which the transaction to be committed is running. */
  session: string;
  /** Commit a previously-started transaction. */
  transactionId?:
    | Buffer
    | undefined;
  /**
   * Execute mutations in a temporary transaction. Note that unlike
   * commit of a previously-started transaction, commit with a
   * temporary transaction is non-idempotent. That is, if the
   * `CommitRequest` is sent to Cloud Spanner more than once (for
   * instance, due to retries in the application, or in the
   * transport library), it is possible that the mutations are
   * executed more than once. If this is undesirable, use
   * [BeginTransaction][google.spanner.v1.Spanner.BeginTransaction] and
   * [Commit][google.spanner.v1.Spanner.Commit] instead.
   */
  singleUseTransaction?:
    | TransactionOptions
    | undefined;
  /**
   * The mutations to be executed when this transaction commits. All
   * mutations are applied atomically, in the order they appear in
   * this list.
   */
  mutations: Mutation[];
  /**
   * If `true`, then statistics related to the transaction will be included in
   * the [CommitResponse][google.spanner.v1.CommitResponse.commit_stats].
   * Default value is `false`.
   */
  returnCommitStats: boolean;
  /**
   * Optional. The amount of latency this request is willing to incur in order
   * to improve throughput. If this field is not set, Spanner assumes requests
   * are relatively latency sensitive and automatically determines an
   * appropriate delay time. You can specify a batching delay value between 0
   * and 500 ms.
   */
  maxCommitDelay:
    | Duration
    | undefined;
  /** Common options for this request. */
  requestOptions: RequestOptions | undefined;
}

/** The request for [Rollback][google.spanner.v1.Spanner.Rollback]. */
export interface RollbackRequest {
  /** Required. The session in which the transaction to roll back is running. */
  session: string;
  /** Required. The transaction to roll back. */
  transactionId: Buffer;
}

/** The request for [BatchWrite][google.spanner.v1.Spanner.BatchWrite]. */
export interface BatchWriteRequest {
  /** Required. The session in which the batch request is to be run. */
  session: string;
  /** Common options for this request. */
  requestOptions:
    | RequestOptions
    | undefined;
  /** Required. The groups of mutations to be applied. */
  mutationGroups: BatchWriteRequest_MutationGroup[];
  /**
   * Optional. When `exclude_txn_from_change_streams` is set to `true`:
   *  * Mutations from all transactions in this batch write operation will not
   *  be recorded in change streams with DDL option `allow_txn_exclusion=true`
   *  that are tracking columns modified by these transactions.
   *  * Mutations from all transactions in this batch write operation will be
   *  recorded in change streams with DDL option `allow_txn_exclusion=false or
   *  not set` that are tracking columns modified by these transactions.
   *
   * When `exclude_txn_from_change_streams` is set to `false` or not set,
   * mutations from all transactions in this batch write operation will be
   * recorded in all change streams that are tracking columns modified by these
   * transactions.
   */
  excludeTxnFromChangeStreams: boolean;
}

/**
 * A group of mutations to be committed together. Related mutations should be
 * placed in a group. For example, two mutations inserting rows with the same
 * primary key prefix in both parent and child tables are related.
 */
export interface BatchWriteRequest_MutationGroup {
  /** Required. The mutations in this group. */
  mutations: Mutation[];
}

/** The result of applying a batch of mutations. */
export interface BatchWriteResponse {
  /**
   * The mutation groups applied in this batch. The values index into the
   * `mutation_groups` field in the corresponding `BatchWriteRequest`.
   */
  indexes: number[];
  /** An `OK` status indicates success. Any other status indicates a failure. */
  status:
    | Status
    | undefined;
  /**
   * The commit timestamp of the transaction that applied this batch.
   * Present if `status` is `OK`, absent otherwise.
   */
  commitTimestamp: Date | undefined;
}

function createBaseCreateSessionRequest(): CreateSessionRequest {
  return { database: "", session: undefined };
}

export const CreateSessionRequest: MessageFns<CreateSessionRequest> = {
  encode(message: CreateSessionRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.database !== "") {
      writer.uint32(10).string(message.database);
    }
    if (message.session !== undefined) {
      Session.encode(message.session, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateSessionRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateSessionRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.database = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.session = Session.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateSessionRequest {
    return {
      database: isSet(object.database) ? globalThis.String(object.database) : "",
      session: isSet(object.session) ? Session.fromJSON(object.session) : undefined,
    };
  },

  toJSON(message: CreateSessionRequest): unknown {
    const obj: any = {};
    if (message.database !== "") {
      obj.database = message.database;
    }
    if (message.session !== undefined) {
      obj.session = Session.toJSON(message.session);
    }
    return obj;
  },

  create(base?: DeepPartial<CreateSessionRequest>): CreateSessionRequest {
    return CreateSessionRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateSessionRequest>): CreateSessionRequest {
    const message = createBaseCreateSessionRequest();
    message.database = object.database ?? "";
    message.session = (object.session !== undefined && object.session !== null)
      ? Session.fromPartial(object.session)
      : undefined;
    return message;
  },
};

function createBaseBatchCreateSessionsRequest(): BatchCreateSessionsRequest {
  return { database: "", sessionTemplate: undefined, sessionCount: 0 };
}

export const BatchCreateSessionsRequest: MessageFns<BatchCreateSessionsRequest> = {
  encode(message: BatchCreateSessionsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.database !== "") {
      writer.uint32(10).string(message.database);
    }
    if (message.sessionTemplate !== undefined) {
      Session.encode(message.sessionTemplate, writer.uint32(18).fork()).join();
    }
    if (message.sessionCount !== 0) {
      writer.uint32(24).int32(message.sessionCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchCreateSessionsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchCreateSessionsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.database = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.sessionTemplate = Session.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.sessionCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchCreateSessionsRequest {
    return {
      database: isSet(object.database) ? globalThis.String(object.database) : "",
      sessionTemplate: isSet(object.sessionTemplate) ? Session.fromJSON(object.sessionTemplate) : undefined,
      sessionCount: isSet(object.sessionCount) ? globalThis.Number(object.sessionCount) : 0,
    };
  },

  toJSON(message: BatchCreateSessionsRequest): unknown {
    const obj: any = {};
    if (message.database !== "") {
      obj.database = message.database;
    }
    if (message.sessionTemplate !== undefined) {
      obj.sessionTemplate = Session.toJSON(message.sessionTemplate);
    }
    if (message.sessionCount !== 0) {
      obj.sessionCount = Math.round(message.sessionCount);
    }
    return obj;
  },

  create(base?: DeepPartial<BatchCreateSessionsRequest>): BatchCreateSessionsRequest {
    return BatchCreateSessionsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchCreateSessionsRequest>): BatchCreateSessionsRequest {
    const message = createBaseBatchCreateSessionsRequest();
    message.database = object.database ?? "";
    message.sessionTemplate = (object.sessionTemplate !== undefined && object.sessionTemplate !== null)
      ? Session.fromPartial(object.sessionTemplate)
      : undefined;
    message.sessionCount = object.sessionCount ?? 0;
    return message;
  },
};

function createBaseBatchCreateSessionsResponse(): BatchCreateSessionsResponse {
  return { session: [] };
}

export const BatchCreateSessionsResponse: MessageFns<BatchCreateSessionsResponse> = {
  encode(message: BatchCreateSessionsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.session) {
      Session.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchCreateSessionsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchCreateSessionsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.session.push(Session.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchCreateSessionsResponse {
    return {
      session: globalThis.Array.isArray(object?.session) ? object.session.map((e: any) => Session.fromJSON(e)) : [],
    };
  },

  toJSON(message: BatchCreateSessionsResponse): unknown {
    const obj: any = {};
    if (message.session?.length) {
      obj.session = message.session.map((e) => Session.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchCreateSessionsResponse>): BatchCreateSessionsResponse {
    return BatchCreateSessionsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchCreateSessionsResponse>): BatchCreateSessionsResponse {
    const message = createBaseBatchCreateSessionsResponse();
    message.session = object.session?.map((e) => Session.fromPartial(e)) || [];
    return message;
  },
};

function createBaseSession(): Session {
  return {
    name: "",
    labels: {},
    createTime: undefined,
    approximateLastUseTime: undefined,
    creatorRole: "",
    multiplexed: false,
  };
}

export const Session: MessageFns<Session> = {
  encode(message: Session, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    Object.entries(message.labels).forEach(([key, value]) => {
      Session_LabelsEntry.encode({ key: key as any, value }, writer.uint32(18).fork()).join();
    });
    if (message.createTime !== undefined) {
      Timestamp.encode(toTimestamp(message.createTime), writer.uint32(26).fork()).join();
    }
    if (message.approximateLastUseTime !== undefined) {
      Timestamp.encode(toTimestamp(message.approximateLastUseTime), writer.uint32(34).fork()).join();
    }
    if (message.creatorRole !== "") {
      writer.uint32(42).string(message.creatorRole);
    }
    if (message.multiplexed !== false) {
      writer.uint32(48).bool(message.multiplexed);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Session {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSession();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = Session_LabelsEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.labels[entry2.key] = entry2.value;
          }
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.createTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.approximateLastUseTime = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.creatorRole = reader.string();
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.multiplexed = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Session {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      labels: isObject(object.labels)
        ? Object.entries(object.labels).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      createTime: isSet(object.createTime) ? fromJsonTimestamp(object.createTime) : undefined,
      approximateLastUseTime: isSet(object.approximateLastUseTime)
        ? fromJsonTimestamp(object.approximateLastUseTime)
        : undefined,
      creatorRole: isSet(object.creatorRole) ? globalThis.String(object.creatorRole) : "",
      multiplexed: isSet(object.multiplexed) ? globalThis.Boolean(object.multiplexed) : false,
    };
  },

  toJSON(message: Session): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.labels) {
      const entries = Object.entries(message.labels);
      if (entries.length > 0) {
        obj.labels = {};
        entries.forEach(([k, v]) => {
          obj.labels[k] = v;
        });
      }
    }
    if (message.createTime !== undefined) {
      obj.createTime = message.createTime.toISOString();
    }
    if (message.approximateLastUseTime !== undefined) {
      obj.approximateLastUseTime = message.approximateLastUseTime.toISOString();
    }
    if (message.creatorRole !== "") {
      obj.creatorRole = message.creatorRole;
    }
    if (message.multiplexed !== false) {
      obj.multiplexed = message.multiplexed;
    }
    return obj;
  },

  create(base?: DeepPartial<Session>): Session {
    return Session.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Session>): Session {
    const message = createBaseSession();
    message.name = object.name ?? "";
    message.labels = Object.entries(object.labels ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    message.createTime = object.createTime ?? undefined;
    message.approximateLastUseTime = object.approximateLastUseTime ?? undefined;
    message.creatorRole = object.creatorRole ?? "";
    message.multiplexed = object.multiplexed ?? false;
    return message;
  },
};

function createBaseSession_LabelsEntry(): Session_LabelsEntry {
  return { key: "", value: "" };
}

export const Session_LabelsEntry: MessageFns<Session_LabelsEntry> = {
  encode(message: Session_LabelsEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Session_LabelsEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSession_LabelsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Session_LabelsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: Session_LabelsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create(base?: DeepPartial<Session_LabelsEntry>): Session_LabelsEntry {
    return Session_LabelsEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Session_LabelsEntry>): Session_LabelsEntry {
    const message = createBaseSession_LabelsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseGetSessionRequest(): GetSessionRequest {
  return { name: "" };
}

export const GetSessionRequest: MessageFns<GetSessionRequest> = {
  encode(message: GetSessionRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetSessionRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetSessionRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetSessionRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: GetSessionRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<GetSessionRequest>): GetSessionRequest {
    return GetSessionRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetSessionRequest>): GetSessionRequest {
    const message = createBaseGetSessionRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseListSessionsRequest(): ListSessionsRequest {
  return { database: "", pageSize: 0, pageToken: "", filter: "" };
}

export const ListSessionsRequest: MessageFns<ListSessionsRequest> = {
  encode(message: ListSessionsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.database !== "") {
      writer.uint32(10).string(message.database);
    }
    if (message.pageSize !== 0) {
      writer.uint32(16).int32(message.pageSize);
    }
    if (message.pageToken !== "") {
      writer.uint32(26).string(message.pageToken);
    }
    if (message.filter !== "") {
      writer.uint32(34).string(message.filter);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListSessionsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListSessionsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.database = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.pageSize = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.pageToken = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.filter = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListSessionsRequest {
    return {
      database: isSet(object.database) ? globalThis.String(object.database) : "",
      pageSize: isSet(object.pageSize) ? globalThis.Number(object.pageSize) : 0,
      pageToken: isSet(object.pageToken) ? globalThis.String(object.pageToken) : "",
      filter: isSet(object.filter) ? globalThis.String(object.filter) : "",
    };
  },

  toJSON(message: ListSessionsRequest): unknown {
    const obj: any = {};
    if (message.database !== "") {
      obj.database = message.database;
    }
    if (message.pageSize !== 0) {
      obj.pageSize = Math.round(message.pageSize);
    }
    if (message.pageToken !== "") {
      obj.pageToken = message.pageToken;
    }
    if (message.filter !== "") {
      obj.filter = message.filter;
    }
    return obj;
  },

  create(base?: DeepPartial<ListSessionsRequest>): ListSessionsRequest {
    return ListSessionsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListSessionsRequest>): ListSessionsRequest {
    const message = createBaseListSessionsRequest();
    message.database = object.database ?? "";
    message.pageSize = object.pageSize ?? 0;
    message.pageToken = object.pageToken ?? "";
    message.filter = object.filter ?? "";
    return message;
  },
};

function createBaseListSessionsResponse(): ListSessionsResponse {
  return { sessions: [], nextPageToken: "" };
}

export const ListSessionsResponse: MessageFns<ListSessionsResponse> = {
  encode(message: ListSessionsResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.sessions) {
      Session.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.nextPageToken !== "") {
      writer.uint32(18).string(message.nextPageToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListSessionsResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListSessionsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.sessions.push(Session.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.nextPageToken = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ListSessionsResponse {
    return {
      sessions: globalThis.Array.isArray(object?.sessions) ? object.sessions.map((e: any) => Session.fromJSON(e)) : [],
      nextPageToken: isSet(object.nextPageToken) ? globalThis.String(object.nextPageToken) : "",
    };
  },

  toJSON(message: ListSessionsResponse): unknown {
    const obj: any = {};
    if (message.sessions?.length) {
      obj.sessions = message.sessions.map((e) => Session.toJSON(e));
    }
    if (message.nextPageToken !== "") {
      obj.nextPageToken = message.nextPageToken;
    }
    return obj;
  },

  create(base?: DeepPartial<ListSessionsResponse>): ListSessionsResponse {
    return ListSessionsResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListSessionsResponse>): ListSessionsResponse {
    const message = createBaseListSessionsResponse();
    message.sessions = object.sessions?.map((e) => Session.fromPartial(e)) || [];
    message.nextPageToken = object.nextPageToken ?? "";
    return message;
  },
};

function createBaseDeleteSessionRequest(): DeleteSessionRequest {
  return { name: "" };
}

export const DeleteSessionRequest: MessageFns<DeleteSessionRequest> = {
  encode(message: DeleteSessionRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeleteSessionRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeleteSessionRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeleteSessionRequest {
    return { name: isSet(object.name) ? globalThis.String(object.name) : "" };
  },

  toJSON(message: DeleteSessionRequest): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    return obj;
  },

  create(base?: DeepPartial<DeleteSessionRequest>): DeleteSessionRequest {
    return DeleteSessionRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeleteSessionRequest>): DeleteSessionRequest {
    const message = createBaseDeleteSessionRequest();
    message.name = object.name ?? "";
    return message;
  },
};

function createBaseRequestOptions(): RequestOptions {
  return { priority: 0, requestTag: "", transactionTag: "" };
}

export const RequestOptions: MessageFns<RequestOptions> = {
  encode(message: RequestOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.priority !== 0) {
      writer.uint32(8).int32(message.priority);
    }
    if (message.requestTag !== "") {
      writer.uint32(18).string(message.requestTag);
    }
    if (message.transactionTag !== "") {
      writer.uint32(26).string(message.transactionTag);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RequestOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRequestOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.priority = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.requestTag = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.transactionTag = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RequestOptions {
    return {
      priority: isSet(object.priority) ? requestOptions_PriorityFromJSON(object.priority) : 0,
      requestTag: isSet(object.requestTag) ? globalThis.String(object.requestTag) : "",
      transactionTag: isSet(object.transactionTag) ? globalThis.String(object.transactionTag) : "",
    };
  },

  toJSON(message: RequestOptions): unknown {
    const obj: any = {};
    if (message.priority !== 0) {
      obj.priority = requestOptions_PriorityToJSON(message.priority);
    }
    if (message.requestTag !== "") {
      obj.requestTag = message.requestTag;
    }
    if (message.transactionTag !== "") {
      obj.transactionTag = message.transactionTag;
    }
    return obj;
  },

  create(base?: DeepPartial<RequestOptions>): RequestOptions {
    return RequestOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RequestOptions>): RequestOptions {
    const message = createBaseRequestOptions();
    message.priority = object.priority ?? 0;
    message.requestTag = object.requestTag ?? "";
    message.transactionTag = object.transactionTag ?? "";
    return message;
  },
};

function createBaseDirectedReadOptions(): DirectedReadOptions {
  return { includeReplicas: undefined, excludeReplicas: undefined };
}

export const DirectedReadOptions: MessageFns<DirectedReadOptions> = {
  encode(message: DirectedReadOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.includeReplicas !== undefined) {
      DirectedReadOptions_IncludeReplicas.encode(message.includeReplicas, writer.uint32(10).fork()).join();
    }
    if (message.excludeReplicas !== undefined) {
      DirectedReadOptions_ExcludeReplicas.encode(message.excludeReplicas, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DirectedReadOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDirectedReadOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.includeReplicas = DirectedReadOptions_IncludeReplicas.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.excludeReplicas = DirectedReadOptions_ExcludeReplicas.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DirectedReadOptions {
    return {
      includeReplicas: isSet(object.includeReplicas)
        ? DirectedReadOptions_IncludeReplicas.fromJSON(object.includeReplicas)
        : undefined,
      excludeReplicas: isSet(object.excludeReplicas)
        ? DirectedReadOptions_ExcludeReplicas.fromJSON(object.excludeReplicas)
        : undefined,
    };
  },

  toJSON(message: DirectedReadOptions): unknown {
    const obj: any = {};
    if (message.includeReplicas !== undefined) {
      obj.includeReplicas = DirectedReadOptions_IncludeReplicas.toJSON(message.includeReplicas);
    }
    if (message.excludeReplicas !== undefined) {
      obj.excludeReplicas = DirectedReadOptions_ExcludeReplicas.toJSON(message.excludeReplicas);
    }
    return obj;
  },

  create(base?: DeepPartial<DirectedReadOptions>): DirectedReadOptions {
    return DirectedReadOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DirectedReadOptions>): DirectedReadOptions {
    const message = createBaseDirectedReadOptions();
    message.includeReplicas = (object.includeReplicas !== undefined && object.includeReplicas !== null)
      ? DirectedReadOptions_IncludeReplicas.fromPartial(object.includeReplicas)
      : undefined;
    message.excludeReplicas = (object.excludeReplicas !== undefined && object.excludeReplicas !== null)
      ? DirectedReadOptions_ExcludeReplicas.fromPartial(object.excludeReplicas)
      : undefined;
    return message;
  },
};

function createBaseDirectedReadOptions_ReplicaSelection(): DirectedReadOptions_ReplicaSelection {
  return { location: "", type: 0 };
}

export const DirectedReadOptions_ReplicaSelection: MessageFns<DirectedReadOptions_ReplicaSelection> = {
  encode(message: DirectedReadOptions_ReplicaSelection, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.location !== "") {
      writer.uint32(10).string(message.location);
    }
    if (message.type !== 0) {
      writer.uint32(16).int32(message.type);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DirectedReadOptions_ReplicaSelection {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDirectedReadOptions_ReplicaSelection();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.location = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DirectedReadOptions_ReplicaSelection {
    return {
      location: isSet(object.location) ? globalThis.String(object.location) : "",
      type: isSet(object.type) ? directedReadOptions_ReplicaSelection_TypeFromJSON(object.type) : 0,
    };
  },

  toJSON(message: DirectedReadOptions_ReplicaSelection): unknown {
    const obj: any = {};
    if (message.location !== "") {
      obj.location = message.location;
    }
    if (message.type !== 0) {
      obj.type = directedReadOptions_ReplicaSelection_TypeToJSON(message.type);
    }
    return obj;
  },

  create(base?: DeepPartial<DirectedReadOptions_ReplicaSelection>): DirectedReadOptions_ReplicaSelection {
    return DirectedReadOptions_ReplicaSelection.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DirectedReadOptions_ReplicaSelection>): DirectedReadOptions_ReplicaSelection {
    const message = createBaseDirectedReadOptions_ReplicaSelection();
    message.location = object.location ?? "";
    message.type = object.type ?? 0;
    return message;
  },
};

function createBaseDirectedReadOptions_IncludeReplicas(): DirectedReadOptions_IncludeReplicas {
  return { replicaSelections: [], autoFailoverDisabled: false };
}

export const DirectedReadOptions_IncludeReplicas: MessageFns<DirectedReadOptions_IncludeReplicas> = {
  encode(message: DirectedReadOptions_IncludeReplicas, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.replicaSelections) {
      DirectedReadOptions_ReplicaSelection.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.autoFailoverDisabled !== false) {
      writer.uint32(16).bool(message.autoFailoverDisabled);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DirectedReadOptions_IncludeReplicas {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDirectedReadOptions_IncludeReplicas();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.replicaSelections.push(DirectedReadOptions_ReplicaSelection.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.autoFailoverDisabled = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DirectedReadOptions_IncludeReplicas {
    return {
      replicaSelections: globalThis.Array.isArray(object?.replicaSelections)
        ? object.replicaSelections.map((e: any) => DirectedReadOptions_ReplicaSelection.fromJSON(e))
        : [],
      autoFailoverDisabled: isSet(object.autoFailoverDisabled)
        ? globalThis.Boolean(object.autoFailoverDisabled)
        : false,
    };
  },

  toJSON(message: DirectedReadOptions_IncludeReplicas): unknown {
    const obj: any = {};
    if (message.replicaSelections?.length) {
      obj.replicaSelections = message.replicaSelections.map((e) => DirectedReadOptions_ReplicaSelection.toJSON(e));
    }
    if (message.autoFailoverDisabled !== false) {
      obj.autoFailoverDisabled = message.autoFailoverDisabled;
    }
    return obj;
  },

  create(base?: DeepPartial<DirectedReadOptions_IncludeReplicas>): DirectedReadOptions_IncludeReplicas {
    return DirectedReadOptions_IncludeReplicas.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DirectedReadOptions_IncludeReplicas>): DirectedReadOptions_IncludeReplicas {
    const message = createBaseDirectedReadOptions_IncludeReplicas();
    message.replicaSelections =
      object.replicaSelections?.map((e) => DirectedReadOptions_ReplicaSelection.fromPartial(e)) || [];
    message.autoFailoverDisabled = object.autoFailoverDisabled ?? false;
    return message;
  },
};

function createBaseDirectedReadOptions_ExcludeReplicas(): DirectedReadOptions_ExcludeReplicas {
  return { replicaSelections: [] };
}

export const DirectedReadOptions_ExcludeReplicas: MessageFns<DirectedReadOptions_ExcludeReplicas> = {
  encode(message: DirectedReadOptions_ExcludeReplicas, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.replicaSelections) {
      DirectedReadOptions_ReplicaSelection.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DirectedReadOptions_ExcludeReplicas {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDirectedReadOptions_ExcludeReplicas();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.replicaSelections.push(DirectedReadOptions_ReplicaSelection.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DirectedReadOptions_ExcludeReplicas {
    return {
      replicaSelections: globalThis.Array.isArray(object?.replicaSelections)
        ? object.replicaSelections.map((e: any) => DirectedReadOptions_ReplicaSelection.fromJSON(e))
        : [],
    };
  },

  toJSON(message: DirectedReadOptions_ExcludeReplicas): unknown {
    const obj: any = {};
    if (message.replicaSelections?.length) {
      obj.replicaSelections = message.replicaSelections.map((e) => DirectedReadOptions_ReplicaSelection.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<DirectedReadOptions_ExcludeReplicas>): DirectedReadOptions_ExcludeReplicas {
    return DirectedReadOptions_ExcludeReplicas.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DirectedReadOptions_ExcludeReplicas>): DirectedReadOptions_ExcludeReplicas {
    const message = createBaseDirectedReadOptions_ExcludeReplicas();
    message.replicaSelections =
      object.replicaSelections?.map((e) => DirectedReadOptions_ReplicaSelection.fromPartial(e)) || [];
    return message;
  },
};

function createBaseExecuteSqlRequest(): ExecuteSqlRequest {
  return {
    session: "",
    transaction: undefined,
    sql: "",
    params: undefined,
    paramTypes: {},
    resumeToken: Buffer.alloc(0),
    queryMode: 0,
    partitionToken: Buffer.alloc(0),
    seqno: Long.ZERO,
    queryOptions: undefined,
    requestOptions: undefined,
    directedReadOptions: undefined,
    dataBoostEnabled: false,
  };
}

export const ExecuteSqlRequest: MessageFns<ExecuteSqlRequest> = {
  encode(message: ExecuteSqlRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.session !== "") {
      writer.uint32(10).string(message.session);
    }
    if (message.transaction !== undefined) {
      TransactionSelector.encode(message.transaction, writer.uint32(18).fork()).join();
    }
    if (message.sql !== "") {
      writer.uint32(26).string(message.sql);
    }
    if (message.params !== undefined) {
      Struct.encode(Struct.wrap(message.params), writer.uint32(34).fork()).join();
    }
    Object.entries(message.paramTypes).forEach(([key, value]) => {
      ExecuteSqlRequest_ParamTypesEntry.encode({ key: key as any, value }, writer.uint32(42).fork()).join();
    });
    if (message.resumeToken.length !== 0) {
      writer.uint32(50).bytes(message.resumeToken);
    }
    if (message.queryMode !== 0) {
      writer.uint32(56).int32(message.queryMode);
    }
    if (message.partitionToken.length !== 0) {
      writer.uint32(66).bytes(message.partitionToken);
    }
    if (!message.seqno.equals(Long.ZERO)) {
      writer.uint32(72).int64(message.seqno.toString());
    }
    if (message.queryOptions !== undefined) {
      ExecuteSqlRequest_QueryOptions.encode(message.queryOptions, writer.uint32(82).fork()).join();
    }
    if (message.requestOptions !== undefined) {
      RequestOptions.encode(message.requestOptions, writer.uint32(90).fork()).join();
    }
    if (message.directedReadOptions !== undefined) {
      DirectedReadOptions.encode(message.directedReadOptions, writer.uint32(122).fork()).join();
    }
    if (message.dataBoostEnabled !== false) {
      writer.uint32(128).bool(message.dataBoostEnabled);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecuteSqlRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecuteSqlRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.session = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.transaction = TransactionSelector.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.sql = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.params = Struct.unwrap(Struct.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          const entry5 = ExecuteSqlRequest_ParamTypesEntry.decode(reader, reader.uint32());
          if (entry5.value !== undefined) {
            message.paramTypes[entry5.key] = entry5.value;
          }
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.resumeToken = Buffer.from(reader.bytes());
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.queryMode = reader.int32() as any;
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.partitionToken = Buffer.from(reader.bytes());
          continue;
        case 9:
          if (tag !== 72) {
            break;
          }

          message.seqno = Long.fromString(reader.int64().toString());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.queryOptions = ExecuteSqlRequest_QueryOptions.decode(reader, reader.uint32());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.requestOptions = RequestOptions.decode(reader, reader.uint32());
          continue;
        case 15:
          if (tag !== 122) {
            break;
          }

          message.directedReadOptions = DirectedReadOptions.decode(reader, reader.uint32());
          continue;
        case 16:
          if (tag !== 128) {
            break;
          }

          message.dataBoostEnabled = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecuteSqlRequest {
    return {
      session: isSet(object.session) ? globalThis.String(object.session) : "",
      transaction: isSet(object.transaction) ? TransactionSelector.fromJSON(object.transaction) : undefined,
      sql: isSet(object.sql) ? globalThis.String(object.sql) : "",
      params: isObject(object.params) ? object.params : undefined,
      paramTypes: isObject(object.paramTypes)
        ? Object.entries(object.paramTypes).reduce<{ [key: string]: Type }>((acc, [key, value]) => {
          acc[key] = Type.fromJSON(value);
          return acc;
        }, {})
        : {},
      resumeToken: isSet(object.resumeToken) ? Buffer.from(bytesFromBase64(object.resumeToken)) : Buffer.alloc(0),
      queryMode: isSet(object.queryMode) ? executeSqlRequest_QueryModeFromJSON(object.queryMode) : 0,
      partitionToken: isSet(object.partitionToken)
        ? Buffer.from(bytesFromBase64(object.partitionToken))
        : Buffer.alloc(0),
      seqno: isSet(object.seqno) ? Long.fromValue(object.seqno) : Long.ZERO,
      queryOptions: isSet(object.queryOptions)
        ? ExecuteSqlRequest_QueryOptions.fromJSON(object.queryOptions)
        : undefined,
      requestOptions: isSet(object.requestOptions) ? RequestOptions.fromJSON(object.requestOptions) : undefined,
      directedReadOptions: isSet(object.directedReadOptions)
        ? DirectedReadOptions.fromJSON(object.directedReadOptions)
        : undefined,
      dataBoostEnabled: isSet(object.dataBoostEnabled) ? globalThis.Boolean(object.dataBoostEnabled) : false,
    };
  },

  toJSON(message: ExecuteSqlRequest): unknown {
    const obj: any = {};
    if (message.session !== "") {
      obj.session = message.session;
    }
    if (message.transaction !== undefined) {
      obj.transaction = TransactionSelector.toJSON(message.transaction);
    }
    if (message.sql !== "") {
      obj.sql = message.sql;
    }
    if (message.params !== undefined) {
      obj.params = message.params;
    }
    if (message.paramTypes) {
      const entries = Object.entries(message.paramTypes);
      if (entries.length > 0) {
        obj.paramTypes = {};
        entries.forEach(([k, v]) => {
          obj.paramTypes[k] = Type.toJSON(v);
        });
      }
    }
    if (message.resumeToken.length !== 0) {
      obj.resumeToken = base64FromBytes(message.resumeToken);
    }
    if (message.queryMode !== 0) {
      obj.queryMode = executeSqlRequest_QueryModeToJSON(message.queryMode);
    }
    if (message.partitionToken.length !== 0) {
      obj.partitionToken = base64FromBytes(message.partitionToken);
    }
    if (!message.seqno.equals(Long.ZERO)) {
      obj.seqno = (message.seqno || Long.ZERO).toString();
    }
    if (message.queryOptions !== undefined) {
      obj.queryOptions = ExecuteSqlRequest_QueryOptions.toJSON(message.queryOptions);
    }
    if (message.requestOptions !== undefined) {
      obj.requestOptions = RequestOptions.toJSON(message.requestOptions);
    }
    if (message.directedReadOptions !== undefined) {
      obj.directedReadOptions = DirectedReadOptions.toJSON(message.directedReadOptions);
    }
    if (message.dataBoostEnabled !== false) {
      obj.dataBoostEnabled = message.dataBoostEnabled;
    }
    return obj;
  },

  create(base?: DeepPartial<ExecuteSqlRequest>): ExecuteSqlRequest {
    return ExecuteSqlRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecuteSqlRequest>): ExecuteSqlRequest {
    const message = createBaseExecuteSqlRequest();
    message.session = object.session ?? "";
    message.transaction = (object.transaction !== undefined && object.transaction !== null)
      ? TransactionSelector.fromPartial(object.transaction)
      : undefined;
    message.sql = object.sql ?? "";
    message.params = object.params ?? undefined;
    message.paramTypes = Object.entries(object.paramTypes ?? {}).reduce<{ [key: string]: Type }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = Type.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    message.resumeToken = object.resumeToken ?? Buffer.alloc(0);
    message.queryMode = object.queryMode ?? 0;
    message.partitionToken = object.partitionToken ?? Buffer.alloc(0);
    message.seqno = (object.seqno !== undefined && object.seqno !== null) ? Long.fromValue(object.seqno) : Long.ZERO;
    message.queryOptions = (object.queryOptions !== undefined && object.queryOptions !== null)
      ? ExecuteSqlRequest_QueryOptions.fromPartial(object.queryOptions)
      : undefined;
    message.requestOptions = (object.requestOptions !== undefined && object.requestOptions !== null)
      ? RequestOptions.fromPartial(object.requestOptions)
      : undefined;
    message.directedReadOptions = (object.directedReadOptions !== undefined && object.directedReadOptions !== null)
      ? DirectedReadOptions.fromPartial(object.directedReadOptions)
      : undefined;
    message.dataBoostEnabled = object.dataBoostEnabled ?? false;
    return message;
  },
};

function createBaseExecuteSqlRequest_QueryOptions(): ExecuteSqlRequest_QueryOptions {
  return { optimizerVersion: "", optimizerStatisticsPackage: "" };
}

export const ExecuteSqlRequest_QueryOptions: MessageFns<ExecuteSqlRequest_QueryOptions> = {
  encode(message: ExecuteSqlRequest_QueryOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.optimizerVersion !== "") {
      writer.uint32(10).string(message.optimizerVersion);
    }
    if (message.optimizerStatisticsPackage !== "") {
      writer.uint32(18).string(message.optimizerStatisticsPackage);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecuteSqlRequest_QueryOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecuteSqlRequest_QueryOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.optimizerVersion = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.optimizerStatisticsPackage = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecuteSqlRequest_QueryOptions {
    return {
      optimizerVersion: isSet(object.optimizerVersion) ? globalThis.String(object.optimizerVersion) : "",
      optimizerStatisticsPackage: isSet(object.optimizerStatisticsPackage)
        ? globalThis.String(object.optimizerStatisticsPackage)
        : "",
    };
  },

  toJSON(message: ExecuteSqlRequest_QueryOptions): unknown {
    const obj: any = {};
    if (message.optimizerVersion !== "") {
      obj.optimizerVersion = message.optimizerVersion;
    }
    if (message.optimizerStatisticsPackage !== "") {
      obj.optimizerStatisticsPackage = message.optimizerStatisticsPackage;
    }
    return obj;
  },

  create(base?: DeepPartial<ExecuteSqlRequest_QueryOptions>): ExecuteSqlRequest_QueryOptions {
    return ExecuteSqlRequest_QueryOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecuteSqlRequest_QueryOptions>): ExecuteSqlRequest_QueryOptions {
    const message = createBaseExecuteSqlRequest_QueryOptions();
    message.optimizerVersion = object.optimizerVersion ?? "";
    message.optimizerStatisticsPackage = object.optimizerStatisticsPackage ?? "";
    return message;
  },
};

function createBaseExecuteSqlRequest_ParamTypesEntry(): ExecuteSqlRequest_ParamTypesEntry {
  return { key: "", value: undefined };
}

export const ExecuteSqlRequest_ParamTypesEntry: MessageFns<ExecuteSqlRequest_ParamTypesEntry> = {
  encode(message: ExecuteSqlRequest_ParamTypesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      Type.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecuteSqlRequest_ParamTypesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecuteSqlRequest_ParamTypesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = Type.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecuteSqlRequest_ParamTypesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? Type.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: ExecuteSqlRequest_ParamTypesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = Type.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<ExecuteSqlRequest_ParamTypesEntry>): ExecuteSqlRequest_ParamTypesEntry {
    return ExecuteSqlRequest_ParamTypesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecuteSqlRequest_ParamTypesEntry>): ExecuteSqlRequest_ParamTypesEntry {
    const message = createBaseExecuteSqlRequest_ParamTypesEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null) ? Type.fromPartial(object.value) : undefined;
    return message;
  },
};

function createBaseExecuteBatchDmlRequest(): ExecuteBatchDmlRequest {
  return { session: "", transaction: undefined, statements: [], seqno: Long.ZERO, requestOptions: undefined };
}

export const ExecuteBatchDmlRequest: MessageFns<ExecuteBatchDmlRequest> = {
  encode(message: ExecuteBatchDmlRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.session !== "") {
      writer.uint32(10).string(message.session);
    }
    if (message.transaction !== undefined) {
      TransactionSelector.encode(message.transaction, writer.uint32(18).fork()).join();
    }
    for (const v of message.statements) {
      ExecuteBatchDmlRequest_Statement.encode(v!, writer.uint32(26).fork()).join();
    }
    if (!message.seqno.equals(Long.ZERO)) {
      writer.uint32(32).int64(message.seqno.toString());
    }
    if (message.requestOptions !== undefined) {
      RequestOptions.encode(message.requestOptions, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecuteBatchDmlRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecuteBatchDmlRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.session = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.transaction = TransactionSelector.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.statements.push(ExecuteBatchDmlRequest_Statement.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.seqno = Long.fromString(reader.int64().toString());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.requestOptions = RequestOptions.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecuteBatchDmlRequest {
    return {
      session: isSet(object.session) ? globalThis.String(object.session) : "",
      transaction: isSet(object.transaction) ? TransactionSelector.fromJSON(object.transaction) : undefined,
      statements: globalThis.Array.isArray(object?.statements)
        ? object.statements.map((e: any) => ExecuteBatchDmlRequest_Statement.fromJSON(e))
        : [],
      seqno: isSet(object.seqno) ? Long.fromValue(object.seqno) : Long.ZERO,
      requestOptions: isSet(object.requestOptions) ? RequestOptions.fromJSON(object.requestOptions) : undefined,
    };
  },

  toJSON(message: ExecuteBatchDmlRequest): unknown {
    const obj: any = {};
    if (message.session !== "") {
      obj.session = message.session;
    }
    if (message.transaction !== undefined) {
      obj.transaction = TransactionSelector.toJSON(message.transaction);
    }
    if (message.statements?.length) {
      obj.statements = message.statements.map((e) => ExecuteBatchDmlRequest_Statement.toJSON(e));
    }
    if (!message.seqno.equals(Long.ZERO)) {
      obj.seqno = (message.seqno || Long.ZERO).toString();
    }
    if (message.requestOptions !== undefined) {
      obj.requestOptions = RequestOptions.toJSON(message.requestOptions);
    }
    return obj;
  },

  create(base?: DeepPartial<ExecuteBatchDmlRequest>): ExecuteBatchDmlRequest {
    return ExecuteBatchDmlRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecuteBatchDmlRequest>): ExecuteBatchDmlRequest {
    const message = createBaseExecuteBatchDmlRequest();
    message.session = object.session ?? "";
    message.transaction = (object.transaction !== undefined && object.transaction !== null)
      ? TransactionSelector.fromPartial(object.transaction)
      : undefined;
    message.statements = object.statements?.map((e) => ExecuteBatchDmlRequest_Statement.fromPartial(e)) || [];
    message.seqno = (object.seqno !== undefined && object.seqno !== null) ? Long.fromValue(object.seqno) : Long.ZERO;
    message.requestOptions = (object.requestOptions !== undefined && object.requestOptions !== null)
      ? RequestOptions.fromPartial(object.requestOptions)
      : undefined;
    return message;
  },
};

function createBaseExecuteBatchDmlRequest_Statement(): ExecuteBatchDmlRequest_Statement {
  return { sql: "", params: undefined, paramTypes: {} };
}

export const ExecuteBatchDmlRequest_Statement: MessageFns<ExecuteBatchDmlRequest_Statement> = {
  encode(message: ExecuteBatchDmlRequest_Statement, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.sql !== "") {
      writer.uint32(10).string(message.sql);
    }
    if (message.params !== undefined) {
      Struct.encode(Struct.wrap(message.params), writer.uint32(18).fork()).join();
    }
    Object.entries(message.paramTypes).forEach(([key, value]) => {
      ExecuteBatchDmlRequest_Statement_ParamTypesEntry.encode({ key: key as any, value }, writer.uint32(26).fork())
        .join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecuteBatchDmlRequest_Statement {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecuteBatchDmlRequest_Statement();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.sql = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.params = Struct.unwrap(Struct.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          const entry3 = ExecuteBatchDmlRequest_Statement_ParamTypesEntry.decode(reader, reader.uint32());
          if (entry3.value !== undefined) {
            message.paramTypes[entry3.key] = entry3.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecuteBatchDmlRequest_Statement {
    return {
      sql: isSet(object.sql) ? globalThis.String(object.sql) : "",
      params: isObject(object.params) ? object.params : undefined,
      paramTypes: isObject(object.paramTypes)
        ? Object.entries(object.paramTypes).reduce<{ [key: string]: Type }>((acc, [key, value]) => {
          acc[key] = Type.fromJSON(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: ExecuteBatchDmlRequest_Statement): unknown {
    const obj: any = {};
    if (message.sql !== "") {
      obj.sql = message.sql;
    }
    if (message.params !== undefined) {
      obj.params = message.params;
    }
    if (message.paramTypes) {
      const entries = Object.entries(message.paramTypes);
      if (entries.length > 0) {
        obj.paramTypes = {};
        entries.forEach(([k, v]) => {
          obj.paramTypes[k] = Type.toJSON(v);
        });
      }
    }
    return obj;
  },

  create(base?: DeepPartial<ExecuteBatchDmlRequest_Statement>): ExecuteBatchDmlRequest_Statement {
    return ExecuteBatchDmlRequest_Statement.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecuteBatchDmlRequest_Statement>): ExecuteBatchDmlRequest_Statement {
    const message = createBaseExecuteBatchDmlRequest_Statement();
    message.sql = object.sql ?? "";
    message.params = object.params ?? undefined;
    message.paramTypes = Object.entries(object.paramTypes ?? {}).reduce<{ [key: string]: Type }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = Type.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseExecuteBatchDmlRequest_Statement_ParamTypesEntry(): ExecuteBatchDmlRequest_Statement_ParamTypesEntry {
  return { key: "", value: undefined };
}

export const ExecuteBatchDmlRequest_Statement_ParamTypesEntry: MessageFns<
  ExecuteBatchDmlRequest_Statement_ParamTypesEntry
> = {
  encode(
    message: ExecuteBatchDmlRequest_Statement_ParamTypesEntry,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      Type.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecuteBatchDmlRequest_Statement_ParamTypesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecuteBatchDmlRequest_Statement_ParamTypesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = Type.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecuteBatchDmlRequest_Statement_ParamTypesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? Type.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: ExecuteBatchDmlRequest_Statement_ParamTypesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = Type.toJSON(message.value);
    }
    return obj;
  },

  create(
    base?: DeepPartial<ExecuteBatchDmlRequest_Statement_ParamTypesEntry>,
  ): ExecuteBatchDmlRequest_Statement_ParamTypesEntry {
    return ExecuteBatchDmlRequest_Statement_ParamTypesEntry.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<ExecuteBatchDmlRequest_Statement_ParamTypesEntry>,
  ): ExecuteBatchDmlRequest_Statement_ParamTypesEntry {
    const message = createBaseExecuteBatchDmlRequest_Statement_ParamTypesEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null) ? Type.fromPartial(object.value) : undefined;
    return message;
  },
};

function createBaseExecuteBatchDmlResponse(): ExecuteBatchDmlResponse {
  return { resultSets: [], status: undefined };
}

export const ExecuteBatchDmlResponse: MessageFns<ExecuteBatchDmlResponse> = {
  encode(message: ExecuteBatchDmlResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.resultSets) {
      ResultSet.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.status !== undefined) {
      Status.encode(message.status, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ExecuteBatchDmlResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseExecuteBatchDmlResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.resultSets.push(ResultSet.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.status = Status.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ExecuteBatchDmlResponse {
    return {
      resultSets: globalThis.Array.isArray(object?.resultSets)
        ? object.resultSets.map((e: any) => ResultSet.fromJSON(e))
        : [],
      status: isSet(object.status) ? Status.fromJSON(object.status) : undefined,
    };
  },

  toJSON(message: ExecuteBatchDmlResponse): unknown {
    const obj: any = {};
    if (message.resultSets?.length) {
      obj.resultSets = message.resultSets.map((e) => ResultSet.toJSON(e));
    }
    if (message.status !== undefined) {
      obj.status = Status.toJSON(message.status);
    }
    return obj;
  },

  create(base?: DeepPartial<ExecuteBatchDmlResponse>): ExecuteBatchDmlResponse {
    return ExecuteBatchDmlResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ExecuteBatchDmlResponse>): ExecuteBatchDmlResponse {
    const message = createBaseExecuteBatchDmlResponse();
    message.resultSets = object.resultSets?.map((e) => ResultSet.fromPartial(e)) || [];
    message.status = (object.status !== undefined && object.status !== null)
      ? Status.fromPartial(object.status)
      : undefined;
    return message;
  },
};

function createBasePartitionOptions(): PartitionOptions {
  return { partitionSizeBytes: Long.ZERO, maxPartitions: Long.ZERO };
}

export const PartitionOptions: MessageFns<PartitionOptions> = {
  encode(message: PartitionOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (!message.partitionSizeBytes.equals(Long.ZERO)) {
      writer.uint32(8).int64(message.partitionSizeBytes.toString());
    }
    if (!message.maxPartitions.equals(Long.ZERO)) {
      writer.uint32(16).int64(message.maxPartitions.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PartitionOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePartitionOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.partitionSizeBytes = Long.fromString(reader.int64().toString());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.maxPartitions = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PartitionOptions {
    return {
      partitionSizeBytes: isSet(object.partitionSizeBytes) ? Long.fromValue(object.partitionSizeBytes) : Long.ZERO,
      maxPartitions: isSet(object.maxPartitions) ? Long.fromValue(object.maxPartitions) : Long.ZERO,
    };
  },

  toJSON(message: PartitionOptions): unknown {
    const obj: any = {};
    if (!message.partitionSizeBytes.equals(Long.ZERO)) {
      obj.partitionSizeBytes = (message.partitionSizeBytes || Long.ZERO).toString();
    }
    if (!message.maxPartitions.equals(Long.ZERO)) {
      obj.maxPartitions = (message.maxPartitions || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<PartitionOptions>): PartitionOptions {
    return PartitionOptions.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PartitionOptions>): PartitionOptions {
    const message = createBasePartitionOptions();
    message.partitionSizeBytes = (object.partitionSizeBytes !== undefined && object.partitionSizeBytes !== null)
      ? Long.fromValue(object.partitionSizeBytes)
      : Long.ZERO;
    message.maxPartitions = (object.maxPartitions !== undefined && object.maxPartitions !== null)
      ? Long.fromValue(object.maxPartitions)
      : Long.ZERO;
    return message;
  },
};

function createBasePartitionQueryRequest(): PartitionQueryRequest {
  return {
    session: "",
    transaction: undefined,
    sql: "",
    params: undefined,
    paramTypes: {},
    partitionOptions: undefined,
  };
}

export const PartitionQueryRequest: MessageFns<PartitionQueryRequest> = {
  encode(message: PartitionQueryRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.session !== "") {
      writer.uint32(10).string(message.session);
    }
    if (message.transaction !== undefined) {
      TransactionSelector.encode(message.transaction, writer.uint32(18).fork()).join();
    }
    if (message.sql !== "") {
      writer.uint32(26).string(message.sql);
    }
    if (message.params !== undefined) {
      Struct.encode(Struct.wrap(message.params), writer.uint32(34).fork()).join();
    }
    Object.entries(message.paramTypes).forEach(([key, value]) => {
      PartitionQueryRequest_ParamTypesEntry.encode({ key: key as any, value }, writer.uint32(42).fork()).join();
    });
    if (message.partitionOptions !== undefined) {
      PartitionOptions.encode(message.partitionOptions, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PartitionQueryRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePartitionQueryRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.session = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.transaction = TransactionSelector.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.sql = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.params = Struct.unwrap(Struct.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          const entry5 = PartitionQueryRequest_ParamTypesEntry.decode(reader, reader.uint32());
          if (entry5.value !== undefined) {
            message.paramTypes[entry5.key] = entry5.value;
          }
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.partitionOptions = PartitionOptions.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PartitionQueryRequest {
    return {
      session: isSet(object.session) ? globalThis.String(object.session) : "",
      transaction: isSet(object.transaction) ? TransactionSelector.fromJSON(object.transaction) : undefined,
      sql: isSet(object.sql) ? globalThis.String(object.sql) : "",
      params: isObject(object.params) ? object.params : undefined,
      paramTypes: isObject(object.paramTypes)
        ? Object.entries(object.paramTypes).reduce<{ [key: string]: Type }>((acc, [key, value]) => {
          acc[key] = Type.fromJSON(value);
          return acc;
        }, {})
        : {},
      partitionOptions: isSet(object.partitionOptions) ? PartitionOptions.fromJSON(object.partitionOptions) : undefined,
    };
  },

  toJSON(message: PartitionQueryRequest): unknown {
    const obj: any = {};
    if (message.session !== "") {
      obj.session = message.session;
    }
    if (message.transaction !== undefined) {
      obj.transaction = TransactionSelector.toJSON(message.transaction);
    }
    if (message.sql !== "") {
      obj.sql = message.sql;
    }
    if (message.params !== undefined) {
      obj.params = message.params;
    }
    if (message.paramTypes) {
      const entries = Object.entries(message.paramTypes);
      if (entries.length > 0) {
        obj.paramTypes = {};
        entries.forEach(([k, v]) => {
          obj.paramTypes[k] = Type.toJSON(v);
        });
      }
    }
    if (message.partitionOptions !== undefined) {
      obj.partitionOptions = PartitionOptions.toJSON(message.partitionOptions);
    }
    return obj;
  },

  create(base?: DeepPartial<PartitionQueryRequest>): PartitionQueryRequest {
    return PartitionQueryRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PartitionQueryRequest>): PartitionQueryRequest {
    const message = createBasePartitionQueryRequest();
    message.session = object.session ?? "";
    message.transaction = (object.transaction !== undefined && object.transaction !== null)
      ? TransactionSelector.fromPartial(object.transaction)
      : undefined;
    message.sql = object.sql ?? "";
    message.params = object.params ?? undefined;
    message.paramTypes = Object.entries(object.paramTypes ?? {}).reduce<{ [key: string]: Type }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = Type.fromPartial(value);
        }
        return acc;
      },
      {},
    );
    message.partitionOptions = (object.partitionOptions !== undefined && object.partitionOptions !== null)
      ? PartitionOptions.fromPartial(object.partitionOptions)
      : undefined;
    return message;
  },
};

function createBasePartitionQueryRequest_ParamTypesEntry(): PartitionQueryRequest_ParamTypesEntry {
  return { key: "", value: undefined };
}

export const PartitionQueryRequest_ParamTypesEntry: MessageFns<PartitionQueryRequest_ParamTypesEntry> = {
  encode(message: PartitionQueryRequest_ParamTypesEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== undefined) {
      Type.encode(message.value, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PartitionQueryRequest_ParamTypesEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePartitionQueryRequest_ParamTypesEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = Type.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PartitionQueryRequest_ParamTypesEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? Type.fromJSON(object.value) : undefined,
    };
  },

  toJSON(message: PartitionQueryRequest_ParamTypesEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = Type.toJSON(message.value);
    }
    return obj;
  },

  create(base?: DeepPartial<PartitionQueryRequest_ParamTypesEntry>): PartitionQueryRequest_ParamTypesEntry {
    return PartitionQueryRequest_ParamTypesEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PartitionQueryRequest_ParamTypesEntry>): PartitionQueryRequest_ParamTypesEntry {
    const message = createBasePartitionQueryRequest_ParamTypesEntry();
    message.key = object.key ?? "";
    message.value = (object.value !== undefined && object.value !== null) ? Type.fromPartial(object.value) : undefined;
    return message;
  },
};

function createBasePartitionReadRequest(): PartitionReadRequest {
  return {
    session: "",
    transaction: undefined,
    table: "",
    index: "",
    columns: [],
    keySet: undefined,
    partitionOptions: undefined,
  };
}

export const PartitionReadRequest: MessageFns<PartitionReadRequest> = {
  encode(message: PartitionReadRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.session !== "") {
      writer.uint32(10).string(message.session);
    }
    if (message.transaction !== undefined) {
      TransactionSelector.encode(message.transaction, writer.uint32(18).fork()).join();
    }
    if (message.table !== "") {
      writer.uint32(26).string(message.table);
    }
    if (message.index !== "") {
      writer.uint32(34).string(message.index);
    }
    for (const v of message.columns) {
      writer.uint32(42).string(v!);
    }
    if (message.keySet !== undefined) {
      KeySet.encode(message.keySet, writer.uint32(50).fork()).join();
    }
    if (message.partitionOptions !== undefined) {
      PartitionOptions.encode(message.partitionOptions, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PartitionReadRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePartitionReadRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.session = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.transaction = TransactionSelector.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.table = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.index = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.columns.push(reader.string());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.keySet = KeySet.decode(reader, reader.uint32());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.partitionOptions = PartitionOptions.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PartitionReadRequest {
    return {
      session: isSet(object.session) ? globalThis.String(object.session) : "",
      transaction: isSet(object.transaction) ? TransactionSelector.fromJSON(object.transaction) : undefined,
      table: isSet(object.table) ? globalThis.String(object.table) : "",
      index: isSet(object.index) ? globalThis.String(object.index) : "",
      columns: globalThis.Array.isArray(object?.columns) ? object.columns.map((e: any) => globalThis.String(e)) : [],
      keySet: isSet(object.keySet) ? KeySet.fromJSON(object.keySet) : undefined,
      partitionOptions: isSet(object.partitionOptions) ? PartitionOptions.fromJSON(object.partitionOptions) : undefined,
    };
  },

  toJSON(message: PartitionReadRequest): unknown {
    const obj: any = {};
    if (message.session !== "") {
      obj.session = message.session;
    }
    if (message.transaction !== undefined) {
      obj.transaction = TransactionSelector.toJSON(message.transaction);
    }
    if (message.table !== "") {
      obj.table = message.table;
    }
    if (message.index !== "") {
      obj.index = message.index;
    }
    if (message.columns?.length) {
      obj.columns = message.columns;
    }
    if (message.keySet !== undefined) {
      obj.keySet = KeySet.toJSON(message.keySet);
    }
    if (message.partitionOptions !== undefined) {
      obj.partitionOptions = PartitionOptions.toJSON(message.partitionOptions);
    }
    return obj;
  },

  create(base?: DeepPartial<PartitionReadRequest>): PartitionReadRequest {
    return PartitionReadRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PartitionReadRequest>): PartitionReadRequest {
    const message = createBasePartitionReadRequest();
    message.session = object.session ?? "";
    message.transaction = (object.transaction !== undefined && object.transaction !== null)
      ? TransactionSelector.fromPartial(object.transaction)
      : undefined;
    message.table = object.table ?? "";
    message.index = object.index ?? "";
    message.columns = object.columns?.map((e) => e) || [];
    message.keySet = (object.keySet !== undefined && object.keySet !== null)
      ? KeySet.fromPartial(object.keySet)
      : undefined;
    message.partitionOptions = (object.partitionOptions !== undefined && object.partitionOptions !== null)
      ? PartitionOptions.fromPartial(object.partitionOptions)
      : undefined;
    return message;
  },
};

function createBasePartition(): Partition {
  return { partitionToken: Buffer.alloc(0) };
}

export const Partition: MessageFns<Partition> = {
  encode(message: Partition, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.partitionToken.length !== 0) {
      writer.uint32(10).bytes(message.partitionToken);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Partition {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePartition();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.partitionToken = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Partition {
    return {
      partitionToken: isSet(object.partitionToken)
        ? Buffer.from(bytesFromBase64(object.partitionToken))
        : Buffer.alloc(0),
    };
  },

  toJSON(message: Partition): unknown {
    const obj: any = {};
    if (message.partitionToken.length !== 0) {
      obj.partitionToken = base64FromBytes(message.partitionToken);
    }
    return obj;
  },

  create(base?: DeepPartial<Partition>): Partition {
    return Partition.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Partition>): Partition {
    const message = createBasePartition();
    message.partitionToken = object.partitionToken ?? Buffer.alloc(0);
    return message;
  },
};

function createBasePartitionResponse(): PartitionResponse {
  return { partitions: [], transaction: undefined };
}

export const PartitionResponse: MessageFns<PartitionResponse> = {
  encode(message: PartitionResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.partitions) {
      Partition.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.transaction !== undefined) {
      Transaction.encode(message.transaction, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PartitionResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePartitionResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.partitions.push(Partition.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.transaction = Transaction.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PartitionResponse {
    return {
      partitions: globalThis.Array.isArray(object?.partitions)
        ? object.partitions.map((e: any) => Partition.fromJSON(e))
        : [],
      transaction: isSet(object.transaction) ? Transaction.fromJSON(object.transaction) : undefined,
    };
  },

  toJSON(message: PartitionResponse): unknown {
    const obj: any = {};
    if (message.partitions?.length) {
      obj.partitions = message.partitions.map((e) => Partition.toJSON(e));
    }
    if (message.transaction !== undefined) {
      obj.transaction = Transaction.toJSON(message.transaction);
    }
    return obj;
  },

  create(base?: DeepPartial<PartitionResponse>): PartitionResponse {
    return PartitionResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PartitionResponse>): PartitionResponse {
    const message = createBasePartitionResponse();
    message.partitions = object.partitions?.map((e) => Partition.fromPartial(e)) || [];
    message.transaction = (object.transaction !== undefined && object.transaction !== null)
      ? Transaction.fromPartial(object.transaction)
      : undefined;
    return message;
  },
};

function createBaseReadRequest(): ReadRequest {
  return {
    session: "",
    transaction: undefined,
    table: "",
    index: "",
    columns: [],
    keySet: undefined,
    limit: Long.ZERO,
    resumeToken: Buffer.alloc(0),
    partitionToken: Buffer.alloc(0),
    requestOptions: undefined,
    directedReadOptions: undefined,
    dataBoostEnabled: false,
    orderBy: 0,
    lockHint: 0,
  };
}

export const ReadRequest: MessageFns<ReadRequest> = {
  encode(message: ReadRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.session !== "") {
      writer.uint32(10).string(message.session);
    }
    if (message.transaction !== undefined) {
      TransactionSelector.encode(message.transaction, writer.uint32(18).fork()).join();
    }
    if (message.table !== "") {
      writer.uint32(26).string(message.table);
    }
    if (message.index !== "") {
      writer.uint32(34).string(message.index);
    }
    for (const v of message.columns) {
      writer.uint32(42).string(v!);
    }
    if (message.keySet !== undefined) {
      KeySet.encode(message.keySet, writer.uint32(50).fork()).join();
    }
    if (!message.limit.equals(Long.ZERO)) {
      writer.uint32(64).int64(message.limit.toString());
    }
    if (message.resumeToken.length !== 0) {
      writer.uint32(74).bytes(message.resumeToken);
    }
    if (message.partitionToken.length !== 0) {
      writer.uint32(82).bytes(message.partitionToken);
    }
    if (message.requestOptions !== undefined) {
      RequestOptions.encode(message.requestOptions, writer.uint32(90).fork()).join();
    }
    if (message.directedReadOptions !== undefined) {
      DirectedReadOptions.encode(message.directedReadOptions, writer.uint32(114).fork()).join();
    }
    if (message.dataBoostEnabled !== false) {
      writer.uint32(120).bool(message.dataBoostEnabled);
    }
    if (message.orderBy !== 0) {
      writer.uint32(128).int32(message.orderBy);
    }
    if (message.lockHint !== 0) {
      writer.uint32(136).int32(message.lockHint);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ReadRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseReadRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.session = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.transaction = TransactionSelector.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.table = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.index = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.columns.push(reader.string());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.keySet = KeySet.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.limit = Long.fromString(reader.int64().toString());
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          message.resumeToken = Buffer.from(reader.bytes());
          continue;
        case 10:
          if (tag !== 82) {
            break;
          }

          message.partitionToken = Buffer.from(reader.bytes());
          continue;
        case 11:
          if (tag !== 90) {
            break;
          }

          message.requestOptions = RequestOptions.decode(reader, reader.uint32());
          continue;
        case 14:
          if (tag !== 114) {
            break;
          }

          message.directedReadOptions = DirectedReadOptions.decode(reader, reader.uint32());
          continue;
        case 15:
          if (tag !== 120) {
            break;
          }

          message.dataBoostEnabled = reader.bool();
          continue;
        case 16:
          if (tag !== 128) {
            break;
          }

          message.orderBy = reader.int32() as any;
          continue;
        case 17:
          if (tag !== 136) {
            break;
          }

          message.lockHint = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ReadRequest {
    return {
      session: isSet(object.session) ? globalThis.String(object.session) : "",
      transaction: isSet(object.transaction) ? TransactionSelector.fromJSON(object.transaction) : undefined,
      table: isSet(object.table) ? globalThis.String(object.table) : "",
      index: isSet(object.index) ? globalThis.String(object.index) : "",
      columns: globalThis.Array.isArray(object?.columns) ? object.columns.map((e: any) => globalThis.String(e)) : [],
      keySet: isSet(object.keySet) ? KeySet.fromJSON(object.keySet) : undefined,
      limit: isSet(object.limit) ? Long.fromValue(object.limit) : Long.ZERO,
      resumeToken: isSet(object.resumeToken) ? Buffer.from(bytesFromBase64(object.resumeToken)) : Buffer.alloc(0),
      partitionToken: isSet(object.partitionToken)
        ? Buffer.from(bytesFromBase64(object.partitionToken))
        : Buffer.alloc(0),
      requestOptions: isSet(object.requestOptions) ? RequestOptions.fromJSON(object.requestOptions) : undefined,
      directedReadOptions: isSet(object.directedReadOptions)
        ? DirectedReadOptions.fromJSON(object.directedReadOptions)
        : undefined,
      dataBoostEnabled: isSet(object.dataBoostEnabled) ? globalThis.Boolean(object.dataBoostEnabled) : false,
      orderBy: isSet(object.orderBy) ? readRequest_OrderByFromJSON(object.orderBy) : 0,
      lockHint: isSet(object.lockHint) ? readRequest_LockHintFromJSON(object.lockHint) : 0,
    };
  },

  toJSON(message: ReadRequest): unknown {
    const obj: any = {};
    if (message.session !== "") {
      obj.session = message.session;
    }
    if (message.transaction !== undefined) {
      obj.transaction = TransactionSelector.toJSON(message.transaction);
    }
    if (message.table !== "") {
      obj.table = message.table;
    }
    if (message.index !== "") {
      obj.index = message.index;
    }
    if (message.columns?.length) {
      obj.columns = message.columns;
    }
    if (message.keySet !== undefined) {
      obj.keySet = KeySet.toJSON(message.keySet);
    }
    if (!message.limit.equals(Long.ZERO)) {
      obj.limit = (message.limit || Long.ZERO).toString();
    }
    if (message.resumeToken.length !== 0) {
      obj.resumeToken = base64FromBytes(message.resumeToken);
    }
    if (message.partitionToken.length !== 0) {
      obj.partitionToken = base64FromBytes(message.partitionToken);
    }
    if (message.requestOptions !== undefined) {
      obj.requestOptions = RequestOptions.toJSON(message.requestOptions);
    }
    if (message.directedReadOptions !== undefined) {
      obj.directedReadOptions = DirectedReadOptions.toJSON(message.directedReadOptions);
    }
    if (message.dataBoostEnabled !== false) {
      obj.dataBoostEnabled = message.dataBoostEnabled;
    }
    if (message.orderBy !== 0) {
      obj.orderBy = readRequest_OrderByToJSON(message.orderBy);
    }
    if (message.lockHint !== 0) {
      obj.lockHint = readRequest_LockHintToJSON(message.lockHint);
    }
    return obj;
  },

  create(base?: DeepPartial<ReadRequest>): ReadRequest {
    return ReadRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ReadRequest>): ReadRequest {
    const message = createBaseReadRequest();
    message.session = object.session ?? "";
    message.transaction = (object.transaction !== undefined && object.transaction !== null)
      ? TransactionSelector.fromPartial(object.transaction)
      : undefined;
    message.table = object.table ?? "";
    message.index = object.index ?? "";
    message.columns = object.columns?.map((e) => e) || [];
    message.keySet = (object.keySet !== undefined && object.keySet !== null)
      ? KeySet.fromPartial(object.keySet)
      : undefined;
    message.limit = (object.limit !== undefined && object.limit !== null) ? Long.fromValue(object.limit) : Long.ZERO;
    message.resumeToken = object.resumeToken ?? Buffer.alloc(0);
    message.partitionToken = object.partitionToken ?? Buffer.alloc(0);
    message.requestOptions = (object.requestOptions !== undefined && object.requestOptions !== null)
      ? RequestOptions.fromPartial(object.requestOptions)
      : undefined;
    message.directedReadOptions = (object.directedReadOptions !== undefined && object.directedReadOptions !== null)
      ? DirectedReadOptions.fromPartial(object.directedReadOptions)
      : undefined;
    message.dataBoostEnabled = object.dataBoostEnabled ?? false;
    message.orderBy = object.orderBy ?? 0;
    message.lockHint = object.lockHint ?? 0;
    return message;
  },
};

function createBaseBeginTransactionRequest(): BeginTransactionRequest {
  return { session: "", options: undefined, requestOptions: undefined };
}

export const BeginTransactionRequest: MessageFns<BeginTransactionRequest> = {
  encode(message: BeginTransactionRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.session !== "") {
      writer.uint32(10).string(message.session);
    }
    if (message.options !== undefined) {
      TransactionOptions.encode(message.options, writer.uint32(18).fork()).join();
    }
    if (message.requestOptions !== undefined) {
      RequestOptions.encode(message.requestOptions, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BeginTransactionRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBeginTransactionRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.session = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.options = TransactionOptions.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.requestOptions = RequestOptions.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BeginTransactionRequest {
    return {
      session: isSet(object.session) ? globalThis.String(object.session) : "",
      options: isSet(object.options) ? TransactionOptions.fromJSON(object.options) : undefined,
      requestOptions: isSet(object.requestOptions) ? RequestOptions.fromJSON(object.requestOptions) : undefined,
    };
  },

  toJSON(message: BeginTransactionRequest): unknown {
    const obj: any = {};
    if (message.session !== "") {
      obj.session = message.session;
    }
    if (message.options !== undefined) {
      obj.options = TransactionOptions.toJSON(message.options);
    }
    if (message.requestOptions !== undefined) {
      obj.requestOptions = RequestOptions.toJSON(message.requestOptions);
    }
    return obj;
  },

  create(base?: DeepPartial<BeginTransactionRequest>): BeginTransactionRequest {
    return BeginTransactionRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BeginTransactionRequest>): BeginTransactionRequest {
    const message = createBaseBeginTransactionRequest();
    message.session = object.session ?? "";
    message.options = (object.options !== undefined && object.options !== null)
      ? TransactionOptions.fromPartial(object.options)
      : undefined;
    message.requestOptions = (object.requestOptions !== undefined && object.requestOptions !== null)
      ? RequestOptions.fromPartial(object.requestOptions)
      : undefined;
    return message;
  },
};

function createBaseCommitRequest(): CommitRequest {
  return {
    session: "",
    transactionId: undefined,
    singleUseTransaction: undefined,
    mutations: [],
    returnCommitStats: false,
    maxCommitDelay: undefined,
    requestOptions: undefined,
  };
}

export const CommitRequest: MessageFns<CommitRequest> = {
  encode(message: CommitRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.session !== "") {
      writer.uint32(10).string(message.session);
    }
    if (message.transactionId !== undefined) {
      writer.uint32(18).bytes(message.transactionId);
    }
    if (message.singleUseTransaction !== undefined) {
      TransactionOptions.encode(message.singleUseTransaction, writer.uint32(26).fork()).join();
    }
    for (const v of message.mutations) {
      Mutation.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.returnCommitStats !== false) {
      writer.uint32(40).bool(message.returnCommitStats);
    }
    if (message.maxCommitDelay !== undefined) {
      Duration.encode(message.maxCommitDelay, writer.uint32(66).fork()).join();
    }
    if (message.requestOptions !== undefined) {
      RequestOptions.encode(message.requestOptions, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CommitRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCommitRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.session = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.transactionId = Buffer.from(reader.bytes());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.singleUseTransaction = TransactionOptions.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.mutations.push(Mutation.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.returnCommitStats = reader.bool();
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.maxCommitDelay = Duration.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.requestOptions = RequestOptions.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CommitRequest {
    return {
      session: isSet(object.session) ? globalThis.String(object.session) : "",
      transactionId: isSet(object.transactionId) ? Buffer.from(bytesFromBase64(object.transactionId)) : undefined,
      singleUseTransaction: isSet(object.singleUseTransaction)
        ? TransactionOptions.fromJSON(object.singleUseTransaction)
        : undefined,
      mutations: globalThis.Array.isArray(object?.mutations)
        ? object.mutations.map((e: any) => Mutation.fromJSON(e))
        : [],
      returnCommitStats: isSet(object.returnCommitStats) ? globalThis.Boolean(object.returnCommitStats) : false,
      maxCommitDelay: isSet(object.maxCommitDelay) ? Duration.fromJSON(object.maxCommitDelay) : undefined,
      requestOptions: isSet(object.requestOptions) ? RequestOptions.fromJSON(object.requestOptions) : undefined,
    };
  },

  toJSON(message: CommitRequest): unknown {
    const obj: any = {};
    if (message.session !== "") {
      obj.session = message.session;
    }
    if (message.transactionId !== undefined) {
      obj.transactionId = base64FromBytes(message.transactionId);
    }
    if (message.singleUseTransaction !== undefined) {
      obj.singleUseTransaction = TransactionOptions.toJSON(message.singleUseTransaction);
    }
    if (message.mutations?.length) {
      obj.mutations = message.mutations.map((e) => Mutation.toJSON(e));
    }
    if (message.returnCommitStats !== false) {
      obj.returnCommitStats = message.returnCommitStats;
    }
    if (message.maxCommitDelay !== undefined) {
      obj.maxCommitDelay = Duration.toJSON(message.maxCommitDelay);
    }
    if (message.requestOptions !== undefined) {
      obj.requestOptions = RequestOptions.toJSON(message.requestOptions);
    }
    return obj;
  },

  create(base?: DeepPartial<CommitRequest>): CommitRequest {
    return CommitRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CommitRequest>): CommitRequest {
    const message = createBaseCommitRequest();
    message.session = object.session ?? "";
    message.transactionId = object.transactionId ?? undefined;
    message.singleUseTransaction = (object.singleUseTransaction !== undefined && object.singleUseTransaction !== null)
      ? TransactionOptions.fromPartial(object.singleUseTransaction)
      : undefined;
    message.mutations = object.mutations?.map((e) => Mutation.fromPartial(e)) || [];
    message.returnCommitStats = object.returnCommitStats ?? false;
    message.maxCommitDelay = (object.maxCommitDelay !== undefined && object.maxCommitDelay !== null)
      ? Duration.fromPartial(object.maxCommitDelay)
      : undefined;
    message.requestOptions = (object.requestOptions !== undefined && object.requestOptions !== null)
      ? RequestOptions.fromPartial(object.requestOptions)
      : undefined;
    return message;
  },
};

function createBaseRollbackRequest(): RollbackRequest {
  return { session: "", transactionId: Buffer.alloc(0) };
}

export const RollbackRequest: MessageFns<RollbackRequest> = {
  encode(message: RollbackRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.session !== "") {
      writer.uint32(10).string(message.session);
    }
    if (message.transactionId.length !== 0) {
      writer.uint32(18).bytes(message.transactionId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RollbackRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRollbackRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.session = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.transactionId = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RollbackRequest {
    return {
      session: isSet(object.session) ? globalThis.String(object.session) : "",
      transactionId: isSet(object.transactionId) ? Buffer.from(bytesFromBase64(object.transactionId)) : Buffer.alloc(0),
    };
  },

  toJSON(message: RollbackRequest): unknown {
    const obj: any = {};
    if (message.session !== "") {
      obj.session = message.session;
    }
    if (message.transactionId.length !== 0) {
      obj.transactionId = base64FromBytes(message.transactionId);
    }
    return obj;
  },

  create(base?: DeepPartial<RollbackRequest>): RollbackRequest {
    return RollbackRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RollbackRequest>): RollbackRequest {
    const message = createBaseRollbackRequest();
    message.session = object.session ?? "";
    message.transactionId = object.transactionId ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseBatchWriteRequest(): BatchWriteRequest {
  return { session: "", requestOptions: undefined, mutationGroups: [], excludeTxnFromChangeStreams: false };
}

export const BatchWriteRequest: MessageFns<BatchWriteRequest> = {
  encode(message: BatchWriteRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.session !== "") {
      writer.uint32(10).string(message.session);
    }
    if (message.requestOptions !== undefined) {
      RequestOptions.encode(message.requestOptions, writer.uint32(26).fork()).join();
    }
    for (const v of message.mutationGroups) {
      BatchWriteRequest_MutationGroup.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.excludeTxnFromChangeStreams !== false) {
      writer.uint32(40).bool(message.excludeTxnFromChangeStreams);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchWriteRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchWriteRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.session = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.requestOptions = RequestOptions.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.mutationGroups.push(BatchWriteRequest_MutationGroup.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.excludeTxnFromChangeStreams = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchWriteRequest {
    return {
      session: isSet(object.session) ? globalThis.String(object.session) : "",
      requestOptions: isSet(object.requestOptions) ? RequestOptions.fromJSON(object.requestOptions) : undefined,
      mutationGroups: globalThis.Array.isArray(object?.mutationGroups)
        ? object.mutationGroups.map((e: any) => BatchWriteRequest_MutationGroup.fromJSON(e))
        : [],
      excludeTxnFromChangeStreams: isSet(object.excludeTxnFromChangeStreams)
        ? globalThis.Boolean(object.excludeTxnFromChangeStreams)
        : false,
    };
  },

  toJSON(message: BatchWriteRequest): unknown {
    const obj: any = {};
    if (message.session !== "") {
      obj.session = message.session;
    }
    if (message.requestOptions !== undefined) {
      obj.requestOptions = RequestOptions.toJSON(message.requestOptions);
    }
    if (message.mutationGroups?.length) {
      obj.mutationGroups = message.mutationGroups.map((e) => BatchWriteRequest_MutationGroup.toJSON(e));
    }
    if (message.excludeTxnFromChangeStreams !== false) {
      obj.excludeTxnFromChangeStreams = message.excludeTxnFromChangeStreams;
    }
    return obj;
  },

  create(base?: DeepPartial<BatchWriteRequest>): BatchWriteRequest {
    return BatchWriteRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchWriteRequest>): BatchWriteRequest {
    const message = createBaseBatchWriteRequest();
    message.session = object.session ?? "";
    message.requestOptions = (object.requestOptions !== undefined && object.requestOptions !== null)
      ? RequestOptions.fromPartial(object.requestOptions)
      : undefined;
    message.mutationGroups = object.mutationGroups?.map((e) => BatchWriteRequest_MutationGroup.fromPartial(e)) || [];
    message.excludeTxnFromChangeStreams = object.excludeTxnFromChangeStreams ?? false;
    return message;
  },
};

function createBaseBatchWriteRequest_MutationGroup(): BatchWriteRequest_MutationGroup {
  return { mutations: [] };
}

export const BatchWriteRequest_MutationGroup: MessageFns<BatchWriteRequest_MutationGroup> = {
  encode(message: BatchWriteRequest_MutationGroup, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.mutations) {
      Mutation.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchWriteRequest_MutationGroup {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchWriteRequest_MutationGroup();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.mutations.push(Mutation.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchWriteRequest_MutationGroup {
    return {
      mutations: globalThis.Array.isArray(object?.mutations)
        ? object.mutations.map((e: any) => Mutation.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BatchWriteRequest_MutationGroup): unknown {
    const obj: any = {};
    if (message.mutations?.length) {
      obj.mutations = message.mutations.map((e) => Mutation.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<BatchWriteRequest_MutationGroup>): BatchWriteRequest_MutationGroup {
    return BatchWriteRequest_MutationGroup.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchWriteRequest_MutationGroup>): BatchWriteRequest_MutationGroup {
    const message = createBaseBatchWriteRequest_MutationGroup();
    message.mutations = object.mutations?.map((e) => Mutation.fromPartial(e)) || [];
    return message;
  },
};

function createBaseBatchWriteResponse(): BatchWriteResponse {
  return { indexes: [], status: undefined, commitTimestamp: undefined };
}

export const BatchWriteResponse: MessageFns<BatchWriteResponse> = {
  encode(message: BatchWriteResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    writer.uint32(10).fork();
    for (const v of message.indexes) {
      writer.int32(v);
    }
    writer.join();
    if (message.status !== undefined) {
      Status.encode(message.status, writer.uint32(18).fork()).join();
    }
    if (message.commitTimestamp !== undefined) {
      Timestamp.encode(toTimestamp(message.commitTimestamp), writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): BatchWriteResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBatchWriteResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag === 8) {
            message.indexes.push(reader.int32());

            continue;
          }

          if (tag === 10) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.indexes.push(reader.int32());
            }

            continue;
          }

          break;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.status = Status.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.commitTimestamp = fromTimestamp(Timestamp.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BatchWriteResponse {
    return {
      indexes: globalThis.Array.isArray(object?.indexes) ? object.indexes.map((e: any) => globalThis.Number(e)) : [],
      status: isSet(object.status) ? Status.fromJSON(object.status) : undefined,
      commitTimestamp: isSet(object.commitTimestamp) ? fromJsonTimestamp(object.commitTimestamp) : undefined,
    };
  },

  toJSON(message: BatchWriteResponse): unknown {
    const obj: any = {};
    if (message.indexes?.length) {
      obj.indexes = message.indexes.map((e) => Math.round(e));
    }
    if (message.status !== undefined) {
      obj.status = Status.toJSON(message.status);
    }
    if (message.commitTimestamp !== undefined) {
      obj.commitTimestamp = message.commitTimestamp.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<BatchWriteResponse>): BatchWriteResponse {
    return BatchWriteResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BatchWriteResponse>): BatchWriteResponse {
    const message = createBaseBatchWriteResponse();
    message.indexes = object.indexes?.map((e) => e) || [];
    message.status = (object.status !== undefined && object.status !== null)
      ? Status.fromPartial(object.status)
      : undefined;
    message.commitTimestamp = object.commitTimestamp ?? undefined;
    return message;
  },
};

/**
 * Cloud Spanner API
 *
 * The Cloud Spanner API can be used to manage sessions and execute
 * transactions on data stored in Cloud Spanner databases.
 */
export type SpannerDefinition = typeof SpannerDefinition;
export const SpannerDefinition = {
  name: "Spanner",
  fullName: "google.spanner.v1.Spanner",
  methods: {
    /**
     * Creates a new session. A session can be used to perform
     * transactions that read and/or modify data in a Cloud Spanner database.
     * Sessions are meant to be reused for many consecutive
     * transactions.
     *
     * Sessions can only execute one transaction at a time. To execute
     * multiple concurrent read-write/write-only transactions, create
     * multiple sessions. Note that standalone reads and queries use a
     * transaction internally, and count toward the one transaction
     * limit.
     *
     * Active sessions use additional server resources, so it is a good idea to
     * delete idle and unneeded sessions.
     * Aside from explicit deletes, Cloud Spanner may delete sessions for which no
     * operations are sent for more than an hour. If a session is deleted,
     * requests to it return `NOT_FOUND`.
     *
     * Idle sessions can be kept alive by sending a trivial SQL query
     * periodically, e.g., `"SELECT 1"`.
     */
    createSession: {
      name: "CreateSession",
      requestType: CreateSessionRequest,
      requestStream: false,
      responseType: Session,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([8, 100, 97, 116, 97, 98, 97, 115, 101])],
          578365826: [
            Buffer.from([
              63,
              58,
              1,
              42,
              34,
              58,
              47,
              118,
              49,
              47,
              123,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              115,
              47,
              42,
              125,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Creates multiple new sessions.
     *
     * This API can be used to initialize a session cache on the clients.
     * See https://goo.gl/TgSFN2 for best practices on session cache management.
     */
    batchCreateSessions: {
      name: "BatchCreateSessions",
      requestType: BatchCreateSessionsRequest,
      requestStream: false,
      responseType: BatchCreateSessionsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              22,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              44,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              95,
              99,
              111,
              117,
              110,
              116,
            ]),
          ],
          578365826: [
            Buffer.from([
              75,
              58,
              1,
              42,
              34,
              70,
              47,
              118,
              49,
              47,
              123,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              115,
              47,
              42,
              125,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              58,
              98,
              97,
              116,
              99,
              104,
              67,
              114,
              101,
              97,
              116,
              101,
            ]),
          ],
        },
      },
    },
    /**
     * Gets a session. Returns `NOT_FOUND` if the session does not exist.
     * This is mainly useful for determining whether a session is still
     * alive.
     */
    getSession: {
      name: "GetSession",
      requestType: GetSessionRequest,
      requestStream: false,
      responseType: Session,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              58,
              18,
              56,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /** Lists all sessions in a given database. */
    listSessions: {
      name: "ListSessions",
      requestType: ListSessionsRequest,
      requestStream: false,
      responseType: ListSessionsResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([8, 100, 97, 116, 97, 98, 97, 115, 101])],
          578365826: [
            Buffer.from([
              60,
              18,
              58,
              47,
              118,
              49,
              47,
              123,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              115,
              47,
              42,
              125,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
            ]),
          ],
        },
      },
    },
    /**
     * Ends a session, releasing server resources associated with it. This will
     * asynchronously trigger cancellation of any operations that are running with
     * this session.
     */
    deleteSession: {
      name: "DeleteSession",
      requestType: DeleteSessionRequest,
      requestStream: false,
      responseType: Empty,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([4, 110, 97, 109, 101])],
          578365826: [
            Buffer.from([
              58,
              42,
              56,
              47,
              118,
              49,
              47,
              123,
              110,
              97,
              109,
              101,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
            ]),
          ],
        },
      },
    },
    /**
     * Executes an SQL statement, returning all results in a single reply. This
     * method cannot be used to return a result set larger than 10 MiB;
     * if the query yields more data than that, the query fails with
     * a `FAILED_PRECONDITION` error.
     *
     * Operations inside read-write transactions might return `ABORTED`. If
     * this occurs, the application should restart the transaction from
     * the beginning. See [Transaction][google.spanner.v1.Transaction] for more
     * details.
     *
     * Larger result sets can be fetched in streaming fashion by calling
     * [ExecuteStreamingSql][google.spanner.v1.Spanner.ExecuteStreamingSql]
     * instead.
     */
    executeSql: {
      name: "ExecuteSql",
      requestType: ExecuteSqlRequest,
      requestStream: false,
      responseType: ResultSet,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              75,
              58,
              1,
              42,
              34,
              70,
              47,
              118,
              49,
              47,
              123,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              58,
              101,
              120,
              101,
              99,
              117,
              116,
              101,
              83,
              113,
              108,
            ]),
          ],
        },
      },
    },
    /**
     * Like [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql], except returns the
     * result set as a stream. Unlike
     * [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql], there is no limit on
     * the size of the returned result set. However, no individual row in the
     * result set can exceed 100 MiB, and no column value can exceed 10 MiB.
     */
    executeStreamingSql: {
      name: "ExecuteStreamingSql",
      requestType: ExecuteSqlRequest,
      requestStream: false,
      responseType: PartialResultSet,
      responseStream: true,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              84,
              58,
              1,
              42,
              34,
              79,
              47,
              118,
              49,
              47,
              123,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              58,
              101,
              120,
              101,
              99,
              117,
              116,
              101,
              83,
              116,
              114,
              101,
              97,
              109,
              105,
              110,
              103,
              83,
              113,
              108,
            ]),
          ],
        },
      },
    },
    /**
     * Executes a batch of SQL DML statements. This method allows many statements
     * to be run with lower latency than submitting them sequentially with
     * [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql].
     *
     * Statements are executed in sequential order. A request can succeed even if
     * a statement fails. The
     * [ExecuteBatchDmlResponse.status][google.spanner.v1.ExecuteBatchDmlResponse.status]
     * field in the response provides information about the statement that failed.
     * Clients must inspect this field to determine whether an error occurred.
     *
     * Execution stops after the first failed statement; the remaining statements
     * are not executed.
     */
    executeBatchDml: {
      name: "ExecuteBatchDml",
      requestType: ExecuteBatchDmlRequest,
      requestStream: false,
      responseType: ExecuteBatchDmlResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              80,
              58,
              1,
              42,
              34,
              75,
              47,
              118,
              49,
              47,
              123,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              58,
              101,
              120,
              101,
              99,
              117,
              116,
              101,
              66,
              97,
              116,
              99,
              104,
              68,
              109,
              108,
            ]),
          ],
        },
      },
    },
    /**
     * Reads rows from the database using key lookups and scans, as a
     * simple key/value style alternative to
     * [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql].  This method cannot be
     * used to return a result set larger than 10 MiB; if the read matches more
     * data than that, the read fails with a `FAILED_PRECONDITION`
     * error.
     *
     * Reads inside read-write transactions might return `ABORTED`. If
     * this occurs, the application should restart the transaction from
     * the beginning. See [Transaction][google.spanner.v1.Transaction] for more
     * details.
     *
     * Larger result sets can be yielded in streaming fashion by calling
     * [StreamingRead][google.spanner.v1.Spanner.StreamingRead] instead.
     */
    read: {
      name: "Read",
      requestType: ReadRequest,
      requestStream: false,
      responseType: ResultSet,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              69,
              58,
              1,
              42,
              34,
              64,
              47,
              118,
              49,
              47,
              123,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              58,
              114,
              101,
              97,
              100,
            ]),
          ],
        },
      },
    },
    /**
     * Like [Read][google.spanner.v1.Spanner.Read], except returns the result set
     * as a stream. Unlike [Read][google.spanner.v1.Spanner.Read], there is no
     * limit on the size of the returned result set. However, no individual row in
     * the result set can exceed 100 MiB, and no column value can exceed
     * 10 MiB.
     */
    streamingRead: {
      name: "StreamingRead",
      requestType: ReadRequest,
      requestStream: false,
      responseType: PartialResultSet,
      responseStream: true,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              78,
              58,
              1,
              42,
              34,
              73,
              47,
              118,
              49,
              47,
              123,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              58,
              115,
              116,
              114,
              101,
              97,
              109,
              105,
              110,
              103,
              82,
              101,
              97,
              100,
            ]),
          ],
        },
      },
    },
    /**
     * Begins a new transaction. This step can often be skipped:
     * [Read][google.spanner.v1.Spanner.Read],
     * [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql] and
     * [Commit][google.spanner.v1.Spanner.Commit] can begin a new transaction as a
     * side-effect.
     */
    beginTransaction: {
      name: "BeginTransaction",
      requestType: BeginTransactionRequest,
      requestStream: false,
      responseType: Transaction,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [Buffer.from([15, 115, 101, 115, 115, 105, 111, 110, 44, 111, 112, 116, 105, 111, 110, 115])],
          578365826: [
            Buffer.from([
              81,
              58,
              1,
              42,
              34,
              76,
              47,
              118,
              49,
              47,
              123,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              58,
              98,
              101,
              103,
              105,
              110,
              84,
              114,
              97,
              110,
              115,
              97,
              99,
              116,
              105,
              111,
              110,
            ]),
          ],
        },
      },
    },
    /**
     * Commits a transaction. The request includes the mutations to be
     * applied to rows in the database.
     *
     * `Commit` might return an `ABORTED` error. This can occur at any time;
     * commonly, the cause is conflicts with concurrent
     * transactions. However, it can also happen for a variety of other
     * reasons. If `Commit` returns `ABORTED`, the caller should re-attempt
     * the transaction from the beginning, re-using the same session.
     *
     * On very rare occasions, `Commit` might return `UNKNOWN`. This can happen,
     * for example, if the client job experiences a 1+ hour networking failure.
     * At that point, Cloud Spanner has lost track of the transaction outcome and
     * we recommend that you perform another read from the database to see the
     * state of things as they are now.
     */
    commit: {
      name: "Commit",
      requestType: CommitRequest,
      requestStream: false,
      responseType: CommitResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              32,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              44,
              116,
              114,
              97,
              110,
              115,
              97,
              99,
              116,
              105,
              111,
              110,
              95,
              105,
              100,
              44,
              109,
              117,
              116,
              97,
              116,
              105,
              111,
              110,
              115,
            ]),
            Buffer.from([
              40,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              44,
              115,
              105,
              110,
              103,
              108,
              101,
              95,
              117,
              115,
              101,
              95,
              116,
              114,
              97,
              110,
              115,
              97,
              99,
              116,
              105,
              111,
              110,
              44,
              109,
              117,
              116,
              97,
              116,
              105,
              111,
              110,
              115,
            ]),
          ],
          578365826: [
            Buffer.from([
              71,
              58,
              1,
              42,
              34,
              66,
              47,
              118,
              49,
              47,
              123,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              58,
              99,
              111,
              109,
              109,
              105,
              116,
            ]),
          ],
        },
      },
    },
    /**
     * Rolls back a transaction, releasing any locks it holds. It is a good
     * idea to call this for any transaction that includes one or more
     * [Read][google.spanner.v1.Spanner.Read] or
     * [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql] requests and ultimately
     * decides not to commit.
     *
     * `Rollback` returns `OK` if it successfully aborts the transaction, the
     * transaction was already aborted, or the transaction is not
     * found. `Rollback` never returns `ABORTED`.
     */
    rollback: {
      name: "Rollback",
      requestType: RollbackRequest,
      requestStream: false,
      responseType: Empty,
      responseStream: false,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              22,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              44,
              116,
              114,
              97,
              110,
              115,
              97,
              99,
              116,
              105,
              111,
              110,
              95,
              105,
              100,
            ]),
          ],
          578365826: [
            Buffer.from([
              73,
              58,
              1,
              42,
              34,
              68,
              47,
              118,
              49,
              47,
              123,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              58,
              114,
              111,
              108,
              108,
              98,
              97,
              99,
              107,
            ]),
          ],
        },
      },
    },
    /**
     * Creates a set of partition tokens that can be used to execute a query
     * operation in parallel.  Each of the returned partition tokens can be used
     * by [ExecuteStreamingSql][google.spanner.v1.Spanner.ExecuteStreamingSql] to
     * specify a subset of the query result to read.  The same session and
     * read-only transaction must be used by the PartitionQueryRequest used to
     * create the partition tokens and the ExecuteSqlRequests that use the
     * partition tokens.
     *
     * Partition tokens become invalid when the session used to create them
     * is deleted, is idle for too long, begins a new transaction, or becomes too
     * old.  When any of these happen, it is not possible to resume the query, and
     * the whole operation must be restarted from the beginning.
     */
    partitionQuery: {
      name: "PartitionQuery",
      requestType: PartitionQueryRequest,
      requestStream: false,
      responseType: PartitionResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              79,
              58,
              1,
              42,
              34,
              74,
              47,
              118,
              49,
              47,
              123,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              58,
              112,
              97,
              114,
              116,
              105,
              116,
              105,
              111,
              110,
              81,
              117,
              101,
              114,
              121,
            ]),
          ],
        },
      },
    },
    /**
     * Creates a set of partition tokens that can be used to execute a read
     * operation in parallel.  Each of the returned partition tokens can be used
     * by [StreamingRead][google.spanner.v1.Spanner.StreamingRead] to specify a
     * subset of the read result to read.  The same session and read-only
     * transaction must be used by the PartitionReadRequest used to create the
     * partition tokens and the ReadRequests that use the partition tokens.  There
     * are no ordering guarantees on rows returned among the returned partition
     * tokens, or even within each individual StreamingRead call issued with a
     * partition_token.
     *
     * Partition tokens become invalid when the session used to create them
     * is deleted, is idle for too long, begins a new transaction, or becomes too
     * old.  When any of these happen, it is not possible to resume the read, and
     * the whole operation must be restarted from the beginning.
     */
    partitionRead: {
      name: "PartitionRead",
      requestType: PartitionReadRequest,
      requestStream: false,
      responseType: PartitionResponse,
      responseStream: false,
      options: {
        _unknownFields: {
          578365826: [
            Buffer.from([
              78,
              58,
              1,
              42,
              34,
              73,
              47,
              118,
              49,
              47,
              123,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              58,
              112,
              97,
              114,
              116,
              105,
              116,
              105,
              111,
              110,
              82,
              101,
              97,
              100,
            ]),
          ],
        },
      },
    },
    /**
     * Batches the supplied mutation groups in a collection of efficient
     * transactions. All mutations in a group are committed atomically. However,
     * mutations across groups can be committed non-atomically in an unspecified
     * order and thus, they must be independent of each other. Partial failure is
     * possible, i.e., some groups may have been committed successfully, while
     * some may have failed. The results of individual batches are streamed into
     * the response as the batches are applied.
     *
     * BatchWrite requests are not replay protected, meaning that each mutation
     * group may be applied more than once. Replays of non-idempotent mutations
     * may have undesirable effects. For example, replays of an insert mutation
     * may produce an already exists error or if you use generated or commit
     * timestamp-based keys, it may result in additional rows being added to the
     * mutation's table. We recommend structuring your mutation groups to be
     * idempotent to avoid this issue.
     */
    batchWrite: {
      name: "BatchWrite",
      requestType: BatchWriteRequest,
      requestStream: false,
      responseType: BatchWriteResponse,
      responseStream: true,
      options: {
        _unknownFields: {
          8410: [
            Buffer.from([
              23,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              44,
              109,
              117,
              116,
              97,
              116,
              105,
              111,
              110,
              95,
              103,
              114,
              111,
              117,
              112,
              115,
            ]),
          ],
          578365826: [
            Buffer.from([
              75,
              58,
              1,
              42,
              34,
              70,
              47,
              118,
              49,
              47,
              123,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              61,
              112,
              114,
              111,
              106,
              101,
              99,
              116,
              115,
              47,
              42,
              47,
              105,
              110,
              115,
              116,
              97,
              110,
              99,
              101,
              115,
              47,
              42,
              47,
              100,
              97,
              116,
              97,
              98,
              97,
              115,
              101,
              115,
              47,
              42,
              47,
              115,
              101,
              115,
              115,
              105,
              111,
              110,
              115,
              47,
              42,
              125,
              58,
              98,
              97,
              116,
              99,
              104,
              87,
              114,
              105,
              116,
              101,
            ]),
          ],
        },
      },
    },
  },
} as const;

export interface SpannerServiceImplementation<CallContextExt = {}> {
  /**
   * Creates a new session. A session can be used to perform
   * transactions that read and/or modify data in a Cloud Spanner database.
   * Sessions are meant to be reused for many consecutive
   * transactions.
   *
   * Sessions can only execute one transaction at a time. To execute
   * multiple concurrent read-write/write-only transactions, create
   * multiple sessions. Note that standalone reads and queries use a
   * transaction internally, and count toward the one transaction
   * limit.
   *
   * Active sessions use additional server resources, so it is a good idea to
   * delete idle and unneeded sessions.
   * Aside from explicit deletes, Cloud Spanner may delete sessions for which no
   * operations are sent for more than an hour. If a session is deleted,
   * requests to it return `NOT_FOUND`.
   *
   * Idle sessions can be kept alive by sending a trivial SQL query
   * periodically, e.g., `"SELECT 1"`.
   */
  createSession(request: CreateSessionRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Session>>;
  /**
   * Creates multiple new sessions.
   *
   * This API can be used to initialize a session cache on the clients.
   * See https://goo.gl/TgSFN2 for best practices on session cache management.
   */
  batchCreateSessions(
    request: BatchCreateSessionsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<BatchCreateSessionsResponse>>;
  /**
   * Gets a session. Returns `NOT_FOUND` if the session does not exist.
   * This is mainly useful for determining whether a session is still
   * alive.
   */
  getSession(request: GetSessionRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Session>>;
  /** Lists all sessions in a given database. */
  listSessions(
    request: ListSessionsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ListSessionsResponse>>;
  /**
   * Ends a session, releasing server resources associated with it. This will
   * asynchronously trigger cancellation of any operations that are running with
   * this session.
   */
  deleteSession(request: DeleteSessionRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Empty>>;
  /**
   * Executes an SQL statement, returning all results in a single reply. This
   * method cannot be used to return a result set larger than 10 MiB;
   * if the query yields more data than that, the query fails with
   * a `FAILED_PRECONDITION` error.
   *
   * Operations inside read-write transactions might return `ABORTED`. If
   * this occurs, the application should restart the transaction from
   * the beginning. See [Transaction][google.spanner.v1.Transaction] for more
   * details.
   *
   * Larger result sets can be fetched in streaming fashion by calling
   * [ExecuteStreamingSql][google.spanner.v1.Spanner.ExecuteStreamingSql]
   * instead.
   */
  executeSql(request: ExecuteSqlRequest, context: CallContext & CallContextExt): Promise<DeepPartial<ResultSet>>;
  /**
   * Like [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql], except returns the
   * result set as a stream. Unlike
   * [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql], there is no limit on
   * the size of the returned result set. However, no individual row in the
   * result set can exceed 100 MiB, and no column value can exceed 10 MiB.
   */
  executeStreamingSql(
    request: ExecuteSqlRequest,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<PartialResultSet>>;
  /**
   * Executes a batch of SQL DML statements. This method allows many statements
   * to be run with lower latency than submitting them sequentially with
   * [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql].
   *
   * Statements are executed in sequential order. A request can succeed even if
   * a statement fails. The
   * [ExecuteBatchDmlResponse.status][google.spanner.v1.ExecuteBatchDmlResponse.status]
   * field in the response provides information about the statement that failed.
   * Clients must inspect this field to determine whether an error occurred.
   *
   * Execution stops after the first failed statement; the remaining statements
   * are not executed.
   */
  executeBatchDml(
    request: ExecuteBatchDmlRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<ExecuteBatchDmlResponse>>;
  /**
   * Reads rows from the database using key lookups and scans, as a
   * simple key/value style alternative to
   * [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql].  This method cannot be
   * used to return a result set larger than 10 MiB; if the read matches more
   * data than that, the read fails with a `FAILED_PRECONDITION`
   * error.
   *
   * Reads inside read-write transactions might return `ABORTED`. If
   * this occurs, the application should restart the transaction from
   * the beginning. See [Transaction][google.spanner.v1.Transaction] for more
   * details.
   *
   * Larger result sets can be yielded in streaming fashion by calling
   * [StreamingRead][google.spanner.v1.Spanner.StreamingRead] instead.
   */
  read(request: ReadRequest, context: CallContext & CallContextExt): Promise<DeepPartial<ResultSet>>;
  /**
   * Like [Read][google.spanner.v1.Spanner.Read], except returns the result set
   * as a stream. Unlike [Read][google.spanner.v1.Spanner.Read], there is no
   * limit on the size of the returned result set. However, no individual row in
   * the result set can exceed 100 MiB, and no column value can exceed
   * 10 MiB.
   */
  streamingRead(
    request: ReadRequest,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<PartialResultSet>>;
  /**
   * Begins a new transaction. This step can often be skipped:
   * [Read][google.spanner.v1.Spanner.Read],
   * [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql] and
   * [Commit][google.spanner.v1.Spanner.Commit] can begin a new transaction as a
   * side-effect.
   */
  beginTransaction(
    request: BeginTransactionRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Transaction>>;
  /**
   * Commits a transaction. The request includes the mutations to be
   * applied to rows in the database.
   *
   * `Commit` might return an `ABORTED` error. This can occur at any time;
   * commonly, the cause is conflicts with concurrent
   * transactions. However, it can also happen for a variety of other
   * reasons. If `Commit` returns `ABORTED`, the caller should re-attempt
   * the transaction from the beginning, re-using the same session.
   *
   * On very rare occasions, `Commit` might return `UNKNOWN`. This can happen,
   * for example, if the client job experiences a 1+ hour networking failure.
   * At that point, Cloud Spanner has lost track of the transaction outcome and
   * we recommend that you perform another read from the database to see the
   * state of things as they are now.
   */
  commit(request: CommitRequest, context: CallContext & CallContextExt): Promise<DeepPartial<CommitResponse>>;
  /**
   * Rolls back a transaction, releasing any locks it holds. It is a good
   * idea to call this for any transaction that includes one or more
   * [Read][google.spanner.v1.Spanner.Read] or
   * [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql] requests and ultimately
   * decides not to commit.
   *
   * `Rollback` returns `OK` if it successfully aborts the transaction, the
   * transaction was already aborted, or the transaction is not
   * found. `Rollback` never returns `ABORTED`.
   */
  rollback(request: RollbackRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Empty>>;
  /**
   * Creates a set of partition tokens that can be used to execute a query
   * operation in parallel.  Each of the returned partition tokens can be used
   * by [ExecuteStreamingSql][google.spanner.v1.Spanner.ExecuteStreamingSql] to
   * specify a subset of the query result to read.  The same session and
   * read-only transaction must be used by the PartitionQueryRequest used to
   * create the partition tokens and the ExecuteSqlRequests that use the
   * partition tokens.
   *
   * Partition tokens become invalid when the session used to create them
   * is deleted, is idle for too long, begins a new transaction, or becomes too
   * old.  When any of these happen, it is not possible to resume the query, and
   * the whole operation must be restarted from the beginning.
   */
  partitionQuery(
    request: PartitionQueryRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<PartitionResponse>>;
  /**
   * Creates a set of partition tokens that can be used to execute a read
   * operation in parallel.  Each of the returned partition tokens can be used
   * by [StreamingRead][google.spanner.v1.Spanner.StreamingRead] to specify a
   * subset of the read result to read.  The same session and read-only
   * transaction must be used by the PartitionReadRequest used to create the
   * partition tokens and the ReadRequests that use the partition tokens.  There
   * are no ordering guarantees on rows returned among the returned partition
   * tokens, or even within each individual StreamingRead call issued with a
   * partition_token.
   *
   * Partition tokens become invalid when the session used to create them
   * is deleted, is idle for too long, begins a new transaction, or becomes too
   * old.  When any of these happen, it is not possible to resume the read, and
   * the whole operation must be restarted from the beginning.
   */
  partitionRead(
    request: PartitionReadRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<PartitionResponse>>;
  /**
   * Batches the supplied mutation groups in a collection of efficient
   * transactions. All mutations in a group are committed atomically. However,
   * mutations across groups can be committed non-atomically in an unspecified
   * order and thus, they must be independent of each other. Partial failure is
   * possible, i.e., some groups may have been committed successfully, while
   * some may have failed. The results of individual batches are streamed into
   * the response as the batches are applied.
   *
   * BatchWrite requests are not replay protected, meaning that each mutation
   * group may be applied more than once. Replays of non-idempotent mutations
   * may have undesirable effects. For example, replays of an insert mutation
   * may produce an already exists error or if you use generated or commit
   * timestamp-based keys, it may result in additional rows being added to the
   * mutation's table. We recommend structuring your mutation groups to be
   * idempotent to avoid this issue.
   */
  batchWrite(
    request: BatchWriteRequest,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<BatchWriteResponse>>;
}

export interface SpannerClient<CallOptionsExt = {}> {
  /**
   * Creates a new session. A session can be used to perform
   * transactions that read and/or modify data in a Cloud Spanner database.
   * Sessions are meant to be reused for many consecutive
   * transactions.
   *
   * Sessions can only execute one transaction at a time. To execute
   * multiple concurrent read-write/write-only transactions, create
   * multiple sessions. Note that standalone reads and queries use a
   * transaction internally, and count toward the one transaction
   * limit.
   *
   * Active sessions use additional server resources, so it is a good idea to
   * delete idle and unneeded sessions.
   * Aside from explicit deletes, Cloud Spanner may delete sessions for which no
   * operations are sent for more than an hour. If a session is deleted,
   * requests to it return `NOT_FOUND`.
   *
   * Idle sessions can be kept alive by sending a trivial SQL query
   * periodically, e.g., `"SELECT 1"`.
   */
  createSession(request: DeepPartial<CreateSessionRequest>, options?: CallOptions & CallOptionsExt): Promise<Session>;
  /**
   * Creates multiple new sessions.
   *
   * This API can be used to initialize a session cache on the clients.
   * See https://goo.gl/TgSFN2 for best practices on session cache management.
   */
  batchCreateSessions(
    request: DeepPartial<BatchCreateSessionsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<BatchCreateSessionsResponse>;
  /**
   * Gets a session. Returns `NOT_FOUND` if the session does not exist.
   * This is mainly useful for determining whether a session is still
   * alive.
   */
  getSession(request: DeepPartial<GetSessionRequest>, options?: CallOptions & CallOptionsExt): Promise<Session>;
  /** Lists all sessions in a given database. */
  listSessions(
    request: DeepPartial<ListSessionsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ListSessionsResponse>;
  /**
   * Ends a session, releasing server resources associated with it. This will
   * asynchronously trigger cancellation of any operations that are running with
   * this session.
   */
  deleteSession(request: DeepPartial<DeleteSessionRequest>, options?: CallOptions & CallOptionsExt): Promise<Empty>;
  /**
   * Executes an SQL statement, returning all results in a single reply. This
   * method cannot be used to return a result set larger than 10 MiB;
   * if the query yields more data than that, the query fails with
   * a `FAILED_PRECONDITION` error.
   *
   * Operations inside read-write transactions might return `ABORTED`. If
   * this occurs, the application should restart the transaction from
   * the beginning. See [Transaction][google.spanner.v1.Transaction] for more
   * details.
   *
   * Larger result sets can be fetched in streaming fashion by calling
   * [ExecuteStreamingSql][google.spanner.v1.Spanner.ExecuteStreamingSql]
   * instead.
   */
  executeSql(request: DeepPartial<ExecuteSqlRequest>, options?: CallOptions & CallOptionsExt): Promise<ResultSet>;
  /**
   * Like [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql], except returns the
   * result set as a stream. Unlike
   * [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql], there is no limit on
   * the size of the returned result set. However, no individual row in the
   * result set can exceed 100 MiB, and no column value can exceed 10 MiB.
   */
  executeStreamingSql(
    request: DeepPartial<ExecuteSqlRequest>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<PartialResultSet>;
  /**
   * Executes a batch of SQL DML statements. This method allows many statements
   * to be run with lower latency than submitting them sequentially with
   * [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql].
   *
   * Statements are executed in sequential order. A request can succeed even if
   * a statement fails. The
   * [ExecuteBatchDmlResponse.status][google.spanner.v1.ExecuteBatchDmlResponse.status]
   * field in the response provides information about the statement that failed.
   * Clients must inspect this field to determine whether an error occurred.
   *
   * Execution stops after the first failed statement; the remaining statements
   * are not executed.
   */
  executeBatchDml(
    request: DeepPartial<ExecuteBatchDmlRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<ExecuteBatchDmlResponse>;
  /**
   * Reads rows from the database using key lookups and scans, as a
   * simple key/value style alternative to
   * [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql].  This method cannot be
   * used to return a result set larger than 10 MiB; if the read matches more
   * data than that, the read fails with a `FAILED_PRECONDITION`
   * error.
   *
   * Reads inside read-write transactions might return `ABORTED`. If
   * this occurs, the application should restart the transaction from
   * the beginning. See [Transaction][google.spanner.v1.Transaction] for more
   * details.
   *
   * Larger result sets can be yielded in streaming fashion by calling
   * [StreamingRead][google.spanner.v1.Spanner.StreamingRead] instead.
   */
  read(request: DeepPartial<ReadRequest>, options?: CallOptions & CallOptionsExt): Promise<ResultSet>;
  /**
   * Like [Read][google.spanner.v1.Spanner.Read], except returns the result set
   * as a stream. Unlike [Read][google.spanner.v1.Spanner.Read], there is no
   * limit on the size of the returned result set. However, no individual row in
   * the result set can exceed 100 MiB, and no column value can exceed
   * 10 MiB.
   */
  streamingRead(
    request: DeepPartial<ReadRequest>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<PartialResultSet>;
  /**
   * Begins a new transaction. This step can often be skipped:
   * [Read][google.spanner.v1.Spanner.Read],
   * [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql] and
   * [Commit][google.spanner.v1.Spanner.Commit] can begin a new transaction as a
   * side-effect.
   */
  beginTransaction(
    request: DeepPartial<BeginTransactionRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Transaction>;
  /**
   * Commits a transaction. The request includes the mutations to be
   * applied to rows in the database.
   *
   * `Commit` might return an `ABORTED` error. This can occur at any time;
   * commonly, the cause is conflicts with concurrent
   * transactions. However, it can also happen for a variety of other
   * reasons. If `Commit` returns `ABORTED`, the caller should re-attempt
   * the transaction from the beginning, re-using the same session.
   *
   * On very rare occasions, `Commit` might return `UNKNOWN`. This can happen,
   * for example, if the client job experiences a 1+ hour networking failure.
   * At that point, Cloud Spanner has lost track of the transaction outcome and
   * we recommend that you perform another read from the database to see the
   * state of things as they are now.
   */
  commit(request: DeepPartial<CommitRequest>, options?: CallOptions & CallOptionsExt): Promise<CommitResponse>;
  /**
   * Rolls back a transaction, releasing any locks it holds. It is a good
   * idea to call this for any transaction that includes one or more
   * [Read][google.spanner.v1.Spanner.Read] or
   * [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql] requests and ultimately
   * decides not to commit.
   *
   * `Rollback` returns `OK` if it successfully aborts the transaction, the
   * transaction was already aborted, or the transaction is not
   * found. `Rollback` never returns `ABORTED`.
   */
  rollback(request: DeepPartial<RollbackRequest>, options?: CallOptions & CallOptionsExt): Promise<Empty>;
  /**
   * Creates a set of partition tokens that can be used to execute a query
   * operation in parallel.  Each of the returned partition tokens can be used
   * by [ExecuteStreamingSql][google.spanner.v1.Spanner.ExecuteStreamingSql] to
   * specify a subset of the query result to read.  The same session and
   * read-only transaction must be used by the PartitionQueryRequest used to
   * create the partition tokens and the ExecuteSqlRequests that use the
   * partition tokens.
   *
   * Partition tokens become invalid when the session used to create them
   * is deleted, is idle for too long, begins a new transaction, or becomes too
   * old.  When any of these happen, it is not possible to resume the query, and
   * the whole operation must be restarted from the beginning.
   */
  partitionQuery(
    request: DeepPartial<PartitionQueryRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<PartitionResponse>;
  /**
   * Creates a set of partition tokens that can be used to execute a read
   * operation in parallel.  Each of the returned partition tokens can be used
   * by [StreamingRead][google.spanner.v1.Spanner.StreamingRead] to specify a
   * subset of the read result to read.  The same session and read-only
   * transaction must be used by the PartitionReadRequest used to create the
   * partition tokens and the ReadRequests that use the partition tokens.  There
   * are no ordering guarantees on rows returned among the returned partition
   * tokens, or even within each individual StreamingRead call issued with a
   * partition_token.
   *
   * Partition tokens become invalid when the session used to create them
   * is deleted, is idle for too long, begins a new transaction, or becomes too
   * old.  When any of these happen, it is not possible to resume the read, and
   * the whole operation must be restarted from the beginning.
   */
  partitionRead(
    request: DeepPartial<PartitionReadRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<PartitionResponse>;
  /**
   * Batches the supplied mutation groups in a collection of efficient
   * transactions. All mutations in a group are committed atomically. However,
   * mutations across groups can be committed non-atomically in an unspecified
   * order and thus, they must be independent of each other. Partial failure is
   * possible, i.e., some groups may have been committed successfully, while
   * some may have failed. The results of individual batches are streamed into
   * the response as the batches are applied.
   *
   * BatchWrite requests are not replay protected, meaning that each mutation
   * group may be applied more than once. Replays of non-idempotent mutations
   * may have undesirable effects. For example, replays of an insert mutation
   * may produce an already exists error or if you use generated or commit
   * timestamp-based keys, it may result in additional rows being added to the
   * mutation's table. We recommend structuring your mutation groups to be
   * idempotent to avoid this issue.
   */
  batchWrite(
    request: DeepPartial<BatchWriteRequest>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<BatchWriteResponse>;
}

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = numberToLong(Math.trunc(date.getTime() / 1_000));
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds.toNumber() || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function numberToLong(number: number) {
  return Long.fromNumber(number);
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export type ServerStreamingMethodResult<Response> = { [Symbol.asyncIterator](): AsyncIterator<Response, void> };

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
