// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/spanner/v1/result_set.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { ListValue, Struct, Value } from "../../protobuf/struct.js";
import { QueryPlan } from "./query_plan.js";
import { Transaction } from "./transaction.js";
import { StructType } from "./type.js";

export const protobufPackage = "google.spanner.v1";

/**
 * Results from [Read][google.spanner.v1.Spanner.Read] or
 * [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql].
 */
export interface ResultSet {
  /** Metadata about the result set, such as row type information. */
  metadata:
    | ResultSetMetadata
    | undefined;
  /**
   * Each element in `rows` is a row whose format is defined by
   * [metadata.row_type][google.spanner.v1.ResultSetMetadata.row_type]. The ith element
   * in each row matches the ith field in
   * [metadata.row_type][google.spanner.v1.ResultSetMetadata.row_type]. Elements are
   * encoded based on type as described
   * [here][google.spanner.v1.TypeCode].
   */
  rows: Array<any>[];
  /**
   * Query plan and execution statistics for the SQL statement that
   * produced this result set. These can be requested by setting
   * [ExecuteSqlRequest.query_mode][google.spanner.v1.ExecuteSqlRequest.query_mode].
   * DML statements always produce stats containing the number of rows
   * modified, unless executed using the
   * [ExecuteSqlRequest.QueryMode.PLAN][google.spanner.v1.ExecuteSqlRequest.QueryMode.PLAN] [ExecuteSqlRequest.query_mode][google.spanner.v1.ExecuteSqlRequest.query_mode].
   * Other fields may or may not be populated, based on the
   * [ExecuteSqlRequest.query_mode][google.spanner.v1.ExecuteSqlRequest.query_mode].
   */
  stats: ResultSetStats | undefined;
}

/**
 * Partial results from a streaming read or SQL query. Streaming reads and
 * SQL queries better tolerate large result sets, large rows, and large
 * values, but are a little trickier to consume.
 */
export interface PartialResultSet {
  /**
   * Metadata about the result set, such as row type information.
   * Only present in the first response.
   */
  metadata:
    | ResultSetMetadata
    | undefined;
  /**
   * A streamed result set consists of a stream of values, which might
   * be split into many `PartialResultSet` messages to accommodate
   * large rows and/or large values. Every N complete values defines a
   * row, where N is equal to the number of entries in
   * [metadata.row_type.fields][google.spanner.v1.StructType.fields].
   *
   * Most values are encoded based on type as described
   * [here][google.spanner.v1.TypeCode].
   *
   * It is possible that the last value in values is "chunked",
   * meaning that the rest of the value is sent in subsequent
   * `PartialResultSet`(s). This is denoted by the [chunked_value][google.spanner.v1.PartialResultSet.chunked_value]
   * field. Two or more chunked values can be merged to form a
   * complete value as follows:
   *
   *   * `bool/number/null`: cannot be chunked
   *   * `string`: concatenate the strings
   *   * `list`: concatenate the lists. If the last element in a list is a
   *     `string`, `list`, or `object`, merge it with the first element in
   *     the next list by applying these rules recursively.
   *   * `object`: concatenate the (field name, field value) pairs. If a
   *     field name is duplicated, then apply these rules recursively
   *     to merge the field values.
   *
   * Some examples of merging:
   *
   *     # Strings are concatenated.
   *     "foo", "bar" => "foobar"
   *
   *     # Lists of non-strings are concatenated.
   *     [2, 3], [4] => [2, 3, 4]
   *
   *     # Lists are concatenated, but the last and first elements are merged
   *     # because they are strings.
   *     ["a", "b"], ["c", "d"] => ["a", "bc", "d"]
   *
   *     # Lists are concatenated, but the last and first elements are merged
   *     # because they are lists. Recursively, the last and first elements
   *     # of the inner lists are merged because they are strings.
   *     ["a", ["b", "c"]], [["d"], "e"] => ["a", ["b", "cd"], "e"]
   *
   *     # Non-overlapping object fields are combined.
   *     {"a": "1"}, {"b": "2"} => {"a": "1", "b": 2"}
   *
   *     # Overlapping object fields are merged.
   *     {"a": "1"}, {"a": "2"} => {"a": "12"}
   *
   *     # Examples of merging objects containing lists of strings.
   *     {"a": ["1"]}, {"a": ["2"]} => {"a": ["12"]}
   *
   * For a more complete example, suppose a streaming SQL query is
   * yielding a result set whose rows contain a single string
   * field. The following `PartialResultSet`s might be yielded:
   *
   *     {
   *       "metadata": { ... }
   *       "values": ["Hello", "W"]
   *       "chunked_value": true
   *       "resume_token": "Af65..."
   *     }
   *     {
   *       "values": ["orl"]
   *       "chunked_value": true
   *       "resume_token": "Bqp2..."
   *     }
   *     {
   *       "values": ["d"]
   *       "resume_token": "Zx1B..."
   *     }
   *
   * This sequence of `PartialResultSet`s encodes two rows, one
   * containing the field value `"Hello"`, and a second containing the
   * field value `"World" = "W" + "orl" + "d"`.
   */
  values: any[];
  /**
   * If true, then the final value in [values][google.spanner.v1.PartialResultSet.values] is chunked, and must
   * be combined with more values from subsequent `PartialResultSet`s
   * to obtain a complete field value.
   */
  chunkedValue: boolean;
  /**
   * Streaming calls might be interrupted for a variety of reasons, such
   * as TCP connection loss. If this occurs, the stream of results can
   * be resumed by re-sending the original request and including
   * `resume_token`. Note that executing any other transaction in the
   * same session invalidates the token.
   */
  resumeToken: Buffer;
  /**
   * Query plan and execution statistics for the statement that produced this
   * streaming result set. These can be requested by setting
   * [ExecuteSqlRequest.query_mode][google.spanner.v1.ExecuteSqlRequest.query_mode] and are sent
   * only once with the last response in the stream.
   * This field will also be present in the last response for DML
   * statements.
   */
  stats: ResultSetStats | undefined;
}

/** Metadata about a [ResultSet][google.spanner.v1.ResultSet] or [PartialResultSet][google.spanner.v1.PartialResultSet]. */
export interface ResultSetMetadata {
  /**
   * Indicates the field names and types for the rows in the result
   * set.  For example, a SQL query like `"SELECT UserId, UserName FROM
   * Users"` could return a `row_type` value like:
   *
   *     "fields": [
   *       { "name": "UserId", "type": { "code": "INT64" } },
   *       { "name": "UserName", "type": { "code": "STRING" } },
   *     ]
   */
  rowType:
    | StructType
    | undefined;
  /**
   * If the read or SQL query began a transaction as a side-effect, the
   * information about the new transaction is yielded here.
   */
  transaction:
    | Transaction
    | undefined;
  /**
   * A SQL query can be parameterized. In PLAN mode, these parameters can be
   * undeclared. This indicates the field names and types for those undeclared
   * parameters in the SQL query. For example, a SQL query like `"SELECT * FROM
   * Users where UserId = @userId and UserName = @userName "` could return a
   * `undeclared_parameters` value like:
   *
   *     "fields": [
   *       { "name": "UserId", "type": { "code": "INT64" } },
   *       { "name": "UserName", "type": { "code": "STRING" } },
   *     ]
   */
  undeclaredParameters: StructType | undefined;
}

/** Additional statistics about a [ResultSet][google.spanner.v1.ResultSet] or [PartialResultSet][google.spanner.v1.PartialResultSet]. */
export interface ResultSetStats {
  /** [QueryPlan][google.spanner.v1.QueryPlan] for the query associated with this result. */
  queryPlan:
    | QueryPlan
    | undefined;
  /**
   * Aggregated statistics from the execution of the query. Only present when
   * the query is profiled. For example, a query could return the statistics as
   * follows:
   *
   *     {
   *       "rows_returned": "3",
   *       "elapsed_time": "1.22 secs",
   *       "cpu_time": "1.19 secs"
   *     }
   */
  queryStats:
    | { [key: string]: any }
    | undefined;
  /** Standard DML returns an exact count of rows that were modified. */
  rowCountExact?:
    | Long
    | undefined;
  /**
   * Partitioned DML does not offer exactly-once semantics, so it
   * returns a lower bound of the rows modified.
   */
  rowCountLowerBound?: Long | undefined;
}

function createBaseResultSet(): ResultSet {
  return { metadata: undefined, rows: [], stats: undefined };
}

export const ResultSet: MessageFns<ResultSet> = {
  encode(message: ResultSet, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.metadata !== undefined) {
      ResultSetMetadata.encode(message.metadata, writer.uint32(10).fork()).join();
    }
    for (const v of message.rows) {
      ListValue.encode(ListValue.wrap(v!), writer.uint32(18).fork()).join();
    }
    if (message.stats !== undefined) {
      ResultSetStats.encode(message.stats, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ResultSet {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseResultSet();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.metadata = ResultSetMetadata.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.rows.push(ListValue.unwrap(ListValue.decode(reader, reader.uint32())));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.stats = ResultSetStats.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ResultSet {
    return {
      metadata: isSet(object.metadata) ? ResultSetMetadata.fromJSON(object.metadata) : undefined,
      rows: globalThis.Array.isArray(object?.rows) ? object.rows.map((e: any) => [...e]) : [],
      stats: isSet(object.stats) ? ResultSetStats.fromJSON(object.stats) : undefined,
    };
  },

  toJSON(message: ResultSet): unknown {
    const obj: any = {};
    if (message.metadata !== undefined) {
      obj.metadata = ResultSetMetadata.toJSON(message.metadata);
    }
    if (message.rows?.length) {
      obj.rows = message.rows;
    }
    if (message.stats !== undefined) {
      obj.stats = ResultSetStats.toJSON(message.stats);
    }
    return obj;
  },

  create(base?: DeepPartial<ResultSet>): ResultSet {
    return ResultSet.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ResultSet>): ResultSet {
    const message = createBaseResultSet();
    message.metadata = (object.metadata !== undefined && object.metadata !== null)
      ? ResultSetMetadata.fromPartial(object.metadata)
      : undefined;
    message.rows = object.rows?.map((e) => e) || [];
    message.stats = (object.stats !== undefined && object.stats !== null)
      ? ResultSetStats.fromPartial(object.stats)
      : undefined;
    return message;
  },
};

function createBasePartialResultSet(): PartialResultSet {
  return { metadata: undefined, values: [], chunkedValue: false, resumeToken: Buffer.alloc(0), stats: undefined };
}

export const PartialResultSet: MessageFns<PartialResultSet> = {
  encode(message: PartialResultSet, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.metadata !== undefined) {
      ResultSetMetadata.encode(message.metadata, writer.uint32(10).fork()).join();
    }
    for (const v of message.values) {
      Value.encode(Value.wrap(v!), writer.uint32(18).fork()).join();
    }
    if (message.chunkedValue !== false) {
      writer.uint32(24).bool(message.chunkedValue);
    }
    if (message.resumeToken.length !== 0) {
      writer.uint32(34).bytes(message.resumeToken);
    }
    if (message.stats !== undefined) {
      ResultSetStats.encode(message.stats, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PartialResultSet {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePartialResultSet();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.metadata = ResultSetMetadata.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.values.push(Value.unwrap(Value.decode(reader, reader.uint32())));
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.chunkedValue = reader.bool();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.resumeToken = Buffer.from(reader.bytes());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.stats = ResultSetStats.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PartialResultSet {
    return {
      metadata: isSet(object.metadata) ? ResultSetMetadata.fromJSON(object.metadata) : undefined,
      values: globalThis.Array.isArray(object?.values) ? [...object.values] : [],
      chunkedValue: isSet(object.chunkedValue) ? globalThis.Boolean(object.chunkedValue) : false,
      resumeToken: isSet(object.resumeToken) ? Buffer.from(bytesFromBase64(object.resumeToken)) : Buffer.alloc(0),
      stats: isSet(object.stats) ? ResultSetStats.fromJSON(object.stats) : undefined,
    };
  },

  toJSON(message: PartialResultSet): unknown {
    const obj: any = {};
    if (message.metadata !== undefined) {
      obj.metadata = ResultSetMetadata.toJSON(message.metadata);
    }
    if (message.values?.length) {
      obj.values = message.values;
    }
    if (message.chunkedValue !== false) {
      obj.chunkedValue = message.chunkedValue;
    }
    if (message.resumeToken.length !== 0) {
      obj.resumeToken = base64FromBytes(message.resumeToken);
    }
    if (message.stats !== undefined) {
      obj.stats = ResultSetStats.toJSON(message.stats);
    }
    return obj;
  },

  create(base?: DeepPartial<PartialResultSet>): PartialResultSet {
    return PartialResultSet.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<PartialResultSet>): PartialResultSet {
    const message = createBasePartialResultSet();
    message.metadata = (object.metadata !== undefined && object.metadata !== null)
      ? ResultSetMetadata.fromPartial(object.metadata)
      : undefined;
    message.values = object.values?.map((e) => e) || [];
    message.chunkedValue = object.chunkedValue ?? false;
    message.resumeToken = object.resumeToken ?? Buffer.alloc(0);
    message.stats = (object.stats !== undefined && object.stats !== null)
      ? ResultSetStats.fromPartial(object.stats)
      : undefined;
    return message;
  },
};

function createBaseResultSetMetadata(): ResultSetMetadata {
  return { rowType: undefined, transaction: undefined, undeclaredParameters: undefined };
}

export const ResultSetMetadata: MessageFns<ResultSetMetadata> = {
  encode(message: ResultSetMetadata, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.rowType !== undefined) {
      StructType.encode(message.rowType, writer.uint32(10).fork()).join();
    }
    if (message.transaction !== undefined) {
      Transaction.encode(message.transaction, writer.uint32(18).fork()).join();
    }
    if (message.undeclaredParameters !== undefined) {
      StructType.encode(message.undeclaredParameters, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ResultSetMetadata {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseResultSetMetadata();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.rowType = StructType.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.transaction = Transaction.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.undeclaredParameters = StructType.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ResultSetMetadata {
    return {
      rowType: isSet(object.rowType) ? StructType.fromJSON(object.rowType) : undefined,
      transaction: isSet(object.transaction) ? Transaction.fromJSON(object.transaction) : undefined,
      undeclaredParameters: isSet(object.undeclaredParameters)
        ? StructType.fromJSON(object.undeclaredParameters)
        : undefined,
    };
  },

  toJSON(message: ResultSetMetadata): unknown {
    const obj: any = {};
    if (message.rowType !== undefined) {
      obj.rowType = StructType.toJSON(message.rowType);
    }
    if (message.transaction !== undefined) {
      obj.transaction = Transaction.toJSON(message.transaction);
    }
    if (message.undeclaredParameters !== undefined) {
      obj.undeclaredParameters = StructType.toJSON(message.undeclaredParameters);
    }
    return obj;
  },

  create(base?: DeepPartial<ResultSetMetadata>): ResultSetMetadata {
    return ResultSetMetadata.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ResultSetMetadata>): ResultSetMetadata {
    const message = createBaseResultSetMetadata();
    message.rowType = (object.rowType !== undefined && object.rowType !== null)
      ? StructType.fromPartial(object.rowType)
      : undefined;
    message.transaction = (object.transaction !== undefined && object.transaction !== null)
      ? Transaction.fromPartial(object.transaction)
      : undefined;
    message.undeclaredParameters = (object.undeclaredParameters !== undefined && object.undeclaredParameters !== null)
      ? StructType.fromPartial(object.undeclaredParameters)
      : undefined;
    return message;
  },
};

function createBaseResultSetStats(): ResultSetStats {
  return { queryPlan: undefined, queryStats: undefined, rowCountExact: undefined, rowCountLowerBound: undefined };
}

export const ResultSetStats: MessageFns<ResultSetStats> = {
  encode(message: ResultSetStats, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.queryPlan !== undefined) {
      QueryPlan.encode(message.queryPlan, writer.uint32(10).fork()).join();
    }
    if (message.queryStats !== undefined) {
      Struct.encode(Struct.wrap(message.queryStats), writer.uint32(18).fork()).join();
    }
    if (message.rowCountExact !== undefined) {
      writer.uint32(24).int64(message.rowCountExact.toString());
    }
    if (message.rowCountLowerBound !== undefined) {
      writer.uint32(32).int64(message.rowCountLowerBound.toString());
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ResultSetStats {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseResultSetStats();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.queryPlan = QueryPlan.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.queryStats = Struct.unwrap(Struct.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.rowCountExact = Long.fromString(reader.int64().toString());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.rowCountLowerBound = Long.fromString(reader.int64().toString());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ResultSetStats {
    return {
      queryPlan: isSet(object.queryPlan) ? QueryPlan.fromJSON(object.queryPlan) : undefined,
      queryStats: isObject(object.queryStats) ? object.queryStats : undefined,
      rowCountExact: isSet(object.rowCountExact) ? Long.fromValue(object.rowCountExact) : undefined,
      rowCountLowerBound: isSet(object.rowCountLowerBound) ? Long.fromValue(object.rowCountLowerBound) : undefined,
    };
  },

  toJSON(message: ResultSetStats): unknown {
    const obj: any = {};
    if (message.queryPlan !== undefined) {
      obj.queryPlan = QueryPlan.toJSON(message.queryPlan);
    }
    if (message.queryStats !== undefined) {
      obj.queryStats = message.queryStats;
    }
    if (message.rowCountExact !== undefined) {
      obj.rowCountExact = (message.rowCountExact || Long.ZERO).toString();
    }
    if (message.rowCountLowerBound !== undefined) {
      obj.rowCountLowerBound = (message.rowCountLowerBound || Long.ZERO).toString();
    }
    return obj;
  },

  create(base?: DeepPartial<ResultSetStats>): ResultSetStats {
    return ResultSetStats.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ResultSetStats>): ResultSetStats {
    const message = createBaseResultSetStats();
    message.queryPlan = (object.queryPlan !== undefined && object.queryPlan !== null)
      ? QueryPlan.fromPartial(object.queryPlan)
      : undefined;
    message.queryStats = object.queryStats ?? undefined;
    message.rowCountExact = (object.rowCountExact !== undefined && object.rowCountExact !== null)
      ? Long.fromValue(object.rowCountExact)
      : undefined;
    message.rowCountLowerBound = (object.rowCountLowerBound !== undefined && object.rowCountLowerBound !== null)
      ? Long.fromValue(object.rowCountLowerBound)
      : undefined;
    return message;
  },
};

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
