// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/assistant/embedded/v1alpha1/embedded_assistant.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { Status } from "../../../rpc/status.js";

export const protobufPackage = "google.assistant.embedded.v1alpha1";

/** Specifies how to process the `ConverseRequest` messages. */
export interface ConverseConfig {
  /** Required* Specifies how to process the subsequent incoming audio. */
  audioInConfig:
    | AudioInConfig
    | undefined;
  /** Required* Specifies how to format the audio that will be returned. */
  audioOutConfig:
    | AudioOutConfig
    | undefined;
  /** Required* Represents the current dialog state. */
  converseState: ConverseState | undefined;
}

/**
 * Specifies how to process the `audio_in` data that will be provided in
 * subsequent requests. For recommended settings, see the Google Assistant SDK
 * [best
 * practices](https://developers.google.com/assistant/sdk/develop/grpc/best-practices/audio).
 */
export interface AudioInConfig {
  /** Required* Encoding of audio data sent in all `audio_in` messages. */
  encoding: AudioInConfig_Encoding;
  /**
   * Required* Sample rate (in Hertz) of the audio data sent in all `audio_in`
   * messages. Valid values are from 16000-24000, but 16000 is optimal.
   * For best results, set the sampling rate of the audio source to 16000 Hz.
   * If that's not possible, use the native sample rate of the audio source
   * (instead of re-sampling).
   */
  sampleRateHertz: number;
}

/**
 * Audio encoding of the data sent in the audio message.
 * Audio must be one-channel (mono). The only language supported is "en-US".
 */
export enum AudioInConfig_Encoding {
  /** ENCODING_UNSPECIFIED - Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][]. */
  ENCODING_UNSPECIFIED = 0,
  /**
   * LINEAR16 - Uncompressed 16-bit signed little-endian samples (Linear PCM).
   * This encoding includes no header, only the raw audio bytes.
   */
  LINEAR16 = 1,
  /**
   * FLAC - [`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
   * Codec) is the recommended encoding because it is
   * lossless--therefore recognition is not compromised--and
   * requires only about half the bandwidth of `LINEAR16`. This encoding
   * includes the `FLAC` stream header followed by audio data. It supports
   * 16-bit and 24-bit samples, however, not all fields in `STREAMINFO` are
   * supported.
   */
  FLAC = 2,
  UNRECOGNIZED = -1,
}

export function audioInConfig_EncodingFromJSON(object: any): AudioInConfig_Encoding {
  switch (object) {
    case 0:
    case "ENCODING_UNSPECIFIED":
      return AudioInConfig_Encoding.ENCODING_UNSPECIFIED;
    case 1:
    case "LINEAR16":
      return AudioInConfig_Encoding.LINEAR16;
    case 2:
    case "FLAC":
      return AudioInConfig_Encoding.FLAC;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AudioInConfig_Encoding.UNRECOGNIZED;
  }
}

export function audioInConfig_EncodingToJSON(object: AudioInConfig_Encoding): string {
  switch (object) {
    case AudioInConfig_Encoding.ENCODING_UNSPECIFIED:
      return "ENCODING_UNSPECIFIED";
    case AudioInConfig_Encoding.LINEAR16:
      return "LINEAR16";
    case AudioInConfig_Encoding.FLAC:
      return "FLAC";
    case AudioInConfig_Encoding.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Specifies the desired format for the server to use when it returns
 * `audio_out` messages.
 */
export interface AudioOutConfig {
  /**
   * Required* The encoding of audio data to be returned in all `audio_out`
   * messages.
   */
  encoding: AudioOutConfig_Encoding;
  /**
   * Required* The sample rate in Hertz of the audio data returned in
   * `audio_out` messages. Valid values are: 16000-24000.
   */
  sampleRateHertz: number;
  /**
   * Required* Current volume setting of the device's audio output.
   * Valid values are 1 to 100 (corresponding to 1% to 100%).
   */
  volumePercentage: number;
}

/**
 * Audio encoding of the data returned in the audio message. All encodings are
 * raw audio bytes with no header, except as indicated below.
 */
export enum AudioOutConfig_Encoding {
  /** ENCODING_UNSPECIFIED - Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][]. */
  ENCODING_UNSPECIFIED = 0,
  /** LINEAR16 - Uncompressed 16-bit signed little-endian samples (Linear PCM). */
  LINEAR16 = 1,
  /** MP3 - MP3 audio encoding. The sample rate is encoded in the payload. */
  MP3 = 2,
  /**
   * OPUS_IN_OGG - Opus-encoded audio wrapped in an ogg container. The result will be a
   * file which can be played natively on Android and in some browsers (such
   * as Chrome). The quality of the encoding is considerably higher than MP3
   * while using the same bitrate. The sample rate is encoded in the payload.
   */
  OPUS_IN_OGG = 3,
  UNRECOGNIZED = -1,
}

export function audioOutConfig_EncodingFromJSON(object: any): AudioOutConfig_Encoding {
  switch (object) {
    case 0:
    case "ENCODING_UNSPECIFIED":
      return AudioOutConfig_Encoding.ENCODING_UNSPECIFIED;
    case 1:
    case "LINEAR16":
      return AudioOutConfig_Encoding.LINEAR16;
    case 2:
    case "MP3":
      return AudioOutConfig_Encoding.MP3;
    case 3:
    case "OPUS_IN_OGG":
      return AudioOutConfig_Encoding.OPUS_IN_OGG;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AudioOutConfig_Encoding.UNRECOGNIZED;
  }
}

export function audioOutConfig_EncodingToJSON(object: AudioOutConfig_Encoding): string {
  switch (object) {
    case AudioOutConfig_Encoding.ENCODING_UNSPECIFIED:
      return "ENCODING_UNSPECIFIED";
    case AudioOutConfig_Encoding.LINEAR16:
      return "LINEAR16";
    case AudioOutConfig_Encoding.MP3:
      return "MP3";
    case AudioOutConfig_Encoding.OPUS_IN_OGG:
      return "OPUS_IN_OGG";
    case AudioOutConfig_Encoding.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Provides information about the current dialog state. */
export interface ConverseState {
  /**
   * Required* The `conversation_state` value returned in the prior
   * `ConverseResponse`. Omit (do not set the field) if there was no prior
   * `ConverseResponse`. If there was a prior `ConverseResponse`, do not omit
   * this field; doing so will end that conversation (and this new request will
   * start a new conversation).
   */
  conversationState: Buffer;
}

/**
 * The audio containing the assistant's response to the query. Sequential chunks
 * of audio data are received in sequential `ConverseResponse` messages.
 */
export interface AudioOut {
  /**
   * Output-only* The audio data containing the assistant's response to the
   * query. Sequential chunks of audio data are received in sequential
   * `ConverseResponse` messages.
   */
  audioData: Buffer;
}

/** The semantic result for the user's spoken query. */
export interface ConverseResult {
  /** Output-only* The recognized transcript of what the user said. */
  spokenRequestText: string;
  /**
   * Output-only* The text of the assistant's spoken response. This is only
   * returned for an IFTTT action.
   */
  spokenResponseText: string;
  /**
   * Output-only* State information for subsequent `ConverseRequest`. This
   * value should be saved in the client and returned in the
   * `conversation_state` with the next `ConverseRequest`. (The client does not
   * need to interpret or otherwise use this value.) There is no need to save
   * this information across device restarts.
   */
  conversationState: Buffer;
  /**
   * Output-only* Specifies the mode of the microphone after this `Converse`
   * RPC is processed.
   */
  microphoneMode: ConverseResult_MicrophoneMode;
  /**
   * Output-only* Updated volume level. The value will be 0 or omitted
   * (indicating no change) unless a voice command such as "Increase the volume"
   * or "Set volume level 4" was recognized, in which case the value will be
   * between 1 and 100 (corresponding to the new volume level of 1% to 100%).
   * Typically, a client should use this volume level when playing the
   * `audio_out` data, and retain this value as the current volume level and
   * supply it in the `AudioOutConfig` of the next `ConverseRequest`. (Some
   * clients may also implement other ways to allow the current volume level to
   * be changed, for example, by providing a knob that the user can turn.)
   */
  volumePercentage: number;
}

/** Possible states of the microphone after a `Converse` RPC completes. */
export enum ConverseResult_MicrophoneMode {
  /** MICROPHONE_MODE_UNSPECIFIED - No mode specified. */
  MICROPHONE_MODE_UNSPECIFIED = 0,
  /**
   * CLOSE_MICROPHONE - The service is not expecting a follow-on question from the user.
   * The microphone should remain off until the user re-activates it.
   */
  CLOSE_MICROPHONE = 1,
  /**
   * DIALOG_FOLLOW_ON - The service is expecting a follow-on question from the user. The
   * microphone should be re-opened when the `AudioOut` playback completes
   * (by starting a new `Converse` RPC call to send the new audio).
   */
  DIALOG_FOLLOW_ON = 2,
  UNRECOGNIZED = -1,
}

export function converseResult_MicrophoneModeFromJSON(object: any): ConverseResult_MicrophoneMode {
  switch (object) {
    case 0:
    case "MICROPHONE_MODE_UNSPECIFIED":
      return ConverseResult_MicrophoneMode.MICROPHONE_MODE_UNSPECIFIED;
    case 1:
    case "CLOSE_MICROPHONE":
      return ConverseResult_MicrophoneMode.CLOSE_MICROPHONE;
    case 2:
    case "DIALOG_FOLLOW_ON":
      return ConverseResult_MicrophoneMode.DIALOG_FOLLOW_ON;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ConverseResult_MicrophoneMode.UNRECOGNIZED;
  }
}

export function converseResult_MicrophoneModeToJSON(object: ConverseResult_MicrophoneMode): string {
  switch (object) {
    case ConverseResult_MicrophoneMode.MICROPHONE_MODE_UNSPECIFIED:
      return "MICROPHONE_MODE_UNSPECIFIED";
    case ConverseResult_MicrophoneMode.CLOSE_MICROPHONE:
      return "CLOSE_MICROPHONE";
    case ConverseResult_MicrophoneMode.DIALOG_FOLLOW_ON:
      return "DIALOG_FOLLOW_ON";
    case ConverseResult_MicrophoneMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * The top-level message sent by the client. Clients must send at least two, and
 * typically numerous `ConverseRequest` messages. The first message must
 * contain a `config` message and must not contain `audio_in` data. All
 * subsequent messages must contain `audio_in` data and must not contain a
 * `config` message.
 */
export interface ConverseRequest {
  /**
   * The `config` message provides information to the recognizer that
   * specifies how to process the request.
   * The first `ConverseRequest` message must contain a `config` message.
   */
  config?:
    | ConverseConfig
    | undefined;
  /**
   * The audio data to be recognized. Sequential chunks of audio data are sent
   * in sequential `ConverseRequest` messages. The first `ConverseRequest`
   * message must not contain `audio_in` data and all subsequent
   * `ConverseRequest` messages must contain `audio_in` data. The audio bytes
   * must be encoded as specified in `AudioInConfig`.
   * Audio must be sent at approximately real-time (16000 samples per second).
   * An error will be returned if audio is sent significantly faster or
   * slower.
   */
  audioIn?: Buffer | undefined;
}

/**
 * The top-level message received by the client. A series of one or more
 * `ConverseResponse` messages are streamed back to the client.
 */
export interface ConverseResponse {
  /**
   * Output-only* If set, returns a [google.rpc.Status][google.rpc.Status]
   * message that specifies the error for the operation. If an error occurs
   * during processing, this message will be set and there will be no further
   * messages sent.
   */
  error?:
    | Status
    | undefined;
  /** Output-only* Indicates the type of event. */
  eventType?:
    | ConverseResponse_EventType
    | undefined;
  /** Output-only* The audio containing the assistant's response to the query. */
  audioOut?:
    | AudioOut
    | undefined;
  /** Output-only* The semantic result for the user's spoken query. */
  result?: ConverseResult | undefined;
}

/** Indicates the type of event. */
export enum ConverseResponse_EventType {
  /** EVENT_TYPE_UNSPECIFIED - No event specified. */
  EVENT_TYPE_UNSPECIFIED = 0,
  /**
   * END_OF_UTTERANCE - This event indicates that the server has detected the end of the user's
   * speech utterance and expects no additional speech. Therefore, the server
   * will not process additional audio (although it may subsequently return
   * additional results). The client should stop sending additional audio
   * data, half-close the gRPC connection, and wait for any additional results
   * until the server closes the gRPC connection.
   */
  END_OF_UTTERANCE = 1,
  UNRECOGNIZED = -1,
}

export function converseResponse_EventTypeFromJSON(object: any): ConverseResponse_EventType {
  switch (object) {
    case 0:
    case "EVENT_TYPE_UNSPECIFIED":
      return ConverseResponse_EventType.EVENT_TYPE_UNSPECIFIED;
    case 1:
    case "END_OF_UTTERANCE":
      return ConverseResponse_EventType.END_OF_UTTERANCE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ConverseResponse_EventType.UNRECOGNIZED;
  }
}

export function converseResponse_EventTypeToJSON(object: ConverseResponse_EventType): string {
  switch (object) {
    case ConverseResponse_EventType.EVENT_TYPE_UNSPECIFIED:
      return "EVENT_TYPE_UNSPECIFIED";
    case ConverseResponse_EventType.END_OF_UTTERANCE:
      return "END_OF_UTTERANCE";
    case ConverseResponse_EventType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

function createBaseConverseConfig(): ConverseConfig {
  return { audioInConfig: undefined, audioOutConfig: undefined, converseState: undefined };
}

export const ConverseConfig: MessageFns<ConverseConfig> = {
  encode(message: ConverseConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.audioInConfig !== undefined) {
      AudioInConfig.encode(message.audioInConfig, writer.uint32(10).fork()).join();
    }
    if (message.audioOutConfig !== undefined) {
      AudioOutConfig.encode(message.audioOutConfig, writer.uint32(18).fork()).join();
    }
    if (message.converseState !== undefined) {
      ConverseState.encode(message.converseState, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ConverseConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseConverseConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.audioInConfig = AudioInConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.audioOutConfig = AudioOutConfig.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.converseState = ConverseState.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ConverseConfig {
    return {
      audioInConfig: isSet(object.audioInConfig) ? AudioInConfig.fromJSON(object.audioInConfig) : undefined,
      audioOutConfig: isSet(object.audioOutConfig) ? AudioOutConfig.fromJSON(object.audioOutConfig) : undefined,
      converseState: isSet(object.converseState) ? ConverseState.fromJSON(object.converseState) : undefined,
    };
  },

  toJSON(message: ConverseConfig): unknown {
    const obj: any = {};
    if (message.audioInConfig !== undefined) {
      obj.audioInConfig = AudioInConfig.toJSON(message.audioInConfig);
    }
    if (message.audioOutConfig !== undefined) {
      obj.audioOutConfig = AudioOutConfig.toJSON(message.audioOutConfig);
    }
    if (message.converseState !== undefined) {
      obj.converseState = ConverseState.toJSON(message.converseState);
    }
    return obj;
  },

  create(base?: DeepPartial<ConverseConfig>): ConverseConfig {
    return ConverseConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ConverseConfig>): ConverseConfig {
    const message = createBaseConverseConfig();
    message.audioInConfig = (object.audioInConfig !== undefined && object.audioInConfig !== null)
      ? AudioInConfig.fromPartial(object.audioInConfig)
      : undefined;
    message.audioOutConfig = (object.audioOutConfig !== undefined && object.audioOutConfig !== null)
      ? AudioOutConfig.fromPartial(object.audioOutConfig)
      : undefined;
    message.converseState = (object.converseState !== undefined && object.converseState !== null)
      ? ConverseState.fromPartial(object.converseState)
      : undefined;
    return message;
  },
};

function createBaseAudioInConfig(): AudioInConfig {
  return { encoding: 0, sampleRateHertz: 0 };
}

export const AudioInConfig: MessageFns<AudioInConfig> = {
  encode(message: AudioInConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.encoding !== 0) {
      writer.uint32(8).int32(message.encoding);
    }
    if (message.sampleRateHertz !== 0) {
      writer.uint32(16).int32(message.sampleRateHertz);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AudioInConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAudioInConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.encoding = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.sampleRateHertz = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AudioInConfig {
    return {
      encoding: isSet(object.encoding) ? audioInConfig_EncodingFromJSON(object.encoding) : 0,
      sampleRateHertz: isSet(object.sampleRateHertz) ? globalThis.Number(object.sampleRateHertz) : 0,
    };
  },

  toJSON(message: AudioInConfig): unknown {
    const obj: any = {};
    if (message.encoding !== 0) {
      obj.encoding = audioInConfig_EncodingToJSON(message.encoding);
    }
    if (message.sampleRateHertz !== 0) {
      obj.sampleRateHertz = Math.round(message.sampleRateHertz);
    }
    return obj;
  },

  create(base?: DeepPartial<AudioInConfig>): AudioInConfig {
    return AudioInConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AudioInConfig>): AudioInConfig {
    const message = createBaseAudioInConfig();
    message.encoding = object.encoding ?? 0;
    message.sampleRateHertz = object.sampleRateHertz ?? 0;
    return message;
  },
};

function createBaseAudioOutConfig(): AudioOutConfig {
  return { encoding: 0, sampleRateHertz: 0, volumePercentage: 0 };
}

export const AudioOutConfig: MessageFns<AudioOutConfig> = {
  encode(message: AudioOutConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.encoding !== 0) {
      writer.uint32(8).int32(message.encoding);
    }
    if (message.sampleRateHertz !== 0) {
      writer.uint32(16).int32(message.sampleRateHertz);
    }
    if (message.volumePercentage !== 0) {
      writer.uint32(24).int32(message.volumePercentage);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AudioOutConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAudioOutConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.encoding = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.sampleRateHertz = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.volumePercentage = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AudioOutConfig {
    return {
      encoding: isSet(object.encoding) ? audioOutConfig_EncodingFromJSON(object.encoding) : 0,
      sampleRateHertz: isSet(object.sampleRateHertz) ? globalThis.Number(object.sampleRateHertz) : 0,
      volumePercentage: isSet(object.volumePercentage) ? globalThis.Number(object.volumePercentage) : 0,
    };
  },

  toJSON(message: AudioOutConfig): unknown {
    const obj: any = {};
    if (message.encoding !== 0) {
      obj.encoding = audioOutConfig_EncodingToJSON(message.encoding);
    }
    if (message.sampleRateHertz !== 0) {
      obj.sampleRateHertz = Math.round(message.sampleRateHertz);
    }
    if (message.volumePercentage !== 0) {
      obj.volumePercentage = Math.round(message.volumePercentage);
    }
    return obj;
  },

  create(base?: DeepPartial<AudioOutConfig>): AudioOutConfig {
    return AudioOutConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AudioOutConfig>): AudioOutConfig {
    const message = createBaseAudioOutConfig();
    message.encoding = object.encoding ?? 0;
    message.sampleRateHertz = object.sampleRateHertz ?? 0;
    message.volumePercentage = object.volumePercentage ?? 0;
    return message;
  },
};

function createBaseConverseState(): ConverseState {
  return { conversationState: Buffer.alloc(0) };
}

export const ConverseState: MessageFns<ConverseState> = {
  encode(message: ConverseState, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.conversationState.length !== 0) {
      writer.uint32(10).bytes(message.conversationState);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ConverseState {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseConverseState();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.conversationState = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ConverseState {
    return {
      conversationState: isSet(object.conversationState)
        ? Buffer.from(bytesFromBase64(object.conversationState))
        : Buffer.alloc(0),
    };
  },

  toJSON(message: ConverseState): unknown {
    const obj: any = {};
    if (message.conversationState.length !== 0) {
      obj.conversationState = base64FromBytes(message.conversationState);
    }
    return obj;
  },

  create(base?: DeepPartial<ConverseState>): ConverseState {
    return ConverseState.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ConverseState>): ConverseState {
    const message = createBaseConverseState();
    message.conversationState = object.conversationState ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseAudioOut(): AudioOut {
  return { audioData: Buffer.alloc(0) };
}

export const AudioOut: MessageFns<AudioOut> = {
  encode(message: AudioOut, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.audioData.length !== 0) {
      writer.uint32(10).bytes(message.audioData);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AudioOut {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAudioOut();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.audioData = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AudioOut {
    return { audioData: isSet(object.audioData) ? Buffer.from(bytesFromBase64(object.audioData)) : Buffer.alloc(0) };
  },

  toJSON(message: AudioOut): unknown {
    const obj: any = {};
    if (message.audioData.length !== 0) {
      obj.audioData = base64FromBytes(message.audioData);
    }
    return obj;
  },

  create(base?: DeepPartial<AudioOut>): AudioOut {
    return AudioOut.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AudioOut>): AudioOut {
    const message = createBaseAudioOut();
    message.audioData = object.audioData ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseConverseResult(): ConverseResult {
  return {
    spokenRequestText: "",
    spokenResponseText: "",
    conversationState: Buffer.alloc(0),
    microphoneMode: 0,
    volumePercentage: 0,
  };
}

export const ConverseResult: MessageFns<ConverseResult> = {
  encode(message: ConverseResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.spokenRequestText !== "") {
      writer.uint32(10).string(message.spokenRequestText);
    }
    if (message.spokenResponseText !== "") {
      writer.uint32(18).string(message.spokenResponseText);
    }
    if (message.conversationState.length !== 0) {
      writer.uint32(26).bytes(message.conversationState);
    }
    if (message.microphoneMode !== 0) {
      writer.uint32(32).int32(message.microphoneMode);
    }
    if (message.volumePercentage !== 0) {
      writer.uint32(40).int32(message.volumePercentage);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ConverseResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseConverseResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.spokenRequestText = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.spokenResponseText = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.conversationState = Buffer.from(reader.bytes());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.microphoneMode = reader.int32() as any;
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.volumePercentage = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ConverseResult {
    return {
      spokenRequestText: isSet(object.spokenRequestText) ? globalThis.String(object.spokenRequestText) : "",
      spokenResponseText: isSet(object.spokenResponseText) ? globalThis.String(object.spokenResponseText) : "",
      conversationState: isSet(object.conversationState)
        ? Buffer.from(bytesFromBase64(object.conversationState))
        : Buffer.alloc(0),
      microphoneMode: isSet(object.microphoneMode) ? converseResult_MicrophoneModeFromJSON(object.microphoneMode) : 0,
      volumePercentage: isSet(object.volumePercentage) ? globalThis.Number(object.volumePercentage) : 0,
    };
  },

  toJSON(message: ConverseResult): unknown {
    const obj: any = {};
    if (message.spokenRequestText !== "") {
      obj.spokenRequestText = message.spokenRequestText;
    }
    if (message.spokenResponseText !== "") {
      obj.spokenResponseText = message.spokenResponseText;
    }
    if (message.conversationState.length !== 0) {
      obj.conversationState = base64FromBytes(message.conversationState);
    }
    if (message.microphoneMode !== 0) {
      obj.microphoneMode = converseResult_MicrophoneModeToJSON(message.microphoneMode);
    }
    if (message.volumePercentage !== 0) {
      obj.volumePercentage = Math.round(message.volumePercentage);
    }
    return obj;
  },

  create(base?: DeepPartial<ConverseResult>): ConverseResult {
    return ConverseResult.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ConverseResult>): ConverseResult {
    const message = createBaseConverseResult();
    message.spokenRequestText = object.spokenRequestText ?? "";
    message.spokenResponseText = object.spokenResponseText ?? "";
    message.conversationState = object.conversationState ?? Buffer.alloc(0);
    message.microphoneMode = object.microphoneMode ?? 0;
    message.volumePercentage = object.volumePercentage ?? 0;
    return message;
  },
};

function createBaseConverseRequest(): ConverseRequest {
  return { config: undefined, audioIn: undefined };
}

export const ConverseRequest: MessageFns<ConverseRequest> = {
  encode(message: ConverseRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.config !== undefined) {
      ConverseConfig.encode(message.config, writer.uint32(10).fork()).join();
    }
    if (message.audioIn !== undefined) {
      writer.uint32(18).bytes(message.audioIn);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ConverseRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseConverseRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.config = ConverseConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.audioIn = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ConverseRequest {
    return {
      config: isSet(object.config) ? ConverseConfig.fromJSON(object.config) : undefined,
      audioIn: isSet(object.audioIn) ? Buffer.from(bytesFromBase64(object.audioIn)) : undefined,
    };
  },

  toJSON(message: ConverseRequest): unknown {
    const obj: any = {};
    if (message.config !== undefined) {
      obj.config = ConverseConfig.toJSON(message.config);
    }
    if (message.audioIn !== undefined) {
      obj.audioIn = base64FromBytes(message.audioIn);
    }
    return obj;
  },

  create(base?: DeepPartial<ConverseRequest>): ConverseRequest {
    return ConverseRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ConverseRequest>): ConverseRequest {
    const message = createBaseConverseRequest();
    message.config = (object.config !== undefined && object.config !== null)
      ? ConverseConfig.fromPartial(object.config)
      : undefined;
    message.audioIn = object.audioIn ?? undefined;
    return message;
  },
};

function createBaseConverseResponse(): ConverseResponse {
  return { error: undefined, eventType: undefined, audioOut: undefined, result: undefined };
}

export const ConverseResponse: MessageFns<ConverseResponse> = {
  encode(message: ConverseResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.error !== undefined) {
      Status.encode(message.error, writer.uint32(10).fork()).join();
    }
    if (message.eventType !== undefined) {
      writer.uint32(16).int32(message.eventType);
    }
    if (message.audioOut !== undefined) {
      AudioOut.encode(message.audioOut, writer.uint32(26).fork()).join();
    }
    if (message.result !== undefined) {
      ConverseResult.encode(message.result, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ConverseResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseConverseResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.error = Status.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.eventType = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.audioOut = AudioOut.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.result = ConverseResult.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ConverseResponse {
    return {
      error: isSet(object.error) ? Status.fromJSON(object.error) : undefined,
      eventType: isSet(object.eventType) ? converseResponse_EventTypeFromJSON(object.eventType) : undefined,
      audioOut: isSet(object.audioOut) ? AudioOut.fromJSON(object.audioOut) : undefined,
      result: isSet(object.result) ? ConverseResult.fromJSON(object.result) : undefined,
    };
  },

  toJSON(message: ConverseResponse): unknown {
    const obj: any = {};
    if (message.error !== undefined) {
      obj.error = Status.toJSON(message.error);
    }
    if (message.eventType !== undefined) {
      obj.eventType = converseResponse_EventTypeToJSON(message.eventType);
    }
    if (message.audioOut !== undefined) {
      obj.audioOut = AudioOut.toJSON(message.audioOut);
    }
    if (message.result !== undefined) {
      obj.result = ConverseResult.toJSON(message.result);
    }
    return obj;
  },

  create(base?: DeepPartial<ConverseResponse>): ConverseResponse {
    return ConverseResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ConverseResponse>): ConverseResponse {
    const message = createBaseConverseResponse();
    message.error = (object.error !== undefined && object.error !== null)
      ? Status.fromPartial(object.error)
      : undefined;
    message.eventType = object.eventType ?? undefined;
    message.audioOut = (object.audioOut !== undefined && object.audioOut !== null)
      ? AudioOut.fromPartial(object.audioOut)
      : undefined;
    message.result = (object.result !== undefined && object.result !== null)
      ? ConverseResult.fromPartial(object.result)
      : undefined;
    return message;
  },
};

/** Service that implements Google Assistant API. */
export type EmbeddedAssistantDefinition = typeof EmbeddedAssistantDefinition;
export const EmbeddedAssistantDefinition = {
  name: "EmbeddedAssistant",
  fullName: "google.assistant.embedded.v1alpha1.EmbeddedAssistant",
  methods: {
    /**
     * Initiates or continues a conversation with the embedded assistant service.
     * Each call performs one round-trip, sending an audio request to the service
     * and receiving the audio response. Uses bidirectional streaming to receive
     * results, such as the `END_OF_UTTERANCE` event, while sending audio.
     *
     * A conversation is one or more gRPC connections, each consisting of several
     * streamed requests and responses.
     * For example, the user says *Add to my shopping list* and the assistant
     * responds *What do you want to add?*. The sequence of streamed requests and
     * responses in the first gRPC message could be:
     *
     * *   ConverseRequest.config
     * *   ConverseRequest.audio_in
     * *   ConverseRequest.audio_in
     * *   ConverseRequest.audio_in
     * *   ConverseRequest.audio_in
     * *   ConverseResponse.event_type.END_OF_UTTERANCE
     * *   ConverseResponse.result.microphone_mode.DIALOG_FOLLOW_ON
     * *   ConverseResponse.audio_out
     * *   ConverseResponse.audio_out
     * *   ConverseResponse.audio_out
     *
     * The user then says *bagels* and the assistant responds
     * *OK, I've added bagels to your shopping list*. This is sent as another gRPC
     * connection call to the `Converse` method, again with streamed requests and
     * responses, such as:
     *
     * *   ConverseRequest.config
     * *   ConverseRequest.audio_in
     * *   ConverseRequest.audio_in
     * *   ConverseRequest.audio_in
     * *   ConverseResponse.event_type.END_OF_UTTERANCE
     * *   ConverseResponse.result.microphone_mode.CLOSE_MICROPHONE
     * *   ConverseResponse.audio_out
     * *   ConverseResponse.audio_out
     * *   ConverseResponse.audio_out
     * *   ConverseResponse.audio_out
     *
     * Although the precise order of responses is not guaranteed, sequential
     * ConverseResponse.audio_out messages will always contain sequential portions
     * of audio.
     */
    converse: {
      name: "Converse",
      requestType: ConverseRequest,
      requestStream: true,
      responseType: ConverseResponse,
      responseStream: true,
      options: {},
    },
  },
} as const;

export interface EmbeddedAssistantServiceImplementation<CallContextExt = {}> {
  /**
   * Initiates or continues a conversation with the embedded assistant service.
   * Each call performs one round-trip, sending an audio request to the service
   * and receiving the audio response. Uses bidirectional streaming to receive
   * results, such as the `END_OF_UTTERANCE` event, while sending audio.
   *
   * A conversation is one or more gRPC connections, each consisting of several
   * streamed requests and responses.
   * For example, the user says *Add to my shopping list* and the assistant
   * responds *What do you want to add?*. The sequence of streamed requests and
   * responses in the first gRPC message could be:
   *
   * *   ConverseRequest.config
   * *   ConverseRequest.audio_in
   * *   ConverseRequest.audio_in
   * *   ConverseRequest.audio_in
   * *   ConverseRequest.audio_in
   * *   ConverseResponse.event_type.END_OF_UTTERANCE
   * *   ConverseResponse.result.microphone_mode.DIALOG_FOLLOW_ON
   * *   ConverseResponse.audio_out
   * *   ConverseResponse.audio_out
   * *   ConverseResponse.audio_out
   *
   * The user then says *bagels* and the assistant responds
   * *OK, I've added bagels to your shopping list*. This is sent as another gRPC
   * connection call to the `Converse` method, again with streamed requests and
   * responses, such as:
   *
   * *   ConverseRequest.config
   * *   ConverseRequest.audio_in
   * *   ConverseRequest.audio_in
   * *   ConverseRequest.audio_in
   * *   ConverseResponse.event_type.END_OF_UTTERANCE
   * *   ConverseResponse.result.microphone_mode.CLOSE_MICROPHONE
   * *   ConverseResponse.audio_out
   * *   ConverseResponse.audio_out
   * *   ConverseResponse.audio_out
   * *   ConverseResponse.audio_out
   *
   * Although the precise order of responses is not guaranteed, sequential
   * ConverseResponse.audio_out messages will always contain sequential portions
   * of audio.
   */
  converse(
    request: AsyncIterable<ConverseRequest>,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<ConverseResponse>>;
}

export interface EmbeddedAssistantClient<CallOptionsExt = {}> {
  /**
   * Initiates or continues a conversation with the embedded assistant service.
   * Each call performs one round-trip, sending an audio request to the service
   * and receiving the audio response. Uses bidirectional streaming to receive
   * results, such as the `END_OF_UTTERANCE` event, while sending audio.
   *
   * A conversation is one or more gRPC connections, each consisting of several
   * streamed requests and responses.
   * For example, the user says *Add to my shopping list* and the assistant
   * responds *What do you want to add?*. The sequence of streamed requests and
   * responses in the first gRPC message could be:
   *
   * *   ConverseRequest.config
   * *   ConverseRequest.audio_in
   * *   ConverseRequest.audio_in
   * *   ConverseRequest.audio_in
   * *   ConverseRequest.audio_in
   * *   ConverseResponse.event_type.END_OF_UTTERANCE
   * *   ConverseResponse.result.microphone_mode.DIALOG_FOLLOW_ON
   * *   ConverseResponse.audio_out
   * *   ConverseResponse.audio_out
   * *   ConverseResponse.audio_out
   *
   * The user then says *bagels* and the assistant responds
   * *OK, I've added bagels to your shopping list*. This is sent as another gRPC
   * connection call to the `Converse` method, again with streamed requests and
   * responses, such as:
   *
   * *   ConverseRequest.config
   * *   ConverseRequest.audio_in
   * *   ConverseRequest.audio_in
   * *   ConverseRequest.audio_in
   * *   ConverseResponse.event_type.END_OF_UTTERANCE
   * *   ConverseResponse.result.microphone_mode.CLOSE_MICROPHONE
   * *   ConverseResponse.audio_out
   * *   ConverseResponse.audio_out
   * *   ConverseResponse.audio_out
   * *   ConverseResponse.audio_out
   *
   * Although the precise order of responses is not guaranteed, sequential
   * ConverseResponse.audio_out messages will always contain sequential portions
   * of audio.
   */
  converse(
    request: AsyncIterable<DeepPartial<ConverseRequest>>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<ConverseResponse>;
}

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export type ServerStreamingMethodResult<Response> = { [Symbol.asyncIterator](): AsyncIterator<Response, void> };

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
