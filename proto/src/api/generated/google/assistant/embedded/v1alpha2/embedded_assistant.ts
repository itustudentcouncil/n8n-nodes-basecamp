// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.0
//   protoc               unknown
// source: google/assistant/embedded/v1alpha2/embedded_assistant.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import Long from "long";
import { type CallContext, type CallOptions } from "nice-grpc-common";
import { LatLng } from "../../../type/latlng.js";

export const protobufPackage = "google.assistant.embedded.v1alpha2";

/**
 * The top-level message sent by the client. Clients must send at least two, and
 * typically numerous `AssistRequest` messages. The first message must
 * contain a `config` message and must not contain `audio_in` data. All
 * subsequent messages must contain `audio_in` data and must not contain a
 * `config` message.
 */
export interface AssistRequest {
  /**
   * The `config` message provides information to the recognizer that
   * specifies how to process the request.
   * The first `AssistRequest` message must contain a `config` message.
   */
  config?:
    | AssistConfig
    | undefined;
  /**
   * The audio data to be recognized. Sequential chunks of audio data are sent
   * in sequential `AssistRequest` messages. The first `AssistRequest`
   * message must not contain `audio_in` data and all subsequent
   * `AssistRequest` messages must contain `audio_in` data. The audio bytes
   * must be encoded as specified in `AudioInConfig`.
   * Audio must be sent at approximately real-time (16000 samples per second).
   * An error will be returned if audio is sent significantly faster or
   * slower.
   */
  audioIn?: Buffer | undefined;
}

/**
 * The top-level message received by the client. A series of one or more
 * `AssistResponse` messages are streamed back to the client.
 */
export interface AssistResponse {
  /** Output-only* Indicates the type of event. */
  eventType: AssistResponse_EventType;
  /** Output-only* The audio containing the Assistant's response to the query. */
  audioOut:
    | AudioOut
    | undefined;
  /** Output-only* Contains the Assistant's visual response to the query. */
  screenOut:
    | ScreenOut
    | undefined;
  /**
   * Output-only* Contains the action triggered by the query with the
   * appropriate payloads and semantic parsing.
   */
  deviceAction:
    | DeviceAction
    | undefined;
  /**
   * Output-only* This repeated list contains zero or more speech recognition
   * results that correspond to consecutive portions of the audio currently
   * being processed, starting with the portion corresponding to the earliest
   * audio (and most stable portion) to the portion corresponding to the most
   * recent audio. The strings can be concatenated to view the full
   * in-progress response. When the speech recognition completes, this list
   * will contain one item with `stability` of `1.0`.
   */
  speechResults: SpeechRecognitionResult[];
  /** Output-only* Contains output related to the user's query. */
  dialogStateOut:
    | DialogStateOut
    | undefined;
  /**
   * Output-only* Debugging info for developer. Only returned if request set
   * `return_debug_info` to true.
   */
  debugInfo: DebugInfo | undefined;
}

/** Indicates the type of event. */
export enum AssistResponse_EventType {
  /** EVENT_TYPE_UNSPECIFIED - No event specified. */
  EVENT_TYPE_UNSPECIFIED = 0,
  /**
   * END_OF_UTTERANCE - This event indicates that the server has detected the end of the user's
   * speech utterance and expects no additional speech. Therefore, the server
   * will not process additional audio (although it may subsequently return
   * additional results). The client should stop sending additional audio
   * data, half-close the gRPC connection, and wait for any additional results
   * until the server closes the gRPC connection.
   */
  END_OF_UTTERANCE = 1,
  UNRECOGNIZED = -1,
}

export function assistResponse_EventTypeFromJSON(object: any): AssistResponse_EventType {
  switch (object) {
    case 0:
    case "EVENT_TYPE_UNSPECIFIED":
      return AssistResponse_EventType.EVENT_TYPE_UNSPECIFIED;
    case 1:
    case "END_OF_UTTERANCE":
      return AssistResponse_EventType.END_OF_UTTERANCE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AssistResponse_EventType.UNRECOGNIZED;
  }
}

export function assistResponse_EventTypeToJSON(object: AssistResponse_EventType): string {
  switch (object) {
    case AssistResponse_EventType.EVENT_TYPE_UNSPECIFIED:
      return "EVENT_TYPE_UNSPECIFIED";
    case AssistResponse_EventType.END_OF_UTTERANCE:
      return "END_OF_UTTERANCE";
    case AssistResponse_EventType.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Debug info for developer. Only returned if request set `return_debug_info`
 * to true.
 */
export interface DebugInfo {
  /**
   * The original JSON response from an Action-on-Google agent to Google server.
   * See
   * https://developers.google.com/actions/reference/rest/Shared.Types/AppResponse.
   * It will only be populated if the request maker owns the AoG project and the
   * AoG project is in preview mode.
   */
  aogAgentToAssistantJson: string;
}

/** Specifies how to process the `AssistRequest` messages. */
export interface AssistConfig {
  /**
   * Specifies how to process the subsequent incoming audio. Required if
   * [AssistRequest.audio_in][google.assistant.embedded.v1alpha2.AssistRequest.audio_in]
   * bytes will be provided in subsequent requests.
   */
  audioInConfig?:
    | AudioInConfig
    | undefined;
  /**
   * The text input to be sent to the Assistant. This can be populated from a
   * text interface if audio input is not available.
   */
  textQuery?:
    | string
    | undefined;
  /** Required* Specifies how to format the audio that will be returned. */
  audioOutConfig:
    | AudioOutConfig
    | undefined;
  /**
   * Optional* Specifies the desired format to use when server returns a
   * visual screen response.
   */
  screenOutConfig:
    | ScreenOutConfig
    | undefined;
  /** Required* Represents the current dialog state. */
  dialogStateIn:
    | DialogStateIn
    | undefined;
  /** Device configuration that uniquely identifies a specific device. */
  deviceConfig:
    | DeviceConfig
    | undefined;
  /** Optional* Debugging parameters for the whole `Assist` RPC. */
  debugConfig: DebugConfig | undefined;
}

/**
 * Specifies how to process the `audio_in` data that will be provided in
 * subsequent requests. For recommended settings, see the Google Assistant SDK
 * [best
 * practices](https://developers.google.com/assistant/sdk/guides/service/python/best-practices/audio).
 */
export interface AudioInConfig {
  /** Required* Encoding of audio data sent in all `audio_in` messages. */
  encoding: AudioInConfig_Encoding;
  /**
   * Required* Sample rate (in Hertz) of the audio data sent in all `audio_in`
   * messages. Valid values are from 16000-24000, but 16000 is optimal.
   * For best results, set the sampling rate of the audio source to 16000 Hz.
   * If that's not possible, use the native sample rate of the audio source
   * (instead of re-sampling).
   */
  sampleRateHertz: number;
}

/**
 * Audio encoding of the data sent in the audio message.
 * Audio must be one-channel (mono).
 */
export enum AudioInConfig_Encoding {
  /** ENCODING_UNSPECIFIED - Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][]. */
  ENCODING_UNSPECIFIED = 0,
  /**
   * LINEAR16 - Uncompressed 16-bit signed little-endian samples (Linear PCM).
   * This encoding includes no header, only the raw audio bytes.
   */
  LINEAR16 = 1,
  /**
   * FLAC - [`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
   * Codec) is the recommended encoding because it is
   * lossless--therefore recognition is not compromised--and
   * requires only about half the bandwidth of `LINEAR16`. This encoding
   * includes the `FLAC` stream header followed by audio data. It supports
   * 16-bit and 24-bit samples, however, not all fields in `STREAMINFO` are
   * supported.
   */
  FLAC = 2,
  UNRECOGNIZED = -1,
}

export function audioInConfig_EncodingFromJSON(object: any): AudioInConfig_Encoding {
  switch (object) {
    case 0:
    case "ENCODING_UNSPECIFIED":
      return AudioInConfig_Encoding.ENCODING_UNSPECIFIED;
    case 1:
    case "LINEAR16":
      return AudioInConfig_Encoding.LINEAR16;
    case 2:
    case "FLAC":
      return AudioInConfig_Encoding.FLAC;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AudioInConfig_Encoding.UNRECOGNIZED;
  }
}

export function audioInConfig_EncodingToJSON(object: AudioInConfig_Encoding): string {
  switch (object) {
    case AudioInConfig_Encoding.ENCODING_UNSPECIFIED:
      return "ENCODING_UNSPECIFIED";
    case AudioInConfig_Encoding.LINEAR16:
      return "LINEAR16";
    case AudioInConfig_Encoding.FLAC:
      return "FLAC";
    case AudioInConfig_Encoding.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Specifies the desired format for the server to use when it returns
 * `audio_out` messages.
 */
export interface AudioOutConfig {
  /**
   * Required* The encoding of audio data to be returned in all `audio_out`
   * messages.
   */
  encoding: AudioOutConfig_Encoding;
  /**
   * Required* The sample rate in Hertz of the audio data returned in
   * `audio_out` messages. Valid values are: 16000-24000.
   */
  sampleRateHertz: number;
  /**
   * Required* Current volume setting of the device's audio output.
   * Valid values are 1 to 100 (corresponding to 1% to 100%).
   */
  volumePercentage: number;
}

/**
 * Audio encoding of the data returned in the audio message. All encodings are
 * raw audio bytes with no header, except as indicated below.
 */
export enum AudioOutConfig_Encoding {
  /** ENCODING_UNSPECIFIED - Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][]. */
  ENCODING_UNSPECIFIED = 0,
  /** LINEAR16 - Uncompressed 16-bit signed little-endian samples (Linear PCM). */
  LINEAR16 = 1,
  /** MP3 - MP3 audio encoding. The sample rate is encoded in the payload. */
  MP3 = 2,
  /**
   * OPUS_IN_OGG - Opus-encoded audio wrapped in an ogg container. The result will be a
   * file which can be played natively on Android and in some browsers (such
   * as Chrome). The quality of the encoding is considerably higher than MP3
   * while using the same bitrate. The sample rate is encoded in the payload.
   */
  OPUS_IN_OGG = 3,
  UNRECOGNIZED = -1,
}

export function audioOutConfig_EncodingFromJSON(object: any): AudioOutConfig_Encoding {
  switch (object) {
    case 0:
    case "ENCODING_UNSPECIFIED":
      return AudioOutConfig_Encoding.ENCODING_UNSPECIFIED;
    case 1:
    case "LINEAR16":
      return AudioOutConfig_Encoding.LINEAR16;
    case 2:
    case "MP3":
      return AudioOutConfig_Encoding.MP3;
    case 3:
    case "OPUS_IN_OGG":
      return AudioOutConfig_Encoding.OPUS_IN_OGG;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AudioOutConfig_Encoding.UNRECOGNIZED;
  }
}

export function audioOutConfig_EncodingToJSON(object: AudioOutConfig_Encoding): string {
  switch (object) {
    case AudioOutConfig_Encoding.ENCODING_UNSPECIFIED:
      return "ENCODING_UNSPECIFIED";
    case AudioOutConfig_Encoding.LINEAR16:
      return "LINEAR16";
    case AudioOutConfig_Encoding.MP3:
      return "MP3";
    case AudioOutConfig_Encoding.OPUS_IN_OGG:
      return "OPUS_IN_OGG";
    case AudioOutConfig_Encoding.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Specifies the desired format for the server to use when it returns
 * `screen_out` response.
 */
export interface ScreenOutConfig {
  /** Current visual screen-mode for the device while issuing the query. */
  screenMode: ScreenOutConfig_ScreenMode;
}

/** Possible modes for visual screen-output on the device. */
export enum ScreenOutConfig_ScreenMode {
  /**
   * SCREEN_MODE_UNSPECIFIED - No video mode specified.
   * The Assistant may respond as if in `OFF` mode.
   */
  SCREEN_MODE_UNSPECIFIED = 0,
  /**
   * OFF - Screen is off (or has brightness or other settings set so low it is
   * not visible). The Assistant will typically not return a screen response
   * in this mode.
   */
  OFF = 1,
  /**
   * PLAYING - The Assistant will typically return a partial-screen response in this
   * mode.
   */
  PLAYING = 3,
  UNRECOGNIZED = -1,
}

export function screenOutConfig_ScreenModeFromJSON(object: any): ScreenOutConfig_ScreenMode {
  switch (object) {
    case 0:
    case "SCREEN_MODE_UNSPECIFIED":
      return ScreenOutConfig_ScreenMode.SCREEN_MODE_UNSPECIFIED;
    case 1:
    case "OFF":
      return ScreenOutConfig_ScreenMode.OFF;
    case 3:
    case "PLAYING":
      return ScreenOutConfig_ScreenMode.PLAYING;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ScreenOutConfig_ScreenMode.UNRECOGNIZED;
  }
}

export function screenOutConfig_ScreenModeToJSON(object: ScreenOutConfig_ScreenMode): string {
  switch (object) {
    case ScreenOutConfig_ScreenMode.SCREEN_MODE_UNSPECIFIED:
      return "SCREEN_MODE_UNSPECIFIED";
    case ScreenOutConfig_ScreenMode.OFF:
      return "OFF";
    case ScreenOutConfig_ScreenMode.PLAYING:
      return "PLAYING";
    case ScreenOutConfig_ScreenMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Provides information about the current dialog state. */
export interface DialogStateIn {
  /**
   * Required* This field must always be set to the
   * [DialogStateOut.conversation_state][google.assistant.embedded.v1alpha2.DialogStateOut.conversation_state]
   * value that was returned in the prior `Assist` RPC. It should only be
   * omitted (field not set) if there was no prior `Assist` RPC because this is
   * the first `Assist` RPC made by this device after it was first setup and/or
   * a factory-default reset.
   */
  conversationState: Buffer;
  /**
   * Required* Language of the request in
   * [IETF BCP 47 syntax](https://tools.ietf.org/html/bcp47) (for example,
   * "en-US"). See [Language
   * Support](https://developers.google.com/assistant/sdk/reference/rpc/languages)
   * for more information. If you have selected a language for this `device_id`
   * using the
   * [Settings](https://developers.google.com/assistant/sdk/reference/assistant-app/assistant-settings)
   * menu in your phone's Google Assistant app, that selection will override
   * this value.
   */
  languageCode: string;
  /** Optional* Location of the device where the query originated. */
  deviceLocation:
    | DeviceLocation
    | undefined;
  /**
   * Optional* If true, the server will treat the request as a new conversation
   * and not use state from the prior request. Set this field to true when the
   * conversation should be restarted, such as after a device reboot, or after a
   * significant lapse of time since the prior query.
   */
  isNewConversation: boolean;
}

/**
 * Required* Fields that identify the device to the Assistant.
 *
 * See also:
 *
 * *   [Register a Device - REST
 * API](https://developers.google.com/assistant/sdk/reference/device-registration/register-device-manual)
 * *   [Device Model and Instance
 * Schemas](https://developers.google.com/assistant/sdk/reference/device-registration/model-and-instance-schemas)
 * *   [Device
 * Proto](https://developers.google.com/assistant/sdk/reference/rpc/google.assistant.devices.v1alpha2#device)
 */
export interface DeviceConfig {
  /**
   * Required* Unique identifier for the device. The id length must be 128
   * characters or less. Example: DBCDW098234. This MUST match the device_id
   * returned from device registration. This device_id is used to match against
   * the user's registered devices to lookup the supported traits and
   * capabilities of this device. This information should not change across
   * device reboots. However, it should not be saved across
   * factory-default resets.
   */
  deviceId: string;
  /**
   * Required* Unique identifier for the device model. The combination of
   * device_model_id and device_id must have been previously associated through
   * device registration.
   */
  deviceModelId: string;
}

/**
 * The audio containing the Assistant's response to the query. Sequential chunks
 * of audio data are received in sequential `AssistResponse` messages.
 */
export interface AudioOut {
  /**
   * Output-only* The audio data containing the Assistant's response to the
   * query. Sequential chunks of audio data are received in sequential
   * `AssistResponse` messages.
   */
  audioData: Buffer;
}

/**
 * The Assistant's visual output response to query. Enabled by
 * `screen_out_config`.
 */
export interface ScreenOut {
  /** Output-only* The format of the provided screen data. */
  format: ScreenOut_Format;
  /**
   * Output-only* The raw screen data to be displayed as the result of the
   * Assistant query.
   */
  data: Buffer;
}

/** Possible formats of the screen data. */
export enum ScreenOut_Format {
  /** FORMAT_UNSPECIFIED - No format specified. */
  FORMAT_UNSPECIFIED = 0,
  /**
   * HTML - Data will contain a fully-formed HTML5 layout encoded in UTF-8, e.g.
   * `<html><body><div>...</div></body></html>`. It is intended to be rendered
   * along with the audio response. Note that HTML5 doctype should be included
   * in the actual HTML data.
   */
  HTML = 1,
  UNRECOGNIZED = -1,
}

export function screenOut_FormatFromJSON(object: any): ScreenOut_Format {
  switch (object) {
    case 0:
    case "FORMAT_UNSPECIFIED":
      return ScreenOut_Format.FORMAT_UNSPECIFIED;
    case 1:
    case "HTML":
      return ScreenOut_Format.HTML;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ScreenOut_Format.UNRECOGNIZED;
  }
}

export function screenOut_FormatToJSON(object: ScreenOut_Format): string {
  switch (object) {
    case ScreenOut_Format.FORMAT_UNSPECIFIED:
      return "FORMAT_UNSPECIFIED";
    case ScreenOut_Format.HTML:
      return "HTML";
    case ScreenOut_Format.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * The response returned to the device if the user has triggered a Device
 * Action. For example, a device which supports the query *Turn on the light*
 * would receive a `DeviceAction` with a JSON payload containing the semantics
 * of the request.
 */
export interface DeviceAction {
  /**
   * JSON containing the device command response generated from the triggered
   * Device Action grammar. The format is given by the
   * `action.devices.EXECUTE` intent for a given
   * [trait](https://developers.google.com/assistant/sdk/reference/traits/).
   */
  deviceRequestJson: string;
}

/**
 * The estimated transcription of a phrase the user has spoken. This could be
 * a single segment or the full guess of the user's spoken query.
 */
export interface SpeechRecognitionResult {
  /** Output-only* Transcript text representing the words that the user spoke. */
  transcript: string;
  /**
   * Output-only* An estimate of the likelihood that the Assistant will not
   * change its guess about this result. Values range from 0.0 (completely
   * unstable) to 1.0 (completely stable and final). The default of 0.0 is a
   * sentinel value indicating `stability` was not set.
   */
  stability: number;
}

/**
 * The dialog state resulting from the user's query. Multiple of these messages
 * may be received.
 */
export interface DialogStateOut {
  /**
   * Output-only* Supplemental display text from the Assistant. This could be
   * the same as the speech spoken in `AssistResponse.audio_out` or it could
   * be some additional information which aids the user's understanding.
   */
  supplementalDisplayText: string;
  /**
   * Output-only* State information for the subsequent `Assist` RPC. This
   * value should be saved in the client and returned in the
   * [`DialogStateIn.conversation_state`](#dialogstatein) field with the next
   * `Assist` RPC. (The client does not need to interpret or otherwise use this
   * value.) This information should be saved across device reboots. However,
   * this value should be cleared (not saved in the client) during a
   * factory-default reset.
   */
  conversationState: Buffer;
  /**
   * Output-only* Specifies the mode of the microphone after this `Assist`
   * RPC is processed.
   */
  microphoneMode: DialogStateOut_MicrophoneMode;
  /**
   * Output-only* Updated volume level. The value will be 0 or omitted
   * (indicating no change) unless a voice command such as *Increase the volume*
   * or *Set volume level 4* was recognized, in which case the value will be
   * between 1 and 100 (corresponding to the new volume level of 1% to 100%).
   * Typically, a client should use this volume level when playing the
   * `audio_out` data, and retain this value as the current volume level and
   * supply it in the `AudioOutConfig` of the next `AssistRequest`. (Some
   * clients may also implement other ways to allow the current volume level to
   * be changed, for example, by providing a knob that the user can turn.)
   */
  volumePercentage: number;
}

/** Possible states of the microphone after a `Assist` RPC completes. */
export enum DialogStateOut_MicrophoneMode {
  /** MICROPHONE_MODE_UNSPECIFIED - No mode specified. */
  MICROPHONE_MODE_UNSPECIFIED = 0,
  /**
   * CLOSE_MICROPHONE - The service is not expecting a follow-on question from the user.
   * The microphone should remain off until the user re-activates it.
   */
  CLOSE_MICROPHONE = 1,
  /**
   * DIALOG_FOLLOW_ON - The service is expecting a follow-on question from the user. The
   * microphone should be re-opened when the `AudioOut` playback completes
   * (by starting a new `Assist` RPC call to send the new audio).
   */
  DIALOG_FOLLOW_ON = 2,
  UNRECOGNIZED = -1,
}

export function dialogStateOut_MicrophoneModeFromJSON(object: any): DialogStateOut_MicrophoneMode {
  switch (object) {
    case 0:
    case "MICROPHONE_MODE_UNSPECIFIED":
      return DialogStateOut_MicrophoneMode.MICROPHONE_MODE_UNSPECIFIED;
    case 1:
    case "CLOSE_MICROPHONE":
      return DialogStateOut_MicrophoneMode.CLOSE_MICROPHONE;
    case 2:
    case "DIALOG_FOLLOW_ON":
      return DialogStateOut_MicrophoneMode.DIALOG_FOLLOW_ON;
    case -1:
    case "UNRECOGNIZED":
    default:
      return DialogStateOut_MicrophoneMode.UNRECOGNIZED;
  }
}

export function dialogStateOut_MicrophoneModeToJSON(object: DialogStateOut_MicrophoneMode): string {
  switch (object) {
    case DialogStateOut_MicrophoneMode.MICROPHONE_MODE_UNSPECIFIED:
      return "MICROPHONE_MODE_UNSPECIFIED";
    case DialogStateOut_MicrophoneMode.CLOSE_MICROPHONE:
      return "CLOSE_MICROPHONE";
    case DialogStateOut_MicrophoneMode.DIALOG_FOLLOW_ON:
      return "DIALOG_FOLLOW_ON";
    case DialogStateOut_MicrophoneMode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Debugging parameters for the current request. */
export interface DebugConfig {
  /**
   * When this field is set to true, the `debug_info` field in `AssistResponse`
   * may be populated. However it will significantly increase latency of
   * responses. Do not set this field true in production code.
   */
  returnDebugInfo: boolean;
}

/**
 * There are three sources of locations. They are used with this precedence:
 *
 * 1. This `DeviceLocation`, which is primarily used for mobile devices with
 *    GPS .
 * 2. Location specified by the user during device setup; this is per-user, per
 *    device. This location is used if `DeviceLocation` is not specified.
 * 3. Inferred location based on IP address. This is used only if neither of the
 *    above are specified.
 */
export interface DeviceLocation {
  /** Latitude and longitude of device. */
  coordinates?: LatLng | undefined;
}

function createBaseAssistRequest(): AssistRequest {
  return { config: undefined, audioIn: undefined };
}

export const AssistRequest: MessageFns<AssistRequest> = {
  encode(message: AssistRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.config !== undefined) {
      AssistConfig.encode(message.config, writer.uint32(10).fork()).join();
    }
    if (message.audioIn !== undefined) {
      writer.uint32(18).bytes(message.audioIn);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AssistRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAssistRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.config = AssistConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.audioIn = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AssistRequest {
    return {
      config: isSet(object.config) ? AssistConfig.fromJSON(object.config) : undefined,
      audioIn: isSet(object.audioIn) ? Buffer.from(bytesFromBase64(object.audioIn)) : undefined,
    };
  },

  toJSON(message: AssistRequest): unknown {
    const obj: any = {};
    if (message.config !== undefined) {
      obj.config = AssistConfig.toJSON(message.config);
    }
    if (message.audioIn !== undefined) {
      obj.audioIn = base64FromBytes(message.audioIn);
    }
    return obj;
  },

  create(base?: DeepPartial<AssistRequest>): AssistRequest {
    return AssistRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AssistRequest>): AssistRequest {
    const message = createBaseAssistRequest();
    message.config = (object.config !== undefined && object.config !== null)
      ? AssistConfig.fromPartial(object.config)
      : undefined;
    message.audioIn = object.audioIn ?? undefined;
    return message;
  },
};

function createBaseAssistResponse(): AssistResponse {
  return {
    eventType: 0,
    audioOut: undefined,
    screenOut: undefined,
    deviceAction: undefined,
    speechResults: [],
    dialogStateOut: undefined,
    debugInfo: undefined,
  };
}

export const AssistResponse: MessageFns<AssistResponse> = {
  encode(message: AssistResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.eventType !== 0) {
      writer.uint32(8).int32(message.eventType);
    }
    if (message.audioOut !== undefined) {
      AudioOut.encode(message.audioOut, writer.uint32(26).fork()).join();
    }
    if (message.screenOut !== undefined) {
      ScreenOut.encode(message.screenOut, writer.uint32(34).fork()).join();
    }
    if (message.deviceAction !== undefined) {
      DeviceAction.encode(message.deviceAction, writer.uint32(50).fork()).join();
    }
    for (const v of message.speechResults) {
      SpeechRecognitionResult.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.dialogStateOut !== undefined) {
      DialogStateOut.encode(message.dialogStateOut, writer.uint32(42).fork()).join();
    }
    if (message.debugInfo !== undefined) {
      DebugInfo.encode(message.debugInfo, writer.uint32(66).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AssistResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAssistResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.eventType = reader.int32() as any;
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.audioOut = AudioOut.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.screenOut = ScreenOut.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.deviceAction = DeviceAction.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.speechResults.push(SpeechRecognitionResult.decode(reader, reader.uint32()));
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.dialogStateOut = DialogStateOut.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.debugInfo = DebugInfo.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AssistResponse {
    return {
      eventType: isSet(object.eventType) ? assistResponse_EventTypeFromJSON(object.eventType) : 0,
      audioOut: isSet(object.audioOut) ? AudioOut.fromJSON(object.audioOut) : undefined,
      screenOut: isSet(object.screenOut) ? ScreenOut.fromJSON(object.screenOut) : undefined,
      deviceAction: isSet(object.deviceAction) ? DeviceAction.fromJSON(object.deviceAction) : undefined,
      speechResults: globalThis.Array.isArray(object?.speechResults)
        ? object.speechResults.map((e: any) => SpeechRecognitionResult.fromJSON(e))
        : [],
      dialogStateOut: isSet(object.dialogStateOut) ? DialogStateOut.fromJSON(object.dialogStateOut) : undefined,
      debugInfo: isSet(object.debugInfo) ? DebugInfo.fromJSON(object.debugInfo) : undefined,
    };
  },

  toJSON(message: AssistResponse): unknown {
    const obj: any = {};
    if (message.eventType !== 0) {
      obj.eventType = assistResponse_EventTypeToJSON(message.eventType);
    }
    if (message.audioOut !== undefined) {
      obj.audioOut = AudioOut.toJSON(message.audioOut);
    }
    if (message.screenOut !== undefined) {
      obj.screenOut = ScreenOut.toJSON(message.screenOut);
    }
    if (message.deviceAction !== undefined) {
      obj.deviceAction = DeviceAction.toJSON(message.deviceAction);
    }
    if (message.speechResults?.length) {
      obj.speechResults = message.speechResults.map((e) => SpeechRecognitionResult.toJSON(e));
    }
    if (message.dialogStateOut !== undefined) {
      obj.dialogStateOut = DialogStateOut.toJSON(message.dialogStateOut);
    }
    if (message.debugInfo !== undefined) {
      obj.debugInfo = DebugInfo.toJSON(message.debugInfo);
    }
    return obj;
  },

  create(base?: DeepPartial<AssistResponse>): AssistResponse {
    return AssistResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AssistResponse>): AssistResponse {
    const message = createBaseAssistResponse();
    message.eventType = object.eventType ?? 0;
    message.audioOut = (object.audioOut !== undefined && object.audioOut !== null)
      ? AudioOut.fromPartial(object.audioOut)
      : undefined;
    message.screenOut = (object.screenOut !== undefined && object.screenOut !== null)
      ? ScreenOut.fromPartial(object.screenOut)
      : undefined;
    message.deviceAction = (object.deviceAction !== undefined && object.deviceAction !== null)
      ? DeviceAction.fromPartial(object.deviceAction)
      : undefined;
    message.speechResults = object.speechResults?.map((e) => SpeechRecognitionResult.fromPartial(e)) || [];
    message.dialogStateOut = (object.dialogStateOut !== undefined && object.dialogStateOut !== null)
      ? DialogStateOut.fromPartial(object.dialogStateOut)
      : undefined;
    message.debugInfo = (object.debugInfo !== undefined && object.debugInfo !== null)
      ? DebugInfo.fromPartial(object.debugInfo)
      : undefined;
    return message;
  },
};

function createBaseDebugInfo(): DebugInfo {
  return { aogAgentToAssistantJson: "" };
}

export const DebugInfo: MessageFns<DebugInfo> = {
  encode(message: DebugInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.aogAgentToAssistantJson !== "") {
      writer.uint32(10).string(message.aogAgentToAssistantJson);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DebugInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDebugInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.aogAgentToAssistantJson = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DebugInfo {
    return {
      aogAgentToAssistantJson: isSet(object.aogAgentToAssistantJson)
        ? globalThis.String(object.aogAgentToAssistantJson)
        : "",
    };
  },

  toJSON(message: DebugInfo): unknown {
    const obj: any = {};
    if (message.aogAgentToAssistantJson !== "") {
      obj.aogAgentToAssistantJson = message.aogAgentToAssistantJson;
    }
    return obj;
  },

  create(base?: DeepPartial<DebugInfo>): DebugInfo {
    return DebugInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DebugInfo>): DebugInfo {
    const message = createBaseDebugInfo();
    message.aogAgentToAssistantJson = object.aogAgentToAssistantJson ?? "";
    return message;
  },
};

function createBaseAssistConfig(): AssistConfig {
  return {
    audioInConfig: undefined,
    textQuery: undefined,
    audioOutConfig: undefined,
    screenOutConfig: undefined,
    dialogStateIn: undefined,
    deviceConfig: undefined,
    debugConfig: undefined,
  };
}

export const AssistConfig: MessageFns<AssistConfig> = {
  encode(message: AssistConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.audioInConfig !== undefined) {
      AudioInConfig.encode(message.audioInConfig, writer.uint32(10).fork()).join();
    }
    if (message.textQuery !== undefined) {
      writer.uint32(50).string(message.textQuery);
    }
    if (message.audioOutConfig !== undefined) {
      AudioOutConfig.encode(message.audioOutConfig, writer.uint32(18).fork()).join();
    }
    if (message.screenOutConfig !== undefined) {
      ScreenOutConfig.encode(message.screenOutConfig, writer.uint32(66).fork()).join();
    }
    if (message.dialogStateIn !== undefined) {
      DialogStateIn.encode(message.dialogStateIn, writer.uint32(26).fork()).join();
    }
    if (message.deviceConfig !== undefined) {
      DeviceConfig.encode(message.deviceConfig, writer.uint32(34).fork()).join();
    }
    if (message.debugConfig !== undefined) {
      DebugConfig.encode(message.debugConfig, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AssistConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAssistConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.audioInConfig = AudioInConfig.decode(reader, reader.uint32());
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.textQuery = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.audioOutConfig = AudioOutConfig.decode(reader, reader.uint32());
          continue;
        case 8:
          if (tag !== 66) {
            break;
          }

          message.screenOutConfig = ScreenOutConfig.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.dialogStateIn = DialogStateIn.decode(reader, reader.uint32());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.deviceConfig = DeviceConfig.decode(reader, reader.uint32());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.debugConfig = DebugConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AssistConfig {
    return {
      audioInConfig: isSet(object.audioInConfig) ? AudioInConfig.fromJSON(object.audioInConfig) : undefined,
      textQuery: isSet(object.textQuery) ? globalThis.String(object.textQuery) : undefined,
      audioOutConfig: isSet(object.audioOutConfig) ? AudioOutConfig.fromJSON(object.audioOutConfig) : undefined,
      screenOutConfig: isSet(object.screenOutConfig) ? ScreenOutConfig.fromJSON(object.screenOutConfig) : undefined,
      dialogStateIn: isSet(object.dialogStateIn) ? DialogStateIn.fromJSON(object.dialogStateIn) : undefined,
      deviceConfig: isSet(object.deviceConfig) ? DeviceConfig.fromJSON(object.deviceConfig) : undefined,
      debugConfig: isSet(object.debugConfig) ? DebugConfig.fromJSON(object.debugConfig) : undefined,
    };
  },

  toJSON(message: AssistConfig): unknown {
    const obj: any = {};
    if (message.audioInConfig !== undefined) {
      obj.audioInConfig = AudioInConfig.toJSON(message.audioInConfig);
    }
    if (message.textQuery !== undefined) {
      obj.textQuery = message.textQuery;
    }
    if (message.audioOutConfig !== undefined) {
      obj.audioOutConfig = AudioOutConfig.toJSON(message.audioOutConfig);
    }
    if (message.screenOutConfig !== undefined) {
      obj.screenOutConfig = ScreenOutConfig.toJSON(message.screenOutConfig);
    }
    if (message.dialogStateIn !== undefined) {
      obj.dialogStateIn = DialogStateIn.toJSON(message.dialogStateIn);
    }
    if (message.deviceConfig !== undefined) {
      obj.deviceConfig = DeviceConfig.toJSON(message.deviceConfig);
    }
    if (message.debugConfig !== undefined) {
      obj.debugConfig = DebugConfig.toJSON(message.debugConfig);
    }
    return obj;
  },

  create(base?: DeepPartial<AssistConfig>): AssistConfig {
    return AssistConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AssistConfig>): AssistConfig {
    const message = createBaseAssistConfig();
    message.audioInConfig = (object.audioInConfig !== undefined && object.audioInConfig !== null)
      ? AudioInConfig.fromPartial(object.audioInConfig)
      : undefined;
    message.textQuery = object.textQuery ?? undefined;
    message.audioOutConfig = (object.audioOutConfig !== undefined && object.audioOutConfig !== null)
      ? AudioOutConfig.fromPartial(object.audioOutConfig)
      : undefined;
    message.screenOutConfig = (object.screenOutConfig !== undefined && object.screenOutConfig !== null)
      ? ScreenOutConfig.fromPartial(object.screenOutConfig)
      : undefined;
    message.dialogStateIn = (object.dialogStateIn !== undefined && object.dialogStateIn !== null)
      ? DialogStateIn.fromPartial(object.dialogStateIn)
      : undefined;
    message.deviceConfig = (object.deviceConfig !== undefined && object.deviceConfig !== null)
      ? DeviceConfig.fromPartial(object.deviceConfig)
      : undefined;
    message.debugConfig = (object.debugConfig !== undefined && object.debugConfig !== null)
      ? DebugConfig.fromPartial(object.debugConfig)
      : undefined;
    return message;
  },
};

function createBaseAudioInConfig(): AudioInConfig {
  return { encoding: 0, sampleRateHertz: 0 };
}

export const AudioInConfig: MessageFns<AudioInConfig> = {
  encode(message: AudioInConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.encoding !== 0) {
      writer.uint32(8).int32(message.encoding);
    }
    if (message.sampleRateHertz !== 0) {
      writer.uint32(16).int32(message.sampleRateHertz);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AudioInConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAudioInConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.encoding = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.sampleRateHertz = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AudioInConfig {
    return {
      encoding: isSet(object.encoding) ? audioInConfig_EncodingFromJSON(object.encoding) : 0,
      sampleRateHertz: isSet(object.sampleRateHertz) ? globalThis.Number(object.sampleRateHertz) : 0,
    };
  },

  toJSON(message: AudioInConfig): unknown {
    const obj: any = {};
    if (message.encoding !== 0) {
      obj.encoding = audioInConfig_EncodingToJSON(message.encoding);
    }
    if (message.sampleRateHertz !== 0) {
      obj.sampleRateHertz = Math.round(message.sampleRateHertz);
    }
    return obj;
  },

  create(base?: DeepPartial<AudioInConfig>): AudioInConfig {
    return AudioInConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AudioInConfig>): AudioInConfig {
    const message = createBaseAudioInConfig();
    message.encoding = object.encoding ?? 0;
    message.sampleRateHertz = object.sampleRateHertz ?? 0;
    return message;
  },
};

function createBaseAudioOutConfig(): AudioOutConfig {
  return { encoding: 0, sampleRateHertz: 0, volumePercentage: 0 };
}

export const AudioOutConfig: MessageFns<AudioOutConfig> = {
  encode(message: AudioOutConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.encoding !== 0) {
      writer.uint32(8).int32(message.encoding);
    }
    if (message.sampleRateHertz !== 0) {
      writer.uint32(16).int32(message.sampleRateHertz);
    }
    if (message.volumePercentage !== 0) {
      writer.uint32(24).int32(message.volumePercentage);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AudioOutConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAudioOutConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.encoding = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.sampleRateHertz = reader.int32();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.volumePercentage = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AudioOutConfig {
    return {
      encoding: isSet(object.encoding) ? audioOutConfig_EncodingFromJSON(object.encoding) : 0,
      sampleRateHertz: isSet(object.sampleRateHertz) ? globalThis.Number(object.sampleRateHertz) : 0,
      volumePercentage: isSet(object.volumePercentage) ? globalThis.Number(object.volumePercentage) : 0,
    };
  },

  toJSON(message: AudioOutConfig): unknown {
    const obj: any = {};
    if (message.encoding !== 0) {
      obj.encoding = audioOutConfig_EncodingToJSON(message.encoding);
    }
    if (message.sampleRateHertz !== 0) {
      obj.sampleRateHertz = Math.round(message.sampleRateHertz);
    }
    if (message.volumePercentage !== 0) {
      obj.volumePercentage = Math.round(message.volumePercentage);
    }
    return obj;
  },

  create(base?: DeepPartial<AudioOutConfig>): AudioOutConfig {
    return AudioOutConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AudioOutConfig>): AudioOutConfig {
    const message = createBaseAudioOutConfig();
    message.encoding = object.encoding ?? 0;
    message.sampleRateHertz = object.sampleRateHertz ?? 0;
    message.volumePercentage = object.volumePercentage ?? 0;
    return message;
  },
};

function createBaseScreenOutConfig(): ScreenOutConfig {
  return { screenMode: 0 };
}

export const ScreenOutConfig: MessageFns<ScreenOutConfig> = {
  encode(message: ScreenOutConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.screenMode !== 0) {
      writer.uint32(8).int32(message.screenMode);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ScreenOutConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseScreenOutConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.screenMode = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ScreenOutConfig {
    return { screenMode: isSet(object.screenMode) ? screenOutConfig_ScreenModeFromJSON(object.screenMode) : 0 };
  },

  toJSON(message: ScreenOutConfig): unknown {
    const obj: any = {};
    if (message.screenMode !== 0) {
      obj.screenMode = screenOutConfig_ScreenModeToJSON(message.screenMode);
    }
    return obj;
  },

  create(base?: DeepPartial<ScreenOutConfig>): ScreenOutConfig {
    return ScreenOutConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ScreenOutConfig>): ScreenOutConfig {
    const message = createBaseScreenOutConfig();
    message.screenMode = object.screenMode ?? 0;
    return message;
  },
};

function createBaseDialogStateIn(): DialogStateIn {
  return { conversationState: Buffer.alloc(0), languageCode: "", deviceLocation: undefined, isNewConversation: false };
}

export const DialogStateIn: MessageFns<DialogStateIn> = {
  encode(message: DialogStateIn, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.conversationState.length !== 0) {
      writer.uint32(10).bytes(message.conversationState);
    }
    if (message.languageCode !== "") {
      writer.uint32(18).string(message.languageCode);
    }
    if (message.deviceLocation !== undefined) {
      DeviceLocation.encode(message.deviceLocation, writer.uint32(42).fork()).join();
    }
    if (message.isNewConversation !== false) {
      writer.uint32(56).bool(message.isNewConversation);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DialogStateIn {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDialogStateIn();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.conversationState = Buffer.from(reader.bytes());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.languageCode = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.deviceLocation = DeviceLocation.decode(reader, reader.uint32());
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.isNewConversation = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DialogStateIn {
    return {
      conversationState: isSet(object.conversationState)
        ? Buffer.from(bytesFromBase64(object.conversationState))
        : Buffer.alloc(0),
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
      deviceLocation: isSet(object.deviceLocation) ? DeviceLocation.fromJSON(object.deviceLocation) : undefined,
      isNewConversation: isSet(object.isNewConversation) ? globalThis.Boolean(object.isNewConversation) : false,
    };
  },

  toJSON(message: DialogStateIn): unknown {
    const obj: any = {};
    if (message.conversationState.length !== 0) {
      obj.conversationState = base64FromBytes(message.conversationState);
    }
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    if (message.deviceLocation !== undefined) {
      obj.deviceLocation = DeviceLocation.toJSON(message.deviceLocation);
    }
    if (message.isNewConversation !== false) {
      obj.isNewConversation = message.isNewConversation;
    }
    return obj;
  },

  create(base?: DeepPartial<DialogStateIn>): DialogStateIn {
    return DialogStateIn.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DialogStateIn>): DialogStateIn {
    const message = createBaseDialogStateIn();
    message.conversationState = object.conversationState ?? Buffer.alloc(0);
    message.languageCode = object.languageCode ?? "";
    message.deviceLocation = (object.deviceLocation !== undefined && object.deviceLocation !== null)
      ? DeviceLocation.fromPartial(object.deviceLocation)
      : undefined;
    message.isNewConversation = object.isNewConversation ?? false;
    return message;
  },
};

function createBaseDeviceConfig(): DeviceConfig {
  return { deviceId: "", deviceModelId: "" };
}

export const DeviceConfig: MessageFns<DeviceConfig> = {
  encode(message: DeviceConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.deviceId !== "") {
      writer.uint32(10).string(message.deviceId);
    }
    if (message.deviceModelId !== "") {
      writer.uint32(26).string(message.deviceModelId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeviceConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeviceConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.deviceId = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.deviceModelId = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeviceConfig {
    return {
      deviceId: isSet(object.deviceId) ? globalThis.String(object.deviceId) : "",
      deviceModelId: isSet(object.deviceModelId) ? globalThis.String(object.deviceModelId) : "",
    };
  },

  toJSON(message: DeviceConfig): unknown {
    const obj: any = {};
    if (message.deviceId !== "") {
      obj.deviceId = message.deviceId;
    }
    if (message.deviceModelId !== "") {
      obj.deviceModelId = message.deviceModelId;
    }
    return obj;
  },

  create(base?: DeepPartial<DeviceConfig>): DeviceConfig {
    return DeviceConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeviceConfig>): DeviceConfig {
    const message = createBaseDeviceConfig();
    message.deviceId = object.deviceId ?? "";
    message.deviceModelId = object.deviceModelId ?? "";
    return message;
  },
};

function createBaseAudioOut(): AudioOut {
  return { audioData: Buffer.alloc(0) };
}

export const AudioOut: MessageFns<AudioOut> = {
  encode(message: AudioOut, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.audioData.length !== 0) {
      writer.uint32(10).bytes(message.audioData);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AudioOut {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAudioOut();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.audioData = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AudioOut {
    return { audioData: isSet(object.audioData) ? Buffer.from(bytesFromBase64(object.audioData)) : Buffer.alloc(0) };
  },

  toJSON(message: AudioOut): unknown {
    const obj: any = {};
    if (message.audioData.length !== 0) {
      obj.audioData = base64FromBytes(message.audioData);
    }
    return obj;
  },

  create(base?: DeepPartial<AudioOut>): AudioOut {
    return AudioOut.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<AudioOut>): AudioOut {
    const message = createBaseAudioOut();
    message.audioData = object.audioData ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseScreenOut(): ScreenOut {
  return { format: 0, data: Buffer.alloc(0) };
}

export const ScreenOut: MessageFns<ScreenOut> = {
  encode(message: ScreenOut, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.format !== 0) {
      writer.uint32(8).int32(message.format);
    }
    if (message.data.length !== 0) {
      writer.uint32(18).bytes(message.data);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ScreenOut {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseScreenOut();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.format = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.data = Buffer.from(reader.bytes());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ScreenOut {
    return {
      format: isSet(object.format) ? screenOut_FormatFromJSON(object.format) : 0,
      data: isSet(object.data) ? Buffer.from(bytesFromBase64(object.data)) : Buffer.alloc(0),
    };
  },

  toJSON(message: ScreenOut): unknown {
    const obj: any = {};
    if (message.format !== 0) {
      obj.format = screenOut_FormatToJSON(message.format);
    }
    if (message.data.length !== 0) {
      obj.data = base64FromBytes(message.data);
    }
    return obj;
  },

  create(base?: DeepPartial<ScreenOut>): ScreenOut {
    return ScreenOut.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ScreenOut>): ScreenOut {
    const message = createBaseScreenOut();
    message.format = object.format ?? 0;
    message.data = object.data ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseDeviceAction(): DeviceAction {
  return { deviceRequestJson: "" };
}

export const DeviceAction: MessageFns<DeviceAction> = {
  encode(message: DeviceAction, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.deviceRequestJson !== "") {
      writer.uint32(10).string(message.deviceRequestJson);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeviceAction {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeviceAction();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.deviceRequestJson = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeviceAction {
    return { deviceRequestJson: isSet(object.deviceRequestJson) ? globalThis.String(object.deviceRequestJson) : "" };
  },

  toJSON(message: DeviceAction): unknown {
    const obj: any = {};
    if (message.deviceRequestJson !== "") {
      obj.deviceRequestJson = message.deviceRequestJson;
    }
    return obj;
  },

  create(base?: DeepPartial<DeviceAction>): DeviceAction {
    return DeviceAction.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeviceAction>): DeviceAction {
    const message = createBaseDeviceAction();
    message.deviceRequestJson = object.deviceRequestJson ?? "";
    return message;
  },
};

function createBaseSpeechRecognitionResult(): SpeechRecognitionResult {
  return { transcript: "", stability: 0 };
}

export const SpeechRecognitionResult: MessageFns<SpeechRecognitionResult> = {
  encode(message: SpeechRecognitionResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.transcript !== "") {
      writer.uint32(10).string(message.transcript);
    }
    if (message.stability !== 0) {
      writer.uint32(21).float(message.stability);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechRecognitionResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechRecognitionResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.transcript = reader.string();
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.stability = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechRecognitionResult {
    return {
      transcript: isSet(object.transcript) ? globalThis.String(object.transcript) : "",
      stability: isSet(object.stability) ? globalThis.Number(object.stability) : 0,
    };
  },

  toJSON(message: SpeechRecognitionResult): unknown {
    const obj: any = {};
    if (message.transcript !== "") {
      obj.transcript = message.transcript;
    }
    if (message.stability !== 0) {
      obj.stability = message.stability;
    }
    return obj;
  },

  create(base?: DeepPartial<SpeechRecognitionResult>): SpeechRecognitionResult {
    return SpeechRecognitionResult.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<SpeechRecognitionResult>): SpeechRecognitionResult {
    const message = createBaseSpeechRecognitionResult();
    message.transcript = object.transcript ?? "";
    message.stability = object.stability ?? 0;
    return message;
  },
};

function createBaseDialogStateOut(): DialogStateOut {
  return { supplementalDisplayText: "", conversationState: Buffer.alloc(0), microphoneMode: 0, volumePercentage: 0 };
}

export const DialogStateOut: MessageFns<DialogStateOut> = {
  encode(message: DialogStateOut, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.supplementalDisplayText !== "") {
      writer.uint32(10).string(message.supplementalDisplayText);
    }
    if (message.conversationState.length !== 0) {
      writer.uint32(18).bytes(message.conversationState);
    }
    if (message.microphoneMode !== 0) {
      writer.uint32(24).int32(message.microphoneMode);
    }
    if (message.volumePercentage !== 0) {
      writer.uint32(32).int32(message.volumePercentage);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DialogStateOut {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDialogStateOut();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.supplementalDisplayText = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.conversationState = Buffer.from(reader.bytes());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.microphoneMode = reader.int32() as any;
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.volumePercentage = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DialogStateOut {
    return {
      supplementalDisplayText: isSet(object.supplementalDisplayText)
        ? globalThis.String(object.supplementalDisplayText)
        : "",
      conversationState: isSet(object.conversationState)
        ? Buffer.from(bytesFromBase64(object.conversationState))
        : Buffer.alloc(0),
      microphoneMode: isSet(object.microphoneMode) ? dialogStateOut_MicrophoneModeFromJSON(object.microphoneMode) : 0,
      volumePercentage: isSet(object.volumePercentage) ? globalThis.Number(object.volumePercentage) : 0,
    };
  },

  toJSON(message: DialogStateOut): unknown {
    const obj: any = {};
    if (message.supplementalDisplayText !== "") {
      obj.supplementalDisplayText = message.supplementalDisplayText;
    }
    if (message.conversationState.length !== 0) {
      obj.conversationState = base64FromBytes(message.conversationState);
    }
    if (message.microphoneMode !== 0) {
      obj.microphoneMode = dialogStateOut_MicrophoneModeToJSON(message.microphoneMode);
    }
    if (message.volumePercentage !== 0) {
      obj.volumePercentage = Math.round(message.volumePercentage);
    }
    return obj;
  },

  create(base?: DeepPartial<DialogStateOut>): DialogStateOut {
    return DialogStateOut.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DialogStateOut>): DialogStateOut {
    const message = createBaseDialogStateOut();
    message.supplementalDisplayText = object.supplementalDisplayText ?? "";
    message.conversationState = object.conversationState ?? Buffer.alloc(0);
    message.microphoneMode = object.microphoneMode ?? 0;
    message.volumePercentage = object.volumePercentage ?? 0;
    return message;
  },
};

function createBaseDebugConfig(): DebugConfig {
  return { returnDebugInfo: false };
}

export const DebugConfig: MessageFns<DebugConfig> = {
  encode(message: DebugConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.returnDebugInfo !== false) {
      writer.uint32(48).bool(message.returnDebugInfo);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DebugConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDebugConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 6:
          if (tag !== 48) {
            break;
          }

          message.returnDebugInfo = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DebugConfig {
    return { returnDebugInfo: isSet(object.returnDebugInfo) ? globalThis.Boolean(object.returnDebugInfo) : false };
  },

  toJSON(message: DebugConfig): unknown {
    const obj: any = {};
    if (message.returnDebugInfo !== false) {
      obj.returnDebugInfo = message.returnDebugInfo;
    }
    return obj;
  },

  create(base?: DeepPartial<DebugConfig>): DebugConfig {
    return DebugConfig.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DebugConfig>): DebugConfig {
    const message = createBaseDebugConfig();
    message.returnDebugInfo = object.returnDebugInfo ?? false;
    return message;
  },
};

function createBaseDeviceLocation(): DeviceLocation {
  return { coordinates: undefined };
}

export const DeviceLocation: MessageFns<DeviceLocation> = {
  encode(message: DeviceLocation, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.coordinates !== undefined) {
      LatLng.encode(message.coordinates, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DeviceLocation {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDeviceLocation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.coordinates = LatLng.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DeviceLocation {
    return { coordinates: isSet(object.coordinates) ? LatLng.fromJSON(object.coordinates) : undefined };
  },

  toJSON(message: DeviceLocation): unknown {
    const obj: any = {};
    if (message.coordinates !== undefined) {
      obj.coordinates = LatLng.toJSON(message.coordinates);
    }
    return obj;
  },

  create(base?: DeepPartial<DeviceLocation>): DeviceLocation {
    return DeviceLocation.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DeviceLocation>): DeviceLocation {
    const message = createBaseDeviceLocation();
    message.coordinates = (object.coordinates !== undefined && object.coordinates !== null)
      ? LatLng.fromPartial(object.coordinates)
      : undefined;
    return message;
  },
};

/** Service that implements the Google Assistant API. */
export type EmbeddedAssistantDefinition = typeof EmbeddedAssistantDefinition;
export const EmbeddedAssistantDefinition = {
  name: "EmbeddedAssistant",
  fullName: "google.assistant.embedded.v1alpha2.EmbeddedAssistant",
  methods: {
    /**
     * Initiates or continues a conversation with the embedded Assistant Service.
     * Each call performs one round-trip, sending an audio request to the service
     * and receiving the audio response. Uses bidirectional streaming to receive
     * results, such as the `END_OF_UTTERANCE` event, while sending audio.
     *
     * A conversation is one or more gRPC connections, each consisting of several
     * streamed requests and responses.
     * For example, the user says *Add to my shopping list* and the Assistant
     * responds *What do you want to add?*. The sequence of streamed requests and
     * responses in the first gRPC message could be:
     *
     * *   AssistRequest.config
     * *   AssistRequest.audio_in
     * *   AssistRequest.audio_in
     * *   AssistRequest.audio_in
     * *   AssistRequest.audio_in
     * *   AssistResponse.event_type.END_OF_UTTERANCE
     * *   AssistResponse.speech_results.transcript "add to my shopping list"
     * *   AssistResponse.dialog_state_out.microphone_mode.DIALOG_FOLLOW_ON
     * *   AssistResponse.audio_out
     * *   AssistResponse.audio_out
     * *   AssistResponse.audio_out
     *
     * The user then says *bagels* and the Assistant responds
     * *OK, I've added bagels to your shopping list*. This is sent as another gRPC
     * connection call to the `Assist` method, again with streamed requests and
     * responses, such as:
     *
     * *   AssistRequest.config
     * *   AssistRequest.audio_in
     * *   AssistRequest.audio_in
     * *   AssistRequest.audio_in
     * *   AssistResponse.event_type.END_OF_UTTERANCE
     * *   AssistResponse.dialog_state_out.microphone_mode.CLOSE_MICROPHONE
     * *   AssistResponse.audio_out
     * *   AssistResponse.audio_out
     * *   AssistResponse.audio_out
     * *   AssistResponse.audio_out
     *
     * Although the precise order of responses is not guaranteed, sequential
     * `AssistResponse.audio_out` messages will always contain sequential portions
     * of audio.
     */
    assist: {
      name: "Assist",
      requestType: AssistRequest,
      requestStream: true,
      responseType: AssistResponse,
      responseStream: true,
      options: {},
    },
  },
} as const;

export interface EmbeddedAssistantServiceImplementation<CallContextExt = {}> {
  /**
   * Initiates or continues a conversation with the embedded Assistant Service.
   * Each call performs one round-trip, sending an audio request to the service
   * and receiving the audio response. Uses bidirectional streaming to receive
   * results, such as the `END_OF_UTTERANCE` event, while sending audio.
   *
   * A conversation is one or more gRPC connections, each consisting of several
   * streamed requests and responses.
   * For example, the user says *Add to my shopping list* and the Assistant
   * responds *What do you want to add?*. The sequence of streamed requests and
   * responses in the first gRPC message could be:
   *
   * *   AssistRequest.config
   * *   AssistRequest.audio_in
   * *   AssistRequest.audio_in
   * *   AssistRequest.audio_in
   * *   AssistRequest.audio_in
   * *   AssistResponse.event_type.END_OF_UTTERANCE
   * *   AssistResponse.speech_results.transcript "add to my shopping list"
   * *   AssistResponse.dialog_state_out.microphone_mode.DIALOG_FOLLOW_ON
   * *   AssistResponse.audio_out
   * *   AssistResponse.audio_out
   * *   AssistResponse.audio_out
   *
   * The user then says *bagels* and the Assistant responds
   * *OK, I've added bagels to your shopping list*. This is sent as another gRPC
   * connection call to the `Assist` method, again with streamed requests and
   * responses, such as:
   *
   * *   AssistRequest.config
   * *   AssistRequest.audio_in
   * *   AssistRequest.audio_in
   * *   AssistRequest.audio_in
   * *   AssistResponse.event_type.END_OF_UTTERANCE
   * *   AssistResponse.dialog_state_out.microphone_mode.CLOSE_MICROPHONE
   * *   AssistResponse.audio_out
   * *   AssistResponse.audio_out
   * *   AssistResponse.audio_out
   * *   AssistResponse.audio_out
   *
   * Although the precise order of responses is not guaranteed, sequential
   * `AssistResponse.audio_out` messages will always contain sequential portions
   * of audio.
   */
  assist(
    request: AsyncIterable<AssistRequest>,
    context: CallContext & CallContextExt,
  ): ServerStreamingMethodResult<DeepPartial<AssistResponse>>;
}

export interface EmbeddedAssistantClient<CallOptionsExt = {}> {
  /**
   * Initiates or continues a conversation with the embedded Assistant Service.
   * Each call performs one round-trip, sending an audio request to the service
   * and receiving the audio response. Uses bidirectional streaming to receive
   * results, such as the `END_OF_UTTERANCE` event, while sending audio.
   *
   * A conversation is one or more gRPC connections, each consisting of several
   * streamed requests and responses.
   * For example, the user says *Add to my shopping list* and the Assistant
   * responds *What do you want to add?*. The sequence of streamed requests and
   * responses in the first gRPC message could be:
   *
   * *   AssistRequest.config
   * *   AssistRequest.audio_in
   * *   AssistRequest.audio_in
   * *   AssistRequest.audio_in
   * *   AssistRequest.audio_in
   * *   AssistResponse.event_type.END_OF_UTTERANCE
   * *   AssistResponse.speech_results.transcript "add to my shopping list"
   * *   AssistResponse.dialog_state_out.microphone_mode.DIALOG_FOLLOW_ON
   * *   AssistResponse.audio_out
   * *   AssistResponse.audio_out
   * *   AssistResponse.audio_out
   *
   * The user then says *bagels* and the Assistant responds
   * *OK, I've added bagels to your shopping list*. This is sent as another gRPC
   * connection call to the `Assist` method, again with streamed requests and
   * responses, such as:
   *
   * *   AssistRequest.config
   * *   AssistRequest.audio_in
   * *   AssistRequest.audio_in
   * *   AssistRequest.audio_in
   * *   AssistResponse.event_type.END_OF_UTTERANCE
   * *   AssistResponse.dialog_state_out.microphone_mode.CLOSE_MICROPHONE
   * *   AssistResponse.audio_out
   * *   AssistResponse.audio_out
   * *   AssistResponse.audio_out
   * *   AssistResponse.audio_out
   *
   * Although the precise order of responses is not guaranteed, sequential
   * `AssistResponse.audio_out` messages will always contain sequential portions
   * of audio.
   */
  assist(
    request: AsyncIterable<DeepPartial<AssistRequest>>,
    options?: CallOptions & CallOptionsExt,
  ): AsyncIterable<AssistResponse>;
}

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends Long ? string | number | Long : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export type ServerStreamingMethodResult<Response> = { [Symbol.asyncIterator](): AsyncIterator<Response, void> };

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
