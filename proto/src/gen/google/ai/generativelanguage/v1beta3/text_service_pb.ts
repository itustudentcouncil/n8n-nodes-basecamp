// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/ai/generativelanguage/v1beta3/text_service.proto (package google.ai.generativelanguage.v1beta3, syntax proto3)
/* eslint-disable */

import type { GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import type { CitationMetadata } from "./citation_pb";
import { file_google_ai_generativelanguage_v1beta3_citation } from "./citation_pb";
import type { ContentFilter, SafetyFeedback, SafetyRating, SafetySetting } from "./safety_pb";
import { file_google_ai_generativelanguage_v1beta3_safety } from "./safety_pb";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import { file_google_api_resource } from "../../../api/resource_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/ai/generativelanguage/v1beta3/text_service.proto.
 */
export const file_google_ai_generativelanguage_v1beta3_text_service: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_ai_generativelanguage_v1beta3_citation, file_google_ai_generativelanguage_v1beta3_safety, file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_api_resource]);

/**
 * Request to generate a text completion response from the model.
 *
 * @generated from message google.ai.generativelanguage.v1beta3.GenerateTextRequest
 */
export type GenerateTextRequest = Message<"google.ai.generativelanguage.v1beta3.GenerateTextRequest"> & {
  /**
   * Required. The name of the `Model` or `TunedModel` to use for generating the
   * completion.
   * Examples:
   *  models/text-bison-001
   *  tunedModels/sentence-translator-u3b7m
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Required. The free-form input text given to the model as a prompt.
   *
   * Given a prompt, the model will generate a TextCompletion response it
   * predicts as the completion of the input text.
   *
   * @generated from field: google.ai.generativelanguage.v1beta3.TextPrompt prompt = 2;
   */
  prompt?: TextPrompt;

  /**
   * Optional. Controls the randomness of the output.
   * Note: The default value varies by model, see the `Model.temperature`
   * attribute of the `Model` returned the `getModel` function.
   *
   * Values can range from [0.0,1.0],
   * inclusive. A value closer to 1.0 will produce responses that are more
   * varied and creative, while a value closer to 0.0 will typically result in
   * more straightforward responses from the model.
   *
   * @generated from field: optional float temperature = 3;
   */
  temperature?: number;

  /**
   * Optional. Number of generated responses to return.
   *
   * This value must be between [1, 8], inclusive. If unset, this will default
   * to 1.
   *
   * @generated from field: optional int32 candidate_count = 4;
   */
  candidateCount?: number;

  /**
   * Optional. The maximum number of tokens to include in a candidate.
   *
   * If unset, this will default to output_token_limit specified in the `Model`
   * specification.
   *
   * @generated from field: optional int32 max_output_tokens = 5;
   */
  maxOutputTokens?: number;

  /**
   * Optional. The maximum cumulative probability of tokens to consider when
   * sampling.
   *
   * The model uses combined Top-k and nucleus sampling.
   *
   * Tokens are sorted based on their assigned probabilities so that only the
   * most likely tokens are considered. Top-k sampling directly limits the
   * maximum number of tokens to consider, while Nucleus sampling limits number
   * of tokens based on the cumulative probability.
   *
   * Note: The default value varies by model, see the `Model.top_p`
   * attribute of the `Model` returned the `getModel` function.
   *
   * @generated from field: optional float top_p = 6;
   */
  topP?: number;

  /**
   * Optional. The maximum number of tokens to consider when sampling.
   *
   * The model uses combined Top-k and nucleus sampling.
   *
   * Top-k sampling considers the set of `top_k` most probable tokens.
   * Defaults to 40.
   *
   * Note: The default value varies by model, see the `Model.top_k`
   * attribute of the `Model` returned the `getModel` function.
   *
   * @generated from field: optional int32 top_k = 7;
   */
  topK?: number;

  /**
   * A list of unique `SafetySetting` instances for blocking unsafe content.
   *
   * that will be enforced on the `GenerateTextRequest.prompt` and
   * `GenerateTextResponse.candidates`. There should not be more than one
   * setting for each `SafetyCategory` type. The API will block any prompts and
   * responses that fail to meet the thresholds set by these settings. This list
   * overrides the default settings for each `SafetyCategory` specified in the
   * safety_settings. If there is no `SafetySetting` for a given
   * `SafetyCategory` provided in the list, the API will use the default safety
   * setting for that category.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta3.SafetySetting safety_settings = 8;
   */
  safetySettings: SafetySetting[];

  /**
   * The set of character sequences (up to 5) that will stop output generation.
   * If specified, the API will stop at the first appearance of a stop
   * sequence. The stop sequence will not be included as part of the response.
   *
   * @generated from field: repeated string stop_sequences = 9;
   */
  stopSequences: string[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta3.GenerateTextRequest.
 * Use `create(GenerateTextRequestSchema)` to create a new message.
 */
export const GenerateTextRequestSchema: GenMessage<GenerateTextRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta3_text_service, 0);

/**
 * The response from the model, including candidate completions.
 *
 * @generated from message google.ai.generativelanguage.v1beta3.GenerateTextResponse
 */
export type GenerateTextResponse = Message<"google.ai.generativelanguage.v1beta3.GenerateTextResponse"> & {
  /**
   * Candidate responses from the model.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta3.TextCompletion candidates = 1;
   */
  candidates: TextCompletion[];

  /**
   * A set of content filtering metadata for the prompt and response
   * text.
   *
   * This indicates which `SafetyCategory`(s) blocked a
   * candidate from this response, the lowest `HarmProbability`
   * that triggered a block, and the HarmThreshold setting for that category.
   * This indicates the smallest change to the `SafetySettings` that would be
   * necessary to unblock at least 1 response.
   *
   * The blocking is configured by the `SafetySettings` in the request (or the
   * default `SafetySettings` of the API).
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta3.ContentFilter filters = 3;
   */
  filters: ContentFilter[];

  /**
   * Returns any safety feedback related to content filtering.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta3.SafetyFeedback safety_feedback = 4;
   */
  safetyFeedback: SafetyFeedback[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta3.GenerateTextResponse.
 * Use `create(GenerateTextResponseSchema)` to create a new message.
 */
export const GenerateTextResponseSchema: GenMessage<GenerateTextResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta3_text_service, 1);

/**
 * Text given to the model as a prompt.
 *
 * The Model will use this TextPrompt to Generate a text completion.
 *
 * @generated from message google.ai.generativelanguage.v1beta3.TextPrompt
 */
export type TextPrompt = Message<"google.ai.generativelanguage.v1beta3.TextPrompt"> & {
  /**
   * Required. The prompt text.
   *
   * @generated from field: string text = 1;
   */
  text: string;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta3.TextPrompt.
 * Use `create(TextPromptSchema)` to create a new message.
 */
export const TextPromptSchema: GenMessage<TextPrompt> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta3_text_service, 2);

/**
 * Output text returned from a model.
 *
 * @generated from message google.ai.generativelanguage.v1beta3.TextCompletion
 */
export type TextCompletion = Message<"google.ai.generativelanguage.v1beta3.TextCompletion"> & {
  /**
   * Output only. The generated text returned from the model.
   *
   * @generated from field: string output = 1;
   */
  output: string;

  /**
   * Ratings for the safety of a response.
   *
   * There is at most one rating per category.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta3.SafetyRating safety_ratings = 2;
   */
  safetyRatings: SafetyRating[];

  /**
   * Output only. Citation information for model-generated `output` in this
   * `TextCompletion`.
   *
   * This field may be populated with attribution information for any text
   * included in the `output`.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta3.CitationMetadata citation_metadata = 3;
   */
  citationMetadata?: CitationMetadata;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta3.TextCompletion.
 * Use `create(TextCompletionSchema)` to create a new message.
 */
export const TextCompletionSchema: GenMessage<TextCompletion> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta3_text_service, 3);

/**
 * Request to get a text embedding from the model.
 *
 * @generated from message google.ai.generativelanguage.v1beta3.EmbedTextRequest
 */
export type EmbedTextRequest = Message<"google.ai.generativelanguage.v1beta3.EmbedTextRequest"> & {
  /**
   * Required. The model name to use with the format model=models/{model}.
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Required. The free-form input text that the model will turn into an
   * embedding.
   *
   * @generated from field: string text = 2;
   */
  text: string;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta3.EmbedTextRequest.
 * Use `create(EmbedTextRequestSchema)` to create a new message.
 */
export const EmbedTextRequestSchema: GenMessage<EmbedTextRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta3_text_service, 4);

/**
 * The response to a EmbedTextRequest.
 *
 * @generated from message google.ai.generativelanguage.v1beta3.EmbedTextResponse
 */
export type EmbedTextResponse = Message<"google.ai.generativelanguage.v1beta3.EmbedTextResponse"> & {
  /**
   * Output only. The embedding generated from the input text.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta3.Embedding embedding = 1;
   */
  embedding?: Embedding;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta3.EmbedTextResponse.
 * Use `create(EmbedTextResponseSchema)` to create a new message.
 */
export const EmbedTextResponseSchema: GenMessage<EmbedTextResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta3_text_service, 5);

/**
 * Batch request to get a text embedding from the model.
 *
 * @generated from message google.ai.generativelanguage.v1beta3.BatchEmbedTextRequest
 */
export type BatchEmbedTextRequest = Message<"google.ai.generativelanguage.v1beta3.BatchEmbedTextRequest"> & {
  /**
   * Required. The name of the `Model` to use for generating the embedding.
   * Examples:
   *  models/embedding-gecko-001
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Required. The free-form input texts that the model will turn into an
   * embedding.  The current limit is 100 texts, over which an error will be
   * thrown.
   *
   * @generated from field: repeated string texts = 2;
   */
  texts: string[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta3.BatchEmbedTextRequest.
 * Use `create(BatchEmbedTextRequestSchema)` to create a new message.
 */
export const BatchEmbedTextRequestSchema: GenMessage<BatchEmbedTextRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta3_text_service, 6);

/**
 * The response to a EmbedTextRequest.
 *
 * @generated from message google.ai.generativelanguage.v1beta3.BatchEmbedTextResponse
 */
export type BatchEmbedTextResponse = Message<"google.ai.generativelanguage.v1beta3.BatchEmbedTextResponse"> & {
  /**
   * Output only. The embeddings generated from the input text.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta3.Embedding embeddings = 1;
   */
  embeddings: Embedding[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta3.BatchEmbedTextResponse.
 * Use `create(BatchEmbedTextResponseSchema)` to create a new message.
 */
export const BatchEmbedTextResponseSchema: GenMessage<BatchEmbedTextResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta3_text_service, 7);

/**
 * A list of floats representing the embedding.
 *
 * @generated from message google.ai.generativelanguage.v1beta3.Embedding
 */
export type Embedding = Message<"google.ai.generativelanguage.v1beta3.Embedding"> & {
  /**
   * The embedding values.
   *
   * @generated from field: repeated float value = 1;
   */
  value: number[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta3.Embedding.
 * Use `create(EmbeddingSchema)` to create a new message.
 */
export const EmbeddingSchema: GenMessage<Embedding> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta3_text_service, 8);

/**
 * Counts the number of tokens in the `prompt` sent to a model.
 *
 * Models may tokenize text differently, so each model may return a different
 * `token_count`.
 *
 * @generated from message google.ai.generativelanguage.v1beta3.CountTextTokensRequest
 */
export type CountTextTokensRequest = Message<"google.ai.generativelanguage.v1beta3.CountTextTokensRequest"> & {
  /**
   * Required. The model's resource name. This serves as an ID for the Model to
   * use.
   *
   * This name should match a model name returned by the `ListModels` method.
   *
   * Format: `models/{model}`
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Required. The free-form input text given to the model as a prompt.
   *
   * @generated from field: google.ai.generativelanguage.v1beta3.TextPrompt prompt = 2;
   */
  prompt?: TextPrompt;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta3.CountTextTokensRequest.
 * Use `create(CountTextTokensRequestSchema)` to create a new message.
 */
export const CountTextTokensRequestSchema: GenMessage<CountTextTokensRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta3_text_service, 9);

/**
 * A response from `CountTextTokens`.
 *
 * It returns the model's `token_count` for the `prompt`.
 *
 * @generated from message google.ai.generativelanguage.v1beta3.CountTextTokensResponse
 */
export type CountTextTokensResponse = Message<"google.ai.generativelanguage.v1beta3.CountTextTokensResponse"> & {
  /**
   * The number of tokens that the `model` tokenizes the `prompt` into.
   *
   * Always non-negative.
   *
   * @generated from field: int32 token_count = 1;
   */
  tokenCount: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta3.CountTextTokensResponse.
 * Use `create(CountTextTokensResponseSchema)` to create a new message.
 */
export const CountTextTokensResponseSchema: GenMessage<CountTextTokensResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta3_text_service, 10);

/**
 * API for using Generative Language Models (GLMs) trained to generate text.
 *
 * Also known as Large Language Models (LLM)s, these generate text given an
 * input prompt from the user.
 *
 * @generated from service google.ai.generativelanguage.v1beta3.TextService
 */
export const TextService: GenService<{
  /**
   * Generates a response from the model given an input message.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta3.TextService.GenerateText
   */
  generateText: {
    methodKind: "unary";
    input: typeof GenerateTextRequestSchema;
    output: typeof GenerateTextResponseSchema;
  },
  /**
   * Generates an embedding from the model given an input message.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta3.TextService.EmbedText
   */
  embedText: {
    methodKind: "unary";
    input: typeof EmbedTextRequestSchema;
    output: typeof EmbedTextResponseSchema;
  },
  /**
   * Generates multiple embeddings from the model given input text in a
   * synchronous call.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta3.TextService.BatchEmbedText
   */
  batchEmbedText: {
    methodKind: "unary";
    input: typeof BatchEmbedTextRequestSchema;
    output: typeof BatchEmbedTextResponseSchema;
  },
  /**
   * Runs a model's tokenizer on a text and returns the token count.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta3.TextService.CountTextTokens
   */
  countTextTokens: {
    methodKind: "unary";
    input: typeof CountTextTokensRequestSchema;
    output: typeof CountTextTokensResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_ai_generativelanguage_v1beta3_text_service, 0);

