// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/ai/generativelanguage/v1/safety.proto (package google.ai.generativelanguage.v1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/ai/generativelanguage/v1/safety.proto.
 */
export const file_google_ai_generativelanguage_v1_safety: GenFile = /*@__PURE__*/
  fileDesc("Cixnb29nbGUvYWkvZ2VuZXJhdGl2ZWxhbmd1YWdlL3YxL3NhZmV0eS5wcm90bxIfZ29vZ2xlLmFpLmdlbmVyYXRpdmVsYW5ndWFnZS52MSKiAgoMU2FmZXR5UmF0aW5nEkQKCGNhdGVnb3J5GAMgASgOMi0uZ29vZ2xlLmFpLmdlbmVyYXRpdmVsYW5ndWFnZS52MS5IYXJtQ2F0ZWdvcnlCA+BBAhJXCgtwcm9iYWJpbGl0eRgEIAEoDjI9Lmdvb2dsZS5haS5nZW5lcmF0aXZlbGFuZ3VhZ2UudjEuU2FmZXR5UmF0aW5nLkhhcm1Qcm9iYWJpbGl0eUID4EECEg8KB2Jsb2NrZWQYBSABKAgiYgoPSGFybVByb2JhYmlsaXR5EiAKHEhBUk1fUFJPQkFCSUxJVFlfVU5TUEVDSUZJRUQQABIOCgpORUdMSUdJQkxFEAESBwoDTE9XEAISCgoGTUVESVVNEAMSCAoESElHSBAEItACCg1TYWZldHlTZXR0aW5nEkQKCGNhdGVnb3J5GAMgASgOMi0uZ29vZ2xlLmFpLmdlbmVyYXRpdmVsYW5ndWFnZS52MS5IYXJtQ2F0ZWdvcnlCA+BBAhJZCgl0aHJlc2hvbGQYBCABKA4yQS5nb29nbGUuYWkuZ2VuZXJhdGl2ZWxhbmd1YWdlLnYxLlNhZmV0eVNldHRpbmcuSGFybUJsb2NrVGhyZXNob2xkQgPgQQIinQEKEkhhcm1CbG9ja1RocmVzaG9sZBIkCiBIQVJNX0JMT0NLX1RIUkVTSE9MRF9VTlNQRUNJRklFRBAAEhcKE0JMT0NLX0xPV19BTkRfQUJPVkUQARIaChZCTE9DS19NRURJVU1fQU5EX0FCT1ZFEAISEwoPQkxPQ0tfT05MWV9ISUdIEAMSDgoKQkxPQ0tfTk9ORRAEEgcKA09GRhAFKv8CCgxIYXJtQ2F0ZWdvcnkSHQoZSEFSTV9DQVRFR09SWV9VTlNQRUNJRklFRBAAEhwKGEhBUk1fQ0FURUdPUllfREVST0dBVE9SWRABEhoKFkhBUk1fQ0FURUdPUllfVE9YSUNJVFkQAhIaChZIQVJNX0NBVEVHT1JZX1ZJT0xFTkNFEAMSGAoUSEFSTV9DQVRFR09SWV9TRVhVQUwQBBIZChVIQVJNX0NBVEVHT1JZX01FRElDQUwQBRIbChdIQVJNX0NBVEVHT1JZX0RBTkdFUk9VUxAGEhwKGEhBUk1fQ0FURUdPUllfSEFSQVNTTUVOVBAHEh0KGUhBUk1fQ0FURUdPUllfSEFURV9TUEVFQ0gQCBIjCh9IQVJNX0NBVEVHT1JZX1NFWFVBTExZX0VYUExJQ0lUEAkSIwofSEFSTV9DQVRFR09SWV9EQU5HRVJPVVNfQ09OVEVOVBAKEiEKHUhBUk1fQ0FURUdPUllfQ0lWSUNfSU5URUdSSVRZEAtCjwEKI2NvbS5nb29nbGUuYWkuZ2VuZXJhdGl2ZWxhbmd1YWdlLnYxQgtTYWZldHlQcm90b1ABWlljbG91ZC5nb29nbGUuY29tL2dvL2FpL2dlbmVyYXRpdmVsYW5ndWFnZS9hcGl2MS9nZW5lcmF0aXZlbGFuZ3VhZ2VwYjtnZW5lcmF0aXZlbGFuZ3VhZ2VwYmIGcHJvdG8z", [file_google_api_field_behavior]);

/**
 * Safety rating for a piece of content.
 *
 * The safety rating contains the category of harm and the
 * harm probability level in that category for a piece of content.
 * Content is classified for safety across a number of
 * harm categories and the probability of the harm classification is included
 * here.
 *
 * @generated from message google.ai.generativelanguage.v1.SafetyRating
 */
export type SafetyRating = Message<"google.ai.generativelanguage.v1.SafetyRating"> & {
  /**
   * Required. The category for this rating.
   *
   * @generated from field: google.ai.generativelanguage.v1.HarmCategory category = 3;
   */
  category: HarmCategory;

  /**
   * Required. The probability of harm for this content.
   *
   * @generated from field: google.ai.generativelanguage.v1.SafetyRating.HarmProbability probability = 4;
   */
  probability: SafetyRating_HarmProbability;

  /**
   * Was this content blocked because of this rating?
   *
   * @generated from field: bool blocked = 5;
   */
  blocked: boolean;
};

/**
 * Describes the message google.ai.generativelanguage.v1.SafetyRating.
 * Use `create(SafetyRatingSchema)` to create a new message.
 */
export const SafetyRatingSchema: GenMessage<SafetyRating> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_safety, 0);

/**
 * The probability that a piece of content is harmful.
 *
 * The classification system gives the probability of the content being
 * unsafe. This does not indicate the severity of harm for a piece of content.
 *
 * @generated from enum google.ai.generativelanguage.v1.SafetyRating.HarmProbability
 */
export enum SafetyRating_HarmProbability {
  /**
   * Probability is unspecified.
   *
   * @generated from enum value: HARM_PROBABILITY_UNSPECIFIED = 0;
   */
  HARM_PROBABILITY_UNSPECIFIED = 0,

  /**
   * Content has a negligible chance of being unsafe.
   *
   * @generated from enum value: NEGLIGIBLE = 1;
   */
  NEGLIGIBLE = 1,

  /**
   * Content has a low chance of being unsafe.
   *
   * @generated from enum value: LOW = 2;
   */
  LOW = 2,

  /**
   * Content has a medium chance of being unsafe.
   *
   * @generated from enum value: MEDIUM = 3;
   */
  MEDIUM = 3,

  /**
   * Content has a high chance of being unsafe.
   *
   * @generated from enum value: HIGH = 4;
   */
  HIGH = 4,
}

/**
 * Describes the enum google.ai.generativelanguage.v1.SafetyRating.HarmProbability.
 */
export const SafetyRating_HarmProbabilitySchema: GenEnum<SafetyRating_HarmProbability> = /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1_safety, 0, 0);

/**
 * Safety setting, affecting the safety-blocking behavior.
 *
 * Passing a safety setting for a category changes the allowed probability that
 * content is blocked.
 *
 * @generated from message google.ai.generativelanguage.v1.SafetySetting
 */
export type SafetySetting = Message<"google.ai.generativelanguage.v1.SafetySetting"> & {
  /**
   * Required. The category for this setting.
   *
   * @generated from field: google.ai.generativelanguage.v1.HarmCategory category = 3;
   */
  category: HarmCategory;

  /**
   * Required. Controls the probability threshold at which harm is blocked.
   *
   * @generated from field: google.ai.generativelanguage.v1.SafetySetting.HarmBlockThreshold threshold = 4;
   */
  threshold: SafetySetting_HarmBlockThreshold;
};

/**
 * Describes the message google.ai.generativelanguage.v1.SafetySetting.
 * Use `create(SafetySettingSchema)` to create a new message.
 */
export const SafetySettingSchema: GenMessage<SafetySetting> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_safety, 1);

/**
 * Block at and beyond a specified harm probability.
 *
 * @generated from enum google.ai.generativelanguage.v1.SafetySetting.HarmBlockThreshold
 */
export enum SafetySetting_HarmBlockThreshold {
  /**
   * Threshold is unspecified.
   *
   * @generated from enum value: HARM_BLOCK_THRESHOLD_UNSPECIFIED = 0;
   */
  HARM_BLOCK_THRESHOLD_UNSPECIFIED = 0,

  /**
   * Content with NEGLIGIBLE will be allowed.
   *
   * @generated from enum value: BLOCK_LOW_AND_ABOVE = 1;
   */
  BLOCK_LOW_AND_ABOVE = 1,

  /**
   * Content with NEGLIGIBLE and LOW will be allowed.
   *
   * @generated from enum value: BLOCK_MEDIUM_AND_ABOVE = 2;
   */
  BLOCK_MEDIUM_AND_ABOVE = 2,

  /**
   * Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed.
   *
   * @generated from enum value: BLOCK_ONLY_HIGH = 3;
   */
  BLOCK_ONLY_HIGH = 3,

  /**
   * All content will be allowed.
   *
   * @generated from enum value: BLOCK_NONE = 4;
   */
  BLOCK_NONE = 4,

  /**
   * Turn off the safety filter.
   *
   * @generated from enum value: OFF = 5;
   */
  OFF = 5,
}

/**
 * Describes the enum google.ai.generativelanguage.v1.SafetySetting.HarmBlockThreshold.
 */
export const SafetySetting_HarmBlockThresholdSchema: GenEnum<SafetySetting_HarmBlockThreshold> = /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1_safety, 1, 0);

/**
 * The category of a rating.
 *
 * These categories cover various kinds of harms that developers
 * may wish to adjust.
 *
 * @generated from enum google.ai.generativelanguage.v1.HarmCategory
 */
export enum HarmCategory {
  /**
   * Category is unspecified.
   *
   * @generated from enum value: HARM_CATEGORY_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * **PaLM** - Negative or harmful comments targeting identity and/or protected
   * attribute.
   *
   * @generated from enum value: HARM_CATEGORY_DEROGATORY = 1;
   */
  DEROGATORY = 1,

  /**
   * **PaLM** - Content that is rude, disrespectful, or profane.
   *
   * @generated from enum value: HARM_CATEGORY_TOXICITY = 2;
   */
  TOXICITY = 2,

  /**
   * **PaLM** - Describes scenarios depicting violence against an individual or
   * group, or general descriptions of gore.
   *
   * @generated from enum value: HARM_CATEGORY_VIOLENCE = 3;
   */
  VIOLENCE = 3,

  /**
   * **PaLM** - Contains references to sexual acts or other lewd content.
   *
   * @generated from enum value: HARM_CATEGORY_SEXUAL = 4;
   */
  SEXUAL = 4,

  /**
   * **PaLM** - Promotes unchecked medical advice.
   *
   * @generated from enum value: HARM_CATEGORY_MEDICAL = 5;
   */
  MEDICAL = 5,

  /**
   * **PaLM** - Dangerous content that promotes, facilitates, or encourages
   * harmful acts.
   *
   * @generated from enum value: HARM_CATEGORY_DANGEROUS = 6;
   */
  DANGEROUS = 6,

  /**
   * **Gemini** - Harassment content.
   *
   * @generated from enum value: HARM_CATEGORY_HARASSMENT = 7;
   */
  HARASSMENT = 7,

  /**
   * **Gemini** - Hate speech and content.
   *
   * @generated from enum value: HARM_CATEGORY_HATE_SPEECH = 8;
   */
  HATE_SPEECH = 8,

  /**
   * **Gemini** - Sexually explicit content.
   *
   * @generated from enum value: HARM_CATEGORY_SEXUALLY_EXPLICIT = 9;
   */
  SEXUALLY_EXPLICIT = 9,

  /**
   * **Gemini** - Dangerous content.
   *
   * @generated from enum value: HARM_CATEGORY_DANGEROUS_CONTENT = 10;
   */
  DANGEROUS_CONTENT = 10,

  /**
   * **Gemini** - Content that may be used to harm civic integrity.
   *
   * @generated from enum value: HARM_CATEGORY_CIVIC_INTEGRITY = 11;
   */
  CIVIC_INTEGRITY = 11,
}

/**
 * Describes the enum google.ai.generativelanguage.v1.HarmCategory.
 */
export const HarmCategorySchema: GenEnum<HarmCategory> = /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1_safety, 0);

