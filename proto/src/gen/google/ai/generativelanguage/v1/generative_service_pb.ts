// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/ai/generativelanguage/v1/generative_service.proto (package google.ai.generativelanguage.v1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import type { CitationMetadata } from "./citation_pb";
import { file_google_ai_generativelanguage_v1_citation } from "./citation_pb";
import type { Content } from "./content_pb";
import { file_google_ai_generativelanguage_v1_content } from "./content_pb";
import type { SafetyRating, SafetySetting } from "./safety_pb";
import { file_google_ai_generativelanguage_v1_safety } from "./safety_pb";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import { file_google_api_resource } from "../../../api/resource_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/ai/generativelanguage/v1/generative_service.proto.
 */
export const file_google_ai_generativelanguage_v1_generative_service: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_ai_generativelanguage_v1_citation, file_google_ai_generativelanguage_v1_content, file_google_ai_generativelanguage_v1_safety, file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_api_resource]);

/**
 * Request to generate a completion from the model.
 *
 * @generated from message google.ai.generativelanguage.v1.GenerateContentRequest
 */
export type GenerateContentRequest = Message<"google.ai.generativelanguage.v1.GenerateContentRequest"> & {
  /**
   * Required. The name of the `Model` to use for generating the completion.
   *
   * Format: `name=models/{model}`.
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Required. The content of the current conversation with the model.
   *
   * For single-turn queries, this is a single instance. For multi-turn queries
   * like [chat](https://ai.google.dev/gemini-api/docs/text-generation#chat),
   * this is a repeated field that contains the conversation history and the
   * latest request.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1.Content contents = 2;
   */
  contents: Content[];

  /**
   * Optional. A list of unique `SafetySetting` instances for blocking unsafe
   * content.
   *
   * This will be enforced on the `GenerateContentRequest.contents` and
   * `GenerateContentResponse.candidates`. There should not be more than one
   * setting for each `SafetyCategory` type. The API will block any contents and
   * responses that fail to meet the thresholds set by these settings. This list
   * overrides the default settings for each `SafetyCategory` specified in the
   * safety_settings. If there is no `SafetySetting` for a given
   * `SafetyCategory` provided in the list, the API will use the default safety
   * setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH,
   * HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT,
   * HARM_CATEGORY_HARASSMENT are supported. Refer to the
   * [guide](https://ai.google.dev/gemini-api/docs/safety-settings)
   * for detailed information on available safety settings. Also refer to the
   * [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance) to
   * learn how to incorporate safety considerations in your AI applications.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1.SafetySetting safety_settings = 3;
   */
  safetySettings: SafetySetting[];

  /**
   * Optional. Configuration options for model generation and outputs.
   *
   * @generated from field: optional google.ai.generativelanguage.v1.GenerationConfig generation_config = 4;
   */
  generationConfig?: GenerationConfig;
};

/**
 * Describes the message google.ai.generativelanguage.v1.GenerateContentRequest.
 * Use `create(GenerateContentRequestSchema)` to create a new message.
 */
export const GenerateContentRequestSchema: GenMessage<GenerateContentRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_generative_service, 0);

/**
 * Configuration options for model generation and outputs. Not all parameters
 * are configurable for every model.
 *
 * @generated from message google.ai.generativelanguage.v1.GenerationConfig
 */
export type GenerationConfig = Message<"google.ai.generativelanguage.v1.GenerationConfig"> & {
  /**
   * Optional. Number of generated responses to return.
   *
   * Currently, this value can only be set to 1. If unset, this will default
   * to 1.
   *
   * @generated from field: optional int32 candidate_count = 1;
   */
  candidateCount?: number;

  /**
   * Optional. The set of character sequences (up to 5) that will stop output
   * generation. If specified, the API will stop at the first appearance of a
   * `stop_sequence`. The stop sequence will not be included as part of the
   * response.
   *
   * @generated from field: repeated string stop_sequences = 2;
   */
  stopSequences: string[];

  /**
   * Optional. The maximum number of tokens to include in a response candidate.
   *
   * Note: The default value varies by model, see the `Model.output_token_limit`
   * attribute of the `Model` returned from the `getModel` function.
   *
   * @generated from field: optional int32 max_output_tokens = 4;
   */
  maxOutputTokens?: number;

  /**
   * Optional. Controls the randomness of the output.
   *
   * Note: The default value varies by model, see the `Model.temperature`
   * attribute of the `Model` returned from the `getModel` function.
   *
   * Values can range from [0.0, 2.0].
   *
   * @generated from field: optional float temperature = 5;
   */
  temperature?: number;

  /**
   * Optional. The maximum cumulative probability of tokens to consider when
   * sampling.
   *
   * The model uses combined Top-k and Top-p (nucleus) sampling.
   *
   * Tokens are sorted based on their assigned probabilities so that only the
   * most likely tokens are considered. Top-k sampling directly limits the
   * maximum number of tokens to consider, while Nucleus sampling limits the
   * number of tokens based on the cumulative probability.
   *
   * Note: The default value varies by `Model` and is specified by
   * the`Model.top_p` attribute returned from the `getModel` function. An empty
   * `top_k` attribute indicates that the model doesn't apply top-k sampling
   * and doesn't allow setting `top_k` on requests.
   *
   * @generated from field: optional float top_p = 6;
   */
  topP?: number;

  /**
   * Optional. The maximum number of tokens to consider when sampling.
   *
   * Gemini models use Top-p (nucleus) sampling or a combination of Top-k and
   * nucleus sampling. Top-k sampling considers the set of `top_k` most probable
   * tokens. Models running with nucleus sampling don't allow top_k setting.
   *
   * Note: The default value varies by `Model` and is specified by
   * the`Model.top_p` attribute returned from the `getModel` function. An empty
   * `top_k` attribute indicates that the model doesn't apply top-k sampling
   * and doesn't allow setting `top_k` on requests.
   *
   * @generated from field: optional int32 top_k = 7;
   */
  topK?: number;

  /**
   * Optional. Presence penalty applied to the next token's logprobs if the
   * token has already been seen in the response.
   *
   * This penalty is binary on/off and not dependant on the number of times the
   * token is used (after the first). Use
   * [frequency_penalty][google.ai.generativelanguage.v1.GenerationConfig.frequency_penalty]
   * for a penalty that increases with each use.
   *
   * A positive penalty will discourage the use of tokens that have already
   * been used in the response, increasing the vocabulary.
   *
   * A negative penalty will encourage the use of tokens that have already been
   * used in the response, decreasing the vocabulary.
   *
   * @generated from field: optional float presence_penalty = 15;
   */
  presencePenalty?: number;

  /**
   * Optional. Frequency penalty applied to the next token's logprobs,
   * multiplied by the number of times each token has been seen in the respponse
   * so far.
   *
   * A positive penalty will discourage the use of tokens that have already
   * been used, proportional to the number of times the token has been used:
   * The more a token is used, the more dificult it is for the model to use
   * that token again increasing the vocabulary of responses.
   *
   * Caution: A _negative_ penalty will encourage the model to reuse tokens
   * proportional to the number of times the token has been used. Small
   * negative values will reduce the vocabulary of a response. Larger negative
   * values will cause the model to start repeating a common token  until it
   * hits the
   * [max_output_tokens][google.ai.generativelanguage.v1.GenerationConfig.max_output_tokens]
   * limit: "...the the the the the...".
   *
   * @generated from field: optional float frequency_penalty = 16;
   */
  frequencyPenalty?: number;

  /**
   * Optional. If true, export the logprobs results in response.
   *
   * @generated from field: optional bool response_logprobs = 17;
   */
  responseLogprobs?: boolean;

  /**
   * Optional. Only valid if
   * [response_logprobs=True][google.ai.generativelanguage.v1.GenerationConfig.response_logprobs].
   * This sets the number of top logprobs to return at each decoding step in the
   * [Candidate.logprobs_result][google.ai.generativelanguage.v1.Candidate.logprobs_result].
   *
   * @generated from field: optional int32 logprobs = 18;
   */
  logprobs?: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1.GenerationConfig.
 * Use `create(GenerationConfigSchema)` to create a new message.
 */
export const GenerationConfigSchema: GenMessage<GenerationConfig> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_generative_service, 1);

/**
 * Response from the model supporting multiple candidate responses.
 *
 * Safety ratings and content filtering are reported for both
 * prompt in `GenerateContentResponse.prompt_feedback` and for each candidate
 * in `finish_reason` and in `safety_ratings`. The API:
 *  - Returns either all requested candidates or none of them
 *  - Returns no candidates at all only if there was something wrong with the
 *    prompt (check `prompt_feedback`)
 *  - Reports feedback on each candidate in `finish_reason` and
 *    `safety_ratings`.
 *
 * @generated from message google.ai.generativelanguage.v1.GenerateContentResponse
 */
export type GenerateContentResponse = Message<"google.ai.generativelanguage.v1.GenerateContentResponse"> & {
  /**
   * Candidate responses from the model.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1.Candidate candidates = 1;
   */
  candidates: Candidate[];

  /**
   * Returns the prompt's feedback related to the content filters.
   *
   * @generated from field: google.ai.generativelanguage.v1.GenerateContentResponse.PromptFeedback prompt_feedback = 2;
   */
  promptFeedback?: GenerateContentResponse_PromptFeedback;

  /**
   * Output only. Metadata on the generation requests' token usage.
   *
   * @generated from field: google.ai.generativelanguage.v1.GenerateContentResponse.UsageMetadata usage_metadata = 3;
   */
  usageMetadata?: GenerateContentResponse_UsageMetadata;
};

/**
 * Describes the message google.ai.generativelanguage.v1.GenerateContentResponse.
 * Use `create(GenerateContentResponseSchema)` to create a new message.
 */
export const GenerateContentResponseSchema: GenMessage<GenerateContentResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_generative_service, 2);

/**
 * A set of the feedback metadata the prompt specified in
 * `GenerateContentRequest.content`.
 *
 * @generated from message google.ai.generativelanguage.v1.GenerateContentResponse.PromptFeedback
 */
export type GenerateContentResponse_PromptFeedback = Message<"google.ai.generativelanguage.v1.GenerateContentResponse.PromptFeedback"> & {
  /**
   * Optional. If set, the prompt was blocked and no candidates are returned.
   * Rephrase the prompt.
   *
   * @generated from field: google.ai.generativelanguage.v1.GenerateContentResponse.PromptFeedback.BlockReason block_reason = 1;
   */
  blockReason: GenerateContentResponse_PromptFeedback_BlockReason;

  /**
   * Ratings for safety of the prompt.
   * There is at most one rating per category.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1.SafetyRating safety_ratings = 2;
   */
  safetyRatings: SafetyRating[];
};

/**
 * Describes the message google.ai.generativelanguage.v1.GenerateContentResponse.PromptFeedback.
 * Use `create(GenerateContentResponse_PromptFeedbackSchema)` to create a new message.
 */
export const GenerateContentResponse_PromptFeedbackSchema: GenMessage<GenerateContentResponse_PromptFeedback> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_generative_service, 2, 0);

/**
 * Specifies the reason why the prompt was blocked.
 *
 * @generated from enum google.ai.generativelanguage.v1.GenerateContentResponse.PromptFeedback.BlockReason
 */
export enum GenerateContentResponse_PromptFeedback_BlockReason {
  /**
   * Default value. This value is unused.
   *
   * @generated from enum value: BLOCK_REASON_UNSPECIFIED = 0;
   */
  BLOCK_REASON_UNSPECIFIED = 0,

  /**
   * Prompt was blocked due to safety reasons. Inspect `safety_ratings`
   * to understand which safety category blocked it.
   *
   * @generated from enum value: SAFETY = 1;
   */
  SAFETY = 1,

  /**
   * Prompt was blocked due to unknown reasons.
   *
   * @generated from enum value: OTHER = 2;
   */
  OTHER = 2,

  /**
   * Prompt was blocked due to the terms which are included from the
   * terminology blocklist.
   *
   * @generated from enum value: BLOCKLIST = 3;
   */
  BLOCKLIST = 3,

  /**
   * Prompt was blocked due to prohibited content.
   *
   * @generated from enum value: PROHIBITED_CONTENT = 4;
   */
  PROHIBITED_CONTENT = 4,
}

/**
 * Describes the enum google.ai.generativelanguage.v1.GenerateContentResponse.PromptFeedback.BlockReason.
 */
export const GenerateContentResponse_PromptFeedback_BlockReasonSchema: GenEnum<GenerateContentResponse_PromptFeedback_BlockReason> = /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1_generative_service, 2, 0, 0);

/**
 * Metadata on the generation request's token usage.
 *
 * @generated from message google.ai.generativelanguage.v1.GenerateContentResponse.UsageMetadata
 */
export type GenerateContentResponse_UsageMetadata = Message<"google.ai.generativelanguage.v1.GenerateContentResponse.UsageMetadata"> & {
  /**
   * Number of tokens in the prompt. When `cached_content` is set, this is
   * still the total effective prompt size meaning this includes the number of
   * tokens in the cached content.
   *
   * @generated from field: int32 prompt_token_count = 1;
   */
  promptTokenCount: number;

  /**
   * Total number of tokens across all the generated response candidates.
   *
   * @generated from field: int32 candidates_token_count = 2;
   */
  candidatesTokenCount: number;

  /**
   * Total token count for the generation request (prompt + response
   * candidates).
   *
   * @generated from field: int32 total_token_count = 3;
   */
  totalTokenCount: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1.GenerateContentResponse.UsageMetadata.
 * Use `create(GenerateContentResponse_UsageMetadataSchema)` to create a new message.
 */
export const GenerateContentResponse_UsageMetadataSchema: GenMessage<GenerateContentResponse_UsageMetadata> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_generative_service, 2, 1);

/**
 * A response candidate generated from the model.
 *
 * @generated from message google.ai.generativelanguage.v1.Candidate
 */
export type Candidate = Message<"google.ai.generativelanguage.v1.Candidate"> & {
  /**
   * Output only. Index of the candidate in the list of response candidates.
   *
   * @generated from field: optional int32 index = 3;
   */
  index?: number;

  /**
   * Output only. Generated content returned from the model.
   *
   * @generated from field: google.ai.generativelanguage.v1.Content content = 1;
   */
  content?: Content;

  /**
   * Optional. Output only. The reason why the model stopped generating tokens.
   *
   * If empty, the model has not stopped generating tokens.
   *
   * @generated from field: google.ai.generativelanguage.v1.Candidate.FinishReason finish_reason = 2;
   */
  finishReason: Candidate_FinishReason;

  /**
   * List of ratings for the safety of a response candidate.
   *
   * There is at most one rating per category.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1.SafetyRating safety_ratings = 5;
   */
  safetyRatings: SafetyRating[];

  /**
   * Output only. Citation information for model-generated candidate.
   *
   * This field may be populated with recitation information for any text
   * included in the `content`. These are passages that are "recited" from
   * copyrighted material in the foundational LLM's training data.
   *
   * @generated from field: google.ai.generativelanguage.v1.CitationMetadata citation_metadata = 6;
   */
  citationMetadata?: CitationMetadata;

  /**
   * Output only. Token count for this candidate.
   *
   * @generated from field: int32 token_count = 7;
   */
  tokenCount: number;

  /**
   * Output only.
   *
   * @generated from field: double avg_logprobs = 10;
   */
  avgLogprobs: number;

  /**
   * Output only. Log-likelihood scores for the response tokens and top tokens
   *
   * @generated from field: google.ai.generativelanguage.v1.LogprobsResult logprobs_result = 11;
   */
  logprobsResult?: LogprobsResult;
};

/**
 * Describes the message google.ai.generativelanguage.v1.Candidate.
 * Use `create(CandidateSchema)` to create a new message.
 */
export const CandidateSchema: GenMessage<Candidate> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_generative_service, 3);

/**
 * Defines the reason why the model stopped generating tokens.
 *
 * @generated from enum google.ai.generativelanguage.v1.Candidate.FinishReason
 */
export enum Candidate_FinishReason {
  /**
   * Default value. This value is unused.
   *
   * @generated from enum value: FINISH_REASON_UNSPECIFIED = 0;
   */
  FINISH_REASON_UNSPECIFIED = 0,

  /**
   * Natural stop point of the model or provided stop sequence.
   *
   * @generated from enum value: STOP = 1;
   */
  STOP = 1,

  /**
   * The maximum number of tokens as specified in the request was reached.
   *
   * @generated from enum value: MAX_TOKENS = 2;
   */
  MAX_TOKENS = 2,

  /**
   * The response candidate content was flagged for safety reasons.
   *
   * @generated from enum value: SAFETY = 3;
   */
  SAFETY = 3,

  /**
   * The response candidate content was flagged for recitation reasons.
   *
   * @generated from enum value: RECITATION = 4;
   */
  RECITATION = 4,

  /**
   * The response candidate content was flagged for using an unsupported
   * language.
   *
   * @generated from enum value: LANGUAGE = 6;
   */
  LANGUAGE = 6,

  /**
   * Unknown reason.
   *
   * @generated from enum value: OTHER = 5;
   */
  OTHER = 5,

  /**
   * Token generation stopped because the content contains forbidden terms.
   *
   * @generated from enum value: BLOCKLIST = 7;
   */
  BLOCKLIST = 7,

  /**
   * Token generation stopped for potentially containing prohibited content.
   *
   * @generated from enum value: PROHIBITED_CONTENT = 8;
   */
  PROHIBITED_CONTENT = 8,

  /**
   * Token generation stopped because the content potentially contains
   * Sensitive Personally Identifiable Information (SPII).
   *
   * @generated from enum value: SPII = 9;
   */
  SPII = 9,

  /**
   * The function call generated by the model is invalid.
   *
   * @generated from enum value: MALFORMED_FUNCTION_CALL = 10;
   */
  MALFORMED_FUNCTION_CALL = 10,
}

/**
 * Describes the enum google.ai.generativelanguage.v1.Candidate.FinishReason.
 */
export const Candidate_FinishReasonSchema: GenEnum<Candidate_FinishReason> = /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1_generative_service, 3, 0);

/**
 * Logprobs Result
 *
 * @generated from message google.ai.generativelanguage.v1.LogprobsResult
 */
export type LogprobsResult = Message<"google.ai.generativelanguage.v1.LogprobsResult"> & {
  /**
   * Length = total number of decoding steps.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1.LogprobsResult.TopCandidates top_candidates = 1;
   */
  topCandidates: LogprobsResult_TopCandidates[];

  /**
   * Length = total number of decoding steps.
   * The chosen candidates may or may not be in top_candidates.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1.LogprobsResult.Candidate chosen_candidates = 2;
   */
  chosenCandidates: LogprobsResult_Candidate[];
};

/**
 * Describes the message google.ai.generativelanguage.v1.LogprobsResult.
 * Use `create(LogprobsResultSchema)` to create a new message.
 */
export const LogprobsResultSchema: GenMessage<LogprobsResult> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_generative_service, 4);

/**
 * Candidate for the logprobs token and score.
 *
 * @generated from message google.ai.generativelanguage.v1.LogprobsResult.Candidate
 */
export type LogprobsResult_Candidate = Message<"google.ai.generativelanguage.v1.LogprobsResult.Candidate"> & {
  /**
   * The candidate’s token string value.
   *
   * @generated from field: optional string token = 1;
   */
  token?: string;

  /**
   * The candidate’s token id value.
   *
   * @generated from field: optional int32 token_id = 3;
   */
  tokenId?: number;

  /**
   * The candidate's log probability.
   *
   * @generated from field: optional float log_probability = 2;
   */
  logProbability?: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1.LogprobsResult.Candidate.
 * Use `create(LogprobsResult_CandidateSchema)` to create a new message.
 */
export const LogprobsResult_CandidateSchema: GenMessage<LogprobsResult_Candidate> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_generative_service, 4, 0);

/**
 * Candidates with top log probabilities at each decoding step.
 *
 * @generated from message google.ai.generativelanguage.v1.LogprobsResult.TopCandidates
 */
export type LogprobsResult_TopCandidates = Message<"google.ai.generativelanguage.v1.LogprobsResult.TopCandidates"> & {
  /**
   * Sorted by log probability in descending order.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1.LogprobsResult.Candidate candidates = 1;
   */
  candidates: LogprobsResult_Candidate[];
};

/**
 * Describes the message google.ai.generativelanguage.v1.LogprobsResult.TopCandidates.
 * Use `create(LogprobsResult_TopCandidatesSchema)` to create a new message.
 */
export const LogprobsResult_TopCandidatesSchema: GenMessage<LogprobsResult_TopCandidates> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_generative_service, 4, 1);

/**
 * Request containing the `Content` for the model to embed.
 *
 * @generated from message google.ai.generativelanguage.v1.EmbedContentRequest
 */
export type EmbedContentRequest = Message<"google.ai.generativelanguage.v1.EmbedContentRequest"> & {
  /**
   * Required. The model's resource name. This serves as an ID for the Model to
   * use.
   *
   * This name should match a model name returned by the `ListModels` method.
   *
   * Format: `models/{model}`
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Required. The content to embed. Only the `parts.text` fields will be
   * counted.
   *
   * @generated from field: google.ai.generativelanguage.v1.Content content = 2;
   */
  content?: Content;

  /**
   * Optional. Optional task type for which the embeddings will be used. Can
   * only be set for `models/embedding-001`.
   *
   * @generated from field: optional google.ai.generativelanguage.v1.TaskType task_type = 3;
   */
  taskType?: TaskType;

  /**
   * Optional. An optional title for the text. Only applicable when TaskType is
   * `RETRIEVAL_DOCUMENT`.
   *
   * Note: Specifying a `title` for `RETRIEVAL_DOCUMENT` provides better quality
   * embeddings for retrieval.
   *
   * @generated from field: optional string title = 4;
   */
  title?: string;

  /**
   * Optional. Optional reduced dimension for the output embedding. If set,
   * excessive values in the output embedding are truncated from the end.
   * Supported by newer models since 2024 only. You cannot set this value if
   * using the earlier model (`models/embedding-001`).
   *
   * @generated from field: optional int32 output_dimensionality = 5;
   */
  outputDimensionality?: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1.EmbedContentRequest.
 * Use `create(EmbedContentRequestSchema)` to create a new message.
 */
export const EmbedContentRequestSchema: GenMessage<EmbedContentRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_generative_service, 5);

/**
 * A list of floats representing an embedding.
 *
 * @generated from message google.ai.generativelanguage.v1.ContentEmbedding
 */
export type ContentEmbedding = Message<"google.ai.generativelanguage.v1.ContentEmbedding"> & {
  /**
   * The embedding values.
   *
   * @generated from field: repeated float values = 1;
   */
  values: number[];
};

/**
 * Describes the message google.ai.generativelanguage.v1.ContentEmbedding.
 * Use `create(ContentEmbeddingSchema)` to create a new message.
 */
export const ContentEmbeddingSchema: GenMessage<ContentEmbedding> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_generative_service, 6);

/**
 * The response to an `EmbedContentRequest`.
 *
 * @generated from message google.ai.generativelanguage.v1.EmbedContentResponse
 */
export type EmbedContentResponse = Message<"google.ai.generativelanguage.v1.EmbedContentResponse"> & {
  /**
   * Output only. The embedding generated from the input content.
   *
   * @generated from field: google.ai.generativelanguage.v1.ContentEmbedding embedding = 1;
   */
  embedding?: ContentEmbedding;
};

/**
 * Describes the message google.ai.generativelanguage.v1.EmbedContentResponse.
 * Use `create(EmbedContentResponseSchema)` to create a new message.
 */
export const EmbedContentResponseSchema: GenMessage<EmbedContentResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_generative_service, 7);

/**
 * Batch request to get embeddings from the model for a list of prompts.
 *
 * @generated from message google.ai.generativelanguage.v1.BatchEmbedContentsRequest
 */
export type BatchEmbedContentsRequest = Message<"google.ai.generativelanguage.v1.BatchEmbedContentsRequest"> & {
  /**
   * Required. The model's resource name. This serves as an ID for the Model to
   * use.
   *
   * This name should match a model name returned by the `ListModels` method.
   *
   * Format: `models/{model}`
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Required. Embed requests for the batch. The model in each of these requests
   * must match the model specified `BatchEmbedContentsRequest.model`.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1.EmbedContentRequest requests = 2;
   */
  requests: EmbedContentRequest[];
};

/**
 * Describes the message google.ai.generativelanguage.v1.BatchEmbedContentsRequest.
 * Use `create(BatchEmbedContentsRequestSchema)` to create a new message.
 */
export const BatchEmbedContentsRequestSchema: GenMessage<BatchEmbedContentsRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_generative_service, 8);

/**
 * The response to a `BatchEmbedContentsRequest`.
 *
 * @generated from message google.ai.generativelanguage.v1.BatchEmbedContentsResponse
 */
export type BatchEmbedContentsResponse = Message<"google.ai.generativelanguage.v1.BatchEmbedContentsResponse"> & {
  /**
   * Output only. The embeddings for each request, in the same order as provided
   * in the batch request.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1.ContentEmbedding embeddings = 1;
   */
  embeddings: ContentEmbedding[];
};

/**
 * Describes the message google.ai.generativelanguage.v1.BatchEmbedContentsResponse.
 * Use `create(BatchEmbedContentsResponseSchema)` to create a new message.
 */
export const BatchEmbedContentsResponseSchema: GenMessage<BatchEmbedContentsResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_generative_service, 9);

/**
 * Counts the number of tokens in the `prompt` sent to a model.
 *
 * Models may tokenize text differently, so each model may return a different
 * `token_count`.
 *
 * @generated from message google.ai.generativelanguage.v1.CountTokensRequest
 */
export type CountTokensRequest = Message<"google.ai.generativelanguage.v1.CountTokensRequest"> & {
  /**
   * Required. The model's resource name. This serves as an ID for the Model to
   * use.
   *
   * This name should match a model name returned by the `ListModels` method.
   *
   * Format: `models/{model}`
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Optional. The input given to the model as a prompt. This field is ignored
   * when `generate_content_request` is set.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1.Content contents = 2;
   */
  contents: Content[];

  /**
   * Optional. The overall input given to the `Model`. This includes the prompt
   * as well as other model steering information like [system
   * instructions](https://ai.google.dev/gemini-api/docs/system-instructions),
   * and/or function declarations for [function
   * calling](https://ai.google.dev/gemini-api/docs/function-calling).
   * `Model`s/`Content`s and `generate_content_request`s are mutually
   * exclusive. You can either send `Model` + `Content`s or a
   * `generate_content_request`, but never both.
   *
   * @generated from field: google.ai.generativelanguage.v1.GenerateContentRequest generate_content_request = 3;
   */
  generateContentRequest?: GenerateContentRequest;
};

/**
 * Describes the message google.ai.generativelanguage.v1.CountTokensRequest.
 * Use `create(CountTokensRequestSchema)` to create a new message.
 */
export const CountTokensRequestSchema: GenMessage<CountTokensRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_generative_service, 10);

/**
 * A response from `CountTokens`.
 *
 * It returns the model's `token_count` for the `prompt`.
 *
 * @generated from message google.ai.generativelanguage.v1.CountTokensResponse
 */
export type CountTokensResponse = Message<"google.ai.generativelanguage.v1.CountTokensResponse"> & {
  /**
   * The number of tokens that the `Model` tokenizes the `prompt` into. Always
   * non-negative.
   *
   * @generated from field: int32 total_tokens = 1;
   */
  totalTokens: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1.CountTokensResponse.
 * Use `create(CountTokensResponseSchema)` to create a new message.
 */
export const CountTokensResponseSchema: GenMessage<CountTokensResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1_generative_service, 11);

/**
 * Type of task for which the embedding will be used.
 *
 * @generated from enum google.ai.generativelanguage.v1.TaskType
 */
export enum TaskType {
  /**
   * Unset value, which will default to one of the other enum values.
   *
   * @generated from enum value: TASK_TYPE_UNSPECIFIED = 0;
   */
  TASK_TYPE_UNSPECIFIED = 0,

  /**
   * Specifies the given text is a query in a search/retrieval setting.
   *
   * @generated from enum value: RETRIEVAL_QUERY = 1;
   */
  RETRIEVAL_QUERY = 1,

  /**
   * Specifies the given text is a document from the corpus being searched.
   *
   * @generated from enum value: RETRIEVAL_DOCUMENT = 2;
   */
  RETRIEVAL_DOCUMENT = 2,

  /**
   * Specifies the given text will be used for STS.
   *
   * @generated from enum value: SEMANTIC_SIMILARITY = 3;
   */
  SEMANTIC_SIMILARITY = 3,

  /**
   * Specifies that the given text will be classified.
   *
   * @generated from enum value: CLASSIFICATION = 4;
   */
  CLASSIFICATION = 4,

  /**
   * Specifies that the embeddings will be used for clustering.
   *
   * @generated from enum value: CLUSTERING = 5;
   */
  CLUSTERING = 5,

  /**
   * Specifies that the given text will be used for question answering.
   *
   * @generated from enum value: QUESTION_ANSWERING = 6;
   */
  QUESTION_ANSWERING = 6,

  /**
   * Specifies that the given text will be used for fact verification.
   *
   * @generated from enum value: FACT_VERIFICATION = 7;
   */
  FACT_VERIFICATION = 7,
}

/**
 * Describes the enum google.ai.generativelanguage.v1.TaskType.
 */
export const TaskTypeSchema: GenEnum<TaskType> = /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1_generative_service, 0);

/**
 * API for using Large Models that generate multimodal content and have
 * additional capabilities beyond text generation.
 *
 * @generated from service google.ai.generativelanguage.v1.GenerativeService
 */
export const GenerativeService: GenService<{
  /**
   * Generates a model response given an input `GenerateContentRequest`.
   * Refer to the [text generation
   * guide](https://ai.google.dev/gemini-api/docs/text-generation) for detailed
   * usage information. Input capabilities differ between models, including
   * tuned models. Refer to the [model
   * guide](https://ai.google.dev/gemini-api/docs/models/gemini) and [tuning
   * guide](https://ai.google.dev/gemini-api/docs/model-tuning) for details.
   *
   * @generated from rpc google.ai.generativelanguage.v1.GenerativeService.GenerateContent
   */
  generateContent: {
    methodKind: "unary";
    input: typeof GenerateContentRequestSchema;
    output: typeof GenerateContentResponseSchema;
  },
  /**
   * Generates a [streamed
   * response](https://ai.google.dev/gemini-api/docs/text-generation?lang=python#generate-a-text-stream)
   * from the model given an input `GenerateContentRequest`.
   *
   * @generated from rpc google.ai.generativelanguage.v1.GenerativeService.StreamGenerateContent
   */
  streamGenerateContent: {
    methodKind: "server_streaming";
    input: typeof GenerateContentRequestSchema;
    output: typeof GenerateContentResponseSchema;
  },
  /**
   * Generates a text embedding vector from the input `Content` using the
   * specified [Gemini Embedding
   * model](https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding).
   *
   * @generated from rpc google.ai.generativelanguage.v1.GenerativeService.EmbedContent
   */
  embedContent: {
    methodKind: "unary";
    input: typeof EmbedContentRequestSchema;
    output: typeof EmbedContentResponseSchema;
  },
  /**
   * Generates multiple embedding vectors from the input `Content` which
   * consists of a batch of strings represented as `EmbedContentRequest`
   * objects.
   *
   * @generated from rpc google.ai.generativelanguage.v1.GenerativeService.BatchEmbedContents
   */
  batchEmbedContents: {
    methodKind: "unary";
    input: typeof BatchEmbedContentsRequestSchema;
    output: typeof BatchEmbedContentsResponseSchema;
  },
  /**
   * Runs a model's tokenizer on input `Content` and returns the token count.
   * Refer to the [tokens guide](https://ai.google.dev/gemini-api/docs/tokens)
   * to learn more about tokens.
   *
   * @generated from rpc google.ai.generativelanguage.v1.GenerativeService.CountTokens
   */
  countTokens: {
    methodKind: "unary";
    input: typeof CountTokensRequestSchema;
    output: typeof CountTokensResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_ai_generativelanguage_v1_generative_service, 0);

