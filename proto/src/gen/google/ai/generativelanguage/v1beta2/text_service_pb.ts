// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/ai/generativelanguage/v1beta2/text_service.proto (package google.ai.generativelanguage.v1beta2, syntax proto3)
/* eslint-disable */

import type { GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import type { CitationMetadata } from "./citation_pb";
import { file_google_ai_generativelanguage_v1beta2_citation } from "./citation_pb";
import type { ContentFilter, SafetyFeedback, SafetyRating, SafetySetting } from "./safety_pb";
import { file_google_ai_generativelanguage_v1beta2_safety } from "./safety_pb";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import { file_google_api_resource } from "../../../api/resource_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/ai/generativelanguage/v1beta2/text_service.proto.
 */
export const file_google_ai_generativelanguage_v1beta2_text_service: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_ai_generativelanguage_v1beta2_citation, file_google_ai_generativelanguage_v1beta2_safety, file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_api_resource]);

/**
 * Request to generate a text completion response from the model.
 *
 * @generated from message google.ai.generativelanguage.v1beta2.GenerateTextRequest
 */
export type GenerateTextRequest = Message<"google.ai.generativelanguage.v1beta2.GenerateTextRequest"> & {
  /**
   * Required. The model name to use with the format name=models/{model}.
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Required. The free-form input text given to the model as a prompt.
   *
   * Given a prompt, the model will generate a TextCompletion response it
   * predicts as the completion of the input text.
   *
   * @generated from field: google.ai.generativelanguage.v1beta2.TextPrompt prompt = 2;
   */
  prompt?: TextPrompt;

  /**
   * Controls the randomness of the output.
   * Note: The default value varies by model, see the `Model.temperature`
   * attribute of the `Model` returned the `getModel` function.
   *
   * Values can range from [0.0,1.0],
   * inclusive. A value closer to 1.0 will produce responses that are more
   * varied and creative, while a value closer to 0.0 will typically result in
   * more straightforward responses from the model.
   *
   * @generated from field: optional float temperature = 3;
   */
  temperature?: number;

  /**
   * Number of generated responses to return.
   *
   * This value must be between [1, 8], inclusive. If unset, this will default
   * to 1.
   *
   * @generated from field: optional int32 candidate_count = 4;
   */
  candidateCount?: number;

  /**
   * The maximum number of tokens to include in a candidate.
   *
   * If unset, this will default to 64.
   *
   * @generated from field: optional int32 max_output_tokens = 5;
   */
  maxOutputTokens?: number;

  /**
   * The maximum cumulative probability of tokens to consider when sampling.
   *
   * The model uses combined Top-k and nucleus sampling.
   *
   * Tokens are sorted based on their assigned probabilities so that only the
   * most liekly tokens are considered. Top-k sampling directly limits the
   * maximum number of tokens to consider, while Nucleus sampling limits number
   * of tokens based on the cumulative probability.
   *
   * Note: The default value varies by model, see the `Model.top_p`
   * attribute of the `Model` returned the `getModel` function.
   *
   * @generated from field: optional float top_p = 6;
   */
  topP?: number;

  /**
   * The maximum number of tokens to consider when sampling.
   *
   * The model uses combined Top-k and nucleus sampling.
   *
   * Top-k sampling considers the set of `top_k` most probable tokens.
   * Defaults to 40.
   *
   * Note: The default value varies by model, see the `Model.top_k`
   * attribute of the `Model` returned the `getModel` function.
   *
   * @generated from field: optional int32 top_k = 7;
   */
  topK?: number;

  /**
   * A list of unique `SafetySetting` instances for blocking unsafe content.
   *
   * that will be enforced on the `GenerateTextRequest.prompt` and
   * `GenerateTextResponse.candidates`. There should not be more than one
   * setting for each `SafetyCategory` type. The API will block any prompts and
   * responses that fail to meet the thresholds set by these settings. This list
   * overrides the default settings for each `SafetyCategory` specified in the
   * safety_settings. If there is no `SafetySetting` for a given
   * `SafetyCategory` provided in the list, the API will use the default safety
   * setting for that category.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta2.SafetySetting safety_settings = 8;
   */
  safetySettings: SafetySetting[];

  /**
   * The set of character sequences (up to 5) that will stop output generation.
   * If specified, the API will stop at the first appearance of a stop
   * sequence. The stop sequence will not be included as part of the response.
   *
   * @generated from field: repeated string stop_sequences = 9;
   */
  stopSequences: string[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta2.GenerateTextRequest.
 * Use `create(GenerateTextRequestSchema)` to create a new message.
 */
export const GenerateTextRequestSchema: GenMessage<GenerateTextRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta2_text_service, 0);

/**
 * The response from the model, including candidate completions.
 *
 * @generated from message google.ai.generativelanguage.v1beta2.GenerateTextResponse
 */
export type GenerateTextResponse = Message<"google.ai.generativelanguage.v1beta2.GenerateTextResponse"> & {
  /**
   * Candidate responses from the model.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta2.TextCompletion candidates = 1;
   */
  candidates: TextCompletion[];

  /**
   * A set of content filtering metadata for the prompt and response
   * text.
   *
   * This indicates which `SafetyCategory`(s) blocked a
   * candidate from this response, the lowest `HarmProbability`
   * that triggered a block, and the HarmThreshold setting for that category.
   * This indicates the smallest change to the `SafetySettings` that would be
   * necessary to unblock at least 1 response.
   *
   * The blocking is configured by the `SafetySettings` in the request (or the
   * default `SafetySettings` of the API).
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta2.ContentFilter filters = 3;
   */
  filters: ContentFilter[];

  /**
   * Returns any safety feedback related to content filtering.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta2.SafetyFeedback safety_feedback = 4;
   */
  safetyFeedback: SafetyFeedback[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta2.GenerateTextResponse.
 * Use `create(GenerateTextResponseSchema)` to create a new message.
 */
export const GenerateTextResponseSchema: GenMessage<GenerateTextResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta2_text_service, 1);

/**
 * Text given to the model as a prompt.
 *
 * The Model will use this TextPrompt to Generate a text completion.
 *
 * @generated from message google.ai.generativelanguage.v1beta2.TextPrompt
 */
export type TextPrompt = Message<"google.ai.generativelanguage.v1beta2.TextPrompt"> & {
  /**
   * Required. The prompt text.
   *
   * @generated from field: string text = 1;
   */
  text: string;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta2.TextPrompt.
 * Use `create(TextPromptSchema)` to create a new message.
 */
export const TextPromptSchema: GenMessage<TextPrompt> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta2_text_service, 2);

/**
 * Output text returned from a model.
 *
 * @generated from message google.ai.generativelanguage.v1beta2.TextCompletion
 */
export type TextCompletion = Message<"google.ai.generativelanguage.v1beta2.TextCompletion"> & {
  /**
   * Output only. The generated text returned from the model.
   *
   * @generated from field: string output = 1;
   */
  output: string;

  /**
   * Ratings for the safety of a response.
   *
   * There is at most one rating per category.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta2.SafetyRating safety_ratings = 2;
   */
  safetyRatings: SafetyRating[];

  /**
   * Output only. Citation information for model-generated `output` in this
   * `TextCompletion`.
   *
   * This field may be populated with attribution information for any text
   * included in the `output`.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta2.CitationMetadata citation_metadata = 3;
   */
  citationMetadata?: CitationMetadata;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta2.TextCompletion.
 * Use `create(TextCompletionSchema)` to create a new message.
 */
export const TextCompletionSchema: GenMessage<TextCompletion> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta2_text_service, 3);

/**
 * Request to get a text embedding from the model.
 *
 * @generated from message google.ai.generativelanguage.v1beta2.EmbedTextRequest
 */
export type EmbedTextRequest = Message<"google.ai.generativelanguage.v1beta2.EmbedTextRequest"> & {
  /**
   * Required. The model name to use with the format model=models/{model}.
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Required. The free-form input text that the model will turn into an
   * embedding.
   *
   * @generated from field: string text = 2;
   */
  text: string;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta2.EmbedTextRequest.
 * Use `create(EmbedTextRequestSchema)` to create a new message.
 */
export const EmbedTextRequestSchema: GenMessage<EmbedTextRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta2_text_service, 4);

/**
 * The response to a EmbedTextRequest.
 *
 * @generated from message google.ai.generativelanguage.v1beta2.EmbedTextResponse
 */
export type EmbedTextResponse = Message<"google.ai.generativelanguage.v1beta2.EmbedTextResponse"> & {
  /**
   * Output only. The embedding generated from the input text.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta2.Embedding embedding = 1;
   */
  embedding?: Embedding;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta2.EmbedTextResponse.
 * Use `create(EmbedTextResponseSchema)` to create a new message.
 */
export const EmbedTextResponseSchema: GenMessage<EmbedTextResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta2_text_service, 5);

/**
 * A list of floats representing the embedding.
 *
 * @generated from message google.ai.generativelanguage.v1beta2.Embedding
 */
export type Embedding = Message<"google.ai.generativelanguage.v1beta2.Embedding"> & {
  /**
   * The embedding values.
   *
   * @generated from field: repeated float value = 1;
   */
  value: number[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta2.Embedding.
 * Use `create(EmbeddingSchema)` to create a new message.
 */
export const EmbeddingSchema: GenMessage<Embedding> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta2_text_service, 6);

/**
 * API for using Generative Language Models (GLMs) trained to generate text.
 *
 * Also known as Large Language Models (LLM)s, these generate text given an
 * input prompt from the user.
 *
 * @generated from service google.ai.generativelanguage.v1beta2.TextService
 */
export const TextService: GenService<{
  /**
   * Generates a response from the model given an input message.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta2.TextService.GenerateText
   */
  generateText: {
    methodKind: "unary";
    input: typeof GenerateTextRequestSchema;
    output: typeof GenerateTextResponseSchema;
  },
  /**
   * Generates an embedding from the model given an input message.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta2.TextService.EmbedText
   */
  embedText: {
    methodKind: "unary";
    input: typeof EmbedTextRequestSchema;
    output: typeof EmbedTextResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_ai_generativelanguage_v1beta2_text_service, 0);

