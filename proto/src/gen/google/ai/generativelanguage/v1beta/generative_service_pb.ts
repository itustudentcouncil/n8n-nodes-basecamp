// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/ai/generativelanguage/v1beta/generative_service.proto (package google.ai.generativelanguage.v1beta, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import type { CitationMetadata } from "./citation_pb";
import { file_google_ai_generativelanguage_v1beta_citation } from "./citation_pb";
import type { Content, GroundingPassages, Schema, Tool, ToolConfig } from "./content_pb";
import { file_google_ai_generativelanguage_v1beta_content } from "./content_pb";
import type { MetadataFilter } from "./retriever_pb";
import { file_google_ai_generativelanguage_v1beta_retriever } from "./retriever_pb";
import type { SafetyRating, SafetySetting } from "./safety_pb";
import { file_google_ai_generativelanguage_v1beta_safety } from "./safety_pb";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import { file_google_api_resource } from "../../../api/resource_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/ai/generativelanguage/v1beta/generative_service.proto.
 */
export const file_google_ai_generativelanguage_v1beta_generative_service: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_ai_generativelanguage_v1beta_citation, file_google_ai_generativelanguage_v1beta_content, file_google_ai_generativelanguage_v1beta_retriever, file_google_ai_generativelanguage_v1beta_safety, file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_api_resource]);

/**
 * Request to generate a completion from the model.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerateContentRequest
 */
export type GenerateContentRequest = Message<"google.ai.generativelanguage.v1beta.GenerateContentRequest"> & {
  /**
   * Required. The name of the `Model` to use for generating the completion.
   *
   * Format: `name=models/{model}`.
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Optional. Developer set [system
   * instruction(s)](https://ai.google.dev/gemini-api/docs/system-instructions).
   * Currently, text only.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.Content system_instruction = 8;
   */
  systemInstruction?: Content;

  /**
   * Required. The content of the current conversation with the model.
   *
   * For single-turn queries, this is a single instance. For multi-turn queries
   * like [chat](https://ai.google.dev/gemini-api/docs/text-generation#chat),
   * this is a repeated field that contains the conversation history and the
   * latest request.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.Content contents = 2;
   */
  contents: Content[];

  /**
   * Optional. A list of `Tools` the `Model` may use to generate the next
   * response.
   *
   * A `Tool` is a piece of code that enables the system to interact with
   * external systems to perform an action, or set of actions, outside of
   * knowledge and scope of the `Model`. Supported `Tool`s are `Function` and
   * `code_execution`. Refer to the [Function
   * calling](https://ai.google.dev/gemini-api/docs/function-calling) and the
   * [Code execution](https://ai.google.dev/gemini-api/docs/code-execution)
   * guides to learn more.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.Tool tools = 5;
   */
  tools: Tool[];

  /**
   * Optional. Tool configuration for any `Tool` specified in the request. Refer
   * to the [Function calling
   * guide](https://ai.google.dev/gemini-api/docs/function-calling#function_calling_mode)
   * for a usage example.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.ToolConfig tool_config = 7;
   */
  toolConfig?: ToolConfig;

  /**
   * Optional. A list of unique `SafetySetting` instances for blocking unsafe
   * content.
   *
   * This will be enforced on the `GenerateContentRequest.contents` and
   * `GenerateContentResponse.candidates`. There should not be more than one
   * setting for each `SafetyCategory` type. The API will block any contents and
   * responses that fail to meet the thresholds set by these settings. This list
   * overrides the default settings for each `SafetyCategory` specified in the
   * safety_settings. If there is no `SafetySetting` for a given
   * `SafetyCategory` provided in the list, the API will use the default safety
   * setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH,
   * HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT,
   * HARM_CATEGORY_HARASSMENT are supported. Refer to the
   * [guide](https://ai.google.dev/gemini-api/docs/safety-settings)
   * for detailed information on available safety settings. Also refer to the
   * [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance) to
   * learn how to incorporate safety considerations in your AI applications.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.SafetySetting safety_settings = 3;
   */
  safetySettings: SafetySetting[];

  /**
   * Optional. Configuration options for model generation and outputs.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.GenerationConfig generation_config = 4;
   */
  generationConfig?: GenerationConfig;

  /**
   * Optional. The name of the content
   * [cached](https://ai.google.dev/gemini-api/docs/caching) to use as context
   * to serve the prediction. Format: `cachedContents/{cachedContent}`
   *
   * @generated from field: optional string cached_content = 9;
   */
  cachedContent?: string;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerateContentRequest.
 * Use `create(GenerateContentRequestSchema)` to create a new message.
 */
export const GenerateContentRequestSchema: GenMessage<GenerateContentRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 0);

/**
 * Configuration options for model generation and outputs. Not all parameters
 * are configurable for every model.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerationConfig
 */
export type GenerationConfig = Message<"google.ai.generativelanguage.v1beta.GenerationConfig"> & {
  /**
   * Optional. Number of generated responses to return.
   *
   * Currently, this value can only be set to 1. If unset, this will default
   * to 1.
   *
   * @generated from field: optional int32 candidate_count = 1;
   */
  candidateCount?: number;

  /**
   * Optional. The set of character sequences (up to 5) that will stop output
   * generation. If specified, the API will stop at the first appearance of a
   * `stop_sequence`. The stop sequence will not be included as part of the
   * response.
   *
   * @generated from field: repeated string stop_sequences = 2;
   */
  stopSequences: string[];

  /**
   * Optional. The maximum number of tokens to include in a response candidate.
   *
   * Note: The default value varies by model, see the `Model.output_token_limit`
   * attribute of the `Model` returned from the `getModel` function.
   *
   * @generated from field: optional int32 max_output_tokens = 4;
   */
  maxOutputTokens?: number;

  /**
   * Optional. Controls the randomness of the output.
   *
   * Note: The default value varies by model, see the `Model.temperature`
   * attribute of the `Model` returned from the `getModel` function.
   *
   * Values can range from [0.0, 2.0].
   *
   * @generated from field: optional float temperature = 5;
   */
  temperature?: number;

  /**
   * Optional. The maximum cumulative probability of tokens to consider when
   * sampling.
   *
   * The model uses combined Top-k and Top-p (nucleus) sampling.
   *
   * Tokens are sorted based on their assigned probabilities so that only the
   * most likely tokens are considered. Top-k sampling directly limits the
   * maximum number of tokens to consider, while Nucleus sampling limits the
   * number of tokens based on the cumulative probability.
   *
   * Note: The default value varies by `Model` and is specified by
   * the`Model.top_p` attribute returned from the `getModel` function. An empty
   * `top_k` attribute indicates that the model doesn't apply top-k sampling
   * and doesn't allow setting `top_k` on requests.
   *
   * @generated from field: optional float top_p = 6;
   */
  topP?: number;

  /**
   * Optional. The maximum number of tokens to consider when sampling.
   *
   * Gemini models use Top-p (nucleus) sampling or a combination of Top-k and
   * nucleus sampling. Top-k sampling considers the set of `top_k` most probable
   * tokens. Models running with nucleus sampling don't allow top_k setting.
   *
   * Note: The default value varies by `Model` and is specified by
   * the`Model.top_p` attribute returned from the `getModel` function. An empty
   * `top_k` attribute indicates that the model doesn't apply top-k sampling
   * and doesn't allow setting `top_k` on requests.
   *
   * @generated from field: optional int32 top_k = 7;
   */
  topK?: number;

  /**
   * Optional. MIME type of the generated candidate text.
   * Supported MIME types are:
   * `text/plain`: (default) Text output.
   * `application/json`: JSON response in the response candidates.
   * `text/x.enum`: ENUM as a string response in the response candidates.
   * Refer to the
   * [docs](https://ai.google.dev/gemini-api/docs/prompting_with_media#plain_text_formats)
   * for a list of all supported text MIME types.
   *
   * @generated from field: string response_mime_type = 13;
   */
  responseMimeType: string;

  /**
   * Optional. Output schema of the generated candidate text. Schemas must be a
   * subset of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schema)
   * and can be objects, primitives or arrays.
   *
   * If set, a compatible `response_mime_type` must also be set.
   * Compatible MIME types:
   * `application/json`: Schema for JSON response.
   * Refer to the [JSON text generation
   * guide](https://ai.google.dev/gemini-api/docs/json-mode) for more details.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.Schema response_schema = 14;
   */
  responseSchema?: Schema;

  /**
   * Optional. Presence penalty applied to the next token's logprobs if the
   * token has already been seen in the response.
   *
   * This penalty is binary on/off and not dependant on the number of times the
   * token is used (after the first). Use
   * [frequency_penalty][google.ai.generativelanguage.v1beta.GenerationConfig.frequency_penalty]
   * for a penalty that increases with each use.
   *
   * A positive penalty will discourage the use of tokens that have already
   * been used in the response, increasing the vocabulary.
   *
   * A negative penalty will encourage the use of tokens that have already been
   * used in the response, decreasing the vocabulary.
   *
   * @generated from field: optional float presence_penalty = 15;
   */
  presencePenalty?: number;

  /**
   * Optional. Frequency penalty applied to the next token's logprobs,
   * multiplied by the number of times each token has been seen in the respponse
   * so far.
   *
   * A positive penalty will discourage the use of tokens that have already
   * been used, proportional to the number of times the token has been used:
   * The more a token is used, the more dificult it is for the model to use
   * that token again increasing the vocabulary of responses.
   *
   * Caution: A _negative_ penalty will encourage the model to reuse tokens
   * proportional to the number of times the token has been used. Small
   * negative values will reduce the vocabulary of a response. Larger negative
   * values will cause the model to start repeating a common token  until it
   * hits the
   * [max_output_tokens][google.ai.generativelanguage.v1beta.GenerationConfig.max_output_tokens]
   * limit: "...the the the the the...".
   *
   * @generated from field: optional float frequency_penalty = 16;
   */
  frequencyPenalty?: number;

  /**
   * Optional. If true, export the logprobs results in response.
   *
   * @generated from field: optional bool response_logprobs = 17;
   */
  responseLogprobs?: boolean;

  /**
   * Optional. Only valid if
   * [response_logprobs=True][google.ai.generativelanguage.v1beta.GenerationConfig.response_logprobs].
   * This sets the number of top logprobs to return at each decoding step in the
   * [Candidate.logprobs_result][google.ai.generativelanguage.v1beta.Candidate.logprobs_result].
   *
   * @generated from field: optional int32 logprobs = 18;
   */
  logprobs?: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerationConfig.
 * Use `create(GenerationConfigSchema)` to create a new message.
 */
export const GenerationConfigSchema: GenMessage<GenerationConfig> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 1);

/**
 * Configuration for retrieving grounding content from a `Corpus` or
 * `Document` created using the Semantic Retriever API.
 *
 * @generated from message google.ai.generativelanguage.v1beta.SemanticRetrieverConfig
 */
export type SemanticRetrieverConfig = Message<"google.ai.generativelanguage.v1beta.SemanticRetrieverConfig"> & {
  /**
   * Required. Name of the resource for retrieval. Example: `corpora/123` or
   * `corpora/123/documents/abc`.
   *
   * @generated from field: string source = 1;
   */
  source: string;

  /**
   * Required. Query to use for matching `Chunk`s in the given resource by
   * similarity.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.Content query = 2;
   */
  query?: Content;

  /**
   * Optional. Filters for selecting `Document`s and/or `Chunk`s from the
   * resource.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.MetadataFilter metadata_filters = 3;
   */
  metadataFilters: MetadataFilter[];

  /**
   * Optional. Maximum number of relevant `Chunk`s to retrieve.
   *
   * @generated from field: optional int32 max_chunks_count = 4;
   */
  maxChunksCount?: number;

  /**
   * Optional. Minimum relevance score for retrieved relevant `Chunk`s.
   *
   * @generated from field: optional float minimum_relevance_score = 5;
   */
  minimumRelevanceScore?: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.SemanticRetrieverConfig.
 * Use `create(SemanticRetrieverConfigSchema)` to create a new message.
 */
export const SemanticRetrieverConfigSchema: GenMessage<SemanticRetrieverConfig> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 2);

/**
 * Response from the model supporting multiple candidate responses.
 *
 * Safety ratings and content filtering are reported for both
 * prompt in `GenerateContentResponse.prompt_feedback` and for each candidate
 * in `finish_reason` and in `safety_ratings`. The API:
 *  - Returns either all requested candidates or none of them
 *  - Returns no candidates at all only if there was something wrong with the
 *    prompt (check `prompt_feedback`)
 *  - Reports feedback on each candidate in `finish_reason` and
 *    `safety_ratings`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerateContentResponse
 */
export type GenerateContentResponse = Message<"google.ai.generativelanguage.v1beta.GenerateContentResponse"> & {
  /**
   * Candidate responses from the model.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.Candidate candidates = 1;
   */
  candidates: Candidate[];

  /**
   * Returns the prompt's feedback related to the content filters.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.GenerateContentResponse.PromptFeedback prompt_feedback = 2;
   */
  promptFeedback?: GenerateContentResponse_PromptFeedback;

  /**
   * Output only. Metadata on the generation requests' token usage.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.GenerateContentResponse.UsageMetadata usage_metadata = 3;
   */
  usageMetadata?: GenerateContentResponse_UsageMetadata;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerateContentResponse.
 * Use `create(GenerateContentResponseSchema)` to create a new message.
 */
export const GenerateContentResponseSchema: GenMessage<GenerateContentResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 3);

/**
 * A set of the feedback metadata the prompt specified in
 * `GenerateContentRequest.content`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerateContentResponse.PromptFeedback
 */
export type GenerateContentResponse_PromptFeedback = Message<"google.ai.generativelanguage.v1beta.GenerateContentResponse.PromptFeedback"> & {
  /**
   * Optional. If set, the prompt was blocked and no candidates are returned.
   * Rephrase the prompt.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.GenerateContentResponse.PromptFeedback.BlockReason block_reason = 1;
   */
  blockReason: GenerateContentResponse_PromptFeedback_BlockReason;

  /**
   * Ratings for safety of the prompt.
   * There is at most one rating per category.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.SafetyRating safety_ratings = 2;
   */
  safetyRatings: SafetyRating[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerateContentResponse.PromptFeedback.
 * Use `create(GenerateContentResponse_PromptFeedbackSchema)` to create a new message.
 */
export const GenerateContentResponse_PromptFeedbackSchema: GenMessage<GenerateContentResponse_PromptFeedback> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 3, 0);

/**
 * Specifies the reason why the prompt was blocked.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.GenerateContentResponse.PromptFeedback.BlockReason
 */
export enum GenerateContentResponse_PromptFeedback_BlockReason {
  /**
   * Default value. This value is unused.
   *
   * @generated from enum value: BLOCK_REASON_UNSPECIFIED = 0;
   */
  BLOCK_REASON_UNSPECIFIED = 0,

  /**
   * Prompt was blocked due to safety reasons. Inspect `safety_ratings`
   * to understand which safety category blocked it.
   *
   * @generated from enum value: SAFETY = 1;
   */
  SAFETY = 1,

  /**
   * Prompt was blocked due to unknown reasons.
   *
   * @generated from enum value: OTHER = 2;
   */
  OTHER = 2,

  /**
   * Prompt was blocked due to the terms which are included from the
   * terminology blocklist.
   *
   * @generated from enum value: BLOCKLIST = 3;
   */
  BLOCKLIST = 3,

  /**
   * Prompt was blocked due to prohibited content.
   *
   * @generated from enum value: PROHIBITED_CONTENT = 4;
   */
  PROHIBITED_CONTENT = 4,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.GenerateContentResponse.PromptFeedback.BlockReason.
 */
export const GenerateContentResponse_PromptFeedback_BlockReasonSchema: GenEnum<GenerateContentResponse_PromptFeedback_BlockReason> = /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 3, 0, 0);

/**
 * Metadata on the generation request's token usage.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerateContentResponse.UsageMetadata
 */
export type GenerateContentResponse_UsageMetadata = Message<"google.ai.generativelanguage.v1beta.GenerateContentResponse.UsageMetadata"> & {
  /**
   * Number of tokens in the prompt. When `cached_content` is set, this is
   * still the total effective prompt size meaning this includes the number of
   * tokens in the cached content.
   *
   * @generated from field: int32 prompt_token_count = 1;
   */
  promptTokenCount: number;

  /**
   * Number of tokens in the cached part of the prompt (the cached content)
   *
   * @generated from field: int32 cached_content_token_count = 4;
   */
  cachedContentTokenCount: number;

  /**
   * Total number of tokens across all the generated response candidates.
   *
   * @generated from field: int32 candidates_token_count = 2;
   */
  candidatesTokenCount: number;

  /**
   * Total token count for the generation request (prompt + response
   * candidates).
   *
   * @generated from field: int32 total_token_count = 3;
   */
  totalTokenCount: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerateContentResponse.UsageMetadata.
 * Use `create(GenerateContentResponse_UsageMetadataSchema)` to create a new message.
 */
export const GenerateContentResponse_UsageMetadataSchema: GenMessage<GenerateContentResponse_UsageMetadata> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 3, 1);

/**
 * A response candidate generated from the model.
 *
 * @generated from message google.ai.generativelanguage.v1beta.Candidate
 */
export type Candidate = Message<"google.ai.generativelanguage.v1beta.Candidate"> & {
  /**
   * Output only. Index of the candidate in the list of response candidates.
   *
   * @generated from field: optional int32 index = 3;
   */
  index?: number;

  /**
   * Output only. Generated content returned from the model.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.Content content = 1;
   */
  content?: Content;

  /**
   * Optional. Output only. The reason why the model stopped generating tokens.
   *
   * If empty, the model has not stopped generating tokens.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.Candidate.FinishReason finish_reason = 2;
   */
  finishReason: Candidate_FinishReason;

  /**
   * List of ratings for the safety of a response candidate.
   *
   * There is at most one rating per category.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.SafetyRating safety_ratings = 5;
   */
  safetyRatings: SafetyRating[];

  /**
   * Output only. Citation information for model-generated candidate.
   *
   * This field may be populated with recitation information for any text
   * included in the `content`. These are passages that are "recited" from
   * copyrighted material in the foundational LLM's training data.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.CitationMetadata citation_metadata = 6;
   */
  citationMetadata?: CitationMetadata;

  /**
   * Output only. Token count for this candidate.
   *
   * @generated from field: int32 token_count = 7;
   */
  tokenCount: number;

  /**
   * Output only. Attribution information for sources that contributed to a
   * grounded answer.
   *
   * This field is populated for `GenerateAnswer` calls.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.GroundingAttribution grounding_attributions = 8;
   */
  groundingAttributions: GroundingAttribution[];

  /**
   * Output only. Grounding metadata for the candidate.
   *
   * This field is populated for `GenerateContent` calls.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.GroundingMetadata grounding_metadata = 9;
   */
  groundingMetadata?: GroundingMetadata;

  /**
   * Output only.
   *
   * @generated from field: double avg_logprobs = 10;
   */
  avgLogprobs: number;

  /**
   * Output only. Log-likelihood scores for the response tokens and top tokens
   *
   * @generated from field: google.ai.generativelanguage.v1beta.LogprobsResult logprobs_result = 11;
   */
  logprobsResult?: LogprobsResult;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.Candidate.
 * Use `create(CandidateSchema)` to create a new message.
 */
export const CandidateSchema: GenMessage<Candidate> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 4);

/**
 * Defines the reason why the model stopped generating tokens.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.Candidate.FinishReason
 */
export enum Candidate_FinishReason {
  /**
   * Default value. This value is unused.
   *
   * @generated from enum value: FINISH_REASON_UNSPECIFIED = 0;
   */
  FINISH_REASON_UNSPECIFIED = 0,

  /**
   * Natural stop point of the model or provided stop sequence.
   *
   * @generated from enum value: STOP = 1;
   */
  STOP = 1,

  /**
   * The maximum number of tokens as specified in the request was reached.
   *
   * @generated from enum value: MAX_TOKENS = 2;
   */
  MAX_TOKENS = 2,

  /**
   * The response candidate content was flagged for safety reasons.
   *
   * @generated from enum value: SAFETY = 3;
   */
  SAFETY = 3,

  /**
   * The response candidate content was flagged for recitation reasons.
   *
   * @generated from enum value: RECITATION = 4;
   */
  RECITATION = 4,

  /**
   * The response candidate content was flagged for using an unsupported
   * language.
   *
   * @generated from enum value: LANGUAGE = 6;
   */
  LANGUAGE = 6,

  /**
   * Unknown reason.
   *
   * @generated from enum value: OTHER = 5;
   */
  OTHER = 5,

  /**
   * Token generation stopped because the content contains forbidden terms.
   *
   * @generated from enum value: BLOCKLIST = 7;
   */
  BLOCKLIST = 7,

  /**
   * Token generation stopped for potentially containing prohibited content.
   *
   * @generated from enum value: PROHIBITED_CONTENT = 8;
   */
  PROHIBITED_CONTENT = 8,

  /**
   * Token generation stopped because the content potentially contains
   * Sensitive Personally Identifiable Information (SPII).
   *
   * @generated from enum value: SPII = 9;
   */
  SPII = 9,

  /**
   * The function call generated by the model is invalid.
   *
   * @generated from enum value: MALFORMED_FUNCTION_CALL = 10;
   */
  MALFORMED_FUNCTION_CALL = 10,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.Candidate.FinishReason.
 */
export const Candidate_FinishReasonSchema: GenEnum<Candidate_FinishReason> = /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 4, 0);

/**
 * Logprobs Result
 *
 * @generated from message google.ai.generativelanguage.v1beta.LogprobsResult
 */
export type LogprobsResult = Message<"google.ai.generativelanguage.v1beta.LogprobsResult"> & {
  /**
   * Length = total number of decoding steps.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.LogprobsResult.TopCandidates top_candidates = 1;
   */
  topCandidates: LogprobsResult_TopCandidates[];

  /**
   * Length = total number of decoding steps.
   * The chosen candidates may or may not be in top_candidates.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.LogprobsResult.Candidate chosen_candidates = 2;
   */
  chosenCandidates: LogprobsResult_Candidate[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.LogprobsResult.
 * Use `create(LogprobsResultSchema)` to create a new message.
 */
export const LogprobsResultSchema: GenMessage<LogprobsResult> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 5);

/**
 * Candidate for the logprobs token and score.
 *
 * @generated from message google.ai.generativelanguage.v1beta.LogprobsResult.Candidate
 */
export type LogprobsResult_Candidate = Message<"google.ai.generativelanguage.v1beta.LogprobsResult.Candidate"> & {
  /**
   * The candidate’s token string value.
   *
   * @generated from field: optional string token = 1;
   */
  token?: string;

  /**
   * The candidate’s token id value.
   *
   * @generated from field: optional int32 token_id = 3;
   */
  tokenId?: number;

  /**
   * The candidate's log probability.
   *
   * @generated from field: optional float log_probability = 2;
   */
  logProbability?: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.LogprobsResult.Candidate.
 * Use `create(LogprobsResult_CandidateSchema)` to create a new message.
 */
export const LogprobsResult_CandidateSchema: GenMessage<LogprobsResult_Candidate> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 5, 0);

/**
 * Candidates with top log probabilities at each decoding step.
 *
 * @generated from message google.ai.generativelanguage.v1beta.LogprobsResult.TopCandidates
 */
export type LogprobsResult_TopCandidates = Message<"google.ai.generativelanguage.v1beta.LogprobsResult.TopCandidates"> & {
  /**
   * Sorted by log probability in descending order.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.LogprobsResult.Candidate candidates = 1;
   */
  candidates: LogprobsResult_Candidate[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.LogprobsResult.TopCandidates.
 * Use `create(LogprobsResult_TopCandidatesSchema)` to create a new message.
 */
export const LogprobsResult_TopCandidatesSchema: GenMessage<LogprobsResult_TopCandidates> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 5, 1);

/**
 * Identifier for the source contributing to this attribution.
 *
 * @generated from message google.ai.generativelanguage.v1beta.AttributionSourceId
 */
export type AttributionSourceId = Message<"google.ai.generativelanguage.v1beta.AttributionSourceId"> & {
  /**
   * @generated from oneof google.ai.generativelanguage.v1beta.AttributionSourceId.source
   */
  source: {
    /**
     * Identifier for an inline passage.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.AttributionSourceId.GroundingPassageId grounding_passage = 1;
     */
    value: AttributionSourceId_GroundingPassageId;
    case: "groundingPassage";
  } | {
    /**
     * Identifier for a `Chunk` fetched via Semantic Retriever.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.AttributionSourceId.SemanticRetrieverChunk semantic_retriever_chunk = 2;
     */
    value: AttributionSourceId_SemanticRetrieverChunk;
    case: "semanticRetrieverChunk";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.AttributionSourceId.
 * Use `create(AttributionSourceIdSchema)` to create a new message.
 */
export const AttributionSourceIdSchema: GenMessage<AttributionSourceId> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 6);

/**
 * Identifier for a part within a `GroundingPassage`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.AttributionSourceId.GroundingPassageId
 */
export type AttributionSourceId_GroundingPassageId = Message<"google.ai.generativelanguage.v1beta.AttributionSourceId.GroundingPassageId"> & {
  /**
   * Output only. ID of the passage matching the `GenerateAnswerRequest`'s
   * `GroundingPassage.id`.
   *
   * @generated from field: string passage_id = 1;
   */
  passageId: string;

  /**
   * Output only. Index of the part within the `GenerateAnswerRequest`'s
   * `GroundingPassage.content`.
   *
   * @generated from field: int32 part_index = 2;
   */
  partIndex: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.AttributionSourceId.GroundingPassageId.
 * Use `create(AttributionSourceId_GroundingPassageIdSchema)` to create a new message.
 */
export const AttributionSourceId_GroundingPassageIdSchema: GenMessage<AttributionSourceId_GroundingPassageId> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 6, 0);

/**
 * Identifier for a `Chunk` retrieved via Semantic Retriever specified in the
 * `GenerateAnswerRequest` using `SemanticRetrieverConfig`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.AttributionSourceId.SemanticRetrieverChunk
 */
export type AttributionSourceId_SemanticRetrieverChunk = Message<"google.ai.generativelanguage.v1beta.AttributionSourceId.SemanticRetrieverChunk"> & {
  /**
   * Output only. Name of the source matching the request's
   * `SemanticRetrieverConfig.source`. Example: `corpora/123` or
   * `corpora/123/documents/abc`
   *
   * @generated from field: string source = 1;
   */
  source: string;

  /**
   * Output only. Name of the `Chunk` containing the attributed text.
   * Example: `corpora/123/documents/abc/chunks/xyz`
   *
   * @generated from field: string chunk = 2;
   */
  chunk: string;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.AttributionSourceId.SemanticRetrieverChunk.
 * Use `create(AttributionSourceId_SemanticRetrieverChunkSchema)` to create a new message.
 */
export const AttributionSourceId_SemanticRetrieverChunkSchema: GenMessage<AttributionSourceId_SemanticRetrieverChunk> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 6, 1);

/**
 * Attribution for a source that contributed to an answer.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GroundingAttribution
 */
export type GroundingAttribution = Message<"google.ai.generativelanguage.v1beta.GroundingAttribution"> & {
  /**
   * Output only. Identifier for the source contributing to this attribution.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.AttributionSourceId source_id = 3;
   */
  sourceId?: AttributionSourceId;

  /**
   * Grounding source content that makes up this attribution.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.Content content = 2;
   */
  content?: Content;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GroundingAttribution.
 * Use `create(GroundingAttributionSchema)` to create a new message.
 */
export const GroundingAttributionSchema: GenMessage<GroundingAttribution> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 7);

/**
 * Metadata related to retrieval in the grounding flow.
 *
 * @generated from message google.ai.generativelanguage.v1beta.RetrievalMetadata
 */
export type RetrievalMetadata = Message<"google.ai.generativelanguage.v1beta.RetrievalMetadata"> & {
  /**
   * Optional. Score indicating how likely information from google search could
   * help answer the prompt. The score is in the range [0, 1], where 0 is the
   * least likely and 1 is the most likely. This score is only populated when
   * google search grounding and dynamic retrieval is enabled. It will be
   * compared to the threshold to determine whether to trigger google search.
   *
   * @generated from field: float google_search_dynamic_retrieval_score = 2;
   */
  googleSearchDynamicRetrievalScore: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.RetrievalMetadata.
 * Use `create(RetrievalMetadataSchema)` to create a new message.
 */
export const RetrievalMetadataSchema: GenMessage<RetrievalMetadata> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 8);

/**
 * Metadata returned to client when grounding is enabled.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GroundingMetadata
 */
export type GroundingMetadata = Message<"google.ai.generativelanguage.v1beta.GroundingMetadata"> & {
  /**
   * Optional. Google search entry for the following-up web searches.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.SearchEntryPoint search_entry_point = 1;
   */
  searchEntryPoint?: SearchEntryPoint;

  /**
   * List of supporting references retrieved from specified grounding source.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.GroundingChunk grounding_chunks = 2;
   */
  groundingChunks: GroundingChunk[];

  /**
   * List of grounding support.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.GroundingSupport grounding_supports = 3;
   */
  groundingSupports: GroundingSupport[];

  /**
   * Metadata related to retrieval in the grounding flow.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.RetrievalMetadata retrieval_metadata = 4;
   */
  retrievalMetadata?: RetrievalMetadata;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GroundingMetadata.
 * Use `create(GroundingMetadataSchema)` to create a new message.
 */
export const GroundingMetadataSchema: GenMessage<GroundingMetadata> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 9);

/**
 * Google search entry point.
 *
 * @generated from message google.ai.generativelanguage.v1beta.SearchEntryPoint
 */
export type SearchEntryPoint = Message<"google.ai.generativelanguage.v1beta.SearchEntryPoint"> & {
  /**
   * Optional. Web content snippet that can be embedded in a web page or an app
   * webview.
   *
   * @generated from field: string rendered_content = 1;
   */
  renderedContent: string;

  /**
   * Optional. Base64 encoded JSON representing array of <search term, search
   * url> tuple.
   *
   * @generated from field: bytes sdk_blob = 2;
   */
  sdkBlob: Uint8Array;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.SearchEntryPoint.
 * Use `create(SearchEntryPointSchema)` to create a new message.
 */
export const SearchEntryPointSchema: GenMessage<SearchEntryPoint> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 10);

/**
 * Grounding chunk.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GroundingChunk
 */
export type GroundingChunk = Message<"google.ai.generativelanguage.v1beta.GroundingChunk"> & {
  /**
   * Chunk type.
   *
   * @generated from oneof google.ai.generativelanguage.v1beta.GroundingChunk.chunk_type
   */
  chunkType: {
    /**
     * Grounding chunk from the web.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.GroundingChunk.Web web = 1;
     */
    value: GroundingChunk_Web;
    case: "web";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GroundingChunk.
 * Use `create(GroundingChunkSchema)` to create a new message.
 */
export const GroundingChunkSchema: GenMessage<GroundingChunk> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 11);

/**
 * Chunk from the web.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GroundingChunk.Web
 */
export type GroundingChunk_Web = Message<"google.ai.generativelanguage.v1beta.GroundingChunk.Web"> & {
  /**
   * URI reference of the chunk.
   *
   * @generated from field: optional string uri = 1;
   */
  uri?: string;

  /**
   * Title of the chunk.
   *
   * @generated from field: optional string title = 2;
   */
  title?: string;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GroundingChunk.Web.
 * Use `create(GroundingChunk_WebSchema)` to create a new message.
 */
export const GroundingChunk_WebSchema: GenMessage<GroundingChunk_Web> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 11, 0);

/**
 * Segment of the content.
 *
 * @generated from message google.ai.generativelanguage.v1beta.Segment
 */
export type Segment = Message<"google.ai.generativelanguage.v1beta.Segment"> & {
  /**
   * Output only. The index of a Part object within its parent Content object.
   *
   * @generated from field: int32 part_index = 1;
   */
  partIndex: number;

  /**
   * Output only. Start index in the given Part, measured in bytes. Offset from
   * the start of the Part, inclusive, starting at zero.
   *
   * @generated from field: int32 start_index = 2;
   */
  startIndex: number;

  /**
   * Output only. End index in the given Part, measured in bytes. Offset from
   * the start of the Part, exclusive, starting at zero.
   *
   * @generated from field: int32 end_index = 3;
   */
  endIndex: number;

  /**
   * Output only. The text corresponding to the segment from the response.
   *
   * @generated from field: string text = 4;
   */
  text: string;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.Segment.
 * Use `create(SegmentSchema)` to create a new message.
 */
export const SegmentSchema: GenMessage<Segment> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 12);

/**
 * Grounding support.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GroundingSupport
 */
export type GroundingSupport = Message<"google.ai.generativelanguage.v1beta.GroundingSupport"> & {
  /**
   * Segment of the content this support belongs to.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.Segment segment = 1;
   */
  segment?: Segment;

  /**
   * A list of indices (into 'grounding_chunk') specifying the
   * citations associated with the claim. For instance [1,3,4] means
   * that grounding_chunk[1], grounding_chunk[3],
   * grounding_chunk[4] are the retrieved content attributed to the claim.
   *
   * @generated from field: repeated int32 grounding_chunk_indices = 2;
   */
  groundingChunkIndices: number[];

  /**
   * Confidence score of the support references. Ranges from 0 to 1. 1 is the
   * most confident. This list must have the same size as the
   * grounding_chunk_indices.
   *
   * @generated from field: repeated float confidence_scores = 3;
   */
  confidenceScores: number[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GroundingSupport.
 * Use `create(GroundingSupportSchema)` to create a new message.
 */
export const GroundingSupportSchema: GenMessage<GroundingSupport> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 13);

/**
 * Request to generate a grounded answer from the `Model`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerateAnswerRequest
 */
export type GenerateAnswerRequest = Message<"google.ai.generativelanguage.v1beta.GenerateAnswerRequest"> & {
  /**
   * The sources in which to ground the answer.
   *
   * @generated from oneof google.ai.generativelanguage.v1beta.GenerateAnswerRequest.grounding_source
   */
  groundingSource: {
    /**
     * Passages provided inline with the request.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.GroundingPassages inline_passages = 6;
     */
    value: GroundingPassages;
    case: "inlinePassages";
  } | {
    /**
     * Content retrieved from resources created via the Semantic Retriever
     * API.
     *
     * @generated from field: google.ai.generativelanguage.v1beta.SemanticRetrieverConfig semantic_retriever = 7;
     */
    value: SemanticRetrieverConfig;
    case: "semanticRetriever";
  } | { case: undefined; value?: undefined };

  /**
   * Required. The name of the `Model` to use for generating the grounded
   * response.
   *
   * Format: `model=models/{model}`.
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Required. The content of the current conversation with the `Model`. For
   * single-turn queries, this is a single question to answer. For multi-turn
   * queries, this is a repeated field that contains conversation history and
   * the last `Content` in the list containing the question.
   *
   * Note: `GenerateAnswer` only supports queries in English.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.Content contents = 2;
   */
  contents: Content[];

  /**
   * Required. Style in which answers should be returned.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.GenerateAnswerRequest.AnswerStyle answer_style = 5;
   */
  answerStyle: GenerateAnswerRequest_AnswerStyle;

  /**
   * Optional. A list of unique `SafetySetting` instances for blocking unsafe
   * content.
   *
   * This will be enforced on the `GenerateAnswerRequest.contents` and
   * `GenerateAnswerResponse.candidate`. There should not be more than one
   * setting for each `SafetyCategory` type. The API will block any contents and
   * responses that fail to meet the thresholds set by these settings. This list
   * overrides the default settings for each `SafetyCategory` specified in the
   * safety_settings. If there is no `SafetySetting` for a given
   * `SafetyCategory` provided in the list, the API will use the default safety
   * setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH,
   * HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT,
   * HARM_CATEGORY_HARASSMENT are supported.
   * Refer to the
   * [guide](https://ai.google.dev/gemini-api/docs/safety-settings)
   * for detailed information on available safety settings. Also refer to the
   * [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance) to
   * learn how to incorporate safety considerations in your AI applications.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.SafetySetting safety_settings = 3;
   */
  safetySettings: SafetySetting[];

  /**
   * Optional. Controls the randomness of the output.
   *
   * Values can range from [0.0,1.0], inclusive. A value closer to 1.0 will
   * produce responses that are more varied and creative, while a value closer
   * to 0.0 will typically result in more straightforward responses from the
   * model. A low temperature (~0.2) is usually recommended for
   * Attributed-Question-Answering use cases.
   *
   * @generated from field: optional float temperature = 4;
   */
  temperature?: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerateAnswerRequest.
 * Use `create(GenerateAnswerRequestSchema)` to create a new message.
 */
export const GenerateAnswerRequestSchema: GenMessage<GenerateAnswerRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 14);

/**
 * Style for grounded answers.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.GenerateAnswerRequest.AnswerStyle
 */
export enum GenerateAnswerRequest_AnswerStyle {
  /**
   * Unspecified answer style.
   *
   * @generated from enum value: ANSWER_STYLE_UNSPECIFIED = 0;
   */
  ANSWER_STYLE_UNSPECIFIED = 0,

  /**
   * Succint but abstract style.
   *
   * @generated from enum value: ABSTRACTIVE = 1;
   */
  ABSTRACTIVE = 1,

  /**
   * Very brief and extractive style.
   *
   * @generated from enum value: EXTRACTIVE = 2;
   */
  EXTRACTIVE = 2,

  /**
   * Verbose style including extra details. The response may be formatted as a
   * sentence, paragraph, multiple paragraphs, or bullet points, etc.
   *
   * @generated from enum value: VERBOSE = 3;
   */
  VERBOSE = 3,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.GenerateAnswerRequest.AnswerStyle.
 */
export const GenerateAnswerRequest_AnswerStyleSchema: GenEnum<GenerateAnswerRequest_AnswerStyle> = /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 14, 0);

/**
 * Response from the model for a grounded answer.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerateAnswerResponse
 */
export type GenerateAnswerResponse = Message<"google.ai.generativelanguage.v1beta.GenerateAnswerResponse"> & {
  /**
   * Candidate answer from the model.
   *
   * Note: The model *always* attempts to provide a grounded answer, even when
   * the answer is unlikely to be answerable from the given passages.
   * In that case, a low-quality or ungrounded answer may be provided, along
   * with a low `answerable_probability`.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.Candidate answer = 1;
   */
  answer?: Candidate;

  /**
   * Output only. The model's estimate of the probability that its answer is
   * correct and grounded in the input passages.
   *
   * A low `answerable_probability` indicates that the answer might not be
   * grounded in the sources.
   *
   * When `answerable_probability` is low, you may want to:
   *
   * * Display a message to the effect of "We couldn’t answer that question" to
   * the user.
   * * Fall back to a general-purpose LLM that answers the question from world
   * knowledge. The threshold and nature of such fallbacks will depend on
   * individual use cases. `0.5` is a good starting threshold.
   *
   * @generated from field: optional float answerable_probability = 2;
   */
  answerableProbability?: number;

  /**
   * Output only. Feedback related to the input data used to answer the
   * question, as opposed to the model-generated response to the question.
   *
   * The input data can be one or more of the following:
   *
   * - Question specified by the last entry in `GenerateAnswerRequest.content`
   * - Conversation history specified by the other entries in
   * `GenerateAnswerRequest.content`
   * - Grounding sources (`GenerateAnswerRequest.semantic_retriever` or
   * `GenerateAnswerRequest.inline_passages`)
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.GenerateAnswerResponse.InputFeedback input_feedback = 3;
   */
  inputFeedback?: GenerateAnswerResponse_InputFeedback;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerateAnswerResponse.
 * Use `create(GenerateAnswerResponseSchema)` to create a new message.
 */
export const GenerateAnswerResponseSchema: GenMessage<GenerateAnswerResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 15);

/**
 * Feedback related to the input data used to answer the question, as opposed
 * to the model-generated response to the question.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerateAnswerResponse.InputFeedback
 */
export type GenerateAnswerResponse_InputFeedback = Message<"google.ai.generativelanguage.v1beta.GenerateAnswerResponse.InputFeedback"> & {
  /**
   * Optional. If set, the input was blocked and no candidates are returned.
   * Rephrase the input.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.GenerateAnswerResponse.InputFeedback.BlockReason block_reason = 1;
   */
  blockReason?: GenerateAnswerResponse_InputFeedback_BlockReason;

  /**
   * Ratings for safety of the input.
   * There is at most one rating per category.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.SafetyRating safety_ratings = 2;
   */
  safetyRatings: SafetyRating[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerateAnswerResponse.InputFeedback.
 * Use `create(GenerateAnswerResponse_InputFeedbackSchema)` to create a new message.
 */
export const GenerateAnswerResponse_InputFeedbackSchema: GenMessage<GenerateAnswerResponse_InputFeedback> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 15, 0);

/**
 * Specifies what was the reason why input was blocked.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.GenerateAnswerResponse.InputFeedback.BlockReason
 */
export enum GenerateAnswerResponse_InputFeedback_BlockReason {
  /**
   * Default value. This value is unused.
   *
   * @generated from enum value: BLOCK_REASON_UNSPECIFIED = 0;
   */
  BLOCK_REASON_UNSPECIFIED = 0,

  /**
   * Input was blocked due to safety reasons. Inspect
   * `safety_ratings` to understand which safety category blocked it.
   *
   * @generated from enum value: SAFETY = 1;
   */
  SAFETY = 1,

  /**
   * Input was blocked due to other reasons.
   *
   * @generated from enum value: OTHER = 2;
   */
  OTHER = 2,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.GenerateAnswerResponse.InputFeedback.BlockReason.
 */
export const GenerateAnswerResponse_InputFeedback_BlockReasonSchema: GenEnum<GenerateAnswerResponse_InputFeedback_BlockReason> = /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 15, 0, 0);

/**
 * Request containing the `Content` for the model to embed.
 *
 * @generated from message google.ai.generativelanguage.v1beta.EmbedContentRequest
 */
export type EmbedContentRequest = Message<"google.ai.generativelanguage.v1beta.EmbedContentRequest"> & {
  /**
   * Required. The model's resource name. This serves as an ID for the Model to
   * use.
   *
   * This name should match a model name returned by the `ListModels` method.
   *
   * Format: `models/{model}`
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Required. The content to embed. Only the `parts.text` fields will be
   * counted.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.Content content = 2;
   */
  content?: Content;

  /**
   * Optional. Optional task type for which the embeddings will be used. Can
   * only be set for `models/embedding-001`.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.TaskType task_type = 3;
   */
  taskType?: TaskType;

  /**
   * Optional. An optional title for the text. Only applicable when TaskType is
   * `RETRIEVAL_DOCUMENT`.
   *
   * Note: Specifying a `title` for `RETRIEVAL_DOCUMENT` provides better quality
   * embeddings for retrieval.
   *
   * @generated from field: optional string title = 4;
   */
  title?: string;

  /**
   * Optional. Optional reduced dimension for the output embedding. If set,
   * excessive values in the output embedding are truncated from the end.
   * Supported by newer models since 2024 only. You cannot set this value if
   * using the earlier model (`models/embedding-001`).
   *
   * @generated from field: optional int32 output_dimensionality = 5;
   */
  outputDimensionality?: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.EmbedContentRequest.
 * Use `create(EmbedContentRequestSchema)` to create a new message.
 */
export const EmbedContentRequestSchema: GenMessage<EmbedContentRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 16);

/**
 * A list of floats representing an embedding.
 *
 * @generated from message google.ai.generativelanguage.v1beta.ContentEmbedding
 */
export type ContentEmbedding = Message<"google.ai.generativelanguage.v1beta.ContentEmbedding"> & {
  /**
   * The embedding values.
   *
   * @generated from field: repeated float values = 1;
   */
  values: number[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.ContentEmbedding.
 * Use `create(ContentEmbeddingSchema)` to create a new message.
 */
export const ContentEmbeddingSchema: GenMessage<ContentEmbedding> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 17);

/**
 * The response to an `EmbedContentRequest`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.EmbedContentResponse
 */
export type EmbedContentResponse = Message<"google.ai.generativelanguage.v1beta.EmbedContentResponse"> & {
  /**
   * Output only. The embedding generated from the input content.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.ContentEmbedding embedding = 1;
   */
  embedding?: ContentEmbedding;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.EmbedContentResponse.
 * Use `create(EmbedContentResponseSchema)` to create a new message.
 */
export const EmbedContentResponseSchema: GenMessage<EmbedContentResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 18);

/**
 * Batch request to get embeddings from the model for a list of prompts.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BatchEmbedContentsRequest
 */
export type BatchEmbedContentsRequest = Message<"google.ai.generativelanguage.v1beta.BatchEmbedContentsRequest"> & {
  /**
   * Required. The model's resource name. This serves as an ID for the Model to
   * use.
   *
   * This name should match a model name returned by the `ListModels` method.
   *
   * Format: `models/{model}`
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Required. Embed requests for the batch. The model in each of these requests
   * must match the model specified `BatchEmbedContentsRequest.model`.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.EmbedContentRequest requests = 2;
   */
  requests: EmbedContentRequest[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.BatchEmbedContentsRequest.
 * Use `create(BatchEmbedContentsRequestSchema)` to create a new message.
 */
export const BatchEmbedContentsRequestSchema: GenMessage<BatchEmbedContentsRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 19);

/**
 * The response to a `BatchEmbedContentsRequest`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BatchEmbedContentsResponse
 */
export type BatchEmbedContentsResponse = Message<"google.ai.generativelanguage.v1beta.BatchEmbedContentsResponse"> & {
  /**
   * Output only. The embeddings for each request, in the same order as provided
   * in the batch request.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.ContentEmbedding embeddings = 1;
   */
  embeddings: ContentEmbedding[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.BatchEmbedContentsResponse.
 * Use `create(BatchEmbedContentsResponseSchema)` to create a new message.
 */
export const BatchEmbedContentsResponseSchema: GenMessage<BatchEmbedContentsResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 20);

/**
 * Counts the number of tokens in the `prompt` sent to a model.
 *
 * Models may tokenize text differently, so each model may return a different
 * `token_count`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.CountTokensRequest
 */
export type CountTokensRequest = Message<"google.ai.generativelanguage.v1beta.CountTokensRequest"> & {
  /**
   * Required. The model's resource name. This serves as an ID for the Model to
   * use.
   *
   * This name should match a model name returned by the `ListModels` method.
   *
   * Format: `models/{model}`
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Optional. The input given to the model as a prompt. This field is ignored
   * when `generate_content_request` is set.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.Content contents = 2;
   */
  contents: Content[];

  /**
   * Optional. The overall input given to the `Model`. This includes the prompt
   * as well as other model steering information like [system
   * instructions](https://ai.google.dev/gemini-api/docs/system-instructions),
   * and/or function declarations for [function
   * calling](https://ai.google.dev/gemini-api/docs/function-calling).
   * `Model`s/`Content`s and `generate_content_request`s are mutually
   * exclusive. You can either send `Model` + `Content`s or a
   * `generate_content_request`, but never both.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.GenerateContentRequest generate_content_request = 3;
   */
  generateContentRequest?: GenerateContentRequest;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.CountTokensRequest.
 * Use `create(CountTokensRequestSchema)` to create a new message.
 */
export const CountTokensRequestSchema: GenMessage<CountTokensRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 21);

/**
 * A response from `CountTokens`.
 *
 * It returns the model's `token_count` for the `prompt`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.CountTokensResponse
 */
export type CountTokensResponse = Message<"google.ai.generativelanguage.v1beta.CountTokensResponse"> & {
  /**
   * The number of tokens that the `Model` tokenizes the `prompt` into. Always
   * non-negative.
   *
   * @generated from field: int32 total_tokens = 1;
   */
  totalTokens: number;

  /**
   * Number of tokens in the cached part of the prompt (the cached content).
   *
   * @generated from field: int32 cached_content_token_count = 5;
   */
  cachedContentTokenCount: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.CountTokensResponse.
 * Use `create(CountTokensResponseSchema)` to create a new message.
 */
export const CountTokensResponseSchema: GenMessage<CountTokensResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_generative_service, 22);

/**
 * Type of task for which the embedding will be used.
 *
 * @generated from enum google.ai.generativelanguage.v1beta.TaskType
 */
export enum TaskType {
  /**
   * Unset value, which will default to one of the other enum values.
   *
   * @generated from enum value: TASK_TYPE_UNSPECIFIED = 0;
   */
  TASK_TYPE_UNSPECIFIED = 0,

  /**
   * Specifies the given text is a query in a search/retrieval setting.
   *
   * @generated from enum value: RETRIEVAL_QUERY = 1;
   */
  RETRIEVAL_QUERY = 1,

  /**
   * Specifies the given text is a document from the corpus being searched.
   *
   * @generated from enum value: RETRIEVAL_DOCUMENT = 2;
   */
  RETRIEVAL_DOCUMENT = 2,

  /**
   * Specifies the given text will be used for STS.
   *
   * @generated from enum value: SEMANTIC_SIMILARITY = 3;
   */
  SEMANTIC_SIMILARITY = 3,

  /**
   * Specifies that the given text will be classified.
   *
   * @generated from enum value: CLASSIFICATION = 4;
   */
  CLASSIFICATION = 4,

  /**
   * Specifies that the embeddings will be used for clustering.
   *
   * @generated from enum value: CLUSTERING = 5;
   */
  CLUSTERING = 5,

  /**
   * Specifies that the given text will be used for question answering.
   *
   * @generated from enum value: QUESTION_ANSWERING = 6;
   */
  QUESTION_ANSWERING = 6,

  /**
   * Specifies that the given text will be used for fact verification.
   *
   * @generated from enum value: FACT_VERIFICATION = 7;
   */
  FACT_VERIFICATION = 7,
}

/**
 * Describes the enum google.ai.generativelanguage.v1beta.TaskType.
 */
export const TaskTypeSchema: GenEnum<TaskType> = /*@__PURE__*/
  enumDesc(file_google_ai_generativelanguage_v1beta_generative_service, 0);

/**
 * API for using Large Models that generate multimodal content and have
 * additional capabilities beyond text generation.
 *
 * @generated from service google.ai.generativelanguage.v1beta.GenerativeService
 */
export const GenerativeService: GenService<{
  /**
   * Generates a model response given an input `GenerateContentRequest`.
   * Refer to the [text generation
   * guide](https://ai.google.dev/gemini-api/docs/text-generation) for detailed
   * usage information. Input capabilities differ between models, including
   * tuned models. Refer to the [model
   * guide](https://ai.google.dev/gemini-api/docs/models/gemini) and [tuning
   * guide](https://ai.google.dev/gemini-api/docs/model-tuning) for details.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.GenerativeService.GenerateContent
   */
  generateContent: {
    methodKind: "unary";
    input: typeof GenerateContentRequestSchema;
    output: typeof GenerateContentResponseSchema;
  },
  /**
   * Generates a grounded answer from the model given an input
   * `GenerateAnswerRequest`.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.GenerativeService.GenerateAnswer
   */
  generateAnswer: {
    methodKind: "unary";
    input: typeof GenerateAnswerRequestSchema;
    output: typeof GenerateAnswerResponseSchema;
  },
  /**
   * Generates a [streamed
   * response](https://ai.google.dev/gemini-api/docs/text-generation?lang=python#generate-a-text-stream)
   * from the model given an input `GenerateContentRequest`.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.GenerativeService.StreamGenerateContent
   */
  streamGenerateContent: {
    methodKind: "server_streaming";
    input: typeof GenerateContentRequestSchema;
    output: typeof GenerateContentResponseSchema;
  },
  /**
   * Generates a text embedding vector from the input `Content` using the
   * specified [Gemini Embedding
   * model](https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding).
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.GenerativeService.EmbedContent
   */
  embedContent: {
    methodKind: "unary";
    input: typeof EmbedContentRequestSchema;
    output: typeof EmbedContentResponseSchema;
  },
  /**
   * Generates multiple embedding vectors from the input `Content` which
   * consists of a batch of strings represented as `EmbedContentRequest`
   * objects.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents
   */
  batchEmbedContents: {
    methodKind: "unary";
    input: typeof BatchEmbedContentsRequestSchema;
    output: typeof BatchEmbedContentsResponseSchema;
  },
  /**
   * Runs a model's tokenizer on input `Content` and returns the token count.
   * Refer to the [tokens guide](https://ai.google.dev/gemini-api/docs/tokens)
   * to learn more about tokens.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.GenerativeService.CountTokens
   */
  countTokens: {
    methodKind: "unary";
    input: typeof CountTokensRequestSchema;
    output: typeof CountTokensResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_ai_generativelanguage_v1beta_generative_service, 0);

