// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/ai/generativelanguage/v1beta/text_service.proto (package google.ai.generativelanguage.v1beta, syntax proto3)
/* eslint-disable */

import type { GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import type { CitationMetadata } from "./citation_pb";
import { file_google_ai_generativelanguage_v1beta_citation } from "./citation_pb";
import type { ContentFilter, SafetyFeedback, SafetyRating, SafetySetting } from "./safety_pb";
import { file_google_ai_generativelanguage_v1beta_safety } from "./safety_pb";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import { file_google_api_resource } from "../../../api/resource_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/ai/generativelanguage/v1beta/text_service.proto.
 */
export const file_google_ai_generativelanguage_v1beta_text_service: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_ai_generativelanguage_v1beta_citation, file_google_ai_generativelanguage_v1beta_safety, file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_api_resource]);

/**
 * Request to generate a text completion response from the model.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerateTextRequest
 */
export type GenerateTextRequest = Message<"google.ai.generativelanguage.v1beta.GenerateTextRequest"> & {
  /**
   * Required. The name of the `Model` or `TunedModel` to use for generating the
   * completion.
   * Examples:
   *  models/text-bison-001
   *  tunedModels/sentence-translator-u3b7m
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Required. The free-form input text given to the model as a prompt.
   *
   * Given a prompt, the model will generate a TextCompletion response it
   * predicts as the completion of the input text.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.TextPrompt prompt = 2;
   */
  prompt?: TextPrompt;

  /**
   * Optional. Controls the randomness of the output.
   * Note: The default value varies by model, see the `Model.temperature`
   * attribute of the `Model` returned the `getModel` function.
   *
   * Values can range from [0.0,1.0],
   * inclusive. A value closer to 1.0 will produce responses that are more
   * varied and creative, while a value closer to 0.0 will typically result in
   * more straightforward responses from the model.
   *
   * @generated from field: optional float temperature = 3;
   */
  temperature?: number;

  /**
   * Optional. Number of generated responses to return.
   *
   * This value must be between [1, 8], inclusive. If unset, this will default
   * to 1.
   *
   * @generated from field: optional int32 candidate_count = 4;
   */
  candidateCount?: number;

  /**
   * Optional. The maximum number of tokens to include in a candidate.
   *
   * If unset, this will default to output_token_limit specified in the `Model`
   * specification.
   *
   * @generated from field: optional int32 max_output_tokens = 5;
   */
  maxOutputTokens?: number;

  /**
   * Optional. The maximum cumulative probability of tokens to consider when
   * sampling.
   *
   * The model uses combined Top-k and nucleus sampling.
   *
   * Tokens are sorted based on their assigned probabilities so that only the
   * most likely tokens are considered. Top-k sampling directly limits the
   * maximum number of tokens to consider, while Nucleus sampling limits number
   * of tokens based on the cumulative probability.
   *
   * Note: The default value varies by model, see the `Model.top_p`
   * attribute of the `Model` returned the `getModel` function.
   *
   * @generated from field: optional float top_p = 6;
   */
  topP?: number;

  /**
   * Optional. The maximum number of tokens to consider when sampling.
   *
   * The model uses combined Top-k and nucleus sampling.
   *
   * Top-k sampling considers the set of `top_k` most probable tokens.
   * Defaults to 40.
   *
   * Note: The default value varies by model, see the `Model.top_k`
   * attribute of the `Model` returned the `getModel` function.
   *
   * @generated from field: optional int32 top_k = 7;
   */
  topK?: number;

  /**
   * Optional. A list of unique `SafetySetting` instances for blocking unsafe
   * content.
   *
   * that will be enforced on the `GenerateTextRequest.prompt` and
   * `GenerateTextResponse.candidates`. There should not be more than one
   * setting for each `SafetyCategory` type. The API will block any prompts and
   * responses that fail to meet the thresholds set by these settings. This list
   * overrides the default settings for each `SafetyCategory` specified in the
   * safety_settings. If there is no `SafetySetting` for a given
   * `SafetyCategory` provided in the list, the API will use the default safety
   * setting for that category. Harm categories HARM_CATEGORY_DEROGATORY,
   * HARM_CATEGORY_TOXICITY, HARM_CATEGORY_VIOLENCE, HARM_CATEGORY_SEXUAL,
   * HARM_CATEGORY_MEDICAL, HARM_CATEGORY_DANGEROUS are supported in text
   * service.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.SafetySetting safety_settings = 8;
   */
  safetySettings: SafetySetting[];

  /**
   * The set of character sequences (up to 5) that will stop output generation.
   * If specified, the API will stop at the first appearance of a stop
   * sequence. The stop sequence will not be included as part of the response.
   *
   * @generated from field: repeated string stop_sequences = 9;
   */
  stopSequences: string[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerateTextRequest.
 * Use `create(GenerateTextRequestSchema)` to create a new message.
 */
export const GenerateTextRequestSchema: GenMessage<GenerateTextRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_text_service, 0);

/**
 * The response from the model, including candidate completions.
 *
 * @generated from message google.ai.generativelanguage.v1beta.GenerateTextResponse
 */
export type GenerateTextResponse = Message<"google.ai.generativelanguage.v1beta.GenerateTextResponse"> & {
  /**
   * Candidate responses from the model.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.TextCompletion candidates = 1;
   */
  candidates: TextCompletion[];

  /**
   * A set of content filtering metadata for the prompt and response
   * text.
   *
   * This indicates which `SafetyCategory`(s) blocked a
   * candidate from this response, the lowest `HarmProbability`
   * that triggered a block, and the HarmThreshold setting for that category.
   * This indicates the smallest change to the `SafetySettings` that would be
   * necessary to unblock at least 1 response.
   *
   * The blocking is configured by the `SafetySettings` in the request (or the
   * default `SafetySettings` of the API).
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.ContentFilter filters = 3;
   */
  filters: ContentFilter[];

  /**
   * Returns any safety feedback related to content filtering.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.SafetyFeedback safety_feedback = 4;
   */
  safetyFeedback: SafetyFeedback[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.GenerateTextResponse.
 * Use `create(GenerateTextResponseSchema)` to create a new message.
 */
export const GenerateTextResponseSchema: GenMessage<GenerateTextResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_text_service, 1);

/**
 * Text given to the model as a prompt.
 *
 * The Model will use this TextPrompt to Generate a text completion.
 *
 * @generated from message google.ai.generativelanguage.v1beta.TextPrompt
 */
export type TextPrompt = Message<"google.ai.generativelanguage.v1beta.TextPrompt"> & {
  /**
   * Required. The prompt text.
   *
   * @generated from field: string text = 1;
   */
  text: string;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.TextPrompt.
 * Use `create(TextPromptSchema)` to create a new message.
 */
export const TextPromptSchema: GenMessage<TextPrompt> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_text_service, 2);

/**
 * Output text returned from a model.
 *
 * @generated from message google.ai.generativelanguage.v1beta.TextCompletion
 */
export type TextCompletion = Message<"google.ai.generativelanguage.v1beta.TextCompletion"> & {
  /**
   * Output only. The generated text returned from the model.
   *
   * @generated from field: string output = 1;
   */
  output: string;

  /**
   * Ratings for the safety of a response.
   *
   * There is at most one rating per category.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.SafetyRating safety_ratings = 2;
   */
  safetyRatings: SafetyRating[];

  /**
   * Output only. Citation information for model-generated `output` in this
   * `TextCompletion`.
   *
   * This field may be populated with attribution information for any text
   * included in the `output`.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.CitationMetadata citation_metadata = 3;
   */
  citationMetadata?: CitationMetadata;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.TextCompletion.
 * Use `create(TextCompletionSchema)` to create a new message.
 */
export const TextCompletionSchema: GenMessage<TextCompletion> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_text_service, 3);

/**
 * Request to get a text embedding from the model.
 *
 * @generated from message google.ai.generativelanguage.v1beta.EmbedTextRequest
 */
export type EmbedTextRequest = Message<"google.ai.generativelanguage.v1beta.EmbedTextRequest"> & {
  /**
   * Required. The model name to use with the format model=models/{model}.
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Optional. The free-form input text that the model will turn into an
   * embedding.
   *
   * @generated from field: string text = 2;
   */
  text: string;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.EmbedTextRequest.
 * Use `create(EmbedTextRequestSchema)` to create a new message.
 */
export const EmbedTextRequestSchema: GenMessage<EmbedTextRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_text_service, 4);

/**
 * The response to a EmbedTextRequest.
 *
 * @generated from message google.ai.generativelanguage.v1beta.EmbedTextResponse
 */
export type EmbedTextResponse = Message<"google.ai.generativelanguage.v1beta.EmbedTextResponse"> & {
  /**
   * Output only. The embedding generated from the input text.
   *
   * @generated from field: optional google.ai.generativelanguage.v1beta.Embedding embedding = 1;
   */
  embedding?: Embedding;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.EmbedTextResponse.
 * Use `create(EmbedTextResponseSchema)` to create a new message.
 */
export const EmbedTextResponseSchema: GenMessage<EmbedTextResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_text_service, 5);

/**
 * Batch request to get a text embedding from the model.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BatchEmbedTextRequest
 */
export type BatchEmbedTextRequest = Message<"google.ai.generativelanguage.v1beta.BatchEmbedTextRequest"> & {
  /**
   * Required. The name of the `Model` to use for generating the embedding.
   * Examples:
   *  models/embedding-gecko-001
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Optional. The free-form input texts that the model will turn into an
   * embedding. The current limit is 100 texts, over which an error will be
   * thrown.
   *
   * @generated from field: repeated string texts = 2;
   */
  texts: string[];

  /**
   * Optional. Embed requests for the batch. Only one of `texts` or `requests`
   * can be set.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.EmbedTextRequest requests = 3;
   */
  requests: EmbedTextRequest[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.BatchEmbedTextRequest.
 * Use `create(BatchEmbedTextRequestSchema)` to create a new message.
 */
export const BatchEmbedTextRequestSchema: GenMessage<BatchEmbedTextRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_text_service, 6);

/**
 * The response to a EmbedTextRequest.
 *
 * @generated from message google.ai.generativelanguage.v1beta.BatchEmbedTextResponse
 */
export type BatchEmbedTextResponse = Message<"google.ai.generativelanguage.v1beta.BatchEmbedTextResponse"> & {
  /**
   * Output only. The embeddings generated from the input text.
   *
   * @generated from field: repeated google.ai.generativelanguage.v1beta.Embedding embeddings = 1;
   */
  embeddings: Embedding[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.BatchEmbedTextResponse.
 * Use `create(BatchEmbedTextResponseSchema)` to create a new message.
 */
export const BatchEmbedTextResponseSchema: GenMessage<BatchEmbedTextResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_text_service, 7);

/**
 * A list of floats representing the embedding.
 *
 * @generated from message google.ai.generativelanguage.v1beta.Embedding
 */
export type Embedding = Message<"google.ai.generativelanguage.v1beta.Embedding"> & {
  /**
   * The embedding values.
   *
   * @generated from field: repeated float value = 1;
   */
  value: number[];
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.Embedding.
 * Use `create(EmbeddingSchema)` to create a new message.
 */
export const EmbeddingSchema: GenMessage<Embedding> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_text_service, 8);

/**
 * Counts the number of tokens in the `prompt` sent to a model.
 *
 * Models may tokenize text differently, so each model may return a different
 * `token_count`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.CountTextTokensRequest
 */
export type CountTextTokensRequest = Message<"google.ai.generativelanguage.v1beta.CountTextTokensRequest"> & {
  /**
   * Required. The model's resource name. This serves as an ID for the Model to
   * use.
   *
   * This name should match a model name returned by the `ListModels` method.
   *
   * Format: `models/{model}`
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Required. The free-form input text given to the model as a prompt.
   *
   * @generated from field: google.ai.generativelanguage.v1beta.TextPrompt prompt = 2;
   */
  prompt?: TextPrompt;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.CountTextTokensRequest.
 * Use `create(CountTextTokensRequestSchema)` to create a new message.
 */
export const CountTextTokensRequestSchema: GenMessage<CountTextTokensRequest> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_text_service, 9);

/**
 * A response from `CountTextTokens`.
 *
 * It returns the model's `token_count` for the `prompt`.
 *
 * @generated from message google.ai.generativelanguage.v1beta.CountTextTokensResponse
 */
export type CountTextTokensResponse = Message<"google.ai.generativelanguage.v1beta.CountTextTokensResponse"> & {
  /**
   * The number of tokens that the `model` tokenizes the `prompt` into.
   *
   * Always non-negative.
   *
   * @generated from field: int32 token_count = 1;
   */
  tokenCount: number;
};

/**
 * Describes the message google.ai.generativelanguage.v1beta.CountTextTokensResponse.
 * Use `create(CountTextTokensResponseSchema)` to create a new message.
 */
export const CountTextTokensResponseSchema: GenMessage<CountTextTokensResponse> = /*@__PURE__*/
  messageDesc(file_google_ai_generativelanguage_v1beta_text_service, 10);

/**
 * API for using Generative Language Models (GLMs) trained to generate text.
 *
 * Also known as Large Language Models (LLM)s, these generate text given an
 * input prompt from the user.
 *
 * @generated from service google.ai.generativelanguage.v1beta.TextService
 */
export const TextService: GenService<{
  /**
   * Generates a response from the model given an input message.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.TextService.GenerateText
   */
  generateText: {
    methodKind: "unary";
    input: typeof GenerateTextRequestSchema;
    output: typeof GenerateTextResponseSchema;
  },
  /**
   * Generates an embedding from the model given an input message.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.TextService.EmbedText
   */
  embedText: {
    methodKind: "unary";
    input: typeof EmbedTextRequestSchema;
    output: typeof EmbedTextResponseSchema;
  },
  /**
   * Generates multiple embeddings from the model given input text in a
   * synchronous call.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.TextService.BatchEmbedText
   */
  batchEmbedText: {
    methodKind: "unary";
    input: typeof BatchEmbedTextRequestSchema;
    output: typeof BatchEmbedTextResponseSchema;
  },
  /**
   * Runs a model's tokenizer on a text and returns the token count.
   *
   * @generated from rpc google.ai.generativelanguage.v1beta.TextService.CountTextTokens
   */
  countTextTokens: {
    methodKind: "unary";
    input: typeof CountTextTokensRequestSchema;
    output: typeof CountTextTokensResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_ai_generativelanguage_v1beta_text_service, 0);

