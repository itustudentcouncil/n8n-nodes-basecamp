// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/dataflow/v1beta3/environment.proto (package google.dataflow.v1beta3, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_field_behavior } from "../../api/field_behavior_pb";
import type { Any } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_any, file_google_protobuf_struct } from "@bufbuild/protobuf/wkt";
import type { JsonObject, Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/dataflow/v1beta3/environment.proto.
 */
export const file_google_dataflow_v1beta3_environment: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_field_behavior, file_google_protobuf_any, file_google_protobuf_struct]);

/**
 * Describes the environment in which a Dataflow Job runs.
 *
 * @generated from message google.dataflow.v1beta3.Environment
 */
export type Environment = Message<"google.dataflow.v1beta3.Environment"> & {
  /**
   * The prefix of the resources the system should use for temporary
   * storage.  The system will append the suffix "/temp-{JOBNAME} to
   * this resource prefix, where {JOBNAME} is the value of the
   * job_name field.  The resulting bucket and object prefix is used
   * as the prefix of the resources used to store temporary data
   * needed during the job execution.  NOTE: This will override the
   * value in taskrunner_settings.
   * The supported resource type is:
   *
   * Google Cloud Storage:
   *
   *   storage.googleapis.com/{bucket}/{object}
   *   bucket.storage.googleapis.com/{object}
   *
   * @generated from field: string temp_storage_prefix = 1;
   */
  tempStoragePrefix: string;

  /**
   * The type of cluster manager API to use.  If unknown or
   * unspecified, the service will attempt to choose a reasonable
   * default.  This should be in the form of the API service name,
   * e.g. "compute.googleapis.com".
   *
   * @generated from field: string cluster_manager_api_service = 2;
   */
  clusterManagerApiService: string;

  /**
   * The list of experiments to enable. This field should be used for SDK
   * related experiments and not for service related experiments. The proper
   * field for service related experiments is service_options.
   *
   * @generated from field: repeated string experiments = 3;
   */
  experiments: string[];

  /**
   * The list of service options to enable. This field should be used for
   * service related experiments only. These experiments, when graduating to GA,
   * should be replaced by dedicated fields or become default (i.e. always on).
   *
   * @generated from field: repeated string service_options = 16;
   */
  serviceOptions: string[];

  /**
   * If set, contains the Cloud KMS key identifier used to encrypt data
   * at rest, AKA a Customer Managed Encryption Key (CMEK).
   *
   * Format:
   *   projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY
   *
   * @generated from field: string service_kms_key_name = 12;
   */
  serviceKmsKeyName: string;

  /**
   * The worker pools. At least one "harness" worker pool must be
   * specified in order for the job to have workers.
   *
   * @generated from field: repeated google.dataflow.v1beta3.WorkerPool worker_pools = 4;
   */
  workerPools: WorkerPool[];

  /**
   * A description of the process that generated the request.
   *
   * @generated from field: google.protobuf.Struct user_agent = 5;
   */
  userAgent?: JsonObject;

  /**
   * A structure describing which components and their versions of the service
   * are required in order to run the job.
   *
   * @generated from field: google.protobuf.Struct version = 6;
   */
  version?: JsonObject;

  /**
   * The dataset for the current project where various workflow
   * related tables are stored.
   *
   * The supported resource type is:
   *
   * Google BigQuery:
   *   bigquery.googleapis.com/{dataset}
   *
   * @generated from field: string dataset = 7;
   */
  dataset: string;

  /**
   * The Cloud Dataflow SDK pipeline options specified by the user. These
   * options are passed through the service and are used to recreate the
   * SDK pipeline options on the worker in a language agnostic and platform
   * independent way.
   *
   * @generated from field: google.protobuf.Struct sdk_pipeline_options = 8;
   */
  sdkPipelineOptions?: JsonObject;

  /**
   * Experimental settings.
   *
   * @generated from field: google.protobuf.Any internal_experiments = 9;
   */
  internalExperiments?: Any;

  /**
   * Identity to run virtual machines as. Defaults to the default account.
   *
   * @generated from field: string service_account_email = 10;
   */
  serviceAccountEmail: string;

  /**
   * Which Flexible Resource Scheduling mode to run in.
   *
   * @generated from field: google.dataflow.v1beta3.FlexResourceSchedulingGoal flex_resource_scheduling_goal = 11;
   */
  flexResourceSchedulingGoal: FlexResourceSchedulingGoal;

  /**
   * The Compute Engine region
   * (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in
   * which worker processing should occur, e.g. "us-west1". Mutually exclusive
   * with worker_zone. If neither worker_region nor worker_zone is specified,
   * default to the control plane's region.
   *
   * @generated from field: string worker_region = 13;
   */
  workerRegion: string;

  /**
   * The Compute Engine zone
   * (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in
   * which worker processing should occur, e.g. "us-west1-a". Mutually exclusive
   * with worker_region. If neither worker_region nor worker_zone is specified,
   * a zone in the control plane's region is chosen based on available capacity.
   *
   * @generated from field: string worker_zone = 14;
   */
  workerZone: string;

  /**
   * Output only. The shuffle mode used for the job.
   *
   * @generated from field: google.dataflow.v1beta3.ShuffleMode shuffle_mode = 15;
   */
  shuffleMode: ShuffleMode;

  /**
   * Any debugging options to be supplied to the job.
   *
   * @generated from field: google.dataflow.v1beta3.DebugOptions debug_options = 17;
   */
  debugOptions?: DebugOptions;
};

/**
 * Describes the message google.dataflow.v1beta3.Environment.
 * Use `create(EnvironmentSchema)` to create a new message.
 */
export const EnvironmentSchema: GenMessage<Environment> = /*@__PURE__*/
  messageDesc(file_google_dataflow_v1beta3_environment, 0);

/**
 * The packages that must be installed in order for a worker to run the
 * steps of the Cloud Dataflow job that will be assigned to its worker
 * pool.
 *
 * This is the mechanism by which the Cloud Dataflow SDK causes code to
 * be loaded onto the workers. For example, the Cloud Dataflow Java SDK
 * might use this to install jars containing the user's code and all of the
 * various dependencies (libraries, data files, etc.) required in order
 * for that code to run.
 *
 * @generated from message google.dataflow.v1beta3.Package
 */
export type Package = Message<"google.dataflow.v1beta3.Package"> & {
  /**
   * The name of the package.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * The resource to read the package from. The supported resource type is:
   *
   * Google Cloud Storage:
   *
   *   storage.googleapis.com/{bucket}
   *   bucket.storage.googleapis.com/
   *
   * @generated from field: string location = 2;
   */
  location: string;
};

/**
 * Describes the message google.dataflow.v1beta3.Package.
 * Use `create(PackageSchema)` to create a new message.
 */
export const PackageSchema: GenMessage<Package> = /*@__PURE__*/
  messageDesc(file_google_dataflow_v1beta3_environment, 1);

/**
 * Describes the data disk used by a workflow job.
 *
 * @generated from message google.dataflow.v1beta3.Disk
 */
export type Disk = Message<"google.dataflow.v1beta3.Disk"> & {
  /**
   * Size of disk in GB.  If zero or unspecified, the service will
   * attempt to choose a reasonable default.
   *
   * @generated from field: int32 size_gb = 1;
   */
  sizeGb: number;

  /**
   * Disk storage type, as defined by Google Compute Engine.  This
   * must be a disk type appropriate to the project and zone in which
   * the workers will run.  If unknown or unspecified, the service
   * will attempt to choose a reasonable default.
   *
   * For example, the standard persistent disk type is a resource name
   * typically ending in "pd-standard".  If SSD persistent disks are
   * available, the resource name typically ends with "pd-ssd".  The
   * actual valid values are defined the Google Compute Engine API,
   * not by the Cloud Dataflow API; consult the Google Compute Engine
   * documentation for more information about determining the set of
   * available disk types for a particular project and zone.
   *
   * Google Compute Engine Disk types are local to a particular
   * project in a particular zone, and so the resource name will
   * typically look something like this:
   *
   * compute.googleapis.com/projects/project-id/zones/zone/diskTypes/pd-standard
   *
   * @generated from field: string disk_type = 2;
   */
  diskType: string;

  /**
   * Directory in a VM where disk is mounted.
   *
   * @generated from field: string mount_point = 3;
   */
  mountPoint: string;
};

/**
 * Describes the message google.dataflow.v1beta3.Disk.
 * Use `create(DiskSchema)` to create a new message.
 */
export const DiskSchema: GenMessage<Disk> = /*@__PURE__*/
  messageDesc(file_google_dataflow_v1beta3_environment, 2);

/**
 * Provides data to pass through to the worker harness.
 *
 * @generated from message google.dataflow.v1beta3.WorkerSettings
 */
export type WorkerSettings = Message<"google.dataflow.v1beta3.WorkerSettings"> & {
  /**
   * The base URL for accessing Google Cloud APIs.
   *
   * When workers access Google Cloud APIs, they logically do so via
   * relative URLs.  If this field is specified, it supplies the base
   * URL to use for resolving these relative URLs.  The normative
   * algorithm used is defined by RFC 1808, "Relative Uniform Resource
   * Locators".
   *
   * If not specified, the default value is "http://www.googleapis.com/"
   *
   * @generated from field: string base_url = 1;
   */
  baseUrl: string;

  /**
   * Whether to send work progress updates to the service.
   *
   * @generated from field: bool reporting_enabled = 2;
   */
  reportingEnabled: boolean;

  /**
   * The Cloud Dataflow service path relative to the root URL, for example,
   * "dataflow/v1b3/projects".
   *
   * @generated from field: string service_path = 3;
   */
  servicePath: string;

  /**
   * The Shuffle service path relative to the root URL, for example,
   * "shuffle/v1beta1".
   *
   * @generated from field: string shuffle_service_path = 4;
   */
  shuffleServicePath: string;

  /**
   * The ID of the worker running this pipeline.
   *
   * @generated from field: string worker_id = 5;
   */
  workerId: string;

  /**
   * The prefix of the resources the system should use for temporary
   * storage.
   *
   * The supported resource type is:
   *
   * Google Cloud Storage:
   *
   *   storage.googleapis.com/{bucket}/{object}
   *   bucket.storage.googleapis.com/{object}
   *
   * @generated from field: string temp_storage_prefix = 6;
   */
  tempStoragePrefix: string;
};

/**
 * Describes the message google.dataflow.v1beta3.WorkerSettings.
 * Use `create(WorkerSettingsSchema)` to create a new message.
 */
export const WorkerSettingsSchema: GenMessage<WorkerSettings> = /*@__PURE__*/
  messageDesc(file_google_dataflow_v1beta3_environment, 3);

/**
 * Taskrunner configuration settings.
 *
 * @generated from message google.dataflow.v1beta3.TaskRunnerSettings
 */
export type TaskRunnerSettings = Message<"google.dataflow.v1beta3.TaskRunnerSettings"> & {
  /**
   * The UNIX user ID on the worker VM to use for tasks launched by
   * taskrunner; e.g. "root".
   *
   * @generated from field: string task_user = 1;
   */
  taskUser: string;

  /**
   * The UNIX group ID on the worker VM to use for tasks launched by
   * taskrunner; e.g. "wheel".
   *
   * @generated from field: string task_group = 2;
   */
  taskGroup: string;

  /**
   * The OAuth2 scopes to be requested by the taskrunner in order to
   * access the Cloud Dataflow API.
   *
   * @generated from field: repeated string oauth_scopes = 3;
   */
  oauthScopes: string[];

  /**
   * The base URL for the taskrunner to use when accessing Google Cloud APIs.
   *
   * When workers access Google Cloud APIs, they logically do so via
   * relative URLs.  If this field is specified, it supplies the base
   * URL to use for resolving these relative URLs.  The normative
   * algorithm used is defined by RFC 1808, "Relative Uniform Resource
   * Locators".
   *
   * If not specified, the default value is "http://www.googleapis.com/"
   *
   * @generated from field: string base_url = 4;
   */
  baseUrl: string;

  /**
   * The API version of endpoint, e.g. "v1b3"
   *
   * @generated from field: string dataflow_api_version = 5;
   */
  dataflowApiVersion: string;

  /**
   * The settings to pass to the parallel worker harness.
   *
   * @generated from field: google.dataflow.v1beta3.WorkerSettings parallel_worker_settings = 6;
   */
  parallelWorkerSettings?: WorkerSettings;

  /**
   * The location on the worker for task-specific subdirectories.
   *
   * @generated from field: string base_task_dir = 7;
   */
  baseTaskDir: string;

  /**
   * Whether to continue taskrunner if an exception is hit.
   *
   * @generated from field: bool continue_on_exception = 8;
   */
  continueOnException: boolean;

  /**
   * Whether to send taskrunner log info to Google Compute Engine VM serial
   * console.
   *
   * @generated from field: bool log_to_serialconsole = 9;
   */
  logToSerialconsole: boolean;

  /**
   * Whether to also send taskrunner log info to stderr.
   *
   * @generated from field: bool alsologtostderr = 10;
   */
  alsologtostderr: boolean;

  /**
   * Indicates where to put logs.  If this is not specified, the logs
   * will not be uploaded.
   *
   * The supported resource type is:
   *
   * Google Cloud Storage:
   *   storage.googleapis.com/{bucket}/{object}
   *   bucket.storage.googleapis.com/{object}
   *
   * @generated from field: string log_upload_location = 11;
   */
  logUploadLocation: string;

  /**
   * The directory on the VM to store logs.
   *
   * @generated from field: string log_dir = 12;
   */
  logDir: string;

  /**
   * The prefix of the resources the taskrunner should use for
   * temporary storage.
   *
   * The supported resource type is:
   *
   * Google Cloud Storage:
   *   storage.googleapis.com/{bucket}/{object}
   *   bucket.storage.googleapis.com/{object}
   *
   * @generated from field: string temp_storage_prefix = 13;
   */
  tempStoragePrefix: string;

  /**
   * The command to launch the worker harness.
   *
   * @generated from field: string harness_command = 14;
   */
  harnessCommand: string;

  /**
   * The file to store the workflow in.
   *
   * @generated from field: string workflow_file_name = 15;
   */
  workflowFileName: string;

  /**
   * The file to store preprocessing commands in.
   *
   * @generated from field: string commandlines_file_name = 16;
   */
  commandlinesFileName: string;

  /**
   * The ID string of the VM.
   *
   * @generated from field: string vm_id = 17;
   */
  vmId: string;

  /**
   * The suggested backend language.
   *
   * @generated from field: string language_hint = 18;
   */
  languageHint: string;

  /**
   * The streaming worker main class name.
   *
   * @generated from field: string streaming_worker_main_class = 19;
   */
  streamingWorkerMainClass: string;
};

/**
 * Describes the message google.dataflow.v1beta3.TaskRunnerSettings.
 * Use `create(TaskRunnerSettingsSchema)` to create a new message.
 */
export const TaskRunnerSettingsSchema: GenMessage<TaskRunnerSettings> = /*@__PURE__*/
  messageDesc(file_google_dataflow_v1beta3_environment, 4);

/**
 * Settings for WorkerPool autoscaling.
 *
 * @generated from message google.dataflow.v1beta3.AutoscalingSettings
 */
export type AutoscalingSettings = Message<"google.dataflow.v1beta3.AutoscalingSettings"> & {
  /**
   * The algorithm to use for autoscaling.
   *
   * @generated from field: google.dataflow.v1beta3.AutoscalingAlgorithm algorithm = 1;
   */
  algorithm: AutoscalingAlgorithm;

  /**
   * The maximum number of workers to cap scaling at.
   *
   * @generated from field: int32 max_num_workers = 2;
   */
  maxNumWorkers: number;
};

/**
 * Describes the message google.dataflow.v1beta3.AutoscalingSettings.
 * Use `create(AutoscalingSettingsSchema)` to create a new message.
 */
export const AutoscalingSettingsSchema: GenMessage<AutoscalingSettings> = /*@__PURE__*/
  messageDesc(file_google_dataflow_v1beta3_environment, 5);

/**
 * Defines a SDK harness container for executing Dataflow pipelines.
 *
 * @generated from message google.dataflow.v1beta3.SdkHarnessContainerImage
 */
export type SdkHarnessContainerImage = Message<"google.dataflow.v1beta3.SdkHarnessContainerImage"> & {
  /**
   * A docker container image that resides in Google Container Registry.
   *
   * @generated from field: string container_image = 1;
   */
  containerImage: string;

  /**
   * If true, recommends the Dataflow service to use only one core per SDK
   * container instance with this image. If false (or unset) recommends using
   * more than one core per SDK container instance with this image for
   * efficiency. Note that Dataflow service may choose to override this property
   * if needed.
   *
   * @generated from field: bool use_single_core_per_container = 2;
   */
  useSingleCorePerContainer: boolean;

  /**
   * Environment ID for the Beam runner API proto Environment that corresponds
   * to the current SDK Harness.
   *
   * @generated from field: string environment_id = 3;
   */
  environmentId: string;

  /**
   * The set of capabilities enumerated in the above Environment proto. See also
   * https://github.com/apache/beam/blob/master/model/pipeline/src/main/proto/beam_runner_api.proto
   *
   * @generated from field: repeated string capabilities = 4;
   */
  capabilities: string[];
};

/**
 * Describes the message google.dataflow.v1beta3.SdkHarnessContainerImage.
 * Use `create(SdkHarnessContainerImageSchema)` to create a new message.
 */
export const SdkHarnessContainerImageSchema: GenMessage<SdkHarnessContainerImage> = /*@__PURE__*/
  messageDesc(file_google_dataflow_v1beta3_environment, 6);

/**
 * Describes one particular pool of Cloud Dataflow workers to be
 * instantiated by the Cloud Dataflow service in order to perform the
 * computations required by a job.  Note that a workflow job may use
 * multiple pools, in order to match the various computational
 * requirements of the various stages of the job.
 *
 * @generated from message google.dataflow.v1beta3.WorkerPool
 */
export type WorkerPool = Message<"google.dataflow.v1beta3.WorkerPool"> & {
  /**
   * The kind of the worker pool; currently only `harness` and `shuffle`
   * are supported.
   *
   * @generated from field: string kind = 1;
   */
  kind: string;

  /**
   * Number of Google Compute Engine workers in this pool needed to
   * execute the job.  If zero or unspecified, the service will
   * attempt to choose a reasonable default.
   *
   * @generated from field: int32 num_workers = 2;
   */
  numWorkers: number;

  /**
   * Packages to be installed on workers.
   *
   * @generated from field: repeated google.dataflow.v1beta3.Package packages = 3;
   */
  packages: Package[];

  /**
   * The default package set to install.  This allows the service to
   * select a default set of packages which are useful to worker
   * harnesses written in a particular language.
   *
   * @generated from field: google.dataflow.v1beta3.DefaultPackageSet default_package_set = 4;
   */
  defaultPackageSet: DefaultPackageSet;

  /**
   * Machine type (e.g. "n1-standard-1").  If empty or unspecified, the
   * service will attempt to choose a reasonable default.
   *
   * @generated from field: string machine_type = 5;
   */
  machineType: string;

  /**
   * Sets the policy for determining when to turndown worker pool.
   * Allowed values are: `TEARDOWN_ALWAYS`, `TEARDOWN_ON_SUCCESS`, and
   * `TEARDOWN_NEVER`.
   * `TEARDOWN_ALWAYS` means workers are always torn down regardless of whether
   * the job succeeds. `TEARDOWN_ON_SUCCESS` means workers are torn down
   * if the job succeeds. `TEARDOWN_NEVER` means the workers are never torn
   * down.
   *
   * If the workers are not torn down by the service, they will
   * continue to run and use Google Compute Engine VM resources in the
   * user's project until they are explicitly terminated by the user.
   * Because of this, Google recommends using the `TEARDOWN_ALWAYS`
   * policy except for small, manually supervised test jobs.
   *
   * If unknown or unspecified, the service will attempt to choose a reasonable
   * default.
   *
   * @generated from field: google.dataflow.v1beta3.TeardownPolicy teardown_policy = 6;
   */
  teardownPolicy: TeardownPolicy;

  /**
   * Size of root disk for VMs, in GB.  If zero or unspecified, the service will
   * attempt to choose a reasonable default.
   *
   * @generated from field: int32 disk_size_gb = 7;
   */
  diskSizeGb: number;

  /**
   * Type of root disk for VMs.  If empty or unspecified, the service will
   * attempt to choose a reasonable default.
   *
   * @generated from field: string disk_type = 16;
   */
  diskType: string;

  /**
   * Fully qualified source image for disks.
   *
   * @generated from field: string disk_source_image = 8;
   */
  diskSourceImage: string;

  /**
   * Zone to run the worker pools in.  If empty or unspecified, the service
   * will attempt to choose a reasonable default.
   *
   * @generated from field: string zone = 9;
   */
  zone: string;

  /**
   * Settings passed through to Google Compute Engine workers when
   * using the standard Dataflow task runner.  Users should ignore
   * this field.
   *
   * @generated from field: google.dataflow.v1beta3.TaskRunnerSettings taskrunner_settings = 10;
   */
  taskrunnerSettings?: TaskRunnerSettings;

  /**
   * The action to take on host maintenance, as defined by the Google
   * Compute Engine API.
   *
   * @generated from field: string on_host_maintenance = 11;
   */
  onHostMaintenance: string;

  /**
   * Data disks that are used by a VM in this workflow.
   *
   * @generated from field: repeated google.dataflow.v1beta3.Disk data_disks = 12;
   */
  dataDisks: Disk[];

  /**
   * Metadata to set on the Google Compute Engine VMs.
   *
   * @generated from field: map<string, string> metadata = 13;
   */
  metadata: { [key: string]: string };

  /**
   * Settings for autoscaling of this WorkerPool.
   *
   * @generated from field: google.dataflow.v1beta3.AutoscalingSettings autoscaling_settings = 14;
   */
  autoscalingSettings?: AutoscalingSettings;

  /**
   * Extra arguments for this worker pool.
   *
   * @generated from field: google.protobuf.Any pool_args = 15;
   */
  poolArgs?: Any;

  /**
   * Network to which VMs will be assigned.  If empty or unspecified,
   * the service will use the network "default".
   *
   * @generated from field: string network = 17;
   */
  network: string;

  /**
   * Subnetwork to which VMs will be assigned, if desired.  Expected to be of
   * the form "regions/REGION/subnetworks/SUBNETWORK".
   *
   * @generated from field: string subnetwork = 19;
   */
  subnetwork: string;

  /**
   * Required. Docker container image that executes the Cloud Dataflow worker
   * harness, residing in Google Container Registry.
   *
   * Deprecated for the Fn API path. Use sdk_harness_container_images instead.
   *
   * @generated from field: string worker_harness_container_image = 18;
   */
  workerHarnessContainerImage: string;

  /**
   * The number of threads per worker harness. If empty or unspecified, the
   * service will choose a number of threads (according to the number of cores
   * on the selected machine type for batch, or 1 by convention for streaming).
   *
   * @generated from field: int32 num_threads_per_worker = 20;
   */
  numThreadsPerWorker: number;

  /**
   * Configuration for VM IPs.
   *
   * @generated from field: google.dataflow.v1beta3.WorkerIPAddressConfiguration ip_configuration = 21;
   */
  ipConfiguration: WorkerIPAddressConfiguration;

  /**
   * Set of SDK harness containers needed to execute this pipeline. This will
   * only be set in the Fn API path. For non-cross-language pipelines this
   * should have only one entry. Cross-language pipelines will have two or more
   * entries.
   *
   * @generated from field: repeated google.dataflow.v1beta3.SdkHarnessContainerImage sdk_harness_container_images = 22;
   */
  sdkHarnessContainerImages: SdkHarnessContainerImage[];
};

/**
 * Describes the message google.dataflow.v1beta3.WorkerPool.
 * Use `create(WorkerPoolSchema)` to create a new message.
 */
export const WorkerPoolSchema: GenMessage<WorkerPool> = /*@__PURE__*/
  messageDesc(file_google_dataflow_v1beta3_environment, 7);

/**
 * Describes any options that have an effect on the debugging of pipelines.
 *
 * @generated from message google.dataflow.v1beta3.DebugOptions
 */
export type DebugOptions = Message<"google.dataflow.v1beta3.DebugOptions"> & {
  /**
   * When true, enables the logging of the literal hot key to the user's Cloud
   * Logging.
   *
   * @generated from field: bool enable_hot_key_logging = 1;
   */
  enableHotKeyLogging: boolean;
};

/**
 * Describes the message google.dataflow.v1beta3.DebugOptions.
 * Use `create(DebugOptionsSchema)` to create a new message.
 */
export const DebugOptionsSchema: GenMessage<DebugOptions> = /*@__PURE__*/
  messageDesc(file_google_dataflow_v1beta3_environment, 8);

/**
 * Specifies the processing model used by a
 * [google.dataflow.v1beta3.Job], which determines the way the Job is
 * managed by the Cloud Dataflow service (how workers are scheduled, how
 * inputs are sharded, etc).
 *
 * @generated from enum google.dataflow.v1beta3.JobType
 */
export enum JobType {
  /**
   * The type of the job is unspecified, or unknown.
   *
   * @generated from enum value: JOB_TYPE_UNKNOWN = 0;
   */
  UNKNOWN = 0,

  /**
   * A batch job with a well-defined end point: data is read, data is
   * processed, data is written, and the job is done.
   *
   * @generated from enum value: JOB_TYPE_BATCH = 1;
   */
  BATCH = 1,

  /**
   * A continuously streaming job with no end: data is read,
   * processed, and written continuously.
   *
   * @generated from enum value: JOB_TYPE_STREAMING = 2;
   */
  STREAMING = 2,
}

/**
 * Describes the enum google.dataflow.v1beta3.JobType.
 */
export const JobTypeSchema: GenEnum<JobType> = /*@__PURE__*/
  enumDesc(file_google_dataflow_v1beta3_environment, 0);

/**
 * Specifies the resource to optimize for in Flexible Resource Scheduling.
 *
 * @generated from enum google.dataflow.v1beta3.FlexResourceSchedulingGoal
 */
export enum FlexResourceSchedulingGoal {
  /**
   * Run in the default mode.
   *
   * @generated from enum value: FLEXRS_UNSPECIFIED = 0;
   */
  FLEXRS_UNSPECIFIED = 0,

  /**
   * Optimize for lower execution time.
   *
   * @generated from enum value: FLEXRS_SPEED_OPTIMIZED = 1;
   */
  FLEXRS_SPEED_OPTIMIZED = 1,

  /**
   * Optimize for lower cost.
   *
   * @generated from enum value: FLEXRS_COST_OPTIMIZED = 2;
   */
  FLEXRS_COST_OPTIMIZED = 2,
}

/**
 * Describes the enum google.dataflow.v1beta3.FlexResourceSchedulingGoal.
 */
export const FlexResourceSchedulingGoalSchema: GenEnum<FlexResourceSchedulingGoal> = /*@__PURE__*/
  enumDesc(file_google_dataflow_v1beta3_environment, 1);

/**
 * Specifies what happens to a resource when a Cloud Dataflow
 * [google.dataflow.v1beta3.Job][google.dataflow.v1beta3.Job] has completed.
 *
 * @generated from enum google.dataflow.v1beta3.TeardownPolicy
 */
export enum TeardownPolicy {
  /**
   * The teardown policy isn't specified, or is unknown.
   *
   * @generated from enum value: TEARDOWN_POLICY_UNKNOWN = 0;
   */
  TEARDOWN_POLICY_UNKNOWN = 0,

  /**
   * Always teardown the resource.
   *
   * @generated from enum value: TEARDOWN_ALWAYS = 1;
   */
  TEARDOWN_ALWAYS = 1,

  /**
   * Teardown the resource on success. This is useful for debugging
   * failures.
   *
   * @generated from enum value: TEARDOWN_ON_SUCCESS = 2;
   */
  TEARDOWN_ON_SUCCESS = 2,

  /**
   * Never teardown the resource. This is useful for debugging and
   * development.
   *
   * @generated from enum value: TEARDOWN_NEVER = 3;
   */
  TEARDOWN_NEVER = 3,
}

/**
 * Describes the enum google.dataflow.v1beta3.TeardownPolicy.
 */
export const TeardownPolicySchema: GenEnum<TeardownPolicy> = /*@__PURE__*/
  enumDesc(file_google_dataflow_v1beta3_environment, 2);

/**
 * The default set of packages to be staged on a pool of workers.
 *
 * @generated from enum google.dataflow.v1beta3.DefaultPackageSet
 */
export enum DefaultPackageSet {
  /**
   * The default set of packages to stage is unknown, or unspecified.
   *
   * @generated from enum value: DEFAULT_PACKAGE_SET_UNKNOWN = 0;
   */
  UNKNOWN = 0,

  /**
   * Indicates that no packages should be staged at the worker unless
   * explicitly specified by the job.
   *
   * @generated from enum value: DEFAULT_PACKAGE_SET_NONE = 1;
   */
  NONE = 1,

  /**
   * Stage packages typically useful to workers written in Java.
   *
   * @generated from enum value: DEFAULT_PACKAGE_SET_JAVA = 2;
   */
  JAVA = 2,

  /**
   * Stage packages typically useful to workers written in Python.
   *
   * @generated from enum value: DEFAULT_PACKAGE_SET_PYTHON = 3;
   */
  PYTHON = 3,
}

/**
 * Describes the enum google.dataflow.v1beta3.DefaultPackageSet.
 */
export const DefaultPackageSetSchema: GenEnum<DefaultPackageSet> = /*@__PURE__*/
  enumDesc(file_google_dataflow_v1beta3_environment, 3);

/**
 * Specifies the algorithm used to determine the number of worker
 * processes to run at any given point in time, based on the amount of
 * data left to process, the number of workers, and how quickly
 * existing workers are processing data.
 *
 * @generated from enum google.dataflow.v1beta3.AutoscalingAlgorithm
 */
export enum AutoscalingAlgorithm {
  /**
   * The algorithm is unknown, or unspecified.
   *
   * @generated from enum value: AUTOSCALING_ALGORITHM_UNKNOWN = 0;
   */
  UNKNOWN = 0,

  /**
   * Disable autoscaling.
   *
   * @generated from enum value: AUTOSCALING_ALGORITHM_NONE = 1;
   */
  NONE = 1,

  /**
   * Increase worker count over time to reduce job execution time.
   *
   * @generated from enum value: AUTOSCALING_ALGORITHM_BASIC = 2;
   */
  BASIC = 2,
}

/**
 * Describes the enum google.dataflow.v1beta3.AutoscalingAlgorithm.
 */
export const AutoscalingAlgorithmSchema: GenEnum<AutoscalingAlgorithm> = /*@__PURE__*/
  enumDesc(file_google_dataflow_v1beta3_environment, 4);

/**
 * Specifies how IP addresses should be allocated to the worker machines.
 *
 * @generated from enum google.dataflow.v1beta3.WorkerIPAddressConfiguration
 */
export enum WorkerIPAddressConfiguration {
  /**
   * The configuration is unknown, or unspecified.
   *
   * @generated from enum value: WORKER_IP_UNSPECIFIED = 0;
   */
  WORKER_IP_UNSPECIFIED = 0,

  /**
   * Workers should have public IP addresses.
   *
   * @generated from enum value: WORKER_IP_PUBLIC = 1;
   */
  WORKER_IP_PUBLIC = 1,

  /**
   * Workers should have private IP addresses.
   *
   * @generated from enum value: WORKER_IP_PRIVATE = 2;
   */
  WORKER_IP_PRIVATE = 2,
}

/**
 * Describes the enum google.dataflow.v1beta3.WorkerIPAddressConfiguration.
 */
export const WorkerIPAddressConfigurationSchema: GenEnum<WorkerIPAddressConfiguration> = /*@__PURE__*/
  enumDesc(file_google_dataflow_v1beta3_environment, 5);

/**
 * Specifies the shuffle mode used by a
 * [google.dataflow.v1beta3.Job], which determines the approach data is shuffled
 * during processing. More details in:
 * https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#dataflow-shuffle
 *
 * @generated from enum google.dataflow.v1beta3.ShuffleMode
 */
export enum ShuffleMode {
  /**
   * Shuffle mode information is not available.
   *
   * @generated from enum value: SHUFFLE_MODE_UNSPECIFIED = 0;
   */
  SHUFFLE_MODE_UNSPECIFIED = 0,

  /**
   * Shuffle is done on the worker VMs.
   *
   * @generated from enum value: VM_BASED = 1;
   */
  VM_BASED = 1,

  /**
   * Shuffle is done on the service side.
   *
   * @generated from enum value: SERVICE_BASED = 2;
   */
  SERVICE_BASED = 2,
}

/**
 * Describes the enum google.dataflow.v1beta3.ShuffleMode.
 */
export const ShuffleModeSchema: GenEnum<ShuffleMode> = /*@__PURE__*/
  enumDesc(file_google_dataflow_v1beta3_environment, 6);

