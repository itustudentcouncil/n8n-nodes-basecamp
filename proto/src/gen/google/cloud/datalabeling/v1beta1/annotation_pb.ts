// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/datalabeling/v1beta1/annotation.proto (package google.cloud.datalabeling.v1beta1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc } from "@bufbuild/protobuf/codegenv1";
import type { AnnotationSpec } from "./annotation_spec_set_pb";
import { file_google_cloud_datalabeling_v1beta1_annotation_spec_set } from "./annotation_spec_set_pb";
import type { Duration } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_duration } from "@bufbuild/protobuf/wkt";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/datalabeling/v1beta1/annotation.proto.
 */
export const file_google_cloud_datalabeling_v1beta1_annotation: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_cloud_datalabeling_v1beta1_annotation_spec_set, file_google_protobuf_duration]);

/**
 * Annotation for Example. Each example may have one or more annotations. For
 * example in image classification problem, each image might have one or more
 * labels. We call labels binded with this image an Annotation.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.Annotation
 */
export type Annotation = Message<"google.cloud.datalabeling.v1beta1.Annotation"> & {
  /**
   * Output only. Unique name of this annotation, format is:
   *
   * projects/{project_id}/datasets/{dataset_id}/annotatedDatasets/{annotated_dataset}/examples/{example_id}/annotations/{annotation_id}
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Output only. The source of the annotation.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.AnnotationSource annotation_source = 2;
   */
  annotationSource: AnnotationSource;

  /**
   * Output only. This is the actual annotation value, e.g classification,
   * bounding box values are stored here.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.AnnotationValue annotation_value = 3;
   */
  annotationValue?: AnnotationValue;

  /**
   * Output only. Annotation metadata, including information like votes
   * for labels.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.AnnotationMetadata annotation_metadata = 4;
   */
  annotationMetadata?: AnnotationMetadata;

  /**
   * Output only. Sentiment for this annotation.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.AnnotationSentiment annotation_sentiment = 6;
   */
  annotationSentiment: AnnotationSentiment;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.Annotation.
 * Use `create(AnnotationSchema)` to create a new message.
 */
export const AnnotationSchema: GenMessage<Annotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 0);

/**
 * Annotation value for an example.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.AnnotationValue
 */
export type AnnotationValue = Message<"google.cloud.datalabeling.v1beta1.AnnotationValue"> & {
  /**
   * @generated from oneof google.cloud.datalabeling.v1beta1.AnnotationValue.value_type
   */
  valueType: {
    /**
     * Annotation value for image classification case.
     *
     * @generated from field: google.cloud.datalabeling.v1beta1.ImageClassificationAnnotation image_classification_annotation = 1;
     */
    value: ImageClassificationAnnotation;
    case: "imageClassificationAnnotation";
  } | {
    /**
     * Annotation value for image bounding box, oriented bounding box
     * and polygon cases.
     *
     * @generated from field: google.cloud.datalabeling.v1beta1.ImageBoundingPolyAnnotation image_bounding_poly_annotation = 2;
     */
    value: ImageBoundingPolyAnnotation;
    case: "imageBoundingPolyAnnotation";
  } | {
    /**
     * Annotation value for image polyline cases.
     * Polyline here is different from BoundingPoly. It is formed by
     * line segments connected to each other but not closed form(Bounding Poly).
     * The line segments can cross each other.
     *
     * @generated from field: google.cloud.datalabeling.v1beta1.ImagePolylineAnnotation image_polyline_annotation = 8;
     */
    value: ImagePolylineAnnotation;
    case: "imagePolylineAnnotation";
  } | {
    /**
     * Annotation value for image segmentation.
     *
     * @generated from field: google.cloud.datalabeling.v1beta1.ImageSegmentationAnnotation image_segmentation_annotation = 9;
     */
    value: ImageSegmentationAnnotation;
    case: "imageSegmentationAnnotation";
  } | {
    /**
     * Annotation value for text classification case.
     *
     * @generated from field: google.cloud.datalabeling.v1beta1.TextClassificationAnnotation text_classification_annotation = 3;
     */
    value: TextClassificationAnnotation;
    case: "textClassificationAnnotation";
  } | {
    /**
     * Annotation value for text entity extraction case.
     *
     * @generated from field: google.cloud.datalabeling.v1beta1.TextEntityExtractionAnnotation text_entity_extraction_annotation = 10;
     */
    value: TextEntityExtractionAnnotation;
    case: "textEntityExtractionAnnotation";
  } | {
    /**
     * Annotation value for video classification case.
     *
     * @generated from field: google.cloud.datalabeling.v1beta1.VideoClassificationAnnotation video_classification_annotation = 4;
     */
    value: VideoClassificationAnnotation;
    case: "videoClassificationAnnotation";
  } | {
    /**
     * Annotation value for video object detection and tracking case.
     *
     * @generated from field: google.cloud.datalabeling.v1beta1.VideoObjectTrackingAnnotation video_object_tracking_annotation = 5;
     */
    value: VideoObjectTrackingAnnotation;
    case: "videoObjectTrackingAnnotation";
  } | {
    /**
     * Annotation value for video event case.
     *
     * @generated from field: google.cloud.datalabeling.v1beta1.VideoEventAnnotation video_event_annotation = 6;
     */
    value: VideoEventAnnotation;
    case: "videoEventAnnotation";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.AnnotationValue.
 * Use `create(AnnotationValueSchema)` to create a new message.
 */
export const AnnotationValueSchema: GenMessage<AnnotationValue> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 1);

/**
 * Image classification annotation definition.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.ImageClassificationAnnotation
 */
export type ImageClassificationAnnotation = Message<"google.cloud.datalabeling.v1beta1.ImageClassificationAnnotation"> & {
  /**
   * Label of image.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.AnnotationSpec annotation_spec = 1;
   */
  annotationSpec?: AnnotationSpec;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.ImageClassificationAnnotation.
 * Use `create(ImageClassificationAnnotationSchema)` to create a new message.
 */
export const ImageClassificationAnnotationSchema: GenMessage<ImageClassificationAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 2);

/**
 * A vertex represents a 2D point in the image.
 * NOTE: the vertex coordinates are in the same scale as the original image.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.Vertex
 */
export type Vertex = Message<"google.cloud.datalabeling.v1beta1.Vertex"> & {
  /**
   * X coordinate.
   *
   * @generated from field: int32 x = 1;
   */
  x: number;

  /**
   * Y coordinate.
   *
   * @generated from field: int32 y = 2;
   */
  y: number;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.Vertex.
 * Use `create(VertexSchema)` to create a new message.
 */
export const VertexSchema: GenMessage<Vertex> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 3);

/**
 * A vertex represents a 2D point in the image.
 * NOTE: the normalized vertex coordinates are relative to the original image
 * and range from 0 to 1.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.NormalizedVertex
 */
export type NormalizedVertex = Message<"google.cloud.datalabeling.v1beta1.NormalizedVertex"> & {
  /**
   * X coordinate.
   *
   * @generated from field: float x = 1;
   */
  x: number;

  /**
   * Y coordinate.
   *
   * @generated from field: float y = 2;
   */
  y: number;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.NormalizedVertex.
 * Use `create(NormalizedVertexSchema)` to create a new message.
 */
export const NormalizedVertexSchema: GenMessage<NormalizedVertex> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 4);

/**
 * A bounding polygon in the image.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.BoundingPoly
 */
export type BoundingPoly = Message<"google.cloud.datalabeling.v1beta1.BoundingPoly"> & {
  /**
   * The bounding polygon vertices.
   *
   * @generated from field: repeated google.cloud.datalabeling.v1beta1.Vertex vertices = 1;
   */
  vertices: Vertex[];
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.BoundingPoly.
 * Use `create(BoundingPolySchema)` to create a new message.
 */
export const BoundingPolySchema: GenMessage<BoundingPoly> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 5);

/**
 * Normalized bounding polygon.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.NormalizedBoundingPoly
 */
export type NormalizedBoundingPoly = Message<"google.cloud.datalabeling.v1beta1.NormalizedBoundingPoly"> & {
  /**
   * The bounding polygon normalized vertices.
   *
   * @generated from field: repeated google.cloud.datalabeling.v1beta1.NormalizedVertex normalized_vertices = 1;
   */
  normalizedVertices: NormalizedVertex[];
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.NormalizedBoundingPoly.
 * Use `create(NormalizedBoundingPolySchema)` to create a new message.
 */
export const NormalizedBoundingPolySchema: GenMessage<NormalizedBoundingPoly> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 6);

/**
 * Image bounding poly annotation. It represents a polygon including
 * bounding box in the image.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.ImageBoundingPolyAnnotation
 */
export type ImageBoundingPolyAnnotation = Message<"google.cloud.datalabeling.v1beta1.ImageBoundingPolyAnnotation"> & {
  /**
   * The region of the polygon. If it is a bounding box, it is guaranteed to be
   * four points.
   *
   * @generated from oneof google.cloud.datalabeling.v1beta1.ImageBoundingPolyAnnotation.bounded_area
   */
  boundedArea: {
    /**
     * @generated from field: google.cloud.datalabeling.v1beta1.BoundingPoly bounding_poly = 2;
     */
    value: BoundingPoly;
    case: "boundingPoly";
  } | {
    /**
     * @generated from field: google.cloud.datalabeling.v1beta1.NormalizedBoundingPoly normalized_bounding_poly = 3;
     */
    value: NormalizedBoundingPoly;
    case: "normalizedBoundingPoly";
  } | { case: undefined; value?: undefined };

  /**
   * Label of object in this bounding polygon.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.AnnotationSpec annotation_spec = 1;
   */
  annotationSpec?: AnnotationSpec;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.ImageBoundingPolyAnnotation.
 * Use `create(ImageBoundingPolyAnnotationSchema)` to create a new message.
 */
export const ImageBoundingPolyAnnotationSchema: GenMessage<ImageBoundingPolyAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 7);

/**
 * A line with multiple line segments.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.Polyline
 */
export type Polyline = Message<"google.cloud.datalabeling.v1beta1.Polyline"> & {
  /**
   * The polyline vertices.
   *
   * @generated from field: repeated google.cloud.datalabeling.v1beta1.Vertex vertices = 1;
   */
  vertices: Vertex[];
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.Polyline.
 * Use `create(PolylineSchema)` to create a new message.
 */
export const PolylineSchema: GenMessage<Polyline> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 8);

/**
 * Normalized polyline.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.NormalizedPolyline
 */
export type NormalizedPolyline = Message<"google.cloud.datalabeling.v1beta1.NormalizedPolyline"> & {
  /**
   * The normalized polyline vertices.
   *
   * @generated from field: repeated google.cloud.datalabeling.v1beta1.NormalizedVertex normalized_vertices = 1;
   */
  normalizedVertices: NormalizedVertex[];
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.NormalizedPolyline.
 * Use `create(NormalizedPolylineSchema)` to create a new message.
 */
export const NormalizedPolylineSchema: GenMessage<NormalizedPolyline> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 9);

/**
 * A polyline for the image annotation.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.ImagePolylineAnnotation
 */
export type ImagePolylineAnnotation = Message<"google.cloud.datalabeling.v1beta1.ImagePolylineAnnotation"> & {
  /**
   * @generated from oneof google.cloud.datalabeling.v1beta1.ImagePolylineAnnotation.poly
   */
  poly: {
    /**
     * @generated from field: google.cloud.datalabeling.v1beta1.Polyline polyline = 2;
     */
    value: Polyline;
    case: "polyline";
  } | {
    /**
     * @generated from field: google.cloud.datalabeling.v1beta1.NormalizedPolyline normalized_polyline = 3;
     */
    value: NormalizedPolyline;
    case: "normalizedPolyline";
  } | { case: undefined; value?: undefined };

  /**
   * Label of this polyline.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.AnnotationSpec annotation_spec = 1;
   */
  annotationSpec?: AnnotationSpec;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.ImagePolylineAnnotation.
 * Use `create(ImagePolylineAnnotationSchema)` to create a new message.
 */
export const ImagePolylineAnnotationSchema: GenMessage<ImagePolylineAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 10);

/**
 * Image segmentation annotation.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.ImageSegmentationAnnotation
 */
export type ImageSegmentationAnnotation = Message<"google.cloud.datalabeling.v1beta1.ImageSegmentationAnnotation"> & {
  /**
   * The mapping between rgb color and annotation spec. The key is the rgb
   * color represented in format of rgb(0, 0, 0). The value is the
   * AnnotationSpec.
   *
   * @generated from field: map<string, google.cloud.datalabeling.v1beta1.AnnotationSpec> annotation_colors = 1;
   */
  annotationColors: { [key: string]: AnnotationSpec };

  /**
   * Image format.
   *
   * @generated from field: string mime_type = 2;
   */
  mimeType: string;

  /**
   * A byte string of a full image's color map.
   *
   * @generated from field: bytes image_bytes = 3;
   */
  imageBytes: Uint8Array;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.ImageSegmentationAnnotation.
 * Use `create(ImageSegmentationAnnotationSchema)` to create a new message.
 */
export const ImageSegmentationAnnotationSchema: GenMessage<ImageSegmentationAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 11);

/**
 * Text classification annotation.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.TextClassificationAnnotation
 */
export type TextClassificationAnnotation = Message<"google.cloud.datalabeling.v1beta1.TextClassificationAnnotation"> & {
  /**
   * Label of the text.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.AnnotationSpec annotation_spec = 1;
   */
  annotationSpec?: AnnotationSpec;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.TextClassificationAnnotation.
 * Use `create(TextClassificationAnnotationSchema)` to create a new message.
 */
export const TextClassificationAnnotationSchema: GenMessage<TextClassificationAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 12);

/**
 * Text entity extraction annotation.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.TextEntityExtractionAnnotation
 */
export type TextEntityExtractionAnnotation = Message<"google.cloud.datalabeling.v1beta1.TextEntityExtractionAnnotation"> & {
  /**
   * Label of the text entities.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.AnnotationSpec annotation_spec = 1;
   */
  annotationSpec?: AnnotationSpec;

  /**
   * Position of the entity.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.SequentialSegment sequential_segment = 2;
   */
  sequentialSegment?: SequentialSegment;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.TextEntityExtractionAnnotation.
 * Use `create(TextEntityExtractionAnnotationSchema)` to create a new message.
 */
export const TextEntityExtractionAnnotationSchema: GenMessage<TextEntityExtractionAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 13);

/**
 * Start and end position in a sequence (e.g. text segment).
 *
 * @generated from message google.cloud.datalabeling.v1beta1.SequentialSegment
 */
export type SequentialSegment = Message<"google.cloud.datalabeling.v1beta1.SequentialSegment"> & {
  /**
   * Start position (inclusive).
   *
   * @generated from field: int32 start = 1;
   */
  start: number;

  /**
   * End position (exclusive).
   *
   * @generated from field: int32 end = 2;
   */
  end: number;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.SequentialSegment.
 * Use `create(SequentialSegmentSchema)` to create a new message.
 */
export const SequentialSegmentSchema: GenMessage<SequentialSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 14);

/**
 * A time period inside of an example that has a time dimension (e.g. video).
 *
 * @generated from message google.cloud.datalabeling.v1beta1.TimeSegment
 */
export type TimeSegment = Message<"google.cloud.datalabeling.v1beta1.TimeSegment"> & {
  /**
   * Start of the time segment (inclusive), represented as the duration since
   * the example start.
   *
   * @generated from field: google.protobuf.Duration start_time_offset = 1;
   */
  startTimeOffset?: Duration;

  /**
   * End of the time segment (exclusive), represented as the duration since the
   * example start.
   *
   * @generated from field: google.protobuf.Duration end_time_offset = 2;
   */
  endTimeOffset?: Duration;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.TimeSegment.
 * Use `create(TimeSegmentSchema)` to create a new message.
 */
export const TimeSegmentSchema: GenMessage<TimeSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 15);

/**
 * Video classification annotation.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.VideoClassificationAnnotation
 */
export type VideoClassificationAnnotation = Message<"google.cloud.datalabeling.v1beta1.VideoClassificationAnnotation"> & {
  /**
   * The time segment of the video to which the annotation applies.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.TimeSegment time_segment = 1;
   */
  timeSegment?: TimeSegment;

  /**
   * Label of the segment specified by time_segment.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.AnnotationSpec annotation_spec = 2;
   */
  annotationSpec?: AnnotationSpec;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.VideoClassificationAnnotation.
 * Use `create(VideoClassificationAnnotationSchema)` to create a new message.
 */
export const VideoClassificationAnnotationSchema: GenMessage<VideoClassificationAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 16);

/**
 * Video frame level annotation for object detection and tracking.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.ObjectTrackingFrame
 */
export type ObjectTrackingFrame = Message<"google.cloud.datalabeling.v1beta1.ObjectTrackingFrame"> & {
  /**
   * The bounding box location of this object track for the frame.
   *
   * @generated from oneof google.cloud.datalabeling.v1beta1.ObjectTrackingFrame.bounded_area
   */
  boundedArea: {
    /**
     * @generated from field: google.cloud.datalabeling.v1beta1.BoundingPoly bounding_poly = 1;
     */
    value: BoundingPoly;
    case: "boundingPoly";
  } | {
    /**
     * @generated from field: google.cloud.datalabeling.v1beta1.NormalizedBoundingPoly normalized_bounding_poly = 2;
     */
    value: NormalizedBoundingPoly;
    case: "normalizedBoundingPoly";
  } | { case: undefined; value?: undefined };

  /**
   * The time offset of this frame relative to the beginning of the video.
   *
   * @generated from field: google.protobuf.Duration time_offset = 3;
   */
  timeOffset?: Duration;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.ObjectTrackingFrame.
 * Use `create(ObjectTrackingFrameSchema)` to create a new message.
 */
export const ObjectTrackingFrameSchema: GenMessage<ObjectTrackingFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 17);

/**
 * Video object tracking annotation.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.VideoObjectTrackingAnnotation
 */
export type VideoObjectTrackingAnnotation = Message<"google.cloud.datalabeling.v1beta1.VideoObjectTrackingAnnotation"> & {
  /**
   * Label of the object tracked in this annotation.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.AnnotationSpec annotation_spec = 1;
   */
  annotationSpec?: AnnotationSpec;

  /**
   * The time segment of the video to which object tracking applies.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.TimeSegment time_segment = 2;
   */
  timeSegment?: TimeSegment;

  /**
   * The list of frames where this object track appears.
   *
   * @generated from field: repeated google.cloud.datalabeling.v1beta1.ObjectTrackingFrame object_tracking_frames = 3;
   */
  objectTrackingFrames: ObjectTrackingFrame[];
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.VideoObjectTrackingAnnotation.
 * Use `create(VideoObjectTrackingAnnotationSchema)` to create a new message.
 */
export const VideoObjectTrackingAnnotationSchema: GenMessage<VideoObjectTrackingAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 18);

/**
 * Video event annotation.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.VideoEventAnnotation
 */
export type VideoEventAnnotation = Message<"google.cloud.datalabeling.v1beta1.VideoEventAnnotation"> & {
  /**
   * Label of the event in this annotation.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.AnnotationSpec annotation_spec = 1;
   */
  annotationSpec?: AnnotationSpec;

  /**
   * The time segment of the video to which the annotation applies.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.TimeSegment time_segment = 2;
   */
  timeSegment?: TimeSegment;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.VideoEventAnnotation.
 * Use `create(VideoEventAnnotationSchema)` to create a new message.
 */
export const VideoEventAnnotationSchema: GenMessage<VideoEventAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 19);

/**
 * Additional information associated with the annotation.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.AnnotationMetadata
 */
export type AnnotationMetadata = Message<"google.cloud.datalabeling.v1beta1.AnnotationMetadata"> & {
  /**
   * Metadata related to human labeling.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.OperatorMetadata operator_metadata = 2;
   */
  operatorMetadata?: OperatorMetadata;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.AnnotationMetadata.
 * Use `create(AnnotationMetadataSchema)` to create a new message.
 */
export const AnnotationMetadataSchema: GenMessage<AnnotationMetadata> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 20);

/**
 * General information useful for labels coming from contributors.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.OperatorMetadata
 */
export type OperatorMetadata = Message<"google.cloud.datalabeling.v1beta1.OperatorMetadata"> & {
  /**
   * Confidence score corresponding to a label. For examle, if 3 contributors
   * have answered the question and 2 of them agree on the final label, the
   * confidence score will be 0.67 (2/3).
   *
   * @generated from field: float score = 1;
   */
  score: number;

  /**
   * The total number of contributors that answer this question.
   *
   * @generated from field: int32 total_votes = 2;
   */
  totalVotes: number;

  /**
   * The total number of contributors that choose this label.
   *
   * @generated from field: int32 label_votes = 3;
   */
  labelVotes: number;

  /**
   * Comments from contributors.
   *
   * @generated from field: repeated string comments = 4;
   */
  comments: string[];
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.OperatorMetadata.
 * Use `create(OperatorMetadataSchema)` to create a new message.
 */
export const OperatorMetadataSchema: GenMessage<OperatorMetadata> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_annotation, 21);

/**
 * Specifies where the annotation comes from (whether it was provided by a
 * human labeler or a different source).
 *
 * @generated from enum google.cloud.datalabeling.v1beta1.AnnotationSource
 */
export enum AnnotationSource {
  /**
   * @generated from enum value: ANNOTATION_SOURCE_UNSPECIFIED = 0;
   */
  ANNOTATION_SOURCE_UNSPECIFIED = 0,

  /**
   * Answer is provided by a human contributor.
   *
   * @generated from enum value: OPERATOR = 3;
   */
  OPERATOR = 3,
}

/**
 * Describes the enum google.cloud.datalabeling.v1beta1.AnnotationSource.
 */
export const AnnotationSourceSchema: GenEnum<AnnotationSource> = /*@__PURE__*/
  enumDesc(file_google_cloud_datalabeling_v1beta1_annotation, 0);

/**
 * @generated from enum google.cloud.datalabeling.v1beta1.AnnotationSentiment
 */
export enum AnnotationSentiment {
  /**
   * @generated from enum value: ANNOTATION_SENTIMENT_UNSPECIFIED = 0;
   */
  ANNOTATION_SENTIMENT_UNSPECIFIED = 0,

  /**
   * This annotation describes negatively about the data.
   *
   * @generated from enum value: NEGATIVE = 1;
   */
  NEGATIVE = 1,

  /**
   * This label describes positively about the data.
   *
   * @generated from enum value: POSITIVE = 2;
   */
  POSITIVE = 2,
}

/**
 * Describes the enum google.cloud.datalabeling.v1beta1.AnnotationSentiment.
 */
export const AnnotationSentimentSchema: GenEnum<AnnotationSentiment> = /*@__PURE__*/
  enumDesc(file_google_cloud_datalabeling_v1beta1_annotation, 1);

/**
 * @generated from enum google.cloud.datalabeling.v1beta1.AnnotationType
 */
export enum AnnotationType {
  /**
   * @generated from enum value: ANNOTATION_TYPE_UNSPECIFIED = 0;
   */
  ANNOTATION_TYPE_UNSPECIFIED = 0,

  /**
   * Classification annotations in an image. Allowed for continuous evaluation.
   *
   * @generated from enum value: IMAGE_CLASSIFICATION_ANNOTATION = 1;
   */
  IMAGE_CLASSIFICATION_ANNOTATION = 1,

  /**
   * Bounding box annotations in an image. A form of image object detection.
   * Allowed for continuous evaluation.
   *
   * @generated from enum value: IMAGE_BOUNDING_BOX_ANNOTATION = 2;
   */
  IMAGE_BOUNDING_BOX_ANNOTATION = 2,

  /**
   * Oriented bounding box. The box does not have to be parallel to horizontal
   * line.
   *
   * @generated from enum value: IMAGE_ORIENTED_BOUNDING_BOX_ANNOTATION = 13;
   */
  IMAGE_ORIENTED_BOUNDING_BOX_ANNOTATION = 13,

  /**
   * Bounding poly annotations in an image.
   *
   * @generated from enum value: IMAGE_BOUNDING_POLY_ANNOTATION = 10;
   */
  IMAGE_BOUNDING_POLY_ANNOTATION = 10,

  /**
   * Polyline annotations in an image.
   *
   * @generated from enum value: IMAGE_POLYLINE_ANNOTATION = 11;
   */
  IMAGE_POLYLINE_ANNOTATION = 11,

  /**
   * Segmentation annotations in an image.
   *
   * @generated from enum value: IMAGE_SEGMENTATION_ANNOTATION = 12;
   */
  IMAGE_SEGMENTATION_ANNOTATION = 12,

  /**
   * Classification annotations in video shots.
   *
   * @generated from enum value: VIDEO_SHOTS_CLASSIFICATION_ANNOTATION = 3;
   */
  VIDEO_SHOTS_CLASSIFICATION_ANNOTATION = 3,

  /**
   * Video object tracking annotation.
   *
   * @generated from enum value: VIDEO_OBJECT_TRACKING_ANNOTATION = 4;
   */
  VIDEO_OBJECT_TRACKING_ANNOTATION = 4,

  /**
   * Video object detection annotation.
   *
   * @generated from enum value: VIDEO_OBJECT_DETECTION_ANNOTATION = 5;
   */
  VIDEO_OBJECT_DETECTION_ANNOTATION = 5,

  /**
   * Video event annotation.
   *
   * @generated from enum value: VIDEO_EVENT_ANNOTATION = 6;
   */
  VIDEO_EVENT_ANNOTATION = 6,

  /**
   * Classification for text. Allowed for continuous evaluation.
   *
   * @generated from enum value: TEXT_CLASSIFICATION_ANNOTATION = 8;
   */
  TEXT_CLASSIFICATION_ANNOTATION = 8,

  /**
   * Entity extraction for text.
   *
   * @generated from enum value: TEXT_ENTITY_EXTRACTION_ANNOTATION = 9;
   */
  TEXT_ENTITY_EXTRACTION_ANNOTATION = 9,

  /**
   * General classification. Allowed for continuous evaluation.
   *
   * @generated from enum value: GENERAL_CLASSIFICATION_ANNOTATION = 14;
   */
  GENERAL_CLASSIFICATION_ANNOTATION = 14,
}

/**
 * Describes the enum google.cloud.datalabeling.v1beta1.AnnotationType.
 */
export const AnnotationTypeSchema: GenEnum<AnnotationType> = /*@__PURE__*/
  enumDesc(file_google_cloud_datalabeling_v1beta1_annotation, 2);

