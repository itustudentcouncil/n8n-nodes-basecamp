// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/datalabeling/v1beta1/evaluation_job.proto (package google.cloud.datalabeling.v1beta1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_resource } from "../../../api/resource_pb";
import type { InputConfig } from "./dataset_pb";
import { file_google_cloud_datalabeling_v1beta1_dataset } from "./dataset_pb";
import type { EvaluationConfig } from "./evaluation_pb";
import { file_google_cloud_datalabeling_v1beta1_evaluation } from "./evaluation_pb";
import type { BoundingPolyConfig, HumanAnnotationConfig, ImageClassificationConfig, TextClassificationConfig } from "./human_annotation_config_pb";
import { file_google_cloud_datalabeling_v1beta1_human_annotation_config } from "./human_annotation_config_pb";
import type { Timestamp } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_timestamp } from "@bufbuild/protobuf/wkt";
import type { Status } from "../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../rpc/status_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/datalabeling/v1beta1/evaluation_job.proto.
 */
export const file_google_cloud_datalabeling_v1beta1_evaluation_job: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_resource, file_google_cloud_datalabeling_v1beta1_dataset, file_google_cloud_datalabeling_v1beta1_evaluation, file_google_cloud_datalabeling_v1beta1_human_annotation_config, file_google_protobuf_timestamp, file_google_rpc_status]);

/**
 * Defines an evaluation job that runs periodically to generate
 * [Evaluations][google.cloud.datalabeling.v1beta1.Evaluation]. [Creating an evaluation
 * job](/ml-engine/docs/continuous-evaluation/create-job) is the starting point
 * for using continuous evaluation.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.EvaluationJob
 */
export type EvaluationJob = Message<"google.cloud.datalabeling.v1beta1.EvaluationJob"> & {
  /**
   * Output only. After you create a job, Data Labeling Service assigns a name
   * to the job with the following format:
   *
   * "projects/<var>{project_id}</var>/evaluationJobs/<var>{evaluation_job_id}</var>"
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Required. Description of the job. The description can be up to 25,000
   * characters long.
   *
   * @generated from field: string description = 2;
   */
  description: string;

  /**
   * Output only. Describes the current state of the job.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.EvaluationJob.State state = 3;
   */
  state: EvaluationJob_State;

  /**
   * Required. Describes the interval at which the job runs. This interval must
   * be at least 1 day, and it is rounded to the nearest day. For example, if
   * you specify a 50-hour interval, the job runs every 2 days.
   *
   * You can provide the schedule in
   * [crontab format](/scheduler/docs/configuring/cron-job-schedules) or in an
   * [English-like
   * format](/appengine/docs/standard/python/config/cronref#schedule_format).
   *
   * Regardless of what you specify, the job will run at 10:00 AM UTC. Only the
   * interval from this schedule is used, not the specific time of day.
   *
   * @generated from field: string schedule = 4;
   */
  schedule: string;

  /**
   * Required. The [AI Platform Prediction model
   * version](/ml-engine/docs/prediction-overview) to be evaluated. Prediction
   * input and output is sampled from this model version. When creating an
   * evaluation job, specify the model version in the following format:
   *
   * "projects/<var>{project_id}</var>/models/<var>{model_name}</var>/versions/<var>{version_name}</var>"
   *
   * There can only be one evaluation job per model version.
   *
   * @generated from field: string model_version = 5;
   */
  modelVersion: string;

  /**
   * Required. Configuration details for the evaluation job.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.EvaluationJobConfig evaluation_job_config = 6;
   */
  evaluationJobConfig?: EvaluationJobConfig;

  /**
   * Required. Name of the [AnnotationSpecSet][google.cloud.datalabeling.v1beta1.AnnotationSpecSet] describing all the
   * labels that your machine learning model outputs. You must create this
   * resource before you create an evaluation job and provide its name in the
   * following format:
   *
   * "projects/<var>{project_id}</var>/annotationSpecSets/<var>{annotation_spec_set_id}</var>"
   *
   * @generated from field: string annotation_spec_set = 7;
   */
  annotationSpecSet: string;

  /**
   * Required. Whether you want Data Labeling Service to provide ground truth
   * labels for prediction input. If you want the service to assign human
   * labelers to annotate your data, set this to `true`. If you want to provide
   * your own ground truth labels in the evaluation job's BigQuery table, set
   * this to `false`.
   *
   * @generated from field: bool label_missing_ground_truth = 8;
   */
  labelMissingGroundTruth: boolean;

  /**
   * Output only. Every time the evaluation job runs and an error occurs, the
   * failed attempt is appended to this array.
   *
   * @generated from field: repeated google.cloud.datalabeling.v1beta1.Attempt attempts = 9;
   */
  attempts: Attempt[];

  /**
   * Output only. Timestamp of when this evaluation job was created.
   *
   * @generated from field: google.protobuf.Timestamp create_time = 10;
   */
  createTime?: Timestamp;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.EvaluationJob.
 * Use `create(EvaluationJobSchema)` to create a new message.
 */
export const EvaluationJobSchema: GenMessage<EvaluationJob> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_evaluation_job, 0);

/**
 * State of the job.
 *
 * @generated from enum google.cloud.datalabeling.v1beta1.EvaluationJob.State
 */
export enum EvaluationJob_State {
  /**
   * @generated from enum value: STATE_UNSPECIFIED = 0;
   */
  STATE_UNSPECIFIED = 0,

  /**
   * The job is scheduled to run at the [configured interval][google.cloud.datalabeling.v1beta1.EvaluationJob.schedule]. You
   * can [pause][google.cloud.datalabeling.v1beta1.DataLabelingService.PauseEvaluationJob] or
   * [delete][google.cloud.datalabeling.v1beta1.DataLabelingService.DeleteEvaluationJob] the job.
   *
   * When the job is in this state, it samples prediction input and output
   * from your model version into your BigQuery table as predictions occur.
   *
   * @generated from enum value: SCHEDULED = 1;
   */
  SCHEDULED = 1,

  /**
   * The job is currently running. When the job runs, Data Labeling Service
   * does several things:
   *
   * 1. If you have configured your job to use Data Labeling Service for
   *    ground truth labeling, the service creates a
   *    [Dataset][google.cloud.datalabeling.v1beta1.Dataset] and a labeling task for all data sampled
   *    since the last time the job ran. Human labelers provide ground truth
   *    labels for your data. Human labeling may take hours, or even days,
   *    depending on how much data has been sampled. The job remains in the
   *    `RUNNING` state during this time, and it can even be running multiple
   *    times in parallel if it gets triggered again (for example 24 hours
   *    later) before the earlier run has completed. When human labelers have
   *    finished labeling the data, the next step occurs.
   *    <br><br>
   *    If you have configured your job to provide your own ground truth
   *    labels, Data Labeling Service still creates a [Dataset][google.cloud.datalabeling.v1beta1.Dataset] for newly
   *    sampled data, but it expects that you have already added ground truth
   *    labels to the BigQuery table by this time. The next step occurs
   *    immediately.
   *
   * 2. Data Labeling Service creates an [Evaluation][google.cloud.datalabeling.v1beta1.Evaluation] by comparing your
   *    model version's predictions with the ground truth labels.
   *
   * If the job remains in this state for a long time, it continues to sample
   * prediction data into your BigQuery table and will run again at the next
   * interval, even if it causes the job to run multiple times in parallel.
   *
   * @generated from enum value: RUNNING = 2;
   */
  RUNNING = 2,

  /**
   * The job is not sampling prediction input and output into your BigQuery
   * table and it will not run according to its schedule. You can
   * [resume][google.cloud.datalabeling.v1beta1.DataLabelingService.ResumeEvaluationJob] the job.
   *
   * @generated from enum value: PAUSED = 3;
   */
  PAUSED = 3,

  /**
   * The job has this state right before it is deleted.
   *
   * @generated from enum value: STOPPED = 4;
   */
  STOPPED = 4,
}

/**
 * Describes the enum google.cloud.datalabeling.v1beta1.EvaluationJob.State.
 */
export const EvaluationJob_StateSchema: GenEnum<EvaluationJob_State> = /*@__PURE__*/
  enumDesc(file_google_cloud_datalabeling_v1beta1_evaluation_job, 0, 0);

/**
 * Configures specific details of how a continuous evaluation job works. Provide
 * this configuration when you create an EvaluationJob.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.EvaluationJobConfig
 */
export type EvaluationJobConfig = Message<"google.cloud.datalabeling.v1beta1.EvaluationJobConfig"> & {
  /**
   * Required. Details for how you want human reviewers to provide ground truth
   * labels.
   *
   * @generated from oneof google.cloud.datalabeling.v1beta1.EvaluationJobConfig.human_annotation_request_config
   */
  humanAnnotationRequestConfig: {
    /**
     * Specify this field if your model version performs image classification or
     * general classification.
     *
     * `annotationSpecSet` in this configuration must match
     * [EvaluationJob.annotationSpecSet][google.cloud.datalabeling.v1beta1.EvaluationJob.annotation_spec_set].
     * `allowMultiLabel` in this configuration must match
     * `classificationMetadata.isMultiLabel` in [input_config][google.cloud.datalabeling.v1beta1.EvaluationJobConfig.input_config].
     *
     * @generated from field: google.cloud.datalabeling.v1beta1.ImageClassificationConfig image_classification_config = 4;
     */
    value: ImageClassificationConfig;
    case: "imageClassificationConfig";
  } | {
    /**
     * Specify this field if your model version performs image object detection
     * (bounding box detection).
     *
     * `annotationSpecSet` in this configuration must match
     * [EvaluationJob.annotationSpecSet][google.cloud.datalabeling.v1beta1.EvaluationJob.annotation_spec_set].
     *
     * @generated from field: google.cloud.datalabeling.v1beta1.BoundingPolyConfig bounding_poly_config = 5;
     */
    value: BoundingPolyConfig;
    case: "boundingPolyConfig";
  } | {
    /**
     * Specify this field if your model version performs text classification.
     *
     * `annotationSpecSet` in this configuration must match
     * [EvaluationJob.annotationSpecSet][google.cloud.datalabeling.v1beta1.EvaluationJob.annotation_spec_set].
     * `allowMultiLabel` in this configuration must match
     * `classificationMetadata.isMultiLabel` in [input_config][google.cloud.datalabeling.v1beta1.EvaluationJobConfig.input_config].
     *
     * @generated from field: google.cloud.datalabeling.v1beta1.TextClassificationConfig text_classification_config = 8;
     */
    value: TextClassificationConfig;
    case: "textClassificationConfig";
  } | { case: undefined; value?: undefined };

  /**
   * Rquired. Details for the sampled prediction input. Within this
   * configuration, there are requirements for several fields:
   *
   * * `dataType` must be one of `IMAGE`, `TEXT`, or `GENERAL_DATA`.
   * * `annotationType` must be one of `IMAGE_CLASSIFICATION_ANNOTATION`,
   *   `TEXT_CLASSIFICATION_ANNOTATION`, `GENERAL_CLASSIFICATION_ANNOTATION`,
   *   or `IMAGE_BOUNDING_BOX_ANNOTATION` (image object detection).
   * * If your machine learning model performs classification, you must specify
   *   `classificationMetadata.isMultiLabel`.
   * * You must specify `bigquerySource` (not `gcsSource`).
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.InputConfig input_config = 1;
   */
  inputConfig?: InputConfig;

  /**
   * Required. Details for calculating evaluation metrics and creating
   * [Evaulations][google.cloud.datalabeling.v1beta1.Evaluation]. If your model version performs image object
   * detection, you must specify the `boundingBoxEvaluationOptions` field within
   * this configuration. Otherwise, provide an empty object for this
   * configuration.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.EvaluationConfig evaluation_config = 2;
   */
  evaluationConfig?: EvaluationConfig;

  /**
   * Optional. Details for human annotation of your data. If you set
   * [labelMissingGroundTruth][google.cloud.datalabeling.v1beta1.EvaluationJob.label_missing_ground_truth] to
   * `true` for this evaluation job, then you must specify this field. If you
   * plan to provide your own ground truth labels, then omit this field.
   *
   * Note that you must create an [Instruction][google.cloud.datalabeling.v1beta1.Instruction] resource before you can
   * specify this field. Provide the name of the instruction resource in the
   * `instruction` field within this configuration.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.HumanAnnotationConfig human_annotation_config = 3;
   */
  humanAnnotationConfig?: HumanAnnotationConfig;

  /**
   * Required. Prediction keys that tell Data Labeling Service where to find the
   * data for evaluation in your BigQuery table. When the service samples
   * prediction input and output from your model version and saves it to
   * BigQuery, the data gets stored as JSON strings in the BigQuery table. These
   * keys tell Data Labeling Service how to parse the JSON.
   *
   * You can provide the following entries in this field:
   *
   * * `data_json_key`: the data key for prediction input. You must provide
   *   either this key or `reference_json_key`.
   * * `reference_json_key`: the data reference key for prediction input. You
   *   must provide either this key or `data_json_key`.
   * * `label_json_key`: the label key for prediction output. Required.
   * * `label_score_json_key`: the score key for prediction output. Required.
   * * `bounding_box_json_key`: the bounding box key for prediction output.
   *   Required if your model version perform image object detection.
   *
   * Learn [how to configure prediction
   * keys](/ml-engine/docs/continuous-evaluation/create-job#prediction-keys).
   *
   * @generated from field: map<string, string> bigquery_import_keys = 9;
   */
  bigqueryImportKeys: { [key: string]: string };

  /**
   * Required. The maximum number of predictions to sample and save to BigQuery
   * during each [evaluation interval][google.cloud.datalabeling.v1beta1.EvaluationJob.schedule]. This limit
   * overrides `example_sample_percentage`: even if the service has not sampled
   * enough predictions to fulfill `example_sample_perecentage` during an
   * interval, it stops sampling predictions when it meets this limit.
   *
   * @generated from field: int32 example_count = 10;
   */
  exampleCount: number;

  /**
   * Required. Fraction of predictions to sample and save to BigQuery during
   * each [evaluation interval][google.cloud.datalabeling.v1beta1.EvaluationJob.schedule]. For example, 0.1 means
   * 10% of predictions served by your model version get saved to BigQuery.
   *
   * @generated from field: double example_sample_percentage = 11;
   */
  exampleSamplePercentage: number;

  /**
   * Optional. Configuration details for evaluation job alerts. Specify this
   * field if you want to receive email alerts if the evaluation job finds that
   * your predictions have low mean average precision during a run.
   *
   * @generated from field: google.cloud.datalabeling.v1beta1.EvaluationJobAlertConfig evaluation_job_alert_config = 13;
   */
  evaluationJobAlertConfig?: EvaluationJobAlertConfig;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.EvaluationJobConfig.
 * Use `create(EvaluationJobConfigSchema)` to create a new message.
 */
export const EvaluationJobConfigSchema: GenMessage<EvaluationJobConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_evaluation_job, 1);

/**
 * Provides details for how an evaluation job sends email alerts based on the
 * results of a run.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.EvaluationJobAlertConfig
 */
export type EvaluationJobAlertConfig = Message<"google.cloud.datalabeling.v1beta1.EvaluationJobAlertConfig"> & {
  /**
   * Required. An email address to send alerts to.
   *
   * @generated from field: string email = 1;
   */
  email: string;

  /**
   * Required. A number between 0 and 1 that describes a minimum mean average
   * precision threshold. When the evaluation job runs, if it calculates that
   * your model version's predictions from the recent interval have
   * [meanAveragePrecision][google.cloud.datalabeling.v1beta1.PrCurve.mean_average_precision] below this
   * threshold, then it sends an alert to your specified email.
   *
   * @generated from field: double min_acceptable_mean_average_precision = 2;
   */
  minAcceptableMeanAveragePrecision: number;
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.EvaluationJobAlertConfig.
 * Use `create(EvaluationJobAlertConfigSchema)` to create a new message.
 */
export const EvaluationJobAlertConfigSchema: GenMessage<EvaluationJobAlertConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_evaluation_job, 2);

/**
 * Records a failed evaluation job run.
 *
 * @generated from message google.cloud.datalabeling.v1beta1.Attempt
 */
export type Attempt = Message<"google.cloud.datalabeling.v1beta1.Attempt"> & {
  /**
   * @generated from field: google.protobuf.Timestamp attempt_time = 1;
   */
  attemptTime?: Timestamp;

  /**
   * Details of errors that occurred.
   *
   * @generated from field: repeated google.rpc.Status partial_failures = 2;
   */
  partialFailures: Status[];
};

/**
 * Describes the message google.cloud.datalabeling.v1beta1.Attempt.
 * Use `create(AttemptSchema)` to create a new message.
 */
export const AttemptSchema: GenMessage<Attempt> = /*@__PURE__*/
  messageDesc(file_google_cloud_datalabeling_v1beta1_evaluation_job, 3);

