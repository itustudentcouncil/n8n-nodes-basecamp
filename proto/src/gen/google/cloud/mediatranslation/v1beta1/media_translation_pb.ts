// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/mediatranslation/v1beta1/media_translation.proto (package google.cloud.mediatranslation.v1beta1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import type { Status } from "../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../rpc/status_pb";
import { file_google_api_client } from "../../../api/client_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/mediatranslation/v1beta1/media_translation.proto.
 */
export const file_google_cloud_mediatranslation_v1beta1_media_translation: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_field_behavior, file_google_rpc_status, file_google_api_client]);

/**
 * Provides information to the speech translation that specifies how to process
 * the request.
 *
 * @generated from message google.cloud.mediatranslation.v1beta1.TranslateSpeechConfig
 */
export type TranslateSpeechConfig = Message<"google.cloud.mediatranslation.v1beta1.TranslateSpeechConfig"> & {
  /**
   * Required. Encoding of audio data.
   * Supported formats:
   *
   * - `linear16`
   *
   *   Uncompressed 16-bit signed little-endian samples (Linear PCM).
   *
   * - `flac`
   *
   *   `flac` (Free Lossless Audio Codec) is the recommended encoding
   *   because it is lossless--therefore recognition is not compromised--and
   *   requires only about half the bandwidth of `linear16`.
   *
   * - `mulaw`
   *
   *   8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
   *
   * - `amr`
   *
   *   Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000.
   *
   * - `amr-wb`
   *
   *   Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000.
   *
   * - `ogg-opus`
   *
   *   Opus encoded audio frames in [Ogg](https://wikipedia.org/wiki/Ogg)
   *   container. `sample_rate_hertz` must be one of 8000, 12000, 16000, 24000,
   *   or 48000.
   *
   * - `mp3`
   *
   *   MP3 audio. Support all standard MP3 bitrates (which range from 32-320
   *   kbps). When using this encoding, `sample_rate_hertz` has to match the
   *   sample rate of the file being used.
   *
   * @generated from field: string audio_encoding = 1;
   */
  audioEncoding: string;

  /**
   * Required. Source language code (BCP-47) of the input audio.
   *
   * @generated from field: string source_language_code = 2;
   */
  sourceLanguageCode: string;

  /**
   * Required. Target language code (BCP-47) of the output.
   *
   * @generated from field: string target_language_code = 3;
   */
  targetLanguageCode: string;

  /**
   * Optional. Sample rate in Hertz of the audio data. Valid values are:
   * 8000-48000. 16000 is optimal. For best results, set the sampling rate of
   * the audio source to 16000 Hz. If that's not possible, use the native sample
   * rate of the audio source (instead of re-sampling).
   *
   * @generated from field: int32 sample_rate_hertz = 4;
   */
  sampleRateHertz: number;

  /**
   * Optional. `google-provided-model/video` and
   * `google-provided-model/enhanced-phone-call` are premium models.
   * `google-provided-model/phone-call` is not premium model.
   *
   * @generated from field: string model = 5;
   */
  model: string;
};

/**
 * Describes the message google.cloud.mediatranslation.v1beta1.TranslateSpeechConfig.
 * Use `create(TranslateSpeechConfigSchema)` to create a new message.
 */
export const TranslateSpeechConfigSchema: GenMessage<TranslateSpeechConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_mediatranslation_v1beta1_media_translation, 0);

/**
 * Config used for streaming translation.
 *
 * @generated from message google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechConfig
 */
export type StreamingTranslateSpeechConfig = Message<"google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechConfig"> & {
  /**
   * Required. The common config for all the following audio contents.
   *
   * @generated from field: google.cloud.mediatranslation.v1beta1.TranslateSpeechConfig audio_config = 1;
   */
  audioConfig?: TranslateSpeechConfig;

  /**
   * Optional. If `false` or omitted, the system performs
   * continuous translation (continuing to wait for and process audio even if
   * the user pauses speaking) until the client closes the input stream (gRPC
   * API) or until the maximum time limit has been reached. May return multiple
   * `StreamingTranslateSpeechResult`s with the `is_final` flag set to `true`.
   *
   * If `true`, the speech translator will detect a single spoken utterance.
   * When it detects that the user has paused or stopped speaking, it will
   * return an `END_OF_SINGLE_UTTERANCE` event and cease translation.
   * When the client receives 'END_OF_SINGLE_UTTERANCE' event, the client should
   * stop sending the requests. However, clients should keep receiving remaining
   * responses until the stream is terminated. To construct the complete
   * sentence in a streaming way, one should override (if 'is_final' of previous
   * response is false), or append (if 'is_final' of previous response is true).
   *
   * @generated from field: bool single_utterance = 2;
   */
  singleUtterance: boolean;
};

/**
 * Describes the message google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechConfig.
 * Use `create(StreamingTranslateSpeechConfigSchema)` to create a new message.
 */
export const StreamingTranslateSpeechConfigSchema: GenMessage<StreamingTranslateSpeechConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_mediatranslation_v1beta1_media_translation, 1);

/**
 * The top-level message sent by the client for the `StreamingTranslateSpeech`
 * method. Multiple `StreamingTranslateSpeechRequest` messages are sent. The
 * first message must contain a `streaming_config` message and must not contain
 * `audio_content` data. All subsequent messages must contain `audio_content`
 * data and must not contain a `streaming_config` message.
 *
 * @generated from message google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechRequest
 */
export type StreamingTranslateSpeechRequest = Message<"google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechRequest"> & {
  /**
   * The streaming request, which is either a streaming config or content.
   *
   * @generated from oneof google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechRequest.streaming_request
   */
  streamingRequest: {
    /**
     * Provides information to the recognizer that specifies how to process the
     * request. The first `StreamingTranslateSpeechRequest` message must contain
     * a `streaming_config` message.
     *
     * @generated from field: google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechConfig streaming_config = 1;
     */
    value: StreamingTranslateSpeechConfig;
    case: "streamingConfig";
  } | {
    /**
     * The audio data to be translated. Sequential chunks of audio data are sent
     * in sequential `StreamingTranslateSpeechRequest` messages. The first
     * `StreamingTranslateSpeechRequest` message must not contain
     * `audio_content` data and all subsequent `StreamingTranslateSpeechRequest`
     * messages must contain `audio_content` data. The audio bytes must be
     * encoded as specified in `StreamingTranslateSpeechConfig`. Note: as with
     * all bytes fields, protobuffers use a pure binary representation (not
     * base64).
     *
     * @generated from field: bytes audio_content = 2;
     */
    value: Uint8Array;
    case: "audioContent";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechRequest.
 * Use `create(StreamingTranslateSpeechRequestSchema)` to create a new message.
 */
export const StreamingTranslateSpeechRequestSchema: GenMessage<StreamingTranslateSpeechRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_mediatranslation_v1beta1_media_translation, 2);

/**
 * A streaming speech translation result corresponding to a portion of the audio
 * that is currently being processed.
 *
 * @generated from message google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechResult
 */
export type StreamingTranslateSpeechResult = Message<"google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechResult"> & {
  /**
   * Translation result.
   *
   * @generated from oneof google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechResult.result
   */
  result: {
    /**
     * Text translation result.
     *
     * @generated from field: google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechResult.TextTranslationResult text_translation_result = 1;
     */
    value: StreamingTranslateSpeechResult_TextTranslationResult;
    case: "textTranslationResult";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechResult.
 * Use `create(StreamingTranslateSpeechResultSchema)` to create a new message.
 */
export const StreamingTranslateSpeechResultSchema: GenMessage<StreamingTranslateSpeechResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_mediatranslation_v1beta1_media_translation, 3);

/**
 * Text translation result.
 *
 * @generated from message google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechResult.TextTranslationResult
 */
export type StreamingTranslateSpeechResult_TextTranslationResult = Message<"google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechResult.TextTranslationResult"> & {
  /**
   * Output only. The translated sentence.
   *
   * @generated from field: string translation = 1;
   */
  translation: string;

  /**
   * Output only. If `false`, this `StreamingTranslateSpeechResult` represents
   * an interim result that may change. If `true`, this is the final time the
   * translation service will return this particular
   * `StreamingTranslateSpeechResult`, the streaming translator will not
   * return any further hypotheses for this portion of the transcript and
   * corresponding audio.
   *
   * @generated from field: bool is_final = 2;
   */
  isFinal: boolean;
};

/**
 * Describes the message google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechResult.TextTranslationResult.
 * Use `create(StreamingTranslateSpeechResult_TextTranslationResultSchema)` to create a new message.
 */
export const StreamingTranslateSpeechResult_TextTranslationResultSchema: GenMessage<StreamingTranslateSpeechResult_TextTranslationResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_mediatranslation_v1beta1_media_translation, 3, 0);

/**
 * A streaming speech translation response corresponding to a portion of
 * the audio currently processed.
 *
 * @generated from message google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechResponse
 */
export type StreamingTranslateSpeechResponse = Message<"google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechResponse"> & {
  /**
   * Output only. If set, returns a [google.rpc.Status][google.rpc.Status] message that
   * specifies the error for the operation.
   *
   * @generated from field: google.rpc.Status error = 1;
   */
  error?: Status;

  /**
   * Output only. The translation result that is currently being processed (is_final could be
   * true or false).
   *
   * @generated from field: google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechResult result = 2;
   */
  result?: StreamingTranslateSpeechResult;

  /**
   * Output only. Indicates the type of speech event.
   *
   * @generated from field: google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechResponse.SpeechEventType speech_event_type = 3;
   */
  speechEventType: StreamingTranslateSpeechResponse_SpeechEventType;
};

/**
 * Describes the message google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechResponse.
 * Use `create(StreamingTranslateSpeechResponseSchema)` to create a new message.
 */
export const StreamingTranslateSpeechResponseSchema: GenMessage<StreamingTranslateSpeechResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_mediatranslation_v1beta1_media_translation, 4);

/**
 * Indicates the type of speech event.
 *
 * @generated from enum google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechResponse.SpeechEventType
 */
export enum StreamingTranslateSpeechResponse_SpeechEventType {
  /**
   * No speech event specified.
   *
   * @generated from enum value: SPEECH_EVENT_TYPE_UNSPECIFIED = 0;
   */
  SPEECH_EVENT_TYPE_UNSPECIFIED = 0,

  /**
   * This event indicates that the server has detected the end of the user's
   * speech utterance and expects no additional speech. Therefore, the server
   * will not process additional audio (although it may subsequently return
   * additional results). When the client receives 'END_OF_SINGLE_UTTERANCE'
   * event, the client should stop sending the requests. However, clients
   * should keep receiving remaining responses until the stream is terminated.
   * To construct the complete sentence in a streaming way, one should
   * override (if 'is_final' of previous response is false), or append (if
   * 'is_final' of previous response is true). This event is only sent if
   * `single_utterance` was set to `true`, and is not used otherwise.
   *
   * @generated from enum value: END_OF_SINGLE_UTTERANCE = 1;
   */
  END_OF_SINGLE_UTTERANCE = 1,
}

/**
 * Describes the enum google.cloud.mediatranslation.v1beta1.StreamingTranslateSpeechResponse.SpeechEventType.
 */
export const StreamingTranslateSpeechResponse_SpeechEventTypeSchema: GenEnum<StreamingTranslateSpeechResponse_SpeechEventType> = /*@__PURE__*/
  enumDesc(file_google_cloud_mediatranslation_v1beta1_media_translation, 4, 0);

/**
 * Provides translation from/to media types.
 *
 * @generated from service google.cloud.mediatranslation.v1beta1.SpeechTranslationService
 */
export const SpeechTranslationService: GenService<{
  /**
   * Performs bidirectional streaming speech translation: receive results while
   * sending audio. This method is only available via the gRPC API (not REST).
   *
   * @generated from rpc google.cloud.mediatranslation.v1beta1.SpeechTranslationService.StreamingTranslateSpeech
   */
  streamingTranslateSpeech: {
    methodKind: "bidi_streaming";
    input: typeof StreamingTranslateSpeechRequestSchema;
    output: typeof StreamingTranslateSpeechResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_mediatranslation_v1beta1_media_translation, 0);

