// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/videointelligence/v1p1beta1/video_intelligence.proto (package google.cloud.videointelligence.v1p1beta1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import type { OperationSchema } from "../../../longrunning/operations_pb";
import { file_google_longrunning_operations } from "../../../longrunning/operations_pb";
import type { Duration, Timestamp } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_duration, file_google_protobuf_timestamp } from "@bufbuild/protobuf/wkt";
import type { Status } from "../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../rpc/status_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/videointelligence/v1p1beta1/video_intelligence.proto.
 */
export const file_google_cloud_videointelligence_v1p1beta1_video_intelligence: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_longrunning_operations, file_google_protobuf_duration, file_google_protobuf_timestamp, file_google_rpc_status]);

/**
 * Video annotation request.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.AnnotateVideoRequest
 */
export type AnnotateVideoRequest = Message<"google.cloud.videointelligence.v1p1beta1.AnnotateVideoRequest"> & {
  /**
   * Input video location. Currently, only
   * [Google Cloud Storage](https://cloud.google.com/storage/) URIs are
   * supported, which must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
   * more information, see [Request
   * URIs](https://cloud.google.com/storage/docs/request-endpoints). A video URI
   * may include wildcards in `object-id`, and thus identify multiple videos.
   * Supported wildcards: '*' to match 0 or more characters;
   * '?' to match 1 character. If unset, the input video should be embedded
   * in the request as `input_content`. If set, `input_content` should be unset.
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri: string;

  /**
   * The video data bytes.
   * If unset, the input video(s) should be specified via `input_uri`.
   * If set, `input_uri` should be unset.
   *
   * @generated from field: bytes input_content = 6;
   */
  inputContent: Uint8Array;

  /**
   * Required. Requested video annotation features.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p1beta1.Feature features = 2;
   */
  features: Feature[];

  /**
   * Additional video context and/or feature-specific parameters.
   *
   * @generated from field: google.cloud.videointelligence.v1p1beta1.VideoContext video_context = 3;
   */
  videoContext?: VideoContext;

  /**
   * Optional. Location where the output (in JSON format) should be stored.
   * Currently, only [Google Cloud Storage](https://cloud.google.com/storage/)
   * URIs are supported, which must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
   * more information, see [Request
   * URIs](https://cloud.google.com/storage/docs/request-endpoints).
   *
   * @generated from field: string output_uri = 4;
   */
  outputUri: string;

  /**
   * Optional. Cloud region where annotation should take place. Supported cloud
   * regions: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no region
   * is specified, a region will be determined based on video file location.
   *
   * @generated from field: string location_id = 5;
   */
  locationId: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.AnnotateVideoRequest.
 * Use `create(AnnotateVideoRequestSchema)` to create a new message.
 */
export const AnnotateVideoRequestSchema: GenMessage<AnnotateVideoRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 0);

/**
 * Video context and/or feature-specific parameters.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.VideoContext
 */
export type VideoContext = Message<"google.cloud.videointelligence.v1p1beta1.VideoContext"> & {
  /**
   * Video segments to annotate. The segments may overlap and are not required
   * to be contiguous or span the whole video. If unspecified, each video is
   * treated as a single segment.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p1beta1.VideoSegment segments = 1;
   */
  segments: VideoSegment[];

  /**
   * Config for LABEL_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1p1beta1.LabelDetectionConfig label_detection_config = 2;
   */
  labelDetectionConfig?: LabelDetectionConfig;

  /**
   * Config for SHOT_CHANGE_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1p1beta1.ShotChangeDetectionConfig shot_change_detection_config = 3;
   */
  shotChangeDetectionConfig?: ShotChangeDetectionConfig;

  /**
   * Config for EXPLICIT_CONTENT_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1p1beta1.ExplicitContentDetectionConfig explicit_content_detection_config = 4;
   */
  explicitContentDetectionConfig?: ExplicitContentDetectionConfig;

  /**
   * Config for SPEECH_TRANSCRIPTION.
   *
   * @generated from field: google.cloud.videointelligence.v1p1beta1.SpeechTranscriptionConfig speech_transcription_config = 6;
   */
  speechTranscriptionConfig?: SpeechTranscriptionConfig;
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.VideoContext.
 * Use `create(VideoContextSchema)` to create a new message.
 */
export const VideoContextSchema: GenMessage<VideoContext> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 1);

/**
 * Config for LABEL_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.LabelDetectionConfig
 */
export type LabelDetectionConfig = Message<"google.cloud.videointelligence.v1p1beta1.LabelDetectionConfig"> & {
  /**
   * What labels should be detected with LABEL_DETECTION, in addition to
   * video-level labels or segment-level labels.
   * If unspecified, defaults to `SHOT_MODE`.
   *
   * @generated from field: google.cloud.videointelligence.v1p1beta1.LabelDetectionMode label_detection_mode = 1;
   */
  labelDetectionMode: LabelDetectionMode;

  /**
   * Whether the video has been shot from a stationary (i.e. non-moving) camera.
   * When set to true, might improve detection accuracy for moving objects.
   * Should be used with `SHOT_AND_FRAME_MODE` enabled.
   *
   * @generated from field: bool stationary_camera = 2;
   */
  stationaryCamera: boolean;

  /**
   * Model to use for label detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 3;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.LabelDetectionConfig.
 * Use `create(LabelDetectionConfigSchema)` to create a new message.
 */
export const LabelDetectionConfigSchema: GenMessage<LabelDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 2);

/**
 * Config for SHOT_CHANGE_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.ShotChangeDetectionConfig
 */
export type ShotChangeDetectionConfig = Message<"google.cloud.videointelligence.v1p1beta1.ShotChangeDetectionConfig"> & {
  /**
   * Model to use for shot change detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.ShotChangeDetectionConfig.
 * Use `create(ShotChangeDetectionConfigSchema)` to create a new message.
 */
export const ShotChangeDetectionConfigSchema: GenMessage<ShotChangeDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 3);

/**
 * Config for EXPLICIT_CONTENT_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.ExplicitContentDetectionConfig
 */
export type ExplicitContentDetectionConfig = Message<"google.cloud.videointelligence.v1p1beta1.ExplicitContentDetectionConfig"> & {
  /**
   * Model to use for explicit content detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.ExplicitContentDetectionConfig.
 * Use `create(ExplicitContentDetectionConfigSchema)` to create a new message.
 */
export const ExplicitContentDetectionConfigSchema: GenMessage<ExplicitContentDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 4);

/**
 * Video segment.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.VideoSegment
 */
export type VideoSegment = Message<"google.cloud.videointelligence.v1p1beta1.VideoSegment"> & {
  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the start of the segment (inclusive).
   *
   * @generated from field: google.protobuf.Duration start_time_offset = 1;
   */
  startTimeOffset?: Duration;

  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the end of the segment (inclusive).
   *
   * @generated from field: google.protobuf.Duration end_time_offset = 2;
   */
  endTimeOffset?: Duration;
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.VideoSegment.
 * Use `create(VideoSegmentSchema)` to create a new message.
 */
export const VideoSegmentSchema: GenMessage<VideoSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 5);

/**
 * Video segment level annotation results for label detection.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.LabelSegment
 */
export type LabelSegment = Message<"google.cloud.videointelligence.v1p1beta1.LabelSegment"> & {
  /**
   * Video segment where a label was detected.
   *
   * @generated from field: google.cloud.videointelligence.v1p1beta1.VideoSegment segment = 1;
   */
  segment?: VideoSegment;

  /**
   * Confidence that the label is accurate. Range: [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.LabelSegment.
 * Use `create(LabelSegmentSchema)` to create a new message.
 */
export const LabelSegmentSchema: GenMessage<LabelSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 6);

/**
 * Video frame level annotation results for label detection.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.LabelFrame
 */
export type LabelFrame = Message<"google.cloud.videointelligence.v1p1beta1.LabelFrame"> & {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   *
   * @generated from field: google.protobuf.Duration time_offset = 1;
   */
  timeOffset?: Duration;

  /**
   * Confidence that the label is accurate. Range: [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.LabelFrame.
 * Use `create(LabelFrameSchema)` to create a new message.
 */
export const LabelFrameSchema: GenMessage<LabelFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 7);

/**
 * Detected entity from video analysis.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.Entity
 */
export type Entity = Message<"google.cloud.videointelligence.v1p1beta1.Entity"> & {
  /**
   * Opaque entity ID. Some IDs may be available in
   * [Google Knowledge Graph Search
   * API](https://developers.google.com/knowledge-graph/).
   *
   * @generated from field: string entity_id = 1;
   */
  entityId: string;

  /**
   * Textual description, e.g. `Fixed-gear bicycle`.
   *
   * @generated from field: string description = 2;
   */
  description: string;

  /**
   * Language code for `description` in BCP-47 format.
   *
   * @generated from field: string language_code = 3;
   */
  languageCode: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.Entity.
 * Use `create(EntitySchema)` to create a new message.
 */
export const EntitySchema: GenMessage<Entity> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 8);

/**
 * Label annotation.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.LabelAnnotation
 */
export type LabelAnnotation = Message<"google.cloud.videointelligence.v1p1beta1.LabelAnnotation"> & {
  /**
   * Detected entity.
   *
   * @generated from field: google.cloud.videointelligence.v1p1beta1.Entity entity = 1;
   */
  entity?: Entity;

  /**
   * Common categories for the detected entity.
   * E.g. when the label is `Terrier` the category is likely `dog`. And in some
   * cases there might be more than one categories e.g. `Terrier` could also be
   * a `pet`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p1beta1.Entity category_entities = 2;
   */
  categoryEntities: Entity[];

  /**
   * All video segments where a label was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p1beta1.LabelSegment segments = 3;
   */
  segments: LabelSegment[];

  /**
   * All video frames where a label was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p1beta1.LabelFrame frames = 4;
   */
  frames: LabelFrame[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.LabelAnnotation.
 * Use `create(LabelAnnotationSchema)` to create a new message.
 */
export const LabelAnnotationSchema: GenMessage<LabelAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 9);

/**
 * Video frame level annotation results for explicit content.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.ExplicitContentFrame
 */
export type ExplicitContentFrame = Message<"google.cloud.videointelligence.v1p1beta1.ExplicitContentFrame"> & {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   *
   * @generated from field: google.protobuf.Duration time_offset = 1;
   */
  timeOffset?: Duration;

  /**
   * Likelihood of the pornography content..
   *
   * @generated from field: google.cloud.videointelligence.v1p1beta1.Likelihood pornography_likelihood = 2;
   */
  pornographyLikelihood: Likelihood;
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.ExplicitContentFrame.
 * Use `create(ExplicitContentFrameSchema)` to create a new message.
 */
export const ExplicitContentFrameSchema: GenMessage<ExplicitContentFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 10);

/**
 * Explicit content annotation (based on per-frame visual signals only).
 * If no explicit content has been detected in a frame, no annotations are
 * present for that frame.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.ExplicitContentAnnotation
 */
export type ExplicitContentAnnotation = Message<"google.cloud.videointelligence.v1p1beta1.ExplicitContentAnnotation"> & {
  /**
   * All video frames where explicit content was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p1beta1.ExplicitContentFrame frames = 1;
   */
  frames: ExplicitContentFrame[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.ExplicitContentAnnotation.
 * Use `create(ExplicitContentAnnotationSchema)` to create a new message.
 */
export const ExplicitContentAnnotationSchema: GenMessage<ExplicitContentAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 11);

/**
 * Annotation results for a single video.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.VideoAnnotationResults
 */
export type VideoAnnotationResults = Message<"google.cloud.videointelligence.v1p1beta1.VideoAnnotationResults"> & {
  /**
   * Output only. Video file location in
   * [Google Cloud Storage](https://cloud.google.com/storage/).
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri: string;

  /**
   * Label annotations on video level or user specified segment level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p1beta1.LabelAnnotation segment_label_annotations = 2;
   */
  segmentLabelAnnotations: LabelAnnotation[];

  /**
   * Label annotations on shot level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p1beta1.LabelAnnotation shot_label_annotations = 3;
   */
  shotLabelAnnotations: LabelAnnotation[];

  /**
   * Label annotations on frame level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p1beta1.LabelAnnotation frame_label_annotations = 4;
   */
  frameLabelAnnotations: LabelAnnotation[];

  /**
   * Shot annotations. Each shot is represented as a video segment.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p1beta1.VideoSegment shot_annotations = 6;
   */
  shotAnnotations: VideoSegment[];

  /**
   * Explicit content annotation.
   *
   * @generated from field: google.cloud.videointelligence.v1p1beta1.ExplicitContentAnnotation explicit_annotation = 7;
   */
  explicitAnnotation?: ExplicitContentAnnotation;

  /**
   * Speech transcription.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p1beta1.SpeechTranscription speech_transcriptions = 11;
   */
  speechTranscriptions: SpeechTranscription[];

  /**
   * Output only. If set, indicates an error. Note that for a single
   * `AnnotateVideoRequest` some videos may succeed and some may fail.
   *
   * @generated from field: google.rpc.Status error = 9;
   */
  error?: Status;
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.VideoAnnotationResults.
 * Use `create(VideoAnnotationResultsSchema)` to create a new message.
 */
export const VideoAnnotationResultsSchema: GenMessage<VideoAnnotationResults> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 12);

/**
 * Video annotation response. Included in the `response`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.AnnotateVideoResponse
 */
export type AnnotateVideoResponse = Message<"google.cloud.videointelligence.v1p1beta1.AnnotateVideoResponse"> & {
  /**
   * Annotation results for all videos specified in `AnnotateVideoRequest`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p1beta1.VideoAnnotationResults annotation_results = 1;
   */
  annotationResults: VideoAnnotationResults[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.AnnotateVideoResponse.
 * Use `create(AnnotateVideoResponseSchema)` to create a new message.
 */
export const AnnotateVideoResponseSchema: GenMessage<AnnotateVideoResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 13);

/**
 * Annotation progress for a single video.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.VideoAnnotationProgress
 */
export type VideoAnnotationProgress = Message<"google.cloud.videointelligence.v1p1beta1.VideoAnnotationProgress"> & {
  /**
   * Output only. Video file location in
   * [Google Cloud Storage](https://cloud.google.com/storage/).
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri: string;

  /**
   * Output only. Approximate percentage processed thus far. Guaranteed to be
   * 100 when fully processed.
   *
   * @generated from field: int32 progress_percent = 2;
   */
  progressPercent: number;

  /**
   * Output only. Time when the request was received.
   *
   * @generated from field: google.protobuf.Timestamp start_time = 3;
   */
  startTime?: Timestamp;

  /**
   * Output only. Time of the most recent update.
   *
   * @generated from field: google.protobuf.Timestamp update_time = 4;
   */
  updateTime?: Timestamp;
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.VideoAnnotationProgress.
 * Use `create(VideoAnnotationProgressSchema)` to create a new message.
 */
export const VideoAnnotationProgressSchema: GenMessage<VideoAnnotationProgress> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 14);

/**
 * Video annotation progress. Included in the `metadata`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.AnnotateVideoProgress
 */
export type AnnotateVideoProgress = Message<"google.cloud.videointelligence.v1p1beta1.AnnotateVideoProgress"> & {
  /**
   * Progress metadata for all videos specified in `AnnotateVideoRequest`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p1beta1.VideoAnnotationProgress annotation_progress = 1;
   */
  annotationProgress: VideoAnnotationProgress[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.AnnotateVideoProgress.
 * Use `create(AnnotateVideoProgressSchema)` to create a new message.
 */
export const AnnotateVideoProgressSchema: GenMessage<AnnotateVideoProgress> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 15);

/**
 * Config for SPEECH_TRANSCRIPTION.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.SpeechTranscriptionConfig
 */
export type SpeechTranscriptionConfig = Message<"google.cloud.videointelligence.v1p1beta1.SpeechTranscriptionConfig"> & {
  /**
   * Required. *Required* The language of the supplied audio as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Example: "en-US".
   * See [Language Support](https://cloud.google.com/speech/docs/languages)
   * for a list of the currently supported language codes.
   *
   * @generated from field: string language_code = 1;
   */
  languageCode: string;

  /**
   * Optional. Maximum number of recognition hypotheses to be returned.
   * Specifically, the maximum number of `SpeechRecognitionAlternative` messages
   * within each `SpeechTranscription`. The server may return fewer than
   * `max_alternatives`. Valid values are `0`-`30`. A value of `0` or `1` will
   * return a maximum of one. If omitted, will return a maximum of one.
   *
   * @generated from field: int32 max_alternatives = 2;
   */
  maxAlternatives: number;

  /**
   * Optional. If set to `true`, the server will attempt to filter out
   * profanities, replacing all but the initial character in each filtered word
   * with asterisks, e.g. "f***". If set to `false` or omitted, profanities
   * won't be filtered out.
   *
   * @generated from field: bool filter_profanity = 3;
   */
  filterProfanity: boolean;

  /**
   * Optional. A means to provide context to assist the speech recognition.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p1beta1.SpeechContext speech_contexts = 4;
   */
  speechContexts: SpeechContext[];

  /**
   * Optional. If 'true', adds punctuation to recognition result hypotheses.
   * This feature is only available in select languages. Setting this for
   * requests in other languages has no effect at all. The default 'false' value
   * does not add punctuation to result hypotheses. NOTE: "This is currently
   * offered as an experimental service, complimentary to all users. In the
   * future this may be exclusively available as a premium feature."
   *
   * @generated from field: bool enable_automatic_punctuation = 5;
   */
  enableAutomaticPunctuation: boolean;

  /**
   * Optional. For file formats, such as MXF or MKV, supporting multiple audio
   * tracks, specify up to two tracks. Default: track 0.
   *
   * @generated from field: repeated int32 audio_tracks = 6;
   */
  audioTracks: number[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.SpeechTranscriptionConfig.
 * Use `create(SpeechTranscriptionConfigSchema)` to create a new message.
 */
export const SpeechTranscriptionConfigSchema: GenMessage<SpeechTranscriptionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 16);

/**
 * Provides "hints" to the speech recognizer to favor specific words and phrases
 * in the results.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.SpeechContext
 */
export type SpeechContext = Message<"google.cloud.videointelligence.v1p1beta1.SpeechContext"> & {
  /**
   * Optional. A list of strings containing words and phrases "hints" so that
   * the speech recognition is more likely to recognize them. This can be used
   * to improve the accuracy for specific words and phrases, for example, if
   * specific commands are typically spoken by the user. This can also be used
   * to add additional words to the vocabulary of the recognizer. See
   * [usage limits](https://cloud.google.com/speech/limits#content).
   *
   * @generated from field: repeated string phrases = 1;
   */
  phrases: string[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.SpeechContext.
 * Use `create(SpeechContextSchema)` to create a new message.
 */
export const SpeechContextSchema: GenMessage<SpeechContext> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 17);

/**
 * A speech recognition result corresponding to a portion of the audio.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.SpeechTranscription
 */
export type SpeechTranscription = Message<"google.cloud.videointelligence.v1p1beta1.SpeechTranscription"> & {
  /**
   * May contain one or more recognition hypotheses (up to the maximum specified
   * in `max_alternatives`).  These alternatives are ordered in terms of
   * accuracy, with the top (first) alternative being the most probable, as
   * ranked by the recognizer.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p1beta1.SpeechRecognitionAlternative alternatives = 1;
   */
  alternatives: SpeechRecognitionAlternative[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.SpeechTranscription.
 * Use `create(SpeechTranscriptionSchema)` to create a new message.
 */
export const SpeechTranscriptionSchema: GenMessage<SpeechTranscription> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 18);

/**
 * Alternative hypotheses (a.k.a. n-best list).
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.SpeechRecognitionAlternative
 */
export type SpeechRecognitionAlternative = Message<"google.cloud.videointelligence.v1p1beta1.SpeechRecognitionAlternative"> & {
  /**
   * Output only. Transcript text representing the words that the user spoke.
   *
   * @generated from field: string transcript = 1;
   */
  transcript: string;

  /**
   * Output only. The confidence estimate between 0.0 and 1.0. A higher number
   * indicates an estimated greater likelihood that the recognized words are
   * correct. This field is set only for the top alternative.
   * This field is not guaranteed to be accurate and users should not rely on it
   * to be always provided.
   * The default of 0.0 is a sentinel value indicating `confidence` was not set.
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;

  /**
   * Output only. A list of word-specific information for each recognized word.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p1beta1.WordInfo words = 3;
   */
  words: WordInfo[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.SpeechRecognitionAlternative.
 * Use `create(SpeechRecognitionAlternativeSchema)` to create a new message.
 */
export const SpeechRecognitionAlternativeSchema: GenMessage<SpeechRecognitionAlternative> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 19);

/**
 * Word-specific information for recognized words. Word information is only
 * included in the response when certain request parameters are set, such
 * as `enable_word_time_offsets`.
 *
 * @generated from message google.cloud.videointelligence.v1p1beta1.WordInfo
 */
export type WordInfo = Message<"google.cloud.videointelligence.v1p1beta1.WordInfo"> & {
  /**
   * Output only. Time offset relative to the beginning of the audio, and
   * corresponding to the start of the spoken word. This field is only set if
   * `enable_word_time_offsets=true` and only in the top hypothesis. This is an
   * experimental feature and the accuracy of the time offset can vary.
   *
   * @generated from field: google.protobuf.Duration start_time = 1;
   */
  startTime?: Duration;

  /**
   * Output only. Time offset relative to the beginning of the audio, and
   * corresponding to the end of the spoken word. This field is only set if
   * `enable_word_time_offsets=true` and only in the top hypothesis. This is an
   * experimental feature and the accuracy of the time offset can vary.
   *
   * @generated from field: google.protobuf.Duration end_time = 2;
   */
  endTime?: Duration;

  /**
   * Output only. The word corresponding to this set of information.
   *
   * @generated from field: string word = 3;
   */
  word: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p1beta1.WordInfo.
 * Use `create(WordInfoSchema)` to create a new message.
 */
export const WordInfoSchema: GenMessage<WordInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 20);

/**
 * Video annotation feature.
 *
 * @generated from enum google.cloud.videointelligence.v1p1beta1.Feature
 */
export enum Feature {
  /**
   * Unspecified.
   *
   * @generated from enum value: FEATURE_UNSPECIFIED = 0;
   */
  FEATURE_UNSPECIFIED = 0,

  /**
   * Label detection. Detect objects, such as dog or flower.
   *
   * @generated from enum value: LABEL_DETECTION = 1;
   */
  LABEL_DETECTION = 1,

  /**
   * Shot change detection.
   *
   * @generated from enum value: SHOT_CHANGE_DETECTION = 2;
   */
  SHOT_CHANGE_DETECTION = 2,

  /**
   * Explicit content detection.
   *
   * @generated from enum value: EXPLICIT_CONTENT_DETECTION = 3;
   */
  EXPLICIT_CONTENT_DETECTION = 3,

  /**
   * Speech transcription.
   *
   * @generated from enum value: SPEECH_TRANSCRIPTION = 6;
   */
  SPEECH_TRANSCRIPTION = 6,
}

/**
 * Describes the enum google.cloud.videointelligence.v1p1beta1.Feature.
 */
export const FeatureSchema: GenEnum<Feature> = /*@__PURE__*/
  enumDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 0);

/**
 * Label detection mode.
 *
 * @generated from enum google.cloud.videointelligence.v1p1beta1.LabelDetectionMode
 */
export enum LabelDetectionMode {
  /**
   * Unspecified.
   *
   * @generated from enum value: LABEL_DETECTION_MODE_UNSPECIFIED = 0;
   */
  LABEL_DETECTION_MODE_UNSPECIFIED = 0,

  /**
   * Detect shot-level labels.
   *
   * @generated from enum value: SHOT_MODE = 1;
   */
  SHOT_MODE = 1,

  /**
   * Detect frame-level labels.
   *
   * @generated from enum value: FRAME_MODE = 2;
   */
  FRAME_MODE = 2,

  /**
   * Detect both shot-level and frame-level labels.
   *
   * @generated from enum value: SHOT_AND_FRAME_MODE = 3;
   */
  SHOT_AND_FRAME_MODE = 3,
}

/**
 * Describes the enum google.cloud.videointelligence.v1p1beta1.LabelDetectionMode.
 */
export const LabelDetectionModeSchema: GenEnum<LabelDetectionMode> = /*@__PURE__*/
  enumDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 1);

/**
 * Bucketized representation of likelihood.
 *
 * @generated from enum google.cloud.videointelligence.v1p1beta1.Likelihood
 */
export enum Likelihood {
  /**
   * Unspecified likelihood.
   *
   * @generated from enum value: LIKELIHOOD_UNSPECIFIED = 0;
   */
  LIKELIHOOD_UNSPECIFIED = 0,

  /**
   * Very unlikely.
   *
   * @generated from enum value: VERY_UNLIKELY = 1;
   */
  VERY_UNLIKELY = 1,

  /**
   * Unlikely.
   *
   * @generated from enum value: UNLIKELY = 2;
   */
  UNLIKELY = 2,

  /**
   * Possible.
   *
   * @generated from enum value: POSSIBLE = 3;
   */
  POSSIBLE = 3,

  /**
   * Likely.
   *
   * @generated from enum value: LIKELY = 4;
   */
  LIKELY = 4,

  /**
   * Very likely.
   *
   * @generated from enum value: VERY_LIKELY = 5;
   */
  VERY_LIKELY = 5,
}

/**
 * Describes the enum google.cloud.videointelligence.v1p1beta1.Likelihood.
 */
export const LikelihoodSchema: GenEnum<Likelihood> = /*@__PURE__*/
  enumDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 2);

/**
 * Service that implements Google Cloud Video Intelligence API.
 *
 * @generated from service google.cloud.videointelligence.v1p1beta1.VideoIntelligenceService
 */
export const VideoIntelligenceService: GenService<{
  /**
   * Performs asynchronous video annotation. Progress and results can be
   * retrieved through the `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `AnnotateVideoProgress` (progress).
   * `Operation.response` contains `AnnotateVideoResponse` (results).
   *
   * @generated from rpc google.cloud.videointelligence.v1p1beta1.VideoIntelligenceService.AnnotateVideo
   */
  annotateVideo: {
    methodKind: "unary";
    input: typeof AnnotateVideoRequestSchema;
    output: typeof OperationSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_videointelligence_v1p1beta1_video_intelligence, 0);

