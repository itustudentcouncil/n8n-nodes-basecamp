// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/videointelligence/v1p2beta1/video_intelligence.proto (package google.cloud.videointelligence.v1p2beta1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import type { OperationSchema } from "../../../longrunning/operations_pb";
import { file_google_longrunning_operations } from "../../../longrunning/operations_pb";
import type { Duration, Timestamp } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_duration, file_google_protobuf_timestamp } from "@bufbuild/protobuf/wkt";
import type { Status } from "../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../rpc/status_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/videointelligence/v1p2beta1/video_intelligence.proto.
 */
export const file_google_cloud_videointelligence_v1p2beta1_video_intelligence: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_longrunning_operations, file_google_protobuf_duration, file_google_protobuf_timestamp, file_google_rpc_status]);

/**
 * Video annotation request.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.AnnotateVideoRequest
 */
export type AnnotateVideoRequest = Message<"google.cloud.videointelligence.v1p2beta1.AnnotateVideoRequest"> & {
  /**
   * Input video location. Currently, only
   * [Google Cloud Storage](https://cloud.google.com/storage/) URIs are
   * supported, which must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see
   * [Request URIs](https://cloud.google.com/storage/docs/request-endpoints).
   * A video URI may include wildcards in `object-id`, and thus identify
   * multiple videos. Supported wildcards: '*' to match 0 or more characters;
   * '?' to match 1 character. If unset, the input video should be embedded
   * in the request as `input_content`. If set, `input_content` should be unset.
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri: string;

  /**
   * The video data bytes.
   * If unset, the input video(s) should be specified via `input_uri`.
   * If set, `input_uri` should be unset.
   *
   * @generated from field: bytes input_content = 6;
   */
  inputContent: Uint8Array;

  /**
   * Required. Requested video annotation features.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.Feature features = 2;
   */
  features: Feature[];

  /**
   * Additional video context and/or feature-specific parameters.
   *
   * @generated from field: google.cloud.videointelligence.v1p2beta1.VideoContext video_context = 3;
   */
  videoContext?: VideoContext;

  /**
   * Optional. Location where the output (in JSON format) should be stored.
   * Currently, only [Google Cloud Storage](https://cloud.google.com/storage/)
   * URIs are supported, which must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see
   * [Request URIs](https://cloud.google.com/storage/docs/request-endpoints).
   *
   * @generated from field: string output_uri = 4;
   */
  outputUri: string;

  /**
   * Optional. Cloud region where annotation should take place. Supported cloud
   * regions: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no region
   * is specified, a region will be determined based on video file location.
   *
   * @generated from field: string location_id = 5;
   */
  locationId: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.AnnotateVideoRequest.
 * Use `create(AnnotateVideoRequestSchema)` to create a new message.
 */
export const AnnotateVideoRequestSchema: GenMessage<AnnotateVideoRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 0);

/**
 * Video context and/or feature-specific parameters.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.VideoContext
 */
export type VideoContext = Message<"google.cloud.videointelligence.v1p2beta1.VideoContext"> & {
  /**
   * Video segments to annotate. The segments may overlap and are not required
   * to be contiguous or span the whole video. If unspecified, each video is
   * treated as a single segment.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.VideoSegment segments = 1;
   */
  segments: VideoSegment[];

  /**
   * Config for LABEL_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1p2beta1.LabelDetectionConfig label_detection_config = 2;
   */
  labelDetectionConfig?: LabelDetectionConfig;

  /**
   * Config for SHOT_CHANGE_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1p2beta1.ShotChangeDetectionConfig shot_change_detection_config = 3;
   */
  shotChangeDetectionConfig?: ShotChangeDetectionConfig;

  /**
   * Config for EXPLICIT_CONTENT_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1p2beta1.ExplicitContentDetectionConfig explicit_content_detection_config = 4;
   */
  explicitContentDetectionConfig?: ExplicitContentDetectionConfig;

  /**
   * Config for TEXT_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1p2beta1.TextDetectionConfig text_detection_config = 8;
   */
  textDetectionConfig?: TextDetectionConfig;
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.VideoContext.
 * Use `create(VideoContextSchema)` to create a new message.
 */
export const VideoContextSchema: GenMessage<VideoContext> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 1);

/**
 * Config for LABEL_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.LabelDetectionConfig
 */
export type LabelDetectionConfig = Message<"google.cloud.videointelligence.v1p2beta1.LabelDetectionConfig"> & {
  /**
   * What labels should be detected with LABEL_DETECTION, in addition to
   * video-level labels or segment-level labels.
   * If unspecified, defaults to `SHOT_MODE`.
   *
   * @generated from field: google.cloud.videointelligence.v1p2beta1.LabelDetectionMode label_detection_mode = 1;
   */
  labelDetectionMode: LabelDetectionMode;

  /**
   * Whether the video has been shot from a stationary (i.e. non-moving) camera.
   * When set to true, might improve detection accuracy for moving objects.
   * Should be used with `SHOT_AND_FRAME_MODE` enabled.
   *
   * @generated from field: bool stationary_camera = 2;
   */
  stationaryCamera: boolean;

  /**
   * Model to use for label detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 3;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.LabelDetectionConfig.
 * Use `create(LabelDetectionConfigSchema)` to create a new message.
 */
export const LabelDetectionConfigSchema: GenMessage<LabelDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 2);

/**
 * Config for SHOT_CHANGE_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.ShotChangeDetectionConfig
 */
export type ShotChangeDetectionConfig = Message<"google.cloud.videointelligence.v1p2beta1.ShotChangeDetectionConfig"> & {
  /**
   * Model to use for shot change detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.ShotChangeDetectionConfig.
 * Use `create(ShotChangeDetectionConfigSchema)` to create a new message.
 */
export const ShotChangeDetectionConfigSchema: GenMessage<ShotChangeDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 3);

/**
 * Config for EXPLICIT_CONTENT_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.ExplicitContentDetectionConfig
 */
export type ExplicitContentDetectionConfig = Message<"google.cloud.videointelligence.v1p2beta1.ExplicitContentDetectionConfig"> & {
  /**
   * Model to use for explicit content detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.ExplicitContentDetectionConfig.
 * Use `create(ExplicitContentDetectionConfigSchema)` to create a new message.
 */
export const ExplicitContentDetectionConfigSchema: GenMessage<ExplicitContentDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 4);

/**
 * Config for TEXT_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.TextDetectionConfig
 */
export type TextDetectionConfig = Message<"google.cloud.videointelligence.v1p2beta1.TextDetectionConfig"> & {
  /**
   * Language hint can be specified if the language to be detected is known a
   * priori. It can increase the accuracy of the detection. Language hint must
   * be language code in BCP-47 format.
   *
   * Automatic language detection is performed if no hint is provided.
   *
   * @generated from field: repeated string language_hints = 1;
   */
  languageHints: string[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.TextDetectionConfig.
 * Use `create(TextDetectionConfigSchema)` to create a new message.
 */
export const TextDetectionConfigSchema: GenMessage<TextDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 5);

/**
 * Video segment.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.VideoSegment
 */
export type VideoSegment = Message<"google.cloud.videointelligence.v1p2beta1.VideoSegment"> & {
  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the start of the segment (inclusive).
   *
   * @generated from field: google.protobuf.Duration start_time_offset = 1;
   */
  startTimeOffset?: Duration;

  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the end of the segment (inclusive).
   *
   * @generated from field: google.protobuf.Duration end_time_offset = 2;
   */
  endTimeOffset?: Duration;
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.VideoSegment.
 * Use `create(VideoSegmentSchema)` to create a new message.
 */
export const VideoSegmentSchema: GenMessage<VideoSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 6);

/**
 * Video segment level annotation results for label detection.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.LabelSegment
 */
export type LabelSegment = Message<"google.cloud.videointelligence.v1p2beta1.LabelSegment"> & {
  /**
   * Video segment where a label was detected.
   *
   * @generated from field: google.cloud.videointelligence.v1p2beta1.VideoSegment segment = 1;
   */
  segment?: VideoSegment;

  /**
   * Confidence that the label is accurate. Range: [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.LabelSegment.
 * Use `create(LabelSegmentSchema)` to create a new message.
 */
export const LabelSegmentSchema: GenMessage<LabelSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 7);

/**
 * Video frame level annotation results for label detection.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.LabelFrame
 */
export type LabelFrame = Message<"google.cloud.videointelligence.v1p2beta1.LabelFrame"> & {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   *
   * @generated from field: google.protobuf.Duration time_offset = 1;
   */
  timeOffset?: Duration;

  /**
   * Confidence that the label is accurate. Range: [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.LabelFrame.
 * Use `create(LabelFrameSchema)` to create a new message.
 */
export const LabelFrameSchema: GenMessage<LabelFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 8);

/**
 * Detected entity from video analysis.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.Entity
 */
export type Entity = Message<"google.cloud.videointelligence.v1p2beta1.Entity"> & {
  /**
   * Opaque entity ID. Some IDs may be available in
   * [Google Knowledge Graph Search
   * API](https://developers.google.com/knowledge-graph/).
   *
   * @generated from field: string entity_id = 1;
   */
  entityId: string;

  /**
   * Textual description, e.g. `Fixed-gear bicycle`.
   *
   * @generated from field: string description = 2;
   */
  description: string;

  /**
   * Language code for `description` in BCP-47 format.
   *
   * @generated from field: string language_code = 3;
   */
  languageCode: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.Entity.
 * Use `create(EntitySchema)` to create a new message.
 */
export const EntitySchema: GenMessage<Entity> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 9);

/**
 * Label annotation.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.LabelAnnotation
 */
export type LabelAnnotation = Message<"google.cloud.videointelligence.v1p2beta1.LabelAnnotation"> & {
  /**
   * Detected entity.
   *
   * @generated from field: google.cloud.videointelligence.v1p2beta1.Entity entity = 1;
   */
  entity?: Entity;

  /**
   * Common categories for the detected entity.
   * E.g. when the label is `Terrier` the category is likely `dog`. And in some
   * cases there might be more than one categories e.g. `Terrier` could also be
   * a `pet`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.Entity category_entities = 2;
   */
  categoryEntities: Entity[];

  /**
   * All video segments where a label was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.LabelSegment segments = 3;
   */
  segments: LabelSegment[];

  /**
   * All video frames where a label was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.LabelFrame frames = 4;
   */
  frames: LabelFrame[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.LabelAnnotation.
 * Use `create(LabelAnnotationSchema)` to create a new message.
 */
export const LabelAnnotationSchema: GenMessage<LabelAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 10);

/**
 * Video frame level annotation results for explicit content.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.ExplicitContentFrame
 */
export type ExplicitContentFrame = Message<"google.cloud.videointelligence.v1p2beta1.ExplicitContentFrame"> & {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   *
   * @generated from field: google.protobuf.Duration time_offset = 1;
   */
  timeOffset?: Duration;

  /**
   * Likelihood of the pornography content..
   *
   * @generated from field: google.cloud.videointelligence.v1p2beta1.Likelihood pornography_likelihood = 2;
   */
  pornographyLikelihood: Likelihood;
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.ExplicitContentFrame.
 * Use `create(ExplicitContentFrameSchema)` to create a new message.
 */
export const ExplicitContentFrameSchema: GenMessage<ExplicitContentFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 11);

/**
 * Explicit content annotation (based on per-frame visual signals only).
 * If no explicit content has been detected in a frame, no annotations are
 * present for that frame.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.ExplicitContentAnnotation
 */
export type ExplicitContentAnnotation = Message<"google.cloud.videointelligence.v1p2beta1.ExplicitContentAnnotation"> & {
  /**
   * All video frames where explicit content was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.ExplicitContentFrame frames = 1;
   */
  frames: ExplicitContentFrame[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.ExplicitContentAnnotation.
 * Use `create(ExplicitContentAnnotationSchema)` to create a new message.
 */
export const ExplicitContentAnnotationSchema: GenMessage<ExplicitContentAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 12);

/**
 * Normalized bounding box.
 * The normalized vertex coordinates are relative to the original image.
 * Range: [0, 1].
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.NormalizedBoundingBox
 */
export type NormalizedBoundingBox = Message<"google.cloud.videointelligence.v1p2beta1.NormalizedBoundingBox"> & {
  /**
   * Left X coordinate.
   *
   * @generated from field: float left = 1;
   */
  left: number;

  /**
   * Top Y coordinate.
   *
   * @generated from field: float top = 2;
   */
  top: number;

  /**
   * Right X coordinate.
   *
   * @generated from field: float right = 3;
   */
  right: number;

  /**
   * Bottom Y coordinate.
   *
   * @generated from field: float bottom = 4;
   */
  bottom: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.NormalizedBoundingBox.
 * Use `create(NormalizedBoundingBoxSchema)` to create a new message.
 */
export const NormalizedBoundingBoxSchema: GenMessage<NormalizedBoundingBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 13);

/**
 * Annotation results for a single video.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.VideoAnnotationResults
 */
export type VideoAnnotationResults = Message<"google.cloud.videointelligence.v1p2beta1.VideoAnnotationResults"> & {
  /**
   * Video file location in
   * [Google Cloud Storage](https://cloud.google.com/storage/).
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri: string;

  /**
   * Label annotations on video level or user specified segment level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.LabelAnnotation segment_label_annotations = 2;
   */
  segmentLabelAnnotations: LabelAnnotation[];

  /**
   * Label annotations on shot level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.LabelAnnotation shot_label_annotations = 3;
   */
  shotLabelAnnotations: LabelAnnotation[];

  /**
   * Label annotations on frame level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.LabelAnnotation frame_label_annotations = 4;
   */
  frameLabelAnnotations: LabelAnnotation[];

  /**
   * Shot annotations. Each shot is represented as a video segment.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.VideoSegment shot_annotations = 6;
   */
  shotAnnotations: VideoSegment[];

  /**
   * Explicit content annotation.
   *
   * @generated from field: google.cloud.videointelligence.v1p2beta1.ExplicitContentAnnotation explicit_annotation = 7;
   */
  explicitAnnotation?: ExplicitContentAnnotation;

  /**
   * OCR text detection and tracking.
   * Annotations for list of detected text snippets. Each will have list of
   * frame information associated with it.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.TextAnnotation text_annotations = 12;
   */
  textAnnotations: TextAnnotation[];

  /**
   * Annotations for list of objects detected and tracked in video.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.ObjectTrackingAnnotation object_annotations = 14;
   */
  objectAnnotations: ObjectTrackingAnnotation[];

  /**
   * If set, indicates an error. Note that for a single `AnnotateVideoRequest`
   * some videos may succeed and some may fail.
   *
   * @generated from field: google.rpc.Status error = 9;
   */
  error?: Status;
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.VideoAnnotationResults.
 * Use `create(VideoAnnotationResultsSchema)` to create a new message.
 */
export const VideoAnnotationResultsSchema: GenMessage<VideoAnnotationResults> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 14);

/**
 * Video annotation response. Included in the `response`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.AnnotateVideoResponse
 */
export type AnnotateVideoResponse = Message<"google.cloud.videointelligence.v1p2beta1.AnnotateVideoResponse"> & {
  /**
   * Annotation results for all videos specified in `AnnotateVideoRequest`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.VideoAnnotationResults annotation_results = 1;
   */
  annotationResults: VideoAnnotationResults[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.AnnotateVideoResponse.
 * Use `create(AnnotateVideoResponseSchema)` to create a new message.
 */
export const AnnotateVideoResponseSchema: GenMessage<AnnotateVideoResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 15);

/**
 * Annotation progress for a single video.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.VideoAnnotationProgress
 */
export type VideoAnnotationProgress = Message<"google.cloud.videointelligence.v1p2beta1.VideoAnnotationProgress"> & {
  /**
   * Video file location in
   * [Google Cloud Storage](https://cloud.google.com/storage/).
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri: string;

  /**
   * Approximate percentage processed thus far. Guaranteed to be
   * 100 when fully processed.
   *
   * @generated from field: int32 progress_percent = 2;
   */
  progressPercent: number;

  /**
   * Time when the request was received.
   *
   * @generated from field: google.protobuf.Timestamp start_time = 3;
   */
  startTime?: Timestamp;

  /**
   * Time of the most recent update.
   *
   * @generated from field: google.protobuf.Timestamp update_time = 4;
   */
  updateTime?: Timestamp;
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.VideoAnnotationProgress.
 * Use `create(VideoAnnotationProgressSchema)` to create a new message.
 */
export const VideoAnnotationProgressSchema: GenMessage<VideoAnnotationProgress> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 16);

/**
 * Video annotation progress. Included in the `metadata`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.AnnotateVideoProgress
 */
export type AnnotateVideoProgress = Message<"google.cloud.videointelligence.v1p2beta1.AnnotateVideoProgress"> & {
  /**
   * Progress metadata for all videos specified in `AnnotateVideoRequest`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.VideoAnnotationProgress annotation_progress = 1;
   */
  annotationProgress: VideoAnnotationProgress[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.AnnotateVideoProgress.
 * Use `create(AnnotateVideoProgressSchema)` to create a new message.
 */
export const AnnotateVideoProgressSchema: GenMessage<AnnotateVideoProgress> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 17);

/**
 * A vertex represents a 2D point in the image.
 * NOTE: the normalized vertex coordinates are relative to the original image
 * and range from 0 to 1.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.NormalizedVertex
 */
export type NormalizedVertex = Message<"google.cloud.videointelligence.v1p2beta1.NormalizedVertex"> & {
  /**
   * X coordinate.
   *
   * @generated from field: float x = 1;
   */
  x: number;

  /**
   * Y coordinate.
   *
   * @generated from field: float y = 2;
   */
  y: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.NormalizedVertex.
 * Use `create(NormalizedVertexSchema)` to create a new message.
 */
export const NormalizedVertexSchema: GenMessage<NormalizedVertex> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 18);

/**
 * Normalized bounding polygon for text (that might not be aligned with axis).
 * Contains list of the corner points in clockwise order starting from
 * top-left corner. For example, for a rectangular bounding box:
 * When the text is horizontal it might look like:
 *         0----1
 *         |    |
 *         3----2
 *
 * When it's clockwise rotated 180 degrees around the top-left corner it
 * becomes:
 *         2----3
 *         |    |
 *         1----0
 *
 * and the vertex order will still be (0, 1, 2, 3). Note that values can be less
 * than 0, or greater than 1 due to trignometric calculations for location of
 * the box.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.NormalizedBoundingPoly
 */
export type NormalizedBoundingPoly = Message<"google.cloud.videointelligence.v1p2beta1.NormalizedBoundingPoly"> & {
  /**
   * Normalized vertices of the bounding polygon.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.NormalizedVertex vertices = 1;
   */
  vertices: NormalizedVertex[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.NormalizedBoundingPoly.
 * Use `create(NormalizedBoundingPolySchema)` to create a new message.
 */
export const NormalizedBoundingPolySchema: GenMessage<NormalizedBoundingPoly> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 19);

/**
 * Video segment level annotation results for text detection.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.TextSegment
 */
export type TextSegment = Message<"google.cloud.videointelligence.v1p2beta1.TextSegment"> & {
  /**
   * Video segment where a text snippet was detected.
   *
   * @generated from field: google.cloud.videointelligence.v1p2beta1.VideoSegment segment = 1;
   */
  segment?: VideoSegment;

  /**
   * Confidence for the track of detected text. It is calculated as the highest
   * over all frames where OCR detected text appears.
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;

  /**
   * Information related to the frames where OCR detected text appears.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.TextFrame frames = 3;
   */
  frames: TextFrame[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.TextSegment.
 * Use `create(TextSegmentSchema)` to create a new message.
 */
export const TextSegmentSchema: GenMessage<TextSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 20);

/**
 * Video frame level annotation results for text annotation (OCR).
 * Contains information regarding timestamp and bounding box locations for the
 * frames containing detected OCR text snippets.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.TextFrame
 */
export type TextFrame = Message<"google.cloud.videointelligence.v1p2beta1.TextFrame"> & {
  /**
   * Bounding polygon of the detected text for this frame.
   *
   * @generated from field: google.cloud.videointelligence.v1p2beta1.NormalizedBoundingPoly rotated_bounding_box = 1;
   */
  rotatedBoundingBox?: NormalizedBoundingPoly;

  /**
   * Timestamp of this frame.
   *
   * @generated from field: google.protobuf.Duration time_offset = 2;
   */
  timeOffset?: Duration;
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.TextFrame.
 * Use `create(TextFrameSchema)` to create a new message.
 */
export const TextFrameSchema: GenMessage<TextFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 21);

/**
 * Annotations related to one detected OCR text snippet. This will contain the
 * corresponding text, confidence value, and frame level information for each
 * detection.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.TextAnnotation
 */
export type TextAnnotation = Message<"google.cloud.videointelligence.v1p2beta1.TextAnnotation"> & {
  /**
   * The detected text.
   *
   * @generated from field: string text = 1;
   */
  text: string;

  /**
   * All video segments where OCR detected text appears.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.TextSegment segments = 2;
   */
  segments: TextSegment[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.TextAnnotation.
 * Use `create(TextAnnotationSchema)` to create a new message.
 */
export const TextAnnotationSchema: GenMessage<TextAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 22);

/**
 * Video frame level annotations for object detection and tracking. This field
 * stores per frame location, time offset, and confidence.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.ObjectTrackingFrame
 */
export type ObjectTrackingFrame = Message<"google.cloud.videointelligence.v1p2beta1.ObjectTrackingFrame"> & {
  /**
   * The normalized bounding box location of this object track for the frame.
   *
   * @generated from field: google.cloud.videointelligence.v1p2beta1.NormalizedBoundingBox normalized_bounding_box = 1;
   */
  normalizedBoundingBox?: NormalizedBoundingBox;

  /**
   * The timestamp of the frame in microseconds.
   *
   * @generated from field: google.protobuf.Duration time_offset = 2;
   */
  timeOffset?: Duration;
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.ObjectTrackingFrame.
 * Use `create(ObjectTrackingFrameSchema)` to create a new message.
 */
export const ObjectTrackingFrameSchema: GenMessage<ObjectTrackingFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 23);

/**
 * Annotations corresponding to one tracked object.
 *
 * @generated from message google.cloud.videointelligence.v1p2beta1.ObjectTrackingAnnotation
 */
export type ObjectTrackingAnnotation = Message<"google.cloud.videointelligence.v1p2beta1.ObjectTrackingAnnotation"> & {
  /**
   * Different representation of tracking info in non-streaming batch
   * and streaming modes.
   *
   * @generated from oneof google.cloud.videointelligence.v1p2beta1.ObjectTrackingAnnotation.track_info
   */
  trackInfo: {
    /**
     * Non-streaming batch mode ONLY.
     * Each object track corresponds to one video segment where it appears.
     *
     * @generated from field: google.cloud.videointelligence.v1p2beta1.VideoSegment segment = 3;
     */
    value: VideoSegment;
    case: "segment";
  } | {
    /**
     * Streaming mode ONLY.
     * In streaming mode, we do not know the end time of a tracked object
     * before it is completed. Hence, there is no VideoSegment info returned.
     * Instead, we provide a unique identifiable integer track_id so that
     * the customers can correlate the results of the ongoing
     * ObjectTrackAnnotation of the same track_id over time.
     *
     * @generated from field: int64 track_id = 5;
     */
    value: bigint;
    case: "trackId";
  } | { case: undefined; value?: undefined };

  /**
   * Entity to specify the object category that this track is labeled as.
   *
   * @generated from field: google.cloud.videointelligence.v1p2beta1.Entity entity = 1;
   */
  entity?: Entity;

  /**
   * Object category's labeling confidence of this track.
   *
   * @generated from field: float confidence = 4;
   */
  confidence: number;

  /**
   * Information corresponding to all frames where this object track appears.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p2beta1.ObjectTrackingFrame frames = 2;
   */
  frames: ObjectTrackingFrame[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p2beta1.ObjectTrackingAnnotation.
 * Use `create(ObjectTrackingAnnotationSchema)` to create a new message.
 */
export const ObjectTrackingAnnotationSchema: GenMessage<ObjectTrackingAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 24);

/**
 * Video annotation feature.
 *
 * @generated from enum google.cloud.videointelligence.v1p2beta1.Feature
 */
export enum Feature {
  /**
   * Unspecified.
   *
   * @generated from enum value: FEATURE_UNSPECIFIED = 0;
   */
  FEATURE_UNSPECIFIED = 0,

  /**
   * Label detection. Detect objects, such as dog or flower.
   *
   * @generated from enum value: LABEL_DETECTION = 1;
   */
  LABEL_DETECTION = 1,

  /**
   * Shot change detection.
   *
   * @generated from enum value: SHOT_CHANGE_DETECTION = 2;
   */
  SHOT_CHANGE_DETECTION = 2,

  /**
   * Explicit content detection.
   *
   * @generated from enum value: EXPLICIT_CONTENT_DETECTION = 3;
   */
  EXPLICIT_CONTENT_DETECTION = 3,

  /**
   * OCR text detection and tracking.
   *
   * @generated from enum value: TEXT_DETECTION = 7;
   */
  TEXT_DETECTION = 7,

  /**
   * Object detection and tracking.
   *
   * @generated from enum value: OBJECT_TRACKING = 9;
   */
  OBJECT_TRACKING = 9,
}

/**
 * Describes the enum google.cloud.videointelligence.v1p2beta1.Feature.
 */
export const FeatureSchema: GenEnum<Feature> = /*@__PURE__*/
  enumDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 0);

/**
 * Label detection mode.
 *
 * @generated from enum google.cloud.videointelligence.v1p2beta1.LabelDetectionMode
 */
export enum LabelDetectionMode {
  /**
   * Unspecified.
   *
   * @generated from enum value: LABEL_DETECTION_MODE_UNSPECIFIED = 0;
   */
  LABEL_DETECTION_MODE_UNSPECIFIED = 0,

  /**
   * Detect shot-level labels.
   *
   * @generated from enum value: SHOT_MODE = 1;
   */
  SHOT_MODE = 1,

  /**
   * Detect frame-level labels.
   *
   * @generated from enum value: FRAME_MODE = 2;
   */
  FRAME_MODE = 2,

  /**
   * Detect both shot-level and frame-level labels.
   *
   * @generated from enum value: SHOT_AND_FRAME_MODE = 3;
   */
  SHOT_AND_FRAME_MODE = 3,
}

/**
 * Describes the enum google.cloud.videointelligence.v1p2beta1.LabelDetectionMode.
 */
export const LabelDetectionModeSchema: GenEnum<LabelDetectionMode> = /*@__PURE__*/
  enumDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 1);

/**
 * Bucketized representation of likelihood.
 *
 * @generated from enum google.cloud.videointelligence.v1p2beta1.Likelihood
 */
export enum Likelihood {
  /**
   * Unspecified likelihood.
   *
   * @generated from enum value: LIKELIHOOD_UNSPECIFIED = 0;
   */
  LIKELIHOOD_UNSPECIFIED = 0,

  /**
   * Very unlikely.
   *
   * @generated from enum value: VERY_UNLIKELY = 1;
   */
  VERY_UNLIKELY = 1,

  /**
   * Unlikely.
   *
   * @generated from enum value: UNLIKELY = 2;
   */
  UNLIKELY = 2,

  /**
   * Possible.
   *
   * @generated from enum value: POSSIBLE = 3;
   */
  POSSIBLE = 3,

  /**
   * Likely.
   *
   * @generated from enum value: LIKELY = 4;
   */
  LIKELY = 4,

  /**
   * Very likely.
   *
   * @generated from enum value: VERY_LIKELY = 5;
   */
  VERY_LIKELY = 5,
}

/**
 * Describes the enum google.cloud.videointelligence.v1p2beta1.Likelihood.
 */
export const LikelihoodSchema: GenEnum<Likelihood> = /*@__PURE__*/
  enumDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 2);

/**
 * Service that implements Google Cloud Video Intelligence API.
 *
 * @generated from service google.cloud.videointelligence.v1p2beta1.VideoIntelligenceService
 */
export const VideoIntelligenceService: GenService<{
  /**
   * Performs asynchronous video annotation. Progress and results can be
   * retrieved through the `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `AnnotateVideoProgress` (progress).
   * `Operation.response` contains `AnnotateVideoResponse` (results).
   *
   * @generated from rpc google.cloud.videointelligence.v1p2beta1.VideoIntelligenceService.AnnotateVideo
   */
  annotateVideo: {
    methodKind: "unary";
    input: typeof AnnotateVideoRequestSchema;
    output: typeof OperationSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_videointelligence_v1p2beta1_video_intelligence, 0);

