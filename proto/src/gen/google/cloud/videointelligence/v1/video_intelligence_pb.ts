// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/videointelligence/v1/video_intelligence.proto (package google.cloud.videointelligence.v1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import type { OperationSchema } from "../../../longrunning/operations_pb";
import { file_google_longrunning_operations } from "../../../longrunning/operations_pb";
import type { Duration, Timestamp } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_duration, file_google_protobuf_timestamp } from "@bufbuild/protobuf/wkt";
import type { Status } from "../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../rpc/status_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/videointelligence/v1/video_intelligence.proto.
 */
export const file_google_cloud_videointelligence_v1_video_intelligence: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_longrunning_operations, file_google_protobuf_duration, file_google_protobuf_timestamp, file_google_rpc_status]);

/**
 * Video annotation request.
 *
 * @generated from message google.cloud.videointelligence.v1.AnnotateVideoRequest
 */
export type AnnotateVideoRequest = Message<"google.cloud.videointelligence.v1.AnnotateVideoRequest"> & {
  /**
   * Input video location. Currently, only
   * [Cloud Storage](https://cloud.google.com/storage/) URIs are
   * supported. URIs must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
   * more information, see [Request
   * URIs](https://cloud.google.com/storage/docs/request-endpoints). To identify
   * multiple videos, a video URI may include wildcards in the `object-id`.
   * Supported wildcards: '*' to match 0 or more characters;
   * '?' to match 1 character. If unset, the input video should be embedded
   * in the request as `input_content`. If set, `input_content` must be unset.
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri: string;

  /**
   * The video data bytes.
   * If unset, the input video(s) should be specified via the `input_uri`.
   * If set, `input_uri` must be unset.
   *
   * @generated from field: bytes input_content = 6;
   */
  inputContent: Uint8Array;

  /**
   * Required. Requested video annotation features.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.Feature features = 2;
   */
  features: Feature[];

  /**
   * Additional video context and/or feature-specific parameters.
   *
   * @generated from field: google.cloud.videointelligence.v1.VideoContext video_context = 3;
   */
  videoContext?: VideoContext;

  /**
   * Optional. Location where the output (in JSON format) should be stored.
   * Currently, only [Cloud Storage](https://cloud.google.com/storage/)
   * URIs are supported. These must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
   * more information, see [Request
   * URIs](https://cloud.google.com/storage/docs/request-endpoints).
   *
   * @generated from field: string output_uri = 4;
   */
  outputUri: string;

  /**
   * Optional. Cloud region where annotation should take place. Supported cloud
   * regions are: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no
   * region is specified, the region will be determined based on video file
   * location.
   *
   * @generated from field: string location_id = 5;
   */
  locationId: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1.AnnotateVideoRequest.
 * Use `create(AnnotateVideoRequestSchema)` to create a new message.
 */
export const AnnotateVideoRequestSchema: GenMessage<AnnotateVideoRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 0);

/**
 * Video context and/or feature-specific parameters.
 *
 * @generated from message google.cloud.videointelligence.v1.VideoContext
 */
export type VideoContext = Message<"google.cloud.videointelligence.v1.VideoContext"> & {
  /**
   * Video segments to annotate. The segments may overlap and are not required
   * to be contiguous or span the whole video. If unspecified, each video is
   * treated as a single segment.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.VideoSegment segments = 1;
   */
  segments: VideoSegment[];

  /**
   * Config for LABEL_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1.LabelDetectionConfig label_detection_config = 2;
   */
  labelDetectionConfig?: LabelDetectionConfig;

  /**
   * Config for SHOT_CHANGE_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1.ShotChangeDetectionConfig shot_change_detection_config = 3;
   */
  shotChangeDetectionConfig?: ShotChangeDetectionConfig;

  /**
   * Config for EXPLICIT_CONTENT_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1.ExplicitContentDetectionConfig explicit_content_detection_config = 4;
   */
  explicitContentDetectionConfig?: ExplicitContentDetectionConfig;

  /**
   * Config for FACE_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1.FaceDetectionConfig face_detection_config = 5;
   */
  faceDetectionConfig?: FaceDetectionConfig;

  /**
   * Config for SPEECH_TRANSCRIPTION.
   *
   * @generated from field: google.cloud.videointelligence.v1.SpeechTranscriptionConfig speech_transcription_config = 6;
   */
  speechTranscriptionConfig?: SpeechTranscriptionConfig;

  /**
   * Config for TEXT_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1.TextDetectionConfig text_detection_config = 8;
   */
  textDetectionConfig?: TextDetectionConfig;

  /**
   * Config for PERSON_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1.PersonDetectionConfig person_detection_config = 11;
   */
  personDetectionConfig?: PersonDetectionConfig;

  /**
   * Config for OBJECT_TRACKING.
   *
   * @generated from field: google.cloud.videointelligence.v1.ObjectTrackingConfig object_tracking_config = 13;
   */
  objectTrackingConfig?: ObjectTrackingConfig;
};

/**
 * Describes the message google.cloud.videointelligence.v1.VideoContext.
 * Use `create(VideoContextSchema)` to create a new message.
 */
export const VideoContextSchema: GenMessage<VideoContext> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 1);

/**
 * Config for LABEL_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1.LabelDetectionConfig
 */
export type LabelDetectionConfig = Message<"google.cloud.videointelligence.v1.LabelDetectionConfig"> & {
  /**
   * What labels should be detected with LABEL_DETECTION, in addition to
   * video-level labels or segment-level labels.
   * If unspecified, defaults to `SHOT_MODE`.
   *
   * @generated from field: google.cloud.videointelligence.v1.LabelDetectionMode label_detection_mode = 1;
   */
  labelDetectionMode: LabelDetectionMode;

  /**
   * Whether the video has been shot from a stationary (i.e., non-moving)
   * camera. When set to true, might improve detection accuracy for moving
   * objects. Should be used with `SHOT_AND_FRAME_MODE` enabled.
   *
   * @generated from field: bool stationary_camera = 2;
   */
  stationaryCamera: boolean;

  /**
   * Model to use for label detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 3;
   */
  model: string;

  /**
   * The confidence threshold we perform filtering on the labels from
   * frame-level detection. If not set, it is set to 0.4 by default. The valid
   * range for this threshold is [0.1, 0.9]. Any value set outside of this
   * range will be clipped.
   * Note: For best results, follow the default threshold. We will update
   * the default threshold everytime when we release a new model.
   *
   * @generated from field: float frame_confidence_threshold = 4;
   */
  frameConfidenceThreshold: number;

  /**
   * The confidence threshold we perform filtering on the labels from
   * video-level and shot-level detections. If not set, it's set to 0.3 by
   * default. The valid range for this threshold is [0.1, 0.9]. Any value set
   * outside of this range will be clipped.
   * Note: For best results, follow the default threshold. We will update
   * the default threshold everytime when we release a new model.
   *
   * @generated from field: float video_confidence_threshold = 5;
   */
  videoConfidenceThreshold: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1.LabelDetectionConfig.
 * Use `create(LabelDetectionConfigSchema)` to create a new message.
 */
export const LabelDetectionConfigSchema: GenMessage<LabelDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 2);

/**
 * Config for SHOT_CHANGE_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1.ShotChangeDetectionConfig
 */
export type ShotChangeDetectionConfig = Message<"google.cloud.videointelligence.v1.ShotChangeDetectionConfig"> & {
  /**
   * Model to use for shot change detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1.ShotChangeDetectionConfig.
 * Use `create(ShotChangeDetectionConfigSchema)` to create a new message.
 */
export const ShotChangeDetectionConfigSchema: GenMessage<ShotChangeDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 3);

/**
 * Config for OBJECT_TRACKING.
 *
 * @generated from message google.cloud.videointelligence.v1.ObjectTrackingConfig
 */
export type ObjectTrackingConfig = Message<"google.cloud.videointelligence.v1.ObjectTrackingConfig"> & {
  /**
   * Model to use for object tracking.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1.ObjectTrackingConfig.
 * Use `create(ObjectTrackingConfigSchema)` to create a new message.
 */
export const ObjectTrackingConfigSchema: GenMessage<ObjectTrackingConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 4);

/**
 * Config for FACE_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1.FaceDetectionConfig
 */
export type FaceDetectionConfig = Message<"google.cloud.videointelligence.v1.FaceDetectionConfig"> & {
  /**
   * Model to use for face detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Whether bounding boxes are included in the face annotation output.
   *
   * @generated from field: bool include_bounding_boxes = 2;
   */
  includeBoundingBoxes: boolean;

  /**
   * Whether to enable face attributes detection, such as glasses, dark_glasses,
   * mouth_open etc. Ignored if 'include_bounding_boxes' is set to false.
   *
   * @generated from field: bool include_attributes = 5;
   */
  includeAttributes: boolean;
};

/**
 * Describes the message google.cloud.videointelligence.v1.FaceDetectionConfig.
 * Use `create(FaceDetectionConfigSchema)` to create a new message.
 */
export const FaceDetectionConfigSchema: GenMessage<FaceDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 5);

/**
 * Config for PERSON_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1.PersonDetectionConfig
 */
export type PersonDetectionConfig = Message<"google.cloud.videointelligence.v1.PersonDetectionConfig"> & {
  /**
   * Whether bounding boxes are included in the person detection annotation
   * output.
   *
   * @generated from field: bool include_bounding_boxes = 1;
   */
  includeBoundingBoxes: boolean;

  /**
   * Whether to enable pose landmarks detection. Ignored if
   * 'include_bounding_boxes' is set to false.
   *
   * @generated from field: bool include_pose_landmarks = 2;
   */
  includePoseLandmarks: boolean;

  /**
   * Whether to enable person attributes detection, such as cloth color (black,
   * blue, etc), type (coat, dress, etc), pattern (plain, floral, etc), hair,
   * etc.
   * Ignored if 'include_bounding_boxes' is set to false.
   *
   * @generated from field: bool include_attributes = 3;
   */
  includeAttributes: boolean;
};

/**
 * Describes the message google.cloud.videointelligence.v1.PersonDetectionConfig.
 * Use `create(PersonDetectionConfigSchema)` to create a new message.
 */
export const PersonDetectionConfigSchema: GenMessage<PersonDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 6);

/**
 * Config for EXPLICIT_CONTENT_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1.ExplicitContentDetectionConfig
 */
export type ExplicitContentDetectionConfig = Message<"google.cloud.videointelligence.v1.ExplicitContentDetectionConfig"> & {
  /**
   * Model to use for explicit content detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1.ExplicitContentDetectionConfig.
 * Use `create(ExplicitContentDetectionConfigSchema)` to create a new message.
 */
export const ExplicitContentDetectionConfigSchema: GenMessage<ExplicitContentDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 7);

/**
 * Config for TEXT_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1.TextDetectionConfig
 */
export type TextDetectionConfig = Message<"google.cloud.videointelligence.v1.TextDetectionConfig"> & {
  /**
   * Language hint can be specified if the language to be detected is known a
   * priori. It can increase the accuracy of the detection. Language hint must
   * be language code in BCP-47 format.
   *
   * Automatic language detection is performed if no hint is provided.
   *
   * @generated from field: repeated string language_hints = 1;
   */
  languageHints: string[];

  /**
   * Model to use for text detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 2;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1.TextDetectionConfig.
 * Use `create(TextDetectionConfigSchema)` to create a new message.
 */
export const TextDetectionConfigSchema: GenMessage<TextDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 8);

/**
 * Video segment.
 *
 * @generated from message google.cloud.videointelligence.v1.VideoSegment
 */
export type VideoSegment = Message<"google.cloud.videointelligence.v1.VideoSegment"> & {
  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the start of the segment (inclusive).
   *
   * @generated from field: google.protobuf.Duration start_time_offset = 1;
   */
  startTimeOffset?: Duration;

  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the end of the segment (inclusive).
   *
   * @generated from field: google.protobuf.Duration end_time_offset = 2;
   */
  endTimeOffset?: Duration;
};

/**
 * Describes the message google.cloud.videointelligence.v1.VideoSegment.
 * Use `create(VideoSegmentSchema)` to create a new message.
 */
export const VideoSegmentSchema: GenMessage<VideoSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 9);

/**
 * Video segment level annotation results for label detection.
 *
 * @generated from message google.cloud.videointelligence.v1.LabelSegment
 */
export type LabelSegment = Message<"google.cloud.videointelligence.v1.LabelSegment"> & {
  /**
   * Video segment where a label was detected.
   *
   * @generated from field: google.cloud.videointelligence.v1.VideoSegment segment = 1;
   */
  segment?: VideoSegment;

  /**
   * Confidence that the label is accurate. Range: [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1.LabelSegment.
 * Use `create(LabelSegmentSchema)` to create a new message.
 */
export const LabelSegmentSchema: GenMessage<LabelSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 10);

/**
 * Video frame level annotation results for label detection.
 *
 * @generated from message google.cloud.videointelligence.v1.LabelFrame
 */
export type LabelFrame = Message<"google.cloud.videointelligence.v1.LabelFrame"> & {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   *
   * @generated from field: google.protobuf.Duration time_offset = 1;
   */
  timeOffset?: Duration;

  /**
   * Confidence that the label is accurate. Range: [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1.LabelFrame.
 * Use `create(LabelFrameSchema)` to create a new message.
 */
export const LabelFrameSchema: GenMessage<LabelFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 11);

/**
 * Detected entity from video analysis.
 *
 * @generated from message google.cloud.videointelligence.v1.Entity
 */
export type Entity = Message<"google.cloud.videointelligence.v1.Entity"> & {
  /**
   * Opaque entity ID. Some IDs may be available in
   * [Google Knowledge Graph Search
   * API](https://developers.google.com/knowledge-graph/).
   *
   * @generated from field: string entity_id = 1;
   */
  entityId: string;

  /**
   * Textual description, e.g., `Fixed-gear bicycle`.
   *
   * @generated from field: string description = 2;
   */
  description: string;

  /**
   * Language code for `description` in BCP-47 format.
   *
   * @generated from field: string language_code = 3;
   */
  languageCode: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1.Entity.
 * Use `create(EntitySchema)` to create a new message.
 */
export const EntitySchema: GenMessage<Entity> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 12);

/**
 * Label annotation.
 *
 * @generated from message google.cloud.videointelligence.v1.LabelAnnotation
 */
export type LabelAnnotation = Message<"google.cloud.videointelligence.v1.LabelAnnotation"> & {
  /**
   * Detected entity.
   *
   * @generated from field: google.cloud.videointelligence.v1.Entity entity = 1;
   */
  entity?: Entity;

  /**
   * Common categories for the detected entity.
   * For example, when the label is `Terrier`, the category is likely `dog`. And
   * in some cases there might be more than one categories e.g., `Terrier` could
   * also be a `pet`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.Entity category_entities = 2;
   */
  categoryEntities: Entity[];

  /**
   * All video segments where a label was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.LabelSegment segments = 3;
   */
  segments: LabelSegment[];

  /**
   * All video frames where a label was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.LabelFrame frames = 4;
   */
  frames: LabelFrame[];

  /**
   * Feature version.
   *
   * @generated from field: string version = 5;
   */
  version: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1.LabelAnnotation.
 * Use `create(LabelAnnotationSchema)` to create a new message.
 */
export const LabelAnnotationSchema: GenMessage<LabelAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 13);

/**
 * Video frame level annotation results for explicit content.
 *
 * @generated from message google.cloud.videointelligence.v1.ExplicitContentFrame
 */
export type ExplicitContentFrame = Message<"google.cloud.videointelligence.v1.ExplicitContentFrame"> & {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   *
   * @generated from field: google.protobuf.Duration time_offset = 1;
   */
  timeOffset?: Duration;

  /**
   * Likelihood of the pornography content..
   *
   * @generated from field: google.cloud.videointelligence.v1.Likelihood pornography_likelihood = 2;
   */
  pornographyLikelihood: Likelihood;
};

/**
 * Describes the message google.cloud.videointelligence.v1.ExplicitContentFrame.
 * Use `create(ExplicitContentFrameSchema)` to create a new message.
 */
export const ExplicitContentFrameSchema: GenMessage<ExplicitContentFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 14);

/**
 * Explicit content annotation (based on per-frame visual signals only).
 * If no explicit content has been detected in a frame, no annotations are
 * present for that frame.
 *
 * @generated from message google.cloud.videointelligence.v1.ExplicitContentAnnotation
 */
export type ExplicitContentAnnotation = Message<"google.cloud.videointelligence.v1.ExplicitContentAnnotation"> & {
  /**
   * All video frames where explicit content was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.ExplicitContentFrame frames = 1;
   */
  frames: ExplicitContentFrame[];

  /**
   * Feature version.
   *
   * @generated from field: string version = 2;
   */
  version: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1.ExplicitContentAnnotation.
 * Use `create(ExplicitContentAnnotationSchema)` to create a new message.
 */
export const ExplicitContentAnnotationSchema: GenMessage<ExplicitContentAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 15);

/**
 * Normalized bounding box.
 * The normalized vertex coordinates are relative to the original image.
 * Range: [0, 1].
 *
 * @generated from message google.cloud.videointelligence.v1.NormalizedBoundingBox
 */
export type NormalizedBoundingBox = Message<"google.cloud.videointelligence.v1.NormalizedBoundingBox"> & {
  /**
   * Left X coordinate.
   *
   * @generated from field: float left = 1;
   */
  left: number;

  /**
   * Top Y coordinate.
   *
   * @generated from field: float top = 2;
   */
  top: number;

  /**
   * Right X coordinate.
   *
   * @generated from field: float right = 3;
   */
  right: number;

  /**
   * Bottom Y coordinate.
   *
   * @generated from field: float bottom = 4;
   */
  bottom: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1.NormalizedBoundingBox.
 * Use `create(NormalizedBoundingBoxSchema)` to create a new message.
 */
export const NormalizedBoundingBoxSchema: GenMessage<NormalizedBoundingBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 16);

/**
 * Face detection annotation.
 *
 * @generated from message google.cloud.videointelligence.v1.FaceDetectionAnnotation
 */
export type FaceDetectionAnnotation = Message<"google.cloud.videointelligence.v1.FaceDetectionAnnotation"> & {
  /**
   * The face tracks with attributes.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.Track tracks = 3;
   */
  tracks: Track[];

  /**
   * The thumbnail of a person's face.
   *
   * @generated from field: bytes thumbnail = 4;
   */
  thumbnail: Uint8Array;

  /**
   * Feature version.
   *
   * @generated from field: string version = 5;
   */
  version: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1.FaceDetectionAnnotation.
 * Use `create(FaceDetectionAnnotationSchema)` to create a new message.
 */
export const FaceDetectionAnnotationSchema: GenMessage<FaceDetectionAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 17);

/**
 * Person detection annotation per video.
 *
 * @generated from message google.cloud.videointelligence.v1.PersonDetectionAnnotation
 */
export type PersonDetectionAnnotation = Message<"google.cloud.videointelligence.v1.PersonDetectionAnnotation"> & {
  /**
   * The detected tracks of a person.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.Track tracks = 1;
   */
  tracks: Track[];

  /**
   * Feature version.
   *
   * @generated from field: string version = 2;
   */
  version: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1.PersonDetectionAnnotation.
 * Use `create(PersonDetectionAnnotationSchema)` to create a new message.
 */
export const PersonDetectionAnnotationSchema: GenMessage<PersonDetectionAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 18);

/**
 * Video segment level annotation results for face detection.
 *
 * @generated from message google.cloud.videointelligence.v1.FaceSegment
 */
export type FaceSegment = Message<"google.cloud.videointelligence.v1.FaceSegment"> & {
  /**
   * Video segment where a face was detected.
   *
   * @generated from field: google.cloud.videointelligence.v1.VideoSegment segment = 1;
   */
  segment?: VideoSegment;
};

/**
 * Describes the message google.cloud.videointelligence.v1.FaceSegment.
 * Use `create(FaceSegmentSchema)` to create a new message.
 */
export const FaceSegmentSchema: GenMessage<FaceSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 19);

/**
 * Deprecated. No effect.
 *
 * @generated from message google.cloud.videointelligence.v1.FaceFrame
 * @deprecated
 */
export type FaceFrame = Message<"google.cloud.videointelligence.v1.FaceFrame"> & {
  /**
   * Normalized Bounding boxes in a frame.
   * There can be more than one boxes if the same face is detected in multiple
   * locations within the current frame.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.NormalizedBoundingBox normalized_bounding_boxes = 1;
   */
  normalizedBoundingBoxes: NormalizedBoundingBox[];

  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the video frame for this location.
   *
   * @generated from field: google.protobuf.Duration time_offset = 2;
   */
  timeOffset?: Duration;
};

/**
 * Describes the message google.cloud.videointelligence.v1.FaceFrame.
 * Use `create(FaceFrameSchema)` to create a new message.
 * @deprecated
 */
export const FaceFrameSchema: GenMessage<FaceFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 20);

/**
 * Deprecated. No effect.
 *
 * @generated from message google.cloud.videointelligence.v1.FaceAnnotation
 * @deprecated
 */
export type FaceAnnotation = Message<"google.cloud.videointelligence.v1.FaceAnnotation"> & {
  /**
   * Thumbnail of a representative face view (in JPEG format).
   *
   * @generated from field: bytes thumbnail = 1;
   */
  thumbnail: Uint8Array;

  /**
   * All video segments where a face was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.FaceSegment segments = 2;
   */
  segments: FaceSegment[];

  /**
   * All video frames where a face was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.FaceFrame frames = 3;
   */
  frames: FaceFrame[];
};

/**
 * Describes the message google.cloud.videointelligence.v1.FaceAnnotation.
 * Use `create(FaceAnnotationSchema)` to create a new message.
 * @deprecated
 */
export const FaceAnnotationSchema: GenMessage<FaceAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 21);

/**
 * For tracking related features.
 * An object at time_offset with attributes, and located with
 * normalized_bounding_box.
 *
 * @generated from message google.cloud.videointelligence.v1.TimestampedObject
 */
export type TimestampedObject = Message<"google.cloud.videointelligence.v1.TimestampedObject"> & {
  /**
   * Normalized Bounding box in a frame, where the object is located.
   *
   * @generated from field: google.cloud.videointelligence.v1.NormalizedBoundingBox normalized_bounding_box = 1;
   */
  normalizedBoundingBox?: NormalizedBoundingBox;

  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the video frame for this object.
   *
   * @generated from field: google.protobuf.Duration time_offset = 2;
   */
  timeOffset?: Duration;

  /**
   * Optional. The attributes of the object in the bounding box.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.DetectedAttribute attributes = 3;
   */
  attributes: DetectedAttribute[];

  /**
   * Optional. The detected landmarks.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.DetectedLandmark landmarks = 4;
   */
  landmarks: DetectedLandmark[];
};

/**
 * Describes the message google.cloud.videointelligence.v1.TimestampedObject.
 * Use `create(TimestampedObjectSchema)` to create a new message.
 */
export const TimestampedObjectSchema: GenMessage<TimestampedObject> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 22);

/**
 * A track of an object instance.
 *
 * @generated from message google.cloud.videointelligence.v1.Track
 */
export type Track = Message<"google.cloud.videointelligence.v1.Track"> & {
  /**
   * Video segment of a track.
   *
   * @generated from field: google.cloud.videointelligence.v1.VideoSegment segment = 1;
   */
  segment?: VideoSegment;

  /**
   * The object with timestamp and attributes per frame in the track.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.TimestampedObject timestamped_objects = 2;
   */
  timestampedObjects: TimestampedObject[];

  /**
   * Optional. Attributes in the track level.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.DetectedAttribute attributes = 3;
   */
  attributes: DetectedAttribute[];

  /**
   * Optional. The confidence score of the tracked object.
   *
   * @generated from field: float confidence = 4;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1.Track.
 * Use `create(TrackSchema)` to create a new message.
 */
export const TrackSchema: GenMessage<Track> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 23);

/**
 * A generic detected attribute represented by name in string format.
 *
 * @generated from message google.cloud.videointelligence.v1.DetectedAttribute
 */
export type DetectedAttribute = Message<"google.cloud.videointelligence.v1.DetectedAttribute"> & {
  /**
   * The name of the attribute, for example, glasses, dark_glasses, mouth_open.
   * A full list of supported type names will be provided in the document.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Detected attribute confidence. Range [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;

  /**
   * Text value of the detection result. For example, the value for "HairColor"
   * can be "black", "blonde", etc.
   *
   * @generated from field: string value = 3;
   */
  value: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1.DetectedAttribute.
 * Use `create(DetectedAttributeSchema)` to create a new message.
 */
export const DetectedAttributeSchema: GenMessage<DetectedAttribute> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 24);

/**
 * A generic detected landmark represented by name in string format and a 2D
 * location.
 *
 * @generated from message google.cloud.videointelligence.v1.DetectedLandmark
 */
export type DetectedLandmark = Message<"google.cloud.videointelligence.v1.DetectedLandmark"> & {
  /**
   * The name of this landmark, for example, left_hand, right_shoulder.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * The 2D point of the detected landmark using the normalized image
   * coordindate system. The normalized coordinates have the range from 0 to 1.
   *
   * @generated from field: google.cloud.videointelligence.v1.NormalizedVertex point = 2;
   */
  point?: NormalizedVertex;

  /**
   * The confidence score of the detected landmark. Range [0, 1].
   *
   * @generated from field: float confidence = 3;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1.DetectedLandmark.
 * Use `create(DetectedLandmarkSchema)` to create a new message.
 */
export const DetectedLandmarkSchema: GenMessage<DetectedLandmark> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 25);

/**
 * Annotation results for a single video.
 *
 * @generated from message google.cloud.videointelligence.v1.VideoAnnotationResults
 */
export type VideoAnnotationResults = Message<"google.cloud.videointelligence.v1.VideoAnnotationResults"> & {
  /**
   * Video file location in
   * [Cloud Storage](https://cloud.google.com/storage/).
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri: string;

  /**
   * Video segment on which the annotation is run.
   *
   * @generated from field: google.cloud.videointelligence.v1.VideoSegment segment = 10;
   */
  segment?: VideoSegment;

  /**
   * Topical label annotations on video level or user-specified segment level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.LabelAnnotation segment_label_annotations = 2;
   */
  segmentLabelAnnotations: LabelAnnotation[];

  /**
   * Presence label annotations on video level or user-specified segment level.
   * There is exactly one element for each unique label. Compared to the
   * existing topical `segment_label_annotations`, this field presents more
   * fine-grained, segment-level labels detected in video content and is made
   * available only when the client sets `LabelDetectionConfig.model` to
   * "builtin/latest" in the request.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.LabelAnnotation segment_presence_label_annotations = 23;
   */
  segmentPresenceLabelAnnotations: LabelAnnotation[];

  /**
   * Topical label annotations on shot level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.LabelAnnotation shot_label_annotations = 3;
   */
  shotLabelAnnotations: LabelAnnotation[];

  /**
   * Presence label annotations on shot level. There is exactly one element for
   * each unique label. Compared to the existing topical
   * `shot_label_annotations`, this field presents more fine-grained, shot-level
   * labels detected in video content and is made available only when the client
   * sets `LabelDetectionConfig.model` to "builtin/latest" in the request.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.LabelAnnotation shot_presence_label_annotations = 24;
   */
  shotPresenceLabelAnnotations: LabelAnnotation[];

  /**
   * Label annotations on frame level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.LabelAnnotation frame_label_annotations = 4;
   */
  frameLabelAnnotations: LabelAnnotation[];

  /**
   * Deprecated. Please use `face_detection_annotations` instead.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.FaceAnnotation face_annotations = 5 [deprecated = true];
   * @deprecated
   */
  faceAnnotations: FaceAnnotation[];

  /**
   * Face detection annotations.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.FaceDetectionAnnotation face_detection_annotations = 13;
   */
  faceDetectionAnnotations: FaceDetectionAnnotation[];

  /**
   * Shot annotations. Each shot is represented as a video segment.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.VideoSegment shot_annotations = 6;
   */
  shotAnnotations: VideoSegment[];

  /**
   * Explicit content annotation.
   *
   * @generated from field: google.cloud.videointelligence.v1.ExplicitContentAnnotation explicit_annotation = 7;
   */
  explicitAnnotation?: ExplicitContentAnnotation;

  /**
   * Speech transcription.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.SpeechTranscription speech_transcriptions = 11;
   */
  speechTranscriptions: SpeechTranscription[];

  /**
   * OCR text detection and tracking.
   * Annotations for list of detected text snippets. Each will have list of
   * frame information associated with it.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.TextAnnotation text_annotations = 12;
   */
  textAnnotations: TextAnnotation[];

  /**
   * Annotations for list of objects detected and tracked in video.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.ObjectTrackingAnnotation object_annotations = 14;
   */
  objectAnnotations: ObjectTrackingAnnotation[];

  /**
   * Annotations for list of logos detected, tracked and recognized in video.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.LogoRecognitionAnnotation logo_recognition_annotations = 19;
   */
  logoRecognitionAnnotations: LogoRecognitionAnnotation[];

  /**
   * Person detection annotations.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.PersonDetectionAnnotation person_detection_annotations = 20;
   */
  personDetectionAnnotations: PersonDetectionAnnotation[];

  /**
   * If set, indicates an error. Note that for a single `AnnotateVideoRequest`
   * some videos may succeed and some may fail.
   *
   * @generated from field: google.rpc.Status error = 9;
   */
  error?: Status;
};

/**
 * Describes the message google.cloud.videointelligence.v1.VideoAnnotationResults.
 * Use `create(VideoAnnotationResultsSchema)` to create a new message.
 */
export const VideoAnnotationResultsSchema: GenMessage<VideoAnnotationResults> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 26);

/**
 * Video annotation response. Included in the `response`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 *
 * @generated from message google.cloud.videointelligence.v1.AnnotateVideoResponse
 */
export type AnnotateVideoResponse = Message<"google.cloud.videointelligence.v1.AnnotateVideoResponse"> & {
  /**
   * Annotation results for all videos specified in `AnnotateVideoRequest`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.VideoAnnotationResults annotation_results = 1;
   */
  annotationResults: VideoAnnotationResults[];
};

/**
 * Describes the message google.cloud.videointelligence.v1.AnnotateVideoResponse.
 * Use `create(AnnotateVideoResponseSchema)` to create a new message.
 */
export const AnnotateVideoResponseSchema: GenMessage<AnnotateVideoResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 27);

/**
 * Annotation progress for a single video.
 *
 * @generated from message google.cloud.videointelligence.v1.VideoAnnotationProgress
 */
export type VideoAnnotationProgress = Message<"google.cloud.videointelligence.v1.VideoAnnotationProgress"> & {
  /**
   * Video file location in
   * [Cloud Storage](https://cloud.google.com/storage/).
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri: string;

  /**
   * Approximate percentage processed thus far. Guaranteed to be
   * 100 when fully processed.
   *
   * @generated from field: int32 progress_percent = 2;
   */
  progressPercent: number;

  /**
   * Time when the request was received.
   *
   * @generated from field: google.protobuf.Timestamp start_time = 3;
   */
  startTime?: Timestamp;

  /**
   * Time of the most recent update.
   *
   * @generated from field: google.protobuf.Timestamp update_time = 4;
   */
  updateTime?: Timestamp;

  /**
   * Specifies which feature is being tracked if the request contains more than
   * one feature.
   *
   * @generated from field: google.cloud.videointelligence.v1.Feature feature = 5;
   */
  feature: Feature;

  /**
   * Specifies which segment is being tracked if the request contains more than
   * one segment.
   *
   * @generated from field: google.cloud.videointelligence.v1.VideoSegment segment = 6;
   */
  segment?: VideoSegment;
};

/**
 * Describes the message google.cloud.videointelligence.v1.VideoAnnotationProgress.
 * Use `create(VideoAnnotationProgressSchema)` to create a new message.
 */
export const VideoAnnotationProgressSchema: GenMessage<VideoAnnotationProgress> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 28);

/**
 * Video annotation progress. Included in the `metadata`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 *
 * @generated from message google.cloud.videointelligence.v1.AnnotateVideoProgress
 */
export type AnnotateVideoProgress = Message<"google.cloud.videointelligence.v1.AnnotateVideoProgress"> & {
  /**
   * Progress metadata for all videos specified in `AnnotateVideoRequest`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.VideoAnnotationProgress annotation_progress = 1;
   */
  annotationProgress: VideoAnnotationProgress[];
};

/**
 * Describes the message google.cloud.videointelligence.v1.AnnotateVideoProgress.
 * Use `create(AnnotateVideoProgressSchema)` to create a new message.
 */
export const AnnotateVideoProgressSchema: GenMessage<AnnotateVideoProgress> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 29);

/**
 * Config for SPEECH_TRANSCRIPTION.
 *
 * @generated from message google.cloud.videointelligence.v1.SpeechTranscriptionConfig
 */
export type SpeechTranscriptionConfig = Message<"google.cloud.videointelligence.v1.SpeechTranscriptionConfig"> & {
  /**
   * Required. *Required* The language of the supplied audio as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Example: "en-US".
   * See [Language Support](https://cloud.google.com/speech/docs/languages)
   * for a list of the currently supported language codes.
   *
   * @generated from field: string language_code = 1;
   */
  languageCode: string;

  /**
   * Optional. Maximum number of recognition hypotheses to be returned.
   * Specifically, the maximum number of `SpeechRecognitionAlternative` messages
   * within each `SpeechTranscription`. The server may return fewer than
   * `max_alternatives`. Valid values are `0`-`30`. A value of `0` or `1` will
   * return a maximum of one. If omitted, will return a maximum of one.
   *
   * @generated from field: int32 max_alternatives = 2;
   */
  maxAlternatives: number;

  /**
   * Optional. If set to `true`, the server will attempt to filter out
   * profanities, replacing all but the initial character in each filtered word
   * with asterisks, e.g. "f***". If set to `false` or omitted, profanities
   * won't be filtered out.
   *
   * @generated from field: bool filter_profanity = 3;
   */
  filterProfanity: boolean;

  /**
   * Optional. A means to provide context to assist the speech recognition.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.SpeechContext speech_contexts = 4;
   */
  speechContexts: SpeechContext[];

  /**
   * Optional. If 'true', adds punctuation to recognition result hypotheses.
   * This feature is only available in select languages. Setting this for
   * requests in other languages has no effect at all. The default 'false' value
   * does not add punctuation to result hypotheses. NOTE: "This is currently
   * offered as an experimental service, complimentary to all users. In the
   * future this may be exclusively available as a premium feature."
   *
   * @generated from field: bool enable_automatic_punctuation = 5;
   */
  enableAutomaticPunctuation: boolean;

  /**
   * Optional. For file formats, such as MXF or MKV, supporting multiple audio
   * tracks, specify up to two tracks. Default: track 0.
   *
   * @generated from field: repeated int32 audio_tracks = 6;
   */
  audioTracks: number[];

  /**
   * Optional. If 'true', enables speaker detection for each recognized word in
   * the top alternative of the recognition result using a speaker_tag provided
   * in the WordInfo.
   * Note: When this is true, we send all the words from the beginning of the
   * audio for the top alternative in every consecutive response.
   * This is done in order to improve our speaker tags as our models learn to
   * identify the speakers in the conversation over time.
   *
   * @generated from field: bool enable_speaker_diarization = 7;
   */
  enableSpeakerDiarization: boolean;

  /**
   * Optional. If set, specifies the estimated number of speakers in the
   * conversation. If not set, defaults to '2'. Ignored unless
   * enable_speaker_diarization is set to true.
   *
   * @generated from field: int32 diarization_speaker_count = 8;
   */
  diarizationSpeakerCount: number;

  /**
   * Optional. If `true`, the top result includes a list of words and the
   * confidence for those words. If `false`, no word-level confidence
   * information is returned. The default is `false`.
   *
   * @generated from field: bool enable_word_confidence = 9;
   */
  enableWordConfidence: boolean;
};

/**
 * Describes the message google.cloud.videointelligence.v1.SpeechTranscriptionConfig.
 * Use `create(SpeechTranscriptionConfigSchema)` to create a new message.
 */
export const SpeechTranscriptionConfigSchema: GenMessage<SpeechTranscriptionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 30);

/**
 * Provides "hints" to the speech recognizer to favor specific words and phrases
 * in the results.
 *
 * @generated from message google.cloud.videointelligence.v1.SpeechContext
 */
export type SpeechContext = Message<"google.cloud.videointelligence.v1.SpeechContext"> & {
  /**
   * Optional. A list of strings containing words and phrases "hints" so that
   * the speech recognition is more likely to recognize them. This can be used
   * to improve the accuracy for specific words and phrases, for example, if
   * specific commands are typically spoken by the user. This can also be used
   * to add additional words to the vocabulary of the recognizer. See
   * [usage limits](https://cloud.google.com/speech/limits#content).
   *
   * @generated from field: repeated string phrases = 1;
   */
  phrases: string[];
};

/**
 * Describes the message google.cloud.videointelligence.v1.SpeechContext.
 * Use `create(SpeechContextSchema)` to create a new message.
 */
export const SpeechContextSchema: GenMessage<SpeechContext> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 31);

/**
 * A speech recognition result corresponding to a portion of the audio.
 *
 * @generated from message google.cloud.videointelligence.v1.SpeechTranscription
 */
export type SpeechTranscription = Message<"google.cloud.videointelligence.v1.SpeechTranscription"> & {
  /**
   * May contain one or more recognition hypotheses (up to the maximum specified
   * in `max_alternatives`).  These alternatives are ordered in terms of
   * accuracy, with the top (first) alternative being the most probable, as
   * ranked by the recognizer.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.SpeechRecognitionAlternative alternatives = 1;
   */
  alternatives: SpeechRecognitionAlternative[];

  /**
   * Output only. The [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt)
   * language tag of the language in this result. This language code was
   * detected to have the most likelihood of being spoken in the audio.
   *
   * @generated from field: string language_code = 2;
   */
  languageCode: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1.SpeechTranscription.
 * Use `create(SpeechTranscriptionSchema)` to create a new message.
 */
export const SpeechTranscriptionSchema: GenMessage<SpeechTranscription> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 32);

/**
 * Alternative hypotheses (a.k.a. n-best list).
 *
 * @generated from message google.cloud.videointelligence.v1.SpeechRecognitionAlternative
 */
export type SpeechRecognitionAlternative = Message<"google.cloud.videointelligence.v1.SpeechRecognitionAlternative"> & {
  /**
   * Transcript text representing the words that the user spoke.
   *
   * @generated from field: string transcript = 1;
   */
  transcript: string;

  /**
   * Output only. The confidence estimate between 0.0 and 1.0. A higher number
   * indicates an estimated greater likelihood that the recognized words are
   * correct. This field is set only for the top alternative.
   * This field is not guaranteed to be accurate and users should not rely on it
   * to be always provided.
   * The default of 0.0 is a sentinel value indicating `confidence` was not set.
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;

  /**
   * Output only. A list of word-specific information for each recognized word.
   * Note: When `enable_speaker_diarization` is set to true, you will see all
   * the words from the beginning of the audio.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.WordInfo words = 3;
   */
  words: WordInfo[];
};

/**
 * Describes the message google.cloud.videointelligence.v1.SpeechRecognitionAlternative.
 * Use `create(SpeechRecognitionAlternativeSchema)` to create a new message.
 */
export const SpeechRecognitionAlternativeSchema: GenMessage<SpeechRecognitionAlternative> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 33);

/**
 * Word-specific information for recognized words. Word information is only
 * included in the response when certain request parameters are set, such
 * as `enable_word_time_offsets`.
 *
 * @generated from message google.cloud.videointelligence.v1.WordInfo
 */
export type WordInfo = Message<"google.cloud.videointelligence.v1.WordInfo"> & {
  /**
   * Time offset relative to the beginning of the audio, and
   * corresponding to the start of the spoken word. This field is only set if
   * `enable_word_time_offsets=true` and only in the top hypothesis. This is an
   * experimental feature and the accuracy of the time offset can vary.
   *
   * @generated from field: google.protobuf.Duration start_time = 1;
   */
  startTime?: Duration;

  /**
   * Time offset relative to the beginning of the audio, and
   * corresponding to the end of the spoken word. This field is only set if
   * `enable_word_time_offsets=true` and only in the top hypothesis. This is an
   * experimental feature and the accuracy of the time offset can vary.
   *
   * @generated from field: google.protobuf.Duration end_time = 2;
   */
  endTime?: Duration;

  /**
   * The word corresponding to this set of information.
   *
   * @generated from field: string word = 3;
   */
  word: string;

  /**
   * Output only. The confidence estimate between 0.0 and 1.0. A higher number
   * indicates an estimated greater likelihood that the recognized words are
   * correct. This field is set only for the top alternative.
   * This field is not guaranteed to be accurate and users should not rely on it
   * to be always provided.
   * The default of 0.0 is a sentinel value indicating `confidence` was not set.
   *
   * @generated from field: float confidence = 4;
   */
  confidence: number;

  /**
   * Output only. A distinct integer value is assigned for every speaker within
   * the audio. This field specifies which one of those speakers was detected to
   * have spoken this word. Value ranges from 1 up to diarization_speaker_count,
   * and is only set if speaker diarization is enabled.
   *
   * @generated from field: int32 speaker_tag = 5;
   */
  speakerTag: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1.WordInfo.
 * Use `create(WordInfoSchema)` to create a new message.
 */
export const WordInfoSchema: GenMessage<WordInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 34);

/**
 * A vertex represents a 2D point in the image.
 * NOTE: the normalized vertex coordinates are relative to the original image
 * and range from 0 to 1.
 *
 * @generated from message google.cloud.videointelligence.v1.NormalizedVertex
 */
export type NormalizedVertex = Message<"google.cloud.videointelligence.v1.NormalizedVertex"> & {
  /**
   * X coordinate.
   *
   * @generated from field: float x = 1;
   */
  x: number;

  /**
   * Y coordinate.
   *
   * @generated from field: float y = 2;
   */
  y: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1.NormalizedVertex.
 * Use `create(NormalizedVertexSchema)` to create a new message.
 */
export const NormalizedVertexSchema: GenMessage<NormalizedVertex> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 35);

/**
 * Normalized bounding polygon for text (that might not be aligned with axis).
 * Contains list of the corner points in clockwise order starting from
 * top-left corner. For example, for a rectangular bounding box:
 * When the text is horizontal it might look like:
 *         0----1
 *         |    |
 *         3----2
 *
 * When it's clockwise rotated 180 degrees around the top-left corner it
 * becomes:
 *         2----3
 *         |    |
 *         1----0
 *
 * and the vertex order will still be (0, 1, 2, 3). Note that values can be less
 * than 0, or greater than 1 due to trignometric calculations for location of
 * the box.
 *
 * @generated from message google.cloud.videointelligence.v1.NormalizedBoundingPoly
 */
export type NormalizedBoundingPoly = Message<"google.cloud.videointelligence.v1.NormalizedBoundingPoly"> & {
  /**
   * Normalized vertices of the bounding polygon.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.NormalizedVertex vertices = 1;
   */
  vertices: NormalizedVertex[];
};

/**
 * Describes the message google.cloud.videointelligence.v1.NormalizedBoundingPoly.
 * Use `create(NormalizedBoundingPolySchema)` to create a new message.
 */
export const NormalizedBoundingPolySchema: GenMessage<NormalizedBoundingPoly> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 36);

/**
 * Video segment level annotation results for text detection.
 *
 * @generated from message google.cloud.videointelligence.v1.TextSegment
 */
export type TextSegment = Message<"google.cloud.videointelligence.v1.TextSegment"> & {
  /**
   * Video segment where a text snippet was detected.
   *
   * @generated from field: google.cloud.videointelligence.v1.VideoSegment segment = 1;
   */
  segment?: VideoSegment;

  /**
   * Confidence for the track of detected text. It is calculated as the highest
   * over all frames where OCR detected text appears.
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;

  /**
   * Information related to the frames where OCR detected text appears.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.TextFrame frames = 3;
   */
  frames: TextFrame[];
};

/**
 * Describes the message google.cloud.videointelligence.v1.TextSegment.
 * Use `create(TextSegmentSchema)` to create a new message.
 */
export const TextSegmentSchema: GenMessage<TextSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 37);

/**
 * Video frame level annotation results for text annotation (OCR).
 * Contains information regarding timestamp and bounding box locations for the
 * frames containing detected OCR text snippets.
 *
 * @generated from message google.cloud.videointelligence.v1.TextFrame
 */
export type TextFrame = Message<"google.cloud.videointelligence.v1.TextFrame"> & {
  /**
   * Bounding polygon of the detected text for this frame.
   *
   * @generated from field: google.cloud.videointelligence.v1.NormalizedBoundingPoly rotated_bounding_box = 1;
   */
  rotatedBoundingBox?: NormalizedBoundingPoly;

  /**
   * Timestamp of this frame.
   *
   * @generated from field: google.protobuf.Duration time_offset = 2;
   */
  timeOffset?: Duration;
};

/**
 * Describes the message google.cloud.videointelligence.v1.TextFrame.
 * Use `create(TextFrameSchema)` to create a new message.
 */
export const TextFrameSchema: GenMessage<TextFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 38);

/**
 * Annotations related to one detected OCR text snippet. This will contain the
 * corresponding text, confidence value, and frame level information for each
 * detection.
 *
 * @generated from message google.cloud.videointelligence.v1.TextAnnotation
 */
export type TextAnnotation = Message<"google.cloud.videointelligence.v1.TextAnnotation"> & {
  /**
   * The detected text.
   *
   * @generated from field: string text = 1;
   */
  text: string;

  /**
   * All video segments where OCR detected text appears.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.TextSegment segments = 2;
   */
  segments: TextSegment[];

  /**
   * Feature version.
   *
   * @generated from field: string version = 3;
   */
  version: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1.TextAnnotation.
 * Use `create(TextAnnotationSchema)` to create a new message.
 */
export const TextAnnotationSchema: GenMessage<TextAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 39);

/**
 * Video frame level annotations for object detection and tracking. This field
 * stores per frame location, time offset, and confidence.
 *
 * @generated from message google.cloud.videointelligence.v1.ObjectTrackingFrame
 */
export type ObjectTrackingFrame = Message<"google.cloud.videointelligence.v1.ObjectTrackingFrame"> & {
  /**
   * The normalized bounding box location of this object track for the frame.
   *
   * @generated from field: google.cloud.videointelligence.v1.NormalizedBoundingBox normalized_bounding_box = 1;
   */
  normalizedBoundingBox?: NormalizedBoundingBox;

  /**
   * The timestamp of the frame in microseconds.
   *
   * @generated from field: google.protobuf.Duration time_offset = 2;
   */
  timeOffset?: Duration;
};

/**
 * Describes the message google.cloud.videointelligence.v1.ObjectTrackingFrame.
 * Use `create(ObjectTrackingFrameSchema)` to create a new message.
 */
export const ObjectTrackingFrameSchema: GenMessage<ObjectTrackingFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 40);

/**
 * Annotations corresponding to one tracked object.
 *
 * @generated from message google.cloud.videointelligence.v1.ObjectTrackingAnnotation
 */
export type ObjectTrackingAnnotation = Message<"google.cloud.videointelligence.v1.ObjectTrackingAnnotation"> & {
  /**
   * Different representation of tracking info in non-streaming batch
   * and streaming modes.
   *
   * @generated from oneof google.cloud.videointelligence.v1.ObjectTrackingAnnotation.track_info
   */
  trackInfo: {
    /**
     * Non-streaming batch mode ONLY.
     * Each object track corresponds to one video segment where it appears.
     *
     * @generated from field: google.cloud.videointelligence.v1.VideoSegment segment = 3;
     */
    value: VideoSegment;
    case: "segment";
  } | {
    /**
     * Streaming mode ONLY.
     * In streaming mode, we do not know the end time of a tracked object
     * before it is completed. Hence, there is no VideoSegment info returned.
     * Instead, we provide a unique identifiable integer track_id so that
     * the customers can correlate the results of the ongoing
     * ObjectTrackAnnotation of the same track_id over time.
     *
     * @generated from field: int64 track_id = 5;
     */
    value: bigint;
    case: "trackId";
  } | { case: undefined; value?: undefined };

  /**
   * Entity to specify the object category that this track is labeled as.
   *
   * @generated from field: google.cloud.videointelligence.v1.Entity entity = 1;
   */
  entity?: Entity;

  /**
   * Object category's labeling confidence of this track.
   *
   * @generated from field: float confidence = 4;
   */
  confidence: number;

  /**
   * Information corresponding to all frames where this object track appears.
   * Non-streaming batch mode: it may be one or multiple ObjectTrackingFrame
   * messages in frames.
   * Streaming mode: it can only be one ObjectTrackingFrame message in frames.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.ObjectTrackingFrame frames = 2;
   */
  frames: ObjectTrackingFrame[];

  /**
   * Feature version.
   *
   * @generated from field: string version = 6;
   */
  version: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1.ObjectTrackingAnnotation.
 * Use `create(ObjectTrackingAnnotationSchema)` to create a new message.
 */
export const ObjectTrackingAnnotationSchema: GenMessage<ObjectTrackingAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 41);

/**
 * Annotation corresponding to one detected, tracked and recognized logo class.
 *
 * @generated from message google.cloud.videointelligence.v1.LogoRecognitionAnnotation
 */
export type LogoRecognitionAnnotation = Message<"google.cloud.videointelligence.v1.LogoRecognitionAnnotation"> & {
  /**
   * Entity category information to specify the logo class that all the logo
   * tracks within this LogoRecognitionAnnotation are recognized as.
   *
   * @generated from field: google.cloud.videointelligence.v1.Entity entity = 1;
   */
  entity?: Entity;

  /**
   * All logo tracks where the recognized logo appears. Each track corresponds
   * to one logo instance appearing in consecutive frames.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.Track tracks = 2;
   */
  tracks: Track[];

  /**
   * All video segments where the recognized logo appears. There might be
   * multiple instances of the same logo class appearing in one VideoSegment.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1.VideoSegment segments = 3;
   */
  segments: VideoSegment[];
};

/**
 * Describes the message google.cloud.videointelligence.v1.LogoRecognitionAnnotation.
 * Use `create(LogoRecognitionAnnotationSchema)` to create a new message.
 */
export const LogoRecognitionAnnotationSchema: GenMessage<LogoRecognitionAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1_video_intelligence, 42);

/**
 * Video annotation feature.
 *
 * @generated from enum google.cloud.videointelligence.v1.Feature
 */
export enum Feature {
  /**
   * Unspecified.
   *
   * @generated from enum value: FEATURE_UNSPECIFIED = 0;
   */
  FEATURE_UNSPECIFIED = 0,

  /**
   * Label detection. Detect objects, such as dog or flower.
   *
   * @generated from enum value: LABEL_DETECTION = 1;
   */
  LABEL_DETECTION = 1,

  /**
   * Shot change detection.
   *
   * @generated from enum value: SHOT_CHANGE_DETECTION = 2;
   */
  SHOT_CHANGE_DETECTION = 2,

  /**
   * Explicit content detection.
   *
   * @generated from enum value: EXPLICIT_CONTENT_DETECTION = 3;
   */
  EXPLICIT_CONTENT_DETECTION = 3,

  /**
   * Human face detection.
   *
   * @generated from enum value: FACE_DETECTION = 4;
   */
  FACE_DETECTION = 4,

  /**
   * Speech transcription.
   *
   * @generated from enum value: SPEECH_TRANSCRIPTION = 6;
   */
  SPEECH_TRANSCRIPTION = 6,

  /**
   * OCR text detection and tracking.
   *
   * @generated from enum value: TEXT_DETECTION = 7;
   */
  TEXT_DETECTION = 7,

  /**
   * Object detection and tracking.
   *
   * @generated from enum value: OBJECT_TRACKING = 9;
   */
  OBJECT_TRACKING = 9,

  /**
   * Logo detection, tracking, and recognition.
   *
   * @generated from enum value: LOGO_RECOGNITION = 12;
   */
  LOGO_RECOGNITION = 12,

  /**
   * Person detection.
   *
   * @generated from enum value: PERSON_DETECTION = 14;
   */
  PERSON_DETECTION = 14,
}

/**
 * Describes the enum google.cloud.videointelligence.v1.Feature.
 */
export const FeatureSchema: GenEnum<Feature> = /*@__PURE__*/
  enumDesc(file_google_cloud_videointelligence_v1_video_intelligence, 0);

/**
 * Label detection mode.
 *
 * @generated from enum google.cloud.videointelligence.v1.LabelDetectionMode
 */
export enum LabelDetectionMode {
  /**
   * Unspecified.
   *
   * @generated from enum value: LABEL_DETECTION_MODE_UNSPECIFIED = 0;
   */
  LABEL_DETECTION_MODE_UNSPECIFIED = 0,

  /**
   * Detect shot-level labels.
   *
   * @generated from enum value: SHOT_MODE = 1;
   */
  SHOT_MODE = 1,

  /**
   * Detect frame-level labels.
   *
   * @generated from enum value: FRAME_MODE = 2;
   */
  FRAME_MODE = 2,

  /**
   * Detect both shot-level and frame-level labels.
   *
   * @generated from enum value: SHOT_AND_FRAME_MODE = 3;
   */
  SHOT_AND_FRAME_MODE = 3,
}

/**
 * Describes the enum google.cloud.videointelligence.v1.LabelDetectionMode.
 */
export const LabelDetectionModeSchema: GenEnum<LabelDetectionMode> = /*@__PURE__*/
  enumDesc(file_google_cloud_videointelligence_v1_video_intelligence, 1);

/**
 * Bucketized representation of likelihood.
 *
 * @generated from enum google.cloud.videointelligence.v1.Likelihood
 */
export enum Likelihood {
  /**
   * Unspecified likelihood.
   *
   * @generated from enum value: LIKELIHOOD_UNSPECIFIED = 0;
   */
  LIKELIHOOD_UNSPECIFIED = 0,

  /**
   * Very unlikely.
   *
   * @generated from enum value: VERY_UNLIKELY = 1;
   */
  VERY_UNLIKELY = 1,

  /**
   * Unlikely.
   *
   * @generated from enum value: UNLIKELY = 2;
   */
  UNLIKELY = 2,

  /**
   * Possible.
   *
   * @generated from enum value: POSSIBLE = 3;
   */
  POSSIBLE = 3,

  /**
   * Likely.
   *
   * @generated from enum value: LIKELY = 4;
   */
  LIKELY = 4,

  /**
   * Very likely.
   *
   * @generated from enum value: VERY_LIKELY = 5;
   */
  VERY_LIKELY = 5,
}

/**
 * Describes the enum google.cloud.videointelligence.v1.Likelihood.
 */
export const LikelihoodSchema: GenEnum<Likelihood> = /*@__PURE__*/
  enumDesc(file_google_cloud_videointelligence_v1_video_intelligence, 2);

/**
 * Service that implements the Video Intelligence API.
 *
 * @generated from service google.cloud.videointelligence.v1.VideoIntelligenceService
 */
export const VideoIntelligenceService: GenService<{
  /**
   * Performs asynchronous video annotation. Progress and results can be
   * retrieved through the `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `AnnotateVideoProgress` (progress).
   * `Operation.response` contains `AnnotateVideoResponse` (results).
   *
   * @generated from rpc google.cloud.videointelligence.v1.VideoIntelligenceService.AnnotateVideo
   */
  annotateVideo: {
    methodKind: "unary";
    input: typeof AnnotateVideoRequestSchema;
    output: typeof OperationSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_videointelligence_v1_video_intelligence, 0);

