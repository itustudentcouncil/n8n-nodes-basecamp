// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/videointelligence/v1p3beta1/video_intelligence.proto (package google.cloud.videointelligence.v1p3beta1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import type { OperationSchema } from "../../../longrunning/operations_pb";
import { file_google_longrunning_operations } from "../../../longrunning/operations_pb";
import type { Duration, Timestamp } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_duration, file_google_protobuf_timestamp } from "@bufbuild/protobuf/wkt";
import type { Status } from "../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../rpc/status_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/videointelligence/v1p3beta1/video_intelligence.proto.
 */
export const file_google_cloud_videointelligence_v1p3beta1_video_intelligence: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_longrunning_operations, file_google_protobuf_duration, file_google_protobuf_timestamp, file_google_rpc_status]);

/**
 * Video annotation request.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.AnnotateVideoRequest
 */
export type AnnotateVideoRequest = Message<"google.cloud.videointelligence.v1p3beta1.AnnotateVideoRequest"> & {
  /**
   * Input video location. Currently, only
   * [Cloud Storage](https://cloud.google.com/storage/) URIs are
   * supported. URIs must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
   * more information, see [Request
   * URIs](https://cloud.google.com/storage/docs/request-endpoints). To identify
   * multiple videos, a video URI may include wildcards in the `object-id`.
   * Supported wildcards: '*' to match 0 or more characters;
   * '?' to match 1 character. If unset, the input video should be embedded
   * in the request as `input_content`. If set, `input_content` must be unset.
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri: string;

  /**
   * The video data bytes.
   * If unset, the input video(s) should be specified via the `input_uri`.
   * If set, `input_uri` must be unset.
   *
   * @generated from field: bytes input_content = 6;
   */
  inputContent: Uint8Array;

  /**
   * Required. Requested video annotation features.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.Feature features = 2;
   */
  features: Feature[];

  /**
   * Additional video context and/or feature-specific parameters.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.VideoContext video_context = 3;
   */
  videoContext?: VideoContext;

  /**
   * Optional. Location where the output (in JSON format) should be stored.
   * Currently, only [Cloud Storage](https://cloud.google.com/storage/)
   * URIs are supported. These must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
   * more information, see [Request
   * URIs](https://cloud.google.com/storage/docs/request-endpoints).
   *
   * @generated from field: string output_uri = 4;
   */
  outputUri: string;

  /**
   * Optional. Cloud region where annotation should take place. Supported cloud
   * regions are: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no
   * region is specified, the region will be determined based on video file
   * location.
   *
   * @generated from field: string location_id = 5;
   */
  locationId: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.AnnotateVideoRequest.
 * Use `create(AnnotateVideoRequestSchema)` to create a new message.
 */
export const AnnotateVideoRequestSchema: GenMessage<AnnotateVideoRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 0);

/**
 * Video context and/or feature-specific parameters.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.VideoContext
 */
export type VideoContext = Message<"google.cloud.videointelligence.v1p3beta1.VideoContext"> & {
  /**
   * Video segments to annotate. The segments may overlap and are not required
   * to be contiguous or span the whole video. If unspecified, each video is
   * treated as a single segment.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.VideoSegment segments = 1;
   */
  segments: VideoSegment[];

  /**
   * Config for LABEL_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.LabelDetectionConfig label_detection_config = 2;
   */
  labelDetectionConfig?: LabelDetectionConfig;

  /**
   * Config for SHOT_CHANGE_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.ShotChangeDetectionConfig shot_change_detection_config = 3;
   */
  shotChangeDetectionConfig?: ShotChangeDetectionConfig;

  /**
   * Config for EXPLICIT_CONTENT_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.ExplicitContentDetectionConfig explicit_content_detection_config = 4;
   */
  explicitContentDetectionConfig?: ExplicitContentDetectionConfig;

  /**
   * Config for FACE_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.FaceDetectionConfig face_detection_config = 5;
   */
  faceDetectionConfig?: FaceDetectionConfig;

  /**
   * Config for SPEECH_TRANSCRIPTION.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.SpeechTranscriptionConfig speech_transcription_config = 6;
   */
  speechTranscriptionConfig?: SpeechTranscriptionConfig;

  /**
   * Config for TEXT_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.TextDetectionConfig text_detection_config = 8;
   */
  textDetectionConfig?: TextDetectionConfig;

  /**
   * Config for PERSON_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.PersonDetectionConfig person_detection_config = 11;
   */
  personDetectionConfig?: PersonDetectionConfig;

  /**
   * Config for OBJECT_TRACKING.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.ObjectTrackingConfig object_tracking_config = 13;
   */
  objectTrackingConfig?: ObjectTrackingConfig;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.VideoContext.
 * Use `create(VideoContextSchema)` to create a new message.
 */
export const VideoContextSchema: GenMessage<VideoContext> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 1);

/**
 * Config for LABEL_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.LabelDetectionConfig
 */
export type LabelDetectionConfig = Message<"google.cloud.videointelligence.v1p3beta1.LabelDetectionConfig"> & {
  /**
   * What labels should be detected with LABEL_DETECTION, in addition to
   * video-level labels or segment-level labels.
   * If unspecified, defaults to `SHOT_MODE`.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.LabelDetectionMode label_detection_mode = 1;
   */
  labelDetectionMode: LabelDetectionMode;

  /**
   * Whether the video has been shot from a stationary (i.e., non-moving)
   * camera. When set to true, might improve detection accuracy for moving
   * objects. Should be used with `SHOT_AND_FRAME_MODE` enabled.
   *
   * @generated from field: bool stationary_camera = 2;
   */
  stationaryCamera: boolean;

  /**
   * Model to use for label detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 3;
   */
  model: string;

  /**
   * The confidence threshold we perform filtering on the labels from
   * frame-level detection. If not set, it is set to 0.4 by default. The valid
   * range for this threshold is [0.1, 0.9]. Any value set outside of this
   * range will be clipped.
   * Note: For best results, follow the default threshold. We will update
   * the default threshold everytime when we release a new model.
   *
   * @generated from field: float frame_confidence_threshold = 4;
   */
  frameConfidenceThreshold: number;

  /**
   * The confidence threshold we perform filtering on the labels from
   * video-level and shot-level detections. If not set, it's set to 0.3 by
   * default. The valid range for this threshold is [0.1, 0.9]. Any value set
   * outside of this range will be clipped.
   * Note: For best results, follow the default threshold. We will update
   * the default threshold everytime when we release a new model.
   *
   * @generated from field: float video_confidence_threshold = 5;
   */
  videoConfidenceThreshold: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.LabelDetectionConfig.
 * Use `create(LabelDetectionConfigSchema)` to create a new message.
 */
export const LabelDetectionConfigSchema: GenMessage<LabelDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 2);

/**
 * Config for SHOT_CHANGE_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.ShotChangeDetectionConfig
 */
export type ShotChangeDetectionConfig = Message<"google.cloud.videointelligence.v1p3beta1.ShotChangeDetectionConfig"> & {
  /**
   * Model to use for shot change detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.ShotChangeDetectionConfig.
 * Use `create(ShotChangeDetectionConfigSchema)` to create a new message.
 */
export const ShotChangeDetectionConfigSchema: GenMessage<ShotChangeDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 3);

/**
 * Config for OBJECT_TRACKING.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.ObjectTrackingConfig
 */
export type ObjectTrackingConfig = Message<"google.cloud.videointelligence.v1p3beta1.ObjectTrackingConfig"> & {
  /**
   * Model to use for object tracking.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.ObjectTrackingConfig.
 * Use `create(ObjectTrackingConfigSchema)` to create a new message.
 */
export const ObjectTrackingConfigSchema: GenMessage<ObjectTrackingConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 4);

/**
 * Config for EXPLICIT_CONTENT_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.ExplicitContentDetectionConfig
 */
export type ExplicitContentDetectionConfig = Message<"google.cloud.videointelligence.v1p3beta1.ExplicitContentDetectionConfig"> & {
  /**
   * Model to use for explicit content detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.ExplicitContentDetectionConfig.
 * Use `create(ExplicitContentDetectionConfigSchema)` to create a new message.
 */
export const ExplicitContentDetectionConfigSchema: GenMessage<ExplicitContentDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 5);

/**
 * Config for FACE_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.FaceDetectionConfig
 */
export type FaceDetectionConfig = Message<"google.cloud.videointelligence.v1p3beta1.FaceDetectionConfig"> & {
  /**
   * Model to use for face detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Whether bounding boxes are included in the face annotation output.
   *
   * @generated from field: bool include_bounding_boxes = 2;
   */
  includeBoundingBoxes: boolean;

  /**
   * Whether to enable face attributes detection, such as glasses, dark_glasses,
   * mouth_open etc. Ignored if 'include_bounding_boxes' is set to false.
   *
   * @generated from field: bool include_attributes = 5;
   */
  includeAttributes: boolean;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.FaceDetectionConfig.
 * Use `create(FaceDetectionConfigSchema)` to create a new message.
 */
export const FaceDetectionConfigSchema: GenMessage<FaceDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 6);

/**
 * Config for PERSON_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.PersonDetectionConfig
 */
export type PersonDetectionConfig = Message<"google.cloud.videointelligence.v1p3beta1.PersonDetectionConfig"> & {
  /**
   * Whether bounding boxes are included in the person detection annotation
   * output.
   *
   * @generated from field: bool include_bounding_boxes = 1;
   */
  includeBoundingBoxes: boolean;

  /**
   * Whether to enable pose landmarks detection. Ignored if
   * 'include_bounding_boxes' is set to false.
   *
   * @generated from field: bool include_pose_landmarks = 2;
   */
  includePoseLandmarks: boolean;

  /**
   * Whether to enable person attributes detection, such as cloth color (black,
   * blue, etc), type (coat, dress, etc), pattern (plain, floral, etc), hair,
   * etc.
   * Ignored if 'include_bounding_boxes' is set to false.
   *
   * @generated from field: bool include_attributes = 3;
   */
  includeAttributes: boolean;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.PersonDetectionConfig.
 * Use `create(PersonDetectionConfigSchema)` to create a new message.
 */
export const PersonDetectionConfigSchema: GenMessage<PersonDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 7);

/**
 * Config for TEXT_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.TextDetectionConfig
 */
export type TextDetectionConfig = Message<"google.cloud.videointelligence.v1p3beta1.TextDetectionConfig"> & {
  /**
   * Language hint can be specified if the language to be detected is known a
   * priori. It can increase the accuracy of the detection. Language hint must
   * be language code in BCP-47 format.
   *
   * Automatic language detection is performed if no hint is provided.
   *
   * @generated from field: repeated string language_hints = 1;
   */
  languageHints: string[];

  /**
   * Model to use for text detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 2;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.TextDetectionConfig.
 * Use `create(TextDetectionConfigSchema)` to create a new message.
 */
export const TextDetectionConfigSchema: GenMessage<TextDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 8);

/**
 * Video segment.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.VideoSegment
 */
export type VideoSegment = Message<"google.cloud.videointelligence.v1p3beta1.VideoSegment"> & {
  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the start of the segment (inclusive).
   *
   * @generated from field: google.protobuf.Duration start_time_offset = 1;
   */
  startTimeOffset?: Duration;

  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the end of the segment (inclusive).
   *
   * @generated from field: google.protobuf.Duration end_time_offset = 2;
   */
  endTimeOffset?: Duration;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.VideoSegment.
 * Use `create(VideoSegmentSchema)` to create a new message.
 */
export const VideoSegmentSchema: GenMessage<VideoSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 9);

/**
 * Video segment level annotation results for label detection.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.LabelSegment
 */
export type LabelSegment = Message<"google.cloud.videointelligence.v1p3beta1.LabelSegment"> & {
  /**
   * Video segment where a label was detected.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.VideoSegment segment = 1;
   */
  segment?: VideoSegment;

  /**
   * Confidence that the label is accurate. Range: [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.LabelSegment.
 * Use `create(LabelSegmentSchema)` to create a new message.
 */
export const LabelSegmentSchema: GenMessage<LabelSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 10);

/**
 * Video frame level annotation results for label detection.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.LabelFrame
 */
export type LabelFrame = Message<"google.cloud.videointelligence.v1p3beta1.LabelFrame"> & {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   *
   * @generated from field: google.protobuf.Duration time_offset = 1;
   */
  timeOffset?: Duration;

  /**
   * Confidence that the label is accurate. Range: [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.LabelFrame.
 * Use `create(LabelFrameSchema)` to create a new message.
 */
export const LabelFrameSchema: GenMessage<LabelFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 11);

/**
 * Detected entity from video analysis.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.Entity
 */
export type Entity = Message<"google.cloud.videointelligence.v1p3beta1.Entity"> & {
  /**
   * Opaque entity ID. Some IDs may be available in
   * [Google Knowledge Graph Search
   * API](https://developers.google.com/knowledge-graph/).
   *
   * @generated from field: string entity_id = 1;
   */
  entityId: string;

  /**
   * Textual description, e.g., `Fixed-gear bicycle`.
   *
   * @generated from field: string description = 2;
   */
  description: string;

  /**
   * Language code for `description` in BCP-47 format.
   *
   * @generated from field: string language_code = 3;
   */
  languageCode: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.Entity.
 * Use `create(EntitySchema)` to create a new message.
 */
export const EntitySchema: GenMessage<Entity> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 12);

/**
 * Label annotation.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.LabelAnnotation
 */
export type LabelAnnotation = Message<"google.cloud.videointelligence.v1p3beta1.LabelAnnotation"> & {
  /**
   * Detected entity.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.Entity entity = 1;
   */
  entity?: Entity;

  /**
   * Common categories for the detected entity.
   * For example, when the label is `Terrier`, the category is likely `dog`. And
   * in some cases there might be more than one categories e.g., `Terrier` could
   * also be a `pet`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.Entity category_entities = 2;
   */
  categoryEntities: Entity[];

  /**
   * All video segments where a label was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.LabelSegment segments = 3;
   */
  segments: LabelSegment[];

  /**
   * All video frames where a label was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.LabelFrame frames = 4;
   */
  frames: LabelFrame[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.LabelAnnotation.
 * Use `create(LabelAnnotationSchema)` to create a new message.
 */
export const LabelAnnotationSchema: GenMessage<LabelAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 13);

/**
 * Video frame level annotation results for explicit content.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.ExplicitContentFrame
 */
export type ExplicitContentFrame = Message<"google.cloud.videointelligence.v1p3beta1.ExplicitContentFrame"> & {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   *
   * @generated from field: google.protobuf.Duration time_offset = 1;
   */
  timeOffset?: Duration;

  /**
   * Likelihood of the pornography content..
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.Likelihood pornography_likelihood = 2;
   */
  pornographyLikelihood: Likelihood;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.ExplicitContentFrame.
 * Use `create(ExplicitContentFrameSchema)` to create a new message.
 */
export const ExplicitContentFrameSchema: GenMessage<ExplicitContentFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 14);

/**
 * Explicit content annotation (based on per-frame visual signals only).
 * If no explicit content has been detected in a frame, no annotations are
 * present for that frame.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.ExplicitContentAnnotation
 */
export type ExplicitContentAnnotation = Message<"google.cloud.videointelligence.v1p3beta1.ExplicitContentAnnotation"> & {
  /**
   * All video frames where explicit content was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.ExplicitContentFrame frames = 1;
   */
  frames: ExplicitContentFrame[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.ExplicitContentAnnotation.
 * Use `create(ExplicitContentAnnotationSchema)` to create a new message.
 */
export const ExplicitContentAnnotationSchema: GenMessage<ExplicitContentAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 15);

/**
 * Normalized bounding box.
 * The normalized vertex coordinates are relative to the original image.
 * Range: [0, 1].
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.NormalizedBoundingBox
 */
export type NormalizedBoundingBox = Message<"google.cloud.videointelligence.v1p3beta1.NormalizedBoundingBox"> & {
  /**
   * Left X coordinate.
   *
   * @generated from field: float left = 1;
   */
  left: number;

  /**
   * Top Y coordinate.
   *
   * @generated from field: float top = 2;
   */
  top: number;

  /**
   * Right X coordinate.
   *
   * @generated from field: float right = 3;
   */
  right: number;

  /**
   * Bottom Y coordinate.
   *
   * @generated from field: float bottom = 4;
   */
  bottom: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.NormalizedBoundingBox.
 * Use `create(NormalizedBoundingBoxSchema)` to create a new message.
 */
export const NormalizedBoundingBoxSchema: GenMessage<NormalizedBoundingBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 16);

/**
 * For tracking related features.
 * An object at time_offset with attributes, and located with
 * normalized_bounding_box.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.TimestampedObject
 */
export type TimestampedObject = Message<"google.cloud.videointelligence.v1p3beta1.TimestampedObject"> & {
  /**
   * Normalized Bounding box in a frame, where the object is located.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.NormalizedBoundingBox normalized_bounding_box = 1;
   */
  normalizedBoundingBox?: NormalizedBoundingBox;

  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the video frame for this object.
   *
   * @generated from field: google.protobuf.Duration time_offset = 2;
   */
  timeOffset?: Duration;

  /**
   * Optional. The attributes of the object in the bounding box.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.DetectedAttribute attributes = 3;
   */
  attributes: DetectedAttribute[];

  /**
   * Optional. The detected landmarks.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.DetectedLandmark landmarks = 4;
   */
  landmarks: DetectedLandmark[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.TimestampedObject.
 * Use `create(TimestampedObjectSchema)` to create a new message.
 */
export const TimestampedObjectSchema: GenMessage<TimestampedObject> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 17);

/**
 * A track of an object instance.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.Track
 */
export type Track = Message<"google.cloud.videointelligence.v1p3beta1.Track"> & {
  /**
   * Video segment of a track.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.VideoSegment segment = 1;
   */
  segment?: VideoSegment;

  /**
   * The object with timestamp and attributes per frame in the track.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.TimestampedObject timestamped_objects = 2;
   */
  timestampedObjects: TimestampedObject[];

  /**
   * Optional. Attributes in the track level.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.DetectedAttribute attributes = 3;
   */
  attributes: DetectedAttribute[];

  /**
   * Optional. The confidence score of the tracked object.
   *
   * @generated from field: float confidence = 4;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.Track.
 * Use `create(TrackSchema)` to create a new message.
 */
export const TrackSchema: GenMessage<Track> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 18);

/**
 * A generic detected attribute represented by name in string format.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.DetectedAttribute
 */
export type DetectedAttribute = Message<"google.cloud.videointelligence.v1p3beta1.DetectedAttribute"> & {
  /**
   * The name of the attribute, for example, glasses, dark_glasses, mouth_open.
   * A full list of supported type names will be provided in the document.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Detected attribute confidence. Range [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;

  /**
   * Text value of the detection result. For example, the value for "HairColor"
   * can be "black", "blonde", etc.
   *
   * @generated from field: string value = 3;
   */
  value: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.DetectedAttribute.
 * Use `create(DetectedAttributeSchema)` to create a new message.
 */
export const DetectedAttributeSchema: GenMessage<DetectedAttribute> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 19);

/**
 * Celebrity definition.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.Celebrity
 */
export type Celebrity = Message<"google.cloud.videointelligence.v1p3beta1.Celebrity"> & {
  /**
   * The resource name of the celebrity. Have the format
   * `video-intelligence/kg-mid` indicates a celebrity from preloaded gallery.
   * kg-mid is the id in Google knowledge graph, which is unique for the
   * celebrity.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * The celebrity name.
   *
   * @generated from field: string display_name = 2;
   */
  displayName: string;

  /**
   * Textual description of additional information about the celebrity, if
   * applicable.
   *
   * @generated from field: string description = 3;
   */
  description: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.Celebrity.
 * Use `create(CelebritySchema)` to create a new message.
 */
export const CelebritySchema: GenMessage<Celebrity> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 20);

/**
 * The annotation result of a celebrity face track. RecognizedCelebrity field
 * could be empty if the face track does not have any matched celebrities.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.CelebrityTrack
 */
export type CelebrityTrack = Message<"google.cloud.videointelligence.v1p3beta1.CelebrityTrack"> & {
  /**
   * Top N match of the celebrities for the face in this track.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.CelebrityTrack.RecognizedCelebrity celebrities = 1;
   */
  celebrities: CelebrityTrack_RecognizedCelebrity[];

  /**
   * A track of a person's face.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.Track face_track = 3;
   */
  faceTrack?: Track;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.CelebrityTrack.
 * Use `create(CelebrityTrackSchema)` to create a new message.
 */
export const CelebrityTrackSchema: GenMessage<CelebrityTrack> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 21);

/**
 * The recognized celebrity with confidence score.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.CelebrityTrack.RecognizedCelebrity
 */
export type CelebrityTrack_RecognizedCelebrity = Message<"google.cloud.videointelligence.v1p3beta1.CelebrityTrack.RecognizedCelebrity"> & {
  /**
   * The recognized celebrity.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.Celebrity celebrity = 1;
   */
  celebrity?: Celebrity;

  /**
   * Recognition confidence. Range [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.CelebrityTrack.RecognizedCelebrity.
 * Use `create(CelebrityTrack_RecognizedCelebritySchema)` to create a new message.
 */
export const CelebrityTrack_RecognizedCelebritySchema: GenMessage<CelebrityTrack_RecognizedCelebrity> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 21, 0);

/**
 * Celebrity recognition annotation per video.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.CelebrityRecognitionAnnotation
 */
export type CelebrityRecognitionAnnotation = Message<"google.cloud.videointelligence.v1p3beta1.CelebrityRecognitionAnnotation"> & {
  /**
   * The tracks detected from the input video, including recognized celebrities
   * and other detected faces in the video.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.CelebrityTrack celebrity_tracks = 1;
   */
  celebrityTracks: CelebrityTrack[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.CelebrityRecognitionAnnotation.
 * Use `create(CelebrityRecognitionAnnotationSchema)` to create a new message.
 */
export const CelebrityRecognitionAnnotationSchema: GenMessage<CelebrityRecognitionAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 22);

/**
 * A generic detected landmark represented by name in string format and a 2D
 * location.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.DetectedLandmark
 */
export type DetectedLandmark = Message<"google.cloud.videointelligence.v1p3beta1.DetectedLandmark"> & {
  /**
   * The name of this landmark, for example, left_hand, right_shoulder.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * The 2D point of the detected landmark using the normalized image
   * coordindate system. The normalized coordinates have the range from 0 to 1.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.NormalizedVertex point = 2;
   */
  point?: NormalizedVertex;

  /**
   * The confidence score of the detected landmark. Range [0, 1].
   *
   * @generated from field: float confidence = 3;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.DetectedLandmark.
 * Use `create(DetectedLandmarkSchema)` to create a new message.
 */
export const DetectedLandmarkSchema: GenMessage<DetectedLandmark> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 23);

/**
 * Face detection annotation.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.FaceDetectionAnnotation
 */
export type FaceDetectionAnnotation = Message<"google.cloud.videointelligence.v1p3beta1.FaceDetectionAnnotation"> & {
  /**
   * The face tracks with attributes.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.Track tracks = 3;
   */
  tracks: Track[];

  /**
   * The thumbnail of a person's face.
   *
   * @generated from field: bytes thumbnail = 4;
   */
  thumbnail: Uint8Array;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.FaceDetectionAnnotation.
 * Use `create(FaceDetectionAnnotationSchema)` to create a new message.
 */
export const FaceDetectionAnnotationSchema: GenMessage<FaceDetectionAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 24);

/**
 * Person detection annotation per video.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.PersonDetectionAnnotation
 */
export type PersonDetectionAnnotation = Message<"google.cloud.videointelligence.v1p3beta1.PersonDetectionAnnotation"> & {
  /**
   * The detected tracks of a person.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.Track tracks = 1;
   */
  tracks: Track[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.PersonDetectionAnnotation.
 * Use `create(PersonDetectionAnnotationSchema)` to create a new message.
 */
export const PersonDetectionAnnotationSchema: GenMessage<PersonDetectionAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 25);

/**
 * Annotation results for a single video.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.VideoAnnotationResults
 */
export type VideoAnnotationResults = Message<"google.cloud.videointelligence.v1p3beta1.VideoAnnotationResults"> & {
  /**
   * Video file location in
   * [Cloud Storage](https://cloud.google.com/storage/).
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri: string;

  /**
   * Video segment on which the annotation is run.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.VideoSegment segment = 10;
   */
  segment?: VideoSegment;

  /**
   * Topical label annotations on video level or user-specified segment level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.LabelAnnotation segment_label_annotations = 2;
   */
  segmentLabelAnnotations: LabelAnnotation[];

  /**
   * Presence label annotations on video level or user-specified segment level.
   * There is exactly one element for each unique label. Compared to the
   * existing topical `segment_label_annotations`, this field presents more
   * fine-grained, segment-level labels detected in video content and is made
   * available only when the client sets `LabelDetectionConfig.model` to
   * "builtin/latest" in the request.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.LabelAnnotation segment_presence_label_annotations = 23;
   */
  segmentPresenceLabelAnnotations: LabelAnnotation[];

  /**
   * Topical label annotations on shot level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.LabelAnnotation shot_label_annotations = 3;
   */
  shotLabelAnnotations: LabelAnnotation[];

  /**
   * Presence label annotations on shot level. There is exactly one element for
   * each unique label. Compared to the existing topical
   * `shot_label_annotations`, this field presents more fine-grained, shot-level
   * labels detected in video content and is made available only when the client
   * sets `LabelDetectionConfig.model` to "builtin/latest" in the request.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.LabelAnnotation shot_presence_label_annotations = 24;
   */
  shotPresenceLabelAnnotations: LabelAnnotation[];

  /**
   * Label annotations on frame level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.LabelAnnotation frame_label_annotations = 4;
   */
  frameLabelAnnotations: LabelAnnotation[];

  /**
   * Face detection annotations.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.FaceDetectionAnnotation face_detection_annotations = 13;
   */
  faceDetectionAnnotations: FaceDetectionAnnotation[];

  /**
   * Shot annotations. Each shot is represented as a video segment.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.VideoSegment shot_annotations = 6;
   */
  shotAnnotations: VideoSegment[];

  /**
   * Explicit content annotation.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.ExplicitContentAnnotation explicit_annotation = 7;
   */
  explicitAnnotation?: ExplicitContentAnnotation;

  /**
   * Speech transcription.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.SpeechTranscription speech_transcriptions = 11;
   */
  speechTranscriptions: SpeechTranscription[];

  /**
   * OCR text detection and tracking.
   * Annotations for list of detected text snippets. Each will have list of
   * frame information associated with it.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.TextAnnotation text_annotations = 12;
   */
  textAnnotations: TextAnnotation[];

  /**
   * Annotations for list of objects detected and tracked in video.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.ObjectTrackingAnnotation object_annotations = 14;
   */
  objectAnnotations: ObjectTrackingAnnotation[];

  /**
   * Annotations for list of logos detected, tracked and recognized in video.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.LogoRecognitionAnnotation logo_recognition_annotations = 19;
   */
  logoRecognitionAnnotations: LogoRecognitionAnnotation[];

  /**
   * Person detection annotations.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.PersonDetectionAnnotation person_detection_annotations = 20;
   */
  personDetectionAnnotations: PersonDetectionAnnotation[];

  /**
   * Celebrity recognition annotations.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.CelebrityRecognitionAnnotation celebrity_recognition_annotations = 21;
   */
  celebrityRecognitionAnnotations?: CelebrityRecognitionAnnotation;

  /**
   * If set, indicates an error. Note that for a single `AnnotateVideoRequest`
   * some videos may succeed and some may fail.
   *
   * @generated from field: google.rpc.Status error = 9;
   */
  error?: Status;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.VideoAnnotationResults.
 * Use `create(VideoAnnotationResultsSchema)` to create a new message.
 */
export const VideoAnnotationResultsSchema: GenMessage<VideoAnnotationResults> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 26);

/**
 * Video annotation response. Included in the `response`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.AnnotateVideoResponse
 */
export type AnnotateVideoResponse = Message<"google.cloud.videointelligence.v1p3beta1.AnnotateVideoResponse"> & {
  /**
   * Annotation results for all videos specified in `AnnotateVideoRequest`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.VideoAnnotationResults annotation_results = 1;
   */
  annotationResults: VideoAnnotationResults[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.AnnotateVideoResponse.
 * Use `create(AnnotateVideoResponseSchema)` to create a new message.
 */
export const AnnotateVideoResponseSchema: GenMessage<AnnotateVideoResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 27);

/**
 * Annotation progress for a single video.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.VideoAnnotationProgress
 */
export type VideoAnnotationProgress = Message<"google.cloud.videointelligence.v1p3beta1.VideoAnnotationProgress"> & {
  /**
   * Video file location in
   * [Cloud Storage](https://cloud.google.com/storage/).
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri: string;

  /**
   * Approximate percentage processed thus far. Guaranteed to be
   * 100 when fully processed.
   *
   * @generated from field: int32 progress_percent = 2;
   */
  progressPercent: number;

  /**
   * Time when the request was received.
   *
   * @generated from field: google.protobuf.Timestamp start_time = 3;
   */
  startTime?: Timestamp;

  /**
   * Time of the most recent update.
   *
   * @generated from field: google.protobuf.Timestamp update_time = 4;
   */
  updateTime?: Timestamp;

  /**
   * Specifies which feature is being tracked if the request contains more than
   * one feature.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.Feature feature = 5;
   */
  feature: Feature;

  /**
   * Specifies which segment is being tracked if the request contains more than
   * one segment.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.VideoSegment segment = 6;
   */
  segment?: VideoSegment;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.VideoAnnotationProgress.
 * Use `create(VideoAnnotationProgressSchema)` to create a new message.
 */
export const VideoAnnotationProgressSchema: GenMessage<VideoAnnotationProgress> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 28);

/**
 * Video annotation progress. Included in the `metadata`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.AnnotateVideoProgress
 */
export type AnnotateVideoProgress = Message<"google.cloud.videointelligence.v1p3beta1.AnnotateVideoProgress"> & {
  /**
   * Progress metadata for all videos specified in `AnnotateVideoRequest`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.VideoAnnotationProgress annotation_progress = 1;
   */
  annotationProgress: VideoAnnotationProgress[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.AnnotateVideoProgress.
 * Use `create(AnnotateVideoProgressSchema)` to create a new message.
 */
export const AnnotateVideoProgressSchema: GenMessage<AnnotateVideoProgress> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 29);

/**
 * Config for SPEECH_TRANSCRIPTION.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.SpeechTranscriptionConfig
 */
export type SpeechTranscriptionConfig = Message<"google.cloud.videointelligence.v1p3beta1.SpeechTranscriptionConfig"> & {
  /**
   * Required. *Required* The language of the supplied audio as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Example: "en-US".
   * See [Language Support](https://cloud.google.com/speech/docs/languages)
   * for a list of the currently supported language codes.
   *
   * @generated from field: string language_code = 1;
   */
  languageCode: string;

  /**
   * Optional. Maximum number of recognition hypotheses to be returned.
   * Specifically, the maximum number of `SpeechRecognitionAlternative` messages
   * within each `SpeechTranscription`. The server may return fewer than
   * `max_alternatives`. Valid values are `0`-`30`. A value of `0` or `1` will
   * return a maximum of one. If omitted, will return a maximum of one.
   *
   * @generated from field: int32 max_alternatives = 2;
   */
  maxAlternatives: number;

  /**
   * Optional. If set to `true`, the server will attempt to filter out
   * profanities, replacing all but the initial character in each filtered word
   * with asterisks, e.g. "f***". If set to `false` or omitted, profanities
   * won't be filtered out.
   *
   * @generated from field: bool filter_profanity = 3;
   */
  filterProfanity: boolean;

  /**
   * Optional. A means to provide context to assist the speech recognition.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.SpeechContext speech_contexts = 4;
   */
  speechContexts: SpeechContext[];

  /**
   * Optional. If 'true', adds punctuation to recognition result hypotheses.
   * This feature is only available in select languages. Setting this for
   * requests in other languages has no effect at all. The default 'false' value
   * does not add punctuation to result hypotheses. NOTE: "This is currently
   * offered as an experimental service, complimentary to all users. In the
   * future this may be exclusively available as a premium feature."
   *
   * @generated from field: bool enable_automatic_punctuation = 5;
   */
  enableAutomaticPunctuation: boolean;

  /**
   * Optional. For file formats, such as MXF or MKV, supporting multiple audio
   * tracks, specify up to two tracks. Default: track 0.
   *
   * @generated from field: repeated int32 audio_tracks = 6;
   */
  audioTracks: number[];

  /**
   * Optional. If 'true', enables speaker detection for each recognized word in
   * the top alternative of the recognition result using a speaker_tag provided
   * in the WordInfo.
   * Note: When this is true, we send all the words from the beginning of the
   * audio for the top alternative in every consecutive response.
   * This is done in order to improve our speaker tags as our models learn to
   * identify the speakers in the conversation over time.
   *
   * @generated from field: bool enable_speaker_diarization = 7;
   */
  enableSpeakerDiarization: boolean;

  /**
   * Optional. If set, specifies the estimated number of speakers in the
   * conversation. If not set, defaults to '2'. Ignored unless
   * enable_speaker_diarization is set to true.
   *
   * @generated from field: int32 diarization_speaker_count = 8;
   */
  diarizationSpeakerCount: number;

  /**
   * Optional. If `true`, the top result includes a list of words and the
   * confidence for those words. If `false`, no word-level confidence
   * information is returned. The default is `false`.
   *
   * @generated from field: bool enable_word_confidence = 9;
   */
  enableWordConfidence: boolean;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.SpeechTranscriptionConfig.
 * Use `create(SpeechTranscriptionConfigSchema)` to create a new message.
 */
export const SpeechTranscriptionConfigSchema: GenMessage<SpeechTranscriptionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 30);

/**
 * Provides "hints" to the speech recognizer to favor specific words and phrases
 * in the results.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.SpeechContext
 */
export type SpeechContext = Message<"google.cloud.videointelligence.v1p3beta1.SpeechContext"> & {
  /**
   * Optional. A list of strings containing words and phrases "hints" so that
   * the speech recognition is more likely to recognize them. This can be used
   * to improve the accuracy for specific words and phrases, for example, if
   * specific commands are typically spoken by the user. This can also be used
   * to add additional words to the vocabulary of the recognizer. See
   * [usage limits](https://cloud.google.com/speech/limits#content).
   *
   * @generated from field: repeated string phrases = 1;
   */
  phrases: string[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.SpeechContext.
 * Use `create(SpeechContextSchema)` to create a new message.
 */
export const SpeechContextSchema: GenMessage<SpeechContext> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 31);

/**
 * A speech recognition result corresponding to a portion of the audio.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.SpeechTranscription
 */
export type SpeechTranscription = Message<"google.cloud.videointelligence.v1p3beta1.SpeechTranscription"> & {
  /**
   * May contain one or more recognition hypotheses (up to the maximum specified
   * in `max_alternatives`).  These alternatives are ordered in terms of
   * accuracy, with the top (first) alternative being the most probable, as
   * ranked by the recognizer.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.SpeechRecognitionAlternative alternatives = 1;
   */
  alternatives: SpeechRecognitionAlternative[];

  /**
   * Output only. The [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt)
   * language tag of the language in this result. This language code was
   * detected to have the most likelihood of being spoken in the audio.
   *
   * @generated from field: string language_code = 2;
   */
  languageCode: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.SpeechTranscription.
 * Use `create(SpeechTranscriptionSchema)` to create a new message.
 */
export const SpeechTranscriptionSchema: GenMessage<SpeechTranscription> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 32);

/**
 * Alternative hypotheses (a.k.a. n-best list).
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.SpeechRecognitionAlternative
 */
export type SpeechRecognitionAlternative = Message<"google.cloud.videointelligence.v1p3beta1.SpeechRecognitionAlternative"> & {
  /**
   * Transcript text representing the words that the user spoke.
   *
   * @generated from field: string transcript = 1;
   */
  transcript: string;

  /**
   * Output only. The confidence estimate between 0.0 and 1.0. A higher number
   * indicates an estimated greater likelihood that the recognized words are
   * correct. This field is set only for the top alternative.
   * This field is not guaranteed to be accurate and users should not rely on it
   * to be always provided.
   * The default of 0.0 is a sentinel value indicating `confidence` was not set.
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;

  /**
   * Output only. A list of word-specific information for each recognized word.
   * Note: When `enable_speaker_diarization` is set to true, you will see all
   * the words from the beginning of the audio.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.WordInfo words = 3;
   */
  words: WordInfo[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.SpeechRecognitionAlternative.
 * Use `create(SpeechRecognitionAlternativeSchema)` to create a new message.
 */
export const SpeechRecognitionAlternativeSchema: GenMessage<SpeechRecognitionAlternative> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 33);

/**
 * Word-specific information for recognized words. Word information is only
 * included in the response when certain request parameters are set, such
 * as `enable_word_time_offsets`.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.WordInfo
 */
export type WordInfo = Message<"google.cloud.videointelligence.v1p3beta1.WordInfo"> & {
  /**
   * Time offset relative to the beginning of the audio, and
   * corresponding to the start of the spoken word. This field is only set if
   * `enable_word_time_offsets=true` and only in the top hypothesis. This is an
   * experimental feature and the accuracy of the time offset can vary.
   *
   * @generated from field: google.protobuf.Duration start_time = 1;
   */
  startTime?: Duration;

  /**
   * Time offset relative to the beginning of the audio, and
   * corresponding to the end of the spoken word. This field is only set if
   * `enable_word_time_offsets=true` and only in the top hypothesis. This is an
   * experimental feature and the accuracy of the time offset can vary.
   *
   * @generated from field: google.protobuf.Duration end_time = 2;
   */
  endTime?: Duration;

  /**
   * The word corresponding to this set of information.
   *
   * @generated from field: string word = 3;
   */
  word: string;

  /**
   * Output only. The confidence estimate between 0.0 and 1.0. A higher number
   * indicates an estimated greater likelihood that the recognized words are
   * correct. This field is set only for the top alternative.
   * This field is not guaranteed to be accurate and users should not rely on it
   * to be always provided.
   * The default of 0.0 is a sentinel value indicating `confidence` was not set.
   *
   * @generated from field: float confidence = 4;
   */
  confidence: number;

  /**
   * Output only. A distinct integer value is assigned for every speaker within
   * the audio. This field specifies which one of those speakers was detected to
   * have spoken this word. Value ranges from 1 up to diarization_speaker_count,
   * and is only set if speaker diarization is enabled.
   *
   * @generated from field: int32 speaker_tag = 5;
   */
  speakerTag: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.WordInfo.
 * Use `create(WordInfoSchema)` to create a new message.
 */
export const WordInfoSchema: GenMessage<WordInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 34);

/**
 * A vertex represents a 2D point in the image.
 * NOTE: the normalized vertex coordinates are relative to the original image
 * and range from 0 to 1.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.NormalizedVertex
 */
export type NormalizedVertex = Message<"google.cloud.videointelligence.v1p3beta1.NormalizedVertex"> & {
  /**
   * X coordinate.
   *
   * @generated from field: float x = 1;
   */
  x: number;

  /**
   * Y coordinate.
   *
   * @generated from field: float y = 2;
   */
  y: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.NormalizedVertex.
 * Use `create(NormalizedVertexSchema)` to create a new message.
 */
export const NormalizedVertexSchema: GenMessage<NormalizedVertex> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 35);

/**
 * Normalized bounding polygon for text (that might not be aligned with axis).
 * Contains list of the corner points in clockwise order starting from
 * top-left corner. For example, for a rectangular bounding box:
 * When the text is horizontal it might look like:
 *         0----1
 *         |    |
 *         3----2
 *
 * When it's clockwise rotated 180 degrees around the top-left corner it
 * becomes:
 *         2----3
 *         |    |
 *         1----0
 *
 * and the vertex order will still be (0, 1, 2, 3). Note that values can be less
 * than 0, or greater than 1 due to trignometric calculations for location of
 * the box.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.NormalizedBoundingPoly
 */
export type NormalizedBoundingPoly = Message<"google.cloud.videointelligence.v1p3beta1.NormalizedBoundingPoly"> & {
  /**
   * Normalized vertices of the bounding polygon.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.NormalizedVertex vertices = 1;
   */
  vertices: NormalizedVertex[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.NormalizedBoundingPoly.
 * Use `create(NormalizedBoundingPolySchema)` to create a new message.
 */
export const NormalizedBoundingPolySchema: GenMessage<NormalizedBoundingPoly> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 36);

/**
 * Video segment level annotation results for text detection.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.TextSegment
 */
export type TextSegment = Message<"google.cloud.videointelligence.v1p3beta1.TextSegment"> & {
  /**
   * Video segment where a text snippet was detected.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.VideoSegment segment = 1;
   */
  segment?: VideoSegment;

  /**
   * Confidence for the track of detected text. It is calculated as the highest
   * over all frames where OCR detected text appears.
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;

  /**
   * Information related to the frames where OCR detected text appears.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.TextFrame frames = 3;
   */
  frames: TextFrame[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.TextSegment.
 * Use `create(TextSegmentSchema)` to create a new message.
 */
export const TextSegmentSchema: GenMessage<TextSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 37);

/**
 * Video frame level annotation results for text annotation (OCR).
 * Contains information regarding timestamp and bounding box locations for the
 * frames containing detected OCR text snippets.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.TextFrame
 */
export type TextFrame = Message<"google.cloud.videointelligence.v1p3beta1.TextFrame"> & {
  /**
   * Bounding polygon of the detected text for this frame.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.NormalizedBoundingPoly rotated_bounding_box = 1;
   */
  rotatedBoundingBox?: NormalizedBoundingPoly;

  /**
   * Timestamp of this frame.
   *
   * @generated from field: google.protobuf.Duration time_offset = 2;
   */
  timeOffset?: Duration;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.TextFrame.
 * Use `create(TextFrameSchema)` to create a new message.
 */
export const TextFrameSchema: GenMessage<TextFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 38);

/**
 * Annotations related to one detected OCR text snippet. This will contain the
 * corresponding text, confidence value, and frame level information for each
 * detection.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.TextAnnotation
 */
export type TextAnnotation = Message<"google.cloud.videointelligence.v1p3beta1.TextAnnotation"> & {
  /**
   * The detected text.
   *
   * @generated from field: string text = 1;
   */
  text: string;

  /**
   * All video segments where OCR detected text appears.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.TextSegment segments = 2;
   */
  segments: TextSegment[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.TextAnnotation.
 * Use `create(TextAnnotationSchema)` to create a new message.
 */
export const TextAnnotationSchema: GenMessage<TextAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 39);

/**
 * Video frame level annotations for object detection and tracking. This field
 * stores per frame location, time offset, and confidence.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.ObjectTrackingFrame
 */
export type ObjectTrackingFrame = Message<"google.cloud.videointelligence.v1p3beta1.ObjectTrackingFrame"> & {
  /**
   * The normalized bounding box location of this object track for the frame.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.NormalizedBoundingBox normalized_bounding_box = 1;
   */
  normalizedBoundingBox?: NormalizedBoundingBox;

  /**
   * The timestamp of the frame in microseconds.
   *
   * @generated from field: google.protobuf.Duration time_offset = 2;
   */
  timeOffset?: Duration;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.ObjectTrackingFrame.
 * Use `create(ObjectTrackingFrameSchema)` to create a new message.
 */
export const ObjectTrackingFrameSchema: GenMessage<ObjectTrackingFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 40);

/**
 * Annotations corresponding to one tracked object.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.ObjectTrackingAnnotation
 */
export type ObjectTrackingAnnotation = Message<"google.cloud.videointelligence.v1p3beta1.ObjectTrackingAnnotation"> & {
  /**
   * Different representation of tracking info in non-streaming batch
   * and streaming modes.
   *
   * @generated from oneof google.cloud.videointelligence.v1p3beta1.ObjectTrackingAnnotation.track_info
   */
  trackInfo: {
    /**
     * Non-streaming batch mode ONLY.
     * Each object track corresponds to one video segment where it appears.
     *
     * @generated from field: google.cloud.videointelligence.v1p3beta1.VideoSegment segment = 3;
     */
    value: VideoSegment;
    case: "segment";
  } | {
    /**
     * Streaming mode ONLY.
     * In streaming mode, we do not know the end time of a tracked object
     * before it is completed. Hence, there is no VideoSegment info returned.
     * Instead, we provide a unique identifiable integer track_id so that
     * the customers can correlate the results of the ongoing
     * ObjectTrackAnnotation of the same track_id over time.
     *
     * @generated from field: int64 track_id = 5;
     */
    value: bigint;
    case: "trackId";
  } | { case: undefined; value?: undefined };

  /**
   * Entity to specify the object category that this track is labeled as.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.Entity entity = 1;
   */
  entity?: Entity;

  /**
   * Object category's labeling confidence of this track.
   *
   * @generated from field: float confidence = 4;
   */
  confidence: number;

  /**
   * Information corresponding to all frames where this object track appears.
   * Non-streaming batch mode: it may be one or multiple ObjectTrackingFrame
   * messages in frames.
   * Streaming mode: it can only be one ObjectTrackingFrame message in frames.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.ObjectTrackingFrame frames = 2;
   */
  frames: ObjectTrackingFrame[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.ObjectTrackingAnnotation.
 * Use `create(ObjectTrackingAnnotationSchema)` to create a new message.
 */
export const ObjectTrackingAnnotationSchema: GenMessage<ObjectTrackingAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 41);

/**
 * Annotation corresponding to one detected, tracked and recognized logo class.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.LogoRecognitionAnnotation
 */
export type LogoRecognitionAnnotation = Message<"google.cloud.videointelligence.v1p3beta1.LogoRecognitionAnnotation"> & {
  /**
   * Entity category information to specify the logo class that all the logo
   * tracks within this LogoRecognitionAnnotation are recognized as.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.Entity entity = 1;
   */
  entity?: Entity;

  /**
   * All logo tracks where the recognized logo appears. Each track corresponds
   * to one logo instance appearing in consecutive frames.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.Track tracks = 2;
   */
  tracks: Track[];

  /**
   * All video segments where the recognized logo appears. There might be
   * multiple instances of the same logo class appearing in one VideoSegment.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.VideoSegment segments = 3;
   */
  segments: VideoSegment[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.LogoRecognitionAnnotation.
 * Use `create(LogoRecognitionAnnotationSchema)` to create a new message.
 */
export const LogoRecognitionAnnotationSchema: GenMessage<LogoRecognitionAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 42);

/**
 * The top-level message sent by the client for the `StreamingAnnotateVideo`
 * method. Multiple `StreamingAnnotateVideoRequest` messages are sent.
 * The first message must only contain a `StreamingVideoConfig` message.
 * All subsequent messages must only contain `input_content` data.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.StreamingAnnotateVideoRequest
 */
export type StreamingAnnotateVideoRequest = Message<"google.cloud.videointelligence.v1p3beta1.StreamingAnnotateVideoRequest"> & {
  /**
   * *Required* The streaming request, which is either a streaming config or
   * video content.
   *
   * @generated from oneof google.cloud.videointelligence.v1p3beta1.StreamingAnnotateVideoRequest.streaming_request
   */
  streamingRequest: {
    /**
     * Provides information to the annotator, specifing how to process the
     * request. The first `AnnotateStreamingVideoRequest` message must only
     * contain a `video_config` message.
     *
     * @generated from field: google.cloud.videointelligence.v1p3beta1.StreamingVideoConfig video_config = 1;
     */
    value: StreamingVideoConfig;
    case: "videoConfig";
  } | {
    /**
     * The video data to be annotated. Chunks of video data are sequentially
     * sent in `StreamingAnnotateVideoRequest` messages. Except the initial
     * `StreamingAnnotateVideoRequest` message containing only
     * `video_config`, all subsequent `AnnotateStreamingVideoRequest`
     * messages must only contain `input_content` field.
     * Note: as with all bytes fields, protobuffers use a pure binary
     * representation (not base64).
     *
     * @generated from field: bytes input_content = 2;
     */
    value: Uint8Array;
    case: "inputContent";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.StreamingAnnotateVideoRequest.
 * Use `create(StreamingAnnotateVideoRequestSchema)` to create a new message.
 */
export const StreamingAnnotateVideoRequestSchema: GenMessage<StreamingAnnotateVideoRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 43);

/**
 * Provides information to the annotator that specifies how to process the
 * request.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.StreamingVideoConfig
 */
export type StreamingVideoConfig = Message<"google.cloud.videointelligence.v1p3beta1.StreamingVideoConfig"> & {
  /**
   * Config for requested annotation feature.
   *
   * @generated from oneof google.cloud.videointelligence.v1p3beta1.StreamingVideoConfig.streaming_config
   */
  streamingConfig: {
    /**
     * Config for STREAMING_SHOT_CHANGE_DETECTION.
     *
     * @generated from field: google.cloud.videointelligence.v1p3beta1.StreamingShotChangeDetectionConfig shot_change_detection_config = 2;
     */
    value: StreamingShotChangeDetectionConfig;
    case: "shotChangeDetectionConfig";
  } | {
    /**
     * Config for STREAMING_LABEL_DETECTION.
     *
     * @generated from field: google.cloud.videointelligence.v1p3beta1.StreamingLabelDetectionConfig label_detection_config = 3;
     */
    value: StreamingLabelDetectionConfig;
    case: "labelDetectionConfig";
  } | {
    /**
     * Config for STREAMING_EXPLICIT_CONTENT_DETECTION.
     *
     * @generated from field: google.cloud.videointelligence.v1p3beta1.StreamingExplicitContentDetectionConfig explicit_content_detection_config = 4;
     */
    value: StreamingExplicitContentDetectionConfig;
    case: "explicitContentDetectionConfig";
  } | {
    /**
     * Config for STREAMING_OBJECT_TRACKING.
     *
     * @generated from field: google.cloud.videointelligence.v1p3beta1.StreamingObjectTrackingConfig object_tracking_config = 5;
     */
    value: StreamingObjectTrackingConfig;
    case: "objectTrackingConfig";
  } | {
    /**
     * Config for STREAMING_AUTOML_ACTION_RECOGNITION.
     *
     * @generated from field: google.cloud.videointelligence.v1p3beta1.StreamingAutomlActionRecognitionConfig automl_action_recognition_config = 23;
     */
    value: StreamingAutomlActionRecognitionConfig;
    case: "automlActionRecognitionConfig";
  } | {
    /**
     * Config for STREAMING_AUTOML_CLASSIFICATION.
     *
     * @generated from field: google.cloud.videointelligence.v1p3beta1.StreamingAutomlClassificationConfig automl_classification_config = 21;
     */
    value: StreamingAutomlClassificationConfig;
    case: "automlClassificationConfig";
  } | {
    /**
     * Config for STREAMING_AUTOML_OBJECT_TRACKING.
     *
     * @generated from field: google.cloud.videointelligence.v1p3beta1.StreamingAutomlObjectTrackingConfig automl_object_tracking_config = 22;
     */
    value: StreamingAutomlObjectTrackingConfig;
    case: "automlObjectTrackingConfig";
  } | { case: undefined; value?: undefined };

  /**
   * Requested annotation feature.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.StreamingFeature feature = 1;
   */
  feature: StreamingFeature;

  /**
   * Streaming storage option. By default: storage is disabled.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.StreamingStorageConfig storage_config = 30;
   */
  storageConfig?: StreamingStorageConfig;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.StreamingVideoConfig.
 * Use `create(StreamingVideoConfigSchema)` to create a new message.
 */
export const StreamingVideoConfigSchema: GenMessage<StreamingVideoConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 44);

/**
 * `StreamingAnnotateVideoResponse` is the only message returned to the client
 * by `StreamingAnnotateVideo`. A series of zero or more
 * `StreamingAnnotateVideoResponse` messages are streamed back to the client.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.StreamingAnnotateVideoResponse
 */
export type StreamingAnnotateVideoResponse = Message<"google.cloud.videointelligence.v1p3beta1.StreamingAnnotateVideoResponse"> & {
  /**
   * If set, returns a [google.rpc.Status][google.rpc.Status] message that
   * specifies the error for the operation.
   *
   * @generated from field: google.rpc.Status error = 1;
   */
  error?: Status;

  /**
   * Streaming annotation results.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.StreamingVideoAnnotationResults annotation_results = 2;
   */
  annotationResults?: StreamingVideoAnnotationResults;

  /**
   * Google Cloud Storage(GCS) URI that stores annotation results of one
   * streaming session in JSON format.
   * It is the annotation_result_storage_directory
   * from the request followed by '/cloud_project_number-session_id'.
   *
   * @generated from field: string annotation_results_uri = 3;
   */
  annotationResultsUri: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.StreamingAnnotateVideoResponse.
 * Use `create(StreamingAnnotateVideoResponseSchema)` to create a new message.
 */
export const StreamingAnnotateVideoResponseSchema: GenMessage<StreamingAnnotateVideoResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 45);

/**
 * Streaming annotation results corresponding to a portion of the video
 * that is currently being processed.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.StreamingVideoAnnotationResults
 */
export type StreamingVideoAnnotationResults = Message<"google.cloud.videointelligence.v1p3beta1.StreamingVideoAnnotationResults"> & {
  /**
   * Shot annotation results. Each shot is represented as a video segment.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.VideoSegment shot_annotations = 1;
   */
  shotAnnotations: VideoSegment[];

  /**
   * Label annotation results.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.LabelAnnotation label_annotations = 2;
   */
  labelAnnotations: LabelAnnotation[];

  /**
   * Explicit content annotation results.
   *
   * @generated from field: google.cloud.videointelligence.v1p3beta1.ExplicitContentAnnotation explicit_annotation = 3;
   */
  explicitAnnotation?: ExplicitContentAnnotation;

  /**
   * Object tracking results.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1p3beta1.ObjectTrackingAnnotation object_annotations = 4;
   */
  objectAnnotations: ObjectTrackingAnnotation[];
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.StreamingVideoAnnotationResults.
 * Use `create(StreamingVideoAnnotationResultsSchema)` to create a new message.
 */
export const StreamingVideoAnnotationResultsSchema: GenMessage<StreamingVideoAnnotationResults> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 46);

/**
 * Config for STREAMING_SHOT_CHANGE_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.StreamingShotChangeDetectionConfig
 */
export type StreamingShotChangeDetectionConfig = Message<"google.cloud.videointelligence.v1p3beta1.StreamingShotChangeDetectionConfig"> & {
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.StreamingShotChangeDetectionConfig.
 * Use `create(StreamingShotChangeDetectionConfigSchema)` to create a new message.
 */
export const StreamingShotChangeDetectionConfigSchema: GenMessage<StreamingShotChangeDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 47);

/**
 * Config for STREAMING_LABEL_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.StreamingLabelDetectionConfig
 */
export type StreamingLabelDetectionConfig = Message<"google.cloud.videointelligence.v1p3beta1.StreamingLabelDetectionConfig"> & {
  /**
   * Whether the video has been captured from a stationary (i.e. non-moving)
   * camera. When set to true, might improve detection accuracy for moving
   * objects. Default: false.
   *
   * @generated from field: bool stationary_camera = 1;
   */
  stationaryCamera: boolean;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.StreamingLabelDetectionConfig.
 * Use `create(StreamingLabelDetectionConfigSchema)` to create a new message.
 */
export const StreamingLabelDetectionConfigSchema: GenMessage<StreamingLabelDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 48);

/**
 * Config for STREAMING_EXPLICIT_CONTENT_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.StreamingExplicitContentDetectionConfig
 */
export type StreamingExplicitContentDetectionConfig = Message<"google.cloud.videointelligence.v1p3beta1.StreamingExplicitContentDetectionConfig"> & {
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.StreamingExplicitContentDetectionConfig.
 * Use `create(StreamingExplicitContentDetectionConfigSchema)` to create a new message.
 */
export const StreamingExplicitContentDetectionConfigSchema: GenMessage<StreamingExplicitContentDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 49);

/**
 * Config for STREAMING_OBJECT_TRACKING.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.StreamingObjectTrackingConfig
 */
export type StreamingObjectTrackingConfig = Message<"google.cloud.videointelligence.v1p3beta1.StreamingObjectTrackingConfig"> & {
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.StreamingObjectTrackingConfig.
 * Use `create(StreamingObjectTrackingConfigSchema)` to create a new message.
 */
export const StreamingObjectTrackingConfigSchema: GenMessage<StreamingObjectTrackingConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 50);

/**
 * Config for STREAMING_AUTOML_ACTION_RECOGNITION.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.StreamingAutomlActionRecognitionConfig
 */
export type StreamingAutomlActionRecognitionConfig = Message<"google.cloud.videointelligence.v1p3beta1.StreamingAutomlActionRecognitionConfig"> & {
  /**
   * Resource name of AutoML model.
   * Format: `projects/{project_id}/locations/{location_id}/models/{model_id}`
   *
   * @generated from field: string model_name = 1;
   */
  modelName: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.StreamingAutomlActionRecognitionConfig.
 * Use `create(StreamingAutomlActionRecognitionConfigSchema)` to create a new message.
 */
export const StreamingAutomlActionRecognitionConfigSchema: GenMessage<StreamingAutomlActionRecognitionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 51);

/**
 * Config for STREAMING_AUTOML_CLASSIFICATION.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.StreamingAutomlClassificationConfig
 */
export type StreamingAutomlClassificationConfig = Message<"google.cloud.videointelligence.v1p3beta1.StreamingAutomlClassificationConfig"> & {
  /**
   * Resource name of AutoML model.
   * Format:
   * `projects/{project_number}/locations/{location_id}/models/{model_id}`
   *
   * @generated from field: string model_name = 1;
   */
  modelName: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.StreamingAutomlClassificationConfig.
 * Use `create(StreamingAutomlClassificationConfigSchema)` to create a new message.
 */
export const StreamingAutomlClassificationConfigSchema: GenMessage<StreamingAutomlClassificationConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 52);

/**
 * Config for STREAMING_AUTOML_OBJECT_TRACKING.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.StreamingAutomlObjectTrackingConfig
 */
export type StreamingAutomlObjectTrackingConfig = Message<"google.cloud.videointelligence.v1p3beta1.StreamingAutomlObjectTrackingConfig"> & {
  /**
   * Resource name of AutoML model.
   * Format: `projects/{project_id}/locations/{location_id}/models/{model_id}`
   *
   * @generated from field: string model_name = 1;
   */
  modelName: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.StreamingAutomlObjectTrackingConfig.
 * Use `create(StreamingAutomlObjectTrackingConfigSchema)` to create a new message.
 */
export const StreamingAutomlObjectTrackingConfigSchema: GenMessage<StreamingAutomlObjectTrackingConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 53);

/**
 * Config for streaming storage option.
 *
 * @generated from message google.cloud.videointelligence.v1p3beta1.StreamingStorageConfig
 */
export type StreamingStorageConfig = Message<"google.cloud.videointelligence.v1p3beta1.StreamingStorageConfig"> & {
  /**
   * Enable streaming storage. Default: false.
   *
   * @generated from field: bool enable_storage_annotation_result = 1;
   */
  enableStorageAnnotationResult: boolean;

  /**
   * Cloud Storage URI to store all annotation results for one client. Client
   * should specify this field as the top-level storage directory. Annotation
   * results of different sessions will be put into different sub-directories
   * denoted by project_name and session_id. All sub-directories will be auto
   * generated by program and will be made accessible to client in response
   * proto. URIs must be specified in the following format:
   * `gs://bucket-id/object-id` `bucket-id` should be a valid Cloud Storage
   * bucket created by client and bucket permission shall also be configured
   * properly. `object-id` can be arbitrary string that make sense to client.
   * Other URI formats will return error and cause Cloud Storage write failure.
   *
   * @generated from field: string annotation_result_storage_directory = 3;
   */
  annotationResultStorageDirectory: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1p3beta1.StreamingStorageConfig.
 * Use `create(StreamingStorageConfigSchema)` to create a new message.
 */
export const StreamingStorageConfigSchema: GenMessage<StreamingStorageConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 54);

/**
 * Label detection mode.
 *
 * @generated from enum google.cloud.videointelligence.v1p3beta1.LabelDetectionMode
 */
export enum LabelDetectionMode {
  /**
   * Unspecified.
   *
   * @generated from enum value: LABEL_DETECTION_MODE_UNSPECIFIED = 0;
   */
  LABEL_DETECTION_MODE_UNSPECIFIED = 0,

  /**
   * Detect shot-level labels.
   *
   * @generated from enum value: SHOT_MODE = 1;
   */
  SHOT_MODE = 1,

  /**
   * Detect frame-level labels.
   *
   * @generated from enum value: FRAME_MODE = 2;
   */
  FRAME_MODE = 2,

  /**
   * Detect both shot-level and frame-level labels.
   *
   * @generated from enum value: SHOT_AND_FRAME_MODE = 3;
   */
  SHOT_AND_FRAME_MODE = 3,
}

/**
 * Describes the enum google.cloud.videointelligence.v1p3beta1.LabelDetectionMode.
 */
export const LabelDetectionModeSchema: GenEnum<LabelDetectionMode> = /*@__PURE__*/
  enumDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 0);

/**
 * Bucketized representation of likelihood.
 *
 * @generated from enum google.cloud.videointelligence.v1p3beta1.Likelihood
 */
export enum Likelihood {
  /**
   * Unspecified likelihood.
   *
   * @generated from enum value: LIKELIHOOD_UNSPECIFIED = 0;
   */
  LIKELIHOOD_UNSPECIFIED = 0,

  /**
   * Very unlikely.
   *
   * @generated from enum value: VERY_UNLIKELY = 1;
   */
  VERY_UNLIKELY = 1,

  /**
   * Unlikely.
   *
   * @generated from enum value: UNLIKELY = 2;
   */
  UNLIKELY = 2,

  /**
   * Possible.
   *
   * @generated from enum value: POSSIBLE = 3;
   */
  POSSIBLE = 3,

  /**
   * Likely.
   *
   * @generated from enum value: LIKELY = 4;
   */
  LIKELY = 4,

  /**
   * Very likely.
   *
   * @generated from enum value: VERY_LIKELY = 5;
   */
  VERY_LIKELY = 5,
}

/**
 * Describes the enum google.cloud.videointelligence.v1p3beta1.Likelihood.
 */
export const LikelihoodSchema: GenEnum<Likelihood> = /*@__PURE__*/
  enumDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 1);

/**
 * Streaming video annotation feature.
 *
 * @generated from enum google.cloud.videointelligence.v1p3beta1.StreamingFeature
 */
export enum StreamingFeature {
  /**
   * Unspecified.
   *
   * @generated from enum value: STREAMING_FEATURE_UNSPECIFIED = 0;
   */
  STREAMING_FEATURE_UNSPECIFIED = 0,

  /**
   * Label detection. Detect objects, such as dog or flower.
   *
   * @generated from enum value: STREAMING_LABEL_DETECTION = 1;
   */
  STREAMING_LABEL_DETECTION = 1,

  /**
   * Shot change detection.
   *
   * @generated from enum value: STREAMING_SHOT_CHANGE_DETECTION = 2;
   */
  STREAMING_SHOT_CHANGE_DETECTION = 2,

  /**
   * Explicit content detection.
   *
   * @generated from enum value: STREAMING_EXPLICIT_CONTENT_DETECTION = 3;
   */
  STREAMING_EXPLICIT_CONTENT_DETECTION = 3,

  /**
   * Object detection and tracking.
   *
   * @generated from enum value: STREAMING_OBJECT_TRACKING = 4;
   */
  STREAMING_OBJECT_TRACKING = 4,

  /**
   * Action recognition based on AutoML model.
   *
   * @generated from enum value: STREAMING_AUTOML_ACTION_RECOGNITION = 23;
   */
  STREAMING_AUTOML_ACTION_RECOGNITION = 23,

  /**
   * Video classification based on AutoML model.
   *
   * @generated from enum value: STREAMING_AUTOML_CLASSIFICATION = 21;
   */
  STREAMING_AUTOML_CLASSIFICATION = 21,

  /**
   * Object detection and tracking based on AutoML model.
   *
   * @generated from enum value: STREAMING_AUTOML_OBJECT_TRACKING = 22;
   */
  STREAMING_AUTOML_OBJECT_TRACKING = 22,
}

/**
 * Describes the enum google.cloud.videointelligence.v1p3beta1.StreamingFeature.
 */
export const StreamingFeatureSchema: GenEnum<StreamingFeature> = /*@__PURE__*/
  enumDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 2);

/**
 * Video annotation feature.
 *
 * @generated from enum google.cloud.videointelligence.v1p3beta1.Feature
 */
export enum Feature {
  /**
   * Unspecified.
   *
   * @generated from enum value: FEATURE_UNSPECIFIED = 0;
   */
  FEATURE_UNSPECIFIED = 0,

  /**
   * Label detection. Detect objects, such as dog or flower.
   *
   * @generated from enum value: LABEL_DETECTION = 1;
   */
  LABEL_DETECTION = 1,

  /**
   * Shot change detection.
   *
   * @generated from enum value: SHOT_CHANGE_DETECTION = 2;
   */
  SHOT_CHANGE_DETECTION = 2,

  /**
   * Explicit content detection.
   *
   * @generated from enum value: EXPLICIT_CONTENT_DETECTION = 3;
   */
  EXPLICIT_CONTENT_DETECTION = 3,

  /**
   * Human face detection.
   *
   * @generated from enum value: FACE_DETECTION = 4;
   */
  FACE_DETECTION = 4,

  /**
   * Speech transcription.
   *
   * @generated from enum value: SPEECH_TRANSCRIPTION = 6;
   */
  SPEECH_TRANSCRIPTION = 6,

  /**
   * OCR text detection and tracking.
   *
   * @generated from enum value: TEXT_DETECTION = 7;
   */
  TEXT_DETECTION = 7,

  /**
   * Object detection and tracking.
   *
   * @generated from enum value: OBJECT_TRACKING = 9;
   */
  OBJECT_TRACKING = 9,

  /**
   * Logo detection, tracking, and recognition.
   *
   * @generated from enum value: LOGO_RECOGNITION = 12;
   */
  LOGO_RECOGNITION = 12,

  /**
   * Celebrity recognition.
   *
   * @generated from enum value: CELEBRITY_RECOGNITION = 13;
   */
  CELEBRITY_RECOGNITION = 13,

  /**
   * Person detection.
   *
   * @generated from enum value: PERSON_DETECTION = 14;
   */
  PERSON_DETECTION = 14,
}

/**
 * Describes the enum google.cloud.videointelligence.v1p3beta1.Feature.
 */
export const FeatureSchema: GenEnum<Feature> = /*@__PURE__*/
  enumDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 3);

/**
 * Service that implements the Video Intelligence API.
 *
 * @generated from service google.cloud.videointelligence.v1p3beta1.VideoIntelligenceService
 */
export const VideoIntelligenceService: GenService<{
  /**
   * Performs asynchronous video annotation. Progress and results can be
   * retrieved through the `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `AnnotateVideoProgress` (progress).
   * `Operation.response` contains `AnnotateVideoResponse` (results).
   *
   * @generated from rpc google.cloud.videointelligence.v1p3beta1.VideoIntelligenceService.AnnotateVideo
   */
  annotateVideo: {
    methodKind: "unary";
    input: typeof AnnotateVideoRequestSchema;
    output: typeof OperationSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 0);

/**
 * Service that implements streaming Video Intelligence API.
 *
 * @generated from service google.cloud.videointelligence.v1p3beta1.StreamingVideoIntelligenceService
 */
export const StreamingVideoIntelligenceService: GenService<{
  /**
   * Performs video annotation with bidirectional streaming: emitting results
   * while sending video/audio bytes.
   * This method is only available via the gRPC API (not REST).
   *
   * @generated from rpc google.cloud.videointelligence.v1p3beta1.StreamingVideoIntelligenceService.StreamingAnnotateVideo
   */
  streamingAnnotateVideo: {
    methodKind: "bidi_streaming";
    input: typeof StreamingAnnotateVideoRequestSchema;
    output: typeof StreamingAnnotateVideoResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_videointelligence_v1p3beta1_video_intelligence, 1);

