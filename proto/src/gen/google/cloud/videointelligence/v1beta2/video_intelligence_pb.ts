// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/videointelligence/v1beta2/video_intelligence.proto (package google.cloud.videointelligence.v1beta2, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import type { OperationSchema } from "../../../longrunning/operations_pb";
import { file_google_longrunning_operations } from "../../../longrunning/operations_pb";
import type { Duration, Timestamp } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_duration, file_google_protobuf_timestamp } from "@bufbuild/protobuf/wkt";
import type { Status } from "../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../rpc/status_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/videointelligence/v1beta2/video_intelligence.proto.
 */
export const file_google_cloud_videointelligence_v1beta2_video_intelligence: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_longrunning_operations, file_google_protobuf_duration, file_google_protobuf_timestamp, file_google_rpc_status]);

/**
 * Video annotation request.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.AnnotateVideoRequest
 */
export type AnnotateVideoRequest = Message<"google.cloud.videointelligence.v1beta2.AnnotateVideoRequest"> & {
  /**
   * Input video location. Currently, only
   * [Google Cloud Storage](https://cloud.google.com/storage/) URIs are
   * supported, which must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
   * more information, see [Request
   * URIs](https://cloud.google.com/storage/docs/request-endpoints). A video URI
   * may include wildcards in `object-id`, and thus identify multiple videos.
   * Supported wildcards: '*' to match 0 or more characters;
   * '?' to match 1 character. If unset, the input video should be embedded
   * in the request as `input_content`. If set, `input_content` should be unset.
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri: string;

  /**
   * The video data bytes.
   * If unset, the input video(s) should be specified via `input_uri`.
   * If set, `input_uri` should be unset.
   *
   * @generated from field: bytes input_content = 6;
   */
  inputContent: Uint8Array;

  /**
   * Required. Requested video annotation features.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.Feature features = 2;
   */
  features: Feature[];

  /**
   * Additional video context and/or feature-specific parameters.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.VideoContext video_context = 3;
   */
  videoContext?: VideoContext;

  /**
   * Optional. Location where the output (in JSON format) should be stored.
   * Currently, only [Google Cloud Storage](https://cloud.google.com/storage/)
   * URIs are supported, which must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
   * more information, see [Request
   * URIs](https://cloud.google.com/storage/docs/request-endpoints).
   *
   * @generated from field: string output_uri = 4;
   */
  outputUri: string;

  /**
   * Optional. Cloud region where annotation should take place. Supported cloud
   * regions: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no region
   * is specified, a region will be determined based on video file location.
   *
   * @generated from field: string location_id = 5;
   */
  locationId: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.AnnotateVideoRequest.
 * Use `create(AnnotateVideoRequestSchema)` to create a new message.
 */
export const AnnotateVideoRequestSchema: GenMessage<AnnotateVideoRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 0);

/**
 * Video context and/or feature-specific parameters.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.VideoContext
 */
export type VideoContext = Message<"google.cloud.videointelligence.v1beta2.VideoContext"> & {
  /**
   * Video segments to annotate. The segments may overlap and are not required
   * to be contiguous or span the whole video. If unspecified, each video is
   * treated as a single segment.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.VideoSegment segments = 1;
   */
  segments: VideoSegment[];

  /**
   * Config for LABEL_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.LabelDetectionConfig label_detection_config = 2;
   */
  labelDetectionConfig?: LabelDetectionConfig;

  /**
   * Config for SHOT_CHANGE_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.ShotChangeDetectionConfig shot_change_detection_config = 3;
   */
  shotChangeDetectionConfig?: ShotChangeDetectionConfig;

  /**
   * Config for EXPLICIT_CONTENT_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.ExplicitContentDetectionConfig explicit_content_detection_config = 4;
   */
  explicitContentDetectionConfig?: ExplicitContentDetectionConfig;

  /**
   * Config for FACE_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.FaceDetectionConfig face_detection_config = 5;
   */
  faceDetectionConfig?: FaceDetectionConfig;
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.VideoContext.
 * Use `create(VideoContextSchema)` to create a new message.
 */
export const VideoContextSchema: GenMessage<VideoContext> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 1);

/**
 * Config for LABEL_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.LabelDetectionConfig
 */
export type LabelDetectionConfig = Message<"google.cloud.videointelligence.v1beta2.LabelDetectionConfig"> & {
  /**
   * What labels should be detected with LABEL_DETECTION, in addition to
   * video-level labels or segment-level labels.
   * If unspecified, defaults to `SHOT_MODE`.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.LabelDetectionMode label_detection_mode = 1;
   */
  labelDetectionMode: LabelDetectionMode;

  /**
   * Whether the video has been shot from a stationary (i.e. non-moving) camera.
   * When set to true, might improve detection accuracy for moving objects.
   * Should be used with `SHOT_AND_FRAME_MODE` enabled.
   *
   * @generated from field: bool stationary_camera = 2;
   */
  stationaryCamera: boolean;

  /**
   * Model to use for label detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 3;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.LabelDetectionConfig.
 * Use `create(LabelDetectionConfigSchema)` to create a new message.
 */
export const LabelDetectionConfigSchema: GenMessage<LabelDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 2);

/**
 * Config for SHOT_CHANGE_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.ShotChangeDetectionConfig
 */
export type ShotChangeDetectionConfig = Message<"google.cloud.videointelligence.v1beta2.ShotChangeDetectionConfig"> & {
  /**
   * Model to use for shot change detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.ShotChangeDetectionConfig.
 * Use `create(ShotChangeDetectionConfigSchema)` to create a new message.
 */
export const ShotChangeDetectionConfigSchema: GenMessage<ShotChangeDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 3);

/**
 * Config for EXPLICIT_CONTENT_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.ExplicitContentDetectionConfig
 */
export type ExplicitContentDetectionConfig = Message<"google.cloud.videointelligence.v1beta2.ExplicitContentDetectionConfig"> & {
  /**
   * Model to use for explicit content detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.ExplicitContentDetectionConfig.
 * Use `create(ExplicitContentDetectionConfigSchema)` to create a new message.
 */
export const ExplicitContentDetectionConfigSchema: GenMessage<ExplicitContentDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 4);

/**
 * Config for FACE_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.FaceDetectionConfig
 */
export type FaceDetectionConfig = Message<"google.cloud.videointelligence.v1beta2.FaceDetectionConfig"> & {
  /**
   * Model to use for face detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Whether bounding boxes be included in the face annotation output.
   *
   * @generated from field: bool include_bounding_boxes = 2;
   */
  includeBoundingBoxes: boolean;
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.FaceDetectionConfig.
 * Use `create(FaceDetectionConfigSchema)` to create a new message.
 */
export const FaceDetectionConfigSchema: GenMessage<FaceDetectionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 5);

/**
 * Video segment.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.VideoSegment
 */
export type VideoSegment = Message<"google.cloud.videointelligence.v1beta2.VideoSegment"> & {
  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the start of the segment (inclusive).
   *
   * @generated from field: google.protobuf.Duration start_time_offset = 1;
   */
  startTimeOffset?: Duration;

  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the end of the segment (inclusive).
   *
   * @generated from field: google.protobuf.Duration end_time_offset = 2;
   */
  endTimeOffset?: Duration;
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.VideoSegment.
 * Use `create(VideoSegmentSchema)` to create a new message.
 */
export const VideoSegmentSchema: GenMessage<VideoSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 6);

/**
 * Video segment level annotation results for label detection.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.LabelSegment
 */
export type LabelSegment = Message<"google.cloud.videointelligence.v1beta2.LabelSegment"> & {
  /**
   * Video segment where a label was detected.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.VideoSegment segment = 1;
   */
  segment?: VideoSegment;

  /**
   * Confidence that the label is accurate. Range: [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.LabelSegment.
 * Use `create(LabelSegmentSchema)` to create a new message.
 */
export const LabelSegmentSchema: GenMessage<LabelSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 7);

/**
 * Video frame level annotation results for label detection.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.LabelFrame
 */
export type LabelFrame = Message<"google.cloud.videointelligence.v1beta2.LabelFrame"> & {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   *
   * @generated from field: google.protobuf.Duration time_offset = 1;
   */
  timeOffset?: Duration;

  /**
   * Confidence that the label is accurate. Range: [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.LabelFrame.
 * Use `create(LabelFrameSchema)` to create a new message.
 */
export const LabelFrameSchema: GenMessage<LabelFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 8);

/**
 * Detected entity from video analysis.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.Entity
 */
export type Entity = Message<"google.cloud.videointelligence.v1beta2.Entity"> & {
  /**
   * Opaque entity ID. Some IDs may be available in
   * [Google Knowledge Graph Search
   * API](https://developers.google.com/knowledge-graph/).
   *
   * @generated from field: string entity_id = 1;
   */
  entityId: string;

  /**
   * Textual description, e.g. `Fixed-gear bicycle`.
   *
   * @generated from field: string description = 2;
   */
  description: string;

  /**
   * Language code for `description` in BCP-47 format.
   *
   * @generated from field: string language_code = 3;
   */
  languageCode: string;
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.Entity.
 * Use `create(EntitySchema)` to create a new message.
 */
export const EntitySchema: GenMessage<Entity> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 9);

/**
 * Label annotation.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.LabelAnnotation
 */
export type LabelAnnotation = Message<"google.cloud.videointelligence.v1beta2.LabelAnnotation"> & {
  /**
   * Detected entity.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.Entity entity = 1;
   */
  entity?: Entity;

  /**
   * Common categories for the detected entity.
   * E.g. when the label is `Terrier` the category is likely `dog`. And in some
   * cases there might be more than one categories e.g. `Terrier` could also be
   * a `pet`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.Entity category_entities = 2;
   */
  categoryEntities: Entity[];

  /**
   * All video segments where a label was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.LabelSegment segments = 3;
   */
  segments: LabelSegment[];

  /**
   * All video frames where a label was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.LabelFrame frames = 4;
   */
  frames: LabelFrame[];
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.LabelAnnotation.
 * Use `create(LabelAnnotationSchema)` to create a new message.
 */
export const LabelAnnotationSchema: GenMessage<LabelAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 10);

/**
 * Video frame level annotation results for explicit content.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.ExplicitContentFrame
 */
export type ExplicitContentFrame = Message<"google.cloud.videointelligence.v1beta2.ExplicitContentFrame"> & {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   *
   * @generated from field: google.protobuf.Duration time_offset = 1;
   */
  timeOffset?: Duration;

  /**
   * Likelihood of the pornography content..
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.Likelihood pornography_likelihood = 2;
   */
  pornographyLikelihood: Likelihood;
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.ExplicitContentFrame.
 * Use `create(ExplicitContentFrameSchema)` to create a new message.
 */
export const ExplicitContentFrameSchema: GenMessage<ExplicitContentFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 11);

/**
 * Explicit content annotation (based on per-frame visual signals only).
 * If no explicit content has been detected in a frame, no annotations are
 * present for that frame.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.ExplicitContentAnnotation
 */
export type ExplicitContentAnnotation = Message<"google.cloud.videointelligence.v1beta2.ExplicitContentAnnotation"> & {
  /**
   * All video frames where explicit content was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.ExplicitContentFrame frames = 1;
   */
  frames: ExplicitContentFrame[];
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.ExplicitContentAnnotation.
 * Use `create(ExplicitContentAnnotationSchema)` to create a new message.
 */
export const ExplicitContentAnnotationSchema: GenMessage<ExplicitContentAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 12);

/**
 * Normalized bounding box.
 * The normalized vertex coordinates are relative to the original image.
 * Range: [0, 1].
 *
 * @generated from message google.cloud.videointelligence.v1beta2.NormalizedBoundingBox
 */
export type NormalizedBoundingBox = Message<"google.cloud.videointelligence.v1beta2.NormalizedBoundingBox"> & {
  /**
   * Left X coordinate.
   *
   * @generated from field: float left = 1;
   */
  left: number;

  /**
   * Top Y coordinate.
   *
   * @generated from field: float top = 2;
   */
  top: number;

  /**
   * Right X coordinate.
   *
   * @generated from field: float right = 3;
   */
  right: number;

  /**
   * Bottom Y coordinate.
   *
   * @generated from field: float bottom = 4;
   */
  bottom: number;
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.NormalizedBoundingBox.
 * Use `create(NormalizedBoundingBoxSchema)` to create a new message.
 */
export const NormalizedBoundingBoxSchema: GenMessage<NormalizedBoundingBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 13);

/**
 * Video segment level annotation results for face detection.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.FaceSegment
 */
export type FaceSegment = Message<"google.cloud.videointelligence.v1beta2.FaceSegment"> & {
  /**
   * Video segment where a face was detected.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.VideoSegment segment = 1;
   */
  segment?: VideoSegment;
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.FaceSegment.
 * Use `create(FaceSegmentSchema)` to create a new message.
 */
export const FaceSegmentSchema: GenMessage<FaceSegment> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 14);

/**
 * Video frame level annotation results for face detection.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.FaceFrame
 */
export type FaceFrame = Message<"google.cloud.videointelligence.v1beta2.FaceFrame"> & {
  /**
   * Normalized Bounding boxes in a frame.
   * There can be more than one boxes if the same face is detected in multiple
   * locations within the current frame.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.NormalizedBoundingBox normalized_bounding_boxes = 1;
   */
  normalizedBoundingBoxes: NormalizedBoundingBox[];

  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the video frame for this location.
   *
   * @generated from field: google.protobuf.Duration time_offset = 2;
   */
  timeOffset?: Duration;
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.FaceFrame.
 * Use `create(FaceFrameSchema)` to create a new message.
 */
export const FaceFrameSchema: GenMessage<FaceFrame> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 15);

/**
 * Face annotation.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.FaceAnnotation
 */
export type FaceAnnotation = Message<"google.cloud.videointelligence.v1beta2.FaceAnnotation"> & {
  /**
   * Thumbnail of a representative face view (in JPEG format).
   *
   * @generated from field: bytes thumbnail = 1;
   */
  thumbnail: Uint8Array;

  /**
   * All video segments where a face was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.FaceSegment segments = 2;
   */
  segments: FaceSegment[];

  /**
   * All video frames where a face was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.FaceFrame frames = 3;
   */
  frames: FaceFrame[];
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.FaceAnnotation.
 * Use `create(FaceAnnotationSchema)` to create a new message.
 */
export const FaceAnnotationSchema: GenMessage<FaceAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 16);

/**
 * Annotation results for a single video.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.VideoAnnotationResults
 */
export type VideoAnnotationResults = Message<"google.cloud.videointelligence.v1beta2.VideoAnnotationResults"> & {
  /**
   * Video file location in
   * [Google Cloud Storage](https://cloud.google.com/storage/).
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri: string;

  /**
   * Label annotations on video level or user specified segment level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.LabelAnnotation segment_label_annotations = 2;
   */
  segmentLabelAnnotations: LabelAnnotation[];

  /**
   * Label annotations on shot level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.LabelAnnotation shot_label_annotations = 3;
   */
  shotLabelAnnotations: LabelAnnotation[];

  /**
   * Label annotations on frame level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.LabelAnnotation frame_label_annotations = 4;
   */
  frameLabelAnnotations: LabelAnnotation[];

  /**
   * Face annotations. There is exactly one element for each unique face.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.FaceAnnotation face_annotations = 5;
   */
  faceAnnotations: FaceAnnotation[];

  /**
   * Shot annotations. Each shot is represented as a video segment.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.VideoSegment shot_annotations = 6;
   */
  shotAnnotations: VideoSegment[];

  /**
   * Explicit content annotation.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.ExplicitContentAnnotation explicit_annotation = 7;
   */
  explicitAnnotation?: ExplicitContentAnnotation;

  /**
   * If set, indicates an error. Note that for a single `AnnotateVideoRequest`
   * some videos may succeed and some may fail.
   *
   * @generated from field: google.rpc.Status error = 9;
   */
  error?: Status;
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.VideoAnnotationResults.
 * Use `create(VideoAnnotationResultsSchema)` to create a new message.
 */
export const VideoAnnotationResultsSchema: GenMessage<VideoAnnotationResults> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 17);

/**
 * Video annotation response. Included in the `response`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.AnnotateVideoResponse
 */
export type AnnotateVideoResponse = Message<"google.cloud.videointelligence.v1beta2.AnnotateVideoResponse"> & {
  /**
   * Annotation results for all videos specified in `AnnotateVideoRequest`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.VideoAnnotationResults annotation_results = 1;
   */
  annotationResults: VideoAnnotationResults[];
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.AnnotateVideoResponse.
 * Use `create(AnnotateVideoResponseSchema)` to create a new message.
 */
export const AnnotateVideoResponseSchema: GenMessage<AnnotateVideoResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 18);

/**
 * Annotation progress for a single video.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.VideoAnnotationProgress
 */
export type VideoAnnotationProgress = Message<"google.cloud.videointelligence.v1beta2.VideoAnnotationProgress"> & {
  /**
   * Video file location in
   * [Google Cloud Storage](https://cloud.google.com/storage/).
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri: string;

  /**
   * Approximate percentage processed thus far.
   * Guaranteed to be 100 when fully processed.
   *
   * @generated from field: int32 progress_percent = 2;
   */
  progressPercent: number;

  /**
   * Time when the request was received.
   *
   * @generated from field: google.protobuf.Timestamp start_time = 3;
   */
  startTime?: Timestamp;

  /**
   * Time of the most recent update.
   *
   * @generated from field: google.protobuf.Timestamp update_time = 4;
   */
  updateTime?: Timestamp;
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.VideoAnnotationProgress.
 * Use `create(VideoAnnotationProgressSchema)` to create a new message.
 */
export const VideoAnnotationProgressSchema: GenMessage<VideoAnnotationProgress> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 19);

/**
 * Video annotation progress. Included in the `metadata`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.AnnotateVideoProgress
 */
export type AnnotateVideoProgress = Message<"google.cloud.videointelligence.v1beta2.AnnotateVideoProgress"> & {
  /**
   * Progress metadata for all videos specified in `AnnotateVideoRequest`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.VideoAnnotationProgress annotation_progress = 1;
   */
  annotationProgress: VideoAnnotationProgress[];
};

/**
 * Describes the message google.cloud.videointelligence.v1beta2.AnnotateVideoProgress.
 * Use `create(AnnotateVideoProgressSchema)` to create a new message.
 */
export const AnnotateVideoProgressSchema: GenMessage<AnnotateVideoProgress> = /*@__PURE__*/
  messageDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 20);

/**
 * Video annotation feature.
 *
 * @generated from enum google.cloud.videointelligence.v1beta2.Feature
 */
export enum Feature {
  /**
   * Unspecified.
   *
   * @generated from enum value: FEATURE_UNSPECIFIED = 0;
   */
  FEATURE_UNSPECIFIED = 0,

  /**
   * Label detection. Detect objects, such as dog or flower.
   *
   * @generated from enum value: LABEL_DETECTION = 1;
   */
  LABEL_DETECTION = 1,

  /**
   * Shot change detection.
   *
   * @generated from enum value: SHOT_CHANGE_DETECTION = 2;
   */
  SHOT_CHANGE_DETECTION = 2,

  /**
   * Explicit content detection.
   *
   * @generated from enum value: EXPLICIT_CONTENT_DETECTION = 3;
   */
  EXPLICIT_CONTENT_DETECTION = 3,

  /**
   * Human face detection and tracking.
   *
   * @generated from enum value: FACE_DETECTION = 4;
   */
  FACE_DETECTION = 4,
}

/**
 * Describes the enum google.cloud.videointelligence.v1beta2.Feature.
 */
export const FeatureSchema: GenEnum<Feature> = /*@__PURE__*/
  enumDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 0);

/**
 * Label detection mode.
 *
 * @generated from enum google.cloud.videointelligence.v1beta2.LabelDetectionMode
 */
export enum LabelDetectionMode {
  /**
   * Unspecified.
   *
   * @generated from enum value: LABEL_DETECTION_MODE_UNSPECIFIED = 0;
   */
  LABEL_DETECTION_MODE_UNSPECIFIED = 0,

  /**
   * Detect shot-level labels.
   *
   * @generated from enum value: SHOT_MODE = 1;
   */
  SHOT_MODE = 1,

  /**
   * Detect frame-level labels.
   *
   * @generated from enum value: FRAME_MODE = 2;
   */
  FRAME_MODE = 2,

  /**
   * Detect both shot-level and frame-level labels.
   *
   * @generated from enum value: SHOT_AND_FRAME_MODE = 3;
   */
  SHOT_AND_FRAME_MODE = 3,
}

/**
 * Describes the enum google.cloud.videointelligence.v1beta2.LabelDetectionMode.
 */
export const LabelDetectionModeSchema: GenEnum<LabelDetectionMode> = /*@__PURE__*/
  enumDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 1);

/**
 * Bucketized representation of likelihood.
 *
 * @generated from enum google.cloud.videointelligence.v1beta2.Likelihood
 */
export enum Likelihood {
  /**
   * Unspecified likelihood.
   *
   * @generated from enum value: LIKELIHOOD_UNSPECIFIED = 0;
   */
  LIKELIHOOD_UNSPECIFIED = 0,

  /**
   * Very unlikely.
   *
   * @generated from enum value: VERY_UNLIKELY = 1;
   */
  VERY_UNLIKELY = 1,

  /**
   * Unlikely.
   *
   * @generated from enum value: UNLIKELY = 2;
   */
  UNLIKELY = 2,

  /**
   * Possible.
   *
   * @generated from enum value: POSSIBLE = 3;
   */
  POSSIBLE = 3,

  /**
   * Likely.
   *
   * @generated from enum value: LIKELY = 4;
   */
  LIKELY = 4,

  /**
   * Very likely.
   *
   * @generated from enum value: VERY_LIKELY = 5;
   */
  VERY_LIKELY = 5,
}

/**
 * Describes the enum google.cloud.videointelligence.v1beta2.Likelihood.
 */
export const LikelihoodSchema: GenEnum<Likelihood> = /*@__PURE__*/
  enumDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 2);

/**
 * Service that implements Google Cloud Video Intelligence API.
 *
 * @generated from service google.cloud.videointelligence.v1beta2.VideoIntelligenceService
 */
export const VideoIntelligenceService: GenService<{
  /**
   * Performs asynchronous video annotation. Progress and results can be
   * retrieved through the `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `AnnotateVideoProgress` (progress).
   * `Operation.response` contains `AnnotateVideoResponse` (results).
   *
   * @generated from rpc google.cloud.videointelligence.v1beta2.VideoIntelligenceService.AnnotateVideo
   */
  annotateVideo: {
    methodKind: "unary";
    input: typeof AnnotateVideoRequestSchema;
    output: typeof OperationSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_videointelligence_v1beta2_video_intelligence, 0);

