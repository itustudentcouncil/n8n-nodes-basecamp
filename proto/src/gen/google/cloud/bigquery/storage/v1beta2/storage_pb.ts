// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/bigquery/storage/v1beta2/storage.proto (package google.cloud.bigquery.storage.v1beta2, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../../api/annotations_pb";
import { file_google_api_client } from "../../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../../api/field_behavior_pb";
import { file_google_api_resource } from "../../../../api/resource_pb";
import type { ArrowRecordBatch, ArrowSchema } from "./arrow_pb";
import { file_google_cloud_bigquery_storage_v1beta2_arrow } from "./arrow_pb";
import type { AvroRows, AvroSchema } from "./avro_pb";
import { file_google_cloud_bigquery_storage_v1beta2_avro } from "./avro_pb";
import type { ProtoRows, ProtoSchema } from "./protobuf_pb";
import { file_google_cloud_bigquery_storage_v1beta2_protobuf } from "./protobuf_pb";
import type { ReadSession, ReadSessionSchema, ReadStream, WriteStream, WriteStreamSchema } from "./stream_pb";
import { file_google_cloud_bigquery_storage_v1beta2_stream } from "./stream_pb";
import type { TableSchema } from "./table_pb";
import { file_google_cloud_bigquery_storage_v1beta2_table } from "./table_pb";
import type { Timestamp } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_timestamp, file_google_protobuf_wrappers } from "@bufbuild/protobuf/wkt";
import type { Status } from "../../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../../rpc/status_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/bigquery/storage/v1beta2/storage.proto.
 */
export const file_google_cloud_bigquery_storage_v1beta2_storage: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_api_resource, file_google_cloud_bigquery_storage_v1beta2_arrow, file_google_cloud_bigquery_storage_v1beta2_avro, file_google_cloud_bigquery_storage_v1beta2_protobuf, file_google_cloud_bigquery_storage_v1beta2_stream, file_google_cloud_bigquery_storage_v1beta2_table, file_google_protobuf_timestamp, file_google_protobuf_wrappers, file_google_rpc_status]);

/**
 * Request message for `CreateReadSession`.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.CreateReadSessionRequest
 */
export type CreateReadSessionRequest = Message<"google.cloud.bigquery.storage.v1beta2.CreateReadSessionRequest"> & {
  /**
   * Required. The request project that owns the session, in the form of
   * `projects/{project_id}`.
   *
   * @generated from field: string parent = 1;
   */
  parent: string;

  /**
   * Required. Session to be created.
   *
   * @generated from field: google.cloud.bigquery.storage.v1beta2.ReadSession read_session = 2;
   */
  readSession?: ReadSession;

  /**
   * Max initial number of streams. If unset or zero, the server will
   * provide a value of streams so as to produce reasonable throughput. Must be
   * non-negative. The number of streams may be lower than the requested number,
   * depending on the amount parallelism that is reasonable for the table. Error
   * will be returned if the max count is greater than the current system
   * max limit of 1,000.
   *
   * Streams must be read starting from offset 0.
   *
   * @generated from field: int32 max_stream_count = 3;
   */
  maxStreamCount: number;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.CreateReadSessionRequest.
 * Use `create(CreateReadSessionRequestSchema)` to create a new message.
 */
export const CreateReadSessionRequestSchema: GenMessage<CreateReadSessionRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 0);

/**
 * Request message for `ReadRows`.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.ReadRowsRequest
 */
export type ReadRowsRequest = Message<"google.cloud.bigquery.storage.v1beta2.ReadRowsRequest"> & {
  /**
   * Required. Stream to read rows from.
   *
   * @generated from field: string read_stream = 1;
   */
  readStream: string;

  /**
   * The offset requested must be less than the last row read from Read.
   * Requesting a larger offset is undefined. If not specified, start reading
   * from offset zero.
   *
   * @generated from field: int64 offset = 2;
   */
  offset: bigint;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.ReadRowsRequest.
 * Use `create(ReadRowsRequestSchema)` to create a new message.
 */
export const ReadRowsRequestSchema: GenMessage<ReadRowsRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 1);

/**
 * Information on if the current connection is being throttled.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.ThrottleState
 */
export type ThrottleState = Message<"google.cloud.bigquery.storage.v1beta2.ThrottleState"> & {
  /**
   * How much this connection is being throttled. Zero means no throttling,
   * 100 means fully throttled.
   *
   * @generated from field: int32 throttle_percent = 1;
   */
  throttlePercent: number;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.ThrottleState.
 * Use `create(ThrottleStateSchema)` to create a new message.
 */
export const ThrottleStateSchema: GenMessage<ThrottleState> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 2);

/**
 * Estimated stream statistics for a given Stream.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.StreamStats
 */
export type StreamStats = Message<"google.cloud.bigquery.storage.v1beta2.StreamStats"> & {
  /**
   * Represents the progress of the current stream.
   *
   * @generated from field: google.cloud.bigquery.storage.v1beta2.StreamStats.Progress progress = 2;
   */
  progress?: StreamStats_Progress;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.StreamStats.
 * Use `create(StreamStatsSchema)` to create a new message.
 */
export const StreamStatsSchema: GenMessage<StreamStats> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 3);

/**
 * @generated from message google.cloud.bigquery.storage.v1beta2.StreamStats.Progress
 */
export type StreamStats_Progress = Message<"google.cloud.bigquery.storage.v1beta2.StreamStats.Progress"> & {
  /**
   * The fraction of rows assigned to the stream that have been processed by
   * the server so far, not including the rows in the current response
   * message.
   *
   * This value, along with `at_response_end`, can be used to interpolate
   * the progress made as the rows in the message are being processed using
   * the following formula: `at_response_start + (at_response_end -
   * at_response_start) * rows_processed_from_response / rows_in_response`.
   *
   * Note that if a filter is provided, the `at_response_end` value of the
   * previous response may not necessarily be equal to the
   * `at_response_start` value of the current response.
   *
   * @generated from field: double at_response_start = 1;
   */
  atResponseStart: number;

  /**
   * Similar to `at_response_start`, except that this value includes the
   * rows in the current response.
   *
   * @generated from field: double at_response_end = 2;
   */
  atResponseEnd: number;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.StreamStats.Progress.
 * Use `create(StreamStats_ProgressSchema)` to create a new message.
 */
export const StreamStats_ProgressSchema: GenMessage<StreamStats_Progress> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 3, 0);

/**
 * Response from calling `ReadRows` may include row data, progress and
 * throttling information.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.ReadRowsResponse
 */
export type ReadRowsResponse = Message<"google.cloud.bigquery.storage.v1beta2.ReadRowsResponse"> & {
  /**
   * Row data is returned in format specified during session creation.
   *
   * @generated from oneof google.cloud.bigquery.storage.v1beta2.ReadRowsResponse.rows
   */
  rows: {
    /**
     * Serialized row data in AVRO format.
     *
     * @generated from field: google.cloud.bigquery.storage.v1beta2.AvroRows avro_rows = 3;
     */
    value: AvroRows;
    case: "avroRows";
  } | {
    /**
     * Serialized row data in Arrow RecordBatch format.
     *
     * @generated from field: google.cloud.bigquery.storage.v1beta2.ArrowRecordBatch arrow_record_batch = 4;
     */
    value: ArrowRecordBatch;
    case: "arrowRecordBatch";
  } | { case: undefined; value?: undefined };

  /**
   * Number of serialized rows in the rows block.
   *
   * @generated from field: int64 row_count = 6;
   */
  rowCount: bigint;

  /**
   * Statistics for the stream.
   *
   * @generated from field: google.cloud.bigquery.storage.v1beta2.StreamStats stats = 2;
   */
  stats?: StreamStats;

  /**
   * Throttling state. If unset, the latest response still describes
   * the current throttling status.
   *
   * @generated from field: google.cloud.bigquery.storage.v1beta2.ThrottleState throttle_state = 5;
   */
  throttleState?: ThrottleState;

  /**
   * The schema for the read. If read_options.selected_fields is set, the
   * schema may be different from the table schema as it will only contain
   * the selected fields. This schema is equivalent to the one returned by
   * CreateSession. This field is only populated in the first ReadRowsResponse
   * RPC.
   *
   * @generated from oneof google.cloud.bigquery.storage.v1beta2.ReadRowsResponse.schema
   */
  schema: {
    /**
     * Output only. Avro schema.
     *
     * @generated from field: google.cloud.bigquery.storage.v1beta2.AvroSchema avro_schema = 7;
     */
    value: AvroSchema;
    case: "avroSchema";
  } | {
    /**
     * Output only. Arrow schema.
     *
     * @generated from field: google.cloud.bigquery.storage.v1beta2.ArrowSchema arrow_schema = 8;
     */
    value: ArrowSchema;
    case: "arrowSchema";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.ReadRowsResponse.
 * Use `create(ReadRowsResponseSchema)` to create a new message.
 */
export const ReadRowsResponseSchema: GenMessage<ReadRowsResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 4);

/**
 * Request message for `SplitReadStream`.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.SplitReadStreamRequest
 */
export type SplitReadStreamRequest = Message<"google.cloud.bigquery.storage.v1beta2.SplitReadStreamRequest"> & {
  /**
   * Required. Name of the stream to split.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * A value in the range (0.0, 1.0) that specifies the fractional point at
   * which the original stream should be split. The actual split point is
   * evaluated on pre-filtered rows, so if a filter is provided, then there is
   * no guarantee that the division of the rows between the new child streams
   * will be proportional to this fractional value. Additionally, because the
   * server-side unit for assigning data is collections of rows, this fraction
   * will always map to a data storage boundary on the server side.
   *
   * @generated from field: double fraction = 2;
   */
  fraction: number;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.SplitReadStreamRequest.
 * Use `create(SplitReadStreamRequestSchema)` to create a new message.
 */
export const SplitReadStreamRequestSchema: GenMessage<SplitReadStreamRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 5);

/**
 * @generated from message google.cloud.bigquery.storage.v1beta2.SplitReadStreamResponse
 */
export type SplitReadStreamResponse = Message<"google.cloud.bigquery.storage.v1beta2.SplitReadStreamResponse"> & {
  /**
   * Primary stream, which contains the beginning portion of
   * |original_stream|. An empty value indicates that the original stream can no
   * longer be split.
   *
   * @generated from field: google.cloud.bigquery.storage.v1beta2.ReadStream primary_stream = 1;
   */
  primaryStream?: ReadStream;

  /**
   * Remainder stream, which contains the tail of |original_stream|. An empty
   * value indicates that the original stream can no longer be split.
   *
   * @generated from field: google.cloud.bigquery.storage.v1beta2.ReadStream remainder_stream = 2;
   */
  remainderStream?: ReadStream;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.SplitReadStreamResponse.
 * Use `create(SplitReadStreamResponseSchema)` to create a new message.
 */
export const SplitReadStreamResponseSchema: GenMessage<SplitReadStreamResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 6);

/**
 * Request message for `CreateWriteStream`.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.CreateWriteStreamRequest
 */
export type CreateWriteStreamRequest = Message<"google.cloud.bigquery.storage.v1beta2.CreateWriteStreamRequest"> & {
  /**
   * Required. Reference to the table to which the stream belongs, in the format
   * of `projects/{project}/datasets/{dataset}/tables/{table}`.
   *
   * @generated from field: string parent = 1;
   */
  parent: string;

  /**
   * Required. Stream to be created.
   *
   * @generated from field: google.cloud.bigquery.storage.v1beta2.WriteStream write_stream = 2;
   */
  writeStream?: WriteStream;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.CreateWriteStreamRequest.
 * Use `create(CreateWriteStreamRequestSchema)` to create a new message.
 */
export const CreateWriteStreamRequestSchema: GenMessage<CreateWriteStreamRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 7);

/**
 * Request message for `AppendRows`.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.AppendRowsRequest
 */
export type AppendRowsRequest = Message<"google.cloud.bigquery.storage.v1beta2.AppendRowsRequest"> & {
  /**
   * Required. The stream that is the target of the append operation. This value
   * must be specified for the initial request. If subsequent requests specify
   * the stream name, it must equal to the value provided in the first request.
   * To write to the _default stream, populate this field with a string in the
   * format `projects/{project}/datasets/{dataset}/tables/{table}/_default`.
   *
   * @generated from field: string write_stream = 1;
   */
  writeStream: string;

  /**
   * If present, the write is only performed if the next append offset is same
   * as the provided value. If not present, the write is performed at the
   * current end of stream. Specifying a value for this field is not allowed
   * when calling AppendRows for the '_default' stream.
   *
   * @generated from field: google.protobuf.Int64Value offset = 2;
   */
  offset?: bigint;

  /**
   * Input rows. The `writer_schema` field must be specified at the initial
   * request and currently, it will be ignored if specified in following
   * requests. Following requests must have data in the same format as the
   * initial request.
   *
   * @generated from oneof google.cloud.bigquery.storage.v1beta2.AppendRowsRequest.rows
   */
  rows: {
    /**
     * Rows in proto format.
     *
     * @generated from field: google.cloud.bigquery.storage.v1beta2.AppendRowsRequest.ProtoData proto_rows = 4;
     */
    value: AppendRowsRequest_ProtoData;
    case: "protoRows";
  } | { case: undefined; value?: undefined };

  /**
   * Id set by client to annotate its identity. Only initial request setting is
   * respected.
   *
   * @generated from field: string trace_id = 6;
   */
  traceId: string;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.AppendRowsRequest.
 * Use `create(AppendRowsRequestSchema)` to create a new message.
 */
export const AppendRowsRequestSchema: GenMessage<AppendRowsRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 8);

/**
 * Proto schema and data.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.AppendRowsRequest.ProtoData
 */
export type AppendRowsRequest_ProtoData = Message<"google.cloud.bigquery.storage.v1beta2.AppendRowsRequest.ProtoData"> & {
  /**
   * Proto schema used to serialize the data.
   *
   * @generated from field: google.cloud.bigquery.storage.v1beta2.ProtoSchema writer_schema = 1;
   */
  writerSchema?: ProtoSchema;

  /**
   * Serialized row data in protobuf message format.
   *
   * @generated from field: google.cloud.bigquery.storage.v1beta2.ProtoRows rows = 2;
   */
  rows?: ProtoRows;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.AppendRowsRequest.ProtoData.
 * Use `create(AppendRowsRequest_ProtoDataSchema)` to create a new message.
 */
export const AppendRowsRequest_ProtoDataSchema: GenMessage<AppendRowsRequest_ProtoData> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 8, 0);

/**
 * Response message for `AppendRows`.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.AppendRowsResponse
 */
export type AppendRowsResponse = Message<"google.cloud.bigquery.storage.v1beta2.AppendRowsResponse"> & {
  /**
   * @generated from oneof google.cloud.bigquery.storage.v1beta2.AppendRowsResponse.response
   */
  response: {
    /**
     * Result if the append is successful.
     *
     * @generated from field: google.cloud.bigquery.storage.v1beta2.AppendRowsResponse.AppendResult append_result = 1;
     */
    value: AppendRowsResponse_AppendResult;
    case: "appendResult";
  } | {
    /**
     * Error returned when problems were encountered.  If present,
     * it indicates rows were not accepted into the system.
     * Users can retry or continue with other append requests within the
     * same connection.
     *
     * Additional information about error signalling:
     *
     * ALREADY_EXISTS: Happens when an append specified an offset, and the
     * backend already has received data at this offset.  Typically encountered
     * in retry scenarios, and can be ignored.
     *
     * OUT_OF_RANGE: Returned when the specified offset in the stream is beyond
     * the current end of the stream.
     *
     * INVALID_ARGUMENT: Indicates a malformed request or data.
     *
     * ABORTED: Request processing is aborted because of prior failures.  The
     * request can be retried if previous failure is addressed.
     *
     * INTERNAL: Indicates server side error(s) that can be retried.
     *
     * @generated from field: google.rpc.Status error = 2;
     */
    value: Status;
    case: "error";
  } | { case: undefined; value?: undefined };

  /**
   * If backend detects a schema update, pass it to user so that user can
   * use it to input new type of message. It will be empty when no schema
   * updates have occurred.
   *
   * @generated from field: google.cloud.bigquery.storage.v1beta2.TableSchema updated_schema = 3;
   */
  updatedSchema?: TableSchema;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.AppendRowsResponse.
 * Use `create(AppendRowsResponseSchema)` to create a new message.
 */
export const AppendRowsResponseSchema: GenMessage<AppendRowsResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 9);

/**
 * AppendResult is returned for successful append requests.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.AppendRowsResponse.AppendResult
 */
export type AppendRowsResponse_AppendResult = Message<"google.cloud.bigquery.storage.v1beta2.AppendRowsResponse.AppendResult"> & {
  /**
   * The row offset at which the last append occurred. The offset will not be
   * set if appending using default streams.
   *
   * @generated from field: google.protobuf.Int64Value offset = 1;
   */
  offset?: bigint;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.AppendRowsResponse.AppendResult.
 * Use `create(AppendRowsResponse_AppendResultSchema)` to create a new message.
 */
export const AppendRowsResponse_AppendResultSchema: GenMessage<AppendRowsResponse_AppendResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 9, 0);

/**
 * Request message for `GetWriteStreamRequest`.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.GetWriteStreamRequest
 */
export type GetWriteStreamRequest = Message<"google.cloud.bigquery.storage.v1beta2.GetWriteStreamRequest"> & {
  /**
   * Required. Name of the stream to get, in the form of
   * `projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}`.
   *
   * @generated from field: string name = 1;
   */
  name: string;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.GetWriteStreamRequest.
 * Use `create(GetWriteStreamRequestSchema)` to create a new message.
 */
export const GetWriteStreamRequestSchema: GenMessage<GetWriteStreamRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 10);

/**
 * Request message for `BatchCommitWriteStreams`.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.BatchCommitWriteStreamsRequest
 */
export type BatchCommitWriteStreamsRequest = Message<"google.cloud.bigquery.storage.v1beta2.BatchCommitWriteStreamsRequest"> & {
  /**
   * Required. Parent table that all the streams should belong to, in the form
   * of `projects/{project}/datasets/{dataset}/tables/{table}`.
   *
   * @generated from field: string parent = 1;
   */
  parent: string;

  /**
   * Required. The group of streams that will be committed atomically.
   *
   * @generated from field: repeated string write_streams = 2;
   */
  writeStreams: string[];
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.BatchCommitWriteStreamsRequest.
 * Use `create(BatchCommitWriteStreamsRequestSchema)` to create a new message.
 */
export const BatchCommitWriteStreamsRequestSchema: GenMessage<BatchCommitWriteStreamsRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 11);

/**
 * Response message for `BatchCommitWriteStreams`.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.BatchCommitWriteStreamsResponse
 */
export type BatchCommitWriteStreamsResponse = Message<"google.cloud.bigquery.storage.v1beta2.BatchCommitWriteStreamsResponse"> & {
  /**
   * The time at which streams were committed in microseconds granularity.
   * This field will only exist when there are no stream errors.
   * **Note** if this field is not set, it means the commit was not successful.
   *
   * @generated from field: google.protobuf.Timestamp commit_time = 1;
   */
  commitTime?: Timestamp;

  /**
   * Stream level error if commit failed. Only streams with error will be in
   * the list.
   * If empty, there is no error and all streams are committed successfully.
   * If non empty, certain streams have errors and ZERO stream is committed due
   * to atomicity guarantee.
   *
   * @generated from field: repeated google.cloud.bigquery.storage.v1beta2.StorageError stream_errors = 2;
   */
  streamErrors: StorageError[];
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.BatchCommitWriteStreamsResponse.
 * Use `create(BatchCommitWriteStreamsResponseSchema)` to create a new message.
 */
export const BatchCommitWriteStreamsResponseSchema: GenMessage<BatchCommitWriteStreamsResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 12);

/**
 * Request message for invoking `FinalizeWriteStream`.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.FinalizeWriteStreamRequest
 */
export type FinalizeWriteStreamRequest = Message<"google.cloud.bigquery.storage.v1beta2.FinalizeWriteStreamRequest"> & {
  /**
   * Required. Name of the stream to finalize, in the form of
   * `projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}`.
   *
   * @generated from field: string name = 1;
   */
  name: string;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.FinalizeWriteStreamRequest.
 * Use `create(FinalizeWriteStreamRequestSchema)` to create a new message.
 */
export const FinalizeWriteStreamRequestSchema: GenMessage<FinalizeWriteStreamRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 13);

/**
 * Response message for `FinalizeWriteStream`.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.FinalizeWriteStreamResponse
 */
export type FinalizeWriteStreamResponse = Message<"google.cloud.bigquery.storage.v1beta2.FinalizeWriteStreamResponse"> & {
  /**
   * Number of rows in the finalized stream.
   *
   * @generated from field: int64 row_count = 1;
   */
  rowCount: bigint;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.FinalizeWriteStreamResponse.
 * Use `create(FinalizeWriteStreamResponseSchema)` to create a new message.
 */
export const FinalizeWriteStreamResponseSchema: GenMessage<FinalizeWriteStreamResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 14);

/**
 * Request message for `FlushRows`.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.FlushRowsRequest
 */
export type FlushRowsRequest = Message<"google.cloud.bigquery.storage.v1beta2.FlushRowsRequest"> & {
  /**
   * Required. The stream that is the target of the flush operation.
   *
   * @generated from field: string write_stream = 1;
   */
  writeStream: string;

  /**
   * Ending offset of the flush operation. Rows before this offset(including
   * this offset) will be flushed.
   *
   * @generated from field: google.protobuf.Int64Value offset = 2;
   */
  offset?: bigint;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.FlushRowsRequest.
 * Use `create(FlushRowsRequestSchema)` to create a new message.
 */
export const FlushRowsRequestSchema: GenMessage<FlushRowsRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 15);

/**
 * Respond message for `FlushRows`.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.FlushRowsResponse
 */
export type FlushRowsResponse = Message<"google.cloud.bigquery.storage.v1beta2.FlushRowsResponse"> & {
  /**
   * The rows before this offset (including this offset) are flushed.
   *
   * @generated from field: int64 offset = 1;
   */
  offset: bigint;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.FlushRowsResponse.
 * Use `create(FlushRowsResponseSchema)` to create a new message.
 */
export const FlushRowsResponseSchema: GenMessage<FlushRowsResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 16);

/**
 * Structured custom BigQuery Storage error message. The error can be attached
 * as error details in the returned rpc Status. In particular, the use of error
 * codes allows more structured error handling, and reduces the need to evaluate
 * unstructured error text strings.
 *
 * @generated from message google.cloud.bigquery.storage.v1beta2.StorageError
 */
export type StorageError = Message<"google.cloud.bigquery.storage.v1beta2.StorageError"> & {
  /**
   * BigQuery Storage specific error code.
   *
   * @generated from field: google.cloud.bigquery.storage.v1beta2.StorageError.StorageErrorCode code = 1;
   */
  code: StorageError_StorageErrorCode;

  /**
   * Name of the failed entity.
   *
   * @generated from field: string entity = 2;
   */
  entity: string;

  /**
   * Message that describes the error.
   *
   * @generated from field: string error_message = 3;
   */
  errorMessage: string;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1beta2.StorageError.
 * Use `create(StorageErrorSchema)` to create a new message.
 */
export const StorageErrorSchema: GenMessage<StorageError> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 17);

/**
 * Error code for `StorageError`.
 *
 * @generated from enum google.cloud.bigquery.storage.v1beta2.StorageError.StorageErrorCode
 */
export enum StorageError_StorageErrorCode {
  /**
   * Default error.
   *
   * @generated from enum value: STORAGE_ERROR_CODE_UNSPECIFIED = 0;
   */
  STORAGE_ERROR_CODE_UNSPECIFIED = 0,

  /**
   * Table is not found in the system.
   *
   * @generated from enum value: TABLE_NOT_FOUND = 1;
   */
  TABLE_NOT_FOUND = 1,

  /**
   * Stream is already committed.
   *
   * @generated from enum value: STREAM_ALREADY_COMMITTED = 2;
   */
  STREAM_ALREADY_COMMITTED = 2,

  /**
   * Stream is not found.
   *
   * @generated from enum value: STREAM_NOT_FOUND = 3;
   */
  STREAM_NOT_FOUND = 3,

  /**
   * Invalid Stream type.
   * For example, you try to commit a stream that is not pending.
   *
   * @generated from enum value: INVALID_STREAM_TYPE = 4;
   */
  INVALID_STREAM_TYPE = 4,

  /**
   * Invalid Stream state.
   * For example, you try to commit a stream that is not finalized or is
   * garbaged.
   *
   * @generated from enum value: INVALID_STREAM_STATE = 5;
   */
  INVALID_STREAM_STATE = 5,

  /**
   * Stream is finalized.
   *
   * @generated from enum value: STREAM_FINALIZED = 6;
   */
  STREAM_FINALIZED = 6,
}

/**
 * Describes the enum google.cloud.bigquery.storage.v1beta2.StorageError.StorageErrorCode.
 */
export const StorageError_StorageErrorCodeSchema: GenEnum<StorageError_StorageErrorCode> = /*@__PURE__*/
  enumDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 17, 0);

/**
 * BigQuery Read API.
 *
 * The Read API can be used to read data from BigQuery.
 *
 * New code should use the v1 Read API going forward, if they don't use Write
 * API at the same time.
 *
 * @generated from service google.cloud.bigquery.storage.v1beta2.BigQueryRead
 */
export const BigQueryRead: GenService<{
  /**
   * Creates a new read session. A read session divides the contents of a
   * BigQuery table into one or more streams, which can then be used to read
   * data from the table. The read session also specifies properties of the
   * data to be read, such as a list of columns or a push-down filter describing
   * the rows to be returned.
   *
   * A particular row can be read by at most one stream. When the caller has
   * reached the end of each stream in the session, then all the data in the
   * table has been read.
   *
   * Data is assigned to each stream such that roughly the same number of
   * rows can be read from each stream. Because the server-side unit for
   * assigning data is collections of rows, the API does not guarantee that
   * each stream will return the same number or rows. Additionally, the
   * limits are enforced based on the number of pre-filtered rows, so some
   * filters can lead to lopsided assignments.
   *
   * Read sessions automatically expire 6 hours after they are created and do
   * not require manual clean-up by the caller.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1beta2.BigQueryRead.CreateReadSession
   */
  createReadSession: {
    methodKind: "unary";
    input: typeof CreateReadSessionRequestSchema;
    output: typeof ReadSessionSchema;
  },
  /**
   * Reads rows from the stream in the format prescribed by the ReadSession.
   * Each response contains one or more table rows, up to a maximum of 100 MiB
   * per response; read requests which attempt to read individual rows larger
   * than 100 MiB will fail.
   *
   * Each request also returns a set of stream statistics reflecting the current
   * state of the stream.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1beta2.BigQueryRead.ReadRows
   */
  readRows: {
    methodKind: "server_streaming";
    input: typeof ReadRowsRequestSchema;
    output: typeof ReadRowsResponseSchema;
  },
  /**
   * Splits a given `ReadStream` into two `ReadStream` objects. These
   * `ReadStream` objects are referred to as the primary and the residual
   * streams of the split. The original `ReadStream` can still be read from in
   * the same manner as before. Both of the returned `ReadStream` objects can
   * also be read from, and the rows returned by both child streams will be
   * the same as the rows read from the original stream.
   *
   * Moreover, the two child streams will be allocated back-to-back in the
   * original `ReadStream`. Concretely, it is guaranteed that for streams
   * original, primary, and residual, that original[0-j] = primary[0-j] and
   * original[j-n] = residual[0-m] once the streams have been read to
   * completion.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1beta2.BigQueryRead.SplitReadStream
   */
  splitReadStream: {
    methodKind: "unary";
    input: typeof SplitReadStreamRequestSchema;
    output: typeof SplitReadStreamResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 0);

/**
 * BigQuery Write API.
 *
 * The Write API can be used to write data to BigQuery.
 *
 *
 * The [google.cloud.bigquery.storage.v1
 *   API](/bigquery/docs/reference/storage/rpc/google.cloud.bigquery.storage.v1)
 *   should be used instead of the v1beta2 API for BigQueryWrite operations.
 *
 * @generated from service google.cloud.bigquery.storage.v1beta2.BigQueryWrite
 * @deprecated
 */
export const BigQueryWrite: GenService<{
  /**
   * Creates a write stream to the given table.
   * Additionally, every table has a special COMMITTED stream named '_default'
   * to which data can be written. This stream doesn't need to be created using
   * CreateWriteStream. It is a stream that can be used simultaneously by any
   * number of clients. Data written to this stream is considered committed as
   * soon as an acknowledgement is received.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1beta2.BigQueryWrite.CreateWriteStream
   * @deprecated
   */
  createWriteStream: {
    methodKind: "unary";
    input: typeof CreateWriteStreamRequestSchema;
    output: typeof WriteStreamSchema;
  },
  /**
   * Appends data to the given stream.
   *
   * If `offset` is specified, the `offset` is checked against the end of
   * stream. The server returns `OUT_OF_RANGE` in `AppendRowsResponse` if an
   * attempt is made to append to an offset beyond the current end of the stream
   * or `ALREADY_EXISTS` if user provids an `offset` that has already been
   * written to. User can retry with adjusted offset within the same RPC
   * stream. If `offset` is not specified, append happens at the end of the
   * stream.
   *
   * The response contains the offset at which the append happened. Responses
   * are received in the same order in which requests are sent. There will be
   * one response for each successful request. If the `offset` is not set in
   * response, it means append didn't happen due to some errors. If one request
   * fails, all the subsequent requests will also fail until a success request
   * is made again.
   *
   * If the stream is of `PENDING` type, data will only be available for read
   * operations after the stream is committed.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1beta2.BigQueryWrite.AppendRows
   * @deprecated
   */
  appendRows: {
    methodKind: "bidi_streaming";
    input: typeof AppendRowsRequestSchema;
    output: typeof AppendRowsResponseSchema;
  },
  /**
   * Gets a write stream.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1beta2.BigQueryWrite.GetWriteStream
   * @deprecated
   */
  getWriteStream: {
    methodKind: "unary";
    input: typeof GetWriteStreamRequestSchema;
    output: typeof WriteStreamSchema;
  },
  /**
   * Finalize a write stream so that no new data can be appended to the
   * stream. Finalize is not supported on the '_default' stream.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1beta2.BigQueryWrite.FinalizeWriteStream
   * @deprecated
   */
  finalizeWriteStream: {
    methodKind: "unary";
    input: typeof FinalizeWriteStreamRequestSchema;
    output: typeof FinalizeWriteStreamResponseSchema;
  },
  /**
   * Atomically commits a group of `PENDING` streams that belong to the same
   * `parent` table.
   * Streams must be finalized before commit and cannot be committed multiple
   * times. Once a stream is committed, data in the stream becomes available
   * for read operations.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1beta2.BigQueryWrite.BatchCommitWriteStreams
   * @deprecated
   */
  batchCommitWriteStreams: {
    methodKind: "unary";
    input: typeof BatchCommitWriteStreamsRequestSchema;
    output: typeof BatchCommitWriteStreamsResponseSchema;
  },
  /**
   * Flushes rows to a BUFFERED stream.
   * If users are appending rows to BUFFERED stream, flush operation is
   * required in order for the rows to become available for reading. A
   * Flush operation flushes up to any previously flushed offset in a BUFFERED
   * stream, to the offset specified in the request.
   * Flush is not supported on the _default stream, since it is not BUFFERED.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1beta2.BigQueryWrite.FlushRows
   * @deprecated
   */
  flushRows: {
    methodKind: "unary";
    input: typeof FlushRowsRequestSchema;
    output: typeof FlushRowsResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_bigquery_storage_v1beta2_storage, 1);

