// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/bigquery/storage/v1/storage.proto (package google.cloud.bigquery.storage.v1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../../api/annotations_pb";
import { file_google_api_client } from "../../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../../api/field_behavior_pb";
import { file_google_api_resource } from "../../../../api/resource_pb";
import type { ArrowRecordBatch, ArrowSchema } from "./arrow_pb";
import { file_google_cloud_bigquery_storage_v1_arrow } from "./arrow_pb";
import type { AvroRows, AvroSchema } from "./avro_pb";
import { file_google_cloud_bigquery_storage_v1_avro } from "./avro_pb";
import type { ProtoRows, ProtoSchema } from "./protobuf_pb";
import { file_google_cloud_bigquery_storage_v1_protobuf } from "./protobuf_pb";
import type { ReadSession, ReadSessionSchema, ReadStream, WriteStream, WriteStreamSchema, WriteStreamView } from "./stream_pb";
import { file_google_cloud_bigquery_storage_v1_stream } from "./stream_pb";
import type { TableSchema } from "./table_pb";
import { file_google_cloud_bigquery_storage_v1_table } from "./table_pb";
import type { Timestamp } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_timestamp, file_google_protobuf_wrappers } from "@bufbuild/protobuf/wkt";
import type { Status } from "../../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../../rpc/status_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/bigquery/storage/v1/storage.proto.
 */
export const file_google_cloud_bigquery_storage_v1_storage: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_api_resource, file_google_cloud_bigquery_storage_v1_arrow, file_google_cloud_bigquery_storage_v1_avro, file_google_cloud_bigquery_storage_v1_protobuf, file_google_cloud_bigquery_storage_v1_stream, file_google_cloud_bigquery_storage_v1_table, file_google_protobuf_timestamp, file_google_protobuf_wrappers, file_google_rpc_status]);

/**
 * Request message for `CreateReadSession`.
 *
 * @generated from message google.cloud.bigquery.storage.v1.CreateReadSessionRequest
 */
export type CreateReadSessionRequest = Message<"google.cloud.bigquery.storage.v1.CreateReadSessionRequest"> & {
  /**
   * Required. The request project that owns the session, in the form of
   * `projects/{project_id}`.
   *
   * @generated from field: string parent = 1;
   */
  parent: string;

  /**
   * Required. Session to be created.
   *
   * @generated from field: google.cloud.bigquery.storage.v1.ReadSession read_session = 2;
   */
  readSession?: ReadSession;

  /**
   * Max initial number of streams. If unset or zero, the server will
   * provide a value of streams so as to produce reasonable throughput. Must be
   * non-negative. The number of streams may be lower than the requested number,
   * depending on the amount parallelism that is reasonable for the table.
   * There is a default system max limit of 1,000.
   *
   * This must be greater than or equal to preferred_min_stream_count.
   * Typically, clients should either leave this unset to let the system to
   * determine an upper bound OR set this a size for the maximum "units of work"
   * it can gracefully handle.
   *
   * @generated from field: int32 max_stream_count = 3;
   */
  maxStreamCount: number;

  /**
   * The minimum preferred stream count. This parameter can be used to inform
   * the service that there is a desired lower bound on the number of streams.
   * This is typically a target parallelism of the client (e.g. a Spark
   * cluster with N-workers would set this to a low multiple of N to ensure
   * good cluster utilization).
   *
   * The system will make a best effort to provide at least this number of
   * streams, but in some cases might provide less.
   *
   * @generated from field: int32 preferred_min_stream_count = 4;
   */
  preferredMinStreamCount: number;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.CreateReadSessionRequest.
 * Use `create(CreateReadSessionRequestSchema)` to create a new message.
 */
export const CreateReadSessionRequestSchema: GenMessage<CreateReadSessionRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 0);

/**
 * Request message for `ReadRows`.
 *
 * @generated from message google.cloud.bigquery.storage.v1.ReadRowsRequest
 */
export type ReadRowsRequest = Message<"google.cloud.bigquery.storage.v1.ReadRowsRequest"> & {
  /**
   * Required. Stream to read rows from.
   *
   * @generated from field: string read_stream = 1;
   */
  readStream: string;

  /**
   * The offset requested must be less than the last row read from Read.
   * Requesting a larger offset is undefined. If not specified, start reading
   * from offset zero.
   *
   * @generated from field: int64 offset = 2;
   */
  offset: bigint;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.ReadRowsRequest.
 * Use `create(ReadRowsRequestSchema)` to create a new message.
 */
export const ReadRowsRequestSchema: GenMessage<ReadRowsRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 1);

/**
 * Information on if the current connection is being throttled.
 *
 * @generated from message google.cloud.bigquery.storage.v1.ThrottleState
 */
export type ThrottleState = Message<"google.cloud.bigquery.storage.v1.ThrottleState"> & {
  /**
   * How much this connection is being throttled. Zero means no throttling,
   * 100 means fully throttled.
   *
   * @generated from field: int32 throttle_percent = 1;
   */
  throttlePercent: number;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.ThrottleState.
 * Use `create(ThrottleStateSchema)` to create a new message.
 */
export const ThrottleStateSchema: GenMessage<ThrottleState> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 2);

/**
 * Estimated stream statistics for a given read Stream.
 *
 * @generated from message google.cloud.bigquery.storage.v1.StreamStats
 */
export type StreamStats = Message<"google.cloud.bigquery.storage.v1.StreamStats"> & {
  /**
   * Represents the progress of the current stream.
   *
   * @generated from field: google.cloud.bigquery.storage.v1.StreamStats.Progress progress = 2;
   */
  progress?: StreamStats_Progress;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.StreamStats.
 * Use `create(StreamStatsSchema)` to create a new message.
 */
export const StreamStatsSchema: GenMessage<StreamStats> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 3);

/**
 * @generated from message google.cloud.bigquery.storage.v1.StreamStats.Progress
 */
export type StreamStats_Progress = Message<"google.cloud.bigquery.storage.v1.StreamStats.Progress"> & {
  /**
   * The fraction of rows assigned to the stream that have been processed by
   * the server so far, not including the rows in the current response
   * message.
   *
   * This value, along with `at_response_end`, can be used to interpolate
   * the progress made as the rows in the message are being processed using
   * the following formula: `at_response_start + (at_response_end -
   * at_response_start) * rows_processed_from_response / rows_in_response`.
   *
   * Note that if a filter is provided, the `at_response_end` value of the
   * previous response may not necessarily be equal to the
   * `at_response_start` value of the current response.
   *
   * @generated from field: double at_response_start = 1;
   */
  atResponseStart: number;

  /**
   * Similar to `at_response_start`, except that this value includes the
   * rows in the current response.
   *
   * @generated from field: double at_response_end = 2;
   */
  atResponseEnd: number;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.StreamStats.Progress.
 * Use `create(StreamStats_ProgressSchema)` to create a new message.
 */
export const StreamStats_ProgressSchema: GenMessage<StreamStats_Progress> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 3, 0);

/**
 * Response from calling `ReadRows` may include row data, progress and
 * throttling information.
 *
 * @generated from message google.cloud.bigquery.storage.v1.ReadRowsResponse
 */
export type ReadRowsResponse = Message<"google.cloud.bigquery.storage.v1.ReadRowsResponse"> & {
  /**
   * Row data is returned in format specified during session creation.
   *
   * @generated from oneof google.cloud.bigquery.storage.v1.ReadRowsResponse.rows
   */
  rows: {
    /**
     * Serialized row data in AVRO format.
     *
     * @generated from field: google.cloud.bigquery.storage.v1.AvroRows avro_rows = 3;
     */
    value: AvroRows;
    case: "avroRows";
  } | {
    /**
     * Serialized row data in Arrow RecordBatch format.
     *
     * @generated from field: google.cloud.bigquery.storage.v1.ArrowRecordBatch arrow_record_batch = 4;
     */
    value: ArrowRecordBatch;
    case: "arrowRecordBatch";
  } | { case: undefined; value?: undefined };

  /**
   * Number of serialized rows in the rows block.
   *
   * @generated from field: int64 row_count = 6;
   */
  rowCount: bigint;

  /**
   * Statistics for the stream.
   *
   * @generated from field: google.cloud.bigquery.storage.v1.StreamStats stats = 2;
   */
  stats?: StreamStats;

  /**
   * Throttling state. If unset, the latest response still describes
   * the current throttling status.
   *
   * @generated from field: google.cloud.bigquery.storage.v1.ThrottleState throttle_state = 5;
   */
  throttleState?: ThrottleState;

  /**
   * The schema for the read. If read_options.selected_fields is set, the
   * schema may be different from the table schema as it will only contain
   * the selected fields. This schema is equivalent to the one returned by
   * CreateSession. This field is only populated in the first ReadRowsResponse
   * RPC.
   *
   * @generated from oneof google.cloud.bigquery.storage.v1.ReadRowsResponse.schema
   */
  schema: {
    /**
     * Output only. Avro schema.
     *
     * @generated from field: google.cloud.bigquery.storage.v1.AvroSchema avro_schema = 7;
     */
    value: AvroSchema;
    case: "avroSchema";
  } | {
    /**
     * Output only. Arrow schema.
     *
     * @generated from field: google.cloud.bigquery.storage.v1.ArrowSchema arrow_schema = 8;
     */
    value: ArrowSchema;
    case: "arrowSchema";
  } | { case: undefined; value?: undefined };

  /**
   * Optional. If the row data in this ReadRowsResponse is compressed, then
   * uncompressed byte size is the original size of the uncompressed row data.
   * If it is set to a value greater than 0, then decompress into a buffer of
   * size uncompressed_byte_size using the compression codec that was requested
   * during session creation time and which is specified in
   * TableReadOptions.response_compression_codec in ReadSession.
   * This value is not set if no response_compression_codec was not requested
   * and it is -1 if the requested compression would not have reduced the size
   * of this ReadRowsResponse's row data. This attempts to match Apache Arrow's
   * behavior described here https://github.com/apache/arrow/issues/15102 where
   * the uncompressed length may be set to -1 to indicate that the data that
   * follows is not compressed, which can be useful for cases where compression
   * does not yield appreciable savings. When uncompressed_byte_size is not
   * greater than 0, the client should skip decompression.
   *
   * @generated from field: optional int64 uncompressed_byte_size = 9;
   */
  uncompressedByteSize?: bigint;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.ReadRowsResponse.
 * Use `create(ReadRowsResponseSchema)` to create a new message.
 */
export const ReadRowsResponseSchema: GenMessage<ReadRowsResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 4);

/**
 * Request message for `SplitReadStream`.
 *
 * @generated from message google.cloud.bigquery.storage.v1.SplitReadStreamRequest
 */
export type SplitReadStreamRequest = Message<"google.cloud.bigquery.storage.v1.SplitReadStreamRequest"> & {
  /**
   * Required. Name of the stream to split.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * A value in the range (0.0, 1.0) that specifies the fractional point at
   * which the original stream should be split. The actual split point is
   * evaluated on pre-filtered rows, so if a filter is provided, then there is
   * no guarantee that the division of the rows between the new child streams
   * will be proportional to this fractional value. Additionally, because the
   * server-side unit for assigning data is collections of rows, this fraction
   * will always map to a data storage boundary on the server side.
   *
   * @generated from field: double fraction = 2;
   */
  fraction: number;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.SplitReadStreamRequest.
 * Use `create(SplitReadStreamRequestSchema)` to create a new message.
 */
export const SplitReadStreamRequestSchema: GenMessage<SplitReadStreamRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 5);

/**
 * Response message for `SplitReadStream`.
 *
 * @generated from message google.cloud.bigquery.storage.v1.SplitReadStreamResponse
 */
export type SplitReadStreamResponse = Message<"google.cloud.bigquery.storage.v1.SplitReadStreamResponse"> & {
  /**
   * Primary stream, which contains the beginning portion of
   * |original_stream|. An empty value indicates that the original stream can no
   * longer be split.
   *
   * @generated from field: google.cloud.bigquery.storage.v1.ReadStream primary_stream = 1;
   */
  primaryStream?: ReadStream;

  /**
   * Remainder stream, which contains the tail of |original_stream|. An empty
   * value indicates that the original stream can no longer be split.
   *
   * @generated from field: google.cloud.bigquery.storage.v1.ReadStream remainder_stream = 2;
   */
  remainderStream?: ReadStream;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.SplitReadStreamResponse.
 * Use `create(SplitReadStreamResponseSchema)` to create a new message.
 */
export const SplitReadStreamResponseSchema: GenMessage<SplitReadStreamResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 6);

/**
 * Request message for `CreateWriteStream`.
 *
 * @generated from message google.cloud.bigquery.storage.v1.CreateWriteStreamRequest
 */
export type CreateWriteStreamRequest = Message<"google.cloud.bigquery.storage.v1.CreateWriteStreamRequest"> & {
  /**
   * Required. Reference to the table to which the stream belongs, in the format
   * of `projects/{project}/datasets/{dataset}/tables/{table}`.
   *
   * @generated from field: string parent = 1;
   */
  parent: string;

  /**
   * Required. Stream to be created.
   *
   * @generated from field: google.cloud.bigquery.storage.v1.WriteStream write_stream = 2;
   */
  writeStream?: WriteStream;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.CreateWriteStreamRequest.
 * Use `create(CreateWriteStreamRequestSchema)` to create a new message.
 */
export const CreateWriteStreamRequestSchema: GenMessage<CreateWriteStreamRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 7);

/**
 * Request message for `AppendRows`.
 *
 * Because AppendRows is a bidirectional streaming RPC, certain parts of the
 * AppendRowsRequest need only be specified for the first request before
 * switching table destinations. You can also switch table destinations within
 * the same connection for the default stream.
 *
 * The size of a single AppendRowsRequest must be less than 10 MB in size.
 * Requests larger than this return an error, typically `INVALID_ARGUMENT`.
 *
 * @generated from message google.cloud.bigquery.storage.v1.AppendRowsRequest
 */
export type AppendRowsRequest = Message<"google.cloud.bigquery.storage.v1.AppendRowsRequest"> & {
  /**
   * Required. The write_stream identifies the append operation. It must be
   * provided in the following scenarios:
   *
   * * In the first request to an AppendRows connection.
   *
   * * In all subsequent requests to an AppendRows connection, if you use the
   * same connection to write to multiple tables or change the input schema for
   * default streams.
   *
   * For explicitly created write streams, the format is:
   *
   * * `projects/{project}/datasets/{dataset}/tables/{table}/streams/{id}`
   *
   * For the special default stream, the format is:
   *
   * * `projects/{project}/datasets/{dataset}/tables/{table}/streams/_default`.
   *
   * An example of a possible sequence of requests with write_stream fields
   * within a single connection:
   *
   * * r1: {write_stream: stream_name_1}
   *
   * * r2: {write_stream: /*omit*\/}
   *
   * * r3: {write_stream: /*omit*\/}
   *
   * * r4: {write_stream: stream_name_2}
   *
   * * r5: {write_stream: stream_name_2}
   *
   * The destination changed in request_4, so the write_stream field must be
   * populated in all subsequent requests in this stream.
   *
   * @generated from field: string write_stream = 1;
   */
  writeStream: string;

  /**
   * If present, the write is only performed if the next append offset is same
   * as the provided value. If not present, the write is performed at the
   * current end of stream. Specifying a value for this field is not allowed
   * when calling AppendRows for the '_default' stream.
   *
   * @generated from field: google.protobuf.Int64Value offset = 2;
   */
  offset?: bigint;

  /**
   * Input rows. The `writer_schema` field must be specified at the initial
   * request and currently, it will be ignored if specified in following
   * requests. Following requests must have data in the same format as the
   * initial request.
   *
   * @generated from oneof google.cloud.bigquery.storage.v1.AppendRowsRequest.rows
   */
  rows: {
    /**
     * Rows in proto format.
     *
     * @generated from field: google.cloud.bigquery.storage.v1.AppendRowsRequest.ProtoData proto_rows = 4;
     */
    value: AppendRowsRequest_ProtoData;
    case: "protoRows";
  } | { case: undefined; value?: undefined };

  /**
   * Id set by client to annotate its identity. Only initial request setting is
   * respected.
   *
   * @generated from field: string trace_id = 6;
   */
  traceId: string;

  /**
   * A map to indicate how to interpret missing value for some fields. Missing
   * values are fields present in user schema but missing in rows. The key is
   * the field name. The value is the interpretation of missing values for the
   * field.
   *
   * For example, a map {'foo': NULL_VALUE, 'bar': DEFAULT_VALUE} means all
   * missing values in field foo are interpreted as NULL, all missing values in
   * field bar are interpreted as the default value of field bar in table
   * schema.
   *
   * If a field is not in this map and has missing values, the missing values
   * in this field are interpreted as NULL.
   *
   * This field only applies to the current request, it won't affect other
   * requests on the connection.
   *
   * Currently, field name can only be top-level column name, can't be a struct
   * field path like 'foo.bar'.
   *
   * @generated from field: map<string, google.cloud.bigquery.storage.v1.AppendRowsRequest.MissingValueInterpretation> missing_value_interpretations = 7;
   */
  missingValueInterpretations: { [key: string]: AppendRowsRequest_MissingValueInterpretation };

  /**
   * Optional. Default missing value interpretation for all columns in the
   * table. When a value is specified on an `AppendRowsRequest`, it is applied
   * to all requests on the connection from that point forward, until a
   * subsequent `AppendRowsRequest` sets it to a different value.
   * `missing_value_interpretation` can override
   * `default_missing_value_interpretation`. For example, if you want to write
   * `NULL` instead of using default values for some columns, you can set
   * `default_missing_value_interpretation` to `DEFAULT_VALUE` and at the same
   * time, set `missing_value_interpretations` to `NULL_VALUE` on those columns.
   *
   * @generated from field: google.cloud.bigquery.storage.v1.AppendRowsRequest.MissingValueInterpretation default_missing_value_interpretation = 8;
   */
  defaultMissingValueInterpretation: AppendRowsRequest_MissingValueInterpretation;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.AppendRowsRequest.
 * Use `create(AppendRowsRequestSchema)` to create a new message.
 */
export const AppendRowsRequestSchema: GenMessage<AppendRowsRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 8);

/**
 * ProtoData contains the data rows and schema when constructing append
 * requests.
 *
 * @generated from message google.cloud.bigquery.storage.v1.AppendRowsRequest.ProtoData
 */
export type AppendRowsRequest_ProtoData = Message<"google.cloud.bigquery.storage.v1.AppendRowsRequest.ProtoData"> & {
  /**
   * The protocol buffer schema used to serialize the data. Provide this value
   * whenever:
   *
   * * You send the first request of an RPC connection.
   *
   * * You change the input schema.
   *
   * * You specify a new destination table.
   *
   * @generated from field: google.cloud.bigquery.storage.v1.ProtoSchema writer_schema = 1;
   */
  writerSchema?: ProtoSchema;

  /**
   * Serialized row data in protobuf message format.
   * Currently, the backend expects the serialized rows to adhere to
   * proto2 semantics when appending rows, particularly with respect to
   * how default values are encoded.
   *
   * @generated from field: google.cloud.bigquery.storage.v1.ProtoRows rows = 2;
   */
  rows?: ProtoRows;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.AppendRowsRequest.ProtoData.
 * Use `create(AppendRowsRequest_ProtoDataSchema)` to create a new message.
 */
export const AppendRowsRequest_ProtoDataSchema: GenMessage<AppendRowsRequest_ProtoData> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 8, 0);

/**
 * An enum to indicate how to interpret missing values of fields that are
 * present in user schema but missing in rows. A missing value can represent a
 * NULL or a column default value defined in BigQuery table schema.
 *
 * @generated from enum google.cloud.bigquery.storage.v1.AppendRowsRequest.MissingValueInterpretation
 */
export enum AppendRowsRequest_MissingValueInterpretation {
  /**
   * Invalid missing value interpretation. Requests with this value will be
   * rejected.
   *
   * @generated from enum value: MISSING_VALUE_INTERPRETATION_UNSPECIFIED = 0;
   */
  MISSING_VALUE_INTERPRETATION_UNSPECIFIED = 0,

  /**
   * Missing value is interpreted as NULL.
   *
   * @generated from enum value: NULL_VALUE = 1;
   */
  NULL_VALUE = 1,

  /**
   * Missing value is interpreted as column default value if declared in the
   * table schema, NULL otherwise.
   *
   * @generated from enum value: DEFAULT_VALUE = 2;
   */
  DEFAULT_VALUE = 2,
}

/**
 * Describes the enum google.cloud.bigquery.storage.v1.AppendRowsRequest.MissingValueInterpretation.
 */
export const AppendRowsRequest_MissingValueInterpretationSchema: GenEnum<AppendRowsRequest_MissingValueInterpretation> = /*@__PURE__*/
  enumDesc(file_google_cloud_bigquery_storage_v1_storage, 8, 0);

/**
 * Response message for `AppendRows`.
 *
 * @generated from message google.cloud.bigquery.storage.v1.AppendRowsResponse
 */
export type AppendRowsResponse = Message<"google.cloud.bigquery.storage.v1.AppendRowsResponse"> & {
  /**
   * @generated from oneof google.cloud.bigquery.storage.v1.AppendRowsResponse.response
   */
  response: {
    /**
     * Result if the append is successful.
     *
     * @generated from field: google.cloud.bigquery.storage.v1.AppendRowsResponse.AppendResult append_result = 1;
     */
    value: AppendRowsResponse_AppendResult;
    case: "appendResult";
  } | {
    /**
     * Error returned when problems were encountered.  If present,
     * it indicates rows were not accepted into the system.
     * Users can retry or continue with other append requests within the
     * same connection.
     *
     * Additional information about error signalling:
     *
     * ALREADY_EXISTS: Happens when an append specified an offset, and the
     * backend already has received data at this offset.  Typically encountered
     * in retry scenarios, and can be ignored.
     *
     * OUT_OF_RANGE: Returned when the specified offset in the stream is beyond
     * the current end of the stream.
     *
     * INVALID_ARGUMENT: Indicates a malformed request or data.
     *
     * ABORTED: Request processing is aborted because of prior failures.  The
     * request can be retried if previous failure is addressed.
     *
     * INTERNAL: Indicates server side error(s) that can be retried.
     *
     * @generated from field: google.rpc.Status error = 2;
     */
    value: Status;
    case: "error";
  } | { case: undefined; value?: undefined };

  /**
   * If backend detects a schema update, pass it to user so that user can
   * use it to input new type of message. It will be empty when no schema
   * updates have occurred.
   *
   * @generated from field: google.cloud.bigquery.storage.v1.TableSchema updated_schema = 3;
   */
  updatedSchema?: TableSchema;

  /**
   * If a request failed due to corrupted rows, no rows in the batch will be
   * appended. The API will return row level error info, so that the caller can
   * remove the bad rows and retry the request.
   *
   * @generated from field: repeated google.cloud.bigquery.storage.v1.RowError row_errors = 4;
   */
  rowErrors: RowError[];

  /**
   * The target of the append operation. Matches the write_stream in the
   * corresponding request.
   *
   * @generated from field: string write_stream = 5;
   */
  writeStream: string;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.AppendRowsResponse.
 * Use `create(AppendRowsResponseSchema)` to create a new message.
 */
export const AppendRowsResponseSchema: GenMessage<AppendRowsResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 9);

/**
 * AppendResult is returned for successful append requests.
 *
 * @generated from message google.cloud.bigquery.storage.v1.AppendRowsResponse.AppendResult
 */
export type AppendRowsResponse_AppendResult = Message<"google.cloud.bigquery.storage.v1.AppendRowsResponse.AppendResult"> & {
  /**
   * The row offset at which the last append occurred. The offset will not be
   * set if appending using default streams.
   *
   * @generated from field: google.protobuf.Int64Value offset = 1;
   */
  offset?: bigint;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.AppendRowsResponse.AppendResult.
 * Use `create(AppendRowsResponse_AppendResultSchema)` to create a new message.
 */
export const AppendRowsResponse_AppendResultSchema: GenMessage<AppendRowsResponse_AppendResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 9, 0);

/**
 * Request message for `GetWriteStreamRequest`.
 *
 * @generated from message google.cloud.bigquery.storage.v1.GetWriteStreamRequest
 */
export type GetWriteStreamRequest = Message<"google.cloud.bigquery.storage.v1.GetWriteStreamRequest"> & {
  /**
   * Required. Name of the stream to get, in the form of
   * `projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}`.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Indicates whether to get full or partial view of the WriteStream. If
   * not set, view returned will be basic.
   *
   * @generated from field: google.cloud.bigquery.storage.v1.WriteStreamView view = 3;
   */
  view: WriteStreamView;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.GetWriteStreamRequest.
 * Use `create(GetWriteStreamRequestSchema)` to create a new message.
 */
export const GetWriteStreamRequestSchema: GenMessage<GetWriteStreamRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 10);

/**
 * Request message for `BatchCommitWriteStreams`.
 *
 * @generated from message google.cloud.bigquery.storage.v1.BatchCommitWriteStreamsRequest
 */
export type BatchCommitWriteStreamsRequest = Message<"google.cloud.bigquery.storage.v1.BatchCommitWriteStreamsRequest"> & {
  /**
   * Required. Parent table that all the streams should belong to, in the form
   * of `projects/{project}/datasets/{dataset}/tables/{table}`.
   *
   * @generated from field: string parent = 1;
   */
  parent: string;

  /**
   * Required. The group of streams that will be committed atomically.
   *
   * @generated from field: repeated string write_streams = 2;
   */
  writeStreams: string[];
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.BatchCommitWriteStreamsRequest.
 * Use `create(BatchCommitWriteStreamsRequestSchema)` to create a new message.
 */
export const BatchCommitWriteStreamsRequestSchema: GenMessage<BatchCommitWriteStreamsRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 11);

/**
 * Response message for `BatchCommitWriteStreams`.
 *
 * @generated from message google.cloud.bigquery.storage.v1.BatchCommitWriteStreamsResponse
 */
export type BatchCommitWriteStreamsResponse = Message<"google.cloud.bigquery.storage.v1.BatchCommitWriteStreamsResponse"> & {
  /**
   * The time at which streams were committed in microseconds granularity.
   * This field will only exist when there are no stream errors.
   * **Note** if this field is not set, it means the commit was not successful.
   *
   * @generated from field: google.protobuf.Timestamp commit_time = 1;
   */
  commitTime?: Timestamp;

  /**
   * Stream level error if commit failed. Only streams with error will be in
   * the list.
   * If empty, there is no error and all streams are committed successfully.
   * If non empty, certain streams have errors and ZERO stream is committed due
   * to atomicity guarantee.
   *
   * @generated from field: repeated google.cloud.bigquery.storage.v1.StorageError stream_errors = 2;
   */
  streamErrors: StorageError[];
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.BatchCommitWriteStreamsResponse.
 * Use `create(BatchCommitWriteStreamsResponseSchema)` to create a new message.
 */
export const BatchCommitWriteStreamsResponseSchema: GenMessage<BatchCommitWriteStreamsResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 12);

/**
 * Request message for invoking `FinalizeWriteStream`.
 *
 * @generated from message google.cloud.bigquery.storage.v1.FinalizeWriteStreamRequest
 */
export type FinalizeWriteStreamRequest = Message<"google.cloud.bigquery.storage.v1.FinalizeWriteStreamRequest"> & {
  /**
   * Required. Name of the stream to finalize, in the form of
   * `projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}`.
   *
   * @generated from field: string name = 1;
   */
  name: string;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.FinalizeWriteStreamRequest.
 * Use `create(FinalizeWriteStreamRequestSchema)` to create a new message.
 */
export const FinalizeWriteStreamRequestSchema: GenMessage<FinalizeWriteStreamRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 13);

/**
 * Response message for `FinalizeWriteStream`.
 *
 * @generated from message google.cloud.bigquery.storage.v1.FinalizeWriteStreamResponse
 */
export type FinalizeWriteStreamResponse = Message<"google.cloud.bigquery.storage.v1.FinalizeWriteStreamResponse"> & {
  /**
   * Number of rows in the finalized stream.
   *
   * @generated from field: int64 row_count = 1;
   */
  rowCount: bigint;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.FinalizeWriteStreamResponse.
 * Use `create(FinalizeWriteStreamResponseSchema)` to create a new message.
 */
export const FinalizeWriteStreamResponseSchema: GenMessage<FinalizeWriteStreamResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 14);

/**
 * Request message for `FlushRows`.
 *
 * @generated from message google.cloud.bigquery.storage.v1.FlushRowsRequest
 */
export type FlushRowsRequest = Message<"google.cloud.bigquery.storage.v1.FlushRowsRequest"> & {
  /**
   * Required. The stream that is the target of the flush operation.
   *
   * @generated from field: string write_stream = 1;
   */
  writeStream: string;

  /**
   * Ending offset of the flush operation. Rows before this offset(including
   * this offset) will be flushed.
   *
   * @generated from field: google.protobuf.Int64Value offset = 2;
   */
  offset?: bigint;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.FlushRowsRequest.
 * Use `create(FlushRowsRequestSchema)` to create a new message.
 */
export const FlushRowsRequestSchema: GenMessage<FlushRowsRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 15);

/**
 * Respond message for `FlushRows`.
 *
 * @generated from message google.cloud.bigquery.storage.v1.FlushRowsResponse
 */
export type FlushRowsResponse = Message<"google.cloud.bigquery.storage.v1.FlushRowsResponse"> & {
  /**
   * The rows before this offset (including this offset) are flushed.
   *
   * @generated from field: int64 offset = 1;
   */
  offset: bigint;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.FlushRowsResponse.
 * Use `create(FlushRowsResponseSchema)` to create a new message.
 */
export const FlushRowsResponseSchema: GenMessage<FlushRowsResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 16);

/**
 * Structured custom BigQuery Storage error message. The error can be attached
 * as error details in the returned rpc Status. In particular, the use of error
 * codes allows more structured error handling, and reduces the need to evaluate
 * unstructured error text strings.
 *
 * @generated from message google.cloud.bigquery.storage.v1.StorageError
 */
export type StorageError = Message<"google.cloud.bigquery.storage.v1.StorageError"> & {
  /**
   * BigQuery Storage specific error code.
   *
   * @generated from field: google.cloud.bigquery.storage.v1.StorageError.StorageErrorCode code = 1;
   */
  code: StorageError_StorageErrorCode;

  /**
   * Name of the failed entity.
   *
   * @generated from field: string entity = 2;
   */
  entity: string;

  /**
   * Message that describes the error.
   *
   * @generated from field: string error_message = 3;
   */
  errorMessage: string;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.StorageError.
 * Use `create(StorageErrorSchema)` to create a new message.
 */
export const StorageErrorSchema: GenMessage<StorageError> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 17);

/**
 * Error code for `StorageError`.
 *
 * @generated from enum google.cloud.bigquery.storage.v1.StorageError.StorageErrorCode
 */
export enum StorageError_StorageErrorCode {
  /**
   * Default error.
   *
   * @generated from enum value: STORAGE_ERROR_CODE_UNSPECIFIED = 0;
   */
  STORAGE_ERROR_CODE_UNSPECIFIED = 0,

  /**
   * Table is not found in the system.
   *
   * @generated from enum value: TABLE_NOT_FOUND = 1;
   */
  TABLE_NOT_FOUND = 1,

  /**
   * Stream is already committed.
   *
   * @generated from enum value: STREAM_ALREADY_COMMITTED = 2;
   */
  STREAM_ALREADY_COMMITTED = 2,

  /**
   * Stream is not found.
   *
   * @generated from enum value: STREAM_NOT_FOUND = 3;
   */
  STREAM_NOT_FOUND = 3,

  /**
   * Invalid Stream type.
   * For example, you try to commit a stream that is not pending.
   *
   * @generated from enum value: INVALID_STREAM_TYPE = 4;
   */
  INVALID_STREAM_TYPE = 4,

  /**
   * Invalid Stream state.
   * For example, you try to commit a stream that is not finalized or is
   * garbaged.
   *
   * @generated from enum value: INVALID_STREAM_STATE = 5;
   */
  INVALID_STREAM_STATE = 5,

  /**
   * Stream is finalized.
   *
   * @generated from enum value: STREAM_FINALIZED = 6;
   */
  STREAM_FINALIZED = 6,

  /**
   * There is a schema mismatch and it is caused by user schema has extra
   * field than bigquery schema.
   *
   * @generated from enum value: SCHEMA_MISMATCH_EXTRA_FIELDS = 7;
   */
  SCHEMA_MISMATCH_EXTRA_FIELDS = 7,

  /**
   * Offset already exists.
   *
   * @generated from enum value: OFFSET_ALREADY_EXISTS = 8;
   */
  OFFSET_ALREADY_EXISTS = 8,

  /**
   * Offset out of range.
   *
   * @generated from enum value: OFFSET_OUT_OF_RANGE = 9;
   */
  OFFSET_OUT_OF_RANGE = 9,

  /**
   * Customer-managed encryption key (CMEK) not provided for CMEK-enabled
   * data.
   *
   * @generated from enum value: CMEK_NOT_PROVIDED = 10;
   */
  CMEK_NOT_PROVIDED = 10,

  /**
   * Customer-managed encryption key (CMEK) was incorrectly provided.
   *
   * @generated from enum value: INVALID_CMEK_PROVIDED = 11;
   */
  INVALID_CMEK_PROVIDED = 11,

  /**
   * There is an encryption error while using customer-managed encryption key.
   *
   * @generated from enum value: CMEK_ENCRYPTION_ERROR = 12;
   */
  CMEK_ENCRYPTION_ERROR = 12,

  /**
   * Key Management Service (KMS) service returned an error, which can be
   * retried.
   *
   * @generated from enum value: KMS_SERVICE_ERROR = 13;
   */
  KMS_SERVICE_ERROR = 13,

  /**
   * Permission denied while using customer-managed encryption key.
   *
   * @generated from enum value: KMS_PERMISSION_DENIED = 14;
   */
  KMS_PERMISSION_DENIED = 14,
}

/**
 * Describes the enum google.cloud.bigquery.storage.v1.StorageError.StorageErrorCode.
 */
export const StorageError_StorageErrorCodeSchema: GenEnum<StorageError_StorageErrorCode> = /*@__PURE__*/
  enumDesc(file_google_cloud_bigquery_storage_v1_storage, 17, 0);

/**
 * The message that presents row level error info in a request.
 *
 * @generated from message google.cloud.bigquery.storage.v1.RowError
 */
export type RowError = Message<"google.cloud.bigquery.storage.v1.RowError"> & {
  /**
   * Index of the malformed row in the request.
   *
   * @generated from field: int64 index = 1;
   */
  index: bigint;

  /**
   * Structured error reason for a row error.
   *
   * @generated from field: google.cloud.bigquery.storage.v1.RowError.RowErrorCode code = 2;
   */
  code: RowError_RowErrorCode;

  /**
   * Description of the issue encountered when processing the row.
   *
   * @generated from field: string message = 3;
   */
  message: string;
};

/**
 * Describes the message google.cloud.bigquery.storage.v1.RowError.
 * Use `create(RowErrorSchema)` to create a new message.
 */
export const RowErrorSchema: GenMessage<RowError> = /*@__PURE__*/
  messageDesc(file_google_cloud_bigquery_storage_v1_storage, 18);

/**
 * Error code for `RowError`.
 *
 * @generated from enum google.cloud.bigquery.storage.v1.RowError.RowErrorCode
 */
export enum RowError_RowErrorCode {
  /**
   * Default error.
   *
   * @generated from enum value: ROW_ERROR_CODE_UNSPECIFIED = 0;
   */
  ROW_ERROR_CODE_UNSPECIFIED = 0,

  /**
   * One or more fields in the row has errors.
   *
   * @generated from enum value: FIELDS_ERROR = 1;
   */
  FIELDS_ERROR = 1,
}

/**
 * Describes the enum google.cloud.bigquery.storage.v1.RowError.RowErrorCode.
 */
export const RowError_RowErrorCodeSchema: GenEnum<RowError_RowErrorCode> = /*@__PURE__*/
  enumDesc(file_google_cloud_bigquery_storage_v1_storage, 18, 0);

/**
 * BigQuery Read API.
 *
 * The Read API can be used to read data from BigQuery.
 *
 * @generated from service google.cloud.bigquery.storage.v1.BigQueryRead
 */
export const BigQueryRead: GenService<{
  /**
   * Creates a new read session. A read session divides the contents of a
   * BigQuery table into one or more streams, which can then be used to read
   * data from the table. The read session also specifies properties of the
   * data to be read, such as a list of columns or a push-down filter describing
   * the rows to be returned.
   *
   * A particular row can be read by at most one stream. When the caller has
   * reached the end of each stream in the session, then all the data in the
   * table has been read.
   *
   * Data is assigned to each stream such that roughly the same number of
   * rows can be read from each stream. Because the server-side unit for
   * assigning data is collections of rows, the API does not guarantee that
   * each stream will return the same number or rows. Additionally, the
   * limits are enforced based on the number of pre-filtered rows, so some
   * filters can lead to lopsided assignments.
   *
   * Read sessions automatically expire 6 hours after they are created and do
   * not require manual clean-up by the caller.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1.BigQueryRead.CreateReadSession
   */
  createReadSession: {
    methodKind: "unary";
    input: typeof CreateReadSessionRequestSchema;
    output: typeof ReadSessionSchema;
  },
  /**
   * Reads rows from the stream in the format prescribed by the ReadSession.
   * Each response contains one or more table rows, up to a maximum of 100 MiB
   * per response; read requests which attempt to read individual rows larger
   * than 100 MiB will fail.
   *
   * Each request also returns a set of stream statistics reflecting the current
   * state of the stream.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1.BigQueryRead.ReadRows
   */
  readRows: {
    methodKind: "server_streaming";
    input: typeof ReadRowsRequestSchema;
    output: typeof ReadRowsResponseSchema;
  },
  /**
   * Splits a given `ReadStream` into two `ReadStream` objects. These
   * `ReadStream` objects are referred to as the primary and the residual
   * streams of the split. The original `ReadStream` can still be read from in
   * the same manner as before. Both of the returned `ReadStream` objects can
   * also be read from, and the rows returned by both child streams will be
   * the same as the rows read from the original stream.
   *
   * Moreover, the two child streams will be allocated back-to-back in the
   * original `ReadStream`. Concretely, it is guaranteed that for streams
   * original, primary, and residual, that original[0-j] = primary[0-j] and
   * original[j-n] = residual[0-m] once the streams have been read to
   * completion.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1.BigQueryRead.SplitReadStream
   */
  splitReadStream: {
    methodKind: "unary";
    input: typeof SplitReadStreamRequestSchema;
    output: typeof SplitReadStreamResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_bigquery_storage_v1_storage, 0);

/**
 * BigQuery Write API.
 *
 * The Write API can be used to write data to BigQuery.
 *
 * For supplementary information about the Write API, see:
 * https://cloud.google.com/bigquery/docs/write-api
 *
 * @generated from service google.cloud.bigquery.storage.v1.BigQueryWrite
 */
export const BigQueryWrite: GenService<{
  /**
   * Creates a write stream to the given table.
   * Additionally, every table has a special stream named '_default'
   * to which data can be written. This stream doesn't need to be created using
   * CreateWriteStream. It is a stream that can be used simultaneously by any
   * number of clients. Data written to this stream is considered committed as
   * soon as an acknowledgement is received.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1.BigQueryWrite.CreateWriteStream
   */
  createWriteStream: {
    methodKind: "unary";
    input: typeof CreateWriteStreamRequestSchema;
    output: typeof WriteStreamSchema;
  },
  /**
   * Appends data to the given stream.
   *
   * If `offset` is specified, the `offset` is checked against the end of
   * stream. The server returns `OUT_OF_RANGE` in `AppendRowsResponse` if an
   * attempt is made to append to an offset beyond the current end of the stream
   * or `ALREADY_EXISTS` if user provides an `offset` that has already been
   * written to. User can retry with adjusted offset within the same RPC
   * connection. If `offset` is not specified, append happens at the end of the
   * stream.
   *
   * The response contains an optional offset at which the append
   * happened.  No offset information will be returned for appends to a
   * default stream.
   *
   * Responses are received in the same order in which requests are sent.
   * There will be one response for each successful inserted request.  Responses
   * may optionally embed error information if the originating AppendRequest was
   * not successfully processed.
   *
   * The specifics of when successfully appended data is made visible to the
   * table are governed by the type of stream:
   *
   * * For COMMITTED streams (which includes the default stream), data is
   * visible immediately upon successful append.
   *
   * * For BUFFERED streams, data is made visible via a subsequent `FlushRows`
   * rpc which advances a cursor to a newer offset in the stream.
   *
   * * For PENDING streams, data is not made visible until the stream itself is
   * finalized (via the `FinalizeWriteStream` rpc), and the stream is explicitly
   * committed via the `BatchCommitWriteStreams` rpc.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1.BigQueryWrite.AppendRows
   */
  appendRows: {
    methodKind: "bidi_streaming";
    input: typeof AppendRowsRequestSchema;
    output: typeof AppendRowsResponseSchema;
  },
  /**
   * Gets information about a write stream.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1.BigQueryWrite.GetWriteStream
   */
  getWriteStream: {
    methodKind: "unary";
    input: typeof GetWriteStreamRequestSchema;
    output: typeof WriteStreamSchema;
  },
  /**
   * Finalize a write stream so that no new data can be appended to the
   * stream. Finalize is not supported on the '_default' stream.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1.BigQueryWrite.FinalizeWriteStream
   */
  finalizeWriteStream: {
    methodKind: "unary";
    input: typeof FinalizeWriteStreamRequestSchema;
    output: typeof FinalizeWriteStreamResponseSchema;
  },
  /**
   * Atomically commits a group of `PENDING` streams that belong to the same
   * `parent` table.
   *
   * Streams must be finalized before commit and cannot be committed multiple
   * times. Once a stream is committed, data in the stream becomes available
   * for read operations.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1.BigQueryWrite.BatchCommitWriteStreams
   */
  batchCommitWriteStreams: {
    methodKind: "unary";
    input: typeof BatchCommitWriteStreamsRequestSchema;
    output: typeof BatchCommitWriteStreamsResponseSchema;
  },
  /**
   * Flushes rows to a BUFFERED stream.
   *
   * If users are appending rows to BUFFERED stream, flush operation is
   * required in order for the rows to become available for reading. A
   * Flush operation flushes up to any previously flushed offset in a BUFFERED
   * stream, to the offset specified in the request.
   *
   * Flush is not supported on the _default stream, since it is not BUFFERED.
   *
   * @generated from rpc google.cloud.bigquery.storage.v1.BigQueryWrite.FlushRows
   */
  flushRows: {
    methodKind: "unary";
    input: typeof FlushRowsRequestSchema;
    output: typeof FlushRowsResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_bigquery_storage_v1_storage, 1);

