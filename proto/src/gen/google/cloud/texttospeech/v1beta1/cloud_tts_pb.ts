// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/texttospeech/v1beta1/cloud_tts.proto (package google.cloud.texttospeech.v1beta1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import { file_google_api_resource } from "../../../api/resource_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/texttospeech/v1beta1/cloud_tts.proto.
 */
export const file_google_cloud_texttospeech_v1beta1_cloud_tts: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_api_resource]);

/**
 * The top-level message sent by the client for the `ListVoices` method.
 *
 * @generated from message google.cloud.texttospeech.v1beta1.ListVoicesRequest
 */
export type ListVoicesRequest = Message<"google.cloud.texttospeech.v1beta1.ListVoicesRequest"> & {
  /**
   * Optional. Recommended.
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * If not specified, the API will return all supported voices.
   * If specified, the ListVoices call will only return voices that can be used
   * to synthesize this language_code. For example, if you specify `"en-NZ"`,
   * all `"en-NZ"` voices will be returned. If you specify `"no"`, both
   * `"no-\*"` (Norwegian) and `"nb-\*"` (Norwegian Bokmal) voices will be
   * returned.
   *
   * @generated from field: string language_code = 1;
   */
  languageCode: string;
};

/**
 * Describes the message google.cloud.texttospeech.v1beta1.ListVoicesRequest.
 * Use `create(ListVoicesRequestSchema)` to create a new message.
 */
export const ListVoicesRequestSchema: GenMessage<ListVoicesRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 0);

/**
 * The message returned to the client by the `ListVoices` method.
 *
 * @generated from message google.cloud.texttospeech.v1beta1.ListVoicesResponse
 */
export type ListVoicesResponse = Message<"google.cloud.texttospeech.v1beta1.ListVoicesResponse"> & {
  /**
   * The list of voices.
   *
   * @generated from field: repeated google.cloud.texttospeech.v1beta1.Voice voices = 1;
   */
  voices: Voice[];
};

/**
 * Describes the message google.cloud.texttospeech.v1beta1.ListVoicesResponse.
 * Use `create(ListVoicesResponseSchema)` to create a new message.
 */
export const ListVoicesResponseSchema: GenMessage<ListVoicesResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 1);

/**
 * Description of a voice supported by the TTS service.
 *
 * @generated from message google.cloud.texttospeech.v1beta1.Voice
 */
export type Voice = Message<"google.cloud.texttospeech.v1beta1.Voice"> & {
  /**
   * The languages that this voice supports, expressed as
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tags (e.g.
   * "en-US", "es-419", "cmn-tw").
   *
   * @generated from field: repeated string language_codes = 1;
   */
  languageCodes: string[];

  /**
   * The name of this voice.  Each distinct voice has a unique name.
   *
   * @generated from field: string name = 2;
   */
  name: string;

  /**
   * The gender of this voice.
   *
   * @generated from field: google.cloud.texttospeech.v1beta1.SsmlVoiceGender ssml_gender = 3;
   */
  ssmlGender: SsmlVoiceGender;

  /**
   * The natural sample rate (in hertz) for this voice.
   *
   * @generated from field: int32 natural_sample_rate_hertz = 4;
   */
  naturalSampleRateHertz: number;
};

/**
 * Describes the message google.cloud.texttospeech.v1beta1.Voice.
 * Use `create(VoiceSchema)` to create a new message.
 */
export const VoiceSchema: GenMessage<Voice> = /*@__PURE__*/
  messageDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 2);

/**
 * The top-level message sent by the client for the `SynthesizeSpeech` method.
 *
 * @generated from message google.cloud.texttospeech.v1beta1.SynthesizeSpeechRequest
 */
export type SynthesizeSpeechRequest = Message<"google.cloud.texttospeech.v1beta1.SynthesizeSpeechRequest"> & {
  /**
   * Required. The Synthesizer requires either plain text or SSML as input.
   *
   * @generated from field: google.cloud.texttospeech.v1beta1.SynthesisInput input = 1;
   */
  input?: SynthesisInput;

  /**
   * Required. The desired voice of the synthesized audio.
   *
   * @generated from field: google.cloud.texttospeech.v1beta1.VoiceSelectionParams voice = 2;
   */
  voice?: VoiceSelectionParams;

  /**
   * Required. The configuration of the synthesized audio.
   *
   * @generated from field: google.cloud.texttospeech.v1beta1.AudioConfig audio_config = 3;
   */
  audioConfig?: AudioConfig;

  /**
   * Whether and what timepoints are returned in the response.
   *
   * @generated from field: repeated google.cloud.texttospeech.v1beta1.SynthesizeSpeechRequest.TimepointType enable_time_pointing = 4;
   */
  enableTimePointing: SynthesizeSpeechRequest_TimepointType[];
};

/**
 * Describes the message google.cloud.texttospeech.v1beta1.SynthesizeSpeechRequest.
 * Use `create(SynthesizeSpeechRequestSchema)` to create a new message.
 */
export const SynthesizeSpeechRequestSchema: GenMessage<SynthesizeSpeechRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 3);

/**
 * The type of timepoint information that is returned in the response.
 *
 * @generated from enum google.cloud.texttospeech.v1beta1.SynthesizeSpeechRequest.TimepointType
 */
export enum SynthesizeSpeechRequest_TimepointType {
  /**
   * Not specified. No timepoint information will be returned.
   *
   * @generated from enum value: TIMEPOINT_TYPE_UNSPECIFIED = 0;
   */
  TIMEPOINT_TYPE_UNSPECIFIED = 0,

  /**
   * Timepoint information of `<mark>` tags in SSML input will be returned.
   *
   * @generated from enum value: SSML_MARK = 1;
   */
  SSML_MARK = 1,
}

/**
 * Describes the enum google.cloud.texttospeech.v1beta1.SynthesizeSpeechRequest.TimepointType.
 */
export const SynthesizeSpeechRequest_TimepointTypeSchema: GenEnum<SynthesizeSpeechRequest_TimepointType> = /*@__PURE__*/
  enumDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 3, 0);

/**
 * Contains text input to be synthesized. Either `text` or `ssml` must be
 * supplied. Supplying both or neither returns
 * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]. The
 * input size is limited to 5000 bytes.
 *
 * @generated from message google.cloud.texttospeech.v1beta1.SynthesisInput
 */
export type SynthesisInput = Message<"google.cloud.texttospeech.v1beta1.SynthesisInput"> & {
  /**
   * The input source, which is either plain text or SSML.
   *
   * @generated from oneof google.cloud.texttospeech.v1beta1.SynthesisInput.input_source
   */
  inputSource: {
    /**
     * The raw text to be synthesized.
     *
     * @generated from field: string text = 1;
     */
    value: string;
    case: "text";
  } | {
    /**
     * The SSML document to be synthesized. The SSML document must be valid
     * and well-formed. Otherwise the RPC will fail and return
     * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]. For
     * more information, see
     * [SSML](https://cloud.google.com/text-to-speech/docs/ssml).
     *
     * @generated from field: string ssml = 2;
     */
    value: string;
    case: "ssml";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.texttospeech.v1beta1.SynthesisInput.
 * Use `create(SynthesisInputSchema)` to create a new message.
 */
export const SynthesisInputSchema: GenMessage<SynthesisInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 4);

/**
 * Description of which voice to use for a synthesis request.
 *
 * @generated from message google.cloud.texttospeech.v1beta1.VoiceSelectionParams
 */
export type VoiceSelectionParams = Message<"google.cloud.texttospeech.v1beta1.VoiceSelectionParams"> & {
  /**
   * Required. The language (and potentially also the region) of the voice
   * expressed as a [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt)
   * language tag, e.g. "en-US". This should not include a script tag (e.g. use
   * "cmn-cn" rather than "cmn-Hant-cn"), because the script will be inferred
   * from the input provided in the SynthesisInput.  The TTS service
   * will use this parameter to help choose an appropriate voice.  Note that
   * the TTS service may choose a voice with a slightly different language code
   * than the one selected; it may substitute a different region
   * (e.g. using en-US rather than en-CA if there isn't a Canadian voice
   * available), or even a different language, e.g. using "nb" (Norwegian
   * Bokmal) instead of "no" (Norwegian)".
   *
   * @generated from field: string language_code = 1;
   */
  languageCode: string;

  /**
   * The name of the voice. If both the name and the gender are not set,
   * the service will choose a voice based on the other parameters such as
   * language_code.
   *
   * @generated from field: string name = 2;
   */
  name: string;

  /**
   * The preferred gender of the voice. If not set, the service will
   * choose a voice based on the other parameters such as language_code and
   * name. Note that this is only a preference, not requirement; if a
   * voice of the appropriate gender is not available, the synthesizer should
   * substitute a voice with a different gender rather than failing the request.
   *
   * @generated from field: google.cloud.texttospeech.v1beta1.SsmlVoiceGender ssml_gender = 3;
   */
  ssmlGender: SsmlVoiceGender;

  /**
   * The configuration for a custom voice. If [CustomVoiceParams.model] is set,
   * the service will choose the custom voice matching the specified
   * configuration.
   *
   * @generated from field: google.cloud.texttospeech.v1beta1.CustomVoiceParams custom_voice = 4;
   */
  customVoice?: CustomVoiceParams;
};

/**
 * Describes the message google.cloud.texttospeech.v1beta1.VoiceSelectionParams.
 * Use `create(VoiceSelectionParamsSchema)` to create a new message.
 */
export const VoiceSelectionParamsSchema: GenMessage<VoiceSelectionParams> = /*@__PURE__*/
  messageDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 5);

/**
 * Description of audio data to be synthesized.
 *
 * @generated from message google.cloud.texttospeech.v1beta1.AudioConfig
 */
export type AudioConfig = Message<"google.cloud.texttospeech.v1beta1.AudioConfig"> & {
  /**
   * Required. The format of the audio byte stream.
   *
   * @generated from field: google.cloud.texttospeech.v1beta1.AudioEncoding audio_encoding = 1;
   */
  audioEncoding: AudioEncoding;

  /**
   * Optional. Input only. Speaking rate/speed, in the range [0.25, 4.0]. 1.0 is
   * the normal native speed supported by the specific voice. 2.0 is twice as
   * fast, and 0.5 is half as fast. If unset(0.0), defaults to the native 1.0
   * speed. Any other values < 0.25 or > 4.0 will return an error.
   *
   * @generated from field: double speaking_rate = 2;
   */
  speakingRate: number;

  /**
   * Optional. Input only. Speaking pitch, in the range [-20.0, 20.0]. 20 means
   * increase 20 semitones from the original pitch. -20 means decrease 20
   * semitones from the original pitch.
   *
   * @generated from field: double pitch = 3;
   */
  pitch: number;

  /**
   * Optional. Input only. Volume gain (in dB) of the normal native volume
   * supported by the specific voice, in the range [-96.0, 16.0]. If unset, or
   * set to a value of 0.0 (dB), will play at normal native signal amplitude. A
   * value of -6.0 (dB) will play at approximately half the amplitude of the
   * normal native signal amplitude. A value of +6.0 (dB) will play at
   * approximately twice the amplitude of the normal native signal amplitude.
   * Strongly recommend not to exceed +10 (dB) as there's usually no effective
   * increase in loudness for any value greater than that.
   *
   * @generated from field: double volume_gain_db = 4;
   */
  volumeGainDb: number;

  /**
   * Optional. The synthesis sample rate (in hertz) for this audio. When this is
   * specified in SynthesizeSpeechRequest, if this is different from the voice's
   * natural sample rate, then the synthesizer will honor this request by
   * converting to the desired sample rate (which might result in worse audio
   * quality), unless the specified sample rate is not supported for the
   * encoding chosen, in which case it will fail the request and return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT].
   *
   * @generated from field: int32 sample_rate_hertz = 5;
   */
  sampleRateHertz: number;

  /**
   * Optional. Input only. An identifier which selects 'audio effects' profiles
   * that are applied on (post synthesized) text to speech. Effects are applied
   * on top of each other in the order they are given. See
   * [audio
   * profiles](https://cloud.google.com/text-to-speech/docs/audio-profiles) for
   * current supported profile ids.
   *
   * @generated from field: repeated string effects_profile_id = 6;
   */
  effectsProfileId: string[];
};

/**
 * Describes the message google.cloud.texttospeech.v1beta1.AudioConfig.
 * Use `create(AudioConfigSchema)` to create a new message.
 */
export const AudioConfigSchema: GenMessage<AudioConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 6);

/**
 * Description of the custom voice to be synthesized.
 *
 * @generated from message google.cloud.texttospeech.v1beta1.CustomVoiceParams
 */
export type CustomVoiceParams = Message<"google.cloud.texttospeech.v1beta1.CustomVoiceParams"> & {
  /**
   * Required. The name of the AutoML model that synthesizes the custom voice.
   *
   * @generated from field: string model = 1;
   */
  model: string;

  /**
   * Optional. Deprecated. The usage of the synthesized audio to be reported.
   *
   * @generated from field: google.cloud.texttospeech.v1beta1.CustomVoiceParams.ReportedUsage reported_usage = 3 [deprecated = true];
   * @deprecated
   */
  reportedUsage: CustomVoiceParams_ReportedUsage;
};

/**
 * Describes the message google.cloud.texttospeech.v1beta1.CustomVoiceParams.
 * Use `create(CustomVoiceParamsSchema)` to create a new message.
 */
export const CustomVoiceParamsSchema: GenMessage<CustomVoiceParams> = /*@__PURE__*/
  messageDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 7);

/**
 * Deprecated. The usage of the synthesized audio. Usage does not affect
 * billing.
 *
 * @generated from enum google.cloud.texttospeech.v1beta1.CustomVoiceParams.ReportedUsage
 */
export enum CustomVoiceParams_ReportedUsage {
  /**
   * Request with reported usage unspecified will be rejected.
   *
   * @generated from enum value: REPORTED_USAGE_UNSPECIFIED = 0;
   */
  REPORTED_USAGE_UNSPECIFIED = 0,

  /**
   * For scenarios where the synthesized audio is not downloadable and can
   * only be used once. For example, real-time request in IVR system.
   *
   * @generated from enum value: REALTIME = 1;
   */
  REALTIME = 1,

  /**
   * For scenarios where the synthesized audio is downloadable and can be
   * reused. For example, the synthesized audio is downloaded, stored in
   * customer service system and played repeatedly.
   *
   * @generated from enum value: OFFLINE = 2;
   */
  OFFLINE = 2,
}

/**
 * Describes the enum google.cloud.texttospeech.v1beta1.CustomVoiceParams.ReportedUsage.
 */
export const CustomVoiceParams_ReportedUsageSchema: GenEnum<CustomVoiceParams_ReportedUsage> = /*@__PURE__*/
  enumDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 7, 0);

/**
 * The message returned to the client by the `SynthesizeSpeech` method.
 *
 * @generated from message google.cloud.texttospeech.v1beta1.SynthesizeSpeechResponse
 */
export type SynthesizeSpeechResponse = Message<"google.cloud.texttospeech.v1beta1.SynthesizeSpeechResponse"> & {
  /**
   * The audio data bytes encoded as specified in the request, including the
   * header for encodings that are wrapped in containers (e.g. MP3, OGG_OPUS).
   * For LINEAR16 audio, we include the WAV header. Note: as
   * with all bytes fields, protobuffers use a pure binary representation,
   * whereas JSON representations use base64.
   *
   * @generated from field: bytes audio_content = 1;
   */
  audioContent: Uint8Array;

  /**
   * A link between a position in the original request input and a corresponding
   * time in the output audio. It's only supported via `<mark>` of SSML input.
   *
   * @generated from field: repeated google.cloud.texttospeech.v1beta1.Timepoint timepoints = 2;
   */
  timepoints: Timepoint[];

  /**
   * The audio metadata of `audio_content`.
   *
   * @generated from field: google.cloud.texttospeech.v1beta1.AudioConfig audio_config = 4;
   */
  audioConfig?: AudioConfig;
};

/**
 * Describes the message google.cloud.texttospeech.v1beta1.SynthesizeSpeechResponse.
 * Use `create(SynthesizeSpeechResponseSchema)` to create a new message.
 */
export const SynthesizeSpeechResponseSchema: GenMessage<SynthesizeSpeechResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 8);

/**
 * This contains a mapping between a certain point in the input text and a
 * corresponding time in the output audio.
 *
 * @generated from message google.cloud.texttospeech.v1beta1.Timepoint
 */
export type Timepoint = Message<"google.cloud.texttospeech.v1beta1.Timepoint"> & {
  /**
   * Timepoint name as received from the client within `<mark>` tag.
   *
   * @generated from field: string mark_name = 4;
   */
  markName: string;

  /**
   * Time offset in seconds from the start of the synthesized audio.
   *
   * @generated from field: double time_seconds = 3;
   */
  timeSeconds: number;
};

/**
 * Describes the message google.cloud.texttospeech.v1beta1.Timepoint.
 * Use `create(TimepointSchema)` to create a new message.
 */
export const TimepointSchema: GenMessage<Timepoint> = /*@__PURE__*/
  messageDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 9);

/**
 * Provides configuration information for the StreamingSynthesize request.
 *
 * @generated from message google.cloud.texttospeech.v1beta1.StreamingSynthesizeConfig
 */
export type StreamingSynthesizeConfig = Message<"google.cloud.texttospeech.v1beta1.StreamingSynthesizeConfig"> & {
  /**
   * Required. The desired voice of the synthesized audio.
   *
   * @generated from field: google.cloud.texttospeech.v1beta1.VoiceSelectionParams voice = 1;
   */
  voice?: VoiceSelectionParams;
};

/**
 * Describes the message google.cloud.texttospeech.v1beta1.StreamingSynthesizeConfig.
 * Use `create(StreamingSynthesizeConfigSchema)` to create a new message.
 */
export const StreamingSynthesizeConfigSchema: GenMessage<StreamingSynthesizeConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 10);

/**
 * Input to be synthesized.
 *
 * @generated from message google.cloud.texttospeech.v1beta1.StreamingSynthesisInput
 */
export type StreamingSynthesisInput = Message<"google.cloud.texttospeech.v1beta1.StreamingSynthesisInput"> & {
  /**
   * @generated from oneof google.cloud.texttospeech.v1beta1.StreamingSynthesisInput.input_source
   */
  inputSource: {
    /**
     * The raw text to be synthesized. It is recommended that each input
     * contains complete, terminating sentences, as this will likely result in
     * better prosody in the output audio. That being said, users are free to
     * input text however they please.
     *
     * @generated from field: string text = 1;
     */
    value: string;
    case: "text";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.texttospeech.v1beta1.StreamingSynthesisInput.
 * Use `create(StreamingSynthesisInputSchema)` to create a new message.
 */
export const StreamingSynthesisInputSchema: GenMessage<StreamingSynthesisInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 11);

/**
 * Request message for the `StreamingSynthesize` method. Multiple
 * `StreamingSynthesizeRequest` messages are sent in one call.
 * The first message must contain a `streaming_config` that
 * fully specifies the request configuration and must not contain `input`. All
 * subsequent messages must only have `input` set.
 *
 * @generated from message google.cloud.texttospeech.v1beta1.StreamingSynthesizeRequest
 */
export type StreamingSynthesizeRequest = Message<"google.cloud.texttospeech.v1beta1.StreamingSynthesizeRequest"> & {
  /**
   * The request to be sent, either a StreamingSynthesizeConfig or
   * StreamingSynthesisInput.
   *
   * @generated from oneof google.cloud.texttospeech.v1beta1.StreamingSynthesizeRequest.streaming_request
   */
  streamingRequest: {
    /**
     * StreamingSynthesizeConfig to be used in this streaming attempt. Only
     * specified in the first message sent in a `StreamingSynthesize` call.
     *
     * @generated from field: google.cloud.texttospeech.v1beta1.StreamingSynthesizeConfig streaming_config = 1;
     */
    value: StreamingSynthesizeConfig;
    case: "streamingConfig";
  } | {
    /**
     * Input to synthesize. Specified in all messages but the first in a
     * `StreamingSynthesize` call.
     *
     * @generated from field: google.cloud.texttospeech.v1beta1.StreamingSynthesisInput input = 2;
     */
    value: StreamingSynthesisInput;
    case: "input";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.texttospeech.v1beta1.StreamingSynthesizeRequest.
 * Use `create(StreamingSynthesizeRequestSchema)` to create a new message.
 */
export const StreamingSynthesizeRequestSchema: GenMessage<StreamingSynthesizeRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 12);

/**
 * `StreamingSynthesizeResponse` is the only message returned to the
 * client by `StreamingSynthesize` method. A series of zero or more
 * `StreamingSynthesizeResponse` messages are streamed back to the client.
 *
 * @generated from message google.cloud.texttospeech.v1beta1.StreamingSynthesizeResponse
 */
export type StreamingSynthesizeResponse = Message<"google.cloud.texttospeech.v1beta1.StreamingSynthesizeResponse"> & {
  /**
   * The audio data bytes encoded as specified in the request. This is
   * headerless LINEAR16 audio with a sample rate of 24000.
   *
   * @generated from field: bytes audio_content = 1;
   */
  audioContent: Uint8Array;
};

/**
 * Describes the message google.cloud.texttospeech.v1beta1.StreamingSynthesizeResponse.
 * Use `create(StreamingSynthesizeResponseSchema)` to create a new message.
 */
export const StreamingSynthesizeResponseSchema: GenMessage<StreamingSynthesizeResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 13);

/**
 * Gender of the voice as described in
 * [SSML voice element](https://www.w3.org/TR/speech-synthesis11/#edef_voice).
 *
 * @generated from enum google.cloud.texttospeech.v1beta1.SsmlVoiceGender
 */
export enum SsmlVoiceGender {
  /**
   * An unspecified gender.
   * In VoiceSelectionParams, this means that the client doesn't care which
   * gender the selected voice will have. In the Voice field of
   * ListVoicesResponse, this may mean that the voice doesn't fit any of the
   * other categories in this enum, or that the gender of the voice isn't known.
   *
   * @generated from enum value: SSML_VOICE_GENDER_UNSPECIFIED = 0;
   */
  SSML_VOICE_GENDER_UNSPECIFIED = 0,

  /**
   * A male voice.
   *
   * @generated from enum value: MALE = 1;
   */
  MALE = 1,

  /**
   * A female voice.
   *
   * @generated from enum value: FEMALE = 2;
   */
  FEMALE = 2,

  /**
   * A gender-neutral voice. This voice is not yet supported.
   *
   * @generated from enum value: NEUTRAL = 3;
   */
  NEUTRAL = 3,
}

/**
 * Describes the enum google.cloud.texttospeech.v1beta1.SsmlVoiceGender.
 */
export const SsmlVoiceGenderSchema: GenEnum<SsmlVoiceGender> = /*@__PURE__*/
  enumDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 0);

/**
 * Configuration to set up audio encoder. The encoding determines the output
 * audio format that we'd like.
 *
 * @generated from enum google.cloud.texttospeech.v1beta1.AudioEncoding
 */
export enum AudioEncoding {
  /**
   * Not specified. Will return result
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT].
   *
   * @generated from enum value: AUDIO_ENCODING_UNSPECIFIED = 0;
   */
  AUDIO_ENCODING_UNSPECIFIED = 0,

  /**
   * Uncompressed 16-bit signed little-endian samples (Linear PCM).
   * Audio content returned as LINEAR16 also contains a WAV header.
   *
   * @generated from enum value: LINEAR16 = 1;
   */
  LINEAR16 = 1,

  /**
   * MP3 audio at 32kbps.
   *
   * @generated from enum value: MP3 = 2;
   */
  MP3 = 2,

  /**
   * MP3 at 64kbps.
   *
   * @generated from enum value: MP3_64_KBPS = 4;
   */
  MP3_64_KBPS = 4,

  /**
   * Opus encoded audio wrapped in an ogg container. The result will be a
   * file which can be played natively on Android, and in browsers (at least
   * Chrome and Firefox). The quality of the encoding is considerably higher
   * than MP3 while using approximately the same bitrate.
   *
   * @generated from enum value: OGG_OPUS = 3;
   */
  OGG_OPUS = 3,

  /**
   * 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
   * Audio content returned as MULAW also contains a WAV header.
   *
   * @generated from enum value: MULAW = 5;
   */
  MULAW = 5,

  /**
   * 8-bit samples that compand 14-bit audio samples using G.711 PCMU/A-law.
   * Audio content returned as ALAW also contains a WAV header.
   *
   * @generated from enum value: ALAW = 6;
   */
  ALAW = 6,
}

/**
 * Describes the enum google.cloud.texttospeech.v1beta1.AudioEncoding.
 */
export const AudioEncodingSchema: GenEnum<AudioEncoding> = /*@__PURE__*/
  enumDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 1);

/**
 * Service that implements Google Cloud Text-to-Speech API.
 *
 * @generated from service google.cloud.texttospeech.v1beta1.TextToSpeech
 */
export const TextToSpeech: GenService<{
  /**
   * Returns a list of Voice supported for synthesis.
   *
   * @generated from rpc google.cloud.texttospeech.v1beta1.TextToSpeech.ListVoices
   */
  listVoices: {
    methodKind: "unary";
    input: typeof ListVoicesRequestSchema;
    output: typeof ListVoicesResponseSchema;
  },
  /**
   * Synthesizes speech synchronously: receive results after all text input
   * has been processed.
   *
   * @generated from rpc google.cloud.texttospeech.v1beta1.TextToSpeech.SynthesizeSpeech
   */
  synthesizeSpeech: {
    methodKind: "unary";
    input: typeof SynthesizeSpeechRequestSchema;
    output: typeof SynthesizeSpeechResponseSchema;
  },
  /**
   * Performs bidirectional streaming speech synthesis: receive audio while
   * sending text.
   *
   * @generated from rpc google.cloud.texttospeech.v1beta1.TextToSpeech.StreamingSynthesize
   */
  streamingSynthesize: {
    methodKind: "bidi_streaming";
    input: typeof StreamingSynthesizeRequestSchema;
    output: typeof StreamingSynthesizeResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_texttospeech_v1beta1_cloud_tts, 0);

