// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/visionai/v1/annotations.proto (package google.cloud.visionai.v1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc } from "@bufbuild/protobuf/codegenv1";
import type { ListValue, Timestamp } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_struct, file_google_protobuf_timestamp } from "@bufbuild/protobuf/wkt";
import type { JsonObject, Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/visionai/v1/annotations.proto.
 */
export const file_google_cloud_visionai_v1_annotations: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_protobuf_struct, file_google_protobuf_timestamp]);

/**
 * Output format for Personal Protective Equipment Detection Operator.
 *
 * @generated from message google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput
 */
export type PersonalProtectiveEquipmentDetectionOutput = Message<"google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput"> & {
  /**
   * Current timestamp.
   *
   * @generated from field: google.protobuf.Timestamp current_time = 1;
   */
  currentTime?: Timestamp;

  /**
   * A list of DetectedPersons.
   *
   * @generated from field: repeated google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.DetectedPerson detected_persons = 2;
   */
  detectedPersons: PersonalProtectiveEquipmentDetectionOutput_DetectedPerson[];
};

/**
 * Describes the message google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.
 * Use `create(PersonalProtectiveEquipmentDetectionOutputSchema)` to create a new message.
 */
export const PersonalProtectiveEquipmentDetectionOutputSchema: GenMessage<PersonalProtectiveEquipmentDetectionOutput> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 0);

/**
 * The entity info for annotations from person detection prediction result.
 *
 * @generated from message google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.PersonEntity
 */
export type PersonalProtectiveEquipmentDetectionOutput_PersonEntity = Message<"google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.PersonEntity"> & {
  /**
   * Entity id.
   *
   * @generated from field: int64 person_entity_id = 1;
   */
  personEntityId: bigint;
};

/**
 * Describes the message google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.PersonEntity.
 * Use `create(PersonalProtectiveEquipmentDetectionOutput_PersonEntitySchema)` to create a new message.
 */
export const PersonalProtectiveEquipmentDetectionOutput_PersonEntitySchema: GenMessage<PersonalProtectiveEquipmentDetectionOutput_PersonEntity> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 0, 0);

/**
 * The entity info for annotations from PPE detection prediction result.
 *
 * @generated from message google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.PPEEntity
 */
export type PersonalProtectiveEquipmentDetectionOutput_PPEEntity = Message<"google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.PPEEntity"> & {
  /**
   * Label id.
   *
   * @generated from field: int64 ppe_label_id = 1;
   */
  ppeLabelId: bigint;

  /**
   * Human readable string of the label (Examples: helmet, glove, mask).
   *
   * @generated from field: string ppe_label_string = 2;
   */
  ppeLabelString: string;

  /**
   * Human readable string of the super category label (Examples: head_cover,
   * hands_cover, face_cover).
   *
   * @generated from field: string ppe_supercategory_label_string = 3;
   */
  ppeSupercategoryLabelString: string;

  /**
   * Entity id.
   *
   * @generated from field: int64 ppe_entity_id = 4;
   */
  ppeEntityId: bigint;
};

/**
 * Describes the message google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.PPEEntity.
 * Use `create(PersonalProtectiveEquipmentDetectionOutput_PPEEntitySchema)` to create a new message.
 */
export const PersonalProtectiveEquipmentDetectionOutput_PPEEntitySchema: GenMessage<PersonalProtectiveEquipmentDetectionOutput_PPEEntity> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 0, 1);

/**
 * Bounding Box in the normalized coordinates.
 *
 * @generated from message google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.NormalizedBoundingBox
 */
export type PersonalProtectiveEquipmentDetectionOutput_NormalizedBoundingBox = Message<"google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.NormalizedBoundingBox"> & {
  /**
   * Min in x coordinate.
   *
   * @generated from field: float xmin = 1;
   */
  xmin: number;

  /**
   * Min in y coordinate.
   *
   * @generated from field: float ymin = 2;
   */
  ymin: number;

  /**
   * Width of the bounding box.
   *
   * @generated from field: float width = 3;
   */
  width: number;

  /**
   * Height of the bounding box.
   *
   * @generated from field: float height = 4;
   */
  height: number;
};

/**
 * Describes the message google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.NormalizedBoundingBox.
 * Use `create(PersonalProtectiveEquipmentDetectionOutput_NormalizedBoundingBoxSchema)` to create a new message.
 */
export const PersonalProtectiveEquipmentDetectionOutput_NormalizedBoundingBoxSchema: GenMessage<PersonalProtectiveEquipmentDetectionOutput_NormalizedBoundingBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 0, 2);

/**
 * PersonIdentified box contains the location and the entity info of the
 * person.
 *
 * @generated from message google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.PersonIdentifiedBox
 */
export type PersonalProtectiveEquipmentDetectionOutput_PersonIdentifiedBox = Message<"google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.PersonIdentifiedBox"> & {
  /**
   * An unique id for this box.
   *
   * @generated from field: int64 box_id = 1;
   */
  boxId: bigint;

  /**
   * Bounding Box in the normalized coordinates.
   *
   * @generated from field: google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.NormalizedBoundingBox normalized_bounding_box = 2;
   */
  normalizedBoundingBox?: PersonalProtectiveEquipmentDetectionOutput_NormalizedBoundingBox;

  /**
   * Confidence score associated with this box.
   *
   * @generated from field: float confidence_score = 3;
   */
  confidenceScore: number;

  /**
   * Person entity info.
   *
   * @generated from field: google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.PersonEntity person_entity = 4;
   */
  personEntity?: PersonalProtectiveEquipmentDetectionOutput_PersonEntity;
};

/**
 * Describes the message google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.PersonIdentifiedBox.
 * Use `create(PersonalProtectiveEquipmentDetectionOutput_PersonIdentifiedBoxSchema)` to create a new message.
 */
export const PersonalProtectiveEquipmentDetectionOutput_PersonIdentifiedBoxSchema: GenMessage<PersonalProtectiveEquipmentDetectionOutput_PersonIdentifiedBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 0, 3);

/**
 * PPEIdentified box contains the location and the entity info of the PPE.
 *
 * @generated from message google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.PPEIdentifiedBox
 */
export type PersonalProtectiveEquipmentDetectionOutput_PPEIdentifiedBox = Message<"google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.PPEIdentifiedBox"> & {
  /**
   * An unique id for this box.
   *
   * @generated from field: int64 box_id = 1;
   */
  boxId: bigint;

  /**
   * Bounding Box in the normalized coordinates.
   *
   * @generated from field: google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.NormalizedBoundingBox normalized_bounding_box = 2;
   */
  normalizedBoundingBox?: PersonalProtectiveEquipmentDetectionOutput_NormalizedBoundingBox;

  /**
   * Confidence score associated with this box.
   *
   * @generated from field: float confidence_score = 3;
   */
  confidenceScore: number;

  /**
   * PPE entity info.
   *
   * @generated from field: google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.PPEEntity ppe_entity = 4;
   */
  ppeEntity?: PersonalProtectiveEquipmentDetectionOutput_PPEEntity;
};

/**
 * Describes the message google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.PPEIdentifiedBox.
 * Use `create(PersonalProtectiveEquipmentDetectionOutput_PPEIdentifiedBoxSchema)` to create a new message.
 */
export const PersonalProtectiveEquipmentDetectionOutput_PPEIdentifiedBoxSchema: GenMessage<PersonalProtectiveEquipmentDetectionOutput_PPEIdentifiedBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 0, 4);

/**
 * Detected Person contains the detected person and their associated
 * ppes and their protecting information.
 *
 * @generated from message google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.DetectedPerson
 */
export type PersonalProtectiveEquipmentDetectionOutput_DetectedPerson = Message<"google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.DetectedPerson"> & {
  /**
   * The id of detected person.
   *
   * @generated from field: int64 person_id = 1;
   */
  personId: bigint;

  /**
   * The info of detected person identified box.
   *
   * @generated from field: google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.PersonIdentifiedBox detected_person_identified_box = 2;
   */
  detectedPersonIdentifiedBox?: PersonalProtectiveEquipmentDetectionOutput_PersonIdentifiedBox;

  /**
   * The info of detected person associated ppe identified boxes.
   *
   * @generated from field: repeated google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.PPEIdentifiedBox detected_ppe_identified_boxes = 3;
   */
  detectedPpeIdentifiedBoxes: PersonalProtectiveEquipmentDetectionOutput_PPEIdentifiedBox[];

  /**
   * Coverage score for each body part.
   * Coverage score for face.
   *
   * @generated from field: optional float face_coverage_score = 4;
   */
  faceCoverageScore?: number;

  /**
   * Coverage score for eyes.
   *
   * @generated from field: optional float eyes_coverage_score = 5;
   */
  eyesCoverageScore?: number;

  /**
   * Coverage score for head.
   *
   * @generated from field: optional float head_coverage_score = 6;
   */
  headCoverageScore?: number;

  /**
   * Coverage score for hands.
   *
   * @generated from field: optional float hands_coverage_score = 7;
   */
  handsCoverageScore?: number;

  /**
   * Coverage score for body.
   *
   * @generated from field: optional float body_coverage_score = 8;
   */
  bodyCoverageScore?: number;

  /**
   * Coverage score for feet.
   *
   * @generated from field: optional float feet_coverage_score = 9;
   */
  feetCoverageScore?: number;
};

/**
 * Describes the message google.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionOutput.DetectedPerson.
 * Use `create(PersonalProtectiveEquipmentDetectionOutput_DetectedPersonSchema)` to create a new message.
 */
export const PersonalProtectiveEquipmentDetectionOutput_DetectedPersonSchema: GenMessage<PersonalProtectiveEquipmentDetectionOutput_DetectedPerson> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 0, 5);

/**
 * Prediction output format for Generic Object Detection.
 *
 * @generated from message google.cloud.visionai.v1.ObjectDetectionPredictionResult
 */
export type ObjectDetectionPredictionResult = Message<"google.cloud.visionai.v1.ObjectDetectionPredictionResult"> & {
  /**
   * Current timestamp.
   *
   * @generated from field: google.protobuf.Timestamp current_time = 1;
   */
  currentTime?: Timestamp;

  /**
   * A list of identified boxes.
   *
   * @generated from field: repeated google.cloud.visionai.v1.ObjectDetectionPredictionResult.IdentifiedBox identified_boxes = 2;
   */
  identifiedBoxes: ObjectDetectionPredictionResult_IdentifiedBox[];
};

/**
 * Describes the message google.cloud.visionai.v1.ObjectDetectionPredictionResult.
 * Use `create(ObjectDetectionPredictionResultSchema)` to create a new message.
 */
export const ObjectDetectionPredictionResultSchema: GenMessage<ObjectDetectionPredictionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 1);

/**
 * The entity info for annotations from object detection prediction result.
 *
 * @generated from message google.cloud.visionai.v1.ObjectDetectionPredictionResult.Entity
 */
export type ObjectDetectionPredictionResult_Entity = Message<"google.cloud.visionai.v1.ObjectDetectionPredictionResult.Entity"> & {
  /**
   * Label id.
   *
   * @generated from field: int64 label_id = 1;
   */
  labelId: bigint;

  /**
   * Human readable string of the label.
   *
   * @generated from field: string label_string = 2;
   */
  labelString: string;
};

/**
 * Describes the message google.cloud.visionai.v1.ObjectDetectionPredictionResult.Entity.
 * Use `create(ObjectDetectionPredictionResult_EntitySchema)` to create a new message.
 */
export const ObjectDetectionPredictionResult_EntitySchema: GenMessage<ObjectDetectionPredictionResult_Entity> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 1, 0);

/**
 * Identified box contains location and the entity of the object.
 *
 * @generated from message google.cloud.visionai.v1.ObjectDetectionPredictionResult.IdentifiedBox
 */
export type ObjectDetectionPredictionResult_IdentifiedBox = Message<"google.cloud.visionai.v1.ObjectDetectionPredictionResult.IdentifiedBox"> & {
  /**
   * An unique id for this box.
   *
   * @generated from field: int64 box_id = 1;
   */
  boxId: bigint;

  /**
   * Bounding Box in the normalized coordinates.
   *
   * @generated from field: google.cloud.visionai.v1.ObjectDetectionPredictionResult.IdentifiedBox.NormalizedBoundingBox normalized_bounding_box = 2;
   */
  normalizedBoundingBox?: ObjectDetectionPredictionResult_IdentifiedBox_NormalizedBoundingBox;

  /**
   * Confidence score associated with this box.
   *
   * @generated from field: float confidence_score = 3;
   */
  confidenceScore: number;

  /**
   * Entity of this box.
   *
   * @generated from field: google.cloud.visionai.v1.ObjectDetectionPredictionResult.Entity entity = 4;
   */
  entity?: ObjectDetectionPredictionResult_Entity;
};

/**
 * Describes the message google.cloud.visionai.v1.ObjectDetectionPredictionResult.IdentifiedBox.
 * Use `create(ObjectDetectionPredictionResult_IdentifiedBoxSchema)` to create a new message.
 */
export const ObjectDetectionPredictionResult_IdentifiedBoxSchema: GenMessage<ObjectDetectionPredictionResult_IdentifiedBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 1, 1);

/**
 * Bounding Box in the normalized coordinates.
 *
 * @generated from message google.cloud.visionai.v1.ObjectDetectionPredictionResult.IdentifiedBox.NormalizedBoundingBox
 */
export type ObjectDetectionPredictionResult_IdentifiedBox_NormalizedBoundingBox = Message<"google.cloud.visionai.v1.ObjectDetectionPredictionResult.IdentifiedBox.NormalizedBoundingBox"> & {
  /**
   * Min in x coordinate.
   *
   * @generated from field: float xmin = 1;
   */
  xmin: number;

  /**
   * Min in y coordinate.
   *
   * @generated from field: float ymin = 2;
   */
  ymin: number;

  /**
   * Width of the bounding box.
   *
   * @generated from field: float width = 3;
   */
  width: number;

  /**
   * Height of the bounding box.
   *
   * @generated from field: float height = 4;
   */
  height: number;
};

/**
 * Describes the message google.cloud.visionai.v1.ObjectDetectionPredictionResult.IdentifiedBox.NormalizedBoundingBox.
 * Use `create(ObjectDetectionPredictionResult_IdentifiedBox_NormalizedBoundingBoxSchema)` to create a new message.
 */
export const ObjectDetectionPredictionResult_IdentifiedBox_NormalizedBoundingBoxSchema: GenMessage<ObjectDetectionPredictionResult_IdentifiedBox_NormalizedBoundingBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 1, 1, 0);

/**
 * Prediction output format for Image Object Detection.
 *
 * @generated from message google.cloud.visionai.v1.ImageObjectDetectionPredictionResult
 */
export type ImageObjectDetectionPredictionResult = Message<"google.cloud.visionai.v1.ImageObjectDetectionPredictionResult"> & {
  /**
   * The resource IDs of the AnnotationSpecs that had been identified, ordered
   * by the confidence score descendingly. It is the id segment instead of full
   * resource name.
   *
   * @generated from field: repeated int64 ids = 1;
   */
  ids: bigint[];

  /**
   * The display names of the AnnotationSpecs that had been identified, order
   * matches the IDs.
   *
   * @generated from field: repeated string display_names = 2;
   */
  displayNames: string[];

  /**
   * The Model's confidences in correctness of the predicted IDs, higher value
   * means higher confidence. Order matches the Ids.
   *
   * @generated from field: repeated float confidences = 3;
   */
  confidences: number[];

  /**
   * Bounding boxes, i.e. the rectangles over the image, that pinpoint
   * the found AnnotationSpecs. Given in order that matches the IDs. Each
   * bounding box is an array of 4 numbers `xMin`, `xMax`, `yMin`, and
   * `yMax`, which represent the extremal coordinates of the box. They are
   * relative to the image size, and the point 0,0 is in the top left
   * of the image.
   *
   * @generated from field: repeated google.protobuf.ListValue bboxes = 4;
   */
  bboxes: ListValue[];
};

/**
 * Describes the message google.cloud.visionai.v1.ImageObjectDetectionPredictionResult.
 * Use `create(ImageObjectDetectionPredictionResultSchema)` to create a new message.
 */
export const ImageObjectDetectionPredictionResultSchema: GenMessage<ImageObjectDetectionPredictionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 2);

/**
 * Prediction output format for Image and Text Classification.
 *
 * @generated from message google.cloud.visionai.v1.ClassificationPredictionResult
 */
export type ClassificationPredictionResult = Message<"google.cloud.visionai.v1.ClassificationPredictionResult"> & {
  /**
   * The resource IDs of the AnnotationSpecs that had been identified.
   *
   * @generated from field: repeated int64 ids = 1;
   */
  ids: bigint[];

  /**
   * The display names of the AnnotationSpecs that had been identified, order
   * matches the IDs.
   *
   * @generated from field: repeated string display_names = 2;
   */
  displayNames: string[];

  /**
   * The Model's confidences in correctness of the predicted IDs, higher value
   * means higher confidence. Order matches the Ids.
   *
   * @generated from field: repeated float confidences = 3;
   */
  confidences: number[];
};

/**
 * Describes the message google.cloud.visionai.v1.ClassificationPredictionResult.
 * Use `create(ClassificationPredictionResultSchema)` to create a new message.
 */
export const ClassificationPredictionResultSchema: GenMessage<ClassificationPredictionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 3);

/**
 * Prediction output format for Image Segmentation.
 *
 * @generated from message google.cloud.visionai.v1.ImageSegmentationPredictionResult
 */
export type ImageSegmentationPredictionResult = Message<"google.cloud.visionai.v1.ImageSegmentationPredictionResult"> & {
  /**
   * A PNG image where each pixel in the mask represents the category in which
   * the pixel in the original image was predicted to belong to. The size of
   * this image will be the same as the original image. The mapping between the
   * AnntoationSpec and the color can be found in model's metadata. The model
   * will choose the most likely category and if none of the categories reach
   * the confidence threshold, the pixel will be marked as background.
   *
   * @generated from field: string category_mask = 1;
   */
  categoryMask: string;

  /**
   * A one channel image which is encoded as an 8bit lossless PNG. The size of
   * the image will be the same as the original image. For a specific pixel,
   * darker color means less confidence in correctness of the cateogry in the
   * categoryMask for the corresponding pixel. Black means no confidence and
   * white means complete confidence.
   *
   * @generated from field: string confidence_mask = 2;
   */
  confidenceMask: string;
};

/**
 * Describes the message google.cloud.visionai.v1.ImageSegmentationPredictionResult.
 * Use `create(ImageSegmentationPredictionResultSchema)` to create a new message.
 */
export const ImageSegmentationPredictionResultSchema: GenMessage<ImageSegmentationPredictionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 4);

/**
 * Prediction output format for Video Action Recognition.
 *
 * @generated from message google.cloud.visionai.v1.VideoActionRecognitionPredictionResult
 */
export type VideoActionRecognitionPredictionResult = Message<"google.cloud.visionai.v1.VideoActionRecognitionPredictionResult"> & {
  /**
   * The beginning, inclusive, of the video's time segment in which the
   * actions have been identified.
   *
   * @generated from field: google.protobuf.Timestamp segment_start_time = 1;
   */
  segmentStartTime?: Timestamp;

  /**
   * The end, inclusive, of the video's time segment in which the actions have
   * been identified. Particularly, if the end is the same as the start, it
   * means the identification happens on a specific video frame.
   *
   * @generated from field: google.protobuf.Timestamp segment_end_time = 2;
   */
  segmentEndTime?: Timestamp;

  /**
   * All of the actions identified in the time range.
   *
   * @generated from field: repeated google.cloud.visionai.v1.VideoActionRecognitionPredictionResult.IdentifiedAction actions = 3;
   */
  actions: VideoActionRecognitionPredictionResult_IdentifiedAction[];
};

/**
 * Describes the message google.cloud.visionai.v1.VideoActionRecognitionPredictionResult.
 * Use `create(VideoActionRecognitionPredictionResultSchema)` to create a new message.
 */
export const VideoActionRecognitionPredictionResultSchema: GenMessage<VideoActionRecognitionPredictionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 5);

/**
 * Each IdentifiedAction is one particular identification of an action
 * specified with the AnnotationSpec id, display_name and the associated
 * confidence score.
 *
 * @generated from message google.cloud.visionai.v1.VideoActionRecognitionPredictionResult.IdentifiedAction
 */
export type VideoActionRecognitionPredictionResult_IdentifiedAction = Message<"google.cloud.visionai.v1.VideoActionRecognitionPredictionResult.IdentifiedAction"> & {
  /**
   * The resource ID of the AnnotationSpec that had been identified.
   *
   * @generated from field: string id = 1;
   */
  id: string;

  /**
   * The display name of the AnnotationSpec that had been identified.
   *
   * @generated from field: string display_name = 2;
   */
  displayName: string;

  /**
   * The Model's confidence in correction of this identification, higher
   * value means higher confidence.
   *
   * @generated from field: float confidence = 3;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.visionai.v1.VideoActionRecognitionPredictionResult.IdentifiedAction.
 * Use `create(VideoActionRecognitionPredictionResult_IdentifiedActionSchema)` to create a new message.
 */
export const VideoActionRecognitionPredictionResult_IdentifiedActionSchema: GenMessage<VideoActionRecognitionPredictionResult_IdentifiedAction> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 5, 0);

/**
 * Prediction output format for Video Object Tracking.
 *
 * @generated from message google.cloud.visionai.v1.VideoObjectTrackingPredictionResult
 */
export type VideoObjectTrackingPredictionResult = Message<"google.cloud.visionai.v1.VideoObjectTrackingPredictionResult"> & {
  /**
   * The beginning, inclusive, of the video's time segment in which the
   * current identifications happens.
   *
   * @generated from field: google.protobuf.Timestamp segment_start_time = 1;
   */
  segmentStartTime?: Timestamp;

  /**
   * The end, inclusive, of the video's time segment in which the current
   * identifications happen. Particularly, if the end is the same as the start,
   * it means the identifications happen on a specific video frame.
   *
   * @generated from field: google.protobuf.Timestamp segment_end_time = 2;
   */
  segmentEndTime?: Timestamp;

  /**
   * All of the objects detected in the specified time range.
   *
   * @generated from field: repeated google.cloud.visionai.v1.VideoObjectTrackingPredictionResult.DetectedObject objects = 3;
   */
  objects: VideoObjectTrackingPredictionResult_DetectedObject[];
};

/**
 * Describes the message google.cloud.visionai.v1.VideoObjectTrackingPredictionResult.
 * Use `create(VideoObjectTrackingPredictionResultSchema)` to create a new message.
 */
export const VideoObjectTrackingPredictionResultSchema: GenMessage<VideoObjectTrackingPredictionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 6);

/**
 * Boundingbox for detected object. I.e. the rectangle over the video frame
 * pinpointing the found AnnotationSpec. The coordinates are relative to the
 * frame size, and the point 0,0 is in the top left of the frame.
 *
 * @generated from message google.cloud.visionai.v1.VideoObjectTrackingPredictionResult.BoundingBox
 */
export type VideoObjectTrackingPredictionResult_BoundingBox = Message<"google.cloud.visionai.v1.VideoObjectTrackingPredictionResult.BoundingBox"> & {
  /**
   * The leftmost coordinate of the bounding box.
   *
   * @generated from field: float x_min = 1;
   */
  xMin: number;

  /**
   * The rightmost coordinate of the bounding box.
   *
   * @generated from field: float x_max = 2;
   */
  xMax: number;

  /**
   * The topmost coordinate of the bounding box.
   *
   * @generated from field: float y_min = 3;
   */
  yMin: number;

  /**
   * The bottommost coordinate of the bounding box.
   *
   * @generated from field: float y_max = 4;
   */
  yMax: number;
};

/**
 * Describes the message google.cloud.visionai.v1.VideoObjectTrackingPredictionResult.BoundingBox.
 * Use `create(VideoObjectTrackingPredictionResult_BoundingBoxSchema)` to create a new message.
 */
export const VideoObjectTrackingPredictionResult_BoundingBoxSchema: GenMessage<VideoObjectTrackingPredictionResult_BoundingBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 6, 0);

/**
 * Each DetectedObject is one particular identification of an object
 * specified with the AnnotationSpec id and display_name, the bounding box,
 * the associated confidence score and the corresponding track_id.
 *
 * @generated from message google.cloud.visionai.v1.VideoObjectTrackingPredictionResult.DetectedObject
 */
export type VideoObjectTrackingPredictionResult_DetectedObject = Message<"google.cloud.visionai.v1.VideoObjectTrackingPredictionResult.DetectedObject"> & {
  /**
   * The resource ID of the AnnotationSpec that had been identified.
   *
   * @generated from field: string id = 1;
   */
  id: string;

  /**
   * The display name of the AnnotationSpec that had been identified.
   *
   * @generated from field: string display_name = 2;
   */
  displayName: string;

  /**
   * Boundingbox.
   *
   * @generated from field: google.cloud.visionai.v1.VideoObjectTrackingPredictionResult.BoundingBox bounding_box = 3;
   */
  boundingBox?: VideoObjectTrackingPredictionResult_BoundingBox;

  /**
   * The Model's confidence in correction of this identification, higher
   * value means higher confidence.
   *
   * @generated from field: float confidence = 4;
   */
  confidence: number;

  /**
   * The same object may be identified on muitiple frames which are typical
   * adjacent. The set of frames where a particular object has been detected
   * form a track. This track_id can be used to trace down all frames for an
   * detected object.
   *
   * @generated from field: int64 track_id = 5;
   */
  trackId: bigint;
};

/**
 * Describes the message google.cloud.visionai.v1.VideoObjectTrackingPredictionResult.DetectedObject.
 * Use `create(VideoObjectTrackingPredictionResult_DetectedObjectSchema)` to create a new message.
 */
export const VideoObjectTrackingPredictionResult_DetectedObjectSchema: GenMessage<VideoObjectTrackingPredictionResult_DetectedObject> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 6, 1);

/**
 * Prediction output format for Video Classification.
 *
 * @generated from message google.cloud.visionai.v1.VideoClassificationPredictionResult
 */
export type VideoClassificationPredictionResult = Message<"google.cloud.visionai.v1.VideoClassificationPredictionResult"> & {
  /**
   * The beginning, inclusive, of the video's time segment in which the
   * classifications have been identified.
   *
   * @generated from field: google.protobuf.Timestamp segment_start_time = 1;
   */
  segmentStartTime?: Timestamp;

  /**
   * The end, inclusive, of the video's time segment in which the
   * classifications have been identified. Particularly, if the end is the same
   * as the start, it means the identification happens on a specific video
   * frame.
   *
   * @generated from field: google.protobuf.Timestamp segment_end_time = 2;
   */
  segmentEndTime?: Timestamp;

  /**
   * All of the classifications identified in the time range.
   *
   * @generated from field: repeated google.cloud.visionai.v1.VideoClassificationPredictionResult.IdentifiedClassification classifications = 3;
   */
  classifications: VideoClassificationPredictionResult_IdentifiedClassification[];
};

/**
 * Describes the message google.cloud.visionai.v1.VideoClassificationPredictionResult.
 * Use `create(VideoClassificationPredictionResultSchema)` to create a new message.
 */
export const VideoClassificationPredictionResultSchema: GenMessage<VideoClassificationPredictionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 7);

/**
 * Each IdentifiedClassification is one particular identification of an
 * classification specified with the AnnotationSpec id and display_name,
 * and the associated confidence score.
 *
 * @generated from message google.cloud.visionai.v1.VideoClassificationPredictionResult.IdentifiedClassification
 */
export type VideoClassificationPredictionResult_IdentifiedClassification = Message<"google.cloud.visionai.v1.VideoClassificationPredictionResult.IdentifiedClassification"> & {
  /**
   * The resource ID of the AnnotationSpec that had been identified.
   *
   * @generated from field: string id = 1;
   */
  id: string;

  /**
   * The display name of the AnnotationSpec that had been identified.
   *
   * @generated from field: string display_name = 2;
   */
  displayName: string;

  /**
   * The Model's confidence in correction of this identification, higher
   * value means higher confidence.
   *
   * @generated from field: float confidence = 3;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.visionai.v1.VideoClassificationPredictionResult.IdentifiedClassification.
 * Use `create(VideoClassificationPredictionResult_IdentifiedClassificationSchema)` to create a new message.
 */
export const VideoClassificationPredictionResult_IdentifiedClassificationSchema: GenMessage<VideoClassificationPredictionResult_IdentifiedClassification> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 7, 0);

/**
 * The prediction result proto for occupancy counting.
 *
 * @generated from message google.cloud.visionai.v1.OccupancyCountingPredictionResult
 */
export type OccupancyCountingPredictionResult = Message<"google.cloud.visionai.v1.OccupancyCountingPredictionResult"> & {
  /**
   * Current timestamp.
   *
   * @generated from field: google.protobuf.Timestamp current_time = 1;
   */
  currentTime?: Timestamp;

  /**
   * A list of identified boxes.
   *
   * @generated from field: repeated google.cloud.visionai.v1.OccupancyCountingPredictionResult.IdentifiedBox identified_boxes = 2;
   */
  identifiedBoxes: OccupancyCountingPredictionResult_IdentifiedBox[];

  /**
   * Detection statistics.
   *
   * @generated from field: google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats stats = 3;
   */
  stats?: OccupancyCountingPredictionResult_Stats;

  /**
   * Track related information. All the tracks that are live at this timestamp.
   * It only exists if tracking is enabled.
   *
   * @generated from field: repeated google.cloud.visionai.v1.OccupancyCountingPredictionResult.TrackInfo track_info = 4;
   */
  trackInfo: OccupancyCountingPredictionResult_TrackInfo[];

  /**
   * Dwell time related information. All the tracks that are live in a given
   * zone with a start and end dwell time timestamp
   *
   * @generated from field: repeated google.cloud.visionai.v1.OccupancyCountingPredictionResult.DwellTimeInfo dwell_time_info = 5;
   */
  dwellTimeInfo: OccupancyCountingPredictionResult_DwellTimeInfo[];

  /**
   * The presentation timestamp of the frame.
   *
   * @generated from field: optional int64 pts = 6;
   */
  pts?: bigint;
};

/**
 * Describes the message google.cloud.visionai.v1.OccupancyCountingPredictionResult.
 * Use `create(OccupancyCountingPredictionResultSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResultSchema: GenMessage<OccupancyCountingPredictionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 8);

/**
 * The entity info for annotations from occupancy counting operator.
 *
 * @generated from message google.cloud.visionai.v1.OccupancyCountingPredictionResult.Entity
 */
export type OccupancyCountingPredictionResult_Entity = Message<"google.cloud.visionai.v1.OccupancyCountingPredictionResult.Entity"> & {
  /**
   * Label id.
   *
   * @generated from field: int64 label_id = 1;
   */
  labelId: bigint;

  /**
   * Human readable string of the label.
   *
   * @generated from field: string label_string = 2;
   */
  labelString: string;
};

/**
 * Describes the message google.cloud.visionai.v1.OccupancyCountingPredictionResult.Entity.
 * Use `create(OccupancyCountingPredictionResult_EntitySchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_EntitySchema: GenMessage<OccupancyCountingPredictionResult_Entity> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 8, 0);

/**
 * Identified box contains location and the entity of the object.
 *
 * @generated from message google.cloud.visionai.v1.OccupancyCountingPredictionResult.IdentifiedBox
 */
export type OccupancyCountingPredictionResult_IdentifiedBox = Message<"google.cloud.visionai.v1.OccupancyCountingPredictionResult.IdentifiedBox"> & {
  /**
   * An unique id for this box.
   *
   * @generated from field: int64 box_id = 1;
   */
  boxId: bigint;

  /**
   * Bounding Box in the normalized coordinates.
   *
   * @generated from field: google.cloud.visionai.v1.OccupancyCountingPredictionResult.IdentifiedBox.NormalizedBoundingBox normalized_bounding_box = 2;
   */
  normalizedBoundingBox?: OccupancyCountingPredictionResult_IdentifiedBox_NormalizedBoundingBox;

  /**
   * Confidence score associated with this box.
   *
   * @generated from field: float score = 3;
   */
  score: number;

  /**
   * Entity of this box.
   *
   * @generated from field: google.cloud.visionai.v1.OccupancyCountingPredictionResult.Entity entity = 4;
   */
  entity?: OccupancyCountingPredictionResult_Entity;

  /**
   * An unique id to identify a track. It should be consistent across frames.
   * It only exists if tracking is enabled.
   *
   * @generated from field: int64 track_id = 5;
   */
  trackId: bigint;
};

/**
 * Describes the message google.cloud.visionai.v1.OccupancyCountingPredictionResult.IdentifiedBox.
 * Use `create(OccupancyCountingPredictionResult_IdentifiedBoxSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_IdentifiedBoxSchema: GenMessage<OccupancyCountingPredictionResult_IdentifiedBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 8, 1);

/**
 * Bounding Box in the normalized coordinates.
 *
 * @generated from message google.cloud.visionai.v1.OccupancyCountingPredictionResult.IdentifiedBox.NormalizedBoundingBox
 */
export type OccupancyCountingPredictionResult_IdentifiedBox_NormalizedBoundingBox = Message<"google.cloud.visionai.v1.OccupancyCountingPredictionResult.IdentifiedBox.NormalizedBoundingBox"> & {
  /**
   * Min in x coordinate.
   *
   * @generated from field: float xmin = 1;
   */
  xmin: number;

  /**
   * Min in y coordinate.
   *
   * @generated from field: float ymin = 2;
   */
  ymin: number;

  /**
   * Width of the bounding box.
   *
   * @generated from field: float width = 3;
   */
  width: number;

  /**
   * Height of the bounding box.
   *
   * @generated from field: float height = 4;
   */
  height: number;
};

/**
 * Describes the message google.cloud.visionai.v1.OccupancyCountingPredictionResult.IdentifiedBox.NormalizedBoundingBox.
 * Use `create(OccupancyCountingPredictionResult_IdentifiedBox_NormalizedBoundingBoxSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_IdentifiedBox_NormalizedBoundingBoxSchema: GenMessage<OccupancyCountingPredictionResult_IdentifiedBox_NormalizedBoundingBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 8, 1, 0);

/**
 * The statistics info for annotations from occupancy counting operator.
 *
 * @generated from message google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats
 */
export type OccupancyCountingPredictionResult_Stats = Message<"google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats"> & {
  /**
   * Counts of the full frame.
   *
   * @generated from field: repeated google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.ObjectCount full_frame_count = 1;
   */
  fullFrameCount: OccupancyCountingPredictionResult_Stats_ObjectCount[];

  /**
   * Crossing line counts.
   *
   * @generated from field: repeated google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.CrossingLineCount crossing_line_counts = 2;
   */
  crossingLineCounts: OccupancyCountingPredictionResult_Stats_CrossingLineCount[];

  /**
   * Active zone counts.
   *
   * @generated from field: repeated google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.ActiveZoneCount active_zone_counts = 3;
   */
  activeZoneCounts: OccupancyCountingPredictionResult_Stats_ActiveZoneCount[];
};

/**
 * Describes the message google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.
 * Use `create(OccupancyCountingPredictionResult_StatsSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_StatsSchema: GenMessage<OccupancyCountingPredictionResult_Stats> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 8, 2);

/**
 * The object info and instant count for annotations from occupancy counting
 * operator.
 *
 * @generated from message google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.ObjectCount
 */
export type OccupancyCountingPredictionResult_Stats_ObjectCount = Message<"google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.ObjectCount"> & {
  /**
   * Entity of this object.
   *
   * @generated from field: google.cloud.visionai.v1.OccupancyCountingPredictionResult.Entity entity = 1;
   */
  entity?: OccupancyCountingPredictionResult_Entity;

  /**
   * Count of the object.
   *
   * @generated from field: int32 count = 2;
   */
  count: number;
};

/**
 * Describes the message google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.ObjectCount.
 * Use `create(OccupancyCountingPredictionResult_Stats_ObjectCountSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_Stats_ObjectCountSchema: GenMessage<OccupancyCountingPredictionResult_Stats_ObjectCount> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 8, 2, 0);

/**
 * The object info and accumulated count for annotations from occupancy
 * counting operator.
 *
 * @generated from message google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.AccumulatedObjectCount
 */
export type OccupancyCountingPredictionResult_Stats_AccumulatedObjectCount = Message<"google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.AccumulatedObjectCount"> & {
  /**
   * The start time of the accumulated count.
   *
   * @generated from field: google.protobuf.Timestamp start_time = 1;
   */
  startTime?: Timestamp;

  /**
   * The object count for the accumulated count.
   *
   * @generated from field: google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.ObjectCount object_count = 2;
   */
  objectCount?: OccupancyCountingPredictionResult_Stats_ObjectCount;
};

/**
 * Describes the message google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.AccumulatedObjectCount.
 * Use `create(OccupancyCountingPredictionResult_Stats_AccumulatedObjectCountSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_Stats_AccumulatedObjectCountSchema: GenMessage<OccupancyCountingPredictionResult_Stats_AccumulatedObjectCount> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 8, 2, 1);

/**
 * Message for Crossing line count.
 *
 * @generated from message google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.CrossingLineCount
 */
export type OccupancyCountingPredictionResult_Stats_CrossingLineCount = Message<"google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.CrossingLineCount"> & {
  /**
   * Line annotation from the user.
   *
   * @generated from field: google.cloud.visionai.v1.StreamAnnotation annotation = 1;
   */
  annotation?: StreamAnnotation;

  /**
   * The direction that follows the right hand rule.
   *
   * @generated from field: repeated google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.ObjectCount positive_direction_counts = 2;
   */
  positiveDirectionCounts: OccupancyCountingPredictionResult_Stats_ObjectCount[];

  /**
   * The direction that is opposite to the right hand rule.
   *
   * @generated from field: repeated google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.ObjectCount negative_direction_counts = 3;
   */
  negativeDirectionCounts: OccupancyCountingPredictionResult_Stats_ObjectCount[];

  /**
   * The accumulated positive count.
   *
   * @generated from field: repeated google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.AccumulatedObjectCount accumulated_positive_direction_counts = 4;
   */
  accumulatedPositiveDirectionCounts: OccupancyCountingPredictionResult_Stats_AccumulatedObjectCount[];

  /**
   * The accumulated negative count.
   *
   * @generated from field: repeated google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.AccumulatedObjectCount accumulated_negative_direction_counts = 5;
   */
  accumulatedNegativeDirectionCounts: OccupancyCountingPredictionResult_Stats_AccumulatedObjectCount[];
};

/**
 * Describes the message google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.CrossingLineCount.
 * Use `create(OccupancyCountingPredictionResult_Stats_CrossingLineCountSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_Stats_CrossingLineCountSchema: GenMessage<OccupancyCountingPredictionResult_Stats_CrossingLineCount> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 8, 2, 2);

/**
 * Message for the active zone count.
 *
 * @generated from message google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.ActiveZoneCount
 */
export type OccupancyCountingPredictionResult_Stats_ActiveZoneCount = Message<"google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.ActiveZoneCount"> & {
  /**
   * Active zone annotation from the user.
   *
   * @generated from field: google.cloud.visionai.v1.StreamAnnotation annotation = 1;
   */
  annotation?: StreamAnnotation;

  /**
   * Counts in the zone.
   *
   * @generated from field: repeated google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.ObjectCount counts = 2;
   */
  counts: OccupancyCountingPredictionResult_Stats_ObjectCount[];
};

/**
 * Describes the message google.cloud.visionai.v1.OccupancyCountingPredictionResult.Stats.ActiveZoneCount.
 * Use `create(OccupancyCountingPredictionResult_Stats_ActiveZoneCountSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_Stats_ActiveZoneCountSchema: GenMessage<OccupancyCountingPredictionResult_Stats_ActiveZoneCount> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 8, 2, 3);

/**
 * The track info for annotations from occupancy counting operator.
 *
 * @generated from message google.cloud.visionai.v1.OccupancyCountingPredictionResult.TrackInfo
 */
export type OccupancyCountingPredictionResult_TrackInfo = Message<"google.cloud.visionai.v1.OccupancyCountingPredictionResult.TrackInfo"> & {
  /**
   * An unique id to identify a track. It should be consistent across frames.
   *
   * @generated from field: string track_id = 1;
   */
  trackId: string;

  /**
   * Start timestamp of this track.
   *
   * @generated from field: google.protobuf.Timestamp start_time = 2;
   */
  startTime?: Timestamp;
};

/**
 * Describes the message google.cloud.visionai.v1.OccupancyCountingPredictionResult.TrackInfo.
 * Use `create(OccupancyCountingPredictionResult_TrackInfoSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_TrackInfoSchema: GenMessage<OccupancyCountingPredictionResult_TrackInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 8, 3);

/**
 * The dwell time info for annotations from occupancy counting operator.
 *
 * @generated from message google.cloud.visionai.v1.OccupancyCountingPredictionResult.DwellTimeInfo
 */
export type OccupancyCountingPredictionResult_DwellTimeInfo = Message<"google.cloud.visionai.v1.OccupancyCountingPredictionResult.DwellTimeInfo"> & {
  /**
   * An unique id to identify a track. It should be consistent across frames.
   *
   * @generated from field: string track_id = 1;
   */
  trackId: string;

  /**
   * The unique id for the zone in which the object is dwelling/waiting.
   *
   * @generated from field: string zone_id = 2;
   */
  zoneId: string;

  /**
   * The beginning time when a dwelling object has been identified in a zone.
   *
   * @generated from field: google.protobuf.Timestamp dwell_start_time = 3;
   */
  dwellStartTime?: Timestamp;

  /**
   * The end time when a dwelling object has exited in a zone.
   *
   * @generated from field: google.protobuf.Timestamp dwell_end_time = 4;
   */
  dwellEndTime?: Timestamp;
};

/**
 * Describes the message google.cloud.visionai.v1.OccupancyCountingPredictionResult.DwellTimeInfo.
 * Use `create(OccupancyCountingPredictionResult_DwellTimeInfoSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_DwellTimeInfoSchema: GenMessage<OccupancyCountingPredictionResult_DwellTimeInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 8, 4);

/**
 * message about annotations about Vision AI stream resource.
 *
 * @generated from message google.cloud.visionai.v1.StreamAnnotation
 */
export type StreamAnnotation = Message<"google.cloud.visionai.v1.StreamAnnotation"> & {
  /**
   * @generated from oneof google.cloud.visionai.v1.StreamAnnotation.annotation_payload
   */
  annotationPayload: {
    /**
     * Annotation for type ACTIVE_ZONE
     *
     * @generated from field: google.cloud.visionai.v1.NormalizedPolygon active_zone = 5;
     */
    value: NormalizedPolygon;
    case: "activeZone";
  } | {
    /**
     * Annotation for type CROSSING_LINE
     *
     * @generated from field: google.cloud.visionai.v1.NormalizedPolyline crossing_line = 6;
     */
    value: NormalizedPolyline;
    case: "crossingLine";
  } | { case: undefined; value?: undefined };

  /**
   * ID of the annotation. It must be unique when used in the certain context.
   * For example, all the annotations to one input streams of a Vision AI
   * application.
   *
   * @generated from field: string id = 1;
   */
  id: string;

  /**
   * User-friendly name for the annotation.
   *
   * @generated from field: string display_name = 2;
   */
  displayName: string;

  /**
   * The Vision AI stream resource name.
   *
   * @generated from field: string source_stream = 3;
   */
  sourceStream: string;

  /**
   * The actual type of Annotation.
   *
   * @generated from field: google.cloud.visionai.v1.StreamAnnotationType type = 4;
   */
  type: StreamAnnotationType;
};

/**
 * Describes the message google.cloud.visionai.v1.StreamAnnotation.
 * Use `create(StreamAnnotationSchema)` to create a new message.
 */
export const StreamAnnotationSchema: GenMessage<StreamAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 9);

/**
 * A wrapper of repeated StreamAnnotation.
 *
 * @generated from message google.cloud.visionai.v1.StreamAnnotations
 */
export type StreamAnnotations = Message<"google.cloud.visionai.v1.StreamAnnotations"> & {
  /**
   * Multiple annotations.
   *
   * @generated from field: repeated google.cloud.visionai.v1.StreamAnnotation stream_annotations = 1;
   */
  streamAnnotations: StreamAnnotation[];
};

/**
 * Describes the message google.cloud.visionai.v1.StreamAnnotations.
 * Use `create(StreamAnnotationsSchema)` to create a new message.
 */
export const StreamAnnotationsSchema: GenMessage<StreamAnnotations> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 10);

/**
 * Normalized Polygon.
 *
 * @generated from message google.cloud.visionai.v1.NormalizedPolygon
 */
export type NormalizedPolygon = Message<"google.cloud.visionai.v1.NormalizedPolygon"> & {
  /**
   * The bounding polygon normalized vertices. Top left corner of the image
   * will be [0, 0].
   *
   * @generated from field: repeated google.cloud.visionai.v1.NormalizedVertex normalized_vertices = 1;
   */
  normalizedVertices: NormalizedVertex[];
};

/**
 * Describes the message google.cloud.visionai.v1.NormalizedPolygon.
 * Use `create(NormalizedPolygonSchema)` to create a new message.
 */
export const NormalizedPolygonSchema: GenMessage<NormalizedPolygon> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 11);

/**
 * Normalized Pplyline, which represents a curve consisting of connected
 * straight-line segments.
 *
 * @generated from message google.cloud.visionai.v1.NormalizedPolyline
 */
export type NormalizedPolyline = Message<"google.cloud.visionai.v1.NormalizedPolyline"> & {
  /**
   * A sequence of vertices connected by straight lines.
   *
   * @generated from field: repeated google.cloud.visionai.v1.NormalizedVertex normalized_vertices = 1;
   */
  normalizedVertices: NormalizedVertex[];
};

/**
 * Describes the message google.cloud.visionai.v1.NormalizedPolyline.
 * Use `create(NormalizedPolylineSchema)` to create a new message.
 */
export const NormalizedPolylineSchema: GenMessage<NormalizedPolyline> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 12);

/**
 * A vertex represents a 2D point in the image.
 * NOTE: the normalized vertex coordinates are relative to the original image
 * and range from 0 to 1.
 *
 * @generated from message google.cloud.visionai.v1.NormalizedVertex
 */
export type NormalizedVertex = Message<"google.cloud.visionai.v1.NormalizedVertex"> & {
  /**
   * X coordinate.
   *
   * @generated from field: float x = 1;
   */
  x: number;

  /**
   * Y coordinate.
   *
   * @generated from field: float y = 2;
   */
  y: number;
};

/**
 * Describes the message google.cloud.visionai.v1.NormalizedVertex.
 * Use `create(NormalizedVertexSchema)` to create a new message.
 */
export const NormalizedVertexSchema: GenMessage<NormalizedVertex> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 13);

/**
 * Message of essential metadata of App Platform.
 * This message is usually attached to a certain processor output annotation for
 * customer to identify the source of the data.
 *
 * @generated from message google.cloud.visionai.v1.AppPlatformMetadata
 */
export type AppPlatformMetadata = Message<"google.cloud.visionai.v1.AppPlatformMetadata"> & {
  /**
   * The application resource name.
   *
   * @generated from field: string application = 1;
   */
  application: string;

  /**
   * The instance resource id. Instance is the nested resource of application
   * under collection 'instances'.
   *
   * @generated from field: string instance_id = 2;
   */
  instanceId: string;

  /**
   * The node name of the application graph.
   *
   * @generated from field: string node = 3;
   */
  node: string;

  /**
   * The referred processor resource name of the application node.
   *
   * @generated from field: string processor = 4;
   */
  processor: string;
};

/**
 * Describes the message google.cloud.visionai.v1.AppPlatformMetadata.
 * Use `create(AppPlatformMetadataSchema)` to create a new message.
 */
export const AppPlatformMetadataSchema: GenMessage<AppPlatformMetadata> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 14);

/**
 * For any cloud function based customer processing logic, customer's cloud
 * function is expected to receive AppPlatformCloudFunctionRequest as request
 * and send back AppPlatformCloudFunctionResponse as response.
 * Message of request from AppPlatform to Cloud Function.
 *
 * @generated from message google.cloud.visionai.v1.AppPlatformCloudFunctionRequest
 */
export type AppPlatformCloudFunctionRequest = Message<"google.cloud.visionai.v1.AppPlatformCloudFunctionRequest"> & {
  /**
   * The metadata of the AppPlatform for customer to identify the source of the
   * payload.
   *
   * @generated from field: google.cloud.visionai.v1.AppPlatformMetadata app_platform_metadata = 1;
   */
  appPlatformMetadata?: AppPlatformMetadata;

  /**
   * The actual annotations to be processed by the customized Cloud Function.
   *
   * @generated from field: repeated google.cloud.visionai.v1.AppPlatformCloudFunctionRequest.StructedInputAnnotation annotations = 2;
   */
  annotations: AppPlatformCloudFunctionRequest_StructedInputAnnotation[];
};

/**
 * Describes the message google.cloud.visionai.v1.AppPlatformCloudFunctionRequest.
 * Use `create(AppPlatformCloudFunctionRequestSchema)` to create a new message.
 */
export const AppPlatformCloudFunctionRequestSchema: GenMessage<AppPlatformCloudFunctionRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 15);

/**
 * A general annotation message that uses struct format to represent different
 * concrete annotation protobufs.
 *
 * @generated from message google.cloud.visionai.v1.AppPlatformCloudFunctionRequest.StructedInputAnnotation
 */
export type AppPlatformCloudFunctionRequest_StructedInputAnnotation = Message<"google.cloud.visionai.v1.AppPlatformCloudFunctionRequest.StructedInputAnnotation"> & {
  /**
   * The ingestion time of the current annotation.
   *
   * @generated from field: int64 ingestion_time_micros = 1;
   */
  ingestionTimeMicros: bigint;

  /**
   * The struct format of the actual annotation.
   *
   * @generated from field: google.protobuf.Struct annotation = 2;
   */
  annotation?: JsonObject;
};

/**
 * Describes the message google.cloud.visionai.v1.AppPlatformCloudFunctionRequest.StructedInputAnnotation.
 * Use `create(AppPlatformCloudFunctionRequest_StructedInputAnnotationSchema)` to create a new message.
 */
export const AppPlatformCloudFunctionRequest_StructedInputAnnotationSchema: GenMessage<AppPlatformCloudFunctionRequest_StructedInputAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 15, 0);

/**
 * Message of the response from customer's Cloud Function to AppPlatform.
 *
 * @generated from message google.cloud.visionai.v1.AppPlatformCloudFunctionResponse
 */
export type AppPlatformCloudFunctionResponse = Message<"google.cloud.visionai.v1.AppPlatformCloudFunctionResponse"> & {
  /**
   * The modified annotations that is returned back to AppPlatform.
   * If the annotations fields are empty, then those annotations will be dropped
   * by AppPlatform.
   *
   * @generated from field: repeated google.cloud.visionai.v1.AppPlatformCloudFunctionResponse.StructedOutputAnnotation annotations = 2;
   */
  annotations: AppPlatformCloudFunctionResponse_StructedOutputAnnotation[];

  /**
   * If set to true, AppPlatform will use original annotations instead of
   * dropping them, even if it is empty in the annotations filed.
   *
   * @generated from field: bool annotation_passthrough = 3;
   */
  annotationPassthrough: boolean;

  /**
   * The event notifications that is returned back to AppPlatform. Typically it
   * will then be configured to be consumed/forwared to a operator that handles
   * events, such as Pub/Sub operator.
   *
   * @generated from field: repeated google.cloud.visionai.v1.AppPlatformEventBody events = 4;
   */
  events: AppPlatformEventBody[];
};

/**
 * Describes the message google.cloud.visionai.v1.AppPlatformCloudFunctionResponse.
 * Use `create(AppPlatformCloudFunctionResponseSchema)` to create a new message.
 */
export const AppPlatformCloudFunctionResponseSchema: GenMessage<AppPlatformCloudFunctionResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 16);

/**
 * A general annotation message that uses struct format to represent different
 * concrete annotation protobufs.
 *
 * @generated from message google.cloud.visionai.v1.AppPlatformCloudFunctionResponse.StructedOutputAnnotation
 */
export type AppPlatformCloudFunctionResponse_StructedOutputAnnotation = Message<"google.cloud.visionai.v1.AppPlatformCloudFunctionResponse.StructedOutputAnnotation"> & {
  /**
   * The struct format of the actual annotation.
   *
   * @generated from field: google.protobuf.Struct annotation = 1;
   */
  annotation?: JsonObject;
};

/**
 * Describes the message google.cloud.visionai.v1.AppPlatformCloudFunctionResponse.StructedOutputAnnotation.
 * Use `create(AppPlatformCloudFunctionResponse_StructedOutputAnnotationSchema)` to create a new message.
 */
export const AppPlatformCloudFunctionResponse_StructedOutputAnnotationSchema: GenMessage<AppPlatformCloudFunctionResponse_StructedOutputAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 16, 0);

/**
 * Message of content of appPlatform event
 *
 * @generated from message google.cloud.visionai.v1.AppPlatformEventBody
 */
export type AppPlatformEventBody = Message<"google.cloud.visionai.v1.AppPlatformEventBody"> & {
  /**
   * Human readable string of the event like "There are more than 6 people in
   * the scene". or "Shelf is empty!".
   *
   * @generated from field: string event_message = 1;
   */
  eventMessage: string;

  /**
   * For the case of Pub/Sub, it will be stored in the message attributes.
   * pubsub.proto
   *
   * @generated from field: google.protobuf.Struct payload = 2;
   */
  payload?: JsonObject;

  /**
   * User defined Event Id, used to classify event, within a delivery interval,
   * events from the same application instance with the same id will be
   * de-duplicated & only first one will be sent out. Empty event_id will be
   * treated as "".
   *
   * @generated from field: string event_id = 3;
   */
  eventId: string;
};

/**
 * Describes the message google.cloud.visionai.v1.AppPlatformEventBody.
 * Use `create(AppPlatformEventBodySchema)` to create a new message.
 */
export const AppPlatformEventBodySchema: GenMessage<AppPlatformEventBody> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1_annotations, 17);

/**
 * Enum describing all possible types of a stream annotation.
 *
 * @generated from enum google.cloud.visionai.v1.StreamAnnotationType
 */
export enum StreamAnnotationType {
  /**
   * Type UNSPECIFIED.
   *
   * @generated from enum value: STREAM_ANNOTATION_TYPE_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * active_zone annotation defines a polygon on top of the content from an
   * image/video based stream, following processing will only focus on the
   * content inside the active zone.
   *
   * @generated from enum value: STREAM_ANNOTATION_TYPE_ACTIVE_ZONE = 1;
   */
  ACTIVE_ZONE = 1,

  /**
   * crossing_line annotation defines a polyline on top of the content from an
   * image/video based Vision AI stream, events happening across the line will
   * be captured. For example, the counts of people who goes acroos the line
   * in Occupancy Analytic Processor.
   *
   * @generated from enum value: STREAM_ANNOTATION_TYPE_CROSSING_LINE = 2;
   */
  CROSSING_LINE = 2,
}

/**
 * Describes the enum google.cloud.visionai.v1.StreamAnnotationType.
 */
export const StreamAnnotationTypeSchema: GenEnum<StreamAnnotationType> = /*@__PURE__*/
  enumDesc(file_google_cloud_visionai_v1_annotations, 0);

