// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/visionai/v1alpha1/annotations.proto (package google.cloud.visionai.v1alpha1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc } from "@bufbuild/protobuf/codegenv1";
import type { ListValue, Timestamp } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_struct, file_google_protobuf_timestamp } from "@bufbuild/protobuf/wkt";
import type { JsonObject, Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/visionai/v1alpha1/annotations.proto.
 */
export const file_google_cloud_visionai_v1alpha1_annotations: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_protobuf_struct, file_google_protobuf_timestamp]);

/**
 * Output format for Personal Protective Equipment Detection Operator.
 *
 * @generated from message google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput
 */
export type PersonalProtectiveEquipmentDetectionOutput = Message<"google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput"> & {
  /**
   * Current timestamp.
   *
   * @generated from field: google.protobuf.Timestamp current_time = 1;
   */
  currentTime?: Timestamp;

  /**
   * A list of DetectedPersons.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.DetectedPerson detected_persons = 2;
   */
  detectedPersons: PersonalProtectiveEquipmentDetectionOutput_DetectedPerson[];
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.
 * Use `create(PersonalProtectiveEquipmentDetectionOutputSchema)` to create a new message.
 */
export const PersonalProtectiveEquipmentDetectionOutputSchema: GenMessage<PersonalProtectiveEquipmentDetectionOutput> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 0);

/**
 * The entity info for annotations from person detection prediction result.
 *
 * @generated from message google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.PersonEntity
 */
export type PersonalProtectiveEquipmentDetectionOutput_PersonEntity = Message<"google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.PersonEntity"> & {
  /**
   * Entity id.
   *
   * @generated from field: int64 person_entity_id = 1;
   */
  personEntityId: bigint;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.PersonEntity.
 * Use `create(PersonalProtectiveEquipmentDetectionOutput_PersonEntitySchema)` to create a new message.
 */
export const PersonalProtectiveEquipmentDetectionOutput_PersonEntitySchema: GenMessage<PersonalProtectiveEquipmentDetectionOutput_PersonEntity> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 0, 0);

/**
 * The entity info for annotations from PPE detection prediction result.
 *
 * @generated from message google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.PPEEntity
 */
export type PersonalProtectiveEquipmentDetectionOutput_PPEEntity = Message<"google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.PPEEntity"> & {
  /**
   * Label id.
   *
   * @generated from field: int64 ppe_label_id = 1;
   */
  ppeLabelId: bigint;

  /**
   * Human readable string of the label (Examples: helmet, glove, mask).
   *
   * @generated from field: string ppe_label_string = 2;
   */
  ppeLabelString: string;

  /**
   * Human readable string of the super category label (Examples: head_cover,
   * hands_cover, face_cover).
   *
   * @generated from field: string ppe_supercategory_label_string = 3;
   */
  ppeSupercategoryLabelString: string;

  /**
   * Entity id.
   *
   * @generated from field: int64 ppe_entity_id = 4;
   */
  ppeEntityId: bigint;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.PPEEntity.
 * Use `create(PersonalProtectiveEquipmentDetectionOutput_PPEEntitySchema)` to create a new message.
 */
export const PersonalProtectiveEquipmentDetectionOutput_PPEEntitySchema: GenMessage<PersonalProtectiveEquipmentDetectionOutput_PPEEntity> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 0, 1);

/**
 * Bounding Box in the normalized coordinates.
 *
 * @generated from message google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.NormalizedBoundingBox
 */
export type PersonalProtectiveEquipmentDetectionOutput_NormalizedBoundingBox = Message<"google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.NormalizedBoundingBox"> & {
  /**
   * Min in x coordinate.
   *
   * @generated from field: float xmin = 1;
   */
  xmin: number;

  /**
   * Min in y coordinate.
   *
   * @generated from field: float ymin = 2;
   */
  ymin: number;

  /**
   * Width of the bounding box.
   *
   * @generated from field: float width = 3;
   */
  width: number;

  /**
   * Height of the bounding box.
   *
   * @generated from field: float height = 4;
   */
  height: number;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.NormalizedBoundingBox.
 * Use `create(PersonalProtectiveEquipmentDetectionOutput_NormalizedBoundingBoxSchema)` to create a new message.
 */
export const PersonalProtectiveEquipmentDetectionOutput_NormalizedBoundingBoxSchema: GenMessage<PersonalProtectiveEquipmentDetectionOutput_NormalizedBoundingBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 0, 2);

/**
 * PersonIdentified box contains the location and the entity info of the
 * person.
 *
 * @generated from message google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.PersonIdentifiedBox
 */
export type PersonalProtectiveEquipmentDetectionOutput_PersonIdentifiedBox = Message<"google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.PersonIdentifiedBox"> & {
  /**
   * An unique id for this box.
   *
   * @generated from field: int64 box_id = 1;
   */
  boxId: bigint;

  /**
   * Bounding Box in the normalized coordinates.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.NormalizedBoundingBox normalized_bounding_box = 2;
   */
  normalizedBoundingBox?: PersonalProtectiveEquipmentDetectionOutput_NormalizedBoundingBox;

  /**
   * Confidence score associated with this box.
   *
   * @generated from field: float confidence_score = 3;
   */
  confidenceScore: number;

  /**
   * Person entity info.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.PersonEntity person_entity = 4;
   */
  personEntity?: PersonalProtectiveEquipmentDetectionOutput_PersonEntity;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.PersonIdentifiedBox.
 * Use `create(PersonalProtectiveEquipmentDetectionOutput_PersonIdentifiedBoxSchema)` to create a new message.
 */
export const PersonalProtectiveEquipmentDetectionOutput_PersonIdentifiedBoxSchema: GenMessage<PersonalProtectiveEquipmentDetectionOutput_PersonIdentifiedBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 0, 3);

/**
 * PPEIdentified box contains the location and the entity info of the PPE.
 *
 * @generated from message google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.PPEIdentifiedBox
 */
export type PersonalProtectiveEquipmentDetectionOutput_PPEIdentifiedBox = Message<"google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.PPEIdentifiedBox"> & {
  /**
   * An unique id for this box.
   *
   * @generated from field: int64 box_id = 1;
   */
  boxId: bigint;

  /**
   * Bounding Box in the normalized coordinates.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.NormalizedBoundingBox normalized_bounding_box = 2;
   */
  normalizedBoundingBox?: PersonalProtectiveEquipmentDetectionOutput_NormalizedBoundingBox;

  /**
   * Confidence score associated with this box.
   *
   * @generated from field: float confidence_score = 3;
   */
  confidenceScore: number;

  /**
   * PPE entity info.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.PPEEntity ppe_entity = 4;
   */
  ppeEntity?: PersonalProtectiveEquipmentDetectionOutput_PPEEntity;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.PPEIdentifiedBox.
 * Use `create(PersonalProtectiveEquipmentDetectionOutput_PPEIdentifiedBoxSchema)` to create a new message.
 */
export const PersonalProtectiveEquipmentDetectionOutput_PPEIdentifiedBoxSchema: GenMessage<PersonalProtectiveEquipmentDetectionOutput_PPEIdentifiedBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 0, 4);

/**
 * Detected Person contains the detected person and their associated
 * ppes and their protecting information.
 *
 * @generated from message google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.DetectedPerson
 */
export type PersonalProtectiveEquipmentDetectionOutput_DetectedPerson = Message<"google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.DetectedPerson"> & {
  /**
   * The id of detected person.
   *
   * @generated from field: int64 person_id = 1;
   */
  personId: bigint;

  /**
   * The info of detected person identified box.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.PersonIdentifiedBox detected_person_identified_box = 2;
   */
  detectedPersonIdentifiedBox?: PersonalProtectiveEquipmentDetectionOutput_PersonIdentifiedBox;

  /**
   * The info of detected person associated ppe identified boxes.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.PPEIdentifiedBox detected_ppe_identified_boxes = 3;
   */
  detectedPpeIdentifiedBoxes: PersonalProtectiveEquipmentDetectionOutput_PPEIdentifiedBox[];

  /**
   * Coverage score for each body part.
   * Coverage score for face.
   *
   * @generated from field: optional float face_coverage_score = 4;
   */
  faceCoverageScore?: number;

  /**
   * Coverage score for eyes.
   *
   * @generated from field: optional float eyes_coverage_score = 5;
   */
  eyesCoverageScore?: number;

  /**
   * Coverage score for head.
   *
   * @generated from field: optional float head_coverage_score = 6;
   */
  headCoverageScore?: number;

  /**
   * Coverage score for hands.
   *
   * @generated from field: optional float hands_coverage_score = 7;
   */
  handsCoverageScore?: number;

  /**
   * Coverage score for body.
   *
   * @generated from field: optional float body_coverage_score = 8;
   */
  bodyCoverageScore?: number;

  /**
   * Coverage score for feet.
   *
   * @generated from field: optional float feet_coverage_score = 9;
   */
  feetCoverageScore?: number;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.PersonalProtectiveEquipmentDetectionOutput.DetectedPerson.
 * Use `create(PersonalProtectiveEquipmentDetectionOutput_DetectedPersonSchema)` to create a new message.
 */
export const PersonalProtectiveEquipmentDetectionOutput_DetectedPersonSchema: GenMessage<PersonalProtectiveEquipmentDetectionOutput_DetectedPerson> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 0, 5);

/**
 * Prediction output format for Generic Object Detection.
 *
 * @generated from message google.cloud.visionai.v1alpha1.ObjectDetectionPredictionResult
 */
export type ObjectDetectionPredictionResult = Message<"google.cloud.visionai.v1alpha1.ObjectDetectionPredictionResult"> & {
  /**
   * Current timestamp.
   *
   * @generated from field: google.protobuf.Timestamp current_time = 1;
   */
  currentTime?: Timestamp;

  /**
   * A list of identified boxes.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.ObjectDetectionPredictionResult.IdentifiedBox identified_boxes = 2;
   */
  identifiedBoxes: ObjectDetectionPredictionResult_IdentifiedBox[];
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.ObjectDetectionPredictionResult.
 * Use `create(ObjectDetectionPredictionResultSchema)` to create a new message.
 */
export const ObjectDetectionPredictionResultSchema: GenMessage<ObjectDetectionPredictionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 1);

/**
 * The entity info for annotations from object detection prediction result.
 *
 * @generated from message google.cloud.visionai.v1alpha1.ObjectDetectionPredictionResult.Entity
 */
export type ObjectDetectionPredictionResult_Entity = Message<"google.cloud.visionai.v1alpha1.ObjectDetectionPredictionResult.Entity"> & {
  /**
   * Label id.
   *
   * @generated from field: int64 label_id = 1;
   */
  labelId: bigint;

  /**
   * Human readable string of the label.
   *
   * @generated from field: string label_string = 2;
   */
  labelString: string;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.ObjectDetectionPredictionResult.Entity.
 * Use `create(ObjectDetectionPredictionResult_EntitySchema)` to create a new message.
 */
export const ObjectDetectionPredictionResult_EntitySchema: GenMessage<ObjectDetectionPredictionResult_Entity> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 1, 0);

/**
 * Identified box contains location and the entity of the object.
 *
 * @generated from message google.cloud.visionai.v1alpha1.ObjectDetectionPredictionResult.IdentifiedBox
 */
export type ObjectDetectionPredictionResult_IdentifiedBox = Message<"google.cloud.visionai.v1alpha1.ObjectDetectionPredictionResult.IdentifiedBox"> & {
  /**
   * An unique id for this box.
   *
   * @generated from field: int64 box_id = 1;
   */
  boxId: bigint;

  /**
   * Bounding Box in the normalized coordinates.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.ObjectDetectionPredictionResult.IdentifiedBox.NormalizedBoundingBox normalized_bounding_box = 2;
   */
  normalizedBoundingBox?: ObjectDetectionPredictionResult_IdentifiedBox_NormalizedBoundingBox;

  /**
   * Confidence score associated with this box.
   *
   * @generated from field: float confidence_score = 3;
   */
  confidenceScore: number;

  /**
   * Entity of this box.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.ObjectDetectionPredictionResult.Entity entity = 4;
   */
  entity?: ObjectDetectionPredictionResult_Entity;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.ObjectDetectionPredictionResult.IdentifiedBox.
 * Use `create(ObjectDetectionPredictionResult_IdentifiedBoxSchema)` to create a new message.
 */
export const ObjectDetectionPredictionResult_IdentifiedBoxSchema: GenMessage<ObjectDetectionPredictionResult_IdentifiedBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 1, 1);

/**
 * Bounding Box in the normalized coordinates.
 *
 * @generated from message google.cloud.visionai.v1alpha1.ObjectDetectionPredictionResult.IdentifiedBox.NormalizedBoundingBox
 */
export type ObjectDetectionPredictionResult_IdentifiedBox_NormalizedBoundingBox = Message<"google.cloud.visionai.v1alpha1.ObjectDetectionPredictionResult.IdentifiedBox.NormalizedBoundingBox"> & {
  /**
   * Min in x coordinate.
   *
   * @generated from field: float xmin = 1;
   */
  xmin: number;

  /**
   * Min in y coordinate.
   *
   * @generated from field: float ymin = 2;
   */
  ymin: number;

  /**
   * Width of the bounding box.
   *
   * @generated from field: float width = 3;
   */
  width: number;

  /**
   * Height of the bounding box.
   *
   * @generated from field: float height = 4;
   */
  height: number;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.ObjectDetectionPredictionResult.IdentifiedBox.NormalizedBoundingBox.
 * Use `create(ObjectDetectionPredictionResult_IdentifiedBox_NormalizedBoundingBoxSchema)` to create a new message.
 */
export const ObjectDetectionPredictionResult_IdentifiedBox_NormalizedBoundingBoxSchema: GenMessage<ObjectDetectionPredictionResult_IdentifiedBox_NormalizedBoundingBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 1, 1, 0);

/**
 * Prediction output format for Image Object Detection.
 *
 * @generated from message google.cloud.visionai.v1alpha1.ImageObjectDetectionPredictionResult
 */
export type ImageObjectDetectionPredictionResult = Message<"google.cloud.visionai.v1alpha1.ImageObjectDetectionPredictionResult"> & {
  /**
   * The resource IDs of the AnnotationSpecs that had been identified, ordered
   * by the confidence score descendingly. It is the id segment instead of full
   * resource name.
   *
   * @generated from field: repeated int64 ids = 1;
   */
  ids: bigint[];

  /**
   * The display names of the AnnotationSpecs that had been identified, order
   * matches the IDs.
   *
   * @generated from field: repeated string display_names = 2;
   */
  displayNames: string[];

  /**
   * The Model's confidences in correctness of the predicted IDs, higher value
   * means higher confidence. Order matches the Ids.
   *
   * @generated from field: repeated float confidences = 3;
   */
  confidences: number[];

  /**
   * Bounding boxes, i.e. the rectangles over the image, that pinpoint
   * the found AnnotationSpecs. Given in order that matches the IDs. Each
   * bounding box is an array of 4 numbers `xMin`, `xMax`, `yMin`, and
   * `yMax`, which represent the extremal coordinates of the box. They are
   * relative to the image size, and the point 0,0 is in the top left
   * of the image.
   *
   * @generated from field: repeated google.protobuf.ListValue bboxes = 4;
   */
  bboxes: ListValue[];
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.ImageObjectDetectionPredictionResult.
 * Use `create(ImageObjectDetectionPredictionResultSchema)` to create a new message.
 */
export const ImageObjectDetectionPredictionResultSchema: GenMessage<ImageObjectDetectionPredictionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 2);

/**
 * Prediction output format for Image and Text Classification.
 *
 * @generated from message google.cloud.visionai.v1alpha1.ClassificationPredictionResult
 */
export type ClassificationPredictionResult = Message<"google.cloud.visionai.v1alpha1.ClassificationPredictionResult"> & {
  /**
   * The resource IDs of the AnnotationSpecs that had been identified.
   *
   * @generated from field: repeated int64 ids = 1;
   */
  ids: bigint[];

  /**
   * The display names of the AnnotationSpecs that had been identified, order
   * matches the IDs.
   *
   * @generated from field: repeated string display_names = 2;
   */
  displayNames: string[];

  /**
   * The Model's confidences in correctness of the predicted IDs, higher value
   * means higher confidence. Order matches the Ids.
   *
   * @generated from field: repeated float confidences = 3;
   */
  confidences: number[];
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.ClassificationPredictionResult.
 * Use `create(ClassificationPredictionResultSchema)` to create a new message.
 */
export const ClassificationPredictionResultSchema: GenMessage<ClassificationPredictionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 3);

/**
 * Prediction output format for Image Segmentation.
 *
 * @generated from message google.cloud.visionai.v1alpha1.ImageSegmentationPredictionResult
 */
export type ImageSegmentationPredictionResult = Message<"google.cloud.visionai.v1alpha1.ImageSegmentationPredictionResult"> & {
  /**
   * A PNG image where each pixel in the mask represents the category in which
   * the pixel in the original image was predicted to belong to. The size of
   * this image will be the same as the original image. The mapping between the
   * AnntoationSpec and the color can be found in model's metadata. The model
   * will choose the most likely category and if none of the categories reach
   * the confidence threshold, the pixel will be marked as background.
   *
   * @generated from field: string category_mask = 1;
   */
  categoryMask: string;

  /**
   * A one channel image which is encoded as an 8bit lossless PNG. The size of
   * the image will be the same as the original image. For a specific pixel,
   * darker color means less confidence in correctness of the cateogry in the
   * categoryMask for the corresponding pixel. Black means no confidence and
   * white means complete confidence.
   *
   * @generated from field: string confidence_mask = 2;
   */
  confidenceMask: string;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.ImageSegmentationPredictionResult.
 * Use `create(ImageSegmentationPredictionResultSchema)` to create a new message.
 */
export const ImageSegmentationPredictionResultSchema: GenMessage<ImageSegmentationPredictionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 4);

/**
 * Prediction output format for Video Action Recognition.
 *
 * @generated from message google.cloud.visionai.v1alpha1.VideoActionRecognitionPredictionResult
 */
export type VideoActionRecognitionPredictionResult = Message<"google.cloud.visionai.v1alpha1.VideoActionRecognitionPredictionResult"> & {
  /**
   * The beginning, inclusive, of the video's time segment in which the
   * actions have been identified.
   *
   * @generated from field: google.protobuf.Timestamp segment_start_time = 1;
   */
  segmentStartTime?: Timestamp;

  /**
   * The end, inclusive, of the video's time segment in which the actions have
   * been identified. Particularly, if the end is the same as the start, it
   * means the identification happens on a specific video frame.
   *
   * @generated from field: google.protobuf.Timestamp segment_end_time = 2;
   */
  segmentEndTime?: Timestamp;

  /**
   * All of the actions identified in the time range.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.VideoActionRecognitionPredictionResult.IdentifiedAction actions = 3;
   */
  actions: VideoActionRecognitionPredictionResult_IdentifiedAction[];
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.VideoActionRecognitionPredictionResult.
 * Use `create(VideoActionRecognitionPredictionResultSchema)` to create a new message.
 */
export const VideoActionRecognitionPredictionResultSchema: GenMessage<VideoActionRecognitionPredictionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 5);

/**
 * Each IdentifiedAction is one particular identification of an action
 * specified with the AnnotationSpec id, display_name and the associated
 * confidence score.
 *
 * @generated from message google.cloud.visionai.v1alpha1.VideoActionRecognitionPredictionResult.IdentifiedAction
 */
export type VideoActionRecognitionPredictionResult_IdentifiedAction = Message<"google.cloud.visionai.v1alpha1.VideoActionRecognitionPredictionResult.IdentifiedAction"> & {
  /**
   * The resource ID of the AnnotationSpec that had been identified.
   *
   * @generated from field: string id = 1;
   */
  id: string;

  /**
   * The display name of the AnnotationSpec that had been identified.
   *
   * @generated from field: string display_name = 2;
   */
  displayName: string;

  /**
   * The Model's confidence in correction of this identification, higher
   * value means higher confidence.
   *
   * @generated from field: float confidence = 3;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.VideoActionRecognitionPredictionResult.IdentifiedAction.
 * Use `create(VideoActionRecognitionPredictionResult_IdentifiedActionSchema)` to create a new message.
 */
export const VideoActionRecognitionPredictionResult_IdentifiedActionSchema: GenMessage<VideoActionRecognitionPredictionResult_IdentifiedAction> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 5, 0);

/**
 * Prediction output format for Video Object Tracking.
 *
 * @generated from message google.cloud.visionai.v1alpha1.VideoObjectTrackingPredictionResult
 */
export type VideoObjectTrackingPredictionResult = Message<"google.cloud.visionai.v1alpha1.VideoObjectTrackingPredictionResult"> & {
  /**
   * The beginning, inclusive, of the video's time segment in which the
   * current identifications happens.
   *
   * @generated from field: google.protobuf.Timestamp segment_start_time = 1;
   */
  segmentStartTime?: Timestamp;

  /**
   * The end, inclusive, of the video's time segment in which the current
   * identifications happen. Particularly, if the end is the same as the start,
   * it means the identifications happen on a specific video frame.
   *
   * @generated from field: google.protobuf.Timestamp segment_end_time = 2;
   */
  segmentEndTime?: Timestamp;

  /**
   * All of the objects detected in the specified time range.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.VideoObjectTrackingPredictionResult.DetectedObject objects = 3;
   */
  objects: VideoObjectTrackingPredictionResult_DetectedObject[];
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.VideoObjectTrackingPredictionResult.
 * Use `create(VideoObjectTrackingPredictionResultSchema)` to create a new message.
 */
export const VideoObjectTrackingPredictionResultSchema: GenMessage<VideoObjectTrackingPredictionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 6);

/**
 * Boundingbox for detected object. I.e. the rectangle over the video frame
 * pinpointing the found AnnotationSpec. The coordinates are relative to the
 * frame size, and the point 0,0 is in the top left of the frame.
 *
 * @generated from message google.cloud.visionai.v1alpha1.VideoObjectTrackingPredictionResult.BoundingBox
 */
export type VideoObjectTrackingPredictionResult_BoundingBox = Message<"google.cloud.visionai.v1alpha1.VideoObjectTrackingPredictionResult.BoundingBox"> & {
  /**
   * The leftmost coordinate of the bounding box.
   *
   * @generated from field: float x_min = 1;
   */
  xMin: number;

  /**
   * The rightmost coordinate of the bounding box.
   *
   * @generated from field: float x_max = 2;
   */
  xMax: number;

  /**
   * The topmost coordinate of the bounding box.
   *
   * @generated from field: float y_min = 3;
   */
  yMin: number;

  /**
   * The bottommost coordinate of the bounding box.
   *
   * @generated from field: float y_max = 4;
   */
  yMax: number;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.VideoObjectTrackingPredictionResult.BoundingBox.
 * Use `create(VideoObjectTrackingPredictionResult_BoundingBoxSchema)` to create a new message.
 */
export const VideoObjectTrackingPredictionResult_BoundingBoxSchema: GenMessage<VideoObjectTrackingPredictionResult_BoundingBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 6, 0);

/**
 * Each DetectedObject is one particular identification of an object
 * specified with the AnnotationSpec id and display_name, the bounding box,
 * the associated confidence score and the corresponding track_id.
 *
 * @generated from message google.cloud.visionai.v1alpha1.VideoObjectTrackingPredictionResult.DetectedObject
 */
export type VideoObjectTrackingPredictionResult_DetectedObject = Message<"google.cloud.visionai.v1alpha1.VideoObjectTrackingPredictionResult.DetectedObject"> & {
  /**
   * The resource ID of the AnnotationSpec that had been identified.
   *
   * @generated from field: string id = 1;
   */
  id: string;

  /**
   * The display name of the AnnotationSpec that had been identified.
   *
   * @generated from field: string display_name = 2;
   */
  displayName: string;

  /**
   * Boundingbox.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.VideoObjectTrackingPredictionResult.BoundingBox bounding_box = 3;
   */
  boundingBox?: VideoObjectTrackingPredictionResult_BoundingBox;

  /**
   * The Model's confidence in correction of this identification, higher
   * value means higher confidence.
   *
   * @generated from field: float confidence = 4;
   */
  confidence: number;

  /**
   * The same object may be identified on muitiple frames which are typical
   * adjacent. The set of frames where a particular object has been detected
   * form a track. This track_id can be used to trace down all frames for an
   * detected object.
   *
   * @generated from field: int64 track_id = 5;
   */
  trackId: bigint;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.VideoObjectTrackingPredictionResult.DetectedObject.
 * Use `create(VideoObjectTrackingPredictionResult_DetectedObjectSchema)` to create a new message.
 */
export const VideoObjectTrackingPredictionResult_DetectedObjectSchema: GenMessage<VideoObjectTrackingPredictionResult_DetectedObject> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 6, 1);

/**
 * Prediction output format for Video Classification.
 *
 * @generated from message google.cloud.visionai.v1alpha1.VideoClassificationPredictionResult
 */
export type VideoClassificationPredictionResult = Message<"google.cloud.visionai.v1alpha1.VideoClassificationPredictionResult"> & {
  /**
   * The beginning, inclusive, of the video's time segment in which the
   * classifications have been identified.
   *
   * @generated from field: google.protobuf.Timestamp segment_start_time = 1;
   */
  segmentStartTime?: Timestamp;

  /**
   * The end, inclusive, of the video's time segment in which the
   * classifications have been identified. Particularly, if the end is the same
   * as the start, it means the identification happens on a specific video
   * frame.
   *
   * @generated from field: google.protobuf.Timestamp segment_end_time = 2;
   */
  segmentEndTime?: Timestamp;

  /**
   * All of the classifications identified in the time range.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.VideoClassificationPredictionResult.IdentifiedClassification classifications = 3;
   */
  classifications: VideoClassificationPredictionResult_IdentifiedClassification[];
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.VideoClassificationPredictionResult.
 * Use `create(VideoClassificationPredictionResultSchema)` to create a new message.
 */
export const VideoClassificationPredictionResultSchema: GenMessage<VideoClassificationPredictionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 7);

/**
 * Each IdentifiedClassification is one particular identification of an
 * classification specified with the AnnotationSpec id and display_name,
 * and the associated confidence score.
 *
 * @generated from message google.cloud.visionai.v1alpha1.VideoClassificationPredictionResult.IdentifiedClassification
 */
export type VideoClassificationPredictionResult_IdentifiedClassification = Message<"google.cloud.visionai.v1alpha1.VideoClassificationPredictionResult.IdentifiedClassification"> & {
  /**
   * The resource ID of the AnnotationSpec that had been identified.
   *
   * @generated from field: string id = 1;
   */
  id: string;

  /**
   * The display name of the AnnotationSpec that had been identified.
   *
   * @generated from field: string display_name = 2;
   */
  displayName: string;

  /**
   * The Model's confidence in correction of this identification, higher
   * value means higher confidence.
   *
   * @generated from field: float confidence = 3;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.VideoClassificationPredictionResult.IdentifiedClassification.
 * Use `create(VideoClassificationPredictionResult_IdentifiedClassificationSchema)` to create a new message.
 */
export const VideoClassificationPredictionResult_IdentifiedClassificationSchema: GenMessage<VideoClassificationPredictionResult_IdentifiedClassification> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 7, 0);

/**
 * The prediction result proto for occupancy counting.
 *
 * @generated from message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult
 */
export type OccupancyCountingPredictionResult = Message<"google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult"> & {
  /**
   * Current timestamp.
   *
   * @generated from field: google.protobuf.Timestamp current_time = 1;
   */
  currentTime?: Timestamp;

  /**
   * A list of identified boxes.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.IdentifiedBox identified_boxes = 2;
   */
  identifiedBoxes: OccupancyCountingPredictionResult_IdentifiedBox[];

  /**
   * Detection statistics.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats stats = 3;
   */
  stats?: OccupancyCountingPredictionResult_Stats;

  /**
   * Track related information. All the tracks that are live at this timestamp.
   * It only exists if tracking is enabled.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.TrackInfo track_info = 4;
   */
  trackInfo: OccupancyCountingPredictionResult_TrackInfo[];

  /**
   * Dwell time related information. All the tracks that are live in a given
   * zone with a start and end dwell time timestamp
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.DwellTimeInfo dwell_time_info = 5;
   */
  dwellTimeInfo: OccupancyCountingPredictionResult_DwellTimeInfo[];
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.
 * Use `create(OccupancyCountingPredictionResultSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResultSchema: GenMessage<OccupancyCountingPredictionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 8);

/**
 * The entity info for annotations from occupancy counting operator.
 *
 * @generated from message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Entity
 */
export type OccupancyCountingPredictionResult_Entity = Message<"google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Entity"> & {
  /**
   * Label id.
   *
   * @generated from field: int64 label_id = 1;
   */
  labelId: bigint;

  /**
   * Human readable string of the label.
   *
   * @generated from field: string label_string = 2;
   */
  labelString: string;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Entity.
 * Use `create(OccupancyCountingPredictionResult_EntitySchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_EntitySchema: GenMessage<OccupancyCountingPredictionResult_Entity> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 8, 0);

/**
 * Identified box contains location and the entity of the object.
 *
 * @generated from message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.IdentifiedBox
 */
export type OccupancyCountingPredictionResult_IdentifiedBox = Message<"google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.IdentifiedBox"> & {
  /**
   * An unique id for this box.
   *
   * @generated from field: int64 box_id = 1;
   */
  boxId: bigint;

  /**
   * Bounding Box in the normalized coordinates.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.IdentifiedBox.NormalizedBoundingBox normalized_bounding_box = 2;
   */
  normalizedBoundingBox?: OccupancyCountingPredictionResult_IdentifiedBox_NormalizedBoundingBox;

  /**
   * Confidence score associated with this box.
   *
   * @generated from field: float score = 3;
   */
  score: number;

  /**
   * Entity of this box.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Entity entity = 4;
   */
  entity?: OccupancyCountingPredictionResult_Entity;

  /**
   * An unique id to identify a track. It should be consistent across frames.
   * It only exists if tracking is enabled.
   *
   * @generated from field: int64 track_id = 5;
   */
  trackId: bigint;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.IdentifiedBox.
 * Use `create(OccupancyCountingPredictionResult_IdentifiedBoxSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_IdentifiedBoxSchema: GenMessage<OccupancyCountingPredictionResult_IdentifiedBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 8, 1);

/**
 * Bounding Box in the normalized coordinates.
 *
 * @generated from message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.IdentifiedBox.NormalizedBoundingBox
 */
export type OccupancyCountingPredictionResult_IdentifiedBox_NormalizedBoundingBox = Message<"google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.IdentifiedBox.NormalizedBoundingBox"> & {
  /**
   * Min in x coordinate.
   *
   * @generated from field: float xmin = 1;
   */
  xmin: number;

  /**
   * Min in y coordinate.
   *
   * @generated from field: float ymin = 2;
   */
  ymin: number;

  /**
   * Width of the bounding box.
   *
   * @generated from field: float width = 3;
   */
  width: number;

  /**
   * Height of the bounding box.
   *
   * @generated from field: float height = 4;
   */
  height: number;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.IdentifiedBox.NormalizedBoundingBox.
 * Use `create(OccupancyCountingPredictionResult_IdentifiedBox_NormalizedBoundingBoxSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_IdentifiedBox_NormalizedBoundingBoxSchema: GenMessage<OccupancyCountingPredictionResult_IdentifiedBox_NormalizedBoundingBox> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 8, 1, 0);

/**
 * The statistics info for annotations from occupancy counting operator.
 *
 * @generated from message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats
 */
export type OccupancyCountingPredictionResult_Stats = Message<"google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats"> & {
  /**
   * Counts of the full frame.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.ObjectCount full_frame_count = 1;
   */
  fullFrameCount: OccupancyCountingPredictionResult_Stats_ObjectCount[];

  /**
   * Crossing line counts.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.CrossingLineCount crossing_line_counts = 2;
   */
  crossingLineCounts: OccupancyCountingPredictionResult_Stats_CrossingLineCount[];

  /**
   * Active zone counts.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.ActiveZoneCount active_zone_counts = 3;
   */
  activeZoneCounts: OccupancyCountingPredictionResult_Stats_ActiveZoneCount[];
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.
 * Use `create(OccupancyCountingPredictionResult_StatsSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_StatsSchema: GenMessage<OccupancyCountingPredictionResult_Stats> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 8, 2);

/**
 * The object info and instant count for annotations from occupancy counting
 * operator.
 *
 * @generated from message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.ObjectCount
 */
export type OccupancyCountingPredictionResult_Stats_ObjectCount = Message<"google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.ObjectCount"> & {
  /**
   * Entity of this object.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Entity entity = 1;
   */
  entity?: OccupancyCountingPredictionResult_Entity;

  /**
   * Count of the object.
   *
   * @generated from field: int32 count = 2;
   */
  count: number;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.ObjectCount.
 * Use `create(OccupancyCountingPredictionResult_Stats_ObjectCountSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_Stats_ObjectCountSchema: GenMessage<OccupancyCountingPredictionResult_Stats_ObjectCount> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 8, 2, 0);

/**
 * The object info and accumulated count for annotations from occupancy
 * counting operator.
 *
 * @generated from message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.AccumulatedObjectCount
 */
export type OccupancyCountingPredictionResult_Stats_AccumulatedObjectCount = Message<"google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.AccumulatedObjectCount"> & {
  /**
   * The start time of the accumulated count.
   *
   * @generated from field: google.protobuf.Timestamp start_time = 1;
   */
  startTime?: Timestamp;

  /**
   * The object count for the accumulated count.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.ObjectCount object_count = 2;
   */
  objectCount?: OccupancyCountingPredictionResult_Stats_ObjectCount;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.AccumulatedObjectCount.
 * Use `create(OccupancyCountingPredictionResult_Stats_AccumulatedObjectCountSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_Stats_AccumulatedObjectCountSchema: GenMessage<OccupancyCountingPredictionResult_Stats_AccumulatedObjectCount> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 8, 2, 1);

/**
 * Message for Crossing line count.
 *
 * @generated from message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.CrossingLineCount
 */
export type OccupancyCountingPredictionResult_Stats_CrossingLineCount = Message<"google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.CrossingLineCount"> & {
  /**
   * Line annotation from the user.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.StreamAnnotation annotation = 1;
   */
  annotation?: StreamAnnotation;

  /**
   * The direction that follows the right hand rule.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.ObjectCount positive_direction_counts = 2;
   */
  positiveDirectionCounts: OccupancyCountingPredictionResult_Stats_ObjectCount[];

  /**
   * The direction that is opposite to the right hand rule.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.ObjectCount negative_direction_counts = 3;
   */
  negativeDirectionCounts: OccupancyCountingPredictionResult_Stats_ObjectCount[];

  /**
   * The accumulated positive count.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.AccumulatedObjectCount accumulated_positive_direction_counts = 4;
   */
  accumulatedPositiveDirectionCounts: OccupancyCountingPredictionResult_Stats_AccumulatedObjectCount[];

  /**
   * The accumulated negative count.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.AccumulatedObjectCount accumulated_negative_direction_counts = 5;
   */
  accumulatedNegativeDirectionCounts: OccupancyCountingPredictionResult_Stats_AccumulatedObjectCount[];
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.CrossingLineCount.
 * Use `create(OccupancyCountingPredictionResult_Stats_CrossingLineCountSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_Stats_CrossingLineCountSchema: GenMessage<OccupancyCountingPredictionResult_Stats_CrossingLineCount> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 8, 2, 2);

/**
 * Message for the active zone count.
 *
 * @generated from message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.ActiveZoneCount
 */
export type OccupancyCountingPredictionResult_Stats_ActiveZoneCount = Message<"google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.ActiveZoneCount"> & {
  /**
   * Active zone annotation from the user.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.StreamAnnotation annotation = 1;
   */
  annotation?: StreamAnnotation;

  /**
   * Counts in the zone.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.ObjectCount counts = 2;
   */
  counts: OccupancyCountingPredictionResult_Stats_ObjectCount[];
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.Stats.ActiveZoneCount.
 * Use `create(OccupancyCountingPredictionResult_Stats_ActiveZoneCountSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_Stats_ActiveZoneCountSchema: GenMessage<OccupancyCountingPredictionResult_Stats_ActiveZoneCount> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 8, 2, 3);

/**
 * The track info for annotations from occupancy counting operator.
 *
 * @generated from message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.TrackInfo
 */
export type OccupancyCountingPredictionResult_TrackInfo = Message<"google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.TrackInfo"> & {
  /**
   * An unique id to identify a track. It should be consistent across frames.
   *
   * @generated from field: string track_id = 1;
   */
  trackId: string;

  /**
   * Start timestamp of this track.
   *
   * @generated from field: google.protobuf.Timestamp start_time = 2;
   */
  startTime?: Timestamp;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.TrackInfo.
 * Use `create(OccupancyCountingPredictionResult_TrackInfoSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_TrackInfoSchema: GenMessage<OccupancyCountingPredictionResult_TrackInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 8, 3);

/**
 * The dwell time info for annotations from occupancy counting operator.
 *
 * @generated from message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.DwellTimeInfo
 */
export type OccupancyCountingPredictionResult_DwellTimeInfo = Message<"google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.DwellTimeInfo"> & {
  /**
   * An unique id to identify a track. It should be consistent across frames.
   *
   * @generated from field: string track_id = 1;
   */
  trackId: string;

  /**
   * The unique id for the zone in which the object is dwelling/waiting.
   *
   * @generated from field: string zone_id = 2;
   */
  zoneId: string;

  /**
   * The beginning time when a dwelling object has been identified in a zone.
   *
   * @generated from field: google.protobuf.Timestamp dwell_start_time = 3;
   */
  dwellStartTime?: Timestamp;

  /**
   * The end time when a dwelling object has exited in a zone.
   *
   * @generated from field: google.protobuf.Timestamp dwell_end_time = 4;
   */
  dwellEndTime?: Timestamp;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.OccupancyCountingPredictionResult.DwellTimeInfo.
 * Use `create(OccupancyCountingPredictionResult_DwellTimeInfoSchema)` to create a new message.
 */
export const OccupancyCountingPredictionResult_DwellTimeInfoSchema: GenMessage<OccupancyCountingPredictionResult_DwellTimeInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 8, 4);

/**
 * message about annotations about Vision AI stream resource.
 *
 * @generated from message google.cloud.visionai.v1alpha1.StreamAnnotation
 */
export type StreamAnnotation = Message<"google.cloud.visionai.v1alpha1.StreamAnnotation"> & {
  /**
   * @generated from oneof google.cloud.visionai.v1alpha1.StreamAnnotation.annotation_payload
   */
  annotationPayload: {
    /**
     * Annotation for type ACTIVE_ZONE
     *
     * @generated from field: google.cloud.visionai.v1alpha1.NormalizedPolygon active_zone = 5;
     */
    value: NormalizedPolygon;
    case: "activeZone";
  } | {
    /**
     * Annotation for type CROSSING_LINE
     *
     * @generated from field: google.cloud.visionai.v1alpha1.NormalizedPolyline crossing_line = 6;
     */
    value: NormalizedPolyline;
    case: "crossingLine";
  } | { case: undefined; value?: undefined };

  /**
   * ID of the annotation. It must be unique when used in the certain context.
   * For example, all the annotations to one input streams of a Vision AI
   * application.
   *
   * @generated from field: string id = 1;
   */
  id: string;

  /**
   * User-friendly name for the annotation.
   *
   * @generated from field: string display_name = 2;
   */
  displayName: string;

  /**
   * The Vision AI stream resource name.
   *
   * @generated from field: string source_stream = 3;
   */
  sourceStream: string;

  /**
   * The actual type of Annotation.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.StreamAnnotationType type = 4;
   */
  type: StreamAnnotationType;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.StreamAnnotation.
 * Use `create(StreamAnnotationSchema)` to create a new message.
 */
export const StreamAnnotationSchema: GenMessage<StreamAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 9);

/**
 * A wrapper of repeated StreamAnnotation.
 *
 * @generated from message google.cloud.visionai.v1alpha1.StreamAnnotations
 */
export type StreamAnnotations = Message<"google.cloud.visionai.v1alpha1.StreamAnnotations"> & {
  /**
   * Multiple annotations.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.StreamAnnotation stream_annotations = 1;
   */
  streamAnnotations: StreamAnnotation[];
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.StreamAnnotations.
 * Use `create(StreamAnnotationsSchema)` to create a new message.
 */
export const StreamAnnotationsSchema: GenMessage<StreamAnnotations> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 10);

/**
 * Normalized Polygon.
 *
 * @generated from message google.cloud.visionai.v1alpha1.NormalizedPolygon
 */
export type NormalizedPolygon = Message<"google.cloud.visionai.v1alpha1.NormalizedPolygon"> & {
  /**
   * The bounding polygon normalized vertices. Top left corner of the image
   * will be [0, 0].
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.NormalizedVertex normalized_vertices = 1;
   */
  normalizedVertices: NormalizedVertex[];
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.NormalizedPolygon.
 * Use `create(NormalizedPolygonSchema)` to create a new message.
 */
export const NormalizedPolygonSchema: GenMessage<NormalizedPolygon> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 11);

/**
 * Normalized Pplyline, which represents a curve consisting of connected
 * straight-line segments.
 *
 * @generated from message google.cloud.visionai.v1alpha1.NormalizedPolyline
 */
export type NormalizedPolyline = Message<"google.cloud.visionai.v1alpha1.NormalizedPolyline"> & {
  /**
   * A sequence of vertices connected by straight lines.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.NormalizedVertex normalized_vertices = 1;
   */
  normalizedVertices: NormalizedVertex[];
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.NormalizedPolyline.
 * Use `create(NormalizedPolylineSchema)` to create a new message.
 */
export const NormalizedPolylineSchema: GenMessage<NormalizedPolyline> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 12);

/**
 * A vertex represents a 2D point in the image.
 * NOTE: the normalized vertex coordinates are relative to the original image
 * and range from 0 to 1.
 *
 * @generated from message google.cloud.visionai.v1alpha1.NormalizedVertex
 */
export type NormalizedVertex = Message<"google.cloud.visionai.v1alpha1.NormalizedVertex"> & {
  /**
   * X coordinate.
   *
   * @generated from field: float x = 1;
   */
  x: number;

  /**
   * Y coordinate.
   *
   * @generated from field: float y = 2;
   */
  y: number;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.NormalizedVertex.
 * Use `create(NormalizedVertexSchema)` to create a new message.
 */
export const NormalizedVertexSchema: GenMessage<NormalizedVertex> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 13);

/**
 * Message of essential metadata of App Platform.
 * This message is usually attached to a certain processor output annotation for
 * customer to identify the source of the data.
 *
 * @generated from message google.cloud.visionai.v1alpha1.AppPlatformMetadata
 */
export type AppPlatformMetadata = Message<"google.cloud.visionai.v1alpha1.AppPlatformMetadata"> & {
  /**
   * The application resource name.
   *
   * @generated from field: string application = 1;
   */
  application: string;

  /**
   * The instance resource id. Instance is the nested resource of application
   * under collection 'instances'.
   *
   * @generated from field: string instance_id = 2;
   */
  instanceId: string;

  /**
   * The node name of the application graph.
   *
   * @generated from field: string node = 3;
   */
  node: string;

  /**
   * The referred processor resource name of the application node.
   *
   * @generated from field: string processor = 4;
   */
  processor: string;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.AppPlatformMetadata.
 * Use `create(AppPlatformMetadataSchema)` to create a new message.
 */
export const AppPlatformMetadataSchema: GenMessage<AppPlatformMetadata> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 14);

/**
 * For any cloud function based customer processing logic, customer's cloud
 * function is expected to receive AppPlatformCloudFunctionRequest as request
 * and send back AppPlatformCloudFunctionResponse as response.
 * Message of request from AppPlatform to Cloud Function.
 *
 * @generated from message google.cloud.visionai.v1alpha1.AppPlatformCloudFunctionRequest
 */
export type AppPlatformCloudFunctionRequest = Message<"google.cloud.visionai.v1alpha1.AppPlatformCloudFunctionRequest"> & {
  /**
   * The metadata of the AppPlatform for customer to identify the source of the
   * payload.
   *
   * @generated from field: google.cloud.visionai.v1alpha1.AppPlatformMetadata app_platform_metadata = 1;
   */
  appPlatformMetadata?: AppPlatformMetadata;

  /**
   * The actual annotations to be processed by the customized Cloud Function.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.AppPlatformCloudFunctionRequest.StructedInputAnnotation annotations = 2;
   */
  annotations: AppPlatformCloudFunctionRequest_StructedInputAnnotation[];
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.AppPlatformCloudFunctionRequest.
 * Use `create(AppPlatformCloudFunctionRequestSchema)` to create a new message.
 */
export const AppPlatformCloudFunctionRequestSchema: GenMessage<AppPlatformCloudFunctionRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 15);

/**
 * A general annotation message that uses struct format to represent different
 * concrete annotation protobufs.
 *
 * @generated from message google.cloud.visionai.v1alpha1.AppPlatformCloudFunctionRequest.StructedInputAnnotation
 */
export type AppPlatformCloudFunctionRequest_StructedInputAnnotation = Message<"google.cloud.visionai.v1alpha1.AppPlatformCloudFunctionRequest.StructedInputAnnotation"> & {
  /**
   * The ingestion time of the current annotation.
   *
   * @generated from field: int64 ingestion_time_micros = 1;
   */
  ingestionTimeMicros: bigint;

  /**
   * The struct format of the actual annotation.
   *
   * @generated from field: google.protobuf.Struct annotation = 2;
   */
  annotation?: JsonObject;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.AppPlatformCloudFunctionRequest.StructedInputAnnotation.
 * Use `create(AppPlatformCloudFunctionRequest_StructedInputAnnotationSchema)` to create a new message.
 */
export const AppPlatformCloudFunctionRequest_StructedInputAnnotationSchema: GenMessage<AppPlatformCloudFunctionRequest_StructedInputAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 15, 0);

/**
 * Message of the response from customer's Cloud Function to AppPlatform.
 *
 * @generated from message google.cloud.visionai.v1alpha1.AppPlatformCloudFunctionResponse
 */
export type AppPlatformCloudFunctionResponse = Message<"google.cloud.visionai.v1alpha1.AppPlatformCloudFunctionResponse"> & {
  /**
   * The modified annotations that is returned back to AppPlatform.
   * If the annotations fields are empty, then those annotations will be dropped
   * by AppPlatform.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.AppPlatformCloudFunctionResponse.StructedOutputAnnotation annotations = 2;
   */
  annotations: AppPlatformCloudFunctionResponse_StructedOutputAnnotation[];

  /**
   * If set to true, AppPlatform will use original annotations instead of
   * dropping them, even if it is empty in the annotations filed.
   *
   * @generated from field: bool annotation_passthrough = 3;
   */
  annotationPassthrough: boolean;

  /**
   * The event notifications that is returned back to AppPlatform. Typically it
   * will then be configured to be consumed/forwared to a operator that handles
   * events, such as Pub/Sub operator.
   *
   * @generated from field: repeated google.cloud.visionai.v1alpha1.AppPlatformEventBody events = 4;
   */
  events: AppPlatformEventBody[];
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.AppPlatformCloudFunctionResponse.
 * Use `create(AppPlatformCloudFunctionResponseSchema)` to create a new message.
 */
export const AppPlatformCloudFunctionResponseSchema: GenMessage<AppPlatformCloudFunctionResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 16);

/**
 * A general annotation message that uses struct format to represent different
 * concrete annotation protobufs.
 *
 * @generated from message google.cloud.visionai.v1alpha1.AppPlatformCloudFunctionResponse.StructedOutputAnnotation
 */
export type AppPlatformCloudFunctionResponse_StructedOutputAnnotation = Message<"google.cloud.visionai.v1alpha1.AppPlatformCloudFunctionResponse.StructedOutputAnnotation"> & {
  /**
   * The struct format of the actual annotation.
   *
   * @generated from field: google.protobuf.Struct annotation = 1;
   */
  annotation?: JsonObject;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.AppPlatformCloudFunctionResponse.StructedOutputAnnotation.
 * Use `create(AppPlatformCloudFunctionResponse_StructedOutputAnnotationSchema)` to create a new message.
 */
export const AppPlatformCloudFunctionResponse_StructedOutputAnnotationSchema: GenMessage<AppPlatformCloudFunctionResponse_StructedOutputAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 16, 0);

/**
 * Message of content of appPlatform event
 *
 * @generated from message google.cloud.visionai.v1alpha1.AppPlatformEventBody
 */
export type AppPlatformEventBody = Message<"google.cloud.visionai.v1alpha1.AppPlatformEventBody"> & {
  /**
   * Human readable string of the event like "There are more than 6 people in
   * the scene". or "Shelf is empty!".
   *
   * @generated from field: string event_message = 1;
   */
  eventMessage: string;

  /**
   * For the case of Pub/Sub, it will be stored in the message attributes.
   * pubsub.proto
   *
   * @generated from field: google.protobuf.Struct payload = 2;
   */
  payload?: JsonObject;

  /**
   * User defined Event Id, used to classify event, within a delivery interval,
   * events from the same application instance with the same id will be
   * de-duplicated & only first one will be sent out. Empty event_id will be
   * treated as "".
   *
   * @generated from field: string event_id = 3;
   */
  eventId: string;
};

/**
 * Describes the message google.cloud.visionai.v1alpha1.AppPlatformEventBody.
 * Use `create(AppPlatformEventBodySchema)` to create a new message.
 */
export const AppPlatformEventBodySchema: GenMessage<AppPlatformEventBody> = /*@__PURE__*/
  messageDesc(file_google_cloud_visionai_v1alpha1_annotations, 17);

/**
 * Enum describing all possible types of a stream annotation.
 *
 * @generated from enum google.cloud.visionai.v1alpha1.StreamAnnotationType
 */
export enum StreamAnnotationType {
  /**
   * Type UNSPECIFIED.
   *
   * @generated from enum value: STREAM_ANNOTATION_TYPE_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * active_zone annotation defines a polygon on top of the content from an
   * image/video based stream, following processing will only focus on the
   * content inside the active zone.
   *
   * @generated from enum value: STREAM_ANNOTATION_TYPE_ACTIVE_ZONE = 1;
   */
  ACTIVE_ZONE = 1,

  /**
   * crossing_line annotation defines a polyline on top of the content from an
   * image/video based Vision AI stream, events happening across the line will
   * be captured. For example, the counts of people who goes acroos the line
   * in Occupancy Analytic Processor.
   *
   * @generated from enum value: STREAM_ANNOTATION_TYPE_CROSSING_LINE = 2;
   */
  CROSSING_LINE = 2,
}

/**
 * Describes the enum google.cloud.visionai.v1alpha1.StreamAnnotationType.
 */
export const StreamAnnotationTypeSchema: GenEnum<StreamAnnotationType> = /*@__PURE__*/
  enumDesc(file_google_cloud_visionai_v1alpha1_annotations, 0);

