// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/dataproc/v1/batches.proto (package google.cloud.dataproc.v1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import { file_google_api_resource } from "../../../api/resource_pb";
import type { EnvironmentConfig, RuntimeConfig, RuntimeInfo } from "./shared_pb";
import { file_google_cloud_dataproc_v1_shared } from "./shared_pb";
import type { OperationSchema } from "../../../longrunning/operations_pb";
import { file_google_longrunning_operations } from "../../../longrunning/operations_pb";
import type { EmptySchema, Timestamp } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_empty, file_google_protobuf_timestamp } from "@bufbuild/protobuf/wkt";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/dataproc/v1/batches.proto.
 */
export const file_google_cloud_dataproc_v1_batches: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_api_resource, file_google_cloud_dataproc_v1_shared, file_google_longrunning_operations, file_google_protobuf_empty, file_google_protobuf_timestamp]);

/**
 * A request to create a batch workload.
 *
 * @generated from message google.cloud.dataproc.v1.CreateBatchRequest
 */
export type CreateBatchRequest = Message<"google.cloud.dataproc.v1.CreateBatchRequest"> & {
  /**
   * Required. The parent resource where this batch will be created.
   *
   * @generated from field: string parent = 1;
   */
  parent: string;

  /**
   * Required. The batch to create.
   *
   * @generated from field: google.cloud.dataproc.v1.Batch batch = 2;
   */
  batch?: Batch;

  /**
   * Optional. The ID to use for the batch, which will become the final
   * component of the batch's resource name.
   *
   * This value must be 4-63 characters. Valid characters are `/[a-z][0-9]-/`.
   *
   * @generated from field: string batch_id = 3;
   */
  batchId: string;

  /**
   * Optional. A unique ID used to identify the request. If the service
   * receives two
   * [CreateBatchRequest](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.CreateBatchRequest)s
   * with the same request_id, the second request is ignored and the
   * Operation that corresponds to the first Batch created and stored
   * in the backend is returned.
   *
   * Recommendation: Set this value to a
   * [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).
   *
   * The value must contain only letters (a-z, A-Z), numbers (0-9),
   * underscores (_), and hyphens (-). The maximum length is 40 characters.
   *
   * @generated from field: string request_id = 4;
   */
  requestId: string;
};

/**
 * Describes the message google.cloud.dataproc.v1.CreateBatchRequest.
 * Use `create(CreateBatchRequestSchema)` to create a new message.
 */
export const CreateBatchRequestSchema: GenMessage<CreateBatchRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_dataproc_v1_batches, 0);

/**
 * A request to get the resource representation for a batch workload.
 *
 * @generated from message google.cloud.dataproc.v1.GetBatchRequest
 */
export type GetBatchRequest = Message<"google.cloud.dataproc.v1.GetBatchRequest"> & {
  /**
   * Required. The fully qualified name of the batch to retrieve
   * in the format
   * "projects/PROJECT_ID/locations/DATAPROC_REGION/batches/BATCH_ID"
   *
   * @generated from field: string name = 1;
   */
  name: string;
};

/**
 * Describes the message google.cloud.dataproc.v1.GetBatchRequest.
 * Use `create(GetBatchRequestSchema)` to create a new message.
 */
export const GetBatchRequestSchema: GenMessage<GetBatchRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_dataproc_v1_batches, 1);

/**
 * A request to list batch workloads in a project.
 *
 * @generated from message google.cloud.dataproc.v1.ListBatchesRequest
 */
export type ListBatchesRequest = Message<"google.cloud.dataproc.v1.ListBatchesRequest"> & {
  /**
   * Required. The parent, which owns this collection of batches.
   *
   * @generated from field: string parent = 1;
   */
  parent: string;

  /**
   * Optional. The maximum number of batches to return in each response.
   * The service may return fewer than this value.
   * The default page size is 20; the maximum page size is 1000.
   *
   * @generated from field: int32 page_size = 2;
   */
  pageSize: number;

  /**
   * Optional. A page token received from a previous `ListBatches` call.
   * Provide this token to retrieve the subsequent page.
   *
   * @generated from field: string page_token = 3;
   */
  pageToken: string;

  /**
   * Optional. A filter for the batches to return in the response.
   *
   * A filter is a logical expression constraining the values of various fields
   * in each batch resource. Filters are case sensitive, and may contain
   * multiple clauses combined with logical operators (AND/OR).
   * Supported fields are `batch_id`, `batch_uuid`, `state`, and `create_time`.
   *
   * e.g. `state = RUNNING and create_time < "2023-01-01T00:00:00Z"`
   * filters for batches in state RUNNING that were created before 2023-01-01
   *
   * See https://google.aip.dev/assets/misc/ebnf-filtering.txt for a detailed
   * description of the filter syntax and a list of supported comparisons.
   *
   * @generated from field: string filter = 4;
   */
  filter: string;

  /**
   * Optional. Field(s) on which to sort the list of batches.
   *
   * Currently the only supported sort orders are unspecified (empty) and
   * `create_time desc` to sort by most recently created batches first.
   *
   * See https://google.aip.dev/132#ordering for more details.
   *
   * @generated from field: string order_by = 5;
   */
  orderBy: string;
};

/**
 * Describes the message google.cloud.dataproc.v1.ListBatchesRequest.
 * Use `create(ListBatchesRequestSchema)` to create a new message.
 */
export const ListBatchesRequestSchema: GenMessage<ListBatchesRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_dataproc_v1_batches, 2);

/**
 * A list of batch workloads.
 *
 * @generated from message google.cloud.dataproc.v1.ListBatchesResponse
 */
export type ListBatchesResponse = Message<"google.cloud.dataproc.v1.ListBatchesResponse"> & {
  /**
   * The batches from the specified collection.
   *
   * @generated from field: repeated google.cloud.dataproc.v1.Batch batches = 1;
   */
  batches: Batch[];

  /**
   * A token, which can be sent as `page_token` to retrieve the next page.
   * If this field is omitted, there are no subsequent pages.
   *
   * @generated from field: string next_page_token = 2;
   */
  nextPageToken: string;

  /**
   * Output only. List of Batches that could not be included in the response.
   * Attempting to get one of these resources may indicate why it was not
   * included in the list response.
   *
   * @generated from field: repeated string unreachable = 3;
   */
  unreachable: string[];
};

/**
 * Describes the message google.cloud.dataproc.v1.ListBatchesResponse.
 * Use `create(ListBatchesResponseSchema)` to create a new message.
 */
export const ListBatchesResponseSchema: GenMessage<ListBatchesResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_dataproc_v1_batches, 3);

/**
 * A request to delete a batch workload.
 *
 * @generated from message google.cloud.dataproc.v1.DeleteBatchRequest
 */
export type DeleteBatchRequest = Message<"google.cloud.dataproc.v1.DeleteBatchRequest"> & {
  /**
   * Required. The fully qualified name of the batch to retrieve
   * in the format
   * "projects/PROJECT_ID/locations/DATAPROC_REGION/batches/BATCH_ID"
   *
   * @generated from field: string name = 1;
   */
  name: string;
};

/**
 * Describes the message google.cloud.dataproc.v1.DeleteBatchRequest.
 * Use `create(DeleteBatchRequestSchema)` to create a new message.
 */
export const DeleteBatchRequestSchema: GenMessage<DeleteBatchRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_dataproc_v1_batches, 4);

/**
 * A representation of a batch workload in the service.
 *
 * @generated from message google.cloud.dataproc.v1.Batch
 */
export type Batch = Message<"google.cloud.dataproc.v1.Batch"> & {
  /**
   * Output only. The resource name of the batch.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Output only. A batch UUID (Unique Universal Identifier). The service
   * generates this value when it creates the batch.
   *
   * @generated from field: string uuid = 2;
   */
  uuid: string;

  /**
   * Output only. The time when the batch was created.
   *
   * @generated from field: google.protobuf.Timestamp create_time = 3;
   */
  createTime?: Timestamp;

  /**
   * The application/framework-specific portion of the batch configuration.
   *
   * @generated from oneof google.cloud.dataproc.v1.Batch.batch_config
   */
  batchConfig: {
    /**
     * Optional. PySpark batch config.
     *
     * @generated from field: google.cloud.dataproc.v1.PySparkBatch pyspark_batch = 4;
     */
    value: PySparkBatch;
    case: "pysparkBatch";
  } | {
    /**
     * Optional. Spark batch config.
     *
     * @generated from field: google.cloud.dataproc.v1.SparkBatch spark_batch = 5;
     */
    value: SparkBatch;
    case: "sparkBatch";
  } | {
    /**
     * Optional. SparkR batch config.
     *
     * @generated from field: google.cloud.dataproc.v1.SparkRBatch spark_r_batch = 6;
     */
    value: SparkRBatch;
    case: "sparkRBatch";
  } | {
    /**
     * Optional. SparkSql batch config.
     *
     * @generated from field: google.cloud.dataproc.v1.SparkSqlBatch spark_sql_batch = 7;
     */
    value: SparkSqlBatch;
    case: "sparkSqlBatch";
  } | { case: undefined; value?: undefined };

  /**
   * Output only. Runtime information about batch execution.
   *
   * @generated from field: google.cloud.dataproc.v1.RuntimeInfo runtime_info = 8;
   */
  runtimeInfo?: RuntimeInfo;

  /**
   * Output only. The state of the batch.
   *
   * @generated from field: google.cloud.dataproc.v1.Batch.State state = 9;
   */
  state: Batch_State;

  /**
   * Output only. Batch state details, such as a failure
   * description if the state is `FAILED`.
   *
   * @generated from field: string state_message = 10;
   */
  stateMessage: string;

  /**
   * Output only. The time when the batch entered a current state.
   *
   * @generated from field: google.protobuf.Timestamp state_time = 11;
   */
  stateTime?: Timestamp;

  /**
   * Output only. The email address of the user who created the batch.
   *
   * @generated from field: string creator = 12;
   */
  creator: string;

  /**
   * Optional. The labels to associate with this batch.
   * Label **keys** must contain 1 to 63 characters, and must conform to
   * [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).
   * Label **values** may be empty, but, if present, must contain 1 to 63
   * characters, and must conform to [RFC
   * 1035](https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be
   * associated with a batch.
   *
   * @generated from field: map<string, string> labels = 13;
   */
  labels: { [key: string]: string };

  /**
   * Optional. Runtime configuration for the batch execution.
   *
   * @generated from field: google.cloud.dataproc.v1.RuntimeConfig runtime_config = 14;
   */
  runtimeConfig?: RuntimeConfig;

  /**
   * Optional. Environment configuration for the batch execution.
   *
   * @generated from field: google.cloud.dataproc.v1.EnvironmentConfig environment_config = 15;
   */
  environmentConfig?: EnvironmentConfig;

  /**
   * Output only. The resource name of the operation associated with this batch.
   *
   * @generated from field: string operation = 16;
   */
  operation: string;

  /**
   * Output only. Historical state information for the batch.
   *
   * @generated from field: repeated google.cloud.dataproc.v1.Batch.StateHistory state_history = 17;
   */
  stateHistory: Batch_StateHistory[];
};

/**
 * Describes the message google.cloud.dataproc.v1.Batch.
 * Use `create(BatchSchema)` to create a new message.
 */
export const BatchSchema: GenMessage<Batch> = /*@__PURE__*/
  messageDesc(file_google_cloud_dataproc_v1_batches, 5);

/**
 * Historical state information.
 *
 * @generated from message google.cloud.dataproc.v1.Batch.StateHistory
 */
export type Batch_StateHistory = Message<"google.cloud.dataproc.v1.Batch.StateHistory"> & {
  /**
   * Output only. The state of the batch at this point in history.
   *
   * @generated from field: google.cloud.dataproc.v1.Batch.State state = 1;
   */
  state: Batch_State;

  /**
   * Output only. Details about the state at this point in history.
   *
   * @generated from field: string state_message = 2;
   */
  stateMessage: string;

  /**
   * Output only. The time when the batch entered the historical state.
   *
   * @generated from field: google.protobuf.Timestamp state_start_time = 3;
   */
  stateStartTime?: Timestamp;
};

/**
 * Describes the message google.cloud.dataproc.v1.Batch.StateHistory.
 * Use `create(Batch_StateHistorySchema)` to create a new message.
 */
export const Batch_StateHistorySchema: GenMessage<Batch_StateHistory> = /*@__PURE__*/
  messageDesc(file_google_cloud_dataproc_v1_batches, 5, 0);

/**
 * The batch state.
 *
 * @generated from enum google.cloud.dataproc.v1.Batch.State
 */
export enum Batch_State {
  /**
   * The batch state is unknown.
   *
   * @generated from enum value: STATE_UNSPECIFIED = 0;
   */
  STATE_UNSPECIFIED = 0,

  /**
   * The batch is created before running.
   *
   * @generated from enum value: PENDING = 1;
   */
  PENDING = 1,

  /**
   * The batch is running.
   *
   * @generated from enum value: RUNNING = 2;
   */
  RUNNING = 2,

  /**
   * The batch is cancelling.
   *
   * @generated from enum value: CANCELLING = 3;
   */
  CANCELLING = 3,

  /**
   * The batch cancellation was successful.
   *
   * @generated from enum value: CANCELLED = 4;
   */
  CANCELLED = 4,

  /**
   * The batch completed successfully.
   *
   * @generated from enum value: SUCCEEDED = 5;
   */
  SUCCEEDED = 5,

  /**
   * The batch is no longer running due to an error.
   *
   * @generated from enum value: FAILED = 6;
   */
  FAILED = 6,
}

/**
 * Describes the enum google.cloud.dataproc.v1.Batch.State.
 */
export const Batch_StateSchema: GenEnum<Batch_State> = /*@__PURE__*/
  enumDesc(file_google_cloud_dataproc_v1_batches, 5, 0);

/**
 * A configuration for running an
 * [Apache
 * PySpark](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html)
 * batch workload.
 *
 * @generated from message google.cloud.dataproc.v1.PySparkBatch
 */
export type PySparkBatch = Message<"google.cloud.dataproc.v1.PySparkBatch"> & {
  /**
   * Required. The HCFS URI of the main Python file to use as the Spark driver.
   * Must be a .py file.
   *
   * @generated from field: string main_python_file_uri = 1;
   */
  mainPythonFileUri: string;

  /**
   * Optional. The arguments to pass to the driver. Do not include arguments
   * that can be set as batch properties, such as `--conf`, since a collision
   * can occur that causes an incorrect batch submission.
   *
   * @generated from field: repeated string args = 2;
   */
  args: string[];

  /**
   * Optional. HCFS file URIs of Python files to pass to the PySpark
   * framework. Supported file types: `.py`, `.egg`, and `.zip`.
   *
   * @generated from field: repeated string python_file_uris = 3;
   */
  pythonFileUris: string[];

  /**
   * Optional. HCFS URIs of jar files to add to the classpath of the
   * Spark driver and tasks.
   *
   * @generated from field: repeated string jar_file_uris = 4;
   */
  jarFileUris: string[];

  /**
   * Optional. HCFS URIs of files to be placed in the working directory of
   * each executor.
   *
   * @generated from field: repeated string file_uris = 5;
   */
  fileUris: string[];

  /**
   * Optional. HCFS URIs of archives to be extracted into the working directory
   * of each executor. Supported file types:
   * `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.
   *
   * @generated from field: repeated string archive_uris = 6;
   */
  archiveUris: string[];
};

/**
 * Describes the message google.cloud.dataproc.v1.PySparkBatch.
 * Use `create(PySparkBatchSchema)` to create a new message.
 */
export const PySparkBatchSchema: GenMessage<PySparkBatch> = /*@__PURE__*/
  messageDesc(file_google_cloud_dataproc_v1_batches, 6);

/**
 * A configuration for running an [Apache Spark](https://spark.apache.org/)
 * batch workload.
 *
 * @generated from message google.cloud.dataproc.v1.SparkBatch
 */
export type SparkBatch = Message<"google.cloud.dataproc.v1.SparkBatch"> & {
  /**
   * The specification of the main method to call to drive the Spark
   * workload. Specify either the jar file that contains the main class or the
   * main class name. To pass both a main jar and a main class in that jar, add
   * the jar to `jar_file_uris`, and then specify the main class
   * name in `main_class`.
   *
   * @generated from oneof google.cloud.dataproc.v1.SparkBatch.driver
   */
  driver: {
    /**
     * Optional. The HCFS URI of the jar file that contains the main class.
     *
     * @generated from field: string main_jar_file_uri = 1;
     */
    value: string;
    case: "mainJarFileUri";
  } | {
    /**
     * Optional. The name of the driver main class. The jar file that contains
     * the class must be in the classpath or specified in `jar_file_uris`.
     *
     * @generated from field: string main_class = 2;
     */
    value: string;
    case: "mainClass";
  } | { case: undefined; value?: undefined };

  /**
   * Optional. The arguments to pass to the driver. Do not include arguments
   * that can be set as batch properties, such as `--conf`, since a collision
   * can occur that causes an incorrect batch submission.
   *
   * @generated from field: repeated string args = 3;
   */
  args: string[];

  /**
   * Optional. HCFS URIs of jar files to add to the classpath of the
   * Spark driver and tasks.
   *
   * @generated from field: repeated string jar_file_uris = 4;
   */
  jarFileUris: string[];

  /**
   * Optional. HCFS URIs of files to be placed in the working directory of
   * each executor.
   *
   * @generated from field: repeated string file_uris = 5;
   */
  fileUris: string[];

  /**
   * Optional. HCFS URIs of archives to be extracted into the working directory
   * of each executor. Supported file types:
   * `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.
   *
   * @generated from field: repeated string archive_uris = 6;
   */
  archiveUris: string[];
};

/**
 * Describes the message google.cloud.dataproc.v1.SparkBatch.
 * Use `create(SparkBatchSchema)` to create a new message.
 */
export const SparkBatchSchema: GenMessage<SparkBatch> = /*@__PURE__*/
  messageDesc(file_google_cloud_dataproc_v1_batches, 7);

/**
 * A configuration for running an
 * [Apache SparkR](https://spark.apache.org/docs/latest/sparkr.html)
 * batch workload.
 *
 * @generated from message google.cloud.dataproc.v1.SparkRBatch
 */
export type SparkRBatch = Message<"google.cloud.dataproc.v1.SparkRBatch"> & {
  /**
   * Required. The HCFS URI of the main R file to use as the driver.
   * Must be a `.R` or `.r` file.
   *
   * @generated from field: string main_r_file_uri = 1;
   */
  mainRFileUri: string;

  /**
   * Optional. The arguments to pass to the Spark driver. Do not include
   * arguments that can be set as batch properties, such as `--conf`, since a
   * collision can occur that causes an incorrect batch submission.
   *
   * @generated from field: repeated string args = 2;
   */
  args: string[];

  /**
   * Optional. HCFS URIs of files to be placed in the working directory of
   * each executor.
   *
   * @generated from field: repeated string file_uris = 3;
   */
  fileUris: string[];

  /**
   * Optional. HCFS URIs of archives to be extracted into the working directory
   * of each executor. Supported file types:
   * `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.
   *
   * @generated from field: repeated string archive_uris = 4;
   */
  archiveUris: string[];
};

/**
 * Describes the message google.cloud.dataproc.v1.SparkRBatch.
 * Use `create(SparkRBatchSchema)` to create a new message.
 */
export const SparkRBatchSchema: GenMessage<SparkRBatch> = /*@__PURE__*/
  messageDesc(file_google_cloud_dataproc_v1_batches, 8);

/**
 * A configuration for running
 * [Apache Spark SQL](https://spark.apache.org/sql/) queries as a batch
 * workload.
 *
 * @generated from message google.cloud.dataproc.v1.SparkSqlBatch
 */
export type SparkSqlBatch = Message<"google.cloud.dataproc.v1.SparkSqlBatch"> & {
  /**
   * Required. The HCFS URI of the script that contains Spark SQL queries to
   * execute.
   *
   * @generated from field: string query_file_uri = 1;
   */
  queryFileUri: string;

  /**
   * Optional. Mapping of query variable names to values (equivalent to the
   * Spark SQL command: `SET name="value";`).
   *
   * @generated from field: map<string, string> query_variables = 2;
   */
  queryVariables: { [key: string]: string };

  /**
   * Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @generated from field: repeated string jar_file_uris = 3;
   */
  jarFileUris: string[];
};

/**
 * Describes the message google.cloud.dataproc.v1.SparkSqlBatch.
 * Use `create(SparkSqlBatchSchema)` to create a new message.
 */
export const SparkSqlBatchSchema: GenMessage<SparkSqlBatch> = /*@__PURE__*/
  messageDesc(file_google_cloud_dataproc_v1_batches, 9);

/**
 * The BatchController provides methods to manage batch workloads.
 *
 * @generated from service google.cloud.dataproc.v1.BatchController
 */
export const BatchController: GenService<{
  /**
   * Creates a batch workload that executes asynchronously.
   *
   * @generated from rpc google.cloud.dataproc.v1.BatchController.CreateBatch
   */
  createBatch: {
    methodKind: "unary";
    input: typeof CreateBatchRequestSchema;
    output: typeof OperationSchema;
  },
  /**
   * Gets the batch workload resource representation.
   *
   * @generated from rpc google.cloud.dataproc.v1.BatchController.GetBatch
   */
  getBatch: {
    methodKind: "unary";
    input: typeof GetBatchRequestSchema;
    output: typeof BatchSchema;
  },
  /**
   * Lists batch workloads.
   *
   * @generated from rpc google.cloud.dataproc.v1.BatchController.ListBatches
   */
  listBatches: {
    methodKind: "unary";
    input: typeof ListBatchesRequestSchema;
    output: typeof ListBatchesResponseSchema;
  },
  /**
   * Deletes the batch workload resource. If the batch is not in terminal state,
   * the delete fails and the response returns `FAILED_PRECONDITION`.
   *
   * @generated from rpc google.cloud.dataproc.v1.BatchController.DeleteBatch
   */
  deleteBatch: {
    methodKind: "unary";
    input: typeof DeleteBatchRequestSchema;
    output: typeof EmptySchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_dataproc_v1_batches, 0);

