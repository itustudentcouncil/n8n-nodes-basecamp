// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/vision/v1/image_annotator.proto (package google.cloud.vision.v1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import type { BoundingPoly, Position } from "./geometry_pb";
import { file_google_cloud_vision_v1_geometry } from "./geometry_pb";
import type { ProductSearchParams, ProductSearchResults } from "./product_search_pb";
import { file_google_cloud_vision_v1_product_search } from "./product_search_pb";
import type { TextAnnotation } from "./text_annotation_pb";
import { file_google_cloud_vision_v1_text_annotation } from "./text_annotation_pb";
import type { WebDetection } from "./web_detection_pb";
import { file_google_cloud_vision_v1_web_detection } from "./web_detection_pb";
import type { OperationSchema } from "../../../longrunning/operations_pb";
import { file_google_longrunning_operations } from "../../../longrunning/operations_pb";
import type { Timestamp } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_timestamp } from "@bufbuild/protobuf/wkt";
import type { Status } from "../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../rpc/status_pb";
import type { Color } from "../../../type/color_pb";
import { file_google_type_color } from "../../../type/color_pb";
import type { LatLng } from "../../../type/latlng_pb";
import { file_google_type_latlng } from "../../../type/latlng_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/vision/v1/image_annotator.proto.
 */
export const file_google_cloud_vision_v1_image_annotator: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_cloud_vision_v1_geometry, file_google_cloud_vision_v1_product_search, file_google_cloud_vision_v1_text_annotation, file_google_cloud_vision_v1_web_detection, file_google_longrunning_operations, file_google_protobuf_timestamp, file_google_rpc_status, file_google_type_color, file_google_type_latlng]);

/**
 * The type of Google Cloud Vision API detection to perform, and the maximum
 * number of results to return for that type. Multiple `Feature` objects can
 * be specified in the `features` list.
 *
 * @generated from message google.cloud.vision.v1.Feature
 */
export type Feature = Message<"google.cloud.vision.v1.Feature"> & {
  /**
   * The feature type.
   *
   * @generated from field: google.cloud.vision.v1.Feature.Type type = 1;
   */
  type: Feature_Type;

  /**
   * Maximum number of results of this type. Does not apply to
   * `TEXT_DETECTION`, `DOCUMENT_TEXT_DETECTION`, or `CROP_HINTS`.
   *
   * @generated from field: int32 max_results = 2;
   */
  maxResults: number;

  /**
   * Model to use for the feature.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest". `DOCUMENT_TEXT_DETECTION` and `TEXT_DETECTION` also
   * support "builtin/weekly" for the bleeding edge release updated weekly.
   *
   * @generated from field: string model = 3;
   */
  model: string;
};

/**
 * Describes the message google.cloud.vision.v1.Feature.
 * Use `create(FeatureSchema)` to create a new message.
 */
export const FeatureSchema: GenMessage<Feature> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 0);

/**
 * Type of Google Cloud Vision API feature to be extracted.
 *
 * @generated from enum google.cloud.vision.v1.Feature.Type
 */
export enum Feature_Type {
  /**
   * Unspecified feature type.
   *
   * @generated from enum value: TYPE_UNSPECIFIED = 0;
   */
  TYPE_UNSPECIFIED = 0,

  /**
   * Run face detection.
   *
   * @generated from enum value: FACE_DETECTION = 1;
   */
  FACE_DETECTION = 1,

  /**
   * Run landmark detection.
   *
   * @generated from enum value: LANDMARK_DETECTION = 2;
   */
  LANDMARK_DETECTION = 2,

  /**
   * Run logo detection.
   *
   * @generated from enum value: LOGO_DETECTION = 3;
   */
  LOGO_DETECTION = 3,

  /**
   * Run label detection.
   *
   * @generated from enum value: LABEL_DETECTION = 4;
   */
  LABEL_DETECTION = 4,

  /**
   * Run text detection / optical character recognition (OCR). Text detection
   * is optimized for areas of text within a larger image; if the image is
   * a document, use `DOCUMENT_TEXT_DETECTION` instead.
   *
   * @generated from enum value: TEXT_DETECTION = 5;
   */
  TEXT_DETECTION = 5,

  /**
   * Run dense text document OCR. Takes precedence when both
   * `DOCUMENT_TEXT_DETECTION` and `TEXT_DETECTION` are present.
   *
   * @generated from enum value: DOCUMENT_TEXT_DETECTION = 11;
   */
  DOCUMENT_TEXT_DETECTION = 11,

  /**
   * Run Safe Search to detect potentially unsafe
   * or undesirable content.
   *
   * @generated from enum value: SAFE_SEARCH_DETECTION = 6;
   */
  SAFE_SEARCH_DETECTION = 6,

  /**
   * Compute a set of image properties, such as the
   * image's dominant colors.
   *
   * @generated from enum value: IMAGE_PROPERTIES = 7;
   */
  IMAGE_PROPERTIES = 7,

  /**
   * Run crop hints.
   *
   * @generated from enum value: CROP_HINTS = 9;
   */
  CROP_HINTS = 9,

  /**
   * Run web detection.
   *
   * @generated from enum value: WEB_DETECTION = 10;
   */
  WEB_DETECTION = 10,

  /**
   * Run Product Search.
   *
   * @generated from enum value: PRODUCT_SEARCH = 12;
   */
  PRODUCT_SEARCH = 12,

  /**
   * Run localizer for object detection.
   *
   * @generated from enum value: OBJECT_LOCALIZATION = 19;
   */
  OBJECT_LOCALIZATION = 19,
}

/**
 * Describes the enum google.cloud.vision.v1.Feature.Type.
 */
export const Feature_TypeSchema: GenEnum<Feature_Type> = /*@__PURE__*/
  enumDesc(file_google_cloud_vision_v1_image_annotator, 0, 0);

/**
 * External image source (Google Cloud Storage or web URL image location).
 *
 * @generated from message google.cloud.vision.v1.ImageSource
 */
export type ImageSource = Message<"google.cloud.vision.v1.ImageSource"> & {
  /**
   * **Use `image_uri` instead.**
   *
   * The Google Cloud Storage  URI of the form
   * `gs://bucket_name/object_name`. Object versioning is not supported. See
   * [Google Cloud Storage Request
   * URIs](https://cloud.google.com/storage/docs/reference-uris) for more info.
   *
   * @generated from field: string gcs_image_uri = 1;
   */
  gcsImageUri: string;

  /**
   * The URI of the source image. Can be either:
   *
   * 1. A Google Cloud Storage URI of the form
   *    `gs://bucket_name/object_name`. Object versioning is not supported. See
   *    [Google Cloud Storage Request
   *    URIs](https://cloud.google.com/storage/docs/reference-uris) for more
   *    info.
   *
   * 2. A publicly-accessible image HTTP/HTTPS URL. When fetching images from
   *    HTTP/HTTPS URLs, Google cannot guarantee that the request will be
   *    completed. Your request may fail if the specified host denies the
   *    request (e.g. due to request throttling or DOS prevention), or if Google
   *    throttles requests to the site for abuse prevention. You should not
   *    depend on externally-hosted images for production applications.
   *
   * When both `gcs_image_uri` and `image_uri` are specified, `image_uri` takes
   * precedence.
   *
   * @generated from field: string image_uri = 2;
   */
  imageUri: string;
};

/**
 * Describes the message google.cloud.vision.v1.ImageSource.
 * Use `create(ImageSourceSchema)` to create a new message.
 */
export const ImageSourceSchema: GenMessage<ImageSource> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 1);

/**
 * Client image to perform Google Cloud Vision API tasks over.
 *
 * @generated from message google.cloud.vision.v1.Image
 */
export type Image = Message<"google.cloud.vision.v1.Image"> & {
  /**
   * Image content, represented as a stream of bytes.
   * Note: As with all `bytes` fields, protobuffers use a pure binary
   * representation, whereas JSON representations use base64.
   *
   * Currently, this field only works for BatchAnnotateImages requests. It does
   * not work for AsyncBatchAnnotateImages requests.
   *
   * @generated from field: bytes content = 1;
   */
  content: Uint8Array;

  /**
   * Google Cloud Storage image location, or publicly-accessible image
   * URL. If both `content` and `source` are provided for an image, `content`
   * takes precedence and is used to perform the image annotation request.
   *
   * @generated from field: google.cloud.vision.v1.ImageSource source = 2;
   */
  source?: ImageSource;
};

/**
 * Describes the message google.cloud.vision.v1.Image.
 * Use `create(ImageSchema)` to create a new message.
 */
export const ImageSchema: GenMessage<Image> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 2);

/**
 * A face annotation object contains the results of face detection.
 *
 * @generated from message google.cloud.vision.v1.FaceAnnotation
 */
export type FaceAnnotation = Message<"google.cloud.vision.v1.FaceAnnotation"> & {
  /**
   * The bounding polygon around the face. The coordinates of the bounding box
   * are in the original image's scale.
   * The bounding box is computed to "frame" the face in accordance with human
   * expectations. It is based on the landmarker results.
   * Note that one or more x and/or y coordinates may not be generated in the
   * `BoundingPoly` (the polygon will be unbounded) if only a partial face
   * appears in the image to be annotated.
   *
   * @generated from field: google.cloud.vision.v1.BoundingPoly bounding_poly = 1;
   */
  boundingPoly?: BoundingPoly;

  /**
   * The `fd_bounding_poly` bounding polygon is tighter than the
   * `boundingPoly`, and encloses only the skin part of the face. Typically, it
   * is used to eliminate the face from any image analysis that detects the
   * "amount of skin" visible in an image. It is not based on the
   * landmarker results, only on the initial face detection, hence
   * the <code>fd</code> (face detection) prefix.
   *
   * @generated from field: google.cloud.vision.v1.BoundingPoly fd_bounding_poly = 2;
   */
  fdBoundingPoly?: BoundingPoly;

  /**
   * Detected face landmarks.
   *
   * @generated from field: repeated google.cloud.vision.v1.FaceAnnotation.Landmark landmarks = 3;
   */
  landmarks: FaceAnnotation_Landmark[];

  /**
   * Roll angle, which indicates the amount of clockwise/anti-clockwise rotation
   * of the face relative to the image vertical about the axis perpendicular to
   * the face. Range [-180,180].
   *
   * @generated from field: float roll_angle = 4;
   */
  rollAngle: number;

  /**
   * Yaw angle, which indicates the leftward/rightward angle that the face is
   * pointing relative to the vertical plane perpendicular to the image. Range
   * [-180,180].
   *
   * @generated from field: float pan_angle = 5;
   */
  panAngle: number;

  /**
   * Pitch angle, which indicates the upwards/downwards angle that the face is
   * pointing relative to the image's horizontal plane. Range [-180,180].
   *
   * @generated from field: float tilt_angle = 6;
   */
  tiltAngle: number;

  /**
   * Detection confidence. Range [0, 1].
   *
   * @generated from field: float detection_confidence = 7;
   */
  detectionConfidence: number;

  /**
   * Face landmarking confidence. Range [0, 1].
   *
   * @generated from field: float landmarking_confidence = 8;
   */
  landmarkingConfidence: number;

  /**
   * Joy likelihood.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood joy_likelihood = 9;
   */
  joyLikelihood: Likelihood;

  /**
   * Sorrow likelihood.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood sorrow_likelihood = 10;
   */
  sorrowLikelihood: Likelihood;

  /**
   * Anger likelihood.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood anger_likelihood = 11;
   */
  angerLikelihood: Likelihood;

  /**
   * Surprise likelihood.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood surprise_likelihood = 12;
   */
  surpriseLikelihood: Likelihood;

  /**
   * Under-exposed likelihood.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood under_exposed_likelihood = 13;
   */
  underExposedLikelihood: Likelihood;

  /**
   * Blurred likelihood.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood blurred_likelihood = 14;
   */
  blurredLikelihood: Likelihood;

  /**
   * Headwear likelihood.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood headwear_likelihood = 15;
   */
  headwearLikelihood: Likelihood;
};

/**
 * Describes the message google.cloud.vision.v1.FaceAnnotation.
 * Use `create(FaceAnnotationSchema)` to create a new message.
 */
export const FaceAnnotationSchema: GenMessage<FaceAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 3);

/**
 * A face-specific landmark (for example, a face feature).
 *
 * @generated from message google.cloud.vision.v1.FaceAnnotation.Landmark
 */
export type FaceAnnotation_Landmark = Message<"google.cloud.vision.v1.FaceAnnotation.Landmark"> & {
  /**
   * Face landmark type.
   *
   * @generated from field: google.cloud.vision.v1.FaceAnnotation.Landmark.Type type = 3;
   */
  type: FaceAnnotation_Landmark_Type;

  /**
   * Face landmark position.
   *
   * @generated from field: google.cloud.vision.v1.Position position = 4;
   */
  position?: Position;
};

/**
 * Describes the message google.cloud.vision.v1.FaceAnnotation.Landmark.
 * Use `create(FaceAnnotation_LandmarkSchema)` to create a new message.
 */
export const FaceAnnotation_LandmarkSchema: GenMessage<FaceAnnotation_Landmark> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 3, 0);

/**
 * Face landmark (feature) type.
 * Left and right are defined from the vantage of the viewer of the image
 * without considering mirror projections typical of photos. So, `LEFT_EYE`,
 * typically, is the person's right eye.
 *
 * @generated from enum google.cloud.vision.v1.FaceAnnotation.Landmark.Type
 */
export enum FaceAnnotation_Landmark_Type {
  /**
   * Unknown face landmark detected. Should not be filled.
   *
   * @generated from enum value: UNKNOWN_LANDMARK = 0;
   */
  UNKNOWN_LANDMARK = 0,

  /**
   * Left eye.
   *
   * @generated from enum value: LEFT_EYE = 1;
   */
  LEFT_EYE = 1,

  /**
   * Right eye.
   *
   * @generated from enum value: RIGHT_EYE = 2;
   */
  RIGHT_EYE = 2,

  /**
   * Left of left eyebrow.
   *
   * @generated from enum value: LEFT_OF_LEFT_EYEBROW = 3;
   */
  LEFT_OF_LEFT_EYEBROW = 3,

  /**
   * Right of left eyebrow.
   *
   * @generated from enum value: RIGHT_OF_LEFT_EYEBROW = 4;
   */
  RIGHT_OF_LEFT_EYEBROW = 4,

  /**
   * Left of right eyebrow.
   *
   * @generated from enum value: LEFT_OF_RIGHT_EYEBROW = 5;
   */
  LEFT_OF_RIGHT_EYEBROW = 5,

  /**
   * Right of right eyebrow.
   *
   * @generated from enum value: RIGHT_OF_RIGHT_EYEBROW = 6;
   */
  RIGHT_OF_RIGHT_EYEBROW = 6,

  /**
   * Midpoint between eyes.
   *
   * @generated from enum value: MIDPOINT_BETWEEN_EYES = 7;
   */
  MIDPOINT_BETWEEN_EYES = 7,

  /**
   * Nose tip.
   *
   * @generated from enum value: NOSE_TIP = 8;
   */
  NOSE_TIP = 8,

  /**
   * Upper lip.
   *
   * @generated from enum value: UPPER_LIP = 9;
   */
  UPPER_LIP = 9,

  /**
   * Lower lip.
   *
   * @generated from enum value: LOWER_LIP = 10;
   */
  LOWER_LIP = 10,

  /**
   * Mouth left.
   *
   * @generated from enum value: MOUTH_LEFT = 11;
   */
  MOUTH_LEFT = 11,

  /**
   * Mouth right.
   *
   * @generated from enum value: MOUTH_RIGHT = 12;
   */
  MOUTH_RIGHT = 12,

  /**
   * Mouth center.
   *
   * @generated from enum value: MOUTH_CENTER = 13;
   */
  MOUTH_CENTER = 13,

  /**
   * Nose, bottom right.
   *
   * @generated from enum value: NOSE_BOTTOM_RIGHT = 14;
   */
  NOSE_BOTTOM_RIGHT = 14,

  /**
   * Nose, bottom left.
   *
   * @generated from enum value: NOSE_BOTTOM_LEFT = 15;
   */
  NOSE_BOTTOM_LEFT = 15,

  /**
   * Nose, bottom center.
   *
   * @generated from enum value: NOSE_BOTTOM_CENTER = 16;
   */
  NOSE_BOTTOM_CENTER = 16,

  /**
   * Left eye, top boundary.
   *
   * @generated from enum value: LEFT_EYE_TOP_BOUNDARY = 17;
   */
  LEFT_EYE_TOP_BOUNDARY = 17,

  /**
   * Left eye, right corner.
   *
   * @generated from enum value: LEFT_EYE_RIGHT_CORNER = 18;
   */
  LEFT_EYE_RIGHT_CORNER = 18,

  /**
   * Left eye, bottom boundary.
   *
   * @generated from enum value: LEFT_EYE_BOTTOM_BOUNDARY = 19;
   */
  LEFT_EYE_BOTTOM_BOUNDARY = 19,

  /**
   * Left eye, left corner.
   *
   * @generated from enum value: LEFT_EYE_LEFT_CORNER = 20;
   */
  LEFT_EYE_LEFT_CORNER = 20,

  /**
   * Right eye, top boundary.
   *
   * @generated from enum value: RIGHT_EYE_TOP_BOUNDARY = 21;
   */
  RIGHT_EYE_TOP_BOUNDARY = 21,

  /**
   * Right eye, right corner.
   *
   * @generated from enum value: RIGHT_EYE_RIGHT_CORNER = 22;
   */
  RIGHT_EYE_RIGHT_CORNER = 22,

  /**
   * Right eye, bottom boundary.
   *
   * @generated from enum value: RIGHT_EYE_BOTTOM_BOUNDARY = 23;
   */
  RIGHT_EYE_BOTTOM_BOUNDARY = 23,

  /**
   * Right eye, left corner.
   *
   * @generated from enum value: RIGHT_EYE_LEFT_CORNER = 24;
   */
  RIGHT_EYE_LEFT_CORNER = 24,

  /**
   * Left eyebrow, upper midpoint.
   *
   * @generated from enum value: LEFT_EYEBROW_UPPER_MIDPOINT = 25;
   */
  LEFT_EYEBROW_UPPER_MIDPOINT = 25,

  /**
   * Right eyebrow, upper midpoint.
   *
   * @generated from enum value: RIGHT_EYEBROW_UPPER_MIDPOINT = 26;
   */
  RIGHT_EYEBROW_UPPER_MIDPOINT = 26,

  /**
   * Left ear tragion.
   *
   * @generated from enum value: LEFT_EAR_TRAGION = 27;
   */
  LEFT_EAR_TRAGION = 27,

  /**
   * Right ear tragion.
   *
   * @generated from enum value: RIGHT_EAR_TRAGION = 28;
   */
  RIGHT_EAR_TRAGION = 28,

  /**
   * Left eye pupil.
   *
   * @generated from enum value: LEFT_EYE_PUPIL = 29;
   */
  LEFT_EYE_PUPIL = 29,

  /**
   * Right eye pupil.
   *
   * @generated from enum value: RIGHT_EYE_PUPIL = 30;
   */
  RIGHT_EYE_PUPIL = 30,

  /**
   * Forehead glabella.
   *
   * @generated from enum value: FOREHEAD_GLABELLA = 31;
   */
  FOREHEAD_GLABELLA = 31,

  /**
   * Chin gnathion.
   *
   * @generated from enum value: CHIN_GNATHION = 32;
   */
  CHIN_GNATHION = 32,

  /**
   * Chin left gonion.
   *
   * @generated from enum value: CHIN_LEFT_GONION = 33;
   */
  CHIN_LEFT_GONION = 33,

  /**
   * Chin right gonion.
   *
   * @generated from enum value: CHIN_RIGHT_GONION = 34;
   */
  CHIN_RIGHT_GONION = 34,

  /**
   * Left cheek center.
   *
   * @generated from enum value: LEFT_CHEEK_CENTER = 35;
   */
  LEFT_CHEEK_CENTER = 35,

  /**
   * Right cheek center.
   *
   * @generated from enum value: RIGHT_CHEEK_CENTER = 36;
   */
  RIGHT_CHEEK_CENTER = 36,
}

/**
 * Describes the enum google.cloud.vision.v1.FaceAnnotation.Landmark.Type.
 */
export const FaceAnnotation_Landmark_TypeSchema: GenEnum<FaceAnnotation_Landmark_Type> = /*@__PURE__*/
  enumDesc(file_google_cloud_vision_v1_image_annotator, 3, 0, 0);

/**
 * Detected entity location information.
 *
 * @generated from message google.cloud.vision.v1.LocationInfo
 */
export type LocationInfo = Message<"google.cloud.vision.v1.LocationInfo"> & {
  /**
   * lat/long location coordinates.
   *
   * @generated from field: google.type.LatLng lat_lng = 1;
   */
  latLng?: LatLng;
};

/**
 * Describes the message google.cloud.vision.v1.LocationInfo.
 * Use `create(LocationInfoSchema)` to create a new message.
 */
export const LocationInfoSchema: GenMessage<LocationInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 4);

/**
 * A `Property` consists of a user-supplied name/value pair.
 *
 * @generated from message google.cloud.vision.v1.Property
 */
export type Property = Message<"google.cloud.vision.v1.Property"> & {
  /**
   * Name of the property.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Value of the property.
   *
   * @generated from field: string value = 2;
   */
  value: string;

  /**
   * Value of numeric properties.
   *
   * @generated from field: uint64 uint64_value = 3;
   */
  uint64Value: bigint;
};

/**
 * Describes the message google.cloud.vision.v1.Property.
 * Use `create(PropertySchema)` to create a new message.
 */
export const PropertySchema: GenMessage<Property> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 5);

/**
 * Set of detected entity features.
 *
 * @generated from message google.cloud.vision.v1.EntityAnnotation
 */
export type EntityAnnotation = Message<"google.cloud.vision.v1.EntityAnnotation"> & {
  /**
   * Opaque entity ID. Some IDs may be available in
   * [Google Knowledge Graph Search
   * API](https://developers.google.com/knowledge-graph/).
   *
   * @generated from field: string mid = 1;
   */
  mid: string;

  /**
   * The language code for the locale in which the entity textual
   * `description` is expressed.
   *
   * @generated from field: string locale = 2;
   */
  locale: string;

  /**
   * Entity textual description, expressed in its `locale` language.
   *
   * @generated from field: string description = 3;
   */
  description: string;

  /**
   * Overall score of the result. Range [0, 1].
   *
   * @generated from field: float score = 4;
   */
  score: number;

  /**
   * **Deprecated. Use `score` instead.**
   * The accuracy of the entity detection in an image.
   * For example, for an image in which the "Eiffel Tower" entity is detected,
   * this field represents the confidence that there is a tower in the query
   * image. Range [0, 1].
   *
   * @generated from field: float confidence = 5 [deprecated = true];
   * @deprecated
   */
  confidence: number;

  /**
   * The relevancy of the ICA (Image Content Annotation) label to the
   * image. For example, the relevancy of "tower" is likely higher to an image
   * containing the detected "Eiffel Tower" than to an image containing a
   * detected distant towering building, even though the confidence that
   * there is a tower in each image may be the same. Range [0, 1].
   *
   * @generated from field: float topicality = 6;
   */
  topicality: number;

  /**
   * Image region to which this entity belongs. Not produced
   * for `LABEL_DETECTION` features.
   *
   * @generated from field: google.cloud.vision.v1.BoundingPoly bounding_poly = 7;
   */
  boundingPoly?: BoundingPoly;

  /**
   * The location information for the detected entity. Multiple
   * `LocationInfo` elements can be present because one location may
   * indicate the location of the scene in the image, and another location
   * may indicate the location of the place where the image was taken.
   * Location information is usually present for landmarks.
   *
   * @generated from field: repeated google.cloud.vision.v1.LocationInfo locations = 8;
   */
  locations: LocationInfo[];

  /**
   * Some entities may have optional user-supplied `Property` (name/value)
   * fields, such a score or string that qualifies the entity.
   *
   * @generated from field: repeated google.cloud.vision.v1.Property properties = 9;
   */
  properties: Property[];
};

/**
 * Describes the message google.cloud.vision.v1.EntityAnnotation.
 * Use `create(EntityAnnotationSchema)` to create a new message.
 */
export const EntityAnnotationSchema: GenMessage<EntityAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 6);

/**
 * Set of detected objects with bounding boxes.
 *
 * @generated from message google.cloud.vision.v1.LocalizedObjectAnnotation
 */
export type LocalizedObjectAnnotation = Message<"google.cloud.vision.v1.LocalizedObjectAnnotation"> & {
  /**
   * Object ID that should align with EntityAnnotation mid.
   *
   * @generated from field: string mid = 1;
   */
  mid: string;

  /**
   * The BCP-47 language code, such as "en-US" or "sr-Latn". For more
   * information, see
   * http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
   *
   * @generated from field: string language_code = 2;
   */
  languageCode: string;

  /**
   * Object name, expressed in its `language_code` language.
   *
   * @generated from field: string name = 3;
   */
  name: string;

  /**
   * Score of the result. Range [0, 1].
   *
   * @generated from field: float score = 4;
   */
  score: number;

  /**
   * Image region to which this object belongs. This must be populated.
   *
   * @generated from field: google.cloud.vision.v1.BoundingPoly bounding_poly = 5;
   */
  boundingPoly?: BoundingPoly;
};

/**
 * Describes the message google.cloud.vision.v1.LocalizedObjectAnnotation.
 * Use `create(LocalizedObjectAnnotationSchema)` to create a new message.
 */
export const LocalizedObjectAnnotationSchema: GenMessage<LocalizedObjectAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 7);

/**
 * Set of features pertaining to the image, computed by computer vision
 * methods over safe-search verticals (for example, adult, spoof, medical,
 * violence).
 *
 * @generated from message google.cloud.vision.v1.SafeSearchAnnotation
 */
export type SafeSearchAnnotation = Message<"google.cloud.vision.v1.SafeSearchAnnotation"> & {
  /**
   * Represents the adult content likelihood for the image. Adult content may
   * contain elements such as nudity, pornographic images or cartoons, or
   * sexual activities.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood adult = 1;
   */
  adult: Likelihood;

  /**
   * Spoof likelihood. The likelihood that an modification
   * was made to the image's canonical version to make it appear
   * funny or offensive.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood spoof = 2;
   */
  spoof: Likelihood;

  /**
   * Likelihood that this is a medical image.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood medical = 3;
   */
  medical: Likelihood;

  /**
   * Likelihood that this image contains violent content. Violent content may
   * include death, serious harm, or injury to individuals or groups of
   * individuals.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood violence = 4;
   */
  violence: Likelihood;

  /**
   * Likelihood that the request image contains racy content. Racy content may
   * include (but is not limited to) skimpy or sheer clothing, strategically
   * covered nudity, lewd or provocative poses, or close-ups of sensitive
   * body areas.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood racy = 9;
   */
  racy: Likelihood;
};

/**
 * Describes the message google.cloud.vision.v1.SafeSearchAnnotation.
 * Use `create(SafeSearchAnnotationSchema)` to create a new message.
 */
export const SafeSearchAnnotationSchema: GenMessage<SafeSearchAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 8);

/**
 * Rectangle determined by min and max `LatLng` pairs.
 *
 * @generated from message google.cloud.vision.v1.LatLongRect
 */
export type LatLongRect = Message<"google.cloud.vision.v1.LatLongRect"> & {
  /**
   * Min lat/long pair.
   *
   * @generated from field: google.type.LatLng min_lat_lng = 1;
   */
  minLatLng?: LatLng;

  /**
   * Max lat/long pair.
   *
   * @generated from field: google.type.LatLng max_lat_lng = 2;
   */
  maxLatLng?: LatLng;
};

/**
 * Describes the message google.cloud.vision.v1.LatLongRect.
 * Use `create(LatLongRectSchema)` to create a new message.
 */
export const LatLongRectSchema: GenMessage<LatLongRect> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 9);

/**
 * Color information consists of RGB channels, score, and the fraction of
 * the image that the color occupies in the image.
 *
 * @generated from message google.cloud.vision.v1.ColorInfo
 */
export type ColorInfo = Message<"google.cloud.vision.v1.ColorInfo"> & {
  /**
   * RGB components of the color.
   *
   * @generated from field: google.type.Color color = 1;
   */
  color?: Color;

  /**
   * Image-specific score for this color. Value in range [0, 1].
   *
   * @generated from field: float score = 2;
   */
  score: number;

  /**
   * The fraction of pixels the color occupies in the image.
   * Value in range [0, 1].
   *
   * @generated from field: float pixel_fraction = 3;
   */
  pixelFraction: number;
};

/**
 * Describes the message google.cloud.vision.v1.ColorInfo.
 * Use `create(ColorInfoSchema)` to create a new message.
 */
export const ColorInfoSchema: GenMessage<ColorInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 10);

/**
 * Set of dominant colors and their corresponding scores.
 *
 * @generated from message google.cloud.vision.v1.DominantColorsAnnotation
 */
export type DominantColorsAnnotation = Message<"google.cloud.vision.v1.DominantColorsAnnotation"> & {
  /**
   * RGB color values with their score and pixel fraction.
   *
   * @generated from field: repeated google.cloud.vision.v1.ColorInfo colors = 1;
   */
  colors: ColorInfo[];
};

/**
 * Describes the message google.cloud.vision.v1.DominantColorsAnnotation.
 * Use `create(DominantColorsAnnotationSchema)` to create a new message.
 */
export const DominantColorsAnnotationSchema: GenMessage<DominantColorsAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 11);

/**
 * Stores image properties, such as dominant colors.
 *
 * @generated from message google.cloud.vision.v1.ImageProperties
 */
export type ImageProperties = Message<"google.cloud.vision.v1.ImageProperties"> & {
  /**
   * If present, dominant colors completed successfully.
   *
   * @generated from field: google.cloud.vision.v1.DominantColorsAnnotation dominant_colors = 1;
   */
  dominantColors?: DominantColorsAnnotation;
};

/**
 * Describes the message google.cloud.vision.v1.ImageProperties.
 * Use `create(ImagePropertiesSchema)` to create a new message.
 */
export const ImagePropertiesSchema: GenMessage<ImageProperties> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 12);

/**
 * Single crop hint that is used to generate a new crop when serving an image.
 *
 * @generated from message google.cloud.vision.v1.CropHint
 */
export type CropHint = Message<"google.cloud.vision.v1.CropHint"> & {
  /**
   * The bounding polygon for the crop region. The coordinates of the bounding
   * box are in the original image's scale.
   *
   * @generated from field: google.cloud.vision.v1.BoundingPoly bounding_poly = 1;
   */
  boundingPoly?: BoundingPoly;

  /**
   * Confidence of this being a salient region.  Range [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;

  /**
   * Fraction of importance of this salient region with respect to the original
   * image.
   *
   * @generated from field: float importance_fraction = 3;
   */
  importanceFraction: number;
};

/**
 * Describes the message google.cloud.vision.v1.CropHint.
 * Use `create(CropHintSchema)` to create a new message.
 */
export const CropHintSchema: GenMessage<CropHint> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 13);

/**
 * Set of crop hints that are used to generate new crops when serving images.
 *
 * @generated from message google.cloud.vision.v1.CropHintsAnnotation
 */
export type CropHintsAnnotation = Message<"google.cloud.vision.v1.CropHintsAnnotation"> & {
  /**
   * Crop hint results.
   *
   * @generated from field: repeated google.cloud.vision.v1.CropHint crop_hints = 1;
   */
  cropHints: CropHint[];
};

/**
 * Describes the message google.cloud.vision.v1.CropHintsAnnotation.
 * Use `create(CropHintsAnnotationSchema)` to create a new message.
 */
export const CropHintsAnnotationSchema: GenMessage<CropHintsAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 14);

/**
 * Parameters for crop hints annotation request.
 *
 * @generated from message google.cloud.vision.v1.CropHintsParams
 */
export type CropHintsParams = Message<"google.cloud.vision.v1.CropHintsParams"> & {
  /**
   * Aspect ratios in floats, representing the ratio of the width to the height
   * of the image. For example, if the desired aspect ratio is 4/3, the
   * corresponding float value should be 1.33333.  If not specified, the
   * best possible crop is returned. The number of provided aspect ratios is
   * limited to a maximum of 16; any aspect ratios provided after the 16th are
   * ignored.
   *
   * @generated from field: repeated float aspect_ratios = 1;
   */
  aspectRatios: number[];
};

/**
 * Describes the message google.cloud.vision.v1.CropHintsParams.
 * Use `create(CropHintsParamsSchema)` to create a new message.
 */
export const CropHintsParamsSchema: GenMessage<CropHintsParams> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 15);

/**
 * Parameters for web detection request.
 *
 * @generated from message google.cloud.vision.v1.WebDetectionParams
 */
export type WebDetectionParams = Message<"google.cloud.vision.v1.WebDetectionParams"> & {
  /**
   * This field has no effect on results.
   *
   * @generated from field: bool include_geo_results = 2 [deprecated = true];
   * @deprecated
   */
  includeGeoResults: boolean;
};

/**
 * Describes the message google.cloud.vision.v1.WebDetectionParams.
 * Use `create(WebDetectionParamsSchema)` to create a new message.
 */
export const WebDetectionParamsSchema: GenMessage<WebDetectionParams> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 16);

/**
 * Parameters for text detections. This is used to control TEXT_DETECTION and
 * DOCUMENT_TEXT_DETECTION features.
 *
 * @generated from message google.cloud.vision.v1.TextDetectionParams
 */
export type TextDetectionParams = Message<"google.cloud.vision.v1.TextDetectionParams"> & {
  /**
   * By default, Cloud Vision API only includes confidence score for
   * DOCUMENT_TEXT_DETECTION result. Set the flag to true to include confidence
   * score for TEXT_DETECTION as well.
   *
   * @generated from field: bool enable_text_detection_confidence_score = 9;
   */
  enableTextDetectionConfidenceScore: boolean;

  /**
   * A list of advanced OCR options to further fine-tune OCR behavior.
   * Current valid values are:
   *
   * - `legacy_layout`: a heuristics layout detection algorithm, which serves as
   * an alternative to the current ML-based layout detection algorithm.
   * Customers can choose the best suitable layout algorithm based on their
   * situation.
   *
   * @generated from field: repeated string advanced_ocr_options = 11;
   */
  advancedOcrOptions: string[];
};

/**
 * Describes the message google.cloud.vision.v1.TextDetectionParams.
 * Use `create(TextDetectionParamsSchema)` to create a new message.
 */
export const TextDetectionParamsSchema: GenMessage<TextDetectionParams> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 17);

/**
 * Image context and/or feature-specific parameters.
 *
 * @generated from message google.cloud.vision.v1.ImageContext
 */
export type ImageContext = Message<"google.cloud.vision.v1.ImageContext"> & {
  /**
   * Not used.
   *
   * @generated from field: google.cloud.vision.v1.LatLongRect lat_long_rect = 1;
   */
  latLongRect?: LatLongRect;

  /**
   * List of languages to use for TEXT_DETECTION. In most cases, an empty value
   * yields the best results since it enables automatic language detection. For
   * languages based on the Latin alphabet, setting `language_hints` is not
   * needed. In rare cases, when the language of the text in the image is known,
   * setting a hint will help get better results (although it will be a
   * significant hindrance if the hint is wrong). Text detection returns an
   * error if one or more of the specified languages is not one of the
   * [supported languages](https://cloud.google.com/vision/docs/languages).
   *
   * @generated from field: repeated string language_hints = 2;
   */
  languageHints: string[];

  /**
   * Parameters for crop hints annotation request.
   *
   * @generated from field: google.cloud.vision.v1.CropHintsParams crop_hints_params = 4;
   */
  cropHintsParams?: CropHintsParams;

  /**
   * Parameters for product search.
   *
   * @generated from field: google.cloud.vision.v1.ProductSearchParams product_search_params = 5;
   */
  productSearchParams?: ProductSearchParams;

  /**
   * Parameters for web detection.
   *
   * @generated from field: google.cloud.vision.v1.WebDetectionParams web_detection_params = 6;
   */
  webDetectionParams?: WebDetectionParams;

  /**
   * Parameters for text detection and document text detection.
   *
   * @generated from field: google.cloud.vision.v1.TextDetectionParams text_detection_params = 12;
   */
  textDetectionParams?: TextDetectionParams;
};

/**
 * Describes the message google.cloud.vision.v1.ImageContext.
 * Use `create(ImageContextSchema)` to create a new message.
 */
export const ImageContextSchema: GenMessage<ImageContext> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 18);

/**
 * Request for performing Google Cloud Vision API tasks over a user-provided
 * image, with user-requested features, and with context information.
 *
 * @generated from message google.cloud.vision.v1.AnnotateImageRequest
 */
export type AnnotateImageRequest = Message<"google.cloud.vision.v1.AnnotateImageRequest"> & {
  /**
   * The image to be processed.
   *
   * @generated from field: google.cloud.vision.v1.Image image = 1;
   */
  image?: Image;

  /**
   * Requested features.
   *
   * @generated from field: repeated google.cloud.vision.v1.Feature features = 2;
   */
  features: Feature[];

  /**
   * Additional context that may accompany the image.
   *
   * @generated from field: google.cloud.vision.v1.ImageContext image_context = 3;
   */
  imageContext?: ImageContext;
};

/**
 * Describes the message google.cloud.vision.v1.AnnotateImageRequest.
 * Use `create(AnnotateImageRequestSchema)` to create a new message.
 */
export const AnnotateImageRequestSchema: GenMessage<AnnotateImageRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 19);

/**
 * If an image was produced from a file (e.g. a PDF), this message gives
 * information about the source of that image.
 *
 * @generated from message google.cloud.vision.v1.ImageAnnotationContext
 */
export type ImageAnnotationContext = Message<"google.cloud.vision.v1.ImageAnnotationContext"> & {
  /**
   * The URI of the file used to produce the image.
   *
   * @generated from field: string uri = 1;
   */
  uri: string;

  /**
   * If the file was a PDF or TIFF, this field gives the page number within
   * the file used to produce the image.
   *
   * @generated from field: int32 page_number = 2;
   */
  pageNumber: number;
};

/**
 * Describes the message google.cloud.vision.v1.ImageAnnotationContext.
 * Use `create(ImageAnnotationContextSchema)` to create a new message.
 */
export const ImageAnnotationContextSchema: GenMessage<ImageAnnotationContext> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 20);

/**
 * Response to an image annotation request.
 *
 * @generated from message google.cloud.vision.v1.AnnotateImageResponse
 */
export type AnnotateImageResponse = Message<"google.cloud.vision.v1.AnnotateImageResponse"> & {
  /**
   * If present, face detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1.FaceAnnotation face_annotations = 1;
   */
  faceAnnotations: FaceAnnotation[];

  /**
   * If present, landmark detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1.EntityAnnotation landmark_annotations = 2;
   */
  landmarkAnnotations: EntityAnnotation[];

  /**
   * If present, logo detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1.EntityAnnotation logo_annotations = 3;
   */
  logoAnnotations: EntityAnnotation[];

  /**
   * If present, label detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1.EntityAnnotation label_annotations = 4;
   */
  labelAnnotations: EntityAnnotation[];

  /**
   * If present, localized object detection has completed successfully.
   * This will be sorted descending by confidence score.
   *
   * @generated from field: repeated google.cloud.vision.v1.LocalizedObjectAnnotation localized_object_annotations = 22;
   */
  localizedObjectAnnotations: LocalizedObjectAnnotation[];

  /**
   * If present, text (OCR) detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1.EntityAnnotation text_annotations = 5;
   */
  textAnnotations: EntityAnnotation[];

  /**
   * If present, text (OCR) detection or document (OCR) text detection has
   * completed successfully.
   * This annotation provides the structural hierarchy for the OCR detected
   * text.
   *
   * @generated from field: google.cloud.vision.v1.TextAnnotation full_text_annotation = 12;
   */
  fullTextAnnotation?: TextAnnotation;

  /**
   * If present, safe-search annotation has completed successfully.
   *
   * @generated from field: google.cloud.vision.v1.SafeSearchAnnotation safe_search_annotation = 6;
   */
  safeSearchAnnotation?: SafeSearchAnnotation;

  /**
   * If present, image properties were extracted successfully.
   *
   * @generated from field: google.cloud.vision.v1.ImageProperties image_properties_annotation = 8;
   */
  imagePropertiesAnnotation?: ImageProperties;

  /**
   * If present, crop hints have completed successfully.
   *
   * @generated from field: google.cloud.vision.v1.CropHintsAnnotation crop_hints_annotation = 11;
   */
  cropHintsAnnotation?: CropHintsAnnotation;

  /**
   * If present, web detection has completed successfully.
   *
   * @generated from field: google.cloud.vision.v1.WebDetection web_detection = 13;
   */
  webDetection?: WebDetection;

  /**
   * If present, product search has completed successfully.
   *
   * @generated from field: google.cloud.vision.v1.ProductSearchResults product_search_results = 14;
   */
  productSearchResults?: ProductSearchResults;

  /**
   * If set, represents the error message for the operation.
   * Note that filled-in image annotations are guaranteed to be
   * correct, even when `error` is set.
   *
   * @generated from field: google.rpc.Status error = 9;
   */
  error?: Status;

  /**
   * If present, contextual information is needed to understand where this image
   * comes from.
   *
   * @generated from field: google.cloud.vision.v1.ImageAnnotationContext context = 21;
   */
  context?: ImageAnnotationContext;
};

/**
 * Describes the message google.cloud.vision.v1.AnnotateImageResponse.
 * Use `create(AnnotateImageResponseSchema)` to create a new message.
 */
export const AnnotateImageResponseSchema: GenMessage<AnnotateImageResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 21);

/**
 * Multiple image annotation requests are batched into a single service call.
 *
 * @generated from message google.cloud.vision.v1.BatchAnnotateImagesRequest
 */
export type BatchAnnotateImagesRequest = Message<"google.cloud.vision.v1.BatchAnnotateImagesRequest"> & {
  /**
   * Required. Individual image annotation requests for this batch.
   *
   * @generated from field: repeated google.cloud.vision.v1.AnnotateImageRequest requests = 1;
   */
  requests: AnnotateImageRequest[];

  /**
   * Optional. Target project and location to make a call.
   *
   * Format: `projects/{project-id}/locations/{location-id}`.
   *
   * If no parent is specified, a region will be chosen automatically.
   *
   * Supported location-ids:
   *     `us`: USA country only,
   *     `asia`: East asia areas, like Japan, Taiwan,
   *     `eu`: The European Union.
   *
   * Example: `projects/project-A/locations/eu`.
   *
   * @generated from field: string parent = 4;
   */
  parent: string;

  /**
   * Optional. The labels with user-defined metadata for the request.
   *
   * Label keys and values can be no longer than 63 characters
   * (Unicode codepoints), can only contain lowercase letters, numeric
   * characters, underscores and dashes. International characters are allowed.
   * Label values are optional. Label keys must start with a letter.
   *
   * @generated from field: map<string, string> labels = 5;
   */
  labels: { [key: string]: string };
};

/**
 * Describes the message google.cloud.vision.v1.BatchAnnotateImagesRequest.
 * Use `create(BatchAnnotateImagesRequestSchema)` to create a new message.
 */
export const BatchAnnotateImagesRequestSchema: GenMessage<BatchAnnotateImagesRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 22);

/**
 * Response to a batch image annotation request.
 *
 * @generated from message google.cloud.vision.v1.BatchAnnotateImagesResponse
 */
export type BatchAnnotateImagesResponse = Message<"google.cloud.vision.v1.BatchAnnotateImagesResponse"> & {
  /**
   * Individual responses to image annotation requests within the batch.
   *
   * @generated from field: repeated google.cloud.vision.v1.AnnotateImageResponse responses = 1;
   */
  responses: AnnotateImageResponse[];
};

/**
 * Describes the message google.cloud.vision.v1.BatchAnnotateImagesResponse.
 * Use `create(BatchAnnotateImagesResponseSchema)` to create a new message.
 */
export const BatchAnnotateImagesResponseSchema: GenMessage<BatchAnnotateImagesResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 23);

/**
 * A request to annotate one single file, e.g. a PDF, TIFF or GIF file.
 *
 * @generated from message google.cloud.vision.v1.AnnotateFileRequest
 */
export type AnnotateFileRequest = Message<"google.cloud.vision.v1.AnnotateFileRequest"> & {
  /**
   * Required. Information about the input file.
   *
   * @generated from field: google.cloud.vision.v1.InputConfig input_config = 1;
   */
  inputConfig?: InputConfig;

  /**
   * Required. Requested features.
   *
   * @generated from field: repeated google.cloud.vision.v1.Feature features = 2;
   */
  features: Feature[];

  /**
   * Additional context that may accompany the image(s) in the file.
   *
   * @generated from field: google.cloud.vision.v1.ImageContext image_context = 3;
   */
  imageContext?: ImageContext;

  /**
   * Pages of the file to perform image annotation.
   *
   * Pages starts from 1, we assume the first page of the file is page 1.
   * At most 5 pages are supported per request. Pages can be negative.
   *
   * Page 1 means the first page.
   * Page 2 means the second page.
   * Page -1 means the last page.
   * Page -2 means the second to the last page.
   *
   * If the file is GIF instead of PDF or TIFF, page refers to GIF frames.
   *
   * If this field is empty, by default the service performs image annotation
   * for the first 5 pages of the file.
   *
   * @generated from field: repeated int32 pages = 4;
   */
  pages: number[];
};

/**
 * Describes the message google.cloud.vision.v1.AnnotateFileRequest.
 * Use `create(AnnotateFileRequestSchema)` to create a new message.
 */
export const AnnotateFileRequestSchema: GenMessage<AnnotateFileRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 24);

/**
 * Response to a single file annotation request. A file may contain one or more
 * images, which individually have their own responses.
 *
 * @generated from message google.cloud.vision.v1.AnnotateFileResponse
 */
export type AnnotateFileResponse = Message<"google.cloud.vision.v1.AnnotateFileResponse"> & {
  /**
   * Information about the file for which this response is generated.
   *
   * @generated from field: google.cloud.vision.v1.InputConfig input_config = 1;
   */
  inputConfig?: InputConfig;

  /**
   * Individual responses to images found within the file. This field will be
   * empty if the `error` field is set.
   *
   * @generated from field: repeated google.cloud.vision.v1.AnnotateImageResponse responses = 2;
   */
  responses: AnnotateImageResponse[];

  /**
   * This field gives the total number of pages in the file.
   *
   * @generated from field: int32 total_pages = 3;
   */
  totalPages: number;

  /**
   * If set, represents the error message for the failed request. The
   * `responses` field will not be set in this case.
   *
   * @generated from field: google.rpc.Status error = 4;
   */
  error?: Status;
};

/**
 * Describes the message google.cloud.vision.v1.AnnotateFileResponse.
 * Use `create(AnnotateFileResponseSchema)` to create a new message.
 */
export const AnnotateFileResponseSchema: GenMessage<AnnotateFileResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 25);

/**
 * A list of requests to annotate files using the BatchAnnotateFiles API.
 *
 * @generated from message google.cloud.vision.v1.BatchAnnotateFilesRequest
 */
export type BatchAnnotateFilesRequest = Message<"google.cloud.vision.v1.BatchAnnotateFilesRequest"> & {
  /**
   * Required. The list of file annotation requests. Right now we support only
   * one AnnotateFileRequest in BatchAnnotateFilesRequest.
   *
   * @generated from field: repeated google.cloud.vision.v1.AnnotateFileRequest requests = 1;
   */
  requests: AnnotateFileRequest[];

  /**
   * Optional. Target project and location to make a call.
   *
   * Format: `projects/{project-id}/locations/{location-id}`.
   *
   * If no parent is specified, a region will be chosen automatically.
   *
   * Supported location-ids:
   *     `us`: USA country only,
   *     `asia`: East asia areas, like Japan, Taiwan,
   *     `eu`: The European Union.
   *
   * Example: `projects/project-A/locations/eu`.
   *
   * @generated from field: string parent = 3;
   */
  parent: string;

  /**
   * Optional. The labels with user-defined metadata for the request.
   *
   * Label keys and values can be no longer than 63 characters
   * (Unicode codepoints), can only contain lowercase letters, numeric
   * characters, underscores and dashes. International characters are allowed.
   * Label values are optional. Label keys must start with a letter.
   *
   * @generated from field: map<string, string> labels = 5;
   */
  labels: { [key: string]: string };
};

/**
 * Describes the message google.cloud.vision.v1.BatchAnnotateFilesRequest.
 * Use `create(BatchAnnotateFilesRequestSchema)` to create a new message.
 */
export const BatchAnnotateFilesRequestSchema: GenMessage<BatchAnnotateFilesRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 26);

/**
 * A list of file annotation responses.
 *
 * @generated from message google.cloud.vision.v1.BatchAnnotateFilesResponse
 */
export type BatchAnnotateFilesResponse = Message<"google.cloud.vision.v1.BatchAnnotateFilesResponse"> & {
  /**
   * The list of file annotation responses, each response corresponding to each
   * AnnotateFileRequest in BatchAnnotateFilesRequest.
   *
   * @generated from field: repeated google.cloud.vision.v1.AnnotateFileResponse responses = 1;
   */
  responses: AnnotateFileResponse[];
};

/**
 * Describes the message google.cloud.vision.v1.BatchAnnotateFilesResponse.
 * Use `create(BatchAnnotateFilesResponseSchema)` to create a new message.
 */
export const BatchAnnotateFilesResponseSchema: GenMessage<BatchAnnotateFilesResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 27);

/**
 * An offline file annotation request.
 *
 * @generated from message google.cloud.vision.v1.AsyncAnnotateFileRequest
 */
export type AsyncAnnotateFileRequest = Message<"google.cloud.vision.v1.AsyncAnnotateFileRequest"> & {
  /**
   * Required. Information about the input file.
   *
   * @generated from field: google.cloud.vision.v1.InputConfig input_config = 1;
   */
  inputConfig?: InputConfig;

  /**
   * Required. Requested features.
   *
   * @generated from field: repeated google.cloud.vision.v1.Feature features = 2;
   */
  features: Feature[];

  /**
   * Additional context that may accompany the image(s) in the file.
   *
   * @generated from field: google.cloud.vision.v1.ImageContext image_context = 3;
   */
  imageContext?: ImageContext;

  /**
   * Required. The desired output location and metadata (e.g. format).
   *
   * @generated from field: google.cloud.vision.v1.OutputConfig output_config = 4;
   */
  outputConfig?: OutputConfig;
};

/**
 * Describes the message google.cloud.vision.v1.AsyncAnnotateFileRequest.
 * Use `create(AsyncAnnotateFileRequestSchema)` to create a new message.
 */
export const AsyncAnnotateFileRequestSchema: GenMessage<AsyncAnnotateFileRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 28);

/**
 * The response for a single offline file annotation request.
 *
 * @generated from message google.cloud.vision.v1.AsyncAnnotateFileResponse
 */
export type AsyncAnnotateFileResponse = Message<"google.cloud.vision.v1.AsyncAnnotateFileResponse"> & {
  /**
   * The output location and metadata from AsyncAnnotateFileRequest.
   *
   * @generated from field: google.cloud.vision.v1.OutputConfig output_config = 1;
   */
  outputConfig?: OutputConfig;
};

/**
 * Describes the message google.cloud.vision.v1.AsyncAnnotateFileResponse.
 * Use `create(AsyncAnnotateFileResponseSchema)` to create a new message.
 */
export const AsyncAnnotateFileResponseSchema: GenMessage<AsyncAnnotateFileResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 29);

/**
 * Request for async image annotation for a list of images.
 *
 * @generated from message google.cloud.vision.v1.AsyncBatchAnnotateImagesRequest
 */
export type AsyncBatchAnnotateImagesRequest = Message<"google.cloud.vision.v1.AsyncBatchAnnotateImagesRequest"> & {
  /**
   * Required. Individual image annotation requests for this batch.
   *
   * @generated from field: repeated google.cloud.vision.v1.AnnotateImageRequest requests = 1;
   */
  requests: AnnotateImageRequest[];

  /**
   * Required. The desired output location and metadata (e.g. format).
   *
   * @generated from field: google.cloud.vision.v1.OutputConfig output_config = 2;
   */
  outputConfig?: OutputConfig;

  /**
   * Optional. Target project and location to make a call.
   *
   * Format: `projects/{project-id}/locations/{location-id}`.
   *
   * If no parent is specified, a region will be chosen automatically.
   *
   * Supported location-ids:
   *     `us`: USA country only,
   *     `asia`: East asia areas, like Japan, Taiwan,
   *     `eu`: The European Union.
   *
   * Example: `projects/project-A/locations/eu`.
   *
   * @generated from field: string parent = 4;
   */
  parent: string;

  /**
   * Optional. The labels with user-defined metadata for the request.
   *
   * Label keys and values can be no longer than 63 characters
   * (Unicode codepoints), can only contain lowercase letters, numeric
   * characters, underscores and dashes. International characters are allowed.
   * Label values are optional. Label keys must start with a letter.
   *
   * @generated from field: map<string, string> labels = 5;
   */
  labels: { [key: string]: string };
};

/**
 * Describes the message google.cloud.vision.v1.AsyncBatchAnnotateImagesRequest.
 * Use `create(AsyncBatchAnnotateImagesRequestSchema)` to create a new message.
 */
export const AsyncBatchAnnotateImagesRequestSchema: GenMessage<AsyncBatchAnnotateImagesRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 30);

/**
 * Response to an async batch image annotation request.
 *
 * @generated from message google.cloud.vision.v1.AsyncBatchAnnotateImagesResponse
 */
export type AsyncBatchAnnotateImagesResponse = Message<"google.cloud.vision.v1.AsyncBatchAnnotateImagesResponse"> & {
  /**
   * The output location and metadata from AsyncBatchAnnotateImagesRequest.
   *
   * @generated from field: google.cloud.vision.v1.OutputConfig output_config = 1;
   */
  outputConfig?: OutputConfig;
};

/**
 * Describes the message google.cloud.vision.v1.AsyncBatchAnnotateImagesResponse.
 * Use `create(AsyncBatchAnnotateImagesResponseSchema)` to create a new message.
 */
export const AsyncBatchAnnotateImagesResponseSchema: GenMessage<AsyncBatchAnnotateImagesResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 31);

/**
 * Multiple async file annotation requests are batched into a single service
 * call.
 *
 * @generated from message google.cloud.vision.v1.AsyncBatchAnnotateFilesRequest
 */
export type AsyncBatchAnnotateFilesRequest = Message<"google.cloud.vision.v1.AsyncBatchAnnotateFilesRequest"> & {
  /**
   * Required. Individual async file annotation requests for this batch.
   *
   * @generated from field: repeated google.cloud.vision.v1.AsyncAnnotateFileRequest requests = 1;
   */
  requests: AsyncAnnotateFileRequest[];

  /**
   * Optional. Target project and location to make a call.
   *
   * Format: `projects/{project-id}/locations/{location-id}`.
   *
   * If no parent is specified, a region will be chosen automatically.
   *
   * Supported location-ids:
   *     `us`: USA country only,
   *     `asia`: East asia areas, like Japan, Taiwan,
   *     `eu`: The European Union.
   *
   * Example: `projects/project-A/locations/eu`.
   *
   * @generated from field: string parent = 4;
   */
  parent: string;

  /**
   * Optional. The labels with user-defined metadata for the request.
   *
   * Label keys and values can be no longer than 63 characters
   * (Unicode codepoints), can only contain lowercase letters, numeric
   * characters, underscores and dashes. International characters are allowed.
   * Label values are optional. Label keys must start with a letter.
   *
   * @generated from field: map<string, string> labels = 5;
   */
  labels: { [key: string]: string };
};

/**
 * Describes the message google.cloud.vision.v1.AsyncBatchAnnotateFilesRequest.
 * Use `create(AsyncBatchAnnotateFilesRequestSchema)` to create a new message.
 */
export const AsyncBatchAnnotateFilesRequestSchema: GenMessage<AsyncBatchAnnotateFilesRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 32);

/**
 * Response to an async batch file annotation request.
 *
 * @generated from message google.cloud.vision.v1.AsyncBatchAnnotateFilesResponse
 */
export type AsyncBatchAnnotateFilesResponse = Message<"google.cloud.vision.v1.AsyncBatchAnnotateFilesResponse"> & {
  /**
   * The list of file annotation responses, one for each request in
   * AsyncBatchAnnotateFilesRequest.
   *
   * @generated from field: repeated google.cloud.vision.v1.AsyncAnnotateFileResponse responses = 1;
   */
  responses: AsyncAnnotateFileResponse[];
};

/**
 * Describes the message google.cloud.vision.v1.AsyncBatchAnnotateFilesResponse.
 * Use `create(AsyncBatchAnnotateFilesResponseSchema)` to create a new message.
 */
export const AsyncBatchAnnotateFilesResponseSchema: GenMessage<AsyncBatchAnnotateFilesResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 33);

/**
 * The desired input location and metadata.
 *
 * @generated from message google.cloud.vision.v1.InputConfig
 */
export type InputConfig = Message<"google.cloud.vision.v1.InputConfig"> & {
  /**
   * The Google Cloud Storage location to read the input from.
   *
   * @generated from field: google.cloud.vision.v1.GcsSource gcs_source = 1;
   */
  gcsSource?: GcsSource;

  /**
   * File content, represented as a stream of bytes.
   * Note: As with all `bytes` fields, protobuffers use a pure binary
   * representation, whereas JSON representations use base64.
   *
   * Currently, this field only works for BatchAnnotateFiles requests. It does
   * not work for AsyncBatchAnnotateFiles requests.
   *
   * @generated from field: bytes content = 3;
   */
  content: Uint8Array;

  /**
   * The type of the file. Currently only "application/pdf", "image/tiff" and
   * "image/gif" are supported. Wildcards are not supported.
   *
   * @generated from field: string mime_type = 2;
   */
  mimeType: string;
};

/**
 * Describes the message google.cloud.vision.v1.InputConfig.
 * Use `create(InputConfigSchema)` to create a new message.
 */
export const InputConfigSchema: GenMessage<InputConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 34);

/**
 * The desired output location and metadata.
 *
 * @generated from message google.cloud.vision.v1.OutputConfig
 */
export type OutputConfig = Message<"google.cloud.vision.v1.OutputConfig"> & {
  /**
   * The Google Cloud Storage location to write the output(s) to.
   *
   * @generated from field: google.cloud.vision.v1.GcsDestination gcs_destination = 1;
   */
  gcsDestination?: GcsDestination;

  /**
   * The max number of response protos to put into each output JSON file on
   * Google Cloud Storage.
   * The valid range is [1, 100]. If not specified, the default value is 20.
   *
   * For example, for one pdf file with 100 pages, 100 response protos will
   * be generated. If `batch_size` = 20, then 5 json files each
   * containing 20 response protos will be written under the prefix
   * `gcs_destination`.`uri`.
   *
   * Currently, batch_size only applies to GcsDestination, with potential future
   * support for other output configurations.
   *
   * @generated from field: int32 batch_size = 2;
   */
  batchSize: number;
};

/**
 * Describes the message google.cloud.vision.v1.OutputConfig.
 * Use `create(OutputConfigSchema)` to create a new message.
 */
export const OutputConfigSchema: GenMessage<OutputConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 35);

/**
 * The Google Cloud Storage location where the input will be read from.
 *
 * @generated from message google.cloud.vision.v1.GcsSource
 */
export type GcsSource = Message<"google.cloud.vision.v1.GcsSource"> & {
  /**
   * Google Cloud Storage URI for the input file. This must only be a
   * Google Cloud Storage object. Wildcards are not currently supported.
   *
   * @generated from field: string uri = 1;
   */
  uri: string;
};

/**
 * Describes the message google.cloud.vision.v1.GcsSource.
 * Use `create(GcsSourceSchema)` to create a new message.
 */
export const GcsSourceSchema: GenMessage<GcsSource> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 36);

/**
 * The Google Cloud Storage location where the output will be written to.
 *
 * @generated from message google.cloud.vision.v1.GcsDestination
 */
export type GcsDestination = Message<"google.cloud.vision.v1.GcsDestination"> & {
  /**
   * Google Cloud Storage URI prefix where the results will be stored. Results
   * will be in JSON format and preceded by its corresponding input URI prefix.
   * This field can either represent a gcs file prefix or gcs directory. In
   * either case, the uri should be unique because in order to get all of the
   * output files, you will need to do a wildcard gcs search on the uri prefix
   * you provide.
   *
   * Examples:
   *
   * *    File Prefix: gs://bucket-name/here/filenameprefix   The output files
   * will be created in gs://bucket-name/here/ and the names of the
   * output files will begin with "filenameprefix".
   *
   * *    Directory Prefix: gs://bucket-name/some/location/   The output files
   * will be created in gs://bucket-name/some/location/ and the names of the
   * output files could be anything because there was no filename prefix
   * specified.
   *
   * If multiple outputs, each response is still AnnotateFileResponse, each of
   * which contains some subset of the full list of AnnotateImageResponse.
   * Multiple outputs can happen if, for example, the output JSON is too large
   * and overflows into multiple sharded files.
   *
   * @generated from field: string uri = 1;
   */
  uri: string;
};

/**
 * Describes the message google.cloud.vision.v1.GcsDestination.
 * Use `create(GcsDestinationSchema)` to create a new message.
 */
export const GcsDestinationSchema: GenMessage<GcsDestination> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 37);

/**
 * Contains metadata for the BatchAnnotateImages operation.
 *
 * @generated from message google.cloud.vision.v1.OperationMetadata
 */
export type OperationMetadata = Message<"google.cloud.vision.v1.OperationMetadata"> & {
  /**
   * Current state of the batch operation.
   *
   * @generated from field: google.cloud.vision.v1.OperationMetadata.State state = 1;
   */
  state: OperationMetadata_State;

  /**
   * The time when the batch request was received.
   *
   * @generated from field: google.protobuf.Timestamp create_time = 5;
   */
  createTime?: Timestamp;

  /**
   * The time when the operation result was last updated.
   *
   * @generated from field: google.protobuf.Timestamp update_time = 6;
   */
  updateTime?: Timestamp;
};

/**
 * Describes the message google.cloud.vision.v1.OperationMetadata.
 * Use `create(OperationMetadataSchema)` to create a new message.
 */
export const OperationMetadataSchema: GenMessage<OperationMetadata> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1_image_annotator, 38);

/**
 * Batch operation states.
 *
 * @generated from enum google.cloud.vision.v1.OperationMetadata.State
 */
export enum OperationMetadata_State {
  /**
   * Invalid.
   *
   * @generated from enum value: STATE_UNSPECIFIED = 0;
   */
  STATE_UNSPECIFIED = 0,

  /**
   * Request is received.
   *
   * @generated from enum value: CREATED = 1;
   */
  CREATED = 1,

  /**
   * Request is actively being processed.
   *
   * @generated from enum value: RUNNING = 2;
   */
  RUNNING = 2,

  /**
   * The batch processing is done.
   *
   * @generated from enum value: DONE = 3;
   */
  DONE = 3,

  /**
   * The batch processing was cancelled.
   *
   * @generated from enum value: CANCELLED = 4;
   */
  CANCELLED = 4,
}

/**
 * Describes the enum google.cloud.vision.v1.OperationMetadata.State.
 */
export const OperationMetadata_StateSchema: GenEnum<OperationMetadata_State> = /*@__PURE__*/
  enumDesc(file_google_cloud_vision_v1_image_annotator, 38, 0);

/**
 * A bucketized representation of likelihood, which is intended to give clients
 * highly stable results across model upgrades.
 *
 * @generated from enum google.cloud.vision.v1.Likelihood
 */
export enum Likelihood {
  /**
   * Unknown likelihood.
   *
   * @generated from enum value: UNKNOWN = 0;
   */
  UNKNOWN = 0,

  /**
   * It is very unlikely.
   *
   * @generated from enum value: VERY_UNLIKELY = 1;
   */
  VERY_UNLIKELY = 1,

  /**
   * It is unlikely.
   *
   * @generated from enum value: UNLIKELY = 2;
   */
  UNLIKELY = 2,

  /**
   * It is possible.
   *
   * @generated from enum value: POSSIBLE = 3;
   */
  POSSIBLE = 3,

  /**
   * It is likely.
   *
   * @generated from enum value: LIKELY = 4;
   */
  LIKELY = 4,

  /**
   * It is very likely.
   *
   * @generated from enum value: VERY_LIKELY = 5;
   */
  VERY_LIKELY = 5,
}

/**
 * Describes the enum google.cloud.vision.v1.Likelihood.
 */
export const LikelihoodSchema: GenEnum<Likelihood> = /*@__PURE__*/
  enumDesc(file_google_cloud_vision_v1_image_annotator, 0);

/**
 * Service that performs Google Cloud Vision API detection tasks over client
 * images, such as face, landmark, logo, label, and text detection. The
 * ImageAnnotator service returns detected entities from the images.
 *
 * @generated from service google.cloud.vision.v1.ImageAnnotator
 */
export const ImageAnnotator: GenService<{
  /**
   * Run image detection and annotation for a batch of images.
   *
   * @generated from rpc google.cloud.vision.v1.ImageAnnotator.BatchAnnotateImages
   */
  batchAnnotateImages: {
    methodKind: "unary";
    input: typeof BatchAnnotateImagesRequestSchema;
    output: typeof BatchAnnotateImagesResponseSchema;
  },
  /**
   * Service that performs image detection and annotation for a batch of files.
   * Now only "application/pdf", "image/tiff" and "image/gif" are supported.
   *
   * This service will extract at most 5 (customers can specify which 5 in
   * AnnotateFileRequest.pages) frames (gif) or pages (pdf or tiff) from each
   * file provided and perform detection and annotation for each image
   * extracted.
   *
   * @generated from rpc google.cloud.vision.v1.ImageAnnotator.BatchAnnotateFiles
   */
  batchAnnotateFiles: {
    methodKind: "unary";
    input: typeof BatchAnnotateFilesRequestSchema;
    output: typeof BatchAnnotateFilesResponseSchema;
  },
  /**
   * Run asynchronous image detection and annotation for a list of images.
   *
   * Progress and results can be retrieved through the
   * `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `OperationMetadata` (metadata).
   * `Operation.response` contains `AsyncBatchAnnotateImagesResponse` (results).
   *
   * This service will write image annotation outputs to json files in customer
   * GCS bucket, each json file containing BatchAnnotateImagesResponse proto.
   *
   * @generated from rpc google.cloud.vision.v1.ImageAnnotator.AsyncBatchAnnotateImages
   */
  asyncBatchAnnotateImages: {
    methodKind: "unary";
    input: typeof AsyncBatchAnnotateImagesRequestSchema;
    output: typeof OperationSchema;
  },
  /**
   * Run asynchronous image detection and annotation for a list of generic
   * files, such as PDF files, which may contain multiple pages and multiple
   * images per page. Progress and results can be retrieved through the
   * `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `OperationMetadata` (metadata).
   * `Operation.response` contains `AsyncBatchAnnotateFilesResponse` (results).
   *
   * @generated from rpc google.cloud.vision.v1.ImageAnnotator.AsyncBatchAnnotateFiles
   */
  asyncBatchAnnotateFiles: {
    methodKind: "unary";
    input: typeof AsyncBatchAnnotateFilesRequestSchema;
    output: typeof OperationSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_vision_v1_image_annotator, 0);

