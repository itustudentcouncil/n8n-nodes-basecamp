// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/vision/v1p4beta1/image_annotator.proto (package google.cloud.vision.v1p4beta1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import type { FaceRecognitionParams, FaceRecognitionResult } from "./face_pb";
import { file_google_cloud_vision_v1p4beta1_face } from "./face_pb";
import type { BoundingPoly, Position } from "./geometry_pb";
import { file_google_cloud_vision_v1p4beta1_geometry } from "./geometry_pb";
import type { ProductSearchParams, ProductSearchResults } from "./product_search_pb";
import { file_google_cloud_vision_v1p4beta1_product_search } from "./product_search_pb";
import type { TextAnnotation } from "./text_annotation_pb";
import { file_google_cloud_vision_v1p4beta1_text_annotation } from "./text_annotation_pb";
import type { WebDetection } from "./web_detection_pb";
import { file_google_cloud_vision_v1p4beta1_web_detection } from "./web_detection_pb";
import type { OperationSchema } from "../../../longrunning/operations_pb";
import { file_google_longrunning_operations } from "../../../longrunning/operations_pb";
import type { Timestamp } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_timestamp } from "@bufbuild/protobuf/wkt";
import type { Status } from "../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../rpc/status_pb";
import type { Color } from "../../../type/color_pb";
import { file_google_type_color } from "../../../type/color_pb";
import type { LatLng } from "../../../type/latlng_pb";
import { file_google_type_latlng } from "../../../type/latlng_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/vision/v1p4beta1/image_annotator.proto.
 */
export const file_google_cloud_vision_v1p4beta1_image_annotator: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_cloud_vision_v1p4beta1_face, file_google_cloud_vision_v1p4beta1_geometry, file_google_cloud_vision_v1p4beta1_product_search, file_google_cloud_vision_v1p4beta1_text_annotation, file_google_cloud_vision_v1p4beta1_web_detection, file_google_longrunning_operations, file_google_protobuf_timestamp, file_google_rpc_status, file_google_type_color, file_google_type_latlng]);

/**
 * The type of Google Cloud Vision API detection to perform, and the maximum
 * number of results to return for that type. Multiple `Feature` objects can
 * be specified in the `features` list.
 *
 * @generated from message google.cloud.vision.v1p4beta1.Feature
 */
export type Feature = Message<"google.cloud.vision.v1p4beta1.Feature"> & {
  /**
   * The feature type.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.Feature.Type type = 1;
   */
  type: Feature_Type;

  /**
   * Maximum number of results of this type. Does not apply to
   * `TEXT_DETECTION`, `DOCUMENT_TEXT_DETECTION`, or `CROP_HINTS`.
   *
   * @generated from field: int32 max_results = 2;
   */
  maxResults: number;

  /**
   * Model to use for the feature.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest". `DOCUMENT_TEXT_DETECTION` and `TEXT_DETECTION` also
   * support "builtin/weekly" for the bleeding edge release updated weekly.
   *
   * @generated from field: string model = 3;
   */
  model: string;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.Feature.
 * Use `create(FeatureSchema)` to create a new message.
 */
export const FeatureSchema: GenMessage<Feature> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 0);

/**
 * Type of Google Cloud Vision API feature to be extracted.
 *
 * @generated from enum google.cloud.vision.v1p4beta1.Feature.Type
 */
export enum Feature_Type {
  /**
   * Unspecified feature type.
   *
   * @generated from enum value: TYPE_UNSPECIFIED = 0;
   */
  TYPE_UNSPECIFIED = 0,

  /**
   * Run face detection.
   *
   * @generated from enum value: FACE_DETECTION = 1;
   */
  FACE_DETECTION = 1,

  /**
   * Run landmark detection.
   *
   * @generated from enum value: LANDMARK_DETECTION = 2;
   */
  LANDMARK_DETECTION = 2,

  /**
   * Run logo detection.
   *
   * @generated from enum value: LOGO_DETECTION = 3;
   */
  LOGO_DETECTION = 3,

  /**
   * Run label detection.
   *
   * @generated from enum value: LABEL_DETECTION = 4;
   */
  LABEL_DETECTION = 4,

  /**
   * Run text detection / optical character recognition (OCR). Text detection
   * is optimized for areas of text within a larger image; if the image is
   * a document, use `DOCUMENT_TEXT_DETECTION` instead.
   *
   * @generated from enum value: TEXT_DETECTION = 5;
   */
  TEXT_DETECTION = 5,

  /**
   * Run dense text document OCR. Takes precedence when both
   * `DOCUMENT_TEXT_DETECTION` and `TEXT_DETECTION` are present.
   *
   * @generated from enum value: DOCUMENT_TEXT_DETECTION = 11;
   */
  DOCUMENT_TEXT_DETECTION = 11,

  /**
   * Run Safe Search to detect potentially unsafe
   * or undesirable content.
   *
   * @generated from enum value: SAFE_SEARCH_DETECTION = 6;
   */
  SAFE_SEARCH_DETECTION = 6,

  /**
   * Compute a set of image properties, such as the
   * image's dominant colors.
   *
   * @generated from enum value: IMAGE_PROPERTIES = 7;
   */
  IMAGE_PROPERTIES = 7,

  /**
   * Run crop hints.
   *
   * @generated from enum value: CROP_HINTS = 9;
   */
  CROP_HINTS = 9,

  /**
   * Run web detection.
   *
   * @generated from enum value: WEB_DETECTION = 10;
   */
  WEB_DETECTION = 10,

  /**
   * Run Product Search.
   *
   * @generated from enum value: PRODUCT_SEARCH = 12;
   */
  PRODUCT_SEARCH = 12,

  /**
   * Run localizer for object detection.
   *
   * @generated from enum value: OBJECT_LOCALIZATION = 19;
   */
  OBJECT_LOCALIZATION = 19,
}

/**
 * Describes the enum google.cloud.vision.v1p4beta1.Feature.Type.
 */
export const Feature_TypeSchema: GenEnum<Feature_Type> = /*@__PURE__*/
  enumDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 0, 0);

/**
 * External image source (Google Cloud Storage or web URL image location).
 *
 * @generated from message google.cloud.vision.v1p4beta1.ImageSource
 */
export type ImageSource = Message<"google.cloud.vision.v1p4beta1.ImageSource"> & {
  /**
   * **Use `image_uri` instead.**
   *
   * The Google Cloud Storage  URI of the form
   * `gs://bucket_name/object_name`. Object versioning is not supported. See
   * [Google Cloud Storage Request
   * URIs](https://cloud.google.com/storage/docs/reference-uris) for more info.
   *
   * @generated from field: string gcs_image_uri = 1;
   */
  gcsImageUri: string;

  /**
   * The URI of the source image. Can be either:
   *
   * 1. A Google Cloud Storage URI of the form
   *    `gs://bucket_name/object_name`. Object versioning is not supported. See
   *    [Google Cloud Storage Request
   *    URIs](https://cloud.google.com/storage/docs/reference-uris) for more
   *    info.
   *
   * 2. A publicly-accessible image HTTP/HTTPS URL. When fetching images from
   *    HTTP/HTTPS URLs, Google cannot guarantee that the request will be
   *    completed. Your request may fail if the specified host denies the
   *    request (e.g. due to request throttling or DOS prevention), or if Google
   *    throttles requests to the site for abuse prevention. You should not
   *    depend on externally-hosted images for production applications.
   *
   * When both `gcs_image_uri` and `image_uri` are specified, `image_uri` takes
   * precedence.
   *
   * @generated from field: string image_uri = 2;
   */
  imageUri: string;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.ImageSource.
 * Use `create(ImageSourceSchema)` to create a new message.
 */
export const ImageSourceSchema: GenMessage<ImageSource> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 1);

/**
 * Client image to perform Google Cloud Vision API tasks over.
 *
 * @generated from message google.cloud.vision.v1p4beta1.Image
 */
export type Image = Message<"google.cloud.vision.v1p4beta1.Image"> & {
  /**
   * Image content, represented as a stream of bytes.
   * Note: As with all `bytes` fields, protobuffers use a pure binary
   * representation, whereas JSON representations use base64.
   *
   * @generated from field: bytes content = 1;
   */
  content: Uint8Array;

  /**
   * Google Cloud Storage image location, or publicly-accessible image
   * URL. If both `content` and `source` are provided for an image, `content`
   * takes precedence and is used to perform the image annotation request.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.ImageSource source = 2;
   */
  source?: ImageSource;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.Image.
 * Use `create(ImageSchema)` to create a new message.
 */
export const ImageSchema: GenMessage<Image> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 2);

/**
 * A face annotation object contains the results of face detection.
 *
 * @generated from message google.cloud.vision.v1p4beta1.FaceAnnotation
 */
export type FaceAnnotation = Message<"google.cloud.vision.v1p4beta1.FaceAnnotation"> & {
  /**
   * The bounding polygon around the face. The coordinates of the bounding box
   * are in the original image's scale.
   * The bounding box is computed to "frame" the face in accordance with human
   * expectations. It is based on the landmarker results.
   * Note that one or more x and/or y coordinates may not be generated in the
   * `BoundingPoly` (the polygon will be unbounded) if only a partial face
   * appears in the image to be annotated.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.BoundingPoly bounding_poly = 1;
   */
  boundingPoly?: BoundingPoly;

  /**
   * The `fd_bounding_poly` bounding polygon is tighter than the
   * `boundingPoly`, and encloses only the skin part of the face. Typically, it
   * is used to eliminate the face from any image analysis that detects the
   * "amount of skin" visible in an image. It is not based on the
   * landmarker results, only on the initial face detection, hence
   * the <code>fd</code> (face detection) prefix.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.BoundingPoly fd_bounding_poly = 2;
   */
  fdBoundingPoly?: BoundingPoly;

  /**
   * Detected face landmarks.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.FaceAnnotation.Landmark landmarks = 3;
   */
  landmarks: FaceAnnotation_Landmark[];

  /**
   * Roll angle, which indicates the amount of clockwise/anti-clockwise rotation
   * of the face relative to the image vertical about the axis perpendicular to
   * the face. Range [-180,180].
   *
   * @generated from field: float roll_angle = 4;
   */
  rollAngle: number;

  /**
   * Yaw angle, which indicates the leftward/rightward angle that the face is
   * pointing relative to the vertical plane perpendicular to the image. Range
   * [-180,180].
   *
   * @generated from field: float pan_angle = 5;
   */
  panAngle: number;

  /**
   * Pitch angle, which indicates the upwards/downwards angle that the face is
   * pointing relative to the image's horizontal plane. Range [-180,180].
   *
   * @generated from field: float tilt_angle = 6;
   */
  tiltAngle: number;

  /**
   * Detection confidence. Range [0, 1].
   *
   * @generated from field: float detection_confidence = 7;
   */
  detectionConfidence: number;

  /**
   * Face landmarking confidence. Range [0, 1].
   *
   * @generated from field: float landmarking_confidence = 8;
   */
  landmarkingConfidence: number;

  /**
   * Joy likelihood.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.Likelihood joy_likelihood = 9;
   */
  joyLikelihood: Likelihood;

  /**
   * Sorrow likelihood.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.Likelihood sorrow_likelihood = 10;
   */
  sorrowLikelihood: Likelihood;

  /**
   * Anger likelihood.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.Likelihood anger_likelihood = 11;
   */
  angerLikelihood: Likelihood;

  /**
   * Surprise likelihood.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.Likelihood surprise_likelihood = 12;
   */
  surpriseLikelihood: Likelihood;

  /**
   * Under-exposed likelihood.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.Likelihood under_exposed_likelihood = 13;
   */
  underExposedLikelihood: Likelihood;

  /**
   * Blurred likelihood.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.Likelihood blurred_likelihood = 14;
   */
  blurredLikelihood: Likelihood;

  /**
   * Headwear likelihood.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.Likelihood headwear_likelihood = 15;
   */
  headwearLikelihood: Likelihood;

  /**
   * Additional recognition information. Only computed if
   * image_context.face_recognition_params is provided, **and** a match is found
   * to a [Celebrity][google.cloud.vision.v1p4beta1.Celebrity] in the input
   * [CelebritySet][google.cloud.vision.v1p4beta1.CelebritySet]. This field is
   * sorted in order of decreasing confidence values.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.FaceRecognitionResult recognition_result = 16;
   */
  recognitionResult: FaceRecognitionResult[];
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.FaceAnnotation.
 * Use `create(FaceAnnotationSchema)` to create a new message.
 */
export const FaceAnnotationSchema: GenMessage<FaceAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 3);

/**
 * A face-specific landmark (for example, a face feature).
 *
 * @generated from message google.cloud.vision.v1p4beta1.FaceAnnotation.Landmark
 */
export type FaceAnnotation_Landmark = Message<"google.cloud.vision.v1p4beta1.FaceAnnotation.Landmark"> & {
  /**
   * Face landmark type.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.FaceAnnotation.Landmark.Type type = 3;
   */
  type: FaceAnnotation_Landmark_Type;

  /**
   * Face landmark position.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.Position position = 4;
   */
  position?: Position;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.FaceAnnotation.Landmark.
 * Use `create(FaceAnnotation_LandmarkSchema)` to create a new message.
 */
export const FaceAnnotation_LandmarkSchema: GenMessage<FaceAnnotation_Landmark> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 3, 0);

/**
 * Face landmark (feature) type.
 * Left and right are defined from the vantage of the viewer of the image
 * without considering mirror projections typical of photos. So, `LEFT_EYE`,
 * typically, is the person's right eye.
 *
 * @generated from enum google.cloud.vision.v1p4beta1.FaceAnnotation.Landmark.Type
 */
export enum FaceAnnotation_Landmark_Type {
  /**
   * Unknown face landmark detected. Should not be filled.
   *
   * @generated from enum value: UNKNOWN_LANDMARK = 0;
   */
  UNKNOWN_LANDMARK = 0,

  /**
   * Left eye.
   *
   * @generated from enum value: LEFT_EYE = 1;
   */
  LEFT_EYE = 1,

  /**
   * Right eye.
   *
   * @generated from enum value: RIGHT_EYE = 2;
   */
  RIGHT_EYE = 2,

  /**
   * Left of left eyebrow.
   *
   * @generated from enum value: LEFT_OF_LEFT_EYEBROW = 3;
   */
  LEFT_OF_LEFT_EYEBROW = 3,

  /**
   * Right of left eyebrow.
   *
   * @generated from enum value: RIGHT_OF_LEFT_EYEBROW = 4;
   */
  RIGHT_OF_LEFT_EYEBROW = 4,

  /**
   * Left of right eyebrow.
   *
   * @generated from enum value: LEFT_OF_RIGHT_EYEBROW = 5;
   */
  LEFT_OF_RIGHT_EYEBROW = 5,

  /**
   * Right of right eyebrow.
   *
   * @generated from enum value: RIGHT_OF_RIGHT_EYEBROW = 6;
   */
  RIGHT_OF_RIGHT_EYEBROW = 6,

  /**
   * Midpoint between eyes.
   *
   * @generated from enum value: MIDPOINT_BETWEEN_EYES = 7;
   */
  MIDPOINT_BETWEEN_EYES = 7,

  /**
   * Nose tip.
   *
   * @generated from enum value: NOSE_TIP = 8;
   */
  NOSE_TIP = 8,

  /**
   * Upper lip.
   *
   * @generated from enum value: UPPER_LIP = 9;
   */
  UPPER_LIP = 9,

  /**
   * Lower lip.
   *
   * @generated from enum value: LOWER_LIP = 10;
   */
  LOWER_LIP = 10,

  /**
   * Mouth left.
   *
   * @generated from enum value: MOUTH_LEFT = 11;
   */
  MOUTH_LEFT = 11,

  /**
   * Mouth right.
   *
   * @generated from enum value: MOUTH_RIGHT = 12;
   */
  MOUTH_RIGHT = 12,

  /**
   * Mouth center.
   *
   * @generated from enum value: MOUTH_CENTER = 13;
   */
  MOUTH_CENTER = 13,

  /**
   * Nose, bottom right.
   *
   * @generated from enum value: NOSE_BOTTOM_RIGHT = 14;
   */
  NOSE_BOTTOM_RIGHT = 14,

  /**
   * Nose, bottom left.
   *
   * @generated from enum value: NOSE_BOTTOM_LEFT = 15;
   */
  NOSE_BOTTOM_LEFT = 15,

  /**
   * Nose, bottom center.
   *
   * @generated from enum value: NOSE_BOTTOM_CENTER = 16;
   */
  NOSE_BOTTOM_CENTER = 16,

  /**
   * Left eye, top boundary.
   *
   * @generated from enum value: LEFT_EYE_TOP_BOUNDARY = 17;
   */
  LEFT_EYE_TOP_BOUNDARY = 17,

  /**
   * Left eye, right corner.
   *
   * @generated from enum value: LEFT_EYE_RIGHT_CORNER = 18;
   */
  LEFT_EYE_RIGHT_CORNER = 18,

  /**
   * Left eye, bottom boundary.
   *
   * @generated from enum value: LEFT_EYE_BOTTOM_BOUNDARY = 19;
   */
  LEFT_EYE_BOTTOM_BOUNDARY = 19,

  /**
   * Left eye, left corner.
   *
   * @generated from enum value: LEFT_EYE_LEFT_CORNER = 20;
   */
  LEFT_EYE_LEFT_CORNER = 20,

  /**
   * Right eye, top boundary.
   *
   * @generated from enum value: RIGHT_EYE_TOP_BOUNDARY = 21;
   */
  RIGHT_EYE_TOP_BOUNDARY = 21,

  /**
   * Right eye, right corner.
   *
   * @generated from enum value: RIGHT_EYE_RIGHT_CORNER = 22;
   */
  RIGHT_EYE_RIGHT_CORNER = 22,

  /**
   * Right eye, bottom boundary.
   *
   * @generated from enum value: RIGHT_EYE_BOTTOM_BOUNDARY = 23;
   */
  RIGHT_EYE_BOTTOM_BOUNDARY = 23,

  /**
   * Right eye, left corner.
   *
   * @generated from enum value: RIGHT_EYE_LEFT_CORNER = 24;
   */
  RIGHT_EYE_LEFT_CORNER = 24,

  /**
   * Left eyebrow, upper midpoint.
   *
   * @generated from enum value: LEFT_EYEBROW_UPPER_MIDPOINT = 25;
   */
  LEFT_EYEBROW_UPPER_MIDPOINT = 25,

  /**
   * Right eyebrow, upper midpoint.
   *
   * @generated from enum value: RIGHT_EYEBROW_UPPER_MIDPOINT = 26;
   */
  RIGHT_EYEBROW_UPPER_MIDPOINT = 26,

  /**
   * Left ear tragion.
   *
   * @generated from enum value: LEFT_EAR_TRAGION = 27;
   */
  LEFT_EAR_TRAGION = 27,

  /**
   * Right ear tragion.
   *
   * @generated from enum value: RIGHT_EAR_TRAGION = 28;
   */
  RIGHT_EAR_TRAGION = 28,

  /**
   * Left eye pupil.
   *
   * @generated from enum value: LEFT_EYE_PUPIL = 29;
   */
  LEFT_EYE_PUPIL = 29,

  /**
   * Right eye pupil.
   *
   * @generated from enum value: RIGHT_EYE_PUPIL = 30;
   */
  RIGHT_EYE_PUPIL = 30,

  /**
   * Forehead glabella.
   *
   * @generated from enum value: FOREHEAD_GLABELLA = 31;
   */
  FOREHEAD_GLABELLA = 31,

  /**
   * Chin gnathion.
   *
   * @generated from enum value: CHIN_GNATHION = 32;
   */
  CHIN_GNATHION = 32,

  /**
   * Chin left gonion.
   *
   * @generated from enum value: CHIN_LEFT_GONION = 33;
   */
  CHIN_LEFT_GONION = 33,

  /**
   * Chin right gonion.
   *
   * @generated from enum value: CHIN_RIGHT_GONION = 34;
   */
  CHIN_RIGHT_GONION = 34,
}

/**
 * Describes the enum google.cloud.vision.v1p4beta1.FaceAnnotation.Landmark.Type.
 */
export const FaceAnnotation_Landmark_TypeSchema: GenEnum<FaceAnnotation_Landmark_Type> = /*@__PURE__*/
  enumDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 3, 0, 0);

/**
 * Detected entity location information.
 *
 * @generated from message google.cloud.vision.v1p4beta1.LocationInfo
 */
export type LocationInfo = Message<"google.cloud.vision.v1p4beta1.LocationInfo"> & {
  /**
   * lat/long location coordinates.
   *
   * @generated from field: google.type.LatLng lat_lng = 1;
   */
  latLng?: LatLng;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.LocationInfo.
 * Use `create(LocationInfoSchema)` to create a new message.
 */
export const LocationInfoSchema: GenMessage<LocationInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 4);

/**
 * A `Property` consists of a user-supplied name/value pair.
 *
 * @generated from message google.cloud.vision.v1p4beta1.Property
 */
export type Property = Message<"google.cloud.vision.v1p4beta1.Property"> & {
  /**
   * Name of the property.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Value of the property.
   *
   * @generated from field: string value = 2;
   */
  value: string;

  /**
   * Value of numeric properties.
   *
   * @generated from field: uint64 uint64_value = 3;
   */
  uint64Value: bigint;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.Property.
 * Use `create(PropertySchema)` to create a new message.
 */
export const PropertySchema: GenMessage<Property> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 5);

/**
 * Set of detected entity features.
 *
 * @generated from message google.cloud.vision.v1p4beta1.EntityAnnotation
 */
export type EntityAnnotation = Message<"google.cloud.vision.v1p4beta1.EntityAnnotation"> & {
  /**
   * Opaque entity ID. Some IDs may be available in
   * [Google Knowledge Graph Search
   * API](https://developers.google.com/knowledge-graph/).
   *
   * @generated from field: string mid = 1;
   */
  mid: string;

  /**
   * The language code for the locale in which the entity textual
   * `description` is expressed.
   *
   * @generated from field: string locale = 2;
   */
  locale: string;

  /**
   * Entity textual description, expressed in its `locale` language.
   *
   * @generated from field: string description = 3;
   */
  description: string;

  /**
   * Overall score of the result. Range [0, 1].
   *
   * @generated from field: float score = 4;
   */
  score: number;

  /**
   * **Deprecated. Use `score` instead.**
   * The accuracy of the entity detection in an image.
   * For example, for an image in which the "Eiffel Tower" entity is detected,
   * this field represents the confidence that there is a tower in the query
   * image. Range [0, 1].
   *
   * @generated from field: float confidence = 5 [deprecated = true];
   * @deprecated
   */
  confidence: number;

  /**
   * The relevancy of the ICA (Image Content Annotation) label to the
   * image. For example, the relevancy of "tower" is likely higher to an image
   * containing the detected "Eiffel Tower" than to an image containing a
   * detected distant towering building, even though the confidence that
   * there is a tower in each image may be the same. Range [0, 1].
   *
   * @generated from field: float topicality = 6;
   */
  topicality: number;

  /**
   * Image region to which this entity belongs. Not produced
   * for `LABEL_DETECTION` features.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.BoundingPoly bounding_poly = 7;
   */
  boundingPoly?: BoundingPoly;

  /**
   * The location information for the detected entity. Multiple
   * `LocationInfo` elements can be present because one location may
   * indicate the location of the scene in the image, and another location
   * may indicate the location of the place where the image was taken.
   * Location information is usually present for landmarks.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.LocationInfo locations = 8;
   */
  locations: LocationInfo[];

  /**
   * Some entities may have optional user-supplied `Property` (name/value)
   * fields, such a score or string that qualifies the entity.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.Property properties = 9;
   */
  properties: Property[];
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.EntityAnnotation.
 * Use `create(EntityAnnotationSchema)` to create a new message.
 */
export const EntityAnnotationSchema: GenMessage<EntityAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 6);

/**
 * Set of detected objects with bounding boxes.
 *
 * @generated from message google.cloud.vision.v1p4beta1.LocalizedObjectAnnotation
 */
export type LocalizedObjectAnnotation = Message<"google.cloud.vision.v1p4beta1.LocalizedObjectAnnotation"> & {
  /**
   * Object ID that should align with EntityAnnotation mid.
   *
   * @generated from field: string mid = 1;
   */
  mid: string;

  /**
   * The BCP-47 language code, such as "en-US" or "sr-Latn". For more
   * information, see
   * http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
   *
   * @generated from field: string language_code = 2;
   */
  languageCode: string;

  /**
   * Object name, expressed in its `language_code` language.
   *
   * @generated from field: string name = 3;
   */
  name: string;

  /**
   * Score of the result. Range [0, 1].
   *
   * @generated from field: float score = 4;
   */
  score: number;

  /**
   * Image region to which this object belongs. This must be populated.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.BoundingPoly bounding_poly = 5;
   */
  boundingPoly?: BoundingPoly;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.LocalizedObjectAnnotation.
 * Use `create(LocalizedObjectAnnotationSchema)` to create a new message.
 */
export const LocalizedObjectAnnotationSchema: GenMessage<LocalizedObjectAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 7);

/**
 * Set of features pertaining to the image, computed by computer vision
 * methods over safe-search verticals (for example, adult, spoof, medical,
 * violence).
 *
 * @generated from message google.cloud.vision.v1p4beta1.SafeSearchAnnotation
 */
export type SafeSearchAnnotation = Message<"google.cloud.vision.v1p4beta1.SafeSearchAnnotation"> & {
  /**
   * Represents the adult content likelihood for the image. Adult content may
   * contain elements such as nudity, pornographic images or cartoons, or
   * sexual activities.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.Likelihood adult = 1;
   */
  adult: Likelihood;

  /**
   * Spoof likelihood. The likelihood that an modification
   * was made to the image's canonical version to make it appear
   * funny or offensive.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.Likelihood spoof = 2;
   */
  spoof: Likelihood;

  /**
   * Likelihood that this is a medical image.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.Likelihood medical = 3;
   */
  medical: Likelihood;

  /**
   * Likelihood that this image contains violent content.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.Likelihood violence = 4;
   */
  violence: Likelihood;

  /**
   * Likelihood that the request image contains racy content. Racy content may
   * include (but is not limited to) skimpy or sheer clothing, strategically
   * covered nudity, lewd or provocative poses, or close-ups of sensitive
   * body areas.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.Likelihood racy = 9;
   */
  racy: Likelihood;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.SafeSearchAnnotation.
 * Use `create(SafeSearchAnnotationSchema)` to create a new message.
 */
export const SafeSearchAnnotationSchema: GenMessage<SafeSearchAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 8);

/**
 * Rectangle determined by min and max `LatLng` pairs.
 *
 * @generated from message google.cloud.vision.v1p4beta1.LatLongRect
 */
export type LatLongRect = Message<"google.cloud.vision.v1p4beta1.LatLongRect"> & {
  /**
   * Min lat/long pair.
   *
   * @generated from field: google.type.LatLng min_lat_lng = 1;
   */
  minLatLng?: LatLng;

  /**
   * Max lat/long pair.
   *
   * @generated from field: google.type.LatLng max_lat_lng = 2;
   */
  maxLatLng?: LatLng;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.LatLongRect.
 * Use `create(LatLongRectSchema)` to create a new message.
 */
export const LatLongRectSchema: GenMessage<LatLongRect> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 9);

/**
 * Color information consists of RGB channels, score, and the fraction of
 * the image that the color occupies in the image.
 *
 * @generated from message google.cloud.vision.v1p4beta1.ColorInfo
 */
export type ColorInfo = Message<"google.cloud.vision.v1p4beta1.ColorInfo"> & {
  /**
   * RGB components of the color.
   *
   * @generated from field: google.type.Color color = 1;
   */
  color?: Color;

  /**
   * Image-specific score for this color. Value in range [0, 1].
   *
   * @generated from field: float score = 2;
   */
  score: number;

  /**
   * The fraction of pixels the color occupies in the image.
   * Value in range [0, 1].
   *
   * @generated from field: float pixel_fraction = 3;
   */
  pixelFraction: number;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.ColorInfo.
 * Use `create(ColorInfoSchema)` to create a new message.
 */
export const ColorInfoSchema: GenMessage<ColorInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 10);

/**
 * Set of dominant colors and their corresponding scores.
 *
 * @generated from message google.cloud.vision.v1p4beta1.DominantColorsAnnotation
 */
export type DominantColorsAnnotation = Message<"google.cloud.vision.v1p4beta1.DominantColorsAnnotation"> & {
  /**
   * RGB color values with their score and pixel fraction.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.ColorInfo colors = 1;
   */
  colors: ColorInfo[];
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.DominantColorsAnnotation.
 * Use `create(DominantColorsAnnotationSchema)` to create a new message.
 */
export const DominantColorsAnnotationSchema: GenMessage<DominantColorsAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 11);

/**
 * Stores image properties, such as dominant colors.
 *
 * @generated from message google.cloud.vision.v1p4beta1.ImageProperties
 */
export type ImageProperties = Message<"google.cloud.vision.v1p4beta1.ImageProperties"> & {
  /**
   * If present, dominant colors completed successfully.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.DominantColorsAnnotation dominant_colors = 1;
   */
  dominantColors?: DominantColorsAnnotation;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.ImageProperties.
 * Use `create(ImagePropertiesSchema)` to create a new message.
 */
export const ImagePropertiesSchema: GenMessage<ImageProperties> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 12);

/**
 * Single crop hint that is used to generate a new crop when serving an image.
 *
 * @generated from message google.cloud.vision.v1p4beta1.CropHint
 */
export type CropHint = Message<"google.cloud.vision.v1p4beta1.CropHint"> & {
  /**
   * The bounding polygon for the crop region. The coordinates of the bounding
   * box are in the original image's scale.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.BoundingPoly bounding_poly = 1;
   */
  boundingPoly?: BoundingPoly;

  /**
   * Confidence of this being a salient region.  Range [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;

  /**
   * Fraction of importance of this salient region with respect to the original
   * image.
   *
   * @generated from field: float importance_fraction = 3;
   */
  importanceFraction: number;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.CropHint.
 * Use `create(CropHintSchema)` to create a new message.
 */
export const CropHintSchema: GenMessage<CropHint> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 13);

/**
 * Set of crop hints that are used to generate new crops when serving images.
 *
 * @generated from message google.cloud.vision.v1p4beta1.CropHintsAnnotation
 */
export type CropHintsAnnotation = Message<"google.cloud.vision.v1p4beta1.CropHintsAnnotation"> & {
  /**
   * Crop hint results.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.CropHint crop_hints = 1;
   */
  cropHints: CropHint[];
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.CropHintsAnnotation.
 * Use `create(CropHintsAnnotationSchema)` to create a new message.
 */
export const CropHintsAnnotationSchema: GenMessage<CropHintsAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 14);

/**
 * Parameters for crop hints annotation request.
 *
 * @generated from message google.cloud.vision.v1p4beta1.CropHintsParams
 */
export type CropHintsParams = Message<"google.cloud.vision.v1p4beta1.CropHintsParams"> & {
  /**
   * Aspect ratios in floats, representing the ratio of the width to the height
   * of the image. For example, if the desired aspect ratio is 4/3, the
   * corresponding float value should be 1.33333.  If not specified, the
   * best possible crop is returned. The number of provided aspect ratios is
   * limited to a maximum of 16; any aspect ratios provided after the 16th are
   * ignored.
   *
   * @generated from field: repeated float aspect_ratios = 1;
   */
  aspectRatios: number[];
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.CropHintsParams.
 * Use `create(CropHintsParamsSchema)` to create a new message.
 */
export const CropHintsParamsSchema: GenMessage<CropHintsParams> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 15);

/**
 * Parameters for web detection request.
 *
 * @generated from message google.cloud.vision.v1p4beta1.WebDetectionParams
 */
export type WebDetectionParams = Message<"google.cloud.vision.v1p4beta1.WebDetectionParams"> & {
  /**
   * Whether to include results derived from the geo information in the image.
   *
   * @generated from field: bool include_geo_results = 2;
   */
  includeGeoResults: boolean;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.WebDetectionParams.
 * Use `create(WebDetectionParamsSchema)` to create a new message.
 */
export const WebDetectionParamsSchema: GenMessage<WebDetectionParams> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 16);

/**
 * Parameters for text detections. This is used to control TEXT_DETECTION and
 * DOCUMENT_TEXT_DETECTION features.
 *
 * @generated from message google.cloud.vision.v1p4beta1.TextDetectionParams
 */
export type TextDetectionParams = Message<"google.cloud.vision.v1p4beta1.TextDetectionParams"> & {
  /**
   * By default, Cloud Vision API only includes confidence score for
   * DOCUMENT_TEXT_DETECTION result. Set the flag to true to include confidence
   * score for TEXT_DETECTION as well.
   *
   * @generated from field: bool enable_text_detection_confidence_score = 9;
   */
  enableTextDetectionConfidenceScore: boolean;

  /**
   * A list of advanced OCR options to fine-tune OCR behavior.
   *
   * @generated from field: repeated string advanced_ocr_options = 11;
   */
  advancedOcrOptions: string[];
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.TextDetectionParams.
 * Use `create(TextDetectionParamsSchema)` to create a new message.
 */
export const TextDetectionParamsSchema: GenMessage<TextDetectionParams> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 17);

/**
 * Image context and/or feature-specific parameters.
 *
 * @generated from message google.cloud.vision.v1p4beta1.ImageContext
 */
export type ImageContext = Message<"google.cloud.vision.v1p4beta1.ImageContext"> & {
  /**
   * Not used.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.LatLongRect lat_long_rect = 1;
   */
  latLongRect?: LatLongRect;

  /**
   * List of languages to use for TEXT_DETECTION. In most cases, an empty value
   * yields the best results since it enables automatic language detection. For
   * languages based on the Latin alphabet, setting `language_hints` is not
   * needed. In rare cases, when the language of the text in the image is known,
   * setting a hint will help get better results (although it will be a
   * significant hindrance if the hint is wrong). Text detection returns an
   * error if one or more of the specified languages is not one of the
   * [supported languages](https://cloud.google.com/vision/docs/languages).
   *
   * @generated from field: repeated string language_hints = 2;
   */
  languageHints: string[];

  /**
   * Parameters for crop hints annotation request.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.CropHintsParams crop_hints_params = 4;
   */
  cropHintsParams?: CropHintsParams;

  /**
   * Parameters for face recognition.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.FaceRecognitionParams face_recognition_params = 10;
   */
  faceRecognitionParams?: FaceRecognitionParams;

  /**
   * Parameters for product search.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.ProductSearchParams product_search_params = 5;
   */
  productSearchParams?: ProductSearchParams;

  /**
   * Parameters for web detection.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.WebDetectionParams web_detection_params = 6;
   */
  webDetectionParams?: WebDetectionParams;

  /**
   * Parameters for text detection and document text detection.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.TextDetectionParams text_detection_params = 12;
   */
  textDetectionParams?: TextDetectionParams;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.ImageContext.
 * Use `create(ImageContextSchema)` to create a new message.
 */
export const ImageContextSchema: GenMessage<ImageContext> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 18);

/**
 * Request for performing Google Cloud Vision API tasks over a user-provided
 * image, with user-requested features, and with context information.
 *
 * @generated from message google.cloud.vision.v1p4beta1.AnnotateImageRequest
 */
export type AnnotateImageRequest = Message<"google.cloud.vision.v1p4beta1.AnnotateImageRequest"> & {
  /**
   * The image to be processed.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.Image image = 1;
   */
  image?: Image;

  /**
   * Requested features.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.Feature features = 2;
   */
  features: Feature[];

  /**
   * Additional context that may accompany the image.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.ImageContext image_context = 3;
   */
  imageContext?: ImageContext;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.AnnotateImageRequest.
 * Use `create(AnnotateImageRequestSchema)` to create a new message.
 */
export const AnnotateImageRequestSchema: GenMessage<AnnotateImageRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 19);

/**
 * If an image was produced from a file (e.g. a PDF), this message gives
 * information about the source of that image.
 *
 * @generated from message google.cloud.vision.v1p4beta1.ImageAnnotationContext
 */
export type ImageAnnotationContext = Message<"google.cloud.vision.v1p4beta1.ImageAnnotationContext"> & {
  /**
   * The URI of the file used to produce the image.
   *
   * @generated from field: string uri = 1;
   */
  uri: string;

  /**
   * If the file was a PDF or TIFF, this field gives the page number within
   * the file used to produce the image.
   *
   * @generated from field: int32 page_number = 2;
   */
  pageNumber: number;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.ImageAnnotationContext.
 * Use `create(ImageAnnotationContextSchema)` to create a new message.
 */
export const ImageAnnotationContextSchema: GenMessage<ImageAnnotationContext> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 20);

/**
 * Response to an image annotation request.
 *
 * @generated from message google.cloud.vision.v1p4beta1.AnnotateImageResponse
 */
export type AnnotateImageResponse = Message<"google.cloud.vision.v1p4beta1.AnnotateImageResponse"> & {
  /**
   * If present, face detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.FaceAnnotation face_annotations = 1;
   */
  faceAnnotations: FaceAnnotation[];

  /**
   * If present, landmark detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.EntityAnnotation landmark_annotations = 2;
   */
  landmarkAnnotations: EntityAnnotation[];

  /**
   * If present, logo detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.EntityAnnotation logo_annotations = 3;
   */
  logoAnnotations: EntityAnnotation[];

  /**
   * If present, label detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.EntityAnnotation label_annotations = 4;
   */
  labelAnnotations: EntityAnnotation[];

  /**
   * If present, localized object detection has completed successfully.
   * This will be sorted descending by confidence score.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.LocalizedObjectAnnotation localized_object_annotations = 22;
   */
  localizedObjectAnnotations: LocalizedObjectAnnotation[];

  /**
   * If present, text (OCR) detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.EntityAnnotation text_annotations = 5;
   */
  textAnnotations: EntityAnnotation[];

  /**
   * If present, text (OCR) detection or document (OCR) text detection has
   * completed successfully.
   * This annotation provides the structural hierarchy for the OCR detected
   * text.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.TextAnnotation full_text_annotation = 12;
   */
  fullTextAnnotation?: TextAnnotation;

  /**
   * If present, safe-search annotation has completed successfully.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.SafeSearchAnnotation safe_search_annotation = 6;
   */
  safeSearchAnnotation?: SafeSearchAnnotation;

  /**
   * If present, image properties were extracted successfully.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.ImageProperties image_properties_annotation = 8;
   */
  imagePropertiesAnnotation?: ImageProperties;

  /**
   * If present, crop hints have completed successfully.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.CropHintsAnnotation crop_hints_annotation = 11;
   */
  cropHintsAnnotation?: CropHintsAnnotation;

  /**
   * If present, web detection has completed successfully.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.WebDetection web_detection = 13;
   */
  webDetection?: WebDetection;

  /**
   * If present, product search has completed successfully.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.ProductSearchResults product_search_results = 14;
   */
  productSearchResults?: ProductSearchResults;

  /**
   * If set, represents the error message for the operation.
   * Note that filled-in image annotations are guaranteed to be
   * correct, even when `error` is set.
   *
   * @generated from field: google.rpc.Status error = 9;
   */
  error?: Status;

  /**
   * If present, contextual information is needed to understand where this image
   * comes from.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.ImageAnnotationContext context = 21;
   */
  context?: ImageAnnotationContext;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.AnnotateImageResponse.
 * Use `create(AnnotateImageResponseSchema)` to create a new message.
 */
export const AnnotateImageResponseSchema: GenMessage<AnnotateImageResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 21);

/**
 * Multiple image annotation requests are batched into a single service call.
 *
 * @generated from message google.cloud.vision.v1p4beta1.BatchAnnotateImagesRequest
 */
export type BatchAnnotateImagesRequest = Message<"google.cloud.vision.v1p4beta1.BatchAnnotateImagesRequest"> & {
  /**
   * Required. Individual image annotation requests for this batch.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.AnnotateImageRequest requests = 1;
   */
  requests: AnnotateImageRequest[];
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.BatchAnnotateImagesRequest.
 * Use `create(BatchAnnotateImagesRequestSchema)` to create a new message.
 */
export const BatchAnnotateImagesRequestSchema: GenMessage<BatchAnnotateImagesRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 22);

/**
 * Response to a batch image annotation request.
 *
 * @generated from message google.cloud.vision.v1p4beta1.BatchAnnotateImagesResponse
 */
export type BatchAnnotateImagesResponse = Message<"google.cloud.vision.v1p4beta1.BatchAnnotateImagesResponse"> & {
  /**
   * Individual responses to image annotation requests within the batch.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.AnnotateImageResponse responses = 1;
   */
  responses: AnnotateImageResponse[];
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.BatchAnnotateImagesResponse.
 * Use `create(BatchAnnotateImagesResponseSchema)` to create a new message.
 */
export const BatchAnnotateImagesResponseSchema: GenMessage<BatchAnnotateImagesResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 23);

/**
 * A request to annotate one single file, e.g. a PDF, TIFF or GIF file.
 *
 * @generated from message google.cloud.vision.v1p4beta1.AnnotateFileRequest
 */
export type AnnotateFileRequest = Message<"google.cloud.vision.v1p4beta1.AnnotateFileRequest"> & {
  /**
   * Required. Information about the input file.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.InputConfig input_config = 1;
   */
  inputConfig?: InputConfig;

  /**
   * Required. Requested features.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.Feature features = 2;
   */
  features: Feature[];

  /**
   * Additional context that may accompany the image(s) in the file.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.ImageContext image_context = 3;
   */
  imageContext?: ImageContext;

  /**
   * Pages of the file to perform image annotation.
   *
   * Pages starts from 1, we assume the first page of the file is page 1.
   * At most 5 pages are supported per request. Pages can be negative.
   *
   * Page 1 means the first page.
   * Page 2 means the second page.
   * Page -1 means the last page.
   * Page -2 means the second to the last page.
   *
   * If the file is GIF instead of PDF or TIFF, page refers to GIF frames.
   *
   * If this field is empty, by default the service performs image annotation
   * for the first 5 pages of the file.
   *
   * @generated from field: repeated int32 pages = 4;
   */
  pages: number[];
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.AnnotateFileRequest.
 * Use `create(AnnotateFileRequestSchema)` to create a new message.
 */
export const AnnotateFileRequestSchema: GenMessage<AnnotateFileRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 24);

/**
 * Response to a single file annotation request. A file may contain one or more
 * images, which individually have their own responses.
 *
 * @generated from message google.cloud.vision.v1p4beta1.AnnotateFileResponse
 */
export type AnnotateFileResponse = Message<"google.cloud.vision.v1p4beta1.AnnotateFileResponse"> & {
  /**
   * Information about the file for which this response is generated.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.InputConfig input_config = 1;
   */
  inputConfig?: InputConfig;

  /**
   * Individual responses to images found within the file. This field will be
   * empty if the `error` field is set.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.AnnotateImageResponse responses = 2;
   */
  responses: AnnotateImageResponse[];

  /**
   * This field gives the total number of pages in the file.
   *
   * @generated from field: int32 total_pages = 3;
   */
  totalPages: number;

  /**
   * If set, represents the error message for the failed request. The
   * `responses` field will not be set in this case.
   *
   * @generated from field: google.rpc.Status error = 4;
   */
  error?: Status;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.AnnotateFileResponse.
 * Use `create(AnnotateFileResponseSchema)` to create a new message.
 */
export const AnnotateFileResponseSchema: GenMessage<AnnotateFileResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 25);

/**
 * A list of requests to annotate files using the BatchAnnotateFiles API.
 *
 * @generated from message google.cloud.vision.v1p4beta1.BatchAnnotateFilesRequest
 */
export type BatchAnnotateFilesRequest = Message<"google.cloud.vision.v1p4beta1.BatchAnnotateFilesRequest"> & {
  /**
   * Required. The list of file annotation requests. Right now we support only
   * one AnnotateFileRequest in BatchAnnotateFilesRequest.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.AnnotateFileRequest requests = 1;
   */
  requests: AnnotateFileRequest[];
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.BatchAnnotateFilesRequest.
 * Use `create(BatchAnnotateFilesRequestSchema)` to create a new message.
 */
export const BatchAnnotateFilesRequestSchema: GenMessage<BatchAnnotateFilesRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 26);

/**
 * A list of file annotation responses.
 *
 * @generated from message google.cloud.vision.v1p4beta1.BatchAnnotateFilesResponse
 */
export type BatchAnnotateFilesResponse = Message<"google.cloud.vision.v1p4beta1.BatchAnnotateFilesResponse"> & {
  /**
   * The list of file annotation responses, each response corresponding to each
   * AnnotateFileRequest in BatchAnnotateFilesRequest.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.AnnotateFileResponse responses = 1;
   */
  responses: AnnotateFileResponse[];
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.BatchAnnotateFilesResponse.
 * Use `create(BatchAnnotateFilesResponseSchema)` to create a new message.
 */
export const BatchAnnotateFilesResponseSchema: GenMessage<BatchAnnotateFilesResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 27);

/**
 * An offline file annotation request.
 *
 * @generated from message google.cloud.vision.v1p4beta1.AsyncAnnotateFileRequest
 */
export type AsyncAnnotateFileRequest = Message<"google.cloud.vision.v1p4beta1.AsyncAnnotateFileRequest"> & {
  /**
   * Required. Information about the input file.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.InputConfig input_config = 1;
   */
  inputConfig?: InputConfig;

  /**
   * Required. Requested features.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.Feature features = 2;
   */
  features: Feature[];

  /**
   * Additional context that may accompany the image(s) in the file.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.ImageContext image_context = 3;
   */
  imageContext?: ImageContext;

  /**
   * Required. The desired output location and metadata (e.g. format).
   *
   * @generated from field: google.cloud.vision.v1p4beta1.OutputConfig output_config = 4;
   */
  outputConfig?: OutputConfig;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.AsyncAnnotateFileRequest.
 * Use `create(AsyncAnnotateFileRequestSchema)` to create a new message.
 */
export const AsyncAnnotateFileRequestSchema: GenMessage<AsyncAnnotateFileRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 28);

/**
 * The response for a single offline file annotation request.
 *
 * @generated from message google.cloud.vision.v1p4beta1.AsyncAnnotateFileResponse
 */
export type AsyncAnnotateFileResponse = Message<"google.cloud.vision.v1p4beta1.AsyncAnnotateFileResponse"> & {
  /**
   * The output location and metadata from AsyncAnnotateFileRequest.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.OutputConfig output_config = 1;
   */
  outputConfig?: OutputConfig;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.AsyncAnnotateFileResponse.
 * Use `create(AsyncAnnotateFileResponseSchema)` to create a new message.
 */
export const AsyncAnnotateFileResponseSchema: GenMessage<AsyncAnnotateFileResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 29);

/**
 * Request for async image annotation for a list of images.
 *
 * @generated from message google.cloud.vision.v1p4beta1.AsyncBatchAnnotateImagesRequest
 */
export type AsyncBatchAnnotateImagesRequest = Message<"google.cloud.vision.v1p4beta1.AsyncBatchAnnotateImagesRequest"> & {
  /**
   * Required. Individual image annotation requests for this batch.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.AnnotateImageRequest requests = 1;
   */
  requests: AnnotateImageRequest[];

  /**
   * Required. The desired output location and metadata (e.g. format).
   *
   * @generated from field: google.cloud.vision.v1p4beta1.OutputConfig output_config = 2;
   */
  outputConfig?: OutputConfig;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.AsyncBatchAnnotateImagesRequest.
 * Use `create(AsyncBatchAnnotateImagesRequestSchema)` to create a new message.
 */
export const AsyncBatchAnnotateImagesRequestSchema: GenMessage<AsyncBatchAnnotateImagesRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 30);

/**
 * Response to an async batch image annotation request.
 *
 * @generated from message google.cloud.vision.v1p4beta1.AsyncBatchAnnotateImagesResponse
 */
export type AsyncBatchAnnotateImagesResponse = Message<"google.cloud.vision.v1p4beta1.AsyncBatchAnnotateImagesResponse"> & {
  /**
   * The output location and metadata from AsyncBatchAnnotateImagesRequest.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.OutputConfig output_config = 1;
   */
  outputConfig?: OutputConfig;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.AsyncBatchAnnotateImagesResponse.
 * Use `create(AsyncBatchAnnotateImagesResponseSchema)` to create a new message.
 */
export const AsyncBatchAnnotateImagesResponseSchema: GenMessage<AsyncBatchAnnotateImagesResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 31);

/**
 * Multiple async file annotation requests are batched into a single service
 * call.
 *
 * @generated from message google.cloud.vision.v1p4beta1.AsyncBatchAnnotateFilesRequest
 */
export type AsyncBatchAnnotateFilesRequest = Message<"google.cloud.vision.v1p4beta1.AsyncBatchAnnotateFilesRequest"> & {
  /**
   * Required. Individual async file annotation requests for this batch.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.AsyncAnnotateFileRequest requests = 1;
   */
  requests: AsyncAnnotateFileRequest[];
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.AsyncBatchAnnotateFilesRequest.
 * Use `create(AsyncBatchAnnotateFilesRequestSchema)` to create a new message.
 */
export const AsyncBatchAnnotateFilesRequestSchema: GenMessage<AsyncBatchAnnotateFilesRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 32);

/**
 * Response to an async batch file annotation request.
 *
 * @generated from message google.cloud.vision.v1p4beta1.AsyncBatchAnnotateFilesResponse
 */
export type AsyncBatchAnnotateFilesResponse = Message<"google.cloud.vision.v1p4beta1.AsyncBatchAnnotateFilesResponse"> & {
  /**
   * The list of file annotation responses, one for each request in
   * AsyncBatchAnnotateFilesRequest.
   *
   * @generated from field: repeated google.cloud.vision.v1p4beta1.AsyncAnnotateFileResponse responses = 1;
   */
  responses: AsyncAnnotateFileResponse[];
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.AsyncBatchAnnotateFilesResponse.
 * Use `create(AsyncBatchAnnotateFilesResponseSchema)` to create a new message.
 */
export const AsyncBatchAnnotateFilesResponseSchema: GenMessage<AsyncBatchAnnotateFilesResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 33);

/**
 * The desired input location and metadata.
 *
 * @generated from message google.cloud.vision.v1p4beta1.InputConfig
 */
export type InputConfig = Message<"google.cloud.vision.v1p4beta1.InputConfig"> & {
  /**
   * The Google Cloud Storage location to read the input from.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.GcsSource gcs_source = 1;
   */
  gcsSource?: GcsSource;

  /**
   * File content, represented as a stream of bytes.
   * Note: As with all `bytes` fields, protobuffers use a pure binary
   * representation, whereas JSON representations use base64.
   *
   * Currently, this field only works for BatchAnnotateFiles requests. It does
   * not work for AsyncBatchAnnotateFiles requests.
   *
   * @generated from field: bytes content = 3;
   */
  content: Uint8Array;

  /**
   * The type of the file. Currently only "application/pdf", "image/tiff" and
   * "image/gif" are supported. Wildcards are not supported.
   *
   * @generated from field: string mime_type = 2;
   */
  mimeType: string;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.InputConfig.
 * Use `create(InputConfigSchema)` to create a new message.
 */
export const InputConfigSchema: GenMessage<InputConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 34);

/**
 * The desired output location and metadata.
 *
 * @generated from message google.cloud.vision.v1p4beta1.OutputConfig
 */
export type OutputConfig = Message<"google.cloud.vision.v1p4beta1.OutputConfig"> & {
  /**
   * The Google Cloud Storage location to write the output(s) to.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.GcsDestination gcs_destination = 1;
   */
  gcsDestination?: GcsDestination;

  /**
   * The max number of response protos to put into each output JSON file on
   * Google Cloud Storage.
   * The valid range is [1, 100]. If not specified, the default value is 20.
   *
   * For example, for one pdf file with 100 pages, 100 response protos will
   * be generated. If `batch_size` = 20, then 5 json files each
   * containing 20 response protos will be written under the prefix
   * `gcs_destination`.`uri`.
   *
   * Currently, batch_size only applies to GcsDestination, with potential future
   * support for other output configurations.
   *
   * @generated from field: int32 batch_size = 2;
   */
  batchSize: number;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.OutputConfig.
 * Use `create(OutputConfigSchema)` to create a new message.
 */
export const OutputConfigSchema: GenMessage<OutputConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 35);

/**
 * The Google Cloud Storage location where the input will be read from.
 *
 * @generated from message google.cloud.vision.v1p4beta1.GcsSource
 */
export type GcsSource = Message<"google.cloud.vision.v1p4beta1.GcsSource"> & {
  /**
   * Google Cloud Storage URI for the input file. This must only be a
   * Google Cloud Storage object. Wildcards are not currently supported.
   *
   * @generated from field: string uri = 1;
   */
  uri: string;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.GcsSource.
 * Use `create(GcsSourceSchema)` to create a new message.
 */
export const GcsSourceSchema: GenMessage<GcsSource> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 36);

/**
 * The Google Cloud Storage location where the output will be written to.
 *
 * @generated from message google.cloud.vision.v1p4beta1.GcsDestination
 */
export type GcsDestination = Message<"google.cloud.vision.v1p4beta1.GcsDestination"> & {
  /**
   * Google Cloud Storage URI prefix where the results will be stored. Results
   * will be in JSON format and preceded by its corresponding input URI prefix.
   * This field can either represent a gcs file prefix or gcs directory. In
   * either case, the uri should be unique because in order to get all of the
   * output files, you will need to do a wildcard gcs search on the uri prefix
   * you provide.
   *
   * Examples:
   *
   * *    File Prefix: gs://bucket-name/here/filenameprefix   The output files
   * will be created in gs://bucket-name/here/ and the names of the
   * output files will begin with "filenameprefix".
   *
   * *    Directory Prefix: gs://bucket-name/some/location/   The output files
   * will be created in gs://bucket-name/some/location/ and the names of the
   * output files could be anything because there was no filename prefix
   * specified.
   *
   * If multiple outputs, each response is still AnnotateFileResponse, each of
   * which contains some subset of the full list of AnnotateImageResponse.
   * Multiple outputs can happen if, for example, the output JSON is too large
   * and overflows into multiple sharded files.
   *
   * @generated from field: string uri = 1;
   */
  uri: string;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.GcsDestination.
 * Use `create(GcsDestinationSchema)` to create a new message.
 */
export const GcsDestinationSchema: GenMessage<GcsDestination> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 37);

/**
 * Contains metadata for the BatchAnnotateImages operation.
 *
 * @generated from message google.cloud.vision.v1p4beta1.OperationMetadata
 */
export type OperationMetadata = Message<"google.cloud.vision.v1p4beta1.OperationMetadata"> & {
  /**
   * Current state of the batch operation.
   *
   * @generated from field: google.cloud.vision.v1p4beta1.OperationMetadata.State state = 1;
   */
  state: OperationMetadata_State;

  /**
   * The time when the batch request was received.
   *
   * @generated from field: google.protobuf.Timestamp create_time = 5;
   */
  createTime?: Timestamp;

  /**
   * The time when the operation result was last updated.
   *
   * @generated from field: google.protobuf.Timestamp update_time = 6;
   */
  updateTime?: Timestamp;
};

/**
 * Describes the message google.cloud.vision.v1p4beta1.OperationMetadata.
 * Use `create(OperationMetadataSchema)` to create a new message.
 */
export const OperationMetadataSchema: GenMessage<OperationMetadata> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 38);

/**
 * Batch operation states.
 *
 * @generated from enum google.cloud.vision.v1p4beta1.OperationMetadata.State
 */
export enum OperationMetadata_State {
  /**
   * Invalid.
   *
   * @generated from enum value: STATE_UNSPECIFIED = 0;
   */
  STATE_UNSPECIFIED = 0,

  /**
   * Request is received.
   *
   * @generated from enum value: CREATED = 1;
   */
  CREATED = 1,

  /**
   * Request is actively being processed.
   *
   * @generated from enum value: RUNNING = 2;
   */
  RUNNING = 2,

  /**
   * The batch processing is done.
   *
   * @generated from enum value: DONE = 3;
   */
  DONE = 3,

  /**
   * The batch processing was cancelled.
   *
   * @generated from enum value: CANCELLED = 4;
   */
  CANCELLED = 4,
}

/**
 * Describes the enum google.cloud.vision.v1p4beta1.OperationMetadata.State.
 */
export const OperationMetadata_StateSchema: GenEnum<OperationMetadata_State> = /*@__PURE__*/
  enumDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 38, 0);

/**
 * A bucketized representation of likelihood, which is intended to give clients
 * highly stable results across model upgrades.
 *
 * @generated from enum google.cloud.vision.v1p4beta1.Likelihood
 */
export enum Likelihood {
  /**
   * Unknown likelihood.
   *
   * @generated from enum value: UNKNOWN = 0;
   */
  UNKNOWN = 0,

  /**
   * It is very unlikely.
   *
   * @generated from enum value: VERY_UNLIKELY = 1;
   */
  VERY_UNLIKELY = 1,

  /**
   * It is unlikely.
   *
   * @generated from enum value: UNLIKELY = 2;
   */
  UNLIKELY = 2,

  /**
   * It is possible.
   *
   * @generated from enum value: POSSIBLE = 3;
   */
  POSSIBLE = 3,

  /**
   * It is likely.
   *
   * @generated from enum value: LIKELY = 4;
   */
  LIKELY = 4,

  /**
   * It is very likely.
   *
   * @generated from enum value: VERY_LIKELY = 5;
   */
  VERY_LIKELY = 5,
}

/**
 * Describes the enum google.cloud.vision.v1p4beta1.Likelihood.
 */
export const LikelihoodSchema: GenEnum<Likelihood> = /*@__PURE__*/
  enumDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 0);

/**
 * Service that performs Google Cloud Vision API detection tasks over client
 * images, such as face, landmark, logo, label, and text detection. The
 * ImageAnnotator service returns detected entities from the images.
 *
 * @generated from service google.cloud.vision.v1p4beta1.ImageAnnotator
 */
export const ImageAnnotator: GenService<{
  /**
   * Run image detection and annotation for a batch of images.
   *
   * @generated from rpc google.cloud.vision.v1p4beta1.ImageAnnotator.BatchAnnotateImages
   */
  batchAnnotateImages: {
    methodKind: "unary";
    input: typeof BatchAnnotateImagesRequestSchema;
    output: typeof BatchAnnotateImagesResponseSchema;
  },
  /**
   * Service that performs image detection and annotation for a batch of files.
   * Now only "application/pdf", "image/tiff" and "image/gif" are supported.
   *
   * This service will extract at most 5 (customers can specify which 5 in
   * AnnotateFileRequest.pages) frames (gif) or pages (pdf or tiff) from each
   * file provided and perform detection and annotation for each image
   * extracted.
   *
   * @generated from rpc google.cloud.vision.v1p4beta1.ImageAnnotator.BatchAnnotateFiles
   */
  batchAnnotateFiles: {
    methodKind: "unary";
    input: typeof BatchAnnotateFilesRequestSchema;
    output: typeof BatchAnnotateFilesResponseSchema;
  },
  /**
   * Run asynchronous image detection and annotation for a list of images.
   *
   * Progress and results can be retrieved through the
   * `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `OperationMetadata` (metadata).
   * `Operation.response` contains `AsyncBatchAnnotateImagesResponse` (results).
   *
   * This service will write image annotation outputs to json files in customer
   * GCS bucket, each json file containing BatchAnnotateImagesResponse proto.
   *
   * @generated from rpc google.cloud.vision.v1p4beta1.ImageAnnotator.AsyncBatchAnnotateImages
   */
  asyncBatchAnnotateImages: {
    methodKind: "unary";
    input: typeof AsyncBatchAnnotateImagesRequestSchema;
    output: typeof OperationSchema;
  },
  /**
   * Run asynchronous image detection and annotation for a list of generic
   * files, such as PDF files, which may contain multiple pages and multiple
   * images per page. Progress and results can be retrieved through the
   * `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `OperationMetadata` (metadata).
   * `Operation.response` contains `AsyncBatchAnnotateFilesResponse` (results).
   *
   * @generated from rpc google.cloud.vision.v1p4beta1.ImageAnnotator.AsyncBatchAnnotateFiles
   */
  asyncBatchAnnotateFiles: {
    methodKind: "unary";
    input: typeof AsyncBatchAnnotateFilesRequestSchema;
    output: typeof OperationSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_vision_v1p4beta1_image_annotator, 0);

