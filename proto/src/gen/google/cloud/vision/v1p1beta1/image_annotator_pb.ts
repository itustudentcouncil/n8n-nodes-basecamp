// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/vision/v1p1beta1/image_annotator.proto (package google.cloud.vision.v1p1beta1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import type { BoundingPoly, Position } from "./geometry_pb";
import { file_google_cloud_vision_v1p1beta1_geometry } from "./geometry_pb";
import type { TextAnnotation } from "./text_annotation_pb";
import { file_google_cloud_vision_v1p1beta1_text_annotation } from "./text_annotation_pb";
import type { WebDetection } from "./web_detection_pb";
import { file_google_cloud_vision_v1p1beta1_web_detection } from "./web_detection_pb";
import type { Status } from "../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../rpc/status_pb";
import type { Color } from "../../../type/color_pb";
import { file_google_type_color } from "../../../type/color_pb";
import type { LatLng } from "../../../type/latlng_pb";
import { file_google_type_latlng } from "../../../type/latlng_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/vision/v1p1beta1/image_annotator.proto.
 */
export const file_google_cloud_vision_v1p1beta1_image_annotator: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_cloud_vision_v1p1beta1_geometry, file_google_cloud_vision_v1p1beta1_text_annotation, file_google_cloud_vision_v1p1beta1_web_detection, file_google_rpc_status, file_google_type_color, file_google_type_latlng]);

/**
 * Users describe the type of Google Cloud Vision API tasks to perform over
 * images by using *Feature*s. Each Feature indicates a type of image
 * detection task to perform. Features encode the Cloud Vision API
 * vertical to operate on and the number of top-scoring results to return.
 *
 * @generated from message google.cloud.vision.v1p1beta1.Feature
 */
export type Feature = Message<"google.cloud.vision.v1p1beta1.Feature"> & {
  /**
   * The feature type.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.Feature.Type type = 1;
   */
  type: Feature_Type;

  /**
   * Maximum number of results of this type.
   *
   * @generated from field: int32 max_results = 2;
   */
  maxResults: number;

  /**
   * Model to use for the feature.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest". `DOCUMENT_TEXT_DETECTION` and `TEXT_DETECTION` also
   * support "builtin/weekly" for the bleeding edge release updated weekly.
   *
   * @generated from field: string model = 3;
   */
  model: string;
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.Feature.
 * Use `create(FeatureSchema)` to create a new message.
 */
export const FeatureSchema: GenMessage<Feature> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 0);

/**
 * Type of image feature.
 *
 * @generated from enum google.cloud.vision.v1p1beta1.Feature.Type
 */
export enum Feature_Type {
  /**
   * Unspecified feature type.
   *
   * @generated from enum value: TYPE_UNSPECIFIED = 0;
   */
  TYPE_UNSPECIFIED = 0,

  /**
   * Run face detection.
   *
   * @generated from enum value: FACE_DETECTION = 1;
   */
  FACE_DETECTION = 1,

  /**
   * Run landmark detection.
   *
   * @generated from enum value: LANDMARK_DETECTION = 2;
   */
  LANDMARK_DETECTION = 2,

  /**
   * Run logo detection.
   *
   * @generated from enum value: LOGO_DETECTION = 3;
   */
  LOGO_DETECTION = 3,

  /**
   * Run label detection.
   *
   * @generated from enum value: LABEL_DETECTION = 4;
   */
  LABEL_DETECTION = 4,

  /**
   * Run OCR.
   *
   * @generated from enum value: TEXT_DETECTION = 5;
   */
  TEXT_DETECTION = 5,

  /**
   * Run dense text document OCR. Takes precedence when both
   * DOCUMENT_TEXT_DETECTION and TEXT_DETECTION are present.
   *
   * @generated from enum value: DOCUMENT_TEXT_DETECTION = 11;
   */
  DOCUMENT_TEXT_DETECTION = 11,

  /**
   * Run computer vision models to compute image safe-search properties.
   *
   * @generated from enum value: SAFE_SEARCH_DETECTION = 6;
   */
  SAFE_SEARCH_DETECTION = 6,

  /**
   * Compute a set of image properties, such as the image's dominant colors.
   *
   * @generated from enum value: IMAGE_PROPERTIES = 7;
   */
  IMAGE_PROPERTIES = 7,

  /**
   * Run crop hints.
   *
   * @generated from enum value: CROP_HINTS = 9;
   */
  CROP_HINTS = 9,

  /**
   * Run web detection.
   *
   * @generated from enum value: WEB_DETECTION = 10;
   */
  WEB_DETECTION = 10,
}

/**
 * Describes the enum google.cloud.vision.v1p1beta1.Feature.Type.
 */
export const Feature_TypeSchema: GenEnum<Feature_Type> = /*@__PURE__*/
  enumDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 0, 0);

/**
 * External image source (Google Cloud Storage image location).
 *
 * @generated from message google.cloud.vision.v1p1beta1.ImageSource
 */
export type ImageSource = Message<"google.cloud.vision.v1p1beta1.ImageSource"> & {
  /**
   * NOTE: For new code `image_uri` below is preferred.
   * Google Cloud Storage image URI, which must be in the following form:
   * `gs://bucket_name/object_name` (for details, see
   * [Google Cloud Storage Request
   * URIs](https://cloud.google.com/storage/docs/reference-uris)).
   * NOTE: Cloud Storage object versioning is not supported.
   *
   * @generated from field: string gcs_image_uri = 1;
   */
  gcsImageUri: string;

  /**
   * Image URI which supports:
   * 1) Google Cloud Storage image URI, which must be in the following form:
   * `gs://bucket_name/object_name` (for details, see
   * [Google Cloud Storage Request
   * URIs](https://cloud.google.com/storage/docs/reference-uris)).
   * NOTE: Cloud Storage object versioning is not supported.
   * 2) Publicly accessible image HTTP/HTTPS URL.
   * This is preferred over the legacy `gcs_image_uri` above. When both
   * `gcs_image_uri` and `image_uri` are specified, `image_uri` takes
   * precedence.
   *
   * @generated from field: string image_uri = 2;
   */
  imageUri: string;
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.ImageSource.
 * Use `create(ImageSourceSchema)` to create a new message.
 */
export const ImageSourceSchema: GenMessage<ImageSource> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 1);

/**
 * Client image to perform Google Cloud Vision API tasks over.
 *
 * @generated from message google.cloud.vision.v1p1beta1.Image
 */
export type Image = Message<"google.cloud.vision.v1p1beta1.Image"> & {
  /**
   * Image content, represented as a stream of bytes.
   * Note: as with all `bytes` fields, protobuffers use a pure binary
   * representation, whereas JSON representations use base64.
   *
   * @generated from field: bytes content = 1;
   */
  content: Uint8Array;

  /**
   * Google Cloud Storage image location. If both `content` and `source`
   * are provided for an image, `content` takes precedence and is
   * used to perform the image annotation request.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.ImageSource source = 2;
   */
  source?: ImageSource;
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.Image.
 * Use `create(ImageSchema)` to create a new message.
 */
export const ImageSchema: GenMessage<Image> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 2);

/**
 * A face annotation object contains the results of face detection.
 *
 * @generated from message google.cloud.vision.v1p1beta1.FaceAnnotation
 */
export type FaceAnnotation = Message<"google.cloud.vision.v1p1beta1.FaceAnnotation"> & {
  /**
   * The bounding polygon around the face. The coordinates of the bounding box
   * are in the original image's scale, as returned in `ImageParams`.
   * The bounding box is computed to "frame" the face in accordance with human
   * expectations. It is based on the landmarker results.
   * Note that one or more x and/or y coordinates may not be generated in the
   * `BoundingPoly` (the polygon will be unbounded) if only a partial face
   * appears in the image to be annotated.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.BoundingPoly bounding_poly = 1;
   */
  boundingPoly?: BoundingPoly;

  /**
   * The `fd_bounding_poly` bounding polygon is tighter than the
   * `boundingPoly`, and encloses only the skin part of the face. Typically, it
   * is used to eliminate the face from any image analysis that detects the
   * "amount of skin" visible in an image. It is not based on the
   * landmarker results, only on the initial face detection, hence
   * the <code>fd</code> (face detection) prefix.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.BoundingPoly fd_bounding_poly = 2;
   */
  fdBoundingPoly?: BoundingPoly;

  /**
   * Detected face landmarks.
   *
   * @generated from field: repeated google.cloud.vision.v1p1beta1.FaceAnnotation.Landmark landmarks = 3;
   */
  landmarks: FaceAnnotation_Landmark[];

  /**
   * Roll angle, which indicates the amount of clockwise/anti-clockwise rotation
   * of the face relative to the image vertical about the axis perpendicular to
   * the face. Range [-180,180].
   *
   * @generated from field: float roll_angle = 4;
   */
  rollAngle: number;

  /**
   * Yaw angle, which indicates the leftward/rightward angle that the face is
   * pointing relative to the vertical plane perpendicular to the image. Range
   * [-180,180].
   *
   * @generated from field: float pan_angle = 5;
   */
  panAngle: number;

  /**
   * Pitch angle, which indicates the upwards/downwards angle that the face is
   * pointing relative to the image's horizontal plane. Range [-180,180].
   *
   * @generated from field: float tilt_angle = 6;
   */
  tiltAngle: number;

  /**
   * Detection confidence. Range [0, 1].
   *
   * @generated from field: float detection_confidence = 7;
   */
  detectionConfidence: number;

  /**
   * Face landmarking confidence. Range [0, 1].
   *
   * @generated from field: float landmarking_confidence = 8;
   */
  landmarkingConfidence: number;

  /**
   * Joy likelihood.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.Likelihood joy_likelihood = 9;
   */
  joyLikelihood: Likelihood;

  /**
   * Sorrow likelihood.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.Likelihood sorrow_likelihood = 10;
   */
  sorrowLikelihood: Likelihood;

  /**
   * Anger likelihood.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.Likelihood anger_likelihood = 11;
   */
  angerLikelihood: Likelihood;

  /**
   * Surprise likelihood.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.Likelihood surprise_likelihood = 12;
   */
  surpriseLikelihood: Likelihood;

  /**
   * Under-exposed likelihood.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.Likelihood under_exposed_likelihood = 13;
   */
  underExposedLikelihood: Likelihood;

  /**
   * Blurred likelihood.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.Likelihood blurred_likelihood = 14;
   */
  blurredLikelihood: Likelihood;

  /**
   * Headwear likelihood.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.Likelihood headwear_likelihood = 15;
   */
  headwearLikelihood: Likelihood;
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.FaceAnnotation.
 * Use `create(FaceAnnotationSchema)` to create a new message.
 */
export const FaceAnnotationSchema: GenMessage<FaceAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 3);

/**
 * A face-specific landmark (for example, a face feature).
 *
 * @generated from message google.cloud.vision.v1p1beta1.FaceAnnotation.Landmark
 */
export type FaceAnnotation_Landmark = Message<"google.cloud.vision.v1p1beta1.FaceAnnotation.Landmark"> & {
  /**
   * Face landmark type.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.FaceAnnotation.Landmark.Type type = 3;
   */
  type: FaceAnnotation_Landmark_Type;

  /**
   * Face landmark position.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.Position position = 4;
   */
  position?: Position;
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.FaceAnnotation.Landmark.
 * Use `create(FaceAnnotation_LandmarkSchema)` to create a new message.
 */
export const FaceAnnotation_LandmarkSchema: GenMessage<FaceAnnotation_Landmark> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 3, 0);

/**
 * Face landmark (feature) type.
 * Left and right are defined from the vantage of the viewer of the image
 * without considering mirror projections typical of photos. So, `LEFT_EYE`,
 * typically, is the person's right eye.
 *
 * @generated from enum google.cloud.vision.v1p1beta1.FaceAnnotation.Landmark.Type
 */
export enum FaceAnnotation_Landmark_Type {
  /**
   * Unknown face landmark detected. Should not be filled.
   *
   * @generated from enum value: UNKNOWN_LANDMARK = 0;
   */
  UNKNOWN_LANDMARK = 0,

  /**
   * Left eye.
   *
   * @generated from enum value: LEFT_EYE = 1;
   */
  LEFT_EYE = 1,

  /**
   * Right eye.
   *
   * @generated from enum value: RIGHT_EYE = 2;
   */
  RIGHT_EYE = 2,

  /**
   * Left of left eyebrow.
   *
   * @generated from enum value: LEFT_OF_LEFT_EYEBROW = 3;
   */
  LEFT_OF_LEFT_EYEBROW = 3,

  /**
   * Right of left eyebrow.
   *
   * @generated from enum value: RIGHT_OF_LEFT_EYEBROW = 4;
   */
  RIGHT_OF_LEFT_EYEBROW = 4,

  /**
   * Left of right eyebrow.
   *
   * @generated from enum value: LEFT_OF_RIGHT_EYEBROW = 5;
   */
  LEFT_OF_RIGHT_EYEBROW = 5,

  /**
   * Right of right eyebrow.
   *
   * @generated from enum value: RIGHT_OF_RIGHT_EYEBROW = 6;
   */
  RIGHT_OF_RIGHT_EYEBROW = 6,

  /**
   * Midpoint between eyes.
   *
   * @generated from enum value: MIDPOINT_BETWEEN_EYES = 7;
   */
  MIDPOINT_BETWEEN_EYES = 7,

  /**
   * Nose tip.
   *
   * @generated from enum value: NOSE_TIP = 8;
   */
  NOSE_TIP = 8,

  /**
   * Upper lip.
   *
   * @generated from enum value: UPPER_LIP = 9;
   */
  UPPER_LIP = 9,

  /**
   * Lower lip.
   *
   * @generated from enum value: LOWER_LIP = 10;
   */
  LOWER_LIP = 10,

  /**
   * Mouth left.
   *
   * @generated from enum value: MOUTH_LEFT = 11;
   */
  MOUTH_LEFT = 11,

  /**
   * Mouth right.
   *
   * @generated from enum value: MOUTH_RIGHT = 12;
   */
  MOUTH_RIGHT = 12,

  /**
   * Mouth center.
   *
   * @generated from enum value: MOUTH_CENTER = 13;
   */
  MOUTH_CENTER = 13,

  /**
   * Nose, bottom right.
   *
   * @generated from enum value: NOSE_BOTTOM_RIGHT = 14;
   */
  NOSE_BOTTOM_RIGHT = 14,

  /**
   * Nose, bottom left.
   *
   * @generated from enum value: NOSE_BOTTOM_LEFT = 15;
   */
  NOSE_BOTTOM_LEFT = 15,

  /**
   * Nose, bottom center.
   *
   * @generated from enum value: NOSE_BOTTOM_CENTER = 16;
   */
  NOSE_BOTTOM_CENTER = 16,

  /**
   * Left eye, top boundary.
   *
   * @generated from enum value: LEFT_EYE_TOP_BOUNDARY = 17;
   */
  LEFT_EYE_TOP_BOUNDARY = 17,

  /**
   * Left eye, right corner.
   *
   * @generated from enum value: LEFT_EYE_RIGHT_CORNER = 18;
   */
  LEFT_EYE_RIGHT_CORNER = 18,

  /**
   * Left eye, bottom boundary.
   *
   * @generated from enum value: LEFT_EYE_BOTTOM_BOUNDARY = 19;
   */
  LEFT_EYE_BOTTOM_BOUNDARY = 19,

  /**
   * Left eye, left corner.
   *
   * @generated from enum value: LEFT_EYE_LEFT_CORNER = 20;
   */
  LEFT_EYE_LEFT_CORNER = 20,

  /**
   * Right eye, top boundary.
   *
   * @generated from enum value: RIGHT_EYE_TOP_BOUNDARY = 21;
   */
  RIGHT_EYE_TOP_BOUNDARY = 21,

  /**
   * Right eye, right corner.
   *
   * @generated from enum value: RIGHT_EYE_RIGHT_CORNER = 22;
   */
  RIGHT_EYE_RIGHT_CORNER = 22,

  /**
   * Right eye, bottom boundary.
   *
   * @generated from enum value: RIGHT_EYE_BOTTOM_BOUNDARY = 23;
   */
  RIGHT_EYE_BOTTOM_BOUNDARY = 23,

  /**
   * Right eye, left corner.
   *
   * @generated from enum value: RIGHT_EYE_LEFT_CORNER = 24;
   */
  RIGHT_EYE_LEFT_CORNER = 24,

  /**
   * Left eyebrow, upper midpoint.
   *
   * @generated from enum value: LEFT_EYEBROW_UPPER_MIDPOINT = 25;
   */
  LEFT_EYEBROW_UPPER_MIDPOINT = 25,

  /**
   * Right eyebrow, upper midpoint.
   *
   * @generated from enum value: RIGHT_EYEBROW_UPPER_MIDPOINT = 26;
   */
  RIGHT_EYEBROW_UPPER_MIDPOINT = 26,

  /**
   * Left ear tragion.
   *
   * @generated from enum value: LEFT_EAR_TRAGION = 27;
   */
  LEFT_EAR_TRAGION = 27,

  /**
   * Right ear tragion.
   *
   * @generated from enum value: RIGHT_EAR_TRAGION = 28;
   */
  RIGHT_EAR_TRAGION = 28,

  /**
   * Left eye pupil.
   *
   * @generated from enum value: LEFT_EYE_PUPIL = 29;
   */
  LEFT_EYE_PUPIL = 29,

  /**
   * Right eye pupil.
   *
   * @generated from enum value: RIGHT_EYE_PUPIL = 30;
   */
  RIGHT_EYE_PUPIL = 30,

  /**
   * Forehead glabella.
   *
   * @generated from enum value: FOREHEAD_GLABELLA = 31;
   */
  FOREHEAD_GLABELLA = 31,

  /**
   * Chin gnathion.
   *
   * @generated from enum value: CHIN_GNATHION = 32;
   */
  CHIN_GNATHION = 32,

  /**
   * Chin left gonion.
   *
   * @generated from enum value: CHIN_LEFT_GONION = 33;
   */
  CHIN_LEFT_GONION = 33,

  /**
   * Chin right gonion.
   *
   * @generated from enum value: CHIN_RIGHT_GONION = 34;
   */
  CHIN_RIGHT_GONION = 34,
}

/**
 * Describes the enum google.cloud.vision.v1p1beta1.FaceAnnotation.Landmark.Type.
 */
export const FaceAnnotation_Landmark_TypeSchema: GenEnum<FaceAnnotation_Landmark_Type> = /*@__PURE__*/
  enumDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 3, 0, 0);

/**
 * Detected entity location information.
 *
 * @generated from message google.cloud.vision.v1p1beta1.LocationInfo
 */
export type LocationInfo = Message<"google.cloud.vision.v1p1beta1.LocationInfo"> & {
  /**
   * lat/long location coordinates.
   *
   * @generated from field: google.type.LatLng lat_lng = 1;
   */
  latLng?: LatLng;
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.LocationInfo.
 * Use `create(LocationInfoSchema)` to create a new message.
 */
export const LocationInfoSchema: GenMessage<LocationInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 4);

/**
 * A `Property` consists of a user-supplied name/value pair.
 *
 * @generated from message google.cloud.vision.v1p1beta1.Property
 */
export type Property = Message<"google.cloud.vision.v1p1beta1.Property"> & {
  /**
   * Name of the property.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Value of the property.
   *
   * @generated from field: string value = 2;
   */
  value: string;

  /**
   * Value of numeric properties.
   *
   * @generated from field: uint64 uint64_value = 3;
   */
  uint64Value: bigint;
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.Property.
 * Use `create(PropertySchema)` to create a new message.
 */
export const PropertySchema: GenMessage<Property> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 5);

/**
 * Set of detected entity features.
 *
 * @generated from message google.cloud.vision.v1p1beta1.EntityAnnotation
 */
export type EntityAnnotation = Message<"google.cloud.vision.v1p1beta1.EntityAnnotation"> & {
  /**
   * Opaque entity ID. Some IDs may be available in
   * [Google Knowledge Graph Search
   * API](https://developers.google.com/knowledge-graph/).
   *
   * @generated from field: string mid = 1;
   */
  mid: string;

  /**
   * The language code for the locale in which the entity textual
   * `description` is expressed.
   *
   * @generated from field: string locale = 2;
   */
  locale: string;

  /**
   * Entity textual description, expressed in its `locale` language.
   *
   * @generated from field: string description = 3;
   */
  description: string;

  /**
   * Overall score of the result. Range [0, 1].
   *
   * @generated from field: float score = 4;
   */
  score: number;

  /**
   * The accuracy of the entity detection in an image.
   * For example, for an image in which the "Eiffel Tower" entity is detected,
   * this field represents the confidence that there is a tower in the query
   * image. Range [0, 1].
   *
   * @generated from field: float confidence = 5;
   */
  confidence: number;

  /**
   * The relevancy of the ICA (Image Content Annotation) label to the
   * image. For example, the relevancy of "tower" is likely higher to an image
   * containing the detected "Eiffel Tower" than to an image containing a
   * detected distant towering building, even though the confidence that
   * there is a tower in each image may be the same. Range [0, 1].
   *
   * @generated from field: float topicality = 6;
   */
  topicality: number;

  /**
   * Image region to which this entity belongs. Not produced
   * for `LABEL_DETECTION` features.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.BoundingPoly bounding_poly = 7;
   */
  boundingPoly?: BoundingPoly;

  /**
   * The location information for the detected entity. Multiple
   * `LocationInfo` elements can be present because one location may
   * indicate the location of the scene in the image, and another location
   * may indicate the location of the place where the image was taken.
   * Location information is usually present for landmarks.
   *
   * @generated from field: repeated google.cloud.vision.v1p1beta1.LocationInfo locations = 8;
   */
  locations: LocationInfo[];

  /**
   * Some entities may have optional user-supplied `Property` (name/value)
   * fields, such a score or string that qualifies the entity.
   *
   * @generated from field: repeated google.cloud.vision.v1p1beta1.Property properties = 9;
   */
  properties: Property[];
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.EntityAnnotation.
 * Use `create(EntityAnnotationSchema)` to create a new message.
 */
export const EntityAnnotationSchema: GenMessage<EntityAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 6);

/**
 * Set of features pertaining to the image, computed by computer vision
 * methods over safe-search verticals (for example, adult, spoof, medical,
 * violence).
 *
 * @generated from message google.cloud.vision.v1p1beta1.SafeSearchAnnotation
 */
export type SafeSearchAnnotation = Message<"google.cloud.vision.v1p1beta1.SafeSearchAnnotation"> & {
  /**
   * Represents the adult content likelihood for the image. Adult content may
   * contain elements such as nudity, pornographic images or cartoons, or
   * sexual activities.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.Likelihood adult = 1;
   */
  adult: Likelihood;

  /**
   * Spoof likelihood. The likelihood that an modification
   * was made to the image's canonical version to make it appear
   * funny or offensive.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.Likelihood spoof = 2;
   */
  spoof: Likelihood;

  /**
   * Likelihood that this is a medical image.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.Likelihood medical = 3;
   */
  medical: Likelihood;

  /**
   * Likelihood that this image contains violent content.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.Likelihood violence = 4;
   */
  violence: Likelihood;

  /**
   * Likelihood that the request image contains racy content. Racy content may
   * include (but is not limited to) skimpy or sheer clothing, strategically
   * covered nudity, lewd or provocative poses, or close-ups of sensitive
   * body areas.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.Likelihood racy = 9;
   */
  racy: Likelihood;
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.SafeSearchAnnotation.
 * Use `create(SafeSearchAnnotationSchema)` to create a new message.
 */
export const SafeSearchAnnotationSchema: GenMessage<SafeSearchAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 7);

/**
 * Rectangle determined by min and max `LatLng` pairs.
 *
 * @generated from message google.cloud.vision.v1p1beta1.LatLongRect
 */
export type LatLongRect = Message<"google.cloud.vision.v1p1beta1.LatLongRect"> & {
  /**
   * Min lat/long pair.
   *
   * @generated from field: google.type.LatLng min_lat_lng = 1;
   */
  minLatLng?: LatLng;

  /**
   * Max lat/long pair.
   *
   * @generated from field: google.type.LatLng max_lat_lng = 2;
   */
  maxLatLng?: LatLng;
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.LatLongRect.
 * Use `create(LatLongRectSchema)` to create a new message.
 */
export const LatLongRectSchema: GenMessage<LatLongRect> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 8);

/**
 * Color information consists of RGB channels, score, and the fraction of
 * the image that the color occupies in the image.
 *
 * @generated from message google.cloud.vision.v1p1beta1.ColorInfo
 */
export type ColorInfo = Message<"google.cloud.vision.v1p1beta1.ColorInfo"> & {
  /**
   * RGB components of the color.
   *
   * @generated from field: google.type.Color color = 1;
   */
  color?: Color;

  /**
   * Image-specific score for this color. Value in range [0, 1].
   *
   * @generated from field: float score = 2;
   */
  score: number;

  /**
   * The fraction of pixels the color occupies in the image.
   * Value in range [0, 1].
   *
   * @generated from field: float pixel_fraction = 3;
   */
  pixelFraction: number;
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.ColorInfo.
 * Use `create(ColorInfoSchema)` to create a new message.
 */
export const ColorInfoSchema: GenMessage<ColorInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 9);

/**
 * Set of dominant colors and their corresponding scores.
 *
 * @generated from message google.cloud.vision.v1p1beta1.DominantColorsAnnotation
 */
export type DominantColorsAnnotation = Message<"google.cloud.vision.v1p1beta1.DominantColorsAnnotation"> & {
  /**
   * RGB color values with their score and pixel fraction.
   *
   * @generated from field: repeated google.cloud.vision.v1p1beta1.ColorInfo colors = 1;
   */
  colors: ColorInfo[];
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.DominantColorsAnnotation.
 * Use `create(DominantColorsAnnotationSchema)` to create a new message.
 */
export const DominantColorsAnnotationSchema: GenMessage<DominantColorsAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 10);

/**
 * Stores image properties, such as dominant colors.
 *
 * @generated from message google.cloud.vision.v1p1beta1.ImageProperties
 */
export type ImageProperties = Message<"google.cloud.vision.v1p1beta1.ImageProperties"> & {
  /**
   * If present, dominant colors completed successfully.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.DominantColorsAnnotation dominant_colors = 1;
   */
  dominantColors?: DominantColorsAnnotation;
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.ImageProperties.
 * Use `create(ImagePropertiesSchema)` to create a new message.
 */
export const ImagePropertiesSchema: GenMessage<ImageProperties> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 11);

/**
 * Single crop hint that is used to generate a new crop when serving an image.
 *
 * @generated from message google.cloud.vision.v1p1beta1.CropHint
 */
export type CropHint = Message<"google.cloud.vision.v1p1beta1.CropHint"> & {
  /**
   * The bounding polygon for the crop region. The coordinates of the bounding
   * box are in the original image's scale, as returned in `ImageParams`.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.BoundingPoly bounding_poly = 1;
   */
  boundingPoly?: BoundingPoly;

  /**
   * Confidence of this being a salient region.  Range [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;

  /**
   * Fraction of importance of this salient region with respect to the original
   * image.
   *
   * @generated from field: float importance_fraction = 3;
   */
  importanceFraction: number;
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.CropHint.
 * Use `create(CropHintSchema)` to create a new message.
 */
export const CropHintSchema: GenMessage<CropHint> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 12);

/**
 * Set of crop hints that are used to generate new crops when serving images.
 *
 * @generated from message google.cloud.vision.v1p1beta1.CropHintsAnnotation
 */
export type CropHintsAnnotation = Message<"google.cloud.vision.v1p1beta1.CropHintsAnnotation"> & {
  /**
   * Crop hint results.
   *
   * @generated from field: repeated google.cloud.vision.v1p1beta1.CropHint crop_hints = 1;
   */
  cropHints: CropHint[];
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.CropHintsAnnotation.
 * Use `create(CropHintsAnnotationSchema)` to create a new message.
 */
export const CropHintsAnnotationSchema: GenMessage<CropHintsAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 13);

/**
 * Parameters for crop hints annotation request.
 *
 * @generated from message google.cloud.vision.v1p1beta1.CropHintsParams
 */
export type CropHintsParams = Message<"google.cloud.vision.v1p1beta1.CropHintsParams"> & {
  /**
   * Aspect ratios in floats, representing the ratio of the width to the height
   * of the image. For example, if the desired aspect ratio is 4/3, the
   * corresponding float value should be 1.33333.  If not specified, the
   * best possible crop is returned. The number of provided aspect ratios is
   * limited to a maximum of 16; any aspect ratios provided after the 16th are
   * ignored.
   *
   * @generated from field: repeated float aspect_ratios = 1;
   */
  aspectRatios: number[];
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.CropHintsParams.
 * Use `create(CropHintsParamsSchema)` to create a new message.
 */
export const CropHintsParamsSchema: GenMessage<CropHintsParams> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 14);

/**
 * Parameters for web detection request.
 *
 * @generated from message google.cloud.vision.v1p1beta1.WebDetectionParams
 */
export type WebDetectionParams = Message<"google.cloud.vision.v1p1beta1.WebDetectionParams"> & {
  /**
   * Whether to include results derived from the geo information in the image.
   *
   * @generated from field: bool include_geo_results = 2;
   */
  includeGeoResults: boolean;
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.WebDetectionParams.
 * Use `create(WebDetectionParamsSchema)` to create a new message.
 */
export const WebDetectionParamsSchema: GenMessage<WebDetectionParams> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 15);

/**
 * Parameters for text detections. This is used to control TEXT_DETECTION and
 * DOCUMENT_TEXT_DETECTION features.
 *
 * @generated from message google.cloud.vision.v1p1beta1.TextDetectionParams
 */
export type TextDetectionParams = Message<"google.cloud.vision.v1p1beta1.TextDetectionParams"> & {
  /**
   * By default, Cloud Vision API only includes confidence score for
   * DOCUMENT_TEXT_DETECTION result. Set the flag to true to include confidence
   * score for TEXT_DETECTION as well.
   *
   * @generated from field: bool enable_text_detection_confidence_score = 9;
   */
  enableTextDetectionConfidenceScore: boolean;

  /**
   * A list of advanced OCR options to fine-tune OCR behavior.
   *
   * @generated from field: repeated string advanced_ocr_options = 11;
   */
  advancedOcrOptions: string[];
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.TextDetectionParams.
 * Use `create(TextDetectionParamsSchema)` to create a new message.
 */
export const TextDetectionParamsSchema: GenMessage<TextDetectionParams> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 16);

/**
 * Image context and/or feature-specific parameters.
 *
 * @generated from message google.cloud.vision.v1p1beta1.ImageContext
 */
export type ImageContext = Message<"google.cloud.vision.v1p1beta1.ImageContext"> & {
  /**
   * lat/long rectangle that specifies the location of the image.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.LatLongRect lat_long_rect = 1;
   */
  latLongRect?: LatLongRect;

  /**
   * List of languages to use for TEXT_DETECTION. In most cases, an empty value
   * yields the best results since it enables automatic language detection. For
   * languages based on the Latin alphabet, setting `language_hints` is not
   * needed. In rare cases, when the language of the text in the image is known,
   * setting a hint will help get better results (although it will be a
   * significant hindrance if the hint is wrong). Text detection returns an
   * error if one or more of the specified languages is not one of the
   * [supported languages](https://cloud.google.com/vision/docs/languages).
   *
   * @generated from field: repeated string language_hints = 2;
   */
  languageHints: string[];

  /**
   * Parameters for crop hints annotation request.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.CropHintsParams crop_hints_params = 4;
   */
  cropHintsParams?: CropHintsParams;

  /**
   * Parameters for web detection.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.WebDetectionParams web_detection_params = 6;
   */
  webDetectionParams?: WebDetectionParams;

  /**
   * Parameters for text detection and document text detection.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.TextDetectionParams text_detection_params = 12;
   */
  textDetectionParams?: TextDetectionParams;
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.ImageContext.
 * Use `create(ImageContextSchema)` to create a new message.
 */
export const ImageContextSchema: GenMessage<ImageContext> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 17);

/**
 * Request for performing Google Cloud Vision API tasks over a user-provided
 * image, with user-requested features.
 *
 * @generated from message google.cloud.vision.v1p1beta1.AnnotateImageRequest
 */
export type AnnotateImageRequest = Message<"google.cloud.vision.v1p1beta1.AnnotateImageRequest"> & {
  /**
   * The image to be processed.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.Image image = 1;
   */
  image?: Image;

  /**
   * Requested features.
   *
   * @generated from field: repeated google.cloud.vision.v1p1beta1.Feature features = 2;
   */
  features: Feature[];

  /**
   * Additional context that may accompany the image.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.ImageContext image_context = 3;
   */
  imageContext?: ImageContext;
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.AnnotateImageRequest.
 * Use `create(AnnotateImageRequestSchema)` to create a new message.
 */
export const AnnotateImageRequestSchema: GenMessage<AnnotateImageRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 18);

/**
 * Response to an image annotation request.
 *
 * @generated from message google.cloud.vision.v1p1beta1.AnnotateImageResponse
 */
export type AnnotateImageResponse = Message<"google.cloud.vision.v1p1beta1.AnnotateImageResponse"> & {
  /**
   * If present, face detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1p1beta1.FaceAnnotation face_annotations = 1;
   */
  faceAnnotations: FaceAnnotation[];

  /**
   * If present, landmark detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1p1beta1.EntityAnnotation landmark_annotations = 2;
   */
  landmarkAnnotations: EntityAnnotation[];

  /**
   * If present, logo detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1p1beta1.EntityAnnotation logo_annotations = 3;
   */
  logoAnnotations: EntityAnnotation[];

  /**
   * If present, label detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1p1beta1.EntityAnnotation label_annotations = 4;
   */
  labelAnnotations: EntityAnnotation[];

  /**
   * If present, text (OCR) detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1p1beta1.EntityAnnotation text_annotations = 5;
   */
  textAnnotations: EntityAnnotation[];

  /**
   * If present, text (OCR) detection or document (OCR) text detection has
   * completed successfully.
   * This annotation provides the structural hierarchy for the OCR detected
   * text.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.TextAnnotation full_text_annotation = 12;
   */
  fullTextAnnotation?: TextAnnotation;

  /**
   * If present, safe-search annotation has completed successfully.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.SafeSearchAnnotation safe_search_annotation = 6;
   */
  safeSearchAnnotation?: SafeSearchAnnotation;

  /**
   * If present, image properties were extracted successfully.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.ImageProperties image_properties_annotation = 8;
   */
  imagePropertiesAnnotation?: ImageProperties;

  /**
   * If present, crop hints have completed successfully.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.CropHintsAnnotation crop_hints_annotation = 11;
   */
  cropHintsAnnotation?: CropHintsAnnotation;

  /**
   * If present, web detection has completed successfully.
   *
   * @generated from field: google.cloud.vision.v1p1beta1.WebDetection web_detection = 13;
   */
  webDetection?: WebDetection;

  /**
   * If set, represents the error message for the operation.
   * Note that filled-in image annotations are guaranteed to be
   * correct, even when `error` is set.
   *
   * @generated from field: google.rpc.Status error = 9;
   */
  error?: Status;
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.AnnotateImageResponse.
 * Use `create(AnnotateImageResponseSchema)` to create a new message.
 */
export const AnnotateImageResponseSchema: GenMessage<AnnotateImageResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 19);

/**
 * Multiple image annotation requests are batched into a single service call.
 *
 * @generated from message google.cloud.vision.v1p1beta1.BatchAnnotateImagesRequest
 */
export type BatchAnnotateImagesRequest = Message<"google.cloud.vision.v1p1beta1.BatchAnnotateImagesRequest"> & {
  /**
   * Required. Individual image annotation requests for this batch.
   *
   * @generated from field: repeated google.cloud.vision.v1p1beta1.AnnotateImageRequest requests = 1;
   */
  requests: AnnotateImageRequest[];
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.BatchAnnotateImagesRequest.
 * Use `create(BatchAnnotateImagesRequestSchema)` to create a new message.
 */
export const BatchAnnotateImagesRequestSchema: GenMessage<BatchAnnotateImagesRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 20);

/**
 * Response to a batch image annotation request.
 *
 * @generated from message google.cloud.vision.v1p1beta1.BatchAnnotateImagesResponse
 */
export type BatchAnnotateImagesResponse = Message<"google.cloud.vision.v1p1beta1.BatchAnnotateImagesResponse"> & {
  /**
   * Individual responses to image annotation requests within the batch.
   *
   * @generated from field: repeated google.cloud.vision.v1p1beta1.AnnotateImageResponse responses = 1;
   */
  responses: AnnotateImageResponse[];
};

/**
 * Describes the message google.cloud.vision.v1p1beta1.BatchAnnotateImagesResponse.
 * Use `create(BatchAnnotateImagesResponseSchema)` to create a new message.
 */
export const BatchAnnotateImagesResponseSchema: GenMessage<BatchAnnotateImagesResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 21);

/**
 * A bucketized representation of likelihood, which is intended to give clients
 * highly stable results across model upgrades.
 *
 * @generated from enum google.cloud.vision.v1p1beta1.Likelihood
 */
export enum Likelihood {
  /**
   * Unknown likelihood.
   *
   * @generated from enum value: UNKNOWN = 0;
   */
  UNKNOWN = 0,

  /**
   * It is very unlikely that the image belongs to the specified vertical.
   *
   * @generated from enum value: VERY_UNLIKELY = 1;
   */
  VERY_UNLIKELY = 1,

  /**
   * It is unlikely that the image belongs to the specified vertical.
   *
   * @generated from enum value: UNLIKELY = 2;
   */
  UNLIKELY = 2,

  /**
   * It is possible that the image belongs to the specified vertical.
   *
   * @generated from enum value: POSSIBLE = 3;
   */
  POSSIBLE = 3,

  /**
   * It is likely that the image belongs to the specified vertical.
   *
   * @generated from enum value: LIKELY = 4;
   */
  LIKELY = 4,

  /**
   * It is very likely that the image belongs to the specified vertical.
   *
   * @generated from enum value: VERY_LIKELY = 5;
   */
  VERY_LIKELY = 5,
}

/**
 * Describes the enum google.cloud.vision.v1p1beta1.Likelihood.
 */
export const LikelihoodSchema: GenEnum<Likelihood> = /*@__PURE__*/
  enumDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 0);

/**
 * Service that performs Google Cloud Vision API detection tasks over client
 * images, such as face, landmark, logo, label, and text detection. The
 * ImageAnnotator service returns detected entities from the images.
 *
 * @generated from service google.cloud.vision.v1p1beta1.ImageAnnotator
 */
export const ImageAnnotator: GenService<{
  /**
   * Run image detection and annotation for a batch of images.
   *
   * @generated from rpc google.cloud.vision.v1p1beta1.ImageAnnotator.BatchAnnotateImages
   */
  batchAnnotateImages: {
    methodKind: "unary";
    input: typeof BatchAnnotateImagesRequestSchema;
    output: typeof BatchAnnotateImagesResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_vision_v1p1beta1_image_annotator, 0);

