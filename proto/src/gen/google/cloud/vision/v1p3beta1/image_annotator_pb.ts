// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/vision/v1p3beta1/image_annotator.proto (package google.cloud.vision.v1p3beta1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import type { BoundingPoly, Position } from "./geometry_pb";
import { file_google_cloud_vision_v1p3beta1_geometry } from "./geometry_pb";
import type { ProductSearchParams, ProductSearchResults } from "./product_search_pb";
import { file_google_cloud_vision_v1p3beta1_product_search } from "./product_search_pb";
import type { TextAnnotation } from "./text_annotation_pb";
import { file_google_cloud_vision_v1p3beta1_text_annotation } from "./text_annotation_pb";
import type { WebDetection } from "./web_detection_pb";
import { file_google_cloud_vision_v1p3beta1_web_detection } from "./web_detection_pb";
import type { OperationSchema } from "../../../longrunning/operations_pb";
import { file_google_longrunning_operations } from "../../../longrunning/operations_pb";
import type { Timestamp } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_timestamp } from "@bufbuild/protobuf/wkt";
import type { Status } from "../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../rpc/status_pb";
import type { Color } from "../../../type/color_pb";
import { file_google_type_color } from "../../../type/color_pb";
import type { LatLng } from "../../../type/latlng_pb";
import { file_google_type_latlng } from "../../../type/latlng_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/vision/v1p3beta1/image_annotator.proto.
 */
export const file_google_cloud_vision_v1p3beta1_image_annotator: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_cloud_vision_v1p3beta1_geometry, file_google_cloud_vision_v1p3beta1_product_search, file_google_cloud_vision_v1p3beta1_text_annotation, file_google_cloud_vision_v1p3beta1_web_detection, file_google_longrunning_operations, file_google_protobuf_timestamp, file_google_rpc_status, file_google_type_color, file_google_type_latlng]);

/**
 * The type of Google Cloud Vision API detection to perform, and the maximum
 * number of results to return for that type. Multiple `Feature` objects can
 * be specified in the `features` list.
 *
 * @generated from message google.cloud.vision.v1p3beta1.Feature
 */
export type Feature = Message<"google.cloud.vision.v1p3beta1.Feature"> & {
  /**
   * The feature type.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.Feature.Type type = 1;
   */
  type: Feature_Type;

  /**
   * Maximum number of results of this type. Does not apply to
   * `TEXT_DETECTION`, `DOCUMENT_TEXT_DETECTION`, or `CROP_HINTS`.
   *
   * @generated from field: int32 max_results = 2;
   */
  maxResults: number;

  /**
   * Model to use for the feature.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest". `DOCUMENT_TEXT_DETECTION` and `TEXT_DETECTION` also
   * support "builtin/weekly" for the bleeding edge release updated weekly.
   *
   * @generated from field: string model = 3;
   */
  model: string;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.Feature.
 * Use `create(FeatureSchema)` to create a new message.
 */
export const FeatureSchema: GenMessage<Feature> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 0);

/**
 * Type of Google Cloud Vision API feature to be extracted.
 *
 * @generated from enum google.cloud.vision.v1p3beta1.Feature.Type
 */
export enum Feature_Type {
  /**
   * Unspecified feature type.
   *
   * @generated from enum value: TYPE_UNSPECIFIED = 0;
   */
  TYPE_UNSPECIFIED = 0,

  /**
   * Run face detection.
   *
   * @generated from enum value: FACE_DETECTION = 1;
   */
  FACE_DETECTION = 1,

  /**
   * Run landmark detection.
   *
   * @generated from enum value: LANDMARK_DETECTION = 2;
   */
  LANDMARK_DETECTION = 2,

  /**
   * Run logo detection.
   *
   * @generated from enum value: LOGO_DETECTION = 3;
   */
  LOGO_DETECTION = 3,

  /**
   * Run label detection.
   *
   * @generated from enum value: LABEL_DETECTION = 4;
   */
  LABEL_DETECTION = 4,

  /**
   * Run text detection / optical character recognition (OCR). Text detection
   * is optimized for areas of text within a larger image; if the image is
   * a document, use `DOCUMENT_TEXT_DETECTION` instead.
   *
   * @generated from enum value: TEXT_DETECTION = 5;
   */
  TEXT_DETECTION = 5,

  /**
   * Run dense text document OCR. Takes precedence when both
   * `DOCUMENT_TEXT_DETECTION` and `TEXT_DETECTION` are present.
   *
   * @generated from enum value: DOCUMENT_TEXT_DETECTION = 11;
   */
  DOCUMENT_TEXT_DETECTION = 11,

  /**
   * Run Safe Search to detect potentially unsafe
   * or undesirable content.
   *
   * @generated from enum value: SAFE_SEARCH_DETECTION = 6;
   */
  SAFE_SEARCH_DETECTION = 6,

  /**
   * Compute a set of image properties, such as the
   * image's dominant colors.
   *
   * @generated from enum value: IMAGE_PROPERTIES = 7;
   */
  IMAGE_PROPERTIES = 7,

  /**
   * Run crop hints.
   *
   * @generated from enum value: CROP_HINTS = 9;
   */
  CROP_HINTS = 9,

  /**
   * Run web detection.
   *
   * @generated from enum value: WEB_DETECTION = 10;
   */
  WEB_DETECTION = 10,

  /**
   * Run Product Search.
   *
   * @generated from enum value: PRODUCT_SEARCH = 12;
   */
  PRODUCT_SEARCH = 12,

  /**
   * Run localizer for object detection.
   *
   * @generated from enum value: OBJECT_LOCALIZATION = 19;
   */
  OBJECT_LOCALIZATION = 19,
}

/**
 * Describes the enum google.cloud.vision.v1p3beta1.Feature.Type.
 */
export const Feature_TypeSchema: GenEnum<Feature_Type> = /*@__PURE__*/
  enumDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 0, 0);

/**
 * External image source (Google Cloud Storage or web URL image location).
 *
 * @generated from message google.cloud.vision.v1p3beta1.ImageSource
 */
export type ImageSource = Message<"google.cloud.vision.v1p3beta1.ImageSource"> & {
  /**
   * **Use `image_uri` instead.**
   *
   * The Google Cloud Storage  URI of the form
   * `gs://bucket_name/object_name`. Object versioning is not supported. See
   * [Google Cloud Storage Request
   * URIs](https://cloud.google.com/storage/docs/reference-uris) for more info.
   *
   * @generated from field: string gcs_image_uri = 1;
   */
  gcsImageUri: string;

  /**
   * The URI of the source image. Can be either:
   *
   * 1. A Google Cloud Storage URI of the form
   *    `gs://bucket_name/object_name`. Object versioning is not supported. See
   *    [Google Cloud Storage Request
   *    URIs](https://cloud.google.com/storage/docs/reference-uris) for more
   *    info.
   *
   * 2. A publicly-accessible image HTTP/HTTPS URL. When fetching images from
   *    HTTP/HTTPS URLs, Google cannot guarantee that the request will be
   *    completed. Your request may fail if the specified host denies the
   *    request (e.g. due to request throttling or DOS prevention), or if Google
   *    throttles requests to the site for abuse prevention. You should not
   *    depend on externally-hosted images for production applications.
   *
   * When both `gcs_image_uri` and `image_uri` are specified, `image_uri` takes
   * precedence.
   *
   * @generated from field: string image_uri = 2;
   */
  imageUri: string;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.ImageSource.
 * Use `create(ImageSourceSchema)` to create a new message.
 */
export const ImageSourceSchema: GenMessage<ImageSource> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 1);

/**
 * Client image to perform Google Cloud Vision API tasks over.
 *
 * @generated from message google.cloud.vision.v1p3beta1.Image
 */
export type Image = Message<"google.cloud.vision.v1p3beta1.Image"> & {
  /**
   * Image content, represented as a stream of bytes.
   * Note: As with all `bytes` fields, protobuffers use a pure binary
   * representation, whereas JSON representations use base64.
   *
   * @generated from field: bytes content = 1;
   */
  content: Uint8Array;

  /**
   * Google Cloud Storage image location, or publicly-accessible image
   * URL. If both `content` and `source` are provided for an image, `content`
   * takes precedence and is used to perform the image annotation request.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.ImageSource source = 2;
   */
  source?: ImageSource;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.Image.
 * Use `create(ImageSchema)` to create a new message.
 */
export const ImageSchema: GenMessage<Image> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 2);

/**
 * A face annotation object contains the results of face detection.
 *
 * @generated from message google.cloud.vision.v1p3beta1.FaceAnnotation
 */
export type FaceAnnotation = Message<"google.cloud.vision.v1p3beta1.FaceAnnotation"> & {
  /**
   * The bounding polygon around the face. The coordinates of the bounding box
   * are in the original image's scale, as returned in `ImageParams`.
   * The bounding box is computed to "frame" the face in accordance with human
   * expectations. It is based on the landmarker results.
   * Note that one or more x and/or y coordinates may not be generated in the
   * `BoundingPoly` (the polygon will be unbounded) if only a partial face
   * appears in the image to be annotated.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.BoundingPoly bounding_poly = 1;
   */
  boundingPoly?: BoundingPoly;

  /**
   * The `fd_bounding_poly` bounding polygon is tighter than the
   * `boundingPoly`, and encloses only the skin part of the face. Typically, it
   * is used to eliminate the face from any image analysis that detects the
   * "amount of skin" visible in an image. It is not based on the
   * landmarker results, only on the initial face detection, hence
   * the <code>fd</code> (face detection) prefix.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.BoundingPoly fd_bounding_poly = 2;
   */
  fdBoundingPoly?: BoundingPoly;

  /**
   * Detected face landmarks.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.FaceAnnotation.Landmark landmarks = 3;
   */
  landmarks: FaceAnnotation_Landmark[];

  /**
   * Roll angle, which indicates the amount of clockwise/anti-clockwise rotation
   * of the face relative to the image vertical about the axis perpendicular to
   * the face. Range [-180,180].
   *
   * @generated from field: float roll_angle = 4;
   */
  rollAngle: number;

  /**
   * Yaw angle, which indicates the leftward/rightward angle that the face is
   * pointing relative to the vertical plane perpendicular to the image. Range
   * [-180,180].
   *
   * @generated from field: float pan_angle = 5;
   */
  panAngle: number;

  /**
   * Pitch angle, which indicates the upwards/downwards angle that the face is
   * pointing relative to the image's horizontal plane. Range [-180,180].
   *
   * @generated from field: float tilt_angle = 6;
   */
  tiltAngle: number;

  /**
   * Detection confidence. Range [0, 1].
   *
   * @generated from field: float detection_confidence = 7;
   */
  detectionConfidence: number;

  /**
   * Face landmarking confidence. Range [0, 1].
   *
   * @generated from field: float landmarking_confidence = 8;
   */
  landmarkingConfidence: number;

  /**
   * Joy likelihood.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.Likelihood joy_likelihood = 9;
   */
  joyLikelihood: Likelihood;

  /**
   * Sorrow likelihood.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.Likelihood sorrow_likelihood = 10;
   */
  sorrowLikelihood: Likelihood;

  /**
   * Anger likelihood.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.Likelihood anger_likelihood = 11;
   */
  angerLikelihood: Likelihood;

  /**
   * Surprise likelihood.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.Likelihood surprise_likelihood = 12;
   */
  surpriseLikelihood: Likelihood;

  /**
   * Under-exposed likelihood.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.Likelihood under_exposed_likelihood = 13;
   */
  underExposedLikelihood: Likelihood;

  /**
   * Blurred likelihood.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.Likelihood blurred_likelihood = 14;
   */
  blurredLikelihood: Likelihood;

  /**
   * Headwear likelihood.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.Likelihood headwear_likelihood = 15;
   */
  headwearLikelihood: Likelihood;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.FaceAnnotation.
 * Use `create(FaceAnnotationSchema)` to create a new message.
 */
export const FaceAnnotationSchema: GenMessage<FaceAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 3);

/**
 * A face-specific landmark (for example, a face feature).
 *
 * @generated from message google.cloud.vision.v1p3beta1.FaceAnnotation.Landmark
 */
export type FaceAnnotation_Landmark = Message<"google.cloud.vision.v1p3beta1.FaceAnnotation.Landmark"> & {
  /**
   * Face landmark type.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.FaceAnnotation.Landmark.Type type = 3;
   */
  type: FaceAnnotation_Landmark_Type;

  /**
   * Face landmark position.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.Position position = 4;
   */
  position?: Position;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.FaceAnnotation.Landmark.
 * Use `create(FaceAnnotation_LandmarkSchema)` to create a new message.
 */
export const FaceAnnotation_LandmarkSchema: GenMessage<FaceAnnotation_Landmark> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 3, 0);

/**
 * Face landmark (feature) type.
 * Left and right are defined from the vantage of the viewer of the image
 * without considering mirror projections typical of photos. So, `LEFT_EYE`,
 * typically, is the person's right eye.
 *
 * @generated from enum google.cloud.vision.v1p3beta1.FaceAnnotation.Landmark.Type
 */
export enum FaceAnnotation_Landmark_Type {
  /**
   * Unknown face landmark detected. Should not be filled.
   *
   * @generated from enum value: UNKNOWN_LANDMARK = 0;
   */
  UNKNOWN_LANDMARK = 0,

  /**
   * Left eye.
   *
   * @generated from enum value: LEFT_EYE = 1;
   */
  LEFT_EYE = 1,

  /**
   * Right eye.
   *
   * @generated from enum value: RIGHT_EYE = 2;
   */
  RIGHT_EYE = 2,

  /**
   * Left of left eyebrow.
   *
   * @generated from enum value: LEFT_OF_LEFT_EYEBROW = 3;
   */
  LEFT_OF_LEFT_EYEBROW = 3,

  /**
   * Right of left eyebrow.
   *
   * @generated from enum value: RIGHT_OF_LEFT_EYEBROW = 4;
   */
  RIGHT_OF_LEFT_EYEBROW = 4,

  /**
   * Left of right eyebrow.
   *
   * @generated from enum value: LEFT_OF_RIGHT_EYEBROW = 5;
   */
  LEFT_OF_RIGHT_EYEBROW = 5,

  /**
   * Right of right eyebrow.
   *
   * @generated from enum value: RIGHT_OF_RIGHT_EYEBROW = 6;
   */
  RIGHT_OF_RIGHT_EYEBROW = 6,

  /**
   * Midpoint between eyes.
   *
   * @generated from enum value: MIDPOINT_BETWEEN_EYES = 7;
   */
  MIDPOINT_BETWEEN_EYES = 7,

  /**
   * Nose tip.
   *
   * @generated from enum value: NOSE_TIP = 8;
   */
  NOSE_TIP = 8,

  /**
   * Upper lip.
   *
   * @generated from enum value: UPPER_LIP = 9;
   */
  UPPER_LIP = 9,

  /**
   * Lower lip.
   *
   * @generated from enum value: LOWER_LIP = 10;
   */
  LOWER_LIP = 10,

  /**
   * Mouth left.
   *
   * @generated from enum value: MOUTH_LEFT = 11;
   */
  MOUTH_LEFT = 11,

  /**
   * Mouth right.
   *
   * @generated from enum value: MOUTH_RIGHT = 12;
   */
  MOUTH_RIGHT = 12,

  /**
   * Mouth center.
   *
   * @generated from enum value: MOUTH_CENTER = 13;
   */
  MOUTH_CENTER = 13,

  /**
   * Nose, bottom right.
   *
   * @generated from enum value: NOSE_BOTTOM_RIGHT = 14;
   */
  NOSE_BOTTOM_RIGHT = 14,

  /**
   * Nose, bottom left.
   *
   * @generated from enum value: NOSE_BOTTOM_LEFT = 15;
   */
  NOSE_BOTTOM_LEFT = 15,

  /**
   * Nose, bottom center.
   *
   * @generated from enum value: NOSE_BOTTOM_CENTER = 16;
   */
  NOSE_BOTTOM_CENTER = 16,

  /**
   * Left eye, top boundary.
   *
   * @generated from enum value: LEFT_EYE_TOP_BOUNDARY = 17;
   */
  LEFT_EYE_TOP_BOUNDARY = 17,

  /**
   * Left eye, right corner.
   *
   * @generated from enum value: LEFT_EYE_RIGHT_CORNER = 18;
   */
  LEFT_EYE_RIGHT_CORNER = 18,

  /**
   * Left eye, bottom boundary.
   *
   * @generated from enum value: LEFT_EYE_BOTTOM_BOUNDARY = 19;
   */
  LEFT_EYE_BOTTOM_BOUNDARY = 19,

  /**
   * Left eye, left corner.
   *
   * @generated from enum value: LEFT_EYE_LEFT_CORNER = 20;
   */
  LEFT_EYE_LEFT_CORNER = 20,

  /**
   * Right eye, top boundary.
   *
   * @generated from enum value: RIGHT_EYE_TOP_BOUNDARY = 21;
   */
  RIGHT_EYE_TOP_BOUNDARY = 21,

  /**
   * Right eye, right corner.
   *
   * @generated from enum value: RIGHT_EYE_RIGHT_CORNER = 22;
   */
  RIGHT_EYE_RIGHT_CORNER = 22,

  /**
   * Right eye, bottom boundary.
   *
   * @generated from enum value: RIGHT_EYE_BOTTOM_BOUNDARY = 23;
   */
  RIGHT_EYE_BOTTOM_BOUNDARY = 23,

  /**
   * Right eye, left corner.
   *
   * @generated from enum value: RIGHT_EYE_LEFT_CORNER = 24;
   */
  RIGHT_EYE_LEFT_CORNER = 24,

  /**
   * Left eyebrow, upper midpoint.
   *
   * @generated from enum value: LEFT_EYEBROW_UPPER_MIDPOINT = 25;
   */
  LEFT_EYEBROW_UPPER_MIDPOINT = 25,

  /**
   * Right eyebrow, upper midpoint.
   *
   * @generated from enum value: RIGHT_EYEBROW_UPPER_MIDPOINT = 26;
   */
  RIGHT_EYEBROW_UPPER_MIDPOINT = 26,

  /**
   * Left ear tragion.
   *
   * @generated from enum value: LEFT_EAR_TRAGION = 27;
   */
  LEFT_EAR_TRAGION = 27,

  /**
   * Right ear tragion.
   *
   * @generated from enum value: RIGHT_EAR_TRAGION = 28;
   */
  RIGHT_EAR_TRAGION = 28,

  /**
   * Left eye pupil.
   *
   * @generated from enum value: LEFT_EYE_PUPIL = 29;
   */
  LEFT_EYE_PUPIL = 29,

  /**
   * Right eye pupil.
   *
   * @generated from enum value: RIGHT_EYE_PUPIL = 30;
   */
  RIGHT_EYE_PUPIL = 30,

  /**
   * Forehead glabella.
   *
   * @generated from enum value: FOREHEAD_GLABELLA = 31;
   */
  FOREHEAD_GLABELLA = 31,

  /**
   * Chin gnathion.
   *
   * @generated from enum value: CHIN_GNATHION = 32;
   */
  CHIN_GNATHION = 32,

  /**
   * Chin left gonion.
   *
   * @generated from enum value: CHIN_LEFT_GONION = 33;
   */
  CHIN_LEFT_GONION = 33,

  /**
   * Chin right gonion.
   *
   * @generated from enum value: CHIN_RIGHT_GONION = 34;
   */
  CHIN_RIGHT_GONION = 34,
}

/**
 * Describes the enum google.cloud.vision.v1p3beta1.FaceAnnotation.Landmark.Type.
 */
export const FaceAnnotation_Landmark_TypeSchema: GenEnum<FaceAnnotation_Landmark_Type> = /*@__PURE__*/
  enumDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 3, 0, 0);

/**
 * Detected entity location information.
 *
 * @generated from message google.cloud.vision.v1p3beta1.LocationInfo
 */
export type LocationInfo = Message<"google.cloud.vision.v1p3beta1.LocationInfo"> & {
  /**
   * lat/long location coordinates.
   *
   * @generated from field: google.type.LatLng lat_lng = 1;
   */
  latLng?: LatLng;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.LocationInfo.
 * Use `create(LocationInfoSchema)` to create a new message.
 */
export const LocationInfoSchema: GenMessage<LocationInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 4);

/**
 * A `Property` consists of a user-supplied name/value pair.
 *
 * @generated from message google.cloud.vision.v1p3beta1.Property
 */
export type Property = Message<"google.cloud.vision.v1p3beta1.Property"> & {
  /**
   * Name of the property.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Value of the property.
   *
   * @generated from field: string value = 2;
   */
  value: string;

  /**
   * Value of numeric properties.
   *
   * @generated from field: uint64 uint64_value = 3;
   */
  uint64Value: bigint;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.Property.
 * Use `create(PropertySchema)` to create a new message.
 */
export const PropertySchema: GenMessage<Property> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 5);

/**
 * Set of detected entity features.
 *
 * @generated from message google.cloud.vision.v1p3beta1.EntityAnnotation
 */
export type EntityAnnotation = Message<"google.cloud.vision.v1p3beta1.EntityAnnotation"> & {
  /**
   * Opaque entity ID. Some IDs may be available in
   * [Google Knowledge Graph Search
   * API](https://developers.google.com/knowledge-graph/).
   *
   * @generated from field: string mid = 1;
   */
  mid: string;

  /**
   * The language code for the locale in which the entity textual
   * `description` is expressed.
   *
   * @generated from field: string locale = 2;
   */
  locale: string;

  /**
   * Entity textual description, expressed in its `locale` language.
   *
   * @generated from field: string description = 3;
   */
  description: string;

  /**
   * Overall score of the result. Range [0, 1].
   *
   * @generated from field: float score = 4;
   */
  score: number;

  /**
   * **Deprecated. Use `score` instead.**
   * The accuracy of the entity detection in an image.
   * For example, for an image in which the "Eiffel Tower" entity is detected,
   * this field represents the confidence that there is a tower in the query
   * image. Range [0, 1].
   *
   * @generated from field: float confidence = 5;
   */
  confidence: number;

  /**
   * The relevancy of the ICA (Image Content Annotation) label to the
   * image. For example, the relevancy of "tower" is likely higher to an image
   * containing the detected "Eiffel Tower" than to an image containing a
   * detected distant towering building, even though the confidence that
   * there is a tower in each image may be the same. Range [0, 1].
   *
   * @generated from field: float topicality = 6;
   */
  topicality: number;

  /**
   * Image region to which this entity belongs. Not produced
   * for `LABEL_DETECTION` features.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.BoundingPoly bounding_poly = 7;
   */
  boundingPoly?: BoundingPoly;

  /**
   * The location information for the detected entity. Multiple
   * `LocationInfo` elements can be present because one location may
   * indicate the location of the scene in the image, and another location
   * may indicate the location of the place where the image was taken.
   * Location information is usually present for landmarks.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.LocationInfo locations = 8;
   */
  locations: LocationInfo[];

  /**
   * Some entities may have optional user-supplied `Property` (name/value)
   * fields, such a score or string that qualifies the entity.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.Property properties = 9;
   */
  properties: Property[];
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.EntityAnnotation.
 * Use `create(EntityAnnotationSchema)` to create a new message.
 */
export const EntityAnnotationSchema: GenMessage<EntityAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 6);

/**
 * Set of detected objects with bounding boxes.
 *
 * @generated from message google.cloud.vision.v1p3beta1.LocalizedObjectAnnotation
 */
export type LocalizedObjectAnnotation = Message<"google.cloud.vision.v1p3beta1.LocalizedObjectAnnotation"> & {
  /**
   * Object ID that should align with EntityAnnotation mid.
   *
   * @generated from field: string mid = 1;
   */
  mid: string;

  /**
   * The BCP-47 language code, such as "en-US" or "sr-Latn". For more
   * information, see
   * http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
   *
   * @generated from field: string language_code = 2;
   */
  languageCode: string;

  /**
   * Object name, expressed in its `language_code` language.
   *
   * @generated from field: string name = 3;
   */
  name: string;

  /**
   * Score of the result. Range [0, 1].
   *
   * @generated from field: float score = 4;
   */
  score: number;

  /**
   * Image region to which this object belongs. This must be populated.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.BoundingPoly bounding_poly = 5;
   */
  boundingPoly?: BoundingPoly;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.LocalizedObjectAnnotation.
 * Use `create(LocalizedObjectAnnotationSchema)` to create a new message.
 */
export const LocalizedObjectAnnotationSchema: GenMessage<LocalizedObjectAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 7);

/**
 * Set of features pertaining to the image, computed by computer vision
 * methods over safe-search verticals (for example, adult, spoof, medical,
 * violence).
 *
 * @generated from message google.cloud.vision.v1p3beta1.SafeSearchAnnotation
 */
export type SafeSearchAnnotation = Message<"google.cloud.vision.v1p3beta1.SafeSearchAnnotation"> & {
  /**
   * Represents the adult content likelihood for the image. Adult content may
   * contain elements such as nudity, pornographic images or cartoons, or
   * sexual activities.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.Likelihood adult = 1;
   */
  adult: Likelihood;

  /**
   * Spoof likelihood. The likelihood that an modification
   * was made to the image's canonical version to make it appear
   * funny or offensive.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.Likelihood spoof = 2;
   */
  spoof: Likelihood;

  /**
   * Likelihood that this is a medical image.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.Likelihood medical = 3;
   */
  medical: Likelihood;

  /**
   * Likelihood that this image contains violent content.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.Likelihood violence = 4;
   */
  violence: Likelihood;

  /**
   * Likelihood that the request image contains racy content. Racy content may
   * include (but is not limited to) skimpy or sheer clothing, strategically
   * covered nudity, lewd or provocative poses, or close-ups of sensitive
   * body areas.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.Likelihood racy = 9;
   */
  racy: Likelihood;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.SafeSearchAnnotation.
 * Use `create(SafeSearchAnnotationSchema)` to create a new message.
 */
export const SafeSearchAnnotationSchema: GenMessage<SafeSearchAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 8);

/**
 * Rectangle determined by min and max `LatLng` pairs.
 *
 * @generated from message google.cloud.vision.v1p3beta1.LatLongRect
 */
export type LatLongRect = Message<"google.cloud.vision.v1p3beta1.LatLongRect"> & {
  /**
   * Min lat/long pair.
   *
   * @generated from field: google.type.LatLng min_lat_lng = 1;
   */
  minLatLng?: LatLng;

  /**
   * Max lat/long pair.
   *
   * @generated from field: google.type.LatLng max_lat_lng = 2;
   */
  maxLatLng?: LatLng;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.LatLongRect.
 * Use `create(LatLongRectSchema)` to create a new message.
 */
export const LatLongRectSchema: GenMessage<LatLongRect> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 9);

/**
 * Color information consists of RGB channels, score, and the fraction of
 * the image that the color occupies in the image.
 *
 * @generated from message google.cloud.vision.v1p3beta1.ColorInfo
 */
export type ColorInfo = Message<"google.cloud.vision.v1p3beta1.ColorInfo"> & {
  /**
   * RGB components of the color.
   *
   * @generated from field: google.type.Color color = 1;
   */
  color?: Color;

  /**
   * Image-specific score for this color. Value in range [0, 1].
   *
   * @generated from field: float score = 2;
   */
  score: number;

  /**
   * The fraction of pixels the color occupies in the image.
   * Value in range [0, 1].
   *
   * @generated from field: float pixel_fraction = 3;
   */
  pixelFraction: number;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.ColorInfo.
 * Use `create(ColorInfoSchema)` to create a new message.
 */
export const ColorInfoSchema: GenMessage<ColorInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 10);

/**
 * Set of dominant colors and their corresponding scores.
 *
 * @generated from message google.cloud.vision.v1p3beta1.DominantColorsAnnotation
 */
export type DominantColorsAnnotation = Message<"google.cloud.vision.v1p3beta1.DominantColorsAnnotation"> & {
  /**
   * RGB color values with their score and pixel fraction.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.ColorInfo colors = 1;
   */
  colors: ColorInfo[];
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.DominantColorsAnnotation.
 * Use `create(DominantColorsAnnotationSchema)` to create a new message.
 */
export const DominantColorsAnnotationSchema: GenMessage<DominantColorsAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 11);

/**
 * Stores image properties, such as dominant colors.
 *
 * @generated from message google.cloud.vision.v1p3beta1.ImageProperties
 */
export type ImageProperties = Message<"google.cloud.vision.v1p3beta1.ImageProperties"> & {
  /**
   * If present, dominant colors completed successfully.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.DominantColorsAnnotation dominant_colors = 1;
   */
  dominantColors?: DominantColorsAnnotation;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.ImageProperties.
 * Use `create(ImagePropertiesSchema)` to create a new message.
 */
export const ImagePropertiesSchema: GenMessage<ImageProperties> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 12);

/**
 * Single crop hint that is used to generate a new crop when serving an image.
 *
 * @generated from message google.cloud.vision.v1p3beta1.CropHint
 */
export type CropHint = Message<"google.cloud.vision.v1p3beta1.CropHint"> & {
  /**
   * The bounding polygon for the crop region. The coordinates of the bounding
   * box are in the original image's scale, as returned in `ImageParams`.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.BoundingPoly bounding_poly = 1;
   */
  boundingPoly?: BoundingPoly;

  /**
   * Confidence of this being a salient region.  Range [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;

  /**
   * Fraction of importance of this salient region with respect to the original
   * image.
   *
   * @generated from field: float importance_fraction = 3;
   */
  importanceFraction: number;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.CropHint.
 * Use `create(CropHintSchema)` to create a new message.
 */
export const CropHintSchema: GenMessage<CropHint> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 13);

/**
 * Set of crop hints that are used to generate new crops when serving images.
 *
 * @generated from message google.cloud.vision.v1p3beta1.CropHintsAnnotation
 */
export type CropHintsAnnotation = Message<"google.cloud.vision.v1p3beta1.CropHintsAnnotation"> & {
  /**
   * Crop hint results.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.CropHint crop_hints = 1;
   */
  cropHints: CropHint[];
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.CropHintsAnnotation.
 * Use `create(CropHintsAnnotationSchema)` to create a new message.
 */
export const CropHintsAnnotationSchema: GenMessage<CropHintsAnnotation> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 14);

/**
 * Parameters for crop hints annotation request.
 *
 * @generated from message google.cloud.vision.v1p3beta1.CropHintsParams
 */
export type CropHintsParams = Message<"google.cloud.vision.v1p3beta1.CropHintsParams"> & {
  /**
   * Aspect ratios in floats, representing the ratio of the width to the height
   * of the image. For example, if the desired aspect ratio is 4/3, the
   * corresponding float value should be 1.33333.  If not specified, the
   * best possible crop is returned. The number of provided aspect ratios is
   * limited to a maximum of 16; any aspect ratios provided after the 16th are
   * ignored.
   *
   * @generated from field: repeated float aspect_ratios = 1;
   */
  aspectRatios: number[];
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.CropHintsParams.
 * Use `create(CropHintsParamsSchema)` to create a new message.
 */
export const CropHintsParamsSchema: GenMessage<CropHintsParams> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 15);

/**
 * Parameters for web detection request.
 *
 * @generated from message google.cloud.vision.v1p3beta1.WebDetectionParams
 */
export type WebDetectionParams = Message<"google.cloud.vision.v1p3beta1.WebDetectionParams"> & {
  /**
   * Whether to include results derived from the geo information in the image.
   *
   * @generated from field: bool include_geo_results = 2;
   */
  includeGeoResults: boolean;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.WebDetectionParams.
 * Use `create(WebDetectionParamsSchema)` to create a new message.
 */
export const WebDetectionParamsSchema: GenMessage<WebDetectionParams> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 16);

/**
 * Parameters for text detections. This is used to control TEXT_DETECTION and
 * DOCUMENT_TEXT_DETECTION features.
 *
 * @generated from message google.cloud.vision.v1p3beta1.TextDetectionParams
 */
export type TextDetectionParams = Message<"google.cloud.vision.v1p3beta1.TextDetectionParams"> & {
  /**
   * By default, Cloud Vision API only includes confidence score for
   * DOCUMENT_TEXT_DETECTION result. Set the flag to true to include confidence
   * score for TEXT_DETECTION as well.
   *
   * @generated from field: bool enable_text_detection_confidence_score = 9;
   */
  enableTextDetectionConfidenceScore: boolean;

  /**
   * A list of advanced OCR options to fine-tune OCR behavior.
   *
   * @generated from field: repeated string advanced_ocr_options = 11;
   */
  advancedOcrOptions: string[];
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.TextDetectionParams.
 * Use `create(TextDetectionParamsSchema)` to create a new message.
 */
export const TextDetectionParamsSchema: GenMessage<TextDetectionParams> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 17);

/**
 * Image context and/or feature-specific parameters.
 *
 * @generated from message google.cloud.vision.v1p3beta1.ImageContext
 */
export type ImageContext = Message<"google.cloud.vision.v1p3beta1.ImageContext"> & {
  /**
   * Not used.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.LatLongRect lat_long_rect = 1;
   */
  latLongRect?: LatLongRect;

  /**
   * List of languages to use for TEXT_DETECTION. In most cases, an empty value
   * yields the best results since it enables automatic language detection. For
   * languages based on the Latin alphabet, setting `language_hints` is not
   * needed. In rare cases, when the language of the text in the image is known,
   * setting a hint will help get better results (although it will be a
   * significant hindrance if the hint is wrong). Text detection returns an
   * error if one or more of the specified languages is not one of the
   * [supported languages](https://cloud.google.com/vision/docs/languages).
   *
   * @generated from field: repeated string language_hints = 2;
   */
  languageHints: string[];

  /**
   * Parameters for crop hints annotation request.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.CropHintsParams crop_hints_params = 4;
   */
  cropHintsParams?: CropHintsParams;

  /**
   * Parameters for product search.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.ProductSearchParams product_search_params = 5;
   */
  productSearchParams?: ProductSearchParams;

  /**
   * Parameters for web detection.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.WebDetectionParams web_detection_params = 6;
   */
  webDetectionParams?: WebDetectionParams;

  /**
   * Parameters for text detection and document text detection.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.TextDetectionParams text_detection_params = 12;
   */
  textDetectionParams?: TextDetectionParams;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.ImageContext.
 * Use `create(ImageContextSchema)` to create a new message.
 */
export const ImageContextSchema: GenMessage<ImageContext> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 18);

/**
 * Request for performing Google Cloud Vision API tasks over a user-provided
 * image, with user-requested features.
 *
 * @generated from message google.cloud.vision.v1p3beta1.AnnotateImageRequest
 */
export type AnnotateImageRequest = Message<"google.cloud.vision.v1p3beta1.AnnotateImageRequest"> & {
  /**
   * The image to be processed.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.Image image = 1;
   */
  image?: Image;

  /**
   * Requested features.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.Feature features = 2;
   */
  features: Feature[];

  /**
   * Additional context that may accompany the image.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.ImageContext image_context = 3;
   */
  imageContext?: ImageContext;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.AnnotateImageRequest.
 * Use `create(AnnotateImageRequestSchema)` to create a new message.
 */
export const AnnotateImageRequestSchema: GenMessage<AnnotateImageRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 19);

/**
 * If an image was produced from a file (e.g. a PDF), this message gives
 * information about the source of that image.
 *
 * @generated from message google.cloud.vision.v1p3beta1.ImageAnnotationContext
 */
export type ImageAnnotationContext = Message<"google.cloud.vision.v1p3beta1.ImageAnnotationContext"> & {
  /**
   * The URI of the file used to produce the image.
   *
   * @generated from field: string uri = 1;
   */
  uri: string;

  /**
   * If the file was a PDF or TIFF, this field gives the page number within
   * the file used to produce the image.
   *
   * @generated from field: int32 page_number = 2;
   */
  pageNumber: number;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.ImageAnnotationContext.
 * Use `create(ImageAnnotationContextSchema)` to create a new message.
 */
export const ImageAnnotationContextSchema: GenMessage<ImageAnnotationContext> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 20);

/**
 * Response to an image annotation request.
 *
 * @generated from message google.cloud.vision.v1p3beta1.AnnotateImageResponse
 */
export type AnnotateImageResponse = Message<"google.cloud.vision.v1p3beta1.AnnotateImageResponse"> & {
  /**
   * If present, face detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.FaceAnnotation face_annotations = 1;
   */
  faceAnnotations: FaceAnnotation[];

  /**
   * If present, landmark detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.EntityAnnotation landmark_annotations = 2;
   */
  landmarkAnnotations: EntityAnnotation[];

  /**
   * If present, logo detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.EntityAnnotation logo_annotations = 3;
   */
  logoAnnotations: EntityAnnotation[];

  /**
   * If present, label detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.EntityAnnotation label_annotations = 4;
   */
  labelAnnotations: EntityAnnotation[];

  /**
   * If present, localized object detection has completed successfully.
   * This will be sorted descending by confidence score.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.LocalizedObjectAnnotation localized_object_annotations = 22;
   */
  localizedObjectAnnotations: LocalizedObjectAnnotation[];

  /**
   * If present, text (OCR) detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.EntityAnnotation text_annotations = 5;
   */
  textAnnotations: EntityAnnotation[];

  /**
   * If present, text (OCR) detection or document (OCR) text detection has
   * completed successfully.
   * This annotation provides the structural hierarchy for the OCR detected
   * text.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.TextAnnotation full_text_annotation = 12;
   */
  fullTextAnnotation?: TextAnnotation;

  /**
   * If present, safe-search annotation has completed successfully.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.SafeSearchAnnotation safe_search_annotation = 6;
   */
  safeSearchAnnotation?: SafeSearchAnnotation;

  /**
   * If present, image properties were extracted successfully.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.ImageProperties image_properties_annotation = 8;
   */
  imagePropertiesAnnotation?: ImageProperties;

  /**
   * If present, crop hints have completed successfully.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.CropHintsAnnotation crop_hints_annotation = 11;
   */
  cropHintsAnnotation?: CropHintsAnnotation;

  /**
   * If present, web detection has completed successfully.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.WebDetection web_detection = 13;
   */
  webDetection?: WebDetection;

  /**
   * If present, product search has completed successfully.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.ProductSearchResults product_search_results = 14;
   */
  productSearchResults?: ProductSearchResults;

  /**
   * If set, represents the error message for the operation.
   * Note that filled-in image annotations are guaranteed to be
   * correct, even when `error` is set.
   *
   * @generated from field: google.rpc.Status error = 9;
   */
  error?: Status;

  /**
   * If present, contextual information is needed to understand where this image
   * comes from.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.ImageAnnotationContext context = 21;
   */
  context?: ImageAnnotationContext;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.AnnotateImageResponse.
 * Use `create(AnnotateImageResponseSchema)` to create a new message.
 */
export const AnnotateImageResponseSchema: GenMessage<AnnotateImageResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 21);

/**
 * Response to a single file annotation request. A file may contain one or more
 * images, which individually have their own responses.
 *
 * @generated from message google.cloud.vision.v1p3beta1.AnnotateFileResponse
 */
export type AnnotateFileResponse = Message<"google.cloud.vision.v1p3beta1.AnnotateFileResponse"> & {
  /**
   * Information about the file for which this response is generated.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.InputConfig input_config = 1;
   */
  inputConfig?: InputConfig;

  /**
   * Individual responses to images found within the file.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.AnnotateImageResponse responses = 2;
   */
  responses: AnnotateImageResponse[];
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.AnnotateFileResponse.
 * Use `create(AnnotateFileResponseSchema)` to create a new message.
 */
export const AnnotateFileResponseSchema: GenMessage<AnnotateFileResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 22);

/**
 * Multiple image annotation requests are batched into a single service call.
 *
 * @generated from message google.cloud.vision.v1p3beta1.BatchAnnotateImagesRequest
 */
export type BatchAnnotateImagesRequest = Message<"google.cloud.vision.v1p3beta1.BatchAnnotateImagesRequest"> & {
  /**
   * Individual image annotation requests for this batch.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.AnnotateImageRequest requests = 1;
   */
  requests: AnnotateImageRequest[];
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.BatchAnnotateImagesRequest.
 * Use `create(BatchAnnotateImagesRequestSchema)` to create a new message.
 */
export const BatchAnnotateImagesRequestSchema: GenMessage<BatchAnnotateImagesRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 23);

/**
 * Response to a batch image annotation request.
 *
 * @generated from message google.cloud.vision.v1p3beta1.BatchAnnotateImagesResponse
 */
export type BatchAnnotateImagesResponse = Message<"google.cloud.vision.v1p3beta1.BatchAnnotateImagesResponse"> & {
  /**
   * Individual responses to image annotation requests within the batch.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.AnnotateImageResponse responses = 1;
   */
  responses: AnnotateImageResponse[];
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.BatchAnnotateImagesResponse.
 * Use `create(BatchAnnotateImagesResponseSchema)` to create a new message.
 */
export const BatchAnnotateImagesResponseSchema: GenMessage<BatchAnnotateImagesResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 24);

/**
 * An offline file annotation request.
 *
 * @generated from message google.cloud.vision.v1p3beta1.AsyncAnnotateFileRequest
 */
export type AsyncAnnotateFileRequest = Message<"google.cloud.vision.v1p3beta1.AsyncAnnotateFileRequest"> & {
  /**
   * Required. Information about the input file.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.InputConfig input_config = 1;
   */
  inputConfig?: InputConfig;

  /**
   * Required. Requested features.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.Feature features = 2;
   */
  features: Feature[];

  /**
   * Additional context that may accompany the image(s) in the file.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.ImageContext image_context = 3;
   */
  imageContext?: ImageContext;

  /**
   * Required. The desired output location and metadata (e.g. format).
   *
   * @generated from field: google.cloud.vision.v1p3beta1.OutputConfig output_config = 4;
   */
  outputConfig?: OutputConfig;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.AsyncAnnotateFileRequest.
 * Use `create(AsyncAnnotateFileRequestSchema)` to create a new message.
 */
export const AsyncAnnotateFileRequestSchema: GenMessage<AsyncAnnotateFileRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 25);

/**
 * The response for a single offline file annotation request.
 *
 * @generated from message google.cloud.vision.v1p3beta1.AsyncAnnotateFileResponse
 */
export type AsyncAnnotateFileResponse = Message<"google.cloud.vision.v1p3beta1.AsyncAnnotateFileResponse"> & {
  /**
   * The output location and metadata from AsyncAnnotateFileRequest.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.OutputConfig output_config = 1;
   */
  outputConfig?: OutputConfig;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.AsyncAnnotateFileResponse.
 * Use `create(AsyncAnnotateFileResponseSchema)` to create a new message.
 */
export const AsyncAnnotateFileResponseSchema: GenMessage<AsyncAnnotateFileResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 26);

/**
 * Multiple async file annotation requests are batched into a single service
 * call.
 *
 * @generated from message google.cloud.vision.v1p3beta1.AsyncBatchAnnotateFilesRequest
 */
export type AsyncBatchAnnotateFilesRequest = Message<"google.cloud.vision.v1p3beta1.AsyncBatchAnnotateFilesRequest"> & {
  /**
   * Required. Individual async file annotation requests for this batch.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.AsyncAnnotateFileRequest requests = 1;
   */
  requests: AsyncAnnotateFileRequest[];
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.AsyncBatchAnnotateFilesRequest.
 * Use `create(AsyncBatchAnnotateFilesRequestSchema)` to create a new message.
 */
export const AsyncBatchAnnotateFilesRequestSchema: GenMessage<AsyncBatchAnnotateFilesRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 27);

/**
 * Response to an async batch file annotation request.
 *
 * @generated from message google.cloud.vision.v1p3beta1.AsyncBatchAnnotateFilesResponse
 */
export type AsyncBatchAnnotateFilesResponse = Message<"google.cloud.vision.v1p3beta1.AsyncBatchAnnotateFilesResponse"> & {
  /**
   * The list of file annotation responses, one for each request in
   * AsyncBatchAnnotateFilesRequest.
   *
   * @generated from field: repeated google.cloud.vision.v1p3beta1.AsyncAnnotateFileResponse responses = 1;
   */
  responses: AsyncAnnotateFileResponse[];
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.AsyncBatchAnnotateFilesResponse.
 * Use `create(AsyncBatchAnnotateFilesResponseSchema)` to create a new message.
 */
export const AsyncBatchAnnotateFilesResponseSchema: GenMessage<AsyncBatchAnnotateFilesResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 28);

/**
 * The desired input location and metadata.
 *
 * @generated from message google.cloud.vision.v1p3beta1.InputConfig
 */
export type InputConfig = Message<"google.cloud.vision.v1p3beta1.InputConfig"> & {
  /**
   * The Google Cloud Storage location to read the input from.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.GcsSource gcs_source = 1;
   */
  gcsSource?: GcsSource;

  /**
   * The type of the file. Currently only "application/pdf" and "image/tiff"
   * are supported. Wildcards are not supported.
   *
   * @generated from field: string mime_type = 2;
   */
  mimeType: string;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.InputConfig.
 * Use `create(InputConfigSchema)` to create a new message.
 */
export const InputConfigSchema: GenMessage<InputConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 29);

/**
 * The desired output location and metadata.
 *
 * @generated from message google.cloud.vision.v1p3beta1.OutputConfig
 */
export type OutputConfig = Message<"google.cloud.vision.v1p3beta1.OutputConfig"> & {
  /**
   * The Google Cloud Storage location to write the output(s) to.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.GcsDestination gcs_destination = 1;
   */
  gcsDestination?: GcsDestination;

  /**
   * The max number of response protos to put into each output JSON file on
   * Google Cloud Storage.
   * The valid range is [1, 100]. If not specified, the default value is 20.
   *
   * For example, for one pdf file with 100 pages, 100 response protos will
   * be generated. If `batch_size` = 20, then 5 json files each
   * containing 20 response protos will be written under the prefix
   * `gcs_destination`.`uri`.
   *
   * Currently, batch_size only applies to GcsDestination, with potential future
   * support for other output configurations.
   *
   * @generated from field: int32 batch_size = 2;
   */
  batchSize: number;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.OutputConfig.
 * Use `create(OutputConfigSchema)` to create a new message.
 */
export const OutputConfigSchema: GenMessage<OutputConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 30);

/**
 * The Google Cloud Storage location where the input will be read from.
 *
 * @generated from message google.cloud.vision.v1p3beta1.GcsSource
 */
export type GcsSource = Message<"google.cloud.vision.v1p3beta1.GcsSource"> & {
  /**
   * Google Cloud Storage URI for the input file. This must only be a
   * Google Cloud Storage object. Wildcards are not currently supported.
   *
   * @generated from field: string uri = 1;
   */
  uri: string;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.GcsSource.
 * Use `create(GcsSourceSchema)` to create a new message.
 */
export const GcsSourceSchema: GenMessage<GcsSource> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 31);

/**
 * The Google Cloud Storage location where the output will be written to.
 *
 * @generated from message google.cloud.vision.v1p3beta1.GcsDestination
 */
export type GcsDestination = Message<"google.cloud.vision.v1p3beta1.GcsDestination"> & {
  /**
   * Google Cloud Storage URI where the results will be stored. Results will
   * be in JSON format and preceded by its corresponding input URI. This field
   * can either represent a single file, or a prefix for multiple outputs.
   * Prefixes must end in a `/`.
   *
   * Examples:
   *
   * *    File: gs://bucket-name/filename.json
   * *    Prefix: gs://bucket-name/prefix/here/
   * *    File: gs://bucket-name/prefix/here
   *
   * If multiple outputs, each response is still AnnotateFileResponse, each of
   * which contains some subset of the full list of AnnotateImageResponse.
   * Multiple outputs can happen if, for example, the output JSON is too large
   * and overflows into multiple sharded files.
   *
   * @generated from field: string uri = 1;
   */
  uri: string;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.GcsDestination.
 * Use `create(GcsDestinationSchema)` to create a new message.
 */
export const GcsDestinationSchema: GenMessage<GcsDestination> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 32);

/**
 * Contains metadata for the BatchAnnotateImages operation.
 *
 * @generated from message google.cloud.vision.v1p3beta1.OperationMetadata
 */
export type OperationMetadata = Message<"google.cloud.vision.v1p3beta1.OperationMetadata"> & {
  /**
   * Current state of the batch operation.
   *
   * @generated from field: google.cloud.vision.v1p3beta1.OperationMetadata.State state = 1;
   */
  state: OperationMetadata_State;

  /**
   * The time when the batch request was received.
   *
   * @generated from field: google.protobuf.Timestamp create_time = 5;
   */
  createTime?: Timestamp;

  /**
   * The time when the operation result was last updated.
   *
   * @generated from field: google.protobuf.Timestamp update_time = 6;
   */
  updateTime?: Timestamp;
};

/**
 * Describes the message google.cloud.vision.v1p3beta1.OperationMetadata.
 * Use `create(OperationMetadataSchema)` to create a new message.
 */
export const OperationMetadataSchema: GenMessage<OperationMetadata> = /*@__PURE__*/
  messageDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 33);

/**
 * Batch operation states.
 *
 * @generated from enum google.cloud.vision.v1p3beta1.OperationMetadata.State
 */
export enum OperationMetadata_State {
  /**
   * Invalid.
   *
   * @generated from enum value: STATE_UNSPECIFIED = 0;
   */
  STATE_UNSPECIFIED = 0,

  /**
   * Request is received.
   *
   * @generated from enum value: CREATED = 1;
   */
  CREATED = 1,

  /**
   * Request is actively being processed.
   *
   * @generated from enum value: RUNNING = 2;
   */
  RUNNING = 2,

  /**
   * The batch processing is done.
   *
   * @generated from enum value: DONE = 3;
   */
  DONE = 3,

  /**
   * The batch processing was cancelled.
   *
   * @generated from enum value: CANCELLED = 4;
   */
  CANCELLED = 4,
}

/**
 * Describes the enum google.cloud.vision.v1p3beta1.OperationMetadata.State.
 */
export const OperationMetadata_StateSchema: GenEnum<OperationMetadata_State> = /*@__PURE__*/
  enumDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 33, 0);

/**
 * A bucketized representation of likelihood, which is intended to give clients
 * highly stable results across model upgrades.
 *
 * @generated from enum google.cloud.vision.v1p3beta1.Likelihood
 */
export enum Likelihood {
  /**
   * Unknown likelihood.
   *
   * @generated from enum value: UNKNOWN = 0;
   */
  UNKNOWN = 0,

  /**
   * It is very unlikely that the image belongs to the specified vertical.
   *
   * @generated from enum value: VERY_UNLIKELY = 1;
   */
  VERY_UNLIKELY = 1,

  /**
   * It is unlikely that the image belongs to the specified vertical.
   *
   * @generated from enum value: UNLIKELY = 2;
   */
  UNLIKELY = 2,

  /**
   * It is possible that the image belongs to the specified vertical.
   *
   * @generated from enum value: POSSIBLE = 3;
   */
  POSSIBLE = 3,

  /**
   * It is likely that the image belongs to the specified vertical.
   *
   * @generated from enum value: LIKELY = 4;
   */
  LIKELY = 4,

  /**
   * It is very likely that the image belongs to the specified vertical.
   *
   * @generated from enum value: VERY_LIKELY = 5;
   */
  VERY_LIKELY = 5,
}

/**
 * Describes the enum google.cloud.vision.v1p3beta1.Likelihood.
 */
export const LikelihoodSchema: GenEnum<Likelihood> = /*@__PURE__*/
  enumDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 0);

/**
 * Service that performs Google Cloud Vision API detection tasks over client
 * images, such as face, landmark, logo, label, and text detection. The
 * ImageAnnotator service returns detected entities from the images.
 *
 * @generated from service google.cloud.vision.v1p3beta1.ImageAnnotator
 */
export const ImageAnnotator: GenService<{
  /**
   * Run image detection and annotation for a batch of images.
   *
   * @generated from rpc google.cloud.vision.v1p3beta1.ImageAnnotator.BatchAnnotateImages
   */
  batchAnnotateImages: {
    methodKind: "unary";
    input: typeof BatchAnnotateImagesRequestSchema;
    output: typeof BatchAnnotateImagesResponseSchema;
  },
  /**
   * Run asynchronous image detection and annotation for a list of generic
   * files, such as PDF files, which may contain multiple pages and multiple
   * images per page. Progress and results can be retrieved through the
   * `google.longrunning.Operations` interface.
   * `Operation.metadata` contains `OperationMetadata` (metadata).
   * `Operation.response` contains `AsyncBatchAnnotateFilesResponse` (results).
   *
   * @generated from rpc google.cloud.vision.v1p3beta1.ImageAnnotator.AsyncBatchAnnotateFiles
   */
  asyncBatchAnnotateFiles: {
    methodKind: "unary";
    input: typeof AsyncBatchAnnotateFilesRequestSchema;
    output: typeof OperationSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_vision_v1p3beta1_image_annotator, 0);

