// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/aiplatform/v1beta1/prediction_service.proto (package google.cloud.aiplatform.v1beta1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import type { HttpBody, HttpBodySchema } from "../../../api/httpbody_pb";
import { file_google_api_httpbody } from "../../../api/httpbody_pb";
import { file_google_api_resource } from "../../../api/resource_pb";
import type { Candidate, Content, GenerationConfig, SafetyRating, SafetySetting } from "./content_pb";
import { file_google_cloud_aiplatform_v1beta1_content } from "./content_pb";
import type { Explanation, ExplanationSpecOverride } from "./explanation_pb";
import { file_google_cloud_aiplatform_v1beta1_explanation } from "./explanation_pb";
import type { Tool, ToolConfig } from "./tool_pb";
import { file_google_cloud_aiplatform_v1beta1_tool } from "./tool_pb";
import type { Tensor } from "./types_pb";
import { file_google_cloud_aiplatform_v1beta1_types } from "./types_pb";
import type { Value } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_struct } from "@bufbuild/protobuf/wkt";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/aiplatform/v1beta1/prediction_service.proto.
 */
export const file_google_cloud_aiplatform_v1beta1_prediction_service: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_api_httpbody, file_google_api_resource, file_google_cloud_aiplatform_v1beta1_content, file_google_cloud_aiplatform_v1beta1_explanation, file_google_cloud_aiplatform_v1beta1_tool, file_google_cloud_aiplatform_v1beta1_types, file_google_protobuf_struct]);

/**
 * Request message for
 * [PredictionService.Predict][google.cloud.aiplatform.v1beta1.PredictionService.Predict].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.PredictRequest
 */
export type PredictRequest = Message<"google.cloud.aiplatform.v1beta1.PredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * Required. The instances that are the input to the prediction call.
   * A DeployedModel may have an upper limit on the number of instances it
   * supports per request, and when it is exceeded the prediction call errors
   * in case of AutoML Models, or, in case of customer created Models, the
   * behaviour is as documented by that Model.
   * The schema of any single instance may be specified via Endpoint's
   * DeployedModels'
   * [Model's][google.cloud.aiplatform.v1beta1.DeployedModel.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
   * [instance_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri].
   *
   * @generated from field: repeated google.protobuf.Value instances = 2;
   */
  instances: Value[];

  /**
   * The parameters that govern the prediction. The schema of the parameters may
   * be specified via Endpoint's DeployedModels' [Model's
   * ][google.cloud.aiplatform.v1beta1.DeployedModel.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
   * [parameters_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.parameters_schema_uri].
   *
   * @generated from field: google.protobuf.Value parameters = 3;
   */
  parameters?: Value;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.PredictRequest.
 * Use `create(PredictRequestSchema)` to create a new message.
 */
export const PredictRequestSchema: GenMessage<PredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 0);

/**
 * Response message for
 * [PredictionService.Predict][google.cloud.aiplatform.v1beta1.PredictionService.Predict].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.PredictResponse
 */
export type PredictResponse = Message<"google.cloud.aiplatform.v1beta1.PredictResponse"> & {
  /**
   * The predictions that are the output of the predictions call.
   * The schema of any single prediction may be specified via Endpoint's
   * DeployedModels' [Model's
   * ][google.cloud.aiplatform.v1beta1.DeployedModel.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
   * [prediction_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.prediction_schema_uri].
   *
   * @generated from field: repeated google.protobuf.Value predictions = 1;
   */
  predictions: Value[];

  /**
   * ID of the Endpoint's DeployedModel that served this prediction.
   *
   * @generated from field: string deployed_model_id = 2;
   */
  deployedModelId: string;

  /**
   * Output only. The resource name of the Model which is deployed as the
   * DeployedModel that this prediction hits.
   *
   * @generated from field: string model = 3;
   */
  model: string;

  /**
   * Output only. The version ID of the Model which is deployed as the
   * DeployedModel that this prediction hits.
   *
   * @generated from field: string model_version_id = 5;
   */
  modelVersionId: string;

  /**
   * Output only. The [display
   * name][google.cloud.aiplatform.v1beta1.Model.display_name] of the Model
   * which is deployed as the DeployedModel that this prediction hits.
   *
   * @generated from field: string model_display_name = 4;
   */
  modelDisplayName: string;

  /**
   * Output only. Request-level metadata returned by the model. The metadata
   * type will be dependent upon the model implementation.
   *
   * @generated from field: google.protobuf.Value metadata = 6;
   */
  metadata?: Value;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.PredictResponse.
 * Use `create(PredictResponseSchema)` to create a new message.
 */
export const PredictResponseSchema: GenMessage<PredictResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 1);

/**
 * Request message for
 * [PredictionService.RawPredict][google.cloud.aiplatform.v1beta1.PredictionService.RawPredict].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.RawPredictRequest
 */
export type RawPredictRequest = Message<"google.cloud.aiplatform.v1beta1.RawPredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * The prediction input. Supports HTTP headers and arbitrary data payload.
   *
   * A [DeployedModel][google.cloud.aiplatform.v1beta1.DeployedModel] may have
   * an upper limit on the number of instances it supports per request. When
   * this limit it is exceeded for an AutoML model, the
   * [RawPredict][google.cloud.aiplatform.v1beta1.PredictionService.RawPredict]
   * method returns an error. When this limit is exceeded for a custom-trained
   * model, the behavior varies depending on the model.
   *
   * You can specify the schema for each instance in the
   * [predict_schemata.instance_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri]
   * field when you create a [Model][google.cloud.aiplatform.v1beta1.Model].
   * This schema applies when you deploy the `Model` as a `DeployedModel` to an
   * [Endpoint][google.cloud.aiplatform.v1beta1.Endpoint] and use the
   * `RawPredict` method.
   *
   * @generated from field: google.api.HttpBody http_body = 2;
   */
  httpBody?: HttpBody;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.RawPredictRequest.
 * Use `create(RawPredictRequestSchema)` to create a new message.
 */
export const RawPredictRequestSchema: GenMessage<RawPredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 2);

/**
 * Request message for
 * [PredictionService.StreamRawPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamRawPredict].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.StreamRawPredictRequest
 */
export type StreamRawPredictRequest = Message<"google.cloud.aiplatform.v1beta1.StreamRawPredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * The prediction input. Supports HTTP headers and arbitrary data payload.
   *
   * @generated from field: google.api.HttpBody http_body = 2;
   */
  httpBody?: HttpBody;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.StreamRawPredictRequest.
 * Use `create(StreamRawPredictRequestSchema)` to create a new message.
 */
export const StreamRawPredictRequestSchema: GenMessage<StreamRawPredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 3);

/**
 * Request message for
 * [PredictionService.DirectPredict][google.cloud.aiplatform.v1beta1.PredictionService.DirectPredict].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.DirectPredictRequest
 */
export type DirectPredictRequest = Message<"google.cloud.aiplatform.v1beta1.DirectPredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * The prediction input.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1beta1.Tensor inputs = 2;
   */
  inputs: Tensor[];

  /**
   * The parameters that govern the prediction.
   *
   * @generated from field: google.cloud.aiplatform.v1beta1.Tensor parameters = 3;
   */
  parameters?: Tensor;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.DirectPredictRequest.
 * Use `create(DirectPredictRequestSchema)` to create a new message.
 */
export const DirectPredictRequestSchema: GenMessage<DirectPredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 4);

/**
 * Response message for
 * [PredictionService.DirectPredict][google.cloud.aiplatform.v1beta1.PredictionService.DirectPredict].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.DirectPredictResponse
 */
export type DirectPredictResponse = Message<"google.cloud.aiplatform.v1beta1.DirectPredictResponse"> & {
  /**
   * The prediction output.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1beta1.Tensor outputs = 1;
   */
  outputs: Tensor[];

  /**
   * The parameters that govern the prediction.
   *
   * @generated from field: google.cloud.aiplatform.v1beta1.Tensor parameters = 2;
   */
  parameters?: Tensor;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.DirectPredictResponse.
 * Use `create(DirectPredictResponseSchema)` to create a new message.
 */
export const DirectPredictResponseSchema: GenMessage<DirectPredictResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 5);

/**
 * Request message for
 * [PredictionService.DirectRawPredict][google.cloud.aiplatform.v1beta1.PredictionService.DirectRawPredict].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.DirectRawPredictRequest
 */
export type DirectRawPredictRequest = Message<"google.cloud.aiplatform.v1beta1.DirectRawPredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * Fully qualified name of the API method being invoked to perform
   * predictions.
   *
   * Format:
   * `/namespace.Service/Method/`
   * Example:
   * `/tensorflow.serving.PredictionService/Predict`
   *
   * @generated from field: string method_name = 2;
   */
  methodName: string;

  /**
   * The prediction input.
   *
   * @generated from field: bytes input = 3;
   */
  input: Uint8Array;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.DirectRawPredictRequest.
 * Use `create(DirectRawPredictRequestSchema)` to create a new message.
 */
export const DirectRawPredictRequestSchema: GenMessage<DirectRawPredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 6);

/**
 * Response message for
 * [PredictionService.DirectRawPredict][google.cloud.aiplatform.v1beta1.PredictionService.DirectRawPredict].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.DirectRawPredictResponse
 */
export type DirectRawPredictResponse = Message<"google.cloud.aiplatform.v1beta1.DirectRawPredictResponse"> & {
  /**
   * The prediction output.
   *
   * @generated from field: bytes output = 1;
   */
  output: Uint8Array;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.DirectRawPredictResponse.
 * Use `create(DirectRawPredictResponseSchema)` to create a new message.
 */
export const DirectRawPredictResponseSchema: GenMessage<DirectRawPredictResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 7);

/**
 * Request message for
 * [PredictionService.StreamDirectPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamDirectPredict].
 *
 * The first message must contain
 * [endpoint][google.cloud.aiplatform.v1beta1.StreamDirectPredictRequest.endpoint]
 * field and optionally [input][]. The subsequent messages must contain
 * [input][].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.StreamDirectPredictRequest
 */
export type StreamDirectPredictRequest = Message<"google.cloud.aiplatform.v1beta1.StreamDirectPredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * Optional. The prediction input.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1beta1.Tensor inputs = 2;
   */
  inputs: Tensor[];

  /**
   * Optional. The parameters that govern the prediction.
   *
   * @generated from field: google.cloud.aiplatform.v1beta1.Tensor parameters = 3;
   */
  parameters?: Tensor;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.StreamDirectPredictRequest.
 * Use `create(StreamDirectPredictRequestSchema)` to create a new message.
 */
export const StreamDirectPredictRequestSchema: GenMessage<StreamDirectPredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 8);

/**
 * Response message for
 * [PredictionService.StreamDirectPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamDirectPredict].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.StreamDirectPredictResponse
 */
export type StreamDirectPredictResponse = Message<"google.cloud.aiplatform.v1beta1.StreamDirectPredictResponse"> & {
  /**
   * The prediction output.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1beta1.Tensor outputs = 1;
   */
  outputs: Tensor[];

  /**
   * The parameters that govern the prediction.
   *
   * @generated from field: google.cloud.aiplatform.v1beta1.Tensor parameters = 2;
   */
  parameters?: Tensor;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.StreamDirectPredictResponse.
 * Use `create(StreamDirectPredictResponseSchema)` to create a new message.
 */
export const StreamDirectPredictResponseSchema: GenMessage<StreamDirectPredictResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 9);

/**
 * Request message for
 * [PredictionService.StreamDirectRawPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamDirectRawPredict].
 *
 * The first message must contain
 * [endpoint][google.cloud.aiplatform.v1beta1.StreamDirectRawPredictRequest.endpoint]
 * and
 * [method_name][google.cloud.aiplatform.v1beta1.StreamDirectRawPredictRequest.method_name]
 * fields and optionally
 * [input][google.cloud.aiplatform.v1beta1.StreamDirectRawPredictRequest.input].
 * The subsequent messages must contain
 * [input][google.cloud.aiplatform.v1beta1.StreamDirectRawPredictRequest.input].
 * [method_name][google.cloud.aiplatform.v1beta1.StreamDirectRawPredictRequest.method_name]
 * in the subsequent messages have no effect.
 *
 * @generated from message google.cloud.aiplatform.v1beta1.StreamDirectRawPredictRequest
 */
export type StreamDirectRawPredictRequest = Message<"google.cloud.aiplatform.v1beta1.StreamDirectRawPredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * Optional. Fully qualified name of the API method being invoked to perform
   * predictions.
   *
   * Format:
   * `/namespace.Service/Method/`
   * Example:
   * `/tensorflow.serving.PredictionService/Predict`
   *
   * @generated from field: string method_name = 2;
   */
  methodName: string;

  /**
   * Optional. The prediction input.
   *
   * @generated from field: bytes input = 3;
   */
  input: Uint8Array;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.StreamDirectRawPredictRequest.
 * Use `create(StreamDirectRawPredictRequestSchema)` to create a new message.
 */
export const StreamDirectRawPredictRequestSchema: GenMessage<StreamDirectRawPredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 10);

/**
 * Response message for
 * [PredictionService.StreamDirectRawPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamDirectRawPredict].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.StreamDirectRawPredictResponse
 */
export type StreamDirectRawPredictResponse = Message<"google.cloud.aiplatform.v1beta1.StreamDirectRawPredictResponse"> & {
  /**
   * The prediction output.
   *
   * @generated from field: bytes output = 1;
   */
  output: Uint8Array;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.StreamDirectRawPredictResponse.
 * Use `create(StreamDirectRawPredictResponseSchema)` to create a new message.
 */
export const StreamDirectRawPredictResponseSchema: GenMessage<StreamDirectRawPredictResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 11);

/**
 * Request message for
 * [PredictionService.StreamingPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamingPredict].
 *
 * The first message must contain
 * [endpoint][google.cloud.aiplatform.v1beta1.StreamingPredictRequest.endpoint]
 * field and optionally [input][]. The subsequent messages must contain
 * [input][].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.StreamingPredictRequest
 */
export type StreamingPredictRequest = Message<"google.cloud.aiplatform.v1beta1.StreamingPredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * The prediction input.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1beta1.Tensor inputs = 2;
   */
  inputs: Tensor[];

  /**
   * The parameters that govern the prediction.
   *
   * @generated from field: google.cloud.aiplatform.v1beta1.Tensor parameters = 3;
   */
  parameters?: Tensor;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.StreamingPredictRequest.
 * Use `create(StreamingPredictRequestSchema)` to create a new message.
 */
export const StreamingPredictRequestSchema: GenMessage<StreamingPredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 12);

/**
 * Response message for
 * [PredictionService.StreamingPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamingPredict].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.StreamingPredictResponse
 */
export type StreamingPredictResponse = Message<"google.cloud.aiplatform.v1beta1.StreamingPredictResponse"> & {
  /**
   * The prediction output.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1beta1.Tensor outputs = 1;
   */
  outputs: Tensor[];

  /**
   * The parameters that govern the prediction.
   *
   * @generated from field: google.cloud.aiplatform.v1beta1.Tensor parameters = 2;
   */
  parameters?: Tensor;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.StreamingPredictResponse.
 * Use `create(StreamingPredictResponseSchema)` to create a new message.
 */
export const StreamingPredictResponseSchema: GenMessage<StreamingPredictResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 13);

/**
 * Request message for
 * [PredictionService.StreamingRawPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamingRawPredict].
 *
 * The first message must contain
 * [endpoint][google.cloud.aiplatform.v1beta1.StreamingRawPredictRequest.endpoint]
 * and
 * [method_name][google.cloud.aiplatform.v1beta1.StreamingRawPredictRequest.method_name]
 * fields and optionally
 * [input][google.cloud.aiplatform.v1beta1.StreamingRawPredictRequest.input].
 * The subsequent messages must contain
 * [input][google.cloud.aiplatform.v1beta1.StreamingRawPredictRequest.input].
 * [method_name][google.cloud.aiplatform.v1beta1.StreamingRawPredictRequest.method_name]
 * in the subsequent messages have no effect.
 *
 * @generated from message google.cloud.aiplatform.v1beta1.StreamingRawPredictRequest
 */
export type StreamingRawPredictRequest = Message<"google.cloud.aiplatform.v1beta1.StreamingRawPredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * Fully qualified name of the API method being invoked to perform
   * predictions.
   *
   * Format:
   * `/namespace.Service/Method/`
   * Example:
   * `/tensorflow.serving.PredictionService/Predict`
   *
   * @generated from field: string method_name = 2;
   */
  methodName: string;

  /**
   * The prediction input.
   *
   * @generated from field: bytes input = 3;
   */
  input: Uint8Array;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.StreamingRawPredictRequest.
 * Use `create(StreamingRawPredictRequestSchema)` to create a new message.
 */
export const StreamingRawPredictRequestSchema: GenMessage<StreamingRawPredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 14);

/**
 * Response message for
 * [PredictionService.StreamingRawPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamingRawPredict].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.StreamingRawPredictResponse
 */
export type StreamingRawPredictResponse = Message<"google.cloud.aiplatform.v1beta1.StreamingRawPredictResponse"> & {
  /**
   * The prediction output.
   *
   * @generated from field: bytes output = 1;
   */
  output: Uint8Array;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.StreamingRawPredictResponse.
 * Use `create(StreamingRawPredictResponseSchema)` to create a new message.
 */
export const StreamingRawPredictResponseSchema: GenMessage<StreamingRawPredictResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 15);

/**
 * Request message for
 * [PredictionService.Explain][google.cloud.aiplatform.v1beta1.PredictionService.Explain].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.ExplainRequest
 */
export type ExplainRequest = Message<"google.cloud.aiplatform.v1beta1.ExplainRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the explanation.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * Required. The instances that are the input to the explanation call.
   * A DeployedModel may have an upper limit on the number of instances it
   * supports per request, and when it is exceeded the explanation call errors
   * in case of AutoML Models, or, in case of customer created Models, the
   * behaviour is as documented by that Model.
   * The schema of any single instance may be specified via Endpoint's
   * DeployedModels'
   * [Model's][google.cloud.aiplatform.v1beta1.DeployedModel.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
   * [instance_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri].
   *
   * @generated from field: repeated google.protobuf.Value instances = 2;
   */
  instances: Value[];

  /**
   * The parameters that govern the prediction. The schema of the parameters may
   * be specified via Endpoint's DeployedModels' [Model's
   * ][google.cloud.aiplatform.v1beta1.DeployedModel.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
   * [parameters_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.parameters_schema_uri].
   *
   * @generated from field: google.protobuf.Value parameters = 4;
   */
  parameters?: Value;

  /**
   * If specified, overrides the
   * [explanation_spec][google.cloud.aiplatform.v1beta1.DeployedModel.explanation_spec]
   * of the DeployedModel. Can be used for explaining prediction results with
   * different configurations, such as:
   *  - Explaining top-5 predictions results as opposed to top-1;
   *  - Increasing path count or step count of the attribution methods to reduce
   *    approximate errors;
   *  - Using different baselines for explaining the prediction results.
   *
   * @generated from field: google.cloud.aiplatform.v1beta1.ExplanationSpecOverride explanation_spec_override = 5;
   */
  explanationSpecOverride?: ExplanationSpecOverride;

  /**
   * Optional. This field is the same as the one above, but supports multiple
   * explanations to occur in parallel. The key can be any string. Each override
   * will be run against the model, then its explanations will be grouped
   * together.
   *
   * Note - these explanations are run **In Addition** to the default
   * Explanation in the deployed model.
   *
   * @generated from field: map<string, google.cloud.aiplatform.v1beta1.ExplanationSpecOverride> concurrent_explanation_spec_override = 6;
   */
  concurrentExplanationSpecOverride: { [key: string]: ExplanationSpecOverride };

  /**
   * If specified, this ExplainRequest will be served by the chosen
   * DeployedModel, overriding
   * [Endpoint.traffic_split][google.cloud.aiplatform.v1beta1.Endpoint.traffic_split].
   *
   * @generated from field: string deployed_model_id = 3;
   */
  deployedModelId: string;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.ExplainRequest.
 * Use `create(ExplainRequestSchema)` to create a new message.
 */
export const ExplainRequestSchema: GenMessage<ExplainRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 16);

/**
 * Response message for
 * [PredictionService.Explain][google.cloud.aiplatform.v1beta1.PredictionService.Explain].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.ExplainResponse
 */
export type ExplainResponse = Message<"google.cloud.aiplatform.v1beta1.ExplainResponse"> & {
  /**
   * The explanations of the Model's
   * [PredictResponse.predictions][google.cloud.aiplatform.v1beta1.PredictResponse.predictions].
   *
   * It has the same number of elements as
   * [instances][google.cloud.aiplatform.v1beta1.ExplainRequest.instances] to be
   * explained.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1beta1.Explanation explanations = 1;
   */
  explanations: Explanation[];

  /**
   * This field stores the results of the explanations run in parallel with
   * The default explanation strategy/method.
   *
   * @generated from field: map<string, google.cloud.aiplatform.v1beta1.ExplainResponse.ConcurrentExplanation> concurrent_explanations = 4;
   */
  concurrentExplanations: { [key: string]: ExplainResponse_ConcurrentExplanation };

  /**
   * ID of the Endpoint's DeployedModel that served this explanation.
   *
   * @generated from field: string deployed_model_id = 2;
   */
  deployedModelId: string;

  /**
   * The predictions that are the output of the predictions call.
   * Same as
   * [PredictResponse.predictions][google.cloud.aiplatform.v1beta1.PredictResponse.predictions].
   *
   * @generated from field: repeated google.protobuf.Value predictions = 3;
   */
  predictions: Value[];
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.ExplainResponse.
 * Use `create(ExplainResponseSchema)` to create a new message.
 */
export const ExplainResponseSchema: GenMessage<ExplainResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 17);

/**
 * This message is a wrapper grouping Concurrent Explanations.
 *
 * @generated from message google.cloud.aiplatform.v1beta1.ExplainResponse.ConcurrentExplanation
 */
export type ExplainResponse_ConcurrentExplanation = Message<"google.cloud.aiplatform.v1beta1.ExplainResponse.ConcurrentExplanation"> & {
  /**
   * The explanations of the Model's
   * [PredictResponse.predictions][google.cloud.aiplatform.v1beta1.PredictResponse.predictions].
   *
   * It has the same number of elements as
   * [instances][google.cloud.aiplatform.v1beta1.ExplainRequest.instances] to
   * be explained.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1beta1.Explanation explanations = 1;
   */
  explanations: Explanation[];
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.ExplainResponse.ConcurrentExplanation.
 * Use `create(ExplainResponse_ConcurrentExplanationSchema)` to create a new message.
 */
export const ExplainResponse_ConcurrentExplanationSchema: GenMessage<ExplainResponse_ConcurrentExplanation> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 17, 0);

/**
 * Request message for
 * [PredictionService.CountTokens][google.cloud.aiplatform.v1beta1.PredictionService.CountTokens].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.CountTokensRequest
 */
export type CountTokensRequest = Message<"google.cloud.aiplatform.v1beta1.CountTokensRequest"> & {
  /**
   * Required. The name of the Endpoint requested to perform token counting.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * Optional. The name of the publisher model requested to serve the
   * prediction. Format:
   * `projects/{project}/locations/{location}/publishers/*\/models/*`
   *
   * @generated from field: string model = 3;
   */
  model: string;

  /**
   * Optional. The instances that are the input to token counting call.
   * Schema is identical to the prediction schema of the underlying model.
   *
   * @generated from field: repeated google.protobuf.Value instances = 2;
   */
  instances: Value[];

  /**
   * Optional. Input content.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1beta1.Content contents = 4;
   */
  contents: Content[];

  /**
   * Optional. The user provided system instructions for the model.
   * Note: only text should be used in parts and content in each part will be in
   * a separate paragraph.
   *
   * @generated from field: optional google.cloud.aiplatform.v1beta1.Content system_instruction = 5;
   */
  systemInstruction?: Content;

  /**
   * Optional. A list of `Tools` the model may use to generate the next
   * response.
   *
   * A `Tool` is a piece of code that enables the system to interact with
   * external systems to perform an action, or set of actions, outside of
   * knowledge and scope of the model.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1beta1.Tool tools = 6;
   */
  tools: Tool[];

  /**
   * Optional. Generation config that the model will use to generate the
   * response.
   *
   * @generated from field: optional google.cloud.aiplatform.v1beta1.GenerationConfig generation_config = 7;
   */
  generationConfig?: GenerationConfig;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.CountTokensRequest.
 * Use `create(CountTokensRequestSchema)` to create a new message.
 */
export const CountTokensRequestSchema: GenMessage<CountTokensRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 18);

/**
 * Response message for
 * [PredictionService.CountTokens][google.cloud.aiplatform.v1beta1.PredictionService.CountTokens].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.CountTokensResponse
 */
export type CountTokensResponse = Message<"google.cloud.aiplatform.v1beta1.CountTokensResponse"> & {
  /**
   * The total number of tokens counted across all instances from the request.
   *
   * @generated from field: int32 total_tokens = 1;
   */
  totalTokens: number;

  /**
   * The total number of billable characters counted across all instances from
   * the request.
   *
   * @generated from field: int32 total_billable_characters = 2;
   */
  totalBillableCharacters: number;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.CountTokensResponse.
 * Use `create(CountTokensResponseSchema)` to create a new message.
 */
export const CountTokensResponseSchema: GenMessage<CountTokensResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 19);

/**
 * Request message for [PredictionService.GenerateContent].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.GenerateContentRequest
 */
export type GenerateContentRequest = Message<"google.cloud.aiplatform.v1beta1.GenerateContentRequest"> & {
  /**
   * Required. The fully qualified name of the publisher model or tuned model
   * endpoint to use.
   *
   * Publisher model format:
   * `projects/{project}/locations/{location}/publishers/*\/models/*`
   *
   * Tuned model endpoint format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string model = 5;
   */
  model: string;

  /**
   * Required. The content of the current conversation with the model.
   *
   * For single-turn queries, this is a single instance. For multi-turn queries,
   * this is a repeated field that contains conversation history + latest
   * request.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1beta1.Content contents = 2;
   */
  contents: Content[];

  /**
   * Optional. The user provided system instructions for the model.
   * Note: only text should be used in parts and content in each part will be in
   * a separate paragraph.
   *
   * @generated from field: optional google.cloud.aiplatform.v1beta1.Content system_instruction = 8;
   */
  systemInstruction?: Content;

  /**
   * Optional. The name of the cached content used as context to serve the
   * prediction. Note: only used in explicit caching, where users can have
   * control over caching (e.g. what content to cache) and enjoy guaranteed cost
   * savings. Format:
   * `projects/{project}/locations/{location}/cachedContents/{cachedContent}`
   *
   * @generated from field: string cached_content = 9;
   */
  cachedContent: string;

  /**
   * Optional. A list of `Tools` the model may use to generate the next
   * response.
   *
   * A `Tool` is a piece of code that enables the system to interact with
   * external systems to perform an action, or set of actions, outside of
   * knowledge and scope of the model.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1beta1.Tool tools = 6;
   */
  tools: Tool[];

  /**
   * Optional. Tool config. This config is shared for all tools provided in the
   * request.
   *
   * @generated from field: google.cloud.aiplatform.v1beta1.ToolConfig tool_config = 7;
   */
  toolConfig?: ToolConfig;

  /**
   * Optional. The labels with user-defined metadata for the request. It is used
   * for billing and reporting only.
   *
   * Label keys and values can be no longer than 63 characters
   * (Unicode codepoints) and can only contain lowercase letters, numeric
   * characters, underscores, and dashes. International characters are allowed.
   * Label values are optional. Label keys must start with a letter.
   *
   * @generated from field: map<string, string> labels = 10;
   */
  labels: { [key: string]: string };

  /**
   * Optional. Per request settings for blocking unsafe content.
   * Enforced on GenerateContentResponse.candidates.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1beta1.SafetySetting safety_settings = 3;
   */
  safetySettings: SafetySetting[];

  /**
   * Optional. Generation config.
   *
   * @generated from field: google.cloud.aiplatform.v1beta1.GenerationConfig generation_config = 4;
   */
  generationConfig?: GenerationConfig;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.GenerateContentRequest.
 * Use `create(GenerateContentRequestSchema)` to create a new message.
 */
export const GenerateContentRequestSchema: GenMessage<GenerateContentRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 20);

/**
 * Response message for [PredictionService.GenerateContent].
 *
 * @generated from message google.cloud.aiplatform.v1beta1.GenerateContentResponse
 */
export type GenerateContentResponse = Message<"google.cloud.aiplatform.v1beta1.GenerateContentResponse"> & {
  /**
   * Output only. Generated candidates.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1beta1.Candidate candidates = 2;
   */
  candidates: Candidate[];

  /**
   * Output only. The model version used to generate the response.
   *
   * @generated from field: string model_version = 11;
   */
  modelVersion: string;

  /**
   * Output only. Content filter results for a prompt sent in the request.
   * Note: Sent only in the first stream chunk.
   * Only happens when no candidates were generated due to content violations.
   *
   * @generated from field: google.cloud.aiplatform.v1beta1.GenerateContentResponse.PromptFeedback prompt_feedback = 3;
   */
  promptFeedback?: GenerateContentResponse_PromptFeedback;

  /**
   * Usage metadata about the response(s).
   *
   * @generated from field: google.cloud.aiplatform.v1beta1.GenerateContentResponse.UsageMetadata usage_metadata = 4;
   */
  usageMetadata?: GenerateContentResponse_UsageMetadata;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.GenerateContentResponse.
 * Use `create(GenerateContentResponseSchema)` to create a new message.
 */
export const GenerateContentResponseSchema: GenMessage<GenerateContentResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 21);

/**
 * Content filter results for a prompt sent in the request.
 *
 * @generated from message google.cloud.aiplatform.v1beta1.GenerateContentResponse.PromptFeedback
 */
export type GenerateContentResponse_PromptFeedback = Message<"google.cloud.aiplatform.v1beta1.GenerateContentResponse.PromptFeedback"> & {
  /**
   * Output only. Blocked reason.
   *
   * @generated from field: google.cloud.aiplatform.v1beta1.GenerateContentResponse.PromptFeedback.BlockedReason block_reason = 1;
   */
  blockReason: GenerateContentResponse_PromptFeedback_BlockedReason;

  /**
   * Output only. Safety ratings.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1beta1.SafetyRating safety_ratings = 2;
   */
  safetyRatings: SafetyRating[];

  /**
   * Output only. A readable block reason message.
   *
   * @generated from field: string block_reason_message = 3;
   */
  blockReasonMessage: string;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.GenerateContentResponse.PromptFeedback.
 * Use `create(GenerateContentResponse_PromptFeedbackSchema)` to create a new message.
 */
export const GenerateContentResponse_PromptFeedbackSchema: GenMessage<GenerateContentResponse_PromptFeedback> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 21, 0);

/**
 * Blocked reason enumeration.
 *
 * @generated from enum google.cloud.aiplatform.v1beta1.GenerateContentResponse.PromptFeedback.BlockedReason
 */
export enum GenerateContentResponse_PromptFeedback_BlockedReason {
  /**
   * Unspecified blocked reason.
   *
   * @generated from enum value: BLOCKED_REASON_UNSPECIFIED = 0;
   */
  BLOCKED_REASON_UNSPECIFIED = 0,

  /**
   * Candidates blocked due to safety.
   *
   * @generated from enum value: SAFETY = 1;
   */
  SAFETY = 1,

  /**
   * Candidates blocked due to other reason.
   *
   * @generated from enum value: OTHER = 2;
   */
  OTHER = 2,

  /**
   * Candidates blocked due to the terms which are included from the
   * terminology blocklist.
   *
   * @generated from enum value: BLOCKLIST = 3;
   */
  BLOCKLIST = 3,

  /**
   * Candidates blocked due to prohibited content.
   *
   * @generated from enum value: PROHIBITED_CONTENT = 4;
   */
  PROHIBITED_CONTENT = 4,
}

/**
 * Describes the enum google.cloud.aiplatform.v1beta1.GenerateContentResponse.PromptFeedback.BlockedReason.
 */
export const GenerateContentResponse_PromptFeedback_BlockedReasonSchema: GenEnum<GenerateContentResponse_PromptFeedback_BlockedReason> = /*@__PURE__*/
  enumDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 21, 0, 0);

/**
 * Usage metadata about response(s).
 *
 * @generated from message google.cloud.aiplatform.v1beta1.GenerateContentResponse.UsageMetadata
 */
export type GenerateContentResponse_UsageMetadata = Message<"google.cloud.aiplatform.v1beta1.GenerateContentResponse.UsageMetadata"> & {
  /**
   * Number of tokens in the request. When `cached_content` is set, this is
   * still the total effective prompt size meaning this includes the number of
   * tokens in the cached content.
   *
   * @generated from field: int32 prompt_token_count = 1;
   */
  promptTokenCount: number;

  /**
   * Number of tokens in the response(s).
   *
   * @generated from field: int32 candidates_token_count = 2;
   */
  candidatesTokenCount: number;

  /**
   * Total token count for prompt and response candidates.
   *
   * @generated from field: int32 total_token_count = 3;
   */
  totalTokenCount: number;

  /**
   * Output only. Number of tokens in the cached part in the input (the cached
   * content).
   *
   * @generated from field: int32 cached_content_token_count = 5;
   */
  cachedContentTokenCount: number;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.GenerateContentResponse.UsageMetadata.
 * Use `create(GenerateContentResponse_UsageMetadataSchema)` to create a new message.
 */
export const GenerateContentResponse_UsageMetadataSchema: GenMessage<GenerateContentResponse_UsageMetadata> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 21, 1);

/**
 * Request message for [PredictionService.ChatCompletions]
 *
 * @generated from message google.cloud.aiplatform.v1beta1.ChatCompletionsRequest
 */
export type ChatCompletionsRequest = Message<"google.cloud.aiplatform.v1beta1.ChatCompletionsRequest"> & {
  /**
   * Required. The name of the endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * Optional. The prediction input. Supports HTTP headers and arbitrary data
   * payload.
   *
   * @generated from field: google.api.HttpBody http_body = 2;
   */
  httpBody?: HttpBody;
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.ChatCompletionsRequest.
 * Use `create(ChatCompletionsRequestSchema)` to create a new message.
 */
export const ChatCompletionsRequestSchema: GenMessage<ChatCompletionsRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 22);

/**
 * Response message for [PredictionService.PredictLongRunning]
 *
 * @generated from message google.cloud.aiplatform.v1beta1.PredictLongRunningResponse
 */
export type PredictLongRunningResponse = Message<"google.cloud.aiplatform.v1beta1.PredictLongRunningResponse"> & {
  /**
   * The response of the long running operation.
   *
   * @generated from oneof google.cloud.aiplatform.v1beta1.PredictLongRunningResponse.response
   */
  response: {
    /**
     * The response of the video generation prediction.
     *
     * @generated from field: google.cloud.aiplatform.v1beta1.GenerateVideoResponse generate_video_response = 1;
     */
    value: GenerateVideoResponse;
    case: "generateVideoResponse";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.PredictLongRunningResponse.
 * Use `create(PredictLongRunningResponseSchema)` to create a new message.
 */
export const PredictLongRunningResponseSchema: GenMessage<PredictLongRunningResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 23);

/**
 * Metadata for PredictLongRunning long running operations.
 *
 * @generated from message google.cloud.aiplatform.v1beta1.PredictLongRunningMetadata
 */
export type PredictLongRunningMetadata = Message<"google.cloud.aiplatform.v1beta1.PredictLongRunningMetadata"> & {
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.PredictLongRunningMetadata.
 * Use `create(PredictLongRunningMetadataSchema)` to create a new message.
 */
export const PredictLongRunningMetadataSchema: GenMessage<PredictLongRunningMetadata> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 24);

/**
 * Generate video response.
 *
 * @generated from message google.cloud.aiplatform.v1beta1.GenerateVideoResponse
 */
export type GenerateVideoResponse = Message<"google.cloud.aiplatform.v1beta1.GenerateVideoResponse"> & {
  /**
   * The cloud storage uris of the generated videos.
   *
   * @generated from field: repeated string generated_samples = 1;
   */
  generatedSamples: string[];

  /**
   * Returns if any videos were filtered due to RAI policies.
   *
   * @generated from field: optional int32 rai_media_filtered_count = 2;
   */
  raiMediaFilteredCount?: number;

  /**
   * Returns rai failure reasons if any.
   *
   * @generated from field: repeated string rai_media_filtered_reasons = 3;
   */
  raiMediaFilteredReasons: string[];
};

/**
 * Describes the message google.cloud.aiplatform.v1beta1.GenerateVideoResponse.
 * Use `create(GenerateVideoResponseSchema)` to create a new message.
 */
export const GenerateVideoResponseSchema: GenMessage<GenerateVideoResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 25);

/**
 * A service for online predictions and explanations.
 *
 * @generated from service google.cloud.aiplatform.v1beta1.PredictionService
 */
export const PredictionService: GenService<{
  /**
   * Perform an online prediction.
   *
   * @generated from rpc google.cloud.aiplatform.v1beta1.PredictionService.Predict
   */
  predict: {
    methodKind: "unary";
    input: typeof PredictRequestSchema;
    output: typeof PredictResponseSchema;
  },
  /**
   * Perform an online prediction with an arbitrary HTTP payload.
   *
   * The response includes the following HTTP headers:
   *
   * * `X-Vertex-AI-Endpoint-Id`: ID of the
   * [Endpoint][google.cloud.aiplatform.v1beta1.Endpoint] that served this
   * prediction.
   *
   * * `X-Vertex-AI-Deployed-Model-Id`: ID of the Endpoint's
   * [DeployedModel][google.cloud.aiplatform.v1beta1.DeployedModel] that served
   * this prediction.
   *
   * @generated from rpc google.cloud.aiplatform.v1beta1.PredictionService.RawPredict
   */
  rawPredict: {
    methodKind: "unary";
    input: typeof RawPredictRequestSchema;
    output: typeof HttpBodySchema;
  },
  /**
   * Perform a streaming online prediction with an arbitrary HTTP payload.
   *
   * @generated from rpc google.cloud.aiplatform.v1beta1.PredictionService.StreamRawPredict
   */
  streamRawPredict: {
    methodKind: "server_streaming";
    input: typeof StreamRawPredictRequestSchema;
    output: typeof HttpBodySchema;
  },
  /**
   * Perform an unary online prediction request to a gRPC model server for
   * Vertex first-party products and frameworks.
   *
   * @generated from rpc google.cloud.aiplatform.v1beta1.PredictionService.DirectPredict
   */
  directPredict: {
    methodKind: "unary";
    input: typeof DirectPredictRequestSchema;
    output: typeof DirectPredictResponseSchema;
  },
  /**
   * Perform an unary online prediction request to a gRPC model server for
   * custom containers.
   *
   * @generated from rpc google.cloud.aiplatform.v1beta1.PredictionService.DirectRawPredict
   */
  directRawPredict: {
    methodKind: "unary";
    input: typeof DirectRawPredictRequestSchema;
    output: typeof DirectRawPredictResponseSchema;
  },
  /**
   * Perform a streaming online prediction request to a gRPC model server for
   * Vertex first-party products and frameworks.
   *
   * @generated from rpc google.cloud.aiplatform.v1beta1.PredictionService.StreamDirectPredict
   */
  streamDirectPredict: {
    methodKind: "bidi_streaming";
    input: typeof StreamDirectPredictRequestSchema;
    output: typeof StreamDirectPredictResponseSchema;
  },
  /**
   * Perform a streaming online prediction request to a gRPC model server for
   * custom containers.
   *
   * @generated from rpc google.cloud.aiplatform.v1beta1.PredictionService.StreamDirectRawPredict
   */
  streamDirectRawPredict: {
    methodKind: "bidi_streaming";
    input: typeof StreamDirectRawPredictRequestSchema;
    output: typeof StreamDirectRawPredictResponseSchema;
  },
  /**
   * Perform a streaming online prediction request for Vertex first-party
   * products and frameworks.
   *
   * @generated from rpc google.cloud.aiplatform.v1beta1.PredictionService.StreamingPredict
   */
  streamingPredict: {
    methodKind: "bidi_streaming";
    input: typeof StreamingPredictRequestSchema;
    output: typeof StreamingPredictResponseSchema;
  },
  /**
   * Perform a server-side streaming online prediction request for Vertex
   * LLM streaming.
   *
   * @generated from rpc google.cloud.aiplatform.v1beta1.PredictionService.ServerStreamingPredict
   */
  serverStreamingPredict: {
    methodKind: "server_streaming";
    input: typeof StreamingPredictRequestSchema;
    output: typeof StreamingPredictResponseSchema;
  },
  /**
   * Perform a streaming online prediction request through gRPC.
   *
   * @generated from rpc google.cloud.aiplatform.v1beta1.PredictionService.StreamingRawPredict
   */
  streamingRawPredict: {
    methodKind: "bidi_streaming";
    input: typeof StreamingRawPredictRequestSchema;
    output: typeof StreamingRawPredictResponseSchema;
  },
  /**
   * Perform an online explanation.
   *
   * If
   * [deployed_model_id][google.cloud.aiplatform.v1beta1.ExplainRequest.deployed_model_id]
   * is specified, the corresponding DeployModel must have
   * [explanation_spec][google.cloud.aiplatform.v1beta1.DeployedModel.explanation_spec]
   * populated. If
   * [deployed_model_id][google.cloud.aiplatform.v1beta1.ExplainRequest.deployed_model_id]
   * is not specified, all DeployedModels must have
   * [explanation_spec][google.cloud.aiplatform.v1beta1.DeployedModel.explanation_spec]
   * populated.
   *
   * @generated from rpc google.cloud.aiplatform.v1beta1.PredictionService.Explain
   */
  explain: {
    methodKind: "unary";
    input: typeof ExplainRequestSchema;
    output: typeof ExplainResponseSchema;
  },
  /**
   * Perform a token counting.
   *
   * @generated from rpc google.cloud.aiplatform.v1beta1.PredictionService.CountTokens
   */
  countTokens: {
    methodKind: "unary";
    input: typeof CountTokensRequestSchema;
    output: typeof CountTokensResponseSchema;
  },
  /**
   * Generate content with multimodal inputs.
   *
   * @generated from rpc google.cloud.aiplatform.v1beta1.PredictionService.GenerateContent
   */
  generateContent: {
    methodKind: "unary";
    input: typeof GenerateContentRequestSchema;
    output: typeof GenerateContentResponseSchema;
  },
  /**
   * Generate content with multimodal inputs with streaming support.
   *
   * @generated from rpc google.cloud.aiplatform.v1beta1.PredictionService.StreamGenerateContent
   */
  streamGenerateContent: {
    methodKind: "server_streaming";
    input: typeof GenerateContentRequestSchema;
    output: typeof GenerateContentResponseSchema;
  },
  /**
   * Exposes an OpenAI-compatible endpoint for chat completions.
   *
   * @generated from rpc google.cloud.aiplatform.v1beta1.PredictionService.ChatCompletions
   */
  chatCompletions: {
    methodKind: "server_streaming";
    input: typeof ChatCompletionsRequestSchema;
    output: typeof HttpBodySchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_aiplatform_v1beta1_prediction_service, 0);

