// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/aiplatform/v1/model.proto (package google.cloud.aiplatform.v1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import { file_google_api_resource } from "../../../api/resource_pb";
import type { DeployedModelRef } from "./deployed_model_ref_pb";
import { file_google_cloud_aiplatform_v1_deployed_model_ref } from "./deployed_model_ref_pb";
import type { EncryptionSpec } from "./encryption_spec_pb";
import { file_google_cloud_aiplatform_v1_encryption_spec } from "./encryption_spec_pb";
import type { EnvVar } from "./env_var_pb";
import { file_google_cloud_aiplatform_v1_env_var } from "./env_var_pb";
import type { ExplanationSpec } from "./explanation_pb";
import { file_google_cloud_aiplatform_v1_explanation } from "./explanation_pb";
import type { Duration, Timestamp, Value } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_duration, file_google_protobuf_struct, file_google_protobuf_timestamp } from "@bufbuild/protobuf/wkt";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/aiplatform/v1/model.proto.
 */
export const file_google_cloud_aiplatform_v1_model: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_field_behavior, file_google_api_resource, file_google_cloud_aiplatform_v1_deployed_model_ref, file_google_cloud_aiplatform_v1_encryption_spec, file_google_cloud_aiplatform_v1_env_var, file_google_cloud_aiplatform_v1_explanation, file_google_protobuf_duration, file_google_protobuf_struct, file_google_protobuf_timestamp]);

/**
 * A trained machine learning Model.
 *
 * @generated from message google.cloud.aiplatform.v1.Model
 */
export type Model = Message<"google.cloud.aiplatform.v1.Model"> & {
  /**
   * The resource name of the Model.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Output only. Immutable. The version ID of the model.
   * A new version is committed when a new model version is uploaded or
   * trained under an existing model id. It is an auto-incrementing decimal
   * number in string representation.
   *
   * @generated from field: string version_id = 28;
   */
  versionId: string;

  /**
   * User provided version aliases so that a model version can be referenced via
   * alias (i.e.
   * `projects/{project}/locations/{location}/models/{model_id}@{version_alias}`
   * instead of auto-generated version id (i.e.
   * `projects/{project}/locations/{location}/models/{model_id}@{version_id})`.
   * The format is [a-z][a-zA-Z0-9-]{0,126}[a-z0-9] to distinguish from
   * version_id. A default version alias will be created for the first version
   * of the model, and there must be exactly one default version alias for a
   * model.
   *
   * @generated from field: repeated string version_aliases = 29;
   */
  versionAliases: string[];

  /**
   * Output only. Timestamp when this version was created.
   *
   * @generated from field: google.protobuf.Timestamp version_create_time = 31;
   */
  versionCreateTime?: Timestamp;

  /**
   * Output only. Timestamp when this version was most recently updated.
   *
   * @generated from field: google.protobuf.Timestamp version_update_time = 32;
   */
  versionUpdateTime?: Timestamp;

  /**
   * Required. The display name of the Model.
   * The name can be up to 128 characters long and can consist of any UTF-8
   * characters.
   *
   * @generated from field: string display_name = 2;
   */
  displayName: string;

  /**
   * The description of the Model.
   *
   * @generated from field: string description = 3;
   */
  description: string;

  /**
   * The description of this version.
   *
   * @generated from field: string version_description = 30;
   */
  versionDescription: string;

  /**
   * The schemata that describe formats of the Model's predictions and
   * explanations as given and returned via
   * [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict]
   * and
   * [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain].
   *
   * @generated from field: google.cloud.aiplatform.v1.PredictSchemata predict_schemata = 4;
   */
  predictSchemata?: PredictSchemata;

  /**
   * Immutable. Points to a YAML file stored on Google Cloud Storage describing
   * additional information about the Model, that is specific to it. Unset if
   * the Model does not have any additional information. The schema is defined
   * as an OpenAPI 3.0.2 [Schema
   * Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject).
   * AutoML Models always have this field populated by Vertex AI, if no
   * additional metadata is needed, this field is set to an empty string.
   * Note: The URI given on output will be immutable and probably different,
   * including the URI scheme, than the one given on input. The output URI will
   * point to a location where the user only has a read access.
   *
   * @generated from field: string metadata_schema_uri = 5;
   */
  metadataSchemaUri: string;

  /**
   * Immutable. An additional information about the Model; the schema of the
   * metadata can be found in
   * [metadata_schema][google.cloud.aiplatform.v1.Model.metadata_schema_uri].
   * Unset if the Model does not have any additional information.
   *
   * @generated from field: google.protobuf.Value metadata = 6;
   */
  metadata?: Value;

  /**
   * Output only. The formats in which this Model may be exported. If empty,
   * this Model is not available for export.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Model.ExportFormat supported_export_formats = 20;
   */
  supportedExportFormats: Model_ExportFormat[];

  /**
   * Output only. The resource name of the TrainingPipeline that uploaded this
   * Model, if any.
   *
   * @generated from field: string training_pipeline = 7;
   */
  trainingPipeline: string;

  /**
   * Optional. This field is populated if the model is produced by a pipeline
   * job.
   *
   * @generated from field: string pipeline_job = 47;
   */
  pipelineJob: string;

  /**
   * Input only. The specification of the container that is to be used when
   * deploying this Model. The specification is ingested upon
   * [ModelService.UploadModel][google.cloud.aiplatform.v1.ModelService.UploadModel],
   * and all binaries it contains are copied and stored internally by Vertex AI.
   * Not required for AutoML Models.
   *
   * @generated from field: google.cloud.aiplatform.v1.ModelContainerSpec container_spec = 9;
   */
  containerSpec?: ModelContainerSpec;

  /**
   * Immutable. The path to the directory containing the Model artifact and any
   * of its supporting files. Not required for AutoML Models.
   *
   * @generated from field: string artifact_uri = 26;
   */
  artifactUri: string;

  /**
   * Output only. When this Model is deployed, its prediction resources are
   * described by the `prediction_resources` field of the
   * [Endpoint.deployed_models][google.cloud.aiplatform.v1.Endpoint.deployed_models]
   * object. Because not all Models support all resource configuration types,
   * the configuration types this Model supports are listed here. If no
   * configuration types are listed, the Model cannot be deployed to an
   * [Endpoint][google.cloud.aiplatform.v1.Endpoint] and does not support
   * online predictions
   * ([PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict]
   * or
   * [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain]).
   * Such a Model can serve predictions by using a
   * [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob], if it
   * has at least one entry each in
   * [supported_input_storage_formats][google.cloud.aiplatform.v1.Model.supported_input_storage_formats]
   * and
   * [supported_output_storage_formats][google.cloud.aiplatform.v1.Model.supported_output_storage_formats].
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Model.DeploymentResourcesType supported_deployment_resources_types = 10;
   */
  supportedDeploymentResourcesTypes: Model_DeploymentResourcesType[];

  /**
   * Output only. The formats this Model supports in
   * [BatchPredictionJob.input_config][google.cloud.aiplatform.v1.BatchPredictionJob.input_config].
   * If
   * [PredictSchemata.instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
   * exists, the instances should be given as per that schema.
   *
   * The possible formats are:
   *
   * * `jsonl`
   * The JSON Lines format, where each instance is a single line. Uses
   * [GcsSource][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.gcs_source].
   *
   * * `csv`
   * The CSV format, where each instance is a single comma-separated line.
   * The first line in the file is the header, containing comma-separated field
   * names. Uses
   * [GcsSource][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.gcs_source].
   *
   * * `tf-record`
   * The TFRecord format, where each instance is a single record in tfrecord
   * syntax. Uses
   * [GcsSource][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.gcs_source].
   *
   * * `tf-record-gzip`
   * Similar to `tf-record`, but the file is gzipped. Uses
   * [GcsSource][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.gcs_source].
   *
   * * `bigquery`
   * Each instance is a single row in BigQuery. Uses
   * [BigQuerySource][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.bigquery_source].
   *
   * * `file-list`
   * Each line of the file is the location of an instance to process, uses
   * `gcs_source` field of the
   * [InputConfig][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig]
   * object.
   *
   *
   * If this Model doesn't support any of these formats it means it cannot be
   * used with a
   * [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob].
   * However, if it has
   * [supported_deployment_resources_types][google.cloud.aiplatform.v1.Model.supported_deployment_resources_types],
   * it could serve online predictions by using
   * [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict]
   * or
   * [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain].
   *
   * @generated from field: repeated string supported_input_storage_formats = 11;
   */
  supportedInputStorageFormats: string[];

  /**
   * Output only. The formats this Model supports in
   * [BatchPredictionJob.output_config][google.cloud.aiplatform.v1.BatchPredictionJob.output_config].
   * If both
   * [PredictSchemata.instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
   * and
   * [PredictSchemata.prediction_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.prediction_schema_uri]
   * exist, the predictions are returned together with their instances. In other
   * words, the prediction has the original instance data first, followed by the
   * actual prediction content (as per the schema).
   *
   * The possible formats are:
   *
   * * `jsonl`
   * The JSON Lines format, where each prediction is a single line. Uses
   * [GcsDestination][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.gcs_destination].
   *
   * * `csv`
   * The CSV format, where each prediction is a single comma-separated line.
   * The first line in the file is the header, containing comma-separated field
   * names. Uses
   * [GcsDestination][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.gcs_destination].
   *
   * * `bigquery`
   * Each prediction is a single row in a BigQuery table, uses
   * [BigQueryDestination][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.bigquery_destination]
   * .
   *
   *
   * If this Model doesn't support any of these formats it means it cannot be
   * used with a
   * [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob].
   * However, if it has
   * [supported_deployment_resources_types][google.cloud.aiplatform.v1.Model.supported_deployment_resources_types],
   * it could serve online predictions by using
   * [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict]
   * or
   * [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain].
   *
   * @generated from field: repeated string supported_output_storage_formats = 12;
   */
  supportedOutputStorageFormats: string[];

  /**
   * Output only. Timestamp when this Model was uploaded into Vertex AI.
   *
   * @generated from field: google.protobuf.Timestamp create_time = 13;
   */
  createTime?: Timestamp;

  /**
   * Output only. Timestamp when this Model was most recently updated.
   *
   * @generated from field: google.protobuf.Timestamp update_time = 14;
   */
  updateTime?: Timestamp;

  /**
   * Output only. The pointers to DeployedModels created from this Model. Note
   * that Model could have been deployed to Endpoints in different Locations.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.DeployedModelRef deployed_models = 15;
   */
  deployedModels: DeployedModelRef[];

  /**
   * The default explanation specification for this Model.
   *
   * The Model can be used for
   * [requesting
   * explanation][google.cloud.aiplatform.v1.PredictionService.Explain] after
   * being [deployed][google.cloud.aiplatform.v1.EndpointService.DeployModel] if
   * it is populated. The Model can be used for [batch
   * explanation][google.cloud.aiplatform.v1.BatchPredictionJob.generate_explanation]
   * if it is populated.
   *
   * All fields of the explanation_spec can be overridden by
   * [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
   * of
   * [DeployModelRequest.deployed_model][google.cloud.aiplatform.v1.DeployModelRequest.deployed_model],
   * or
   * [explanation_spec][google.cloud.aiplatform.v1.BatchPredictionJob.explanation_spec]
   * of [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob].
   *
   * If the default explanation specification is not set for this Model, this
   * Model can still be used for
   * [requesting
   * explanation][google.cloud.aiplatform.v1.PredictionService.Explain] by
   * setting
   * [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
   * of
   * [DeployModelRequest.deployed_model][google.cloud.aiplatform.v1.DeployModelRequest.deployed_model]
   * and for [batch
   * explanation][google.cloud.aiplatform.v1.BatchPredictionJob.generate_explanation]
   * by setting
   * [explanation_spec][google.cloud.aiplatform.v1.BatchPredictionJob.explanation_spec]
   * of [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob].
   *
   * @generated from field: google.cloud.aiplatform.v1.ExplanationSpec explanation_spec = 23;
   */
  explanationSpec?: ExplanationSpec;

  /**
   * Used to perform consistent read-modify-write updates. If not set, a blind
   * "overwrite" update happens.
   *
   * @generated from field: string etag = 16;
   */
  etag: string;

  /**
   * The labels with user-defined metadata to organize your Models.
   *
   * Label keys and values can be no longer than 64 characters
   * (Unicode codepoints), can only contain lowercase letters, numeric
   * characters, underscores and dashes. International characters are allowed.
   *
   * See https://goo.gl/xmQnxf for more information and examples of labels.
   *
   * @generated from field: map<string, string> labels = 17;
   */
  labels: { [key: string]: string };

  /**
   * Stats of data used for training or evaluating the Model.
   *
   * Only populated when the Model is trained by a TrainingPipeline with
   * [data_input_config][TrainingPipeline.data_input_config].
   *
   * @generated from field: google.cloud.aiplatform.v1.Model.DataStats data_stats = 21;
   */
  dataStats?: Model_DataStats;

  /**
   * Customer-managed encryption key spec for a Model. If set, this
   * Model and all sub-resources of this Model will be secured by this key.
   *
   * @generated from field: google.cloud.aiplatform.v1.EncryptionSpec encryption_spec = 24;
   */
  encryptionSpec?: EncryptionSpec;

  /**
   * Output only. Source of a model. It can either be automl training pipeline,
   * custom training pipeline, BigQuery ML, or saved and tuned from Genie or
   * Model Garden.
   *
   * @generated from field: google.cloud.aiplatform.v1.ModelSourceInfo model_source_info = 38;
   */
  modelSourceInfo?: ModelSourceInfo;

  /**
   * Output only. If this Model is a copy of another Model, this contains info
   * about the original.
   *
   * @generated from field: google.cloud.aiplatform.v1.Model.OriginalModelInfo original_model_info = 34;
   */
  originalModelInfo?: Model_OriginalModelInfo;

  /**
   * Output only. The resource name of the Artifact that was created in
   * MetadataStore when creating the Model. The Artifact resource name pattern
   * is
   * `projects/{project}/locations/{location}/metadataStores/{metadata_store}/artifacts/{artifact}`.
   *
   * @generated from field: string metadata_artifact = 44;
   */
  metadataArtifact: string;

  /**
   * Optional. User input field to specify the base model source. Currently it
   * only supports specifing the Model Garden models and Genie models.
   *
   * @generated from field: google.cloud.aiplatform.v1.Model.BaseModelSource base_model_source = 50;
   */
  baseModelSource?: Model_BaseModelSource;

  /**
   * Output only. Reserved for future use.
   *
   * @generated from field: bool satisfies_pzs = 51;
   */
  satisfiesPzs: boolean;

  /**
   * Output only. Reserved for future use.
   *
   * @generated from field: bool satisfies_pzi = 52;
   */
  satisfiesPzi: boolean;
};

/**
 * Describes the message google.cloud.aiplatform.v1.Model.
 * Use `create(ModelSchema)` to create a new message.
 */
export const ModelSchema: GenMessage<Model> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model, 0);

/**
 * Represents export format supported by the Model.
 * All formats export to Google Cloud Storage.
 *
 * @generated from message google.cloud.aiplatform.v1.Model.ExportFormat
 */
export type Model_ExportFormat = Message<"google.cloud.aiplatform.v1.Model.ExportFormat"> & {
  /**
   * Output only. The ID of the export format.
   * The possible format IDs are:
   *
   * * `tflite`
   * Used for Android mobile devices.
   *
   * * `edgetpu-tflite`
   * Used for [Edge TPU](https://cloud.google.com/edge-tpu/) devices.
   *
   * * `tf-saved-model`
   * A tensorflow model in SavedModel format.
   *
   * * `tf-js`
   * A [TensorFlow.js](https://www.tensorflow.org/js) model that can be used
   * in the browser and in Node.js using JavaScript.
   *
   * * `core-ml`
   * Used for iOS mobile devices.
   *
   * * `custom-trained`
   * A Model that was uploaded or trained by custom code.
   *
   * @generated from field: string id = 1;
   */
  id: string;

  /**
   * Output only. The content of this Model that may be exported.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Model.ExportFormat.ExportableContent exportable_contents = 2;
   */
  exportableContents: Model_ExportFormat_ExportableContent[];
};

/**
 * Describes the message google.cloud.aiplatform.v1.Model.ExportFormat.
 * Use `create(Model_ExportFormatSchema)` to create a new message.
 */
export const Model_ExportFormatSchema: GenMessage<Model_ExportFormat> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model, 0, 0);

/**
 * The Model content that can be exported.
 *
 * @generated from enum google.cloud.aiplatform.v1.Model.ExportFormat.ExportableContent
 */
export enum Model_ExportFormat_ExportableContent {
  /**
   * Should not be used.
   *
   * @generated from enum value: EXPORTABLE_CONTENT_UNSPECIFIED = 0;
   */
  EXPORTABLE_CONTENT_UNSPECIFIED = 0,

  /**
   * Model artifact and any of its supported files. Will be exported to the
   * location specified by the `artifactDestination` field of the
   * [ExportModelRequest.output_config][google.cloud.aiplatform.v1.ExportModelRequest.output_config]
   * object.
   *
   * @generated from enum value: ARTIFACT = 1;
   */
  ARTIFACT = 1,

  /**
   * The container image that is to be used when deploying this Model. Will
   * be exported to the location specified by the `imageDestination` field
   * of the
   * [ExportModelRequest.output_config][google.cloud.aiplatform.v1.ExportModelRequest.output_config]
   * object.
   *
   * @generated from enum value: IMAGE = 2;
   */
  IMAGE = 2,
}

/**
 * Describes the enum google.cloud.aiplatform.v1.Model.ExportFormat.ExportableContent.
 */
export const Model_ExportFormat_ExportableContentSchema: GenEnum<Model_ExportFormat_ExportableContent> = /*@__PURE__*/
  enumDesc(file_google_cloud_aiplatform_v1_model, 0, 0, 0);

/**
 * Stats of data used for train or evaluate the Model.
 *
 * @generated from message google.cloud.aiplatform.v1.Model.DataStats
 */
export type Model_DataStats = Message<"google.cloud.aiplatform.v1.Model.DataStats"> & {
  /**
   * Number of DataItems that were used for training this Model.
   *
   * @generated from field: int64 training_data_items_count = 1;
   */
  trainingDataItemsCount: bigint;

  /**
   * Number of DataItems that were used for validating this Model during
   * training.
   *
   * @generated from field: int64 validation_data_items_count = 2;
   */
  validationDataItemsCount: bigint;

  /**
   * Number of DataItems that were used for evaluating this Model. If the
   * Model is evaluated multiple times, this will be the number of test
   * DataItems used by the first evaluation. If the Model is not evaluated,
   * the number is 0.
   *
   * @generated from field: int64 test_data_items_count = 3;
   */
  testDataItemsCount: bigint;

  /**
   * Number of Annotations that are used for training this Model.
   *
   * @generated from field: int64 training_annotations_count = 4;
   */
  trainingAnnotationsCount: bigint;

  /**
   * Number of Annotations that are used for validating this Model during
   * training.
   *
   * @generated from field: int64 validation_annotations_count = 5;
   */
  validationAnnotationsCount: bigint;

  /**
   * Number of Annotations that are used for evaluating this Model. If the
   * Model is evaluated multiple times, this will be the number of test
   * Annotations used by the first evaluation. If the Model is not evaluated,
   * the number is 0.
   *
   * @generated from field: int64 test_annotations_count = 6;
   */
  testAnnotationsCount: bigint;
};

/**
 * Describes the message google.cloud.aiplatform.v1.Model.DataStats.
 * Use `create(Model_DataStatsSchema)` to create a new message.
 */
export const Model_DataStatsSchema: GenMessage<Model_DataStats> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model, 0, 1);

/**
 * Contains information about the original Model if this Model is a copy.
 *
 * @generated from message google.cloud.aiplatform.v1.Model.OriginalModelInfo
 */
export type Model_OriginalModelInfo = Message<"google.cloud.aiplatform.v1.Model.OriginalModelInfo"> & {
  /**
   * Output only. The resource name of the Model this Model is a copy of,
   * including the revision. Format:
   * `projects/{project}/locations/{location}/models/{model_id}@{version_id}`
   *
   * @generated from field: string model = 1;
   */
  model: string;
};

/**
 * Describes the message google.cloud.aiplatform.v1.Model.OriginalModelInfo.
 * Use `create(Model_OriginalModelInfoSchema)` to create a new message.
 */
export const Model_OriginalModelInfoSchema: GenMessage<Model_OriginalModelInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model, 0, 2);

/**
 * User input field to specify the base model source. Currently it only
 * supports specifing the Model Garden models and Genie models.
 *
 * @generated from message google.cloud.aiplatform.v1.Model.BaseModelSource
 */
export type Model_BaseModelSource = Message<"google.cloud.aiplatform.v1.Model.BaseModelSource"> & {
  /**
   * @generated from oneof google.cloud.aiplatform.v1.Model.BaseModelSource.source
   */
  source: {
    /**
     * Source information of Model Garden models.
     *
     * @generated from field: google.cloud.aiplatform.v1.ModelGardenSource model_garden_source = 1;
     */
    value: ModelGardenSource;
    case: "modelGardenSource";
  } | {
    /**
     * Information about the base model of Genie models.
     *
     * @generated from field: google.cloud.aiplatform.v1.GenieSource genie_source = 2;
     */
    value: GenieSource;
    case: "genieSource";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.aiplatform.v1.Model.BaseModelSource.
 * Use `create(Model_BaseModelSourceSchema)` to create a new message.
 */
export const Model_BaseModelSourceSchema: GenMessage<Model_BaseModelSource> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model, 0, 3);

/**
 * Identifies a type of Model's prediction resources.
 *
 * @generated from enum google.cloud.aiplatform.v1.Model.DeploymentResourcesType
 */
export enum Model_DeploymentResourcesType {
  /**
   * Should not be used.
   *
   * @generated from enum value: DEPLOYMENT_RESOURCES_TYPE_UNSPECIFIED = 0;
   */
  DEPLOYMENT_RESOURCES_TYPE_UNSPECIFIED = 0,

  /**
   * Resources that are dedicated to the
   * [DeployedModel][google.cloud.aiplatform.v1.DeployedModel], and that need
   * a higher degree of manual configuration.
   *
   * @generated from enum value: DEDICATED_RESOURCES = 1;
   */
  DEDICATED_RESOURCES = 1,

  /**
   * Resources that to large degree are decided by Vertex AI, and require
   * only a modest additional configuration.
   *
   * @generated from enum value: AUTOMATIC_RESOURCES = 2;
   */
  AUTOMATIC_RESOURCES = 2,

  /**
   * Resources that can be shared by multiple
   * [DeployedModels][google.cloud.aiplatform.v1.DeployedModel]. A
   * pre-configured
   * [DeploymentResourcePool][google.cloud.aiplatform.v1.DeploymentResourcePool]
   * is required.
   *
   * @generated from enum value: SHARED_RESOURCES = 3;
   */
  SHARED_RESOURCES = 3,
}

/**
 * Describes the enum google.cloud.aiplatform.v1.Model.DeploymentResourcesType.
 */
export const Model_DeploymentResourcesTypeSchema: GenEnum<Model_DeploymentResourcesType> = /*@__PURE__*/
  enumDesc(file_google_cloud_aiplatform_v1_model, 0, 0);

/**
 * Contains information about the Large Model.
 *
 * @generated from message google.cloud.aiplatform.v1.LargeModelReference
 */
export type LargeModelReference = Message<"google.cloud.aiplatform.v1.LargeModelReference"> & {
  /**
   * Required. The unique name of the large Foundation or pre-built model. Like
   * "chat-bison", "text-bison". Or model name with version ID, like
   * "chat-bison@001", "text-bison@005", etc.
   *
   * @generated from field: string name = 1;
   */
  name: string;
};

/**
 * Describes the message google.cloud.aiplatform.v1.LargeModelReference.
 * Use `create(LargeModelReferenceSchema)` to create a new message.
 */
export const LargeModelReferenceSchema: GenMessage<LargeModelReference> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model, 1);

/**
 * Contains information about the source of the models generated from Model
 * Garden.
 *
 * @generated from message google.cloud.aiplatform.v1.ModelGardenSource
 */
export type ModelGardenSource = Message<"google.cloud.aiplatform.v1.ModelGardenSource"> & {
  /**
   * Required. The model garden source model resource name.
   *
   * @generated from field: string public_model_name = 1;
   */
  publicModelName: string;
};

/**
 * Describes the message google.cloud.aiplatform.v1.ModelGardenSource.
 * Use `create(ModelGardenSourceSchema)` to create a new message.
 */
export const ModelGardenSourceSchema: GenMessage<ModelGardenSource> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model, 2);

/**
 * Contains information about the source of the models generated from Generative
 * AI Studio.
 *
 * @generated from message google.cloud.aiplatform.v1.GenieSource
 */
export type GenieSource = Message<"google.cloud.aiplatform.v1.GenieSource"> & {
  /**
   * Required. The public base model URI.
   *
   * @generated from field: string base_model_uri = 1;
   */
  baseModelUri: string;
};

/**
 * Describes the message google.cloud.aiplatform.v1.GenieSource.
 * Use `create(GenieSourceSchema)` to create a new message.
 */
export const GenieSourceSchema: GenMessage<GenieSource> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model, 3);

/**
 * Contains the schemata used in Model's predictions and explanations via
 * [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict],
 * [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain]
 * and [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob].
 *
 * @generated from message google.cloud.aiplatform.v1.PredictSchemata
 */
export type PredictSchemata = Message<"google.cloud.aiplatform.v1.PredictSchemata"> & {
  /**
   * Immutable. Points to a YAML file stored on Google Cloud Storage describing
   * the format of a single instance, which are used in
   * [PredictRequest.instances][google.cloud.aiplatform.v1.PredictRequest.instances],
   * [ExplainRequest.instances][google.cloud.aiplatform.v1.ExplainRequest.instances]
   * and
   * [BatchPredictionJob.input_config][google.cloud.aiplatform.v1.BatchPredictionJob.input_config].
   * The schema is defined as an OpenAPI 3.0.2 [Schema
   * Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject).
   * AutoML Models always have this field populated by Vertex AI.
   * Note: The URI given on output will be immutable and probably different,
   * including the URI scheme, than the one given on input. The output URI will
   * point to a location where the user only has a read access.
   *
   * @generated from field: string instance_schema_uri = 1;
   */
  instanceSchemaUri: string;

  /**
   * Immutable. Points to a YAML file stored on Google Cloud Storage describing
   * the parameters of prediction and explanation via
   * [PredictRequest.parameters][google.cloud.aiplatform.v1.PredictRequest.parameters],
   * [ExplainRequest.parameters][google.cloud.aiplatform.v1.ExplainRequest.parameters]
   * and
   * [BatchPredictionJob.model_parameters][google.cloud.aiplatform.v1.BatchPredictionJob.model_parameters].
   * The schema is defined as an OpenAPI 3.0.2 [Schema
   * Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject).
   * AutoML Models always have this field populated by Vertex AI, if no
   * parameters are supported, then it is set to an empty string.
   * Note: The URI given on output will be immutable and probably different,
   * including the URI scheme, than the one given on input. The output URI will
   * point to a location where the user only has a read access.
   *
   * @generated from field: string parameters_schema_uri = 2;
   */
  parametersSchemaUri: string;

  /**
   * Immutable. Points to a YAML file stored on Google Cloud Storage describing
   * the format of a single prediction produced by this Model, which are
   * returned via
   * [PredictResponse.predictions][google.cloud.aiplatform.v1.PredictResponse.predictions],
   * [ExplainResponse.explanations][google.cloud.aiplatform.v1.ExplainResponse.explanations],
   * and
   * [BatchPredictionJob.output_config][google.cloud.aiplatform.v1.BatchPredictionJob.output_config].
   * The schema is defined as an OpenAPI 3.0.2 [Schema
   * Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject).
   * AutoML Models always have this field populated by Vertex AI.
   * Note: The URI given on output will be immutable and probably different,
   * including the URI scheme, than the one given on input. The output URI will
   * point to a location where the user only has a read access.
   *
   * @generated from field: string prediction_schema_uri = 3;
   */
  predictionSchemaUri: string;
};

/**
 * Describes the message google.cloud.aiplatform.v1.PredictSchemata.
 * Use `create(PredictSchemataSchema)` to create a new message.
 */
export const PredictSchemataSchema: GenMessage<PredictSchemata> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model, 4);

/**
 * Specification of a container for serving predictions. Some fields in this
 * message correspond to fields in the [Kubernetes Container v1 core
 * specification](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
 *
 * @generated from message google.cloud.aiplatform.v1.ModelContainerSpec
 */
export type ModelContainerSpec = Message<"google.cloud.aiplatform.v1.ModelContainerSpec"> & {
  /**
   * Required. Immutable. URI of the Docker image to be used as the custom
   * container for serving predictions. This URI must identify an image in
   * Artifact Registry or Container Registry. Learn more about the [container
   * publishing
   * requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#publishing),
   * including permissions requirements for the Vertex AI Service Agent.
   *
   * The container image is ingested upon
   * [ModelService.UploadModel][google.cloud.aiplatform.v1.ModelService.UploadModel],
   * stored internally, and this original path is afterwards not used.
   *
   * To learn about the requirements for the Docker image itself, see
   * [Custom container
   * requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#).
   *
   * You can use the URI to one of Vertex AI's [pre-built container images for
   * prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)
   * in this field.
   *
   * @generated from field: string image_uri = 1;
   */
  imageUri: string;

  /**
   * Immutable. Specifies the command that runs when the container starts. This
   * overrides the container's
   * [ENTRYPOINT](https://docs.docker.com/engine/reference/builder/#entrypoint).
   * Specify this field as an array of executable and arguments, similar to a
   * Docker `ENTRYPOINT`'s "exec" form, not its "shell" form.
   *
   * If you do not specify this field, then the container's `ENTRYPOINT` runs,
   * in conjunction with the
   * [args][google.cloud.aiplatform.v1.ModelContainerSpec.args] field or the
   * container's [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd),
   * if either exists. If this field is not specified and the container does not
   * have an `ENTRYPOINT`, then refer to the Docker documentation about [how
   * `CMD` and `ENTRYPOINT`
   * interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact).
   *
   * If you specify this field, then you can also specify the `args` field to
   * provide additional arguments for this command. However, if you specify this
   * field, then the container's `CMD` is ignored. See the
   * [Kubernetes documentation about how the
   * `command` and `args` fields interact with a container's `ENTRYPOINT` and
   * `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes).
   *
   * In this field, you can reference [environment variables set by Vertex
   * AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables)
   * and environment variables set in the
   * [env][google.cloud.aiplatform.v1.ModelContainerSpec.env] field. You cannot
   * reference environment variables set in the Docker image. In order for
   * environment variables to be expanded, reference them by using the following
   * syntax: <code>$(<var>VARIABLE_NAME</var>)</code> Note that this differs
   * from Bash variable expansion, which does not use parentheses. If a variable
   * cannot be resolved, the reference in the input string is used unchanged. To
   * avoid variable expansion, you can escape this syntax with `$$`; for
   * example: <code>$$(<var>VARIABLE_NAME</var>)</code> This field corresponds
   * to the `command` field of the Kubernetes Containers [v1 core
   * API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
   *
   * @generated from field: repeated string command = 2;
   */
  command: string[];

  /**
   * Immutable. Specifies arguments for the command that runs when the container
   * starts. This overrides the container's
   * [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd). Specify
   * this field as an array of executable and arguments, similar to a Docker
   * `CMD`'s "default parameters" form.
   *
   * If you don't specify this field but do specify the
   * [command][google.cloud.aiplatform.v1.ModelContainerSpec.command] field,
   * then the command from the `command` field runs without any additional
   * arguments. See the [Kubernetes documentation about how the `command` and
   * `args` fields interact with a container's `ENTRYPOINT` and
   * `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes).
   *
   * If you don't specify this field and don't specify the `command` field,
   * then the container's
   * [`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#cmd) and
   * `CMD` determine what runs based on their default behavior. See the Docker
   * documentation about [how `CMD` and `ENTRYPOINT`
   * interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact).
   *
   * In this field, you can reference [environment variables
   * set by Vertex
   * AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables)
   * and environment variables set in the
   * [env][google.cloud.aiplatform.v1.ModelContainerSpec.env] field. You cannot
   * reference environment variables set in the Docker image. In order for
   * environment variables to be expanded, reference them by using the following
   * syntax: <code>$(<var>VARIABLE_NAME</var>)</code> Note that this differs
   * from Bash variable expansion, which does not use parentheses. If a variable
   * cannot be resolved, the reference in the input string is used unchanged. To
   * avoid variable expansion, you can escape this syntax with `$$`; for
   * example: <code>$$(<var>VARIABLE_NAME</var>)</code> This field corresponds
   * to the `args` field of the Kubernetes Containers [v1 core
   * API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
   *
   * @generated from field: repeated string args = 3;
   */
  args: string[];

  /**
   * Immutable. List of environment variables to set in the container. After the
   * container starts running, code running in the container can read these
   * environment variables.
   *
   * Additionally, the
   * [command][google.cloud.aiplatform.v1.ModelContainerSpec.command] and
   * [args][google.cloud.aiplatform.v1.ModelContainerSpec.args] fields can
   * reference these variables. Later entries in this list can also reference
   * earlier entries. For example, the following example sets the variable
   * `VAR_2` to have the value `foo bar`:
   *
   * ```json
   * [
   *   {
   *     "name": "VAR_1",
   *     "value": "foo"
   *   },
   *   {
   *     "name": "VAR_2",
   *     "value": "$(VAR_1) bar"
   *   }
   * ]
   * ```
   *
   * If you switch the order of the variables in the example, then the expansion
   * does not occur.
   *
   * This field corresponds to the `env` field of the Kubernetes Containers
   * [v1 core
   * API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.EnvVar env = 4;
   */
  env: EnvVar[];

  /**
   * Immutable. List of ports to expose from the container. Vertex AI sends any
   * prediction requests that it receives to the first port on this list. Vertex
   * AI also sends
   * [liveness and health
   * checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#liveness)
   * to this port.
   *
   * If you do not specify this field, it defaults to following value:
   *
   * ```json
   * [
   *   {
   *     "containerPort": 8080
   *   }
   * ]
   * ```
   *
   * Vertex AI does not use ports other than the first one listed. This field
   * corresponds to the `ports` field of the Kubernetes Containers
   * [v1 core
   * API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Port ports = 5;
   */
  ports: Port[];

  /**
   * Immutable. HTTP path on the container to send prediction requests to.
   * Vertex AI forwards requests sent using
   * [projects.locations.endpoints.predict][google.cloud.aiplatform.v1.PredictionService.Predict]
   * to this path on the container's IP address and port. Vertex AI then returns
   * the container's response in the API response.
   *
   * For example, if you set this field to `/foo`, then when Vertex AI
   * receives a prediction request, it forwards the request body in a POST
   * request to the `/foo` path on the port of your container specified by the
   * first value of this `ModelContainerSpec`'s
   * [ports][google.cloud.aiplatform.v1.ModelContainerSpec.ports] field.
   *
   * If you don't specify this field, it defaults to the following value when
   * you [deploy this Model to an
   * Endpoint][google.cloud.aiplatform.v1.EndpointService.DeployModel]:
   * <code>/v1/endpoints/<var>ENDPOINT</var>/deployedModels/<var>DEPLOYED_MODEL</var>:predict</code>
   * The placeholders in this value are replaced as follows:
   *
   * * <var>ENDPOINT</var>: The last segment (following `endpoints/`)of the
   *   Endpoint.name][] field of the Endpoint where this Model has been
   *   deployed. (Vertex AI makes this value available to your container code
   *   as the [`AIP_ENDPOINT_ID` environment
   *  variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
   *
   * * <var>DEPLOYED_MODEL</var>:
   * [DeployedModel.id][google.cloud.aiplatform.v1.DeployedModel.id] of the
   * `DeployedModel`.
   *   (Vertex AI makes this value available to your container code
   *   as the [`AIP_DEPLOYED_MODEL_ID` environment
   *   variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
   *
   * @generated from field: string predict_route = 6;
   */
  predictRoute: string;

  /**
   * Immutable. HTTP path on the container to send health checks to. Vertex AI
   * intermittently sends GET requests to this path on the container's IP
   * address and port to check that the container is healthy. Read more about
   * [health
   * checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#health).
   *
   * For example, if you set this field to `/bar`, then Vertex AI
   * intermittently sends a GET request to the `/bar` path on the port of your
   * container specified by the first value of this `ModelContainerSpec`'s
   * [ports][google.cloud.aiplatform.v1.ModelContainerSpec.ports] field.
   *
   * If you don't specify this field, it defaults to the following value when
   * you [deploy this Model to an
   * Endpoint][google.cloud.aiplatform.v1.EndpointService.DeployModel]:
   * <code>/v1/endpoints/<var>ENDPOINT</var>/deployedModels/<var>DEPLOYED_MODEL</var>:predict</code>
   * The placeholders in this value are replaced as follows:
   *
   * * <var>ENDPOINT</var>: The last segment (following `endpoints/`)of the
   *   Endpoint.name][] field of the Endpoint where this Model has been
   *   deployed. (Vertex AI makes this value available to your container code
   *   as the [`AIP_ENDPOINT_ID` environment
   *   variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
   *
   * * <var>DEPLOYED_MODEL</var>:
   * [DeployedModel.id][google.cloud.aiplatform.v1.DeployedModel.id] of the
   * `DeployedModel`.
   *   (Vertex AI makes this value available to your container code as the
   *   [`AIP_DEPLOYED_MODEL_ID` environment
   *   variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
   *
   * @generated from field: string health_route = 7;
   */
  healthRoute: string;

  /**
   * Immutable. List of ports to expose from the container. Vertex AI sends gRPC
   * prediction requests that it receives to the first port on this list. Vertex
   * AI also sends liveness and health checks to this port.
   *
   * If you do not specify this field, gRPC requests to the container will be
   * disabled.
   *
   * Vertex AI does not use ports other than the first one listed. This field
   * corresponds to the `ports` field of the Kubernetes Containers v1 core API.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Port grpc_ports = 9;
   */
  grpcPorts: Port[];

  /**
   * Immutable. Deployment timeout.
   * Limit for deployment timeout is 2 hours.
   *
   * @generated from field: google.protobuf.Duration deployment_timeout = 10;
   */
  deploymentTimeout?: Duration;

  /**
   * Immutable. The amount of the VM memory to reserve as the shared memory for
   * the model in megabytes.
   *
   * @generated from field: int64 shared_memory_size_mb = 11;
   */
  sharedMemorySizeMb: bigint;

  /**
   * Immutable. Specification for Kubernetes startup probe.
   *
   * @generated from field: google.cloud.aiplatform.v1.Probe startup_probe = 12;
   */
  startupProbe?: Probe;

  /**
   * Immutable. Specification for Kubernetes readiness probe.
   *
   * @generated from field: google.cloud.aiplatform.v1.Probe health_probe = 13;
   */
  healthProbe?: Probe;
};

/**
 * Describes the message google.cloud.aiplatform.v1.ModelContainerSpec.
 * Use `create(ModelContainerSpecSchema)` to create a new message.
 */
export const ModelContainerSpecSchema: GenMessage<ModelContainerSpec> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model, 5);

/**
 * Represents a network port in a container.
 *
 * @generated from message google.cloud.aiplatform.v1.Port
 */
export type Port = Message<"google.cloud.aiplatform.v1.Port"> & {
  /**
   * The number of the port to expose on the pod's IP address.
   * Must be a valid port number, between 1 and 65535 inclusive.
   *
   * @generated from field: int32 container_port = 3;
   */
  containerPort: number;
};

/**
 * Describes the message google.cloud.aiplatform.v1.Port.
 * Use `create(PortSchema)` to create a new message.
 */
export const PortSchema: GenMessage<Port> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model, 6);

/**
 * Detail description of the source information of the model.
 *
 * @generated from message google.cloud.aiplatform.v1.ModelSourceInfo
 */
export type ModelSourceInfo = Message<"google.cloud.aiplatform.v1.ModelSourceInfo"> & {
  /**
   * Type of the model source.
   *
   * @generated from field: google.cloud.aiplatform.v1.ModelSourceInfo.ModelSourceType source_type = 1;
   */
  sourceType: ModelSourceInfo_ModelSourceType;

  /**
   * If this Model is copy of another Model. If true then
   * [source_type][google.cloud.aiplatform.v1.ModelSourceInfo.source_type]
   * pertains to the original.
   *
   * @generated from field: bool copy = 2;
   */
  copy: boolean;
};

/**
 * Describes the message google.cloud.aiplatform.v1.ModelSourceInfo.
 * Use `create(ModelSourceInfoSchema)` to create a new message.
 */
export const ModelSourceInfoSchema: GenMessage<ModelSourceInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model, 7);

/**
 * Source of the model.
 * Different from `objective` field, this `ModelSourceType` enum
 * indicates the source from which the model was accessed or obtained,
 * whereas the `objective` indicates the overall aim or function of this
 * model.
 *
 * @generated from enum google.cloud.aiplatform.v1.ModelSourceInfo.ModelSourceType
 */
export enum ModelSourceInfo_ModelSourceType {
  /**
   * Should not be used.
   *
   * @generated from enum value: MODEL_SOURCE_TYPE_UNSPECIFIED = 0;
   */
  MODEL_SOURCE_TYPE_UNSPECIFIED = 0,

  /**
   * The Model is uploaded by automl training pipeline.
   *
   * @generated from enum value: AUTOML = 1;
   */
  AUTOML = 1,

  /**
   * The Model is uploaded by user or custom training pipeline.
   *
   * @generated from enum value: CUSTOM = 2;
   */
  CUSTOM = 2,

  /**
   * The Model is registered and sync'ed from BigQuery ML.
   *
   * @generated from enum value: BQML = 3;
   */
  BQML = 3,

  /**
   * The Model is saved or tuned from Model Garden.
   *
   * @generated from enum value: MODEL_GARDEN = 4;
   */
  MODEL_GARDEN = 4,

  /**
   * The Model is saved or tuned from Genie.
   *
   * @generated from enum value: GENIE = 5;
   */
  GENIE = 5,

  /**
   * The Model is uploaded by text embedding finetuning pipeline.
   *
   * @generated from enum value: CUSTOM_TEXT_EMBEDDING = 6;
   */
  CUSTOM_TEXT_EMBEDDING = 6,

  /**
   * The Model is saved or tuned from Marketplace.
   *
   * @generated from enum value: MARKETPLACE = 7;
   */
  MARKETPLACE = 7,
}

/**
 * Describes the enum google.cloud.aiplatform.v1.ModelSourceInfo.ModelSourceType.
 */
export const ModelSourceInfo_ModelSourceTypeSchema: GenEnum<ModelSourceInfo_ModelSourceType> = /*@__PURE__*/
  enumDesc(file_google_cloud_aiplatform_v1_model, 7, 0);

/**
 * Probe describes a health check to be performed against a container to
 * determine whether it is alive or ready to receive traffic.
 *
 * @generated from message google.cloud.aiplatform.v1.Probe
 */
export type Probe = Message<"google.cloud.aiplatform.v1.Probe"> & {
  /**
   * @generated from oneof google.cloud.aiplatform.v1.Probe.probe_type
   */
  probeType: {
    /**
     * ExecAction probes the health of a container by executing a command.
     *
     * @generated from field: google.cloud.aiplatform.v1.Probe.ExecAction exec = 1;
     */
    value: Probe_ExecAction;
    case: "exec";
  } | { case: undefined; value?: undefined };

  /**
   * How often (in seconds) to perform the probe. Default to 10 seconds.
   * Minimum value is 1. Must be less than timeout_seconds.
   *
   * Maps to Kubernetes probe argument 'periodSeconds'.
   *
   * @generated from field: int32 period_seconds = 2;
   */
  periodSeconds: number;

  /**
   * Number of seconds after which the probe times out. Defaults to 1 second.
   * Minimum value is 1. Must be greater or equal to period_seconds.
   *
   * Maps to Kubernetes probe argument 'timeoutSeconds'.
   *
   * @generated from field: int32 timeout_seconds = 3;
   */
  timeoutSeconds: number;
};

/**
 * Describes the message google.cloud.aiplatform.v1.Probe.
 * Use `create(ProbeSchema)` to create a new message.
 */
export const ProbeSchema: GenMessage<Probe> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model, 8);

/**
 * ExecAction specifies a command to execute.
 *
 * @generated from message google.cloud.aiplatform.v1.Probe.ExecAction
 */
export type Probe_ExecAction = Message<"google.cloud.aiplatform.v1.Probe.ExecAction"> & {
  /**
   * Command is the command line to execute inside the container, the working
   * directory for the command is root ('/') in the container's filesystem.
   * The command is simply exec'd, it is not run inside a shell, so
   * traditional shell instructions ('|', etc) won't work. To use a shell, you
   * need to explicitly call out to that shell. Exit status of 0 is treated as
   * live/healthy and non-zero is unhealthy.
   *
   * @generated from field: repeated string command = 1;
   */
  command: string[];
};

/**
 * Describes the message google.cloud.aiplatform.v1.Probe.ExecAction.
 * Use `create(Probe_ExecActionSchema)` to create a new message.
 */
export const Probe_ExecActionSchema: GenMessage<Probe_ExecAction> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model, 8, 0);

