// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/aiplatform/v1/prediction_service.proto (package google.cloud.aiplatform.v1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import type { HttpBody, HttpBodySchema } from "../../../api/httpbody_pb";
import { file_google_api_httpbody } from "../../../api/httpbody_pb";
import { file_google_api_resource } from "../../../api/resource_pb";
import type { Candidate, Content, GenerationConfig, SafetyRating, SafetySetting } from "./content_pb";
import { file_google_cloud_aiplatform_v1_content } from "./content_pb";
import type { Explanation, ExplanationSpecOverride } from "./explanation_pb";
import { file_google_cloud_aiplatform_v1_explanation } from "./explanation_pb";
import type { Tool, ToolConfig } from "./tool_pb";
import { file_google_cloud_aiplatform_v1_tool } from "./tool_pb";
import type { Tensor } from "./types_pb";
import { file_google_cloud_aiplatform_v1_types } from "./types_pb";
import type { Value } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_struct } from "@bufbuild/protobuf/wkt";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/aiplatform/v1/prediction_service.proto.
 */
export const file_google_cloud_aiplatform_v1_prediction_service: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_api_httpbody, file_google_api_resource, file_google_cloud_aiplatform_v1_content, file_google_cloud_aiplatform_v1_explanation, file_google_cloud_aiplatform_v1_tool, file_google_cloud_aiplatform_v1_types, file_google_protobuf_struct]);

/**
 * Request message for
 * [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict].
 *
 * @generated from message google.cloud.aiplatform.v1.PredictRequest
 */
export type PredictRequest = Message<"google.cloud.aiplatform.v1.PredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * Required. The instances that are the input to the prediction call.
   * A DeployedModel may have an upper limit on the number of instances it
   * supports per request, and when it is exceeded the prediction call errors
   * in case of AutoML Models, or, in case of customer created Models, the
   * behaviour is as documented by that Model.
   * The schema of any single instance may be specified via Endpoint's
   * DeployedModels' [Model's][google.cloud.aiplatform.v1.DeployedModel.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
   * [instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri].
   *
   * @generated from field: repeated google.protobuf.Value instances = 2;
   */
  instances: Value[];

  /**
   * The parameters that govern the prediction. The schema of the parameters may
   * be specified via Endpoint's DeployedModels' [Model's
   * ][google.cloud.aiplatform.v1.DeployedModel.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
   * [parameters_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri].
   *
   * @generated from field: google.protobuf.Value parameters = 3;
   */
  parameters?: Value;
};

/**
 * Describes the message google.cloud.aiplatform.v1.PredictRequest.
 * Use `create(PredictRequestSchema)` to create a new message.
 */
export const PredictRequestSchema: GenMessage<PredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 0);

/**
 * Response message for
 * [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict].
 *
 * @generated from message google.cloud.aiplatform.v1.PredictResponse
 */
export type PredictResponse = Message<"google.cloud.aiplatform.v1.PredictResponse"> & {
  /**
   * The predictions that are the output of the predictions call.
   * The schema of any single prediction may be specified via Endpoint's
   * DeployedModels' [Model's ][google.cloud.aiplatform.v1.DeployedModel.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
   * [prediction_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.prediction_schema_uri].
   *
   * @generated from field: repeated google.protobuf.Value predictions = 1;
   */
  predictions: Value[];

  /**
   * ID of the Endpoint's DeployedModel that served this prediction.
   *
   * @generated from field: string deployed_model_id = 2;
   */
  deployedModelId: string;

  /**
   * Output only. The resource name of the Model which is deployed as the
   * DeployedModel that this prediction hits.
   *
   * @generated from field: string model = 3;
   */
  model: string;

  /**
   * Output only. The version ID of the Model which is deployed as the
   * DeployedModel that this prediction hits.
   *
   * @generated from field: string model_version_id = 5;
   */
  modelVersionId: string;

  /**
   * Output only. The [display
   * name][google.cloud.aiplatform.v1.Model.display_name] of the Model which is
   * deployed as the DeployedModel that this prediction hits.
   *
   * @generated from field: string model_display_name = 4;
   */
  modelDisplayName: string;

  /**
   * Output only. Request-level metadata returned by the model. The metadata
   * type will be dependent upon the model implementation.
   *
   * @generated from field: google.protobuf.Value metadata = 6;
   */
  metadata?: Value;
};

/**
 * Describes the message google.cloud.aiplatform.v1.PredictResponse.
 * Use `create(PredictResponseSchema)` to create a new message.
 */
export const PredictResponseSchema: GenMessage<PredictResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 1);

/**
 * Request message for
 * [PredictionService.RawPredict][google.cloud.aiplatform.v1.PredictionService.RawPredict].
 *
 * @generated from message google.cloud.aiplatform.v1.RawPredictRequest
 */
export type RawPredictRequest = Message<"google.cloud.aiplatform.v1.RawPredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * The prediction input. Supports HTTP headers and arbitrary data payload.
   *
   * A [DeployedModel][google.cloud.aiplatform.v1.DeployedModel] may have an
   * upper limit on the number of instances it supports per request. When this
   * limit it is exceeded for an AutoML model, the
   * [RawPredict][google.cloud.aiplatform.v1.PredictionService.RawPredict]
   * method returns an error. When this limit is exceeded for a custom-trained
   * model, the behavior varies depending on the model.
   *
   * You can specify the schema for each instance in the
   * [predict_schemata.instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
   * field when you create a [Model][google.cloud.aiplatform.v1.Model]. This
   * schema applies when you deploy the `Model` as a `DeployedModel` to an
   * [Endpoint][google.cloud.aiplatform.v1.Endpoint] and use the `RawPredict`
   * method.
   *
   * @generated from field: google.api.HttpBody http_body = 2;
   */
  httpBody?: HttpBody;
};

/**
 * Describes the message google.cloud.aiplatform.v1.RawPredictRequest.
 * Use `create(RawPredictRequestSchema)` to create a new message.
 */
export const RawPredictRequestSchema: GenMessage<RawPredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 2);

/**
 * Request message for
 * [PredictionService.StreamRawPredict][google.cloud.aiplatform.v1.PredictionService.StreamRawPredict].
 *
 * @generated from message google.cloud.aiplatform.v1.StreamRawPredictRequest
 */
export type StreamRawPredictRequest = Message<"google.cloud.aiplatform.v1.StreamRawPredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * The prediction input. Supports HTTP headers and arbitrary data payload.
   *
   * @generated from field: google.api.HttpBody http_body = 2;
   */
  httpBody?: HttpBody;
};

/**
 * Describes the message google.cloud.aiplatform.v1.StreamRawPredictRequest.
 * Use `create(StreamRawPredictRequestSchema)` to create a new message.
 */
export const StreamRawPredictRequestSchema: GenMessage<StreamRawPredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 3);

/**
 * Request message for
 * [PredictionService.DirectPredict][google.cloud.aiplatform.v1.PredictionService.DirectPredict].
 *
 * @generated from message google.cloud.aiplatform.v1.DirectPredictRequest
 */
export type DirectPredictRequest = Message<"google.cloud.aiplatform.v1.DirectPredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * The prediction input.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Tensor inputs = 2;
   */
  inputs: Tensor[];

  /**
   * The parameters that govern the prediction.
   *
   * @generated from field: google.cloud.aiplatform.v1.Tensor parameters = 3;
   */
  parameters?: Tensor;
};

/**
 * Describes the message google.cloud.aiplatform.v1.DirectPredictRequest.
 * Use `create(DirectPredictRequestSchema)` to create a new message.
 */
export const DirectPredictRequestSchema: GenMessage<DirectPredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 4);

/**
 * Response message for
 * [PredictionService.DirectPredict][google.cloud.aiplatform.v1.PredictionService.DirectPredict].
 *
 * @generated from message google.cloud.aiplatform.v1.DirectPredictResponse
 */
export type DirectPredictResponse = Message<"google.cloud.aiplatform.v1.DirectPredictResponse"> & {
  /**
   * The prediction output.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Tensor outputs = 1;
   */
  outputs: Tensor[];

  /**
   * The parameters that govern the prediction.
   *
   * @generated from field: google.cloud.aiplatform.v1.Tensor parameters = 2;
   */
  parameters?: Tensor;
};

/**
 * Describes the message google.cloud.aiplatform.v1.DirectPredictResponse.
 * Use `create(DirectPredictResponseSchema)` to create a new message.
 */
export const DirectPredictResponseSchema: GenMessage<DirectPredictResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 5);

/**
 * Request message for
 * [PredictionService.DirectRawPredict][google.cloud.aiplatform.v1.PredictionService.DirectRawPredict].
 *
 * @generated from message google.cloud.aiplatform.v1.DirectRawPredictRequest
 */
export type DirectRawPredictRequest = Message<"google.cloud.aiplatform.v1.DirectRawPredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * Fully qualified name of the API method being invoked to perform
   * predictions.
   *
   * Format:
   * `/namespace.Service/Method/`
   * Example:
   * `/tensorflow.serving.PredictionService/Predict`
   *
   * @generated from field: string method_name = 2;
   */
  methodName: string;

  /**
   * The prediction input.
   *
   * @generated from field: bytes input = 3;
   */
  input: Uint8Array;
};

/**
 * Describes the message google.cloud.aiplatform.v1.DirectRawPredictRequest.
 * Use `create(DirectRawPredictRequestSchema)` to create a new message.
 */
export const DirectRawPredictRequestSchema: GenMessage<DirectRawPredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 6);

/**
 * Response message for
 * [PredictionService.DirectRawPredict][google.cloud.aiplatform.v1.PredictionService.DirectRawPredict].
 *
 * @generated from message google.cloud.aiplatform.v1.DirectRawPredictResponse
 */
export type DirectRawPredictResponse = Message<"google.cloud.aiplatform.v1.DirectRawPredictResponse"> & {
  /**
   * The prediction output.
   *
   * @generated from field: bytes output = 1;
   */
  output: Uint8Array;
};

/**
 * Describes the message google.cloud.aiplatform.v1.DirectRawPredictResponse.
 * Use `create(DirectRawPredictResponseSchema)` to create a new message.
 */
export const DirectRawPredictResponseSchema: GenMessage<DirectRawPredictResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 7);

/**
 * Request message for
 * [PredictionService.StreamDirectPredict][google.cloud.aiplatform.v1.PredictionService.StreamDirectPredict].
 *
 * The first message must contain
 * [endpoint][google.cloud.aiplatform.v1.StreamDirectPredictRequest.endpoint]
 * field and optionally [input][]. The subsequent messages must contain
 * [input][].
 *
 * @generated from message google.cloud.aiplatform.v1.StreamDirectPredictRequest
 */
export type StreamDirectPredictRequest = Message<"google.cloud.aiplatform.v1.StreamDirectPredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * Optional. The prediction input.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Tensor inputs = 2;
   */
  inputs: Tensor[];

  /**
   * Optional. The parameters that govern the prediction.
   *
   * @generated from field: google.cloud.aiplatform.v1.Tensor parameters = 3;
   */
  parameters?: Tensor;
};

/**
 * Describes the message google.cloud.aiplatform.v1.StreamDirectPredictRequest.
 * Use `create(StreamDirectPredictRequestSchema)` to create a new message.
 */
export const StreamDirectPredictRequestSchema: GenMessage<StreamDirectPredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 8);

/**
 * Response message for
 * [PredictionService.StreamDirectPredict][google.cloud.aiplatform.v1.PredictionService.StreamDirectPredict].
 *
 * @generated from message google.cloud.aiplatform.v1.StreamDirectPredictResponse
 */
export type StreamDirectPredictResponse = Message<"google.cloud.aiplatform.v1.StreamDirectPredictResponse"> & {
  /**
   * The prediction output.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Tensor outputs = 1;
   */
  outputs: Tensor[];

  /**
   * The parameters that govern the prediction.
   *
   * @generated from field: google.cloud.aiplatform.v1.Tensor parameters = 2;
   */
  parameters?: Tensor;
};

/**
 * Describes the message google.cloud.aiplatform.v1.StreamDirectPredictResponse.
 * Use `create(StreamDirectPredictResponseSchema)` to create a new message.
 */
export const StreamDirectPredictResponseSchema: GenMessage<StreamDirectPredictResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 9);

/**
 * Request message for
 * [PredictionService.StreamDirectRawPredict][google.cloud.aiplatform.v1.PredictionService.StreamDirectRawPredict].
 *
 * The first message must contain
 * [endpoint][google.cloud.aiplatform.v1.StreamDirectRawPredictRequest.endpoint]
 * and
 * [method_name][google.cloud.aiplatform.v1.StreamDirectRawPredictRequest.method_name]
 * fields and optionally
 * [input][google.cloud.aiplatform.v1.StreamDirectRawPredictRequest.input]. The
 * subsequent messages must contain
 * [input][google.cloud.aiplatform.v1.StreamDirectRawPredictRequest.input].
 * [method_name][google.cloud.aiplatform.v1.StreamDirectRawPredictRequest.method_name]
 * in the subsequent messages have no effect.
 *
 * @generated from message google.cloud.aiplatform.v1.StreamDirectRawPredictRequest
 */
export type StreamDirectRawPredictRequest = Message<"google.cloud.aiplatform.v1.StreamDirectRawPredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * Optional. Fully qualified name of the API method being invoked to perform
   * predictions.
   *
   * Format:
   * `/namespace.Service/Method/`
   * Example:
   * `/tensorflow.serving.PredictionService/Predict`
   *
   * @generated from field: string method_name = 2;
   */
  methodName: string;

  /**
   * Optional. The prediction input.
   *
   * @generated from field: bytes input = 3;
   */
  input: Uint8Array;
};

/**
 * Describes the message google.cloud.aiplatform.v1.StreamDirectRawPredictRequest.
 * Use `create(StreamDirectRawPredictRequestSchema)` to create a new message.
 */
export const StreamDirectRawPredictRequestSchema: GenMessage<StreamDirectRawPredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 10);

/**
 * Response message for
 * [PredictionService.StreamDirectRawPredict][google.cloud.aiplatform.v1.PredictionService.StreamDirectRawPredict].
 *
 * @generated from message google.cloud.aiplatform.v1.StreamDirectRawPredictResponse
 */
export type StreamDirectRawPredictResponse = Message<"google.cloud.aiplatform.v1.StreamDirectRawPredictResponse"> & {
  /**
   * The prediction output.
   *
   * @generated from field: bytes output = 1;
   */
  output: Uint8Array;
};

/**
 * Describes the message google.cloud.aiplatform.v1.StreamDirectRawPredictResponse.
 * Use `create(StreamDirectRawPredictResponseSchema)` to create a new message.
 */
export const StreamDirectRawPredictResponseSchema: GenMessage<StreamDirectRawPredictResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 11);

/**
 * Request message for
 * [PredictionService.StreamingPredict][google.cloud.aiplatform.v1.PredictionService.StreamingPredict].
 *
 * The first message must contain
 * [endpoint][google.cloud.aiplatform.v1.StreamingPredictRequest.endpoint] field
 * and optionally [input][]. The subsequent messages must contain [input][].
 *
 * @generated from message google.cloud.aiplatform.v1.StreamingPredictRequest
 */
export type StreamingPredictRequest = Message<"google.cloud.aiplatform.v1.StreamingPredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * The prediction input.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Tensor inputs = 2;
   */
  inputs: Tensor[];

  /**
   * The parameters that govern the prediction.
   *
   * @generated from field: google.cloud.aiplatform.v1.Tensor parameters = 3;
   */
  parameters?: Tensor;
};

/**
 * Describes the message google.cloud.aiplatform.v1.StreamingPredictRequest.
 * Use `create(StreamingPredictRequestSchema)` to create a new message.
 */
export const StreamingPredictRequestSchema: GenMessage<StreamingPredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 12);

/**
 * Response message for
 * [PredictionService.StreamingPredict][google.cloud.aiplatform.v1.PredictionService.StreamingPredict].
 *
 * @generated from message google.cloud.aiplatform.v1.StreamingPredictResponse
 */
export type StreamingPredictResponse = Message<"google.cloud.aiplatform.v1.StreamingPredictResponse"> & {
  /**
   * The prediction output.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Tensor outputs = 1;
   */
  outputs: Tensor[];

  /**
   * The parameters that govern the prediction.
   *
   * @generated from field: google.cloud.aiplatform.v1.Tensor parameters = 2;
   */
  parameters?: Tensor;
};

/**
 * Describes the message google.cloud.aiplatform.v1.StreamingPredictResponse.
 * Use `create(StreamingPredictResponseSchema)` to create a new message.
 */
export const StreamingPredictResponseSchema: GenMessage<StreamingPredictResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 13);

/**
 * Request message for
 * [PredictionService.StreamingRawPredict][google.cloud.aiplatform.v1.PredictionService.StreamingRawPredict].
 *
 * The first message must contain
 * [endpoint][google.cloud.aiplatform.v1.StreamingRawPredictRequest.endpoint]
 * and
 * [method_name][google.cloud.aiplatform.v1.StreamingRawPredictRequest.method_name]
 * fields and optionally
 * [input][google.cloud.aiplatform.v1.StreamingRawPredictRequest.input]. The
 * subsequent messages must contain
 * [input][google.cloud.aiplatform.v1.StreamingRawPredictRequest.input].
 * [method_name][google.cloud.aiplatform.v1.StreamingRawPredictRequest.method_name]
 * in the subsequent messages have no effect.
 *
 * @generated from message google.cloud.aiplatform.v1.StreamingRawPredictRequest
 */
export type StreamingRawPredictRequest = Message<"google.cloud.aiplatform.v1.StreamingRawPredictRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the prediction.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * Fully qualified name of the API method being invoked to perform
   * predictions.
   *
   * Format:
   * `/namespace.Service/Method/`
   * Example:
   * `/tensorflow.serving.PredictionService/Predict`
   *
   * @generated from field: string method_name = 2;
   */
  methodName: string;

  /**
   * The prediction input.
   *
   * @generated from field: bytes input = 3;
   */
  input: Uint8Array;
};

/**
 * Describes the message google.cloud.aiplatform.v1.StreamingRawPredictRequest.
 * Use `create(StreamingRawPredictRequestSchema)` to create a new message.
 */
export const StreamingRawPredictRequestSchema: GenMessage<StreamingRawPredictRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 14);

/**
 * Response message for
 * [PredictionService.StreamingRawPredict][google.cloud.aiplatform.v1.PredictionService.StreamingRawPredict].
 *
 * @generated from message google.cloud.aiplatform.v1.StreamingRawPredictResponse
 */
export type StreamingRawPredictResponse = Message<"google.cloud.aiplatform.v1.StreamingRawPredictResponse"> & {
  /**
   * The prediction output.
   *
   * @generated from field: bytes output = 1;
   */
  output: Uint8Array;
};

/**
 * Describes the message google.cloud.aiplatform.v1.StreamingRawPredictResponse.
 * Use `create(StreamingRawPredictResponseSchema)` to create a new message.
 */
export const StreamingRawPredictResponseSchema: GenMessage<StreamingRawPredictResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 15);

/**
 * Request message for
 * [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain].
 *
 * @generated from message google.cloud.aiplatform.v1.ExplainRequest
 */
export type ExplainRequest = Message<"google.cloud.aiplatform.v1.ExplainRequest"> & {
  /**
   * Required. The name of the Endpoint requested to serve the explanation.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * Required. The instances that are the input to the explanation call.
   * A DeployedModel may have an upper limit on the number of instances it
   * supports per request, and when it is exceeded the explanation call errors
   * in case of AutoML Models, or, in case of customer created Models, the
   * behaviour is as documented by that Model.
   * The schema of any single instance may be specified via Endpoint's
   * DeployedModels' [Model's][google.cloud.aiplatform.v1.DeployedModel.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
   * [instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri].
   *
   * @generated from field: repeated google.protobuf.Value instances = 2;
   */
  instances: Value[];

  /**
   * The parameters that govern the prediction. The schema of the parameters may
   * be specified via Endpoint's DeployedModels' [Model's
   * ][google.cloud.aiplatform.v1.DeployedModel.model]
   * [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
   * [parameters_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri].
   *
   * @generated from field: google.protobuf.Value parameters = 4;
   */
  parameters?: Value;

  /**
   * If specified, overrides the
   * [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
   * of the DeployedModel. Can be used for explaining prediction results with
   * different configurations, such as:
   *  - Explaining top-5 predictions results as opposed to top-1;
   *  - Increasing path count or step count of the attribution methods to reduce
   *    approximate errors;
   *  - Using different baselines for explaining the prediction results.
   *
   * @generated from field: google.cloud.aiplatform.v1.ExplanationSpecOverride explanation_spec_override = 5;
   */
  explanationSpecOverride?: ExplanationSpecOverride;

  /**
   * If specified, this ExplainRequest will be served by the chosen
   * DeployedModel, overriding
   * [Endpoint.traffic_split][google.cloud.aiplatform.v1.Endpoint.traffic_split].
   *
   * @generated from field: string deployed_model_id = 3;
   */
  deployedModelId: string;
};

/**
 * Describes the message google.cloud.aiplatform.v1.ExplainRequest.
 * Use `create(ExplainRequestSchema)` to create a new message.
 */
export const ExplainRequestSchema: GenMessage<ExplainRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 16);

/**
 * Response message for
 * [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain].
 *
 * @generated from message google.cloud.aiplatform.v1.ExplainResponse
 */
export type ExplainResponse = Message<"google.cloud.aiplatform.v1.ExplainResponse"> & {
  /**
   * The explanations of the Model's
   * [PredictResponse.predictions][google.cloud.aiplatform.v1.PredictResponse.predictions].
   *
   * It has the same number of elements as
   * [instances][google.cloud.aiplatform.v1.ExplainRequest.instances] to be
   * explained.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Explanation explanations = 1;
   */
  explanations: Explanation[];

  /**
   * ID of the Endpoint's DeployedModel that served this explanation.
   *
   * @generated from field: string deployed_model_id = 2;
   */
  deployedModelId: string;

  /**
   * The predictions that are the output of the predictions call.
   * Same as
   * [PredictResponse.predictions][google.cloud.aiplatform.v1.PredictResponse.predictions].
   *
   * @generated from field: repeated google.protobuf.Value predictions = 3;
   */
  predictions: Value[];
};

/**
 * Describes the message google.cloud.aiplatform.v1.ExplainResponse.
 * Use `create(ExplainResponseSchema)` to create a new message.
 */
export const ExplainResponseSchema: GenMessage<ExplainResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 17);

/**
 * Request message for [PredictionService.CountTokens][].
 *
 * @generated from message google.cloud.aiplatform.v1.CountTokensRequest
 */
export type CountTokensRequest = Message<"google.cloud.aiplatform.v1.CountTokensRequest"> & {
  /**
   * Required. The name of the Endpoint requested to perform token counting.
   * Format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 1;
   */
  endpoint: string;

  /**
   * Optional. The name of the publisher model requested to serve the
   * prediction. Format:
   * `projects/{project}/locations/{location}/publishers/*\/models/*`
   *
   * @generated from field: string model = 3;
   */
  model: string;

  /**
   * Optional. The instances that are the input to token counting call.
   * Schema is identical to the prediction schema of the underlying model.
   *
   * @generated from field: repeated google.protobuf.Value instances = 2;
   */
  instances: Value[];

  /**
   * Optional. Input content.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Content contents = 4;
   */
  contents: Content[];

  /**
   * Optional. The user provided system instructions for the model.
   * Note: only text should be used in parts and content in each part will be in
   * a separate paragraph.
   *
   * @generated from field: optional google.cloud.aiplatform.v1.Content system_instruction = 5;
   */
  systemInstruction?: Content;

  /**
   * Optional. A list of `Tools` the model may use to generate the next
   * response.
   *
   * A `Tool` is a piece of code that enables the system to interact with
   * external systems to perform an action, or set of actions, outside of
   * knowledge and scope of the model.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Tool tools = 6;
   */
  tools: Tool[];

  /**
   * Optional. Generation config that the model will use to generate the
   * response.
   *
   * @generated from field: optional google.cloud.aiplatform.v1.GenerationConfig generation_config = 7;
   */
  generationConfig?: GenerationConfig;
};

/**
 * Describes the message google.cloud.aiplatform.v1.CountTokensRequest.
 * Use `create(CountTokensRequestSchema)` to create a new message.
 */
export const CountTokensRequestSchema: GenMessage<CountTokensRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 18);

/**
 * Response message for [PredictionService.CountTokens][].
 *
 * @generated from message google.cloud.aiplatform.v1.CountTokensResponse
 */
export type CountTokensResponse = Message<"google.cloud.aiplatform.v1.CountTokensResponse"> & {
  /**
   * The total number of tokens counted across all instances from the request.
   *
   * @generated from field: int32 total_tokens = 1;
   */
  totalTokens: number;

  /**
   * The total number of billable characters counted across all instances from
   * the request.
   *
   * @generated from field: int32 total_billable_characters = 2;
   */
  totalBillableCharacters: number;
};

/**
 * Describes the message google.cloud.aiplatform.v1.CountTokensResponse.
 * Use `create(CountTokensResponseSchema)` to create a new message.
 */
export const CountTokensResponseSchema: GenMessage<CountTokensResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 19);

/**
 * Request message for [PredictionService.GenerateContent].
 *
 * @generated from message google.cloud.aiplatform.v1.GenerateContentRequest
 */
export type GenerateContentRequest = Message<"google.cloud.aiplatform.v1.GenerateContentRequest"> & {
  /**
   * Required. The fully qualified name of the publisher model or tuned model
   * endpoint to use.
   *
   * Publisher model format:
   * `projects/{project}/locations/{location}/publishers/*\/models/*`
   *
   * Tuned model endpoint format:
   * `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string model = 5;
   */
  model: string;

  /**
   * Required. The content of the current conversation with the model.
   *
   * For single-turn queries, this is a single instance. For multi-turn queries,
   * this is a repeated field that contains conversation history + latest
   * request.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Content contents = 2;
   */
  contents: Content[];

  /**
   * Optional. The user provided system instructions for the model.
   * Note: only text should be used in parts and content in each part will be in
   * a separate paragraph.
   *
   * @generated from field: optional google.cloud.aiplatform.v1.Content system_instruction = 8;
   */
  systemInstruction?: Content;

  /**
   * Optional. A list of `Tools` the model may use to generate the next
   * response.
   *
   * A `Tool` is a piece of code that enables the system to interact with
   * external systems to perform an action, or set of actions, outside of
   * knowledge and scope of the model.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Tool tools = 6;
   */
  tools: Tool[];

  /**
   * Optional. Tool config. This config is shared for all tools provided in the
   * request.
   *
   * @generated from field: google.cloud.aiplatform.v1.ToolConfig tool_config = 7;
   */
  toolConfig?: ToolConfig;

  /**
   * Optional. The labels with user-defined metadata for the request. It is used
   * for billing and reporting only.
   *
   * Label keys and values can be no longer than 63 characters
   * (Unicode codepoints) and can only contain lowercase letters, numeric
   * characters, underscores, and dashes. International characters are allowed.
   * Label values are optional. Label keys must start with a letter.
   *
   * @generated from field: map<string, string> labels = 10;
   */
  labels: { [key: string]: string };

  /**
   * Optional. Per request settings for blocking unsafe content.
   * Enforced on GenerateContentResponse.candidates.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.SafetySetting safety_settings = 3;
   */
  safetySettings: SafetySetting[];

  /**
   * Optional. Generation config.
   *
   * @generated from field: google.cloud.aiplatform.v1.GenerationConfig generation_config = 4;
   */
  generationConfig?: GenerationConfig;
};

/**
 * Describes the message google.cloud.aiplatform.v1.GenerateContentRequest.
 * Use `create(GenerateContentRequestSchema)` to create a new message.
 */
export const GenerateContentRequestSchema: GenMessage<GenerateContentRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 20);

/**
 * Response message for [PredictionService.GenerateContent].
 *
 * @generated from message google.cloud.aiplatform.v1.GenerateContentResponse
 */
export type GenerateContentResponse = Message<"google.cloud.aiplatform.v1.GenerateContentResponse"> & {
  /**
   * Output only. Generated candidates.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.Candidate candidates = 2;
   */
  candidates: Candidate[];

  /**
   * Output only. The model version used to generate the response.
   *
   * @generated from field: string model_version = 11;
   */
  modelVersion: string;

  /**
   * Output only. Content filter results for a prompt sent in the request.
   * Note: Sent only in the first stream chunk.
   * Only happens when no candidates were generated due to content violations.
   *
   * @generated from field: google.cloud.aiplatform.v1.GenerateContentResponse.PromptFeedback prompt_feedback = 3;
   */
  promptFeedback?: GenerateContentResponse_PromptFeedback;

  /**
   * Usage metadata about the response(s).
   *
   * @generated from field: google.cloud.aiplatform.v1.GenerateContentResponse.UsageMetadata usage_metadata = 4;
   */
  usageMetadata?: GenerateContentResponse_UsageMetadata;
};

/**
 * Describes the message google.cloud.aiplatform.v1.GenerateContentResponse.
 * Use `create(GenerateContentResponseSchema)` to create a new message.
 */
export const GenerateContentResponseSchema: GenMessage<GenerateContentResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 21);

/**
 * Content filter results for a prompt sent in the request.
 *
 * @generated from message google.cloud.aiplatform.v1.GenerateContentResponse.PromptFeedback
 */
export type GenerateContentResponse_PromptFeedback = Message<"google.cloud.aiplatform.v1.GenerateContentResponse.PromptFeedback"> & {
  /**
   * Output only. Blocked reason.
   *
   * @generated from field: google.cloud.aiplatform.v1.GenerateContentResponse.PromptFeedback.BlockedReason block_reason = 1;
   */
  blockReason: GenerateContentResponse_PromptFeedback_BlockedReason;

  /**
   * Output only. Safety ratings.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.SafetyRating safety_ratings = 2;
   */
  safetyRatings: SafetyRating[];

  /**
   * Output only. A readable block reason message.
   *
   * @generated from field: string block_reason_message = 3;
   */
  blockReasonMessage: string;
};

/**
 * Describes the message google.cloud.aiplatform.v1.GenerateContentResponse.PromptFeedback.
 * Use `create(GenerateContentResponse_PromptFeedbackSchema)` to create a new message.
 */
export const GenerateContentResponse_PromptFeedbackSchema: GenMessage<GenerateContentResponse_PromptFeedback> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 21, 0);

/**
 * Blocked reason enumeration.
 *
 * @generated from enum google.cloud.aiplatform.v1.GenerateContentResponse.PromptFeedback.BlockedReason
 */
export enum GenerateContentResponse_PromptFeedback_BlockedReason {
  /**
   * Unspecified blocked reason.
   *
   * @generated from enum value: BLOCKED_REASON_UNSPECIFIED = 0;
   */
  BLOCKED_REASON_UNSPECIFIED = 0,

  /**
   * Candidates blocked due to safety.
   *
   * @generated from enum value: SAFETY = 1;
   */
  SAFETY = 1,

  /**
   * Candidates blocked due to other reason.
   *
   * @generated from enum value: OTHER = 2;
   */
  OTHER = 2,

  /**
   * Candidates blocked due to the terms which are included from the
   * terminology blocklist.
   *
   * @generated from enum value: BLOCKLIST = 3;
   */
  BLOCKLIST = 3,

  /**
   * Candidates blocked due to prohibited content.
   *
   * @generated from enum value: PROHIBITED_CONTENT = 4;
   */
  PROHIBITED_CONTENT = 4,
}

/**
 * Describes the enum google.cloud.aiplatform.v1.GenerateContentResponse.PromptFeedback.BlockedReason.
 */
export const GenerateContentResponse_PromptFeedback_BlockedReasonSchema: GenEnum<GenerateContentResponse_PromptFeedback_BlockedReason> = /*@__PURE__*/
  enumDesc(file_google_cloud_aiplatform_v1_prediction_service, 21, 0, 0);

/**
 * Usage metadata about response(s).
 *
 * @generated from message google.cloud.aiplatform.v1.GenerateContentResponse.UsageMetadata
 */
export type GenerateContentResponse_UsageMetadata = Message<"google.cloud.aiplatform.v1.GenerateContentResponse.UsageMetadata"> & {
  /**
   * Number of tokens in the request. When `cached_content` is set, this is
   * still the total effective prompt size meaning this includes the number of
   * tokens in the cached content.
   *
   * @generated from field: int32 prompt_token_count = 1;
   */
  promptTokenCount: number;

  /**
   * Number of tokens in the response(s).
   *
   * @generated from field: int32 candidates_token_count = 2;
   */
  candidatesTokenCount: number;

  /**
   * Total token count for prompt and response candidates.
   *
   * @generated from field: int32 total_token_count = 3;
   */
  totalTokenCount: number;
};

/**
 * Describes the message google.cloud.aiplatform.v1.GenerateContentResponse.UsageMetadata.
 * Use `create(GenerateContentResponse_UsageMetadataSchema)` to create a new message.
 */
export const GenerateContentResponse_UsageMetadataSchema: GenMessage<GenerateContentResponse_UsageMetadata> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_prediction_service, 21, 1);

/**
 * A service for online predictions and explanations.
 *
 * @generated from service google.cloud.aiplatform.v1.PredictionService
 */
export const PredictionService: GenService<{
  /**
   * Perform an online prediction.
   *
   * @generated from rpc google.cloud.aiplatform.v1.PredictionService.Predict
   */
  predict: {
    methodKind: "unary";
    input: typeof PredictRequestSchema;
    output: typeof PredictResponseSchema;
  },
  /**
   * Perform an online prediction with an arbitrary HTTP payload.
   *
   * The response includes the following HTTP headers:
   *
   * * `X-Vertex-AI-Endpoint-Id`: ID of the
   * [Endpoint][google.cloud.aiplatform.v1.Endpoint] that served this
   * prediction.
   *
   * * `X-Vertex-AI-Deployed-Model-Id`: ID of the Endpoint's
   * [DeployedModel][google.cloud.aiplatform.v1.DeployedModel] that served this
   * prediction.
   *
   * @generated from rpc google.cloud.aiplatform.v1.PredictionService.RawPredict
   */
  rawPredict: {
    methodKind: "unary";
    input: typeof RawPredictRequestSchema;
    output: typeof HttpBodySchema;
  },
  /**
   * Perform a streaming online prediction with an arbitrary HTTP payload.
   *
   * @generated from rpc google.cloud.aiplatform.v1.PredictionService.StreamRawPredict
   */
  streamRawPredict: {
    methodKind: "server_streaming";
    input: typeof StreamRawPredictRequestSchema;
    output: typeof HttpBodySchema;
  },
  /**
   * Perform an unary online prediction request to a gRPC model server for
   * Vertex first-party products and frameworks.
   *
   * @generated from rpc google.cloud.aiplatform.v1.PredictionService.DirectPredict
   */
  directPredict: {
    methodKind: "unary";
    input: typeof DirectPredictRequestSchema;
    output: typeof DirectPredictResponseSchema;
  },
  /**
   * Perform an unary online prediction request to a gRPC model server for
   * custom containers.
   *
   * @generated from rpc google.cloud.aiplatform.v1.PredictionService.DirectRawPredict
   */
  directRawPredict: {
    methodKind: "unary";
    input: typeof DirectRawPredictRequestSchema;
    output: typeof DirectRawPredictResponseSchema;
  },
  /**
   * Perform a streaming online prediction request to a gRPC model server for
   * Vertex first-party products and frameworks.
   *
   * @generated from rpc google.cloud.aiplatform.v1.PredictionService.StreamDirectPredict
   */
  streamDirectPredict: {
    methodKind: "bidi_streaming";
    input: typeof StreamDirectPredictRequestSchema;
    output: typeof StreamDirectPredictResponseSchema;
  },
  /**
   * Perform a streaming online prediction request to a gRPC model server for
   * custom containers.
   *
   * @generated from rpc google.cloud.aiplatform.v1.PredictionService.StreamDirectRawPredict
   */
  streamDirectRawPredict: {
    methodKind: "bidi_streaming";
    input: typeof StreamDirectRawPredictRequestSchema;
    output: typeof StreamDirectRawPredictResponseSchema;
  },
  /**
   * Perform a streaming online prediction request for Vertex first-party
   * products and frameworks.
   *
   * @generated from rpc google.cloud.aiplatform.v1.PredictionService.StreamingPredict
   */
  streamingPredict: {
    methodKind: "bidi_streaming";
    input: typeof StreamingPredictRequestSchema;
    output: typeof StreamingPredictResponseSchema;
  },
  /**
   * Perform a server-side streaming online prediction request for Vertex
   * LLM streaming.
   *
   * @generated from rpc google.cloud.aiplatform.v1.PredictionService.ServerStreamingPredict
   */
  serverStreamingPredict: {
    methodKind: "server_streaming";
    input: typeof StreamingPredictRequestSchema;
    output: typeof StreamingPredictResponseSchema;
  },
  /**
   * Perform a streaming online prediction request through gRPC.
   *
   * @generated from rpc google.cloud.aiplatform.v1.PredictionService.StreamingRawPredict
   */
  streamingRawPredict: {
    methodKind: "bidi_streaming";
    input: typeof StreamingRawPredictRequestSchema;
    output: typeof StreamingRawPredictResponseSchema;
  },
  /**
   * Perform an online explanation.
   *
   * If
   * [deployed_model_id][google.cloud.aiplatform.v1.ExplainRequest.deployed_model_id]
   * is specified, the corresponding DeployModel must have
   * [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
   * populated. If
   * [deployed_model_id][google.cloud.aiplatform.v1.ExplainRequest.deployed_model_id]
   * is not specified, all DeployedModels must have
   * [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
   * populated.
   *
   * @generated from rpc google.cloud.aiplatform.v1.PredictionService.Explain
   */
  explain: {
    methodKind: "unary";
    input: typeof ExplainRequestSchema;
    output: typeof ExplainResponseSchema;
  },
  /**
   * Generate content with multimodal inputs.
   *
   * @generated from rpc google.cloud.aiplatform.v1.PredictionService.GenerateContent
   */
  generateContent: {
    methodKind: "unary";
    input: typeof GenerateContentRequestSchema;
    output: typeof GenerateContentResponseSchema;
  },
  /**
   * Generate content with multimodal inputs with streaming support.
   *
   * @generated from rpc google.cloud.aiplatform.v1.PredictionService.StreamGenerateContent
   */
  streamGenerateContent: {
    methodKind: "server_streaming";
    input: typeof GenerateContentRequestSchema;
    output: typeof GenerateContentResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_aiplatform_v1_prediction_service, 0);

