// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/aiplatform/v1/model_deployment_monitoring_job.proto (package google.cloud.aiplatform.v1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import { file_google_api_resource } from "../../../api/resource_pb";
import type { EncryptionSpec } from "./encryption_spec_pb";
import { file_google_cloud_aiplatform_v1_encryption_spec } from "./encryption_spec_pb";
import type { FeatureStatsAnomaly } from "./feature_monitoring_stats_pb";
import { file_google_cloud_aiplatform_v1_feature_monitoring_stats } from "./feature_monitoring_stats_pb";
import type { GcsDestination } from "./io_pb";
import { file_google_cloud_aiplatform_v1_io } from "./io_pb";
import type { JobState } from "./job_state_pb";
import { file_google_cloud_aiplatform_v1_job_state } from "./job_state_pb";
import type { ModelMonitoringAlertConfig, ModelMonitoringObjectiveConfig, SamplingStrategy, ThresholdConfig } from "./model_monitoring_pb";
import { file_google_cloud_aiplatform_v1_model_monitoring } from "./model_monitoring_pb";
import type { Duration, Timestamp, Value } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_duration, file_google_protobuf_struct, file_google_protobuf_timestamp } from "@bufbuild/protobuf/wkt";
import type { Status } from "../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../rpc/status_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/aiplatform/v1/model_deployment_monitoring_job.proto.
 */
export const file_google_cloud_aiplatform_v1_model_deployment_monitoring_job: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_field_behavior, file_google_api_resource, file_google_cloud_aiplatform_v1_encryption_spec, file_google_cloud_aiplatform_v1_feature_monitoring_stats, file_google_cloud_aiplatform_v1_io, file_google_cloud_aiplatform_v1_job_state, file_google_cloud_aiplatform_v1_model_monitoring, file_google_protobuf_duration, file_google_protobuf_struct, file_google_protobuf_timestamp, file_google_rpc_status]);

/**
 * Represents a job that runs periodically to monitor the deployed models in an
 * endpoint. It will analyze the logged training & prediction data to detect any
 * abnormal behaviors.
 *
 * @generated from message google.cloud.aiplatform.v1.ModelDeploymentMonitoringJob
 */
export type ModelDeploymentMonitoringJob = Message<"google.cloud.aiplatform.v1.ModelDeploymentMonitoringJob"> & {
  /**
   * Output only. Resource name of a ModelDeploymentMonitoringJob.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Required. The user-defined name of the ModelDeploymentMonitoringJob.
   * The name can be up to 128 characters long and can consist of any UTF-8
   * characters.
   * Display name of a ModelDeploymentMonitoringJob.
   *
   * @generated from field: string display_name = 2;
   */
  displayName: string;

  /**
   * Required. Endpoint resource name.
   * Format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
   *
   * @generated from field: string endpoint = 3;
   */
  endpoint: string;

  /**
   * Output only. The detailed state of the monitoring job.
   * When the job is still creating, the state will be 'PENDING'.
   * Once the job is successfully created, the state will be 'RUNNING'.
   * Pause the job, the state will be 'PAUSED'.
   * Resume the job, the state will return to 'RUNNING'.
   *
   * @generated from field: google.cloud.aiplatform.v1.JobState state = 4;
   */
  state: JobState;

  /**
   * Output only. Schedule state when the monitoring job is in Running state.
   *
   * @generated from field: google.cloud.aiplatform.v1.ModelDeploymentMonitoringJob.MonitoringScheduleState schedule_state = 5;
   */
  scheduleState: ModelDeploymentMonitoringJob_MonitoringScheduleState;

  /**
   * Output only. Latest triggered monitoring pipeline metadata.
   *
   * @generated from field: google.cloud.aiplatform.v1.ModelDeploymentMonitoringJob.LatestMonitoringPipelineMetadata latest_monitoring_pipeline_metadata = 25;
   */
  latestMonitoringPipelineMetadata?: ModelDeploymentMonitoringJob_LatestMonitoringPipelineMetadata;

  /**
   * Required. The config for monitoring objectives. This is a per DeployedModel
   * config. Each DeployedModel needs to be configured separately.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.ModelDeploymentMonitoringObjectiveConfig model_deployment_monitoring_objective_configs = 6;
   */
  modelDeploymentMonitoringObjectiveConfigs: ModelDeploymentMonitoringObjectiveConfig[];

  /**
   * Required. Schedule config for running the monitoring job.
   *
   * @generated from field: google.cloud.aiplatform.v1.ModelDeploymentMonitoringScheduleConfig model_deployment_monitoring_schedule_config = 7;
   */
  modelDeploymentMonitoringScheduleConfig?: ModelDeploymentMonitoringScheduleConfig;

  /**
   * Required. Sample Strategy for logging.
   *
   * @generated from field: google.cloud.aiplatform.v1.SamplingStrategy logging_sampling_strategy = 8;
   */
  loggingSamplingStrategy?: SamplingStrategy;

  /**
   * Alert config for model monitoring.
   *
   * @generated from field: google.cloud.aiplatform.v1.ModelMonitoringAlertConfig model_monitoring_alert_config = 15;
   */
  modelMonitoringAlertConfig?: ModelMonitoringAlertConfig;

  /**
   * YAML schema file uri describing the format of a single instance,
   * which are given to format this Endpoint's prediction (and explanation).
   * If not set, we will generate predict schema from collected predict
   * requests.
   *
   * @generated from field: string predict_instance_schema_uri = 9;
   */
  predictInstanceSchemaUri: string;

  /**
   * Sample Predict instance, same format as
   * [PredictRequest.instances][google.cloud.aiplatform.v1.PredictRequest.instances],
   * this can be set as a replacement of
   * [ModelDeploymentMonitoringJob.predict_instance_schema_uri][google.cloud.aiplatform.v1.ModelDeploymentMonitoringJob.predict_instance_schema_uri].
   * If not set, we will generate predict schema from collected predict
   * requests.
   *
   * @generated from field: google.protobuf.Value sample_predict_instance = 19;
   */
  samplePredictInstance?: Value;

  /**
   * YAML schema file uri describing the format of a single instance that you
   * want Tensorflow Data Validation (TFDV) to analyze.
   *
   * If this field is empty, all the feature data types are inferred from
   * [predict_instance_schema_uri][google.cloud.aiplatform.v1.ModelDeploymentMonitoringJob.predict_instance_schema_uri],
   * meaning that TFDV will use the data in the exact format(data type) as
   * prediction request/response.
   * If there are any data type differences between predict instance and TFDV
   * instance, this field can be used to override the schema.
   * For models trained with Vertex AI, this field must be set as all the
   * fields in predict instance formatted as string.
   *
   * @generated from field: string analysis_instance_schema_uri = 16;
   */
  analysisInstanceSchemaUri: string;

  /**
   * Output only. The created bigquery tables for the job under customer
   * project. Customer could do their own query & analysis. There could be 4 log
   * tables in maximum:
   * 1. Training data logging predict request/response
   * 2. Serving data logging predict request/response
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.ModelDeploymentMonitoringBigQueryTable bigquery_tables = 10;
   */
  bigqueryTables: ModelDeploymentMonitoringBigQueryTable[];

  /**
   * The TTL of BigQuery tables in user projects which stores logs.
   * A day is the basic unit of the TTL and we take the ceil of TTL/86400(a
   * day). e.g. { second: 3600} indicates ttl = 1 day.
   *
   * @generated from field: google.protobuf.Duration log_ttl = 17;
   */
  logTtl?: Duration;

  /**
   * The labels with user-defined metadata to organize your
   * ModelDeploymentMonitoringJob.
   *
   * Label keys and values can be no longer than 64 characters
   * (Unicode codepoints), can only contain lowercase letters, numeric
   * characters, underscores and dashes. International characters are allowed.
   *
   * See https://goo.gl/xmQnxf for more information and examples of labels.
   *
   * @generated from field: map<string, string> labels = 11;
   */
  labels: { [key: string]: string };

  /**
   * Output only. Timestamp when this ModelDeploymentMonitoringJob was created.
   *
   * @generated from field: google.protobuf.Timestamp create_time = 12;
   */
  createTime?: Timestamp;

  /**
   * Output only. Timestamp when this ModelDeploymentMonitoringJob was updated
   * most recently.
   *
   * @generated from field: google.protobuf.Timestamp update_time = 13;
   */
  updateTime?: Timestamp;

  /**
   * Output only. Timestamp when this monitoring pipeline will be scheduled to
   * run for the next round.
   *
   * @generated from field: google.protobuf.Timestamp next_schedule_time = 14;
   */
  nextScheduleTime?: Timestamp;

  /**
   * Stats anomalies base folder path.
   *
   * @generated from field: google.cloud.aiplatform.v1.GcsDestination stats_anomalies_base_directory = 20;
   */
  statsAnomaliesBaseDirectory?: GcsDestination;

  /**
   * Customer-managed encryption key spec for a ModelDeploymentMonitoringJob. If
   * set, this ModelDeploymentMonitoringJob and all sub-resources of this
   * ModelDeploymentMonitoringJob will be secured by this key.
   *
   * @generated from field: google.cloud.aiplatform.v1.EncryptionSpec encryption_spec = 21;
   */
  encryptionSpec?: EncryptionSpec;

  /**
   * If true, the scheduled monitoring pipeline logs are sent to
   * Google Cloud Logging, including pipeline status and anomalies detected.
   * Please note the logs incur cost, which are subject to [Cloud Logging
   * pricing](https://cloud.google.com/logging#pricing).
   *
   * @generated from field: bool enable_monitoring_pipeline_logs = 22;
   */
  enableMonitoringPipelineLogs: boolean;

  /**
   * Output only. Only populated when the job's state is `JOB_STATE_FAILED` or
   * `JOB_STATE_CANCELLED`.
   *
   * @generated from field: google.rpc.Status error = 23;
   */
  error?: Status;

  /**
   * Output only. Reserved for future use.
   *
   * @generated from field: bool satisfies_pzs = 26;
   */
  satisfiesPzs: boolean;

  /**
   * Output only. Reserved for future use.
   *
   * @generated from field: bool satisfies_pzi = 27;
   */
  satisfiesPzi: boolean;
};

/**
 * Describes the message google.cloud.aiplatform.v1.ModelDeploymentMonitoringJob.
 * Use `create(ModelDeploymentMonitoringJobSchema)` to create a new message.
 */
export const ModelDeploymentMonitoringJobSchema: GenMessage<ModelDeploymentMonitoringJob> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model_deployment_monitoring_job, 0);

/**
 * All metadata of most recent monitoring pipelines.
 *
 * @generated from message google.cloud.aiplatform.v1.ModelDeploymentMonitoringJob.LatestMonitoringPipelineMetadata
 */
export type ModelDeploymentMonitoringJob_LatestMonitoringPipelineMetadata = Message<"google.cloud.aiplatform.v1.ModelDeploymentMonitoringJob.LatestMonitoringPipelineMetadata"> & {
  /**
   * The time that most recent monitoring pipelines that is related to this
   * run.
   *
   * @generated from field: google.protobuf.Timestamp run_time = 1;
   */
  runTime?: Timestamp;

  /**
   * The status of the most recent monitoring pipeline.
   *
   * @generated from field: google.rpc.Status status = 2;
   */
  status?: Status;
};

/**
 * Describes the message google.cloud.aiplatform.v1.ModelDeploymentMonitoringJob.LatestMonitoringPipelineMetadata.
 * Use `create(ModelDeploymentMonitoringJob_LatestMonitoringPipelineMetadataSchema)` to create a new message.
 */
export const ModelDeploymentMonitoringJob_LatestMonitoringPipelineMetadataSchema: GenMessage<ModelDeploymentMonitoringJob_LatestMonitoringPipelineMetadata> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model_deployment_monitoring_job, 0, 0);

/**
 * The state to Specify the monitoring pipeline.
 *
 * @generated from enum google.cloud.aiplatform.v1.ModelDeploymentMonitoringJob.MonitoringScheduleState
 */
export enum ModelDeploymentMonitoringJob_MonitoringScheduleState {
  /**
   * Unspecified state.
   *
   * @generated from enum value: MONITORING_SCHEDULE_STATE_UNSPECIFIED = 0;
   */
  MONITORING_SCHEDULE_STATE_UNSPECIFIED = 0,

  /**
   * The pipeline is picked up and wait to run.
   *
   * @generated from enum value: PENDING = 1;
   */
  PENDING = 1,

  /**
   * The pipeline is offline and will be scheduled for next run.
   *
   * @generated from enum value: OFFLINE = 2;
   */
  OFFLINE = 2,

  /**
   * The pipeline is running.
   *
   * @generated from enum value: RUNNING = 3;
   */
  RUNNING = 3,
}

/**
 * Describes the enum google.cloud.aiplatform.v1.ModelDeploymentMonitoringJob.MonitoringScheduleState.
 */
export const ModelDeploymentMonitoringJob_MonitoringScheduleStateSchema: GenEnum<ModelDeploymentMonitoringJob_MonitoringScheduleState> = /*@__PURE__*/
  enumDesc(file_google_cloud_aiplatform_v1_model_deployment_monitoring_job, 0, 0);

/**
 * ModelDeploymentMonitoringBigQueryTable specifies the BigQuery table name
 * as well as some information of the logs stored in this table.
 *
 * @generated from message google.cloud.aiplatform.v1.ModelDeploymentMonitoringBigQueryTable
 */
export type ModelDeploymentMonitoringBigQueryTable = Message<"google.cloud.aiplatform.v1.ModelDeploymentMonitoringBigQueryTable"> & {
  /**
   * The source of log.
   *
   * @generated from field: google.cloud.aiplatform.v1.ModelDeploymentMonitoringBigQueryTable.LogSource log_source = 1;
   */
  logSource: ModelDeploymentMonitoringBigQueryTable_LogSource;

  /**
   * The type of log.
   *
   * @generated from field: google.cloud.aiplatform.v1.ModelDeploymentMonitoringBigQueryTable.LogType log_type = 2;
   */
  logType: ModelDeploymentMonitoringBigQueryTable_LogType;

  /**
   * The created BigQuery table to store logs. Customer could do their own query
   * & analysis. Format:
   * `bq://<project_id>.model_deployment_monitoring_<endpoint_id>.<tolower(log_source)>_<tolower(log_type)>`
   *
   * @generated from field: string bigquery_table_path = 3;
   */
  bigqueryTablePath: string;

  /**
   * Output only. The schema version of the request/response logging BigQuery
   * table. Default to v1 if unset.
   *
   * @generated from field: string request_response_logging_schema_version = 4;
   */
  requestResponseLoggingSchemaVersion: string;
};

/**
 * Describes the message google.cloud.aiplatform.v1.ModelDeploymentMonitoringBigQueryTable.
 * Use `create(ModelDeploymentMonitoringBigQueryTableSchema)` to create a new message.
 */
export const ModelDeploymentMonitoringBigQueryTableSchema: GenMessage<ModelDeploymentMonitoringBigQueryTable> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model_deployment_monitoring_job, 1);

/**
 * Indicates where does the log come from.
 *
 * @generated from enum google.cloud.aiplatform.v1.ModelDeploymentMonitoringBigQueryTable.LogSource
 */
export enum ModelDeploymentMonitoringBigQueryTable_LogSource {
  /**
   * Unspecified source.
   *
   * @generated from enum value: LOG_SOURCE_UNSPECIFIED = 0;
   */
  LOG_SOURCE_UNSPECIFIED = 0,

  /**
   * Logs coming from Training dataset.
   *
   * @generated from enum value: TRAINING = 1;
   */
  TRAINING = 1,

  /**
   * Logs coming from Serving traffic.
   *
   * @generated from enum value: SERVING = 2;
   */
  SERVING = 2,
}

/**
 * Describes the enum google.cloud.aiplatform.v1.ModelDeploymentMonitoringBigQueryTable.LogSource.
 */
export const ModelDeploymentMonitoringBigQueryTable_LogSourceSchema: GenEnum<ModelDeploymentMonitoringBigQueryTable_LogSource> = /*@__PURE__*/
  enumDesc(file_google_cloud_aiplatform_v1_model_deployment_monitoring_job, 1, 0);

/**
 * Indicates what type of traffic does the log belong to.
 *
 * @generated from enum google.cloud.aiplatform.v1.ModelDeploymentMonitoringBigQueryTable.LogType
 */
export enum ModelDeploymentMonitoringBigQueryTable_LogType {
  /**
   * Unspecified type.
   *
   * @generated from enum value: LOG_TYPE_UNSPECIFIED = 0;
   */
  LOG_TYPE_UNSPECIFIED = 0,

  /**
   * Predict logs.
   *
   * @generated from enum value: PREDICT = 1;
   */
  PREDICT = 1,

  /**
   * Explain logs.
   *
   * @generated from enum value: EXPLAIN = 2;
   */
  EXPLAIN = 2,
}

/**
 * Describes the enum google.cloud.aiplatform.v1.ModelDeploymentMonitoringBigQueryTable.LogType.
 */
export const ModelDeploymentMonitoringBigQueryTable_LogTypeSchema: GenEnum<ModelDeploymentMonitoringBigQueryTable_LogType> = /*@__PURE__*/
  enumDesc(file_google_cloud_aiplatform_v1_model_deployment_monitoring_job, 1, 1);

/**
 * ModelDeploymentMonitoringObjectiveConfig contains the pair of
 * deployed_model_id to ModelMonitoringObjectiveConfig.
 *
 * @generated from message google.cloud.aiplatform.v1.ModelDeploymentMonitoringObjectiveConfig
 */
export type ModelDeploymentMonitoringObjectiveConfig = Message<"google.cloud.aiplatform.v1.ModelDeploymentMonitoringObjectiveConfig"> & {
  /**
   * The DeployedModel ID of the objective config.
   *
   * @generated from field: string deployed_model_id = 1;
   */
  deployedModelId: string;

  /**
   * The objective config of for the modelmonitoring job of this deployed model.
   *
   * @generated from field: google.cloud.aiplatform.v1.ModelMonitoringObjectiveConfig objective_config = 2;
   */
  objectiveConfig?: ModelMonitoringObjectiveConfig;
};

/**
 * Describes the message google.cloud.aiplatform.v1.ModelDeploymentMonitoringObjectiveConfig.
 * Use `create(ModelDeploymentMonitoringObjectiveConfigSchema)` to create a new message.
 */
export const ModelDeploymentMonitoringObjectiveConfigSchema: GenMessage<ModelDeploymentMonitoringObjectiveConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model_deployment_monitoring_job, 2);

/**
 * The config for scheduling monitoring job.
 *
 * @generated from message google.cloud.aiplatform.v1.ModelDeploymentMonitoringScheduleConfig
 */
export type ModelDeploymentMonitoringScheduleConfig = Message<"google.cloud.aiplatform.v1.ModelDeploymentMonitoringScheduleConfig"> & {
  /**
   * Required. The model monitoring job scheduling interval. It will be rounded
   * up to next full hour. This defines how often the monitoring jobs are
   * triggered.
   *
   * @generated from field: google.protobuf.Duration monitor_interval = 1;
   */
  monitorInterval?: Duration;

  /**
   * The time window of the prediction data being included in each prediction
   * dataset. This window specifies how long the data should be collected from
   * historical model results for each run. If not set,
   * [ModelDeploymentMonitoringScheduleConfig.monitor_interval][google.cloud.aiplatform.v1.ModelDeploymentMonitoringScheduleConfig.monitor_interval]
   * will be used. e.g. If currently the cutoff time is 2022-01-08 14:30:00 and
   * the monitor_window is set to be 3600, then data from 2022-01-08 13:30:00 to
   * 2022-01-08 14:30:00 will be retrieved and aggregated to calculate the
   * monitoring statistics.
   *
   * @generated from field: google.protobuf.Duration monitor_window = 2;
   */
  monitorWindow?: Duration;
};

/**
 * Describes the message google.cloud.aiplatform.v1.ModelDeploymentMonitoringScheduleConfig.
 * Use `create(ModelDeploymentMonitoringScheduleConfigSchema)` to create a new message.
 */
export const ModelDeploymentMonitoringScheduleConfigSchema: GenMessage<ModelDeploymentMonitoringScheduleConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model_deployment_monitoring_job, 3);

/**
 * Statistics and anomalies generated by Model Monitoring.
 *
 * @generated from message google.cloud.aiplatform.v1.ModelMonitoringStatsAnomalies
 */
export type ModelMonitoringStatsAnomalies = Message<"google.cloud.aiplatform.v1.ModelMonitoringStatsAnomalies"> & {
  /**
   * Model Monitoring Objective those stats and anomalies belonging to.
   *
   * @generated from field: google.cloud.aiplatform.v1.ModelDeploymentMonitoringObjectiveType objective = 1;
   */
  objective: ModelDeploymentMonitoringObjectiveType;

  /**
   * Deployed Model ID.
   *
   * @generated from field: string deployed_model_id = 2;
   */
  deployedModelId: string;

  /**
   * Number of anomalies within all stats.
   *
   * @generated from field: int32 anomaly_count = 3;
   */
  anomalyCount: number;

  /**
   * A list of historical Stats and Anomalies generated for all Features.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.ModelMonitoringStatsAnomalies.FeatureHistoricStatsAnomalies feature_stats = 4;
   */
  featureStats: ModelMonitoringStatsAnomalies_FeatureHistoricStatsAnomalies[];
};

/**
 * Describes the message google.cloud.aiplatform.v1.ModelMonitoringStatsAnomalies.
 * Use `create(ModelMonitoringStatsAnomaliesSchema)` to create a new message.
 */
export const ModelMonitoringStatsAnomaliesSchema: GenMessage<ModelMonitoringStatsAnomalies> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model_deployment_monitoring_job, 4);

/**
 * Historical Stats (and Anomalies) for a specific Feature.
 *
 * @generated from message google.cloud.aiplatform.v1.ModelMonitoringStatsAnomalies.FeatureHistoricStatsAnomalies
 */
export type ModelMonitoringStatsAnomalies_FeatureHistoricStatsAnomalies = Message<"google.cloud.aiplatform.v1.ModelMonitoringStatsAnomalies.FeatureHistoricStatsAnomalies"> & {
  /**
   * Display Name of the Feature.
   *
   * @generated from field: string feature_display_name = 1;
   */
  featureDisplayName: string;

  /**
   * Threshold for anomaly detection.
   *
   * @generated from field: google.cloud.aiplatform.v1.ThresholdConfig threshold = 3;
   */
  threshold?: ThresholdConfig;

  /**
   * Stats calculated for the Training Dataset.
   *
   * @generated from field: google.cloud.aiplatform.v1.FeatureStatsAnomaly training_stats = 4;
   */
  trainingStats?: FeatureStatsAnomaly;

  /**
   * A list of historical stats generated by different time window's
   * Prediction Dataset.
   *
   * @generated from field: repeated google.cloud.aiplatform.v1.FeatureStatsAnomaly prediction_stats = 5;
   */
  predictionStats: FeatureStatsAnomaly[];
};

/**
 * Describes the message google.cloud.aiplatform.v1.ModelMonitoringStatsAnomalies.FeatureHistoricStatsAnomalies.
 * Use `create(ModelMonitoringStatsAnomalies_FeatureHistoricStatsAnomaliesSchema)` to create a new message.
 */
export const ModelMonitoringStatsAnomalies_FeatureHistoricStatsAnomaliesSchema: GenMessage<ModelMonitoringStatsAnomalies_FeatureHistoricStatsAnomalies> = /*@__PURE__*/
  messageDesc(file_google_cloud_aiplatform_v1_model_deployment_monitoring_job, 4, 0);

/**
 * The Model Monitoring Objective types.
 *
 * @generated from enum google.cloud.aiplatform.v1.ModelDeploymentMonitoringObjectiveType
 */
export enum ModelDeploymentMonitoringObjectiveType {
  /**
   * Default value, should not be set.
   *
   * @generated from enum value: MODEL_DEPLOYMENT_MONITORING_OBJECTIVE_TYPE_UNSPECIFIED = 0;
   */
  MODEL_DEPLOYMENT_MONITORING_OBJECTIVE_TYPE_UNSPECIFIED = 0,

  /**
   * Raw feature values' stats to detect skew between Training-Prediction
   * datasets.
   *
   * @generated from enum value: RAW_FEATURE_SKEW = 1;
   */
  RAW_FEATURE_SKEW = 1,

  /**
   * Raw feature values' stats to detect drift between Serving-Prediction
   * datasets.
   *
   * @generated from enum value: RAW_FEATURE_DRIFT = 2;
   */
  RAW_FEATURE_DRIFT = 2,

  /**
   * Feature attribution scores to detect skew between Training-Prediction
   * datasets.
   *
   * @generated from enum value: FEATURE_ATTRIBUTION_SKEW = 3;
   */
  FEATURE_ATTRIBUTION_SKEW = 3,

  /**
   * Feature attribution scores to detect skew between Prediction datasets
   * collected within different time windows.
   *
   * @generated from enum value: FEATURE_ATTRIBUTION_DRIFT = 4;
   */
  FEATURE_ATTRIBUTION_DRIFT = 4,
}

/**
 * Describes the enum google.cloud.aiplatform.v1.ModelDeploymentMonitoringObjectiveType.
 */
export const ModelDeploymentMonitoringObjectiveTypeSchema: GenEnum<ModelDeploymentMonitoringObjectiveType> = /*@__PURE__*/
  enumDesc(file_google_cloud_aiplatform_v1_model_deployment_monitoring_job, 0);

