// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/dialogflow/v2beta1/session.proto (package google.cloud.dialogflow.v2beta1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import { file_google_api_resource } from "../../../api/resource_pb";
import type { SubAgent } from "./agent_pb";
import { file_google_cloud_dialogflow_v2beta1_agent } from "./agent_pb";
import type { InputAudioConfig, OutputAudioConfig, SpeechWordInfo, TelephonyDtmfEvents } from "./audio_config_pb";
import { file_google_cloud_dialogflow_v2beta1_audio_config } from "./audio_config_pb";
import type { Context } from "./context_pb";
import { file_google_cloud_dialogflow_v2beta1_context } from "./context_pb";
import type { Intent, Intent_Message } from "./intent_pb";
import { file_google_cloud_dialogflow_v2beta1_intent } from "./intent_pb";
import type { SessionEntityType } from "./session_entity_type_pb";
import { file_google_cloud_dialogflow_v2beta1_session_entity_type } from "./session_entity_type_pb";
import type { Duration, FieldMask } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_duration, file_google_protobuf_field_mask, file_google_protobuf_struct } from "@bufbuild/protobuf/wkt";
import type { Status } from "../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../rpc/status_pb";
import type { LatLng } from "../../../type/latlng_pb";
import { file_google_type_latlng } from "../../../type/latlng_pb";
import type { JsonObject, Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/dialogflow/v2beta1/session.proto.
 */
export const file_google_cloud_dialogflow_v2beta1_session: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_api_resource, file_google_cloud_dialogflow_v2beta1_agent, file_google_cloud_dialogflow_v2beta1_audio_config, file_google_cloud_dialogflow_v2beta1_context, file_google_cloud_dialogflow_v2beta1_intent, file_google_cloud_dialogflow_v2beta1_session_entity_type, file_google_protobuf_duration, file_google_protobuf_field_mask, file_google_protobuf_struct, file_google_rpc_status, file_google_type_latlng]);

/**
 * The request to detect user's intent.
 *
 * @generated from message google.cloud.dialogflow.v2beta1.DetectIntentRequest
 */
export type DetectIntentRequest = Message<"google.cloud.dialogflow.v2beta1.DetectIntentRequest"> & {
  /**
   * Required. The name of the session this query is sent to. Supported formats:
   * - `projects/<Project ID>/agent/sessions/<Session ID>,
   * - `projects/<Project ID>/locations/<Location ID>/agent/sessions/<Session
   *   ID>`,
   * - `projects/<Project ID>/agent/environments/<Environment ID>/users/<User
   *   ID>/sessions/<Session ID>`,
   * - `projects/<Project ID>/locations/<Location
   *   ID>/agent/environments/<Environment ID>/users/<User ID>/sessions/<Session
   *   ID>`,
   *
   * If `Location ID` is not specified we assume default 'us' location. If
   * `Environment ID` is not specified, we assume default 'draft' environment
   * (`Environment ID` might be referred to as environment name at some places).
   * If `User ID` is not specified, we are using "-". It's up to the API caller
   * to choose an appropriate `Session ID` and `User Id`. They can be a random
   * number or some type of user and session identifiers (preferably hashed).
   * The length of the `Session ID` and `User ID` must not exceed 36 characters.
   * For more information, see the [API interactions
   * guide](https://cloud.google.com/dialogflow/docs/api-overview).
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/es/docs/agents-versions).
   *
   * @generated from field: string session = 1;
   */
  session: string;

  /**
   * The parameters of this query.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.QueryParameters query_params = 2;
   */
  queryParams?: QueryParameters;

  /**
   * Required. The input specification. It can be set to:
   *
   * 1. an audio config which instructs the speech recognizer how to process
   * the speech audio,
   *
   * 2. a conversational query in the form of text, or
   *
   * 3. an event that specifies which intent to trigger.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.QueryInput query_input = 3;
   */
  queryInput?: QueryInput;

  /**
   * Instructs the speech synthesizer how to generate the output
   * audio. If this field is not set and agent-level speech synthesizer is not
   * configured, no output audio is generated.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.OutputAudioConfig output_audio_config = 4;
   */
  outputAudioConfig?: OutputAudioConfig;

  /**
   * Mask for
   * [output_audio_config][google.cloud.dialogflow.v2beta1.DetectIntentRequest.output_audio_config]
   * indicating which settings in this request-level config should override
   * speech synthesizer settings defined at agent-level.
   *
   * If unspecified or empty,
   * [output_audio_config][google.cloud.dialogflow.v2beta1.DetectIntentRequest.output_audio_config]
   * replaces the agent-level config in its entirety.
   *
   * @generated from field: google.protobuf.FieldMask output_audio_config_mask = 7;
   */
  outputAudioConfigMask?: FieldMask;

  /**
   * The natural language speech audio to be processed. This field
   * should be populated iff `query_input` is set to an input audio config.
   * A single request can contain up to 1 minute of speech audio data.
   *
   * @generated from field: bytes input_audio = 5;
   */
  inputAudio: Uint8Array;
};

/**
 * Describes the message google.cloud.dialogflow.v2beta1.DetectIntentRequest.
 * Use `create(DetectIntentRequestSchema)` to create a new message.
 */
export const DetectIntentRequestSchema: GenMessage<DetectIntentRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_v2beta1_session, 0);

/**
 * The message returned from the DetectIntent method.
 *
 * @generated from message google.cloud.dialogflow.v2beta1.DetectIntentResponse
 */
export type DetectIntentResponse = Message<"google.cloud.dialogflow.v2beta1.DetectIntentResponse"> & {
  /**
   * The unique identifier of the response. It can be used to
   * locate a response in the training example set or for reporting issues.
   *
   * @generated from field: string response_id = 1;
   */
  responseId: string;

  /**
   * The selected results of the conversational query or event processing.
   * See `alternative_query_results` for additional potential results.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.QueryResult query_result = 2;
   */
  queryResult?: QueryResult;

  /**
   * If Knowledge Connectors are enabled, there could be more than one result
   * returned for a given query or event, and this field will contain all
   * results except for the top one, which is captured in query_result. The
   * alternative results are ordered by decreasing
   * `QueryResult.intent_detection_confidence`. If Knowledge Connectors are
   * disabled, this field will be empty until multiple responses for regular
   * intents are supported, at which point those additional results will be
   * surfaced here.
   *
   * @generated from field: repeated google.cloud.dialogflow.v2beta1.QueryResult alternative_query_results = 5;
   */
  alternativeQueryResults: QueryResult[];

  /**
   * Specifies the status of the webhook request.
   *
   * @generated from field: google.rpc.Status webhook_status = 3;
   */
  webhookStatus?: Status;

  /**
   * The audio data bytes encoded as specified in the request.
   * Note: The output audio is generated based on the values of default platform
   * text responses found in the `query_result.fulfillment_messages` field. If
   * multiple default text responses exist, they will be concatenated when
   * generating audio. If no default platform text responses exist, the
   * generated audio content will be empty.
   *
   * In some scenarios, multiple output audio fields may be present in the
   * response structure. In these cases, only the top-most-level audio output
   * has content.
   *
   * @generated from field: bytes output_audio = 4;
   */
  outputAudio: Uint8Array;

  /**
   * The config used by the speech synthesizer to generate the output audio.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.OutputAudioConfig output_audio_config = 6;
   */
  outputAudioConfig?: OutputAudioConfig;
};

/**
 * Describes the message google.cloud.dialogflow.v2beta1.DetectIntentResponse.
 * Use `create(DetectIntentResponseSchema)` to create a new message.
 */
export const DetectIntentResponseSchema: GenMessage<DetectIntentResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_v2beta1_session, 1);

/**
 * Represents the parameters of the conversational query.
 *
 * @generated from message google.cloud.dialogflow.v2beta1.QueryParameters
 */
export type QueryParameters = Message<"google.cloud.dialogflow.v2beta1.QueryParameters"> & {
  /**
   * The time zone of this conversational query from the
   * [time zone database](https://www.iana.org/time-zones), e.g.,
   * America/New_York, Europe/Paris. If not provided, the time zone specified in
   * agent settings is used.
   *
   * @generated from field: string time_zone = 1;
   */
  timeZone: string;

  /**
   * The geo location of this conversational query.
   *
   * @generated from field: google.type.LatLng geo_location = 2;
   */
  geoLocation?: LatLng;

  /**
   * The collection of contexts to be activated before this query is
   * executed.
   *
   * @generated from field: repeated google.cloud.dialogflow.v2beta1.Context contexts = 3;
   */
  contexts: Context[];

  /**
   * Specifies whether to delete all contexts in the current session
   * before the new ones are activated.
   *
   * @generated from field: bool reset_contexts = 4;
   */
  resetContexts: boolean;

  /**
   * Additional session entity types to replace or extend developer
   * entity types with. The entity synonyms apply to all languages and persist
   * for the session of this query.
   *
   * @generated from field: repeated google.cloud.dialogflow.v2beta1.SessionEntityType session_entity_types = 5;
   */
  sessionEntityTypes: SessionEntityType[];

  /**
   * This field can be used to pass custom data to your webhook.
   * Arbitrary JSON objects are supported.
   * If supplied, the value is used to populate the
   * `WebhookRequest.original_detect_intent_request.payload`
   * field sent to your webhook.
   *
   * @generated from field: google.protobuf.Struct payload = 6;
   */
  payload?: JsonObject;

  /**
   * KnowledgeBases to get alternative results from. If not set, the
   * KnowledgeBases enabled in the agent (through UI) will be used.
   * Format:  `projects/<Project ID>/knowledgeBases/<Knowledge Base ID>`.
   *
   * @generated from field: repeated string knowledge_base_names = 12;
   */
  knowledgeBaseNames: string[];

  /**
   * Configures the type of sentiment analysis to perform. If not
   * provided, sentiment analysis is not performed.
   * Note: Sentiment Analysis is only currently available for Essentials Edition
   * agents.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.SentimentAnalysisRequestConfig sentiment_analysis_request_config = 10;
   */
  sentimentAnalysisRequestConfig?: SentimentAnalysisRequestConfig;

  /**
   * For mega agent query, directly specify which sub agents to query.
   * If any specified sub agent is not linked to the mega agent, an error will
   * be returned. If empty, Dialogflow will decide which sub agents to query.
   * If specified for a non-mega-agent query, will be silently ignored.
   *
   * @generated from field: repeated google.cloud.dialogflow.v2beta1.SubAgent sub_agents = 13;
   */
  subAgents: SubAgent[];

  /**
   * This field can be used to pass HTTP headers for a webhook
   * call. These headers will be sent to webhook along with the headers that
   * have been configured through Dialogflow web console. The headers defined
   * within this field will overwrite the headers configured through Dialogflow
   * console if there is a conflict. Header names are case-insensitive.
   * Google's specified headers are not allowed. Including: "Host",
   * "Content-Length", "Connection", "From", "User-Agent", "Accept-Encoding",
   * "If-Modified-Since", "If-None-Match", "X-Forwarded-For", etc.
   *
   * @generated from field: map<string, string> webhook_headers = 14;
   */
  webhookHeaders: { [key: string]: string };

  /**
   * The platform of the virtual agent response messages.
   *
   * If not empty, only emits messages from this platform in the response.
   * Valid values are the enum names of
   * [platform][google.cloud.dialogflow.v2beta1.Intent.Message.platform].
   *
   * @generated from field: string platform = 18;
   */
  platform: string;
};

/**
 * Describes the message google.cloud.dialogflow.v2beta1.QueryParameters.
 * Use `create(QueryParametersSchema)` to create a new message.
 */
export const QueryParametersSchema: GenMessage<QueryParameters> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_v2beta1_session, 2);

/**
 * Represents the query input. It can contain either:
 *
 * 1. An audio config which instructs the speech recognizer how to process the
 * speech audio.
 *
 * 2. A conversational query in the form of text.
 *
 * 3. An event that specifies which intent to trigger.
 *
 * @generated from message google.cloud.dialogflow.v2beta1.QueryInput
 */
export type QueryInput = Message<"google.cloud.dialogflow.v2beta1.QueryInput"> & {
  /**
   * Required. The input specification.
   *
   * @generated from oneof google.cloud.dialogflow.v2beta1.QueryInput.input
   */
  input: {
    /**
     * Instructs the speech recognizer how to process the speech audio.
     *
     * @generated from field: google.cloud.dialogflow.v2beta1.InputAudioConfig audio_config = 1;
     */
    value: InputAudioConfig;
    case: "audioConfig";
  } | {
    /**
     * The natural language text to be processed.
     *
     * @generated from field: google.cloud.dialogflow.v2beta1.TextInput text = 2;
     */
    value: TextInput;
    case: "text";
  } | {
    /**
     * The event to be processed.
     *
     * @generated from field: google.cloud.dialogflow.v2beta1.EventInput event = 3;
     */
    value: EventInput;
    case: "event";
  } | {
    /**
     * The DTMF digits used to invoke intent and fill in parameter value.
     *
     * @generated from field: google.cloud.dialogflow.v2beta1.TelephonyDtmfEvents dtmf = 4;
     */
    value: TelephonyDtmfEvents;
    case: "dtmf";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.dialogflow.v2beta1.QueryInput.
 * Use `create(QueryInputSchema)` to create a new message.
 */
export const QueryInputSchema: GenMessage<QueryInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_v2beta1_session, 3);

/**
 * Represents the result of conversational query or event processing.
 *
 * @generated from message google.cloud.dialogflow.v2beta1.QueryResult
 */
export type QueryResult = Message<"google.cloud.dialogflow.v2beta1.QueryResult"> & {
  /**
   * The original conversational query text:
   *
   * - If natural language text was provided as input, `query_text` contains
   *   a copy of the input.
   * - If natural language speech audio was provided as input, `query_text`
   *   contains the speech recognition result. If speech recognizer produced
   *   multiple alternatives, a particular one is picked.
   * - If automatic spell correction is enabled, `query_text` will contain the
   *   corrected user input.
   *
   * @generated from field: string query_text = 1;
   */
  queryText: string;

  /**
   * The language that was triggered during intent detection.
   * See [Language
   * Support](https://cloud.google.com/dialogflow/docs/reference/language)
   * for a list of the currently supported language codes.
   *
   * @generated from field: string language_code = 15;
   */
  languageCode: string;

  /**
   * The Speech recognition confidence between 0.0 and 1.0. A higher number
   * indicates an estimated greater likelihood that the recognized words are
   * correct. The default of 0.0 is a sentinel value indicating that confidence
   * was not set.
   *
   * This field is not guaranteed to be accurate or set. In particular this
   * field isn't set for StreamingDetectIntent since the streaming endpoint has
   * separate confidence estimates per portion of the audio in
   * StreamingRecognitionResult.
   *
   * @generated from field: float speech_recognition_confidence = 2;
   */
  speechRecognitionConfidence: number;

  /**
   * The action name from the matched intent.
   *
   * @generated from field: string action = 3;
   */
  action: string;

  /**
   * The collection of extracted parameters.
   *
   * Depending on your protocol or client library language, this is a
   * map, associative array, symbol table, dictionary, or JSON object
   * composed of a collection of (MapKey, MapValue) pairs:
   *
   * * MapKey type: string
   * * MapKey value: parameter name
   * * MapValue type: If parameter's entity type is a composite entity then use
   * map, otherwise, depending on the parameter value type, it could be one of
   * string, number, boolean, null, list or map.
   * * MapValue value: If parameter's entity type is a composite entity then use
   * map from composite entity property names to property values, otherwise,
   * use parameter value.
   *
   * @generated from field: google.protobuf.Struct parameters = 4;
   */
  parameters?: JsonObject;

  /**
   * This field is set to:
   *
   * - `false` if the matched intent has required parameters and not all of
   *    the required parameter values have been collected.
   * - `true` if all required parameter values have been collected, or if the
   *    matched intent doesn't contain any required parameters.
   *
   * @generated from field: bool all_required_params_present = 5;
   */
  allRequiredParamsPresent: boolean;

  /**
   * Indicates whether the conversational query triggers a cancellation for slot
   * filling. For more information, see the [cancel slot filling
   * documentation](https://cloud.google.com/dialogflow/es/docs/intents-actions-parameters#cancel).
   *
   * @generated from field: bool cancels_slot_filling = 21;
   */
  cancelsSlotFilling: boolean;

  /**
   * The text to be pronounced to the user or shown on the screen.
   * Note: This is a legacy field, `fulfillment_messages` should be preferred.
   *
   * @generated from field: string fulfillment_text = 6;
   */
  fulfillmentText: string;

  /**
   * The collection of rich messages to present to the user.
   *
   * @generated from field: repeated google.cloud.dialogflow.v2beta1.Intent.Message fulfillment_messages = 7;
   */
  fulfillmentMessages: Intent_Message[];

  /**
   * If the query was fulfilled by a webhook call, this field is set to the
   * value of the `source` field returned in the webhook response.
   *
   * @generated from field: string webhook_source = 8;
   */
  webhookSource: string;

  /**
   * If the query was fulfilled by a webhook call, this field is set to the
   * value of the `payload` field returned in the webhook response.
   *
   * @generated from field: google.protobuf.Struct webhook_payload = 9;
   */
  webhookPayload?: JsonObject;

  /**
   * The collection of output contexts. If applicable,
   * `output_contexts.parameters` contains entries with name
   * `<parameter name>.original` containing the original parameter values
   * before the query.
   *
   * @generated from field: repeated google.cloud.dialogflow.v2beta1.Context output_contexts = 10;
   */
  outputContexts: Context[];

  /**
   * The intent that matched the conversational query. Some, not
   * all fields are filled in this message, including but not limited to:
   * `name`, `display_name`, `end_interaction` and `is_fallback`.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.Intent intent = 11;
   */
  intent?: Intent;

  /**
   * The intent detection confidence. Values range from 0.0
   * (completely uncertain) to 1.0 (completely certain).
   * This value is for informational purpose only and is only used to
   * help match the best intent within the classification threshold.
   * This value may change for the same end-user expression at any time due to a
   * model retraining or change in implementation.
   * If there are `multiple knowledge_answers` messages, this value is set to
   * the greatest `knowledgeAnswers.match_confidence` value in the list.
   *
   * @generated from field: float intent_detection_confidence = 12;
   */
  intentDetectionConfidence: number;

  /**
   * Free-form diagnostic information for the associated detect intent request.
   * The fields of this data can change without notice, so you should not write
   * code that depends on its structure.
   * The data may contain:
   *
   * - webhook call latency
   * - webhook errors
   *
   * @generated from field: google.protobuf.Struct diagnostic_info = 14;
   */
  diagnosticInfo?: JsonObject;

  /**
   * The sentiment analysis result, which depends on the
   * `sentiment_analysis_request_config` specified in the request.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.SentimentAnalysisResult sentiment_analysis_result = 17;
   */
  sentimentAnalysisResult?: SentimentAnalysisResult;

  /**
   * The result from Knowledge Connector (if any), ordered by decreasing
   * `KnowledgeAnswers.match_confidence`.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.KnowledgeAnswers knowledge_answers = 18;
   */
  knowledgeAnswers?: KnowledgeAnswers;
};

/**
 * Describes the message google.cloud.dialogflow.v2beta1.QueryResult.
 * Use `create(QueryResultSchema)` to create a new message.
 */
export const QueryResultSchema: GenMessage<QueryResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_v2beta1_session, 4);

/**
 * Represents the result of querying a Knowledge base.
 *
 * @generated from message google.cloud.dialogflow.v2beta1.KnowledgeAnswers
 */
export type KnowledgeAnswers = Message<"google.cloud.dialogflow.v2beta1.KnowledgeAnswers"> & {
  /**
   * A list of answers from Knowledge Connector.
   *
   * @generated from field: repeated google.cloud.dialogflow.v2beta1.KnowledgeAnswers.Answer answers = 1;
   */
  answers: KnowledgeAnswers_Answer[];
};

/**
 * Describes the message google.cloud.dialogflow.v2beta1.KnowledgeAnswers.
 * Use `create(KnowledgeAnswersSchema)` to create a new message.
 */
export const KnowledgeAnswersSchema: GenMessage<KnowledgeAnswers> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_v2beta1_session, 5);

/**
 * An answer from Knowledge Connector.
 *
 * @generated from message google.cloud.dialogflow.v2beta1.KnowledgeAnswers.Answer
 */
export type KnowledgeAnswers_Answer = Message<"google.cloud.dialogflow.v2beta1.KnowledgeAnswers.Answer"> & {
  /**
   * Indicates which Knowledge Document this answer was extracted from.
   * Format: `projects/<Project ID>/knowledgeBases/<Knowledge Base
   * ID>/documents/<Document ID>`.
   *
   * @generated from field: string source = 1;
   */
  source: string;

  /**
   * The corresponding FAQ question if the answer was extracted from a FAQ
   * Document, empty otherwise.
   *
   * @generated from field: string faq_question = 2;
   */
  faqQuestion: string;

  /**
   * The piece of text from the `source` knowledge base document that answers
   * this conversational query.
   *
   * @generated from field: string answer = 3;
   */
  answer: string;

  /**
   * The system's confidence level that this knowledge answer is a good match
   * for this conversational query.
   * NOTE: The confidence level for a given `<query, answer>` pair may change
   * without notice, as it depends on models that are constantly being
   * improved. However, it will change less frequently than the confidence
   * score below, and should be preferred for referencing the quality of an
   * answer.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.KnowledgeAnswers.Answer.MatchConfidenceLevel match_confidence_level = 4;
   */
  matchConfidenceLevel: KnowledgeAnswers_Answer_MatchConfidenceLevel;

  /**
   * The system's confidence score that this Knowledge answer is a good match
   * for this conversational query.
   * The range is from 0.0 (completely uncertain) to 1.0 (completely certain).
   * Note: The confidence score is likely to vary somewhat (possibly even for
   * identical requests), as the underlying model is under constant
   * improvement. It may be deprecated in the future. We recommend using
   * `match_confidence_level` which should be generally more stable.
   *
   * @generated from field: float match_confidence = 5;
   */
  matchConfidence: number;
};

/**
 * Describes the message google.cloud.dialogflow.v2beta1.KnowledgeAnswers.Answer.
 * Use `create(KnowledgeAnswers_AnswerSchema)` to create a new message.
 */
export const KnowledgeAnswers_AnswerSchema: GenMessage<KnowledgeAnswers_Answer> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_v2beta1_session, 5, 0);

/**
 * Represents the system's confidence that this knowledge answer is a good
 * match for this conversational query.
 *
 * @generated from enum google.cloud.dialogflow.v2beta1.KnowledgeAnswers.Answer.MatchConfidenceLevel
 */
export enum KnowledgeAnswers_Answer_MatchConfidenceLevel {
  /**
   * Not specified.
   *
   * @generated from enum value: MATCH_CONFIDENCE_LEVEL_UNSPECIFIED = 0;
   */
  MATCH_CONFIDENCE_LEVEL_UNSPECIFIED = 0,

  /**
   * Indicates that the confidence is low.
   *
   * @generated from enum value: LOW = 1;
   */
  LOW = 1,

  /**
   * Indicates our confidence is medium.
   *
   * @generated from enum value: MEDIUM = 2;
   */
  MEDIUM = 2,

  /**
   * Indicates our confidence is high.
   *
   * @generated from enum value: HIGH = 3;
   */
  HIGH = 3,
}

/**
 * Describes the enum google.cloud.dialogflow.v2beta1.KnowledgeAnswers.Answer.MatchConfidenceLevel.
 */
export const KnowledgeAnswers_Answer_MatchConfidenceLevelSchema: GenEnum<KnowledgeAnswers_Answer_MatchConfidenceLevel> = /*@__PURE__*/
  enumDesc(file_google_cloud_dialogflow_v2beta1_session, 5, 0, 0);

/**
 * The top-level message sent by the client to the
 * [Sessions.StreamingDetectIntent][google.cloud.dialogflow.v2beta1.Sessions.StreamingDetectIntent]
 * method.
 *
 * Multiple request messages should be sent in order:
 *
 * 1.  The first message must contain
 * [session][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.session],
 *     [query_input][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.query_input]
 *     plus optionally
 *     [query_params][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.query_params].
 *     If the client wants to receive an audio response, it should also contain
 *     [output_audio_config][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.output_audio_config].
 *     The message must not contain
 *     [input_audio][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.input_audio].
 * 2.  If
 * [query_input][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.query_input]
 * was set to
 *     [query_input.audio_config][google.cloud.dialogflow.v2beta1.InputAudioConfig],
 *     all subsequent messages must contain
 *     [input_audio][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.input_audio]
 *     to continue with Speech recognition. If you decide to rather detect an
 *     intent from text input after you already started Speech recognition,
 *     please send a message with
 *     [query_input.text][google.cloud.dialogflow.v2beta1.QueryInput.text].
 *
 *     However, note that:
 *
 *     * Dialogflow will bill you for the audio duration so far.
 *     * Dialogflow discards all Speech recognition results in favor of the
 *       input text.
 *     * Dialogflow will use the language code from the first message.
 *
 * After you sent all input, you must half-close or abort the request stream.
 *
 * @generated from message google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest
 */
export type StreamingDetectIntentRequest = Message<"google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest"> & {
  /**
   * Required. The name of the session the query is sent to.
   * Supported formats:
   * - `projects/<Project ID>/agent/sessions/<Session ID>,
   * - `projects/<Project ID>/locations/<Location ID>/agent/sessions/<Session
   *   ID>`,
   * - `projects/<Project ID>/agent/environments/<Environment ID>/users/<User
   *   ID>/sessions/<Session ID>`,
   * - `projects/<Project ID>/locations/<Location
   *   ID>/agent/environments/<Environment ID>/users/<User ID>/sessions/<Session
   *   ID>`,
   *
   * If `Location ID` is not specified we assume default 'us' location. If
   * `Environment ID` is not specified, we assume default 'draft' environment.
   * If `User ID` is not specified, we are using "-". It's up to the API caller
   * to choose an appropriate `Session ID` and `User Id`. They can be a random
   * number or some type of user and session identifiers (preferably hashed).
   * The length of the `Session ID` and `User ID` must not exceed 36 characters.
   *
   * For more information, see the [API interactions
   * guide](https://cloud.google.com/dialogflow/docs/api-overview).
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/es/docs/agents-versions).
   *
   * @generated from field: string session = 1;
   */
  session: string;

  /**
   * The parameters of this query.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.QueryParameters query_params = 2;
   */
  queryParams?: QueryParameters;

  /**
   * Required. The input specification. It can be set to:
   *
   * 1. an audio config which instructs the speech recognizer how to process
   * the speech audio,
   *
   * 2. a conversational query in the form of text, or
   *
   * 3. an event that specifies which intent to trigger.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.QueryInput query_input = 3;
   */
  queryInput?: QueryInput;

  /**
   * DEPRECATED. Please use
   * [InputAudioConfig.single_utterance][google.cloud.dialogflow.v2beta1.InputAudioConfig.single_utterance]
   * instead. If `false` (default), recognition does not cease until the client
   * closes the stream. If `true`, the recognizer will detect a single spoken
   * utterance in input audio. Recognition ceases when it detects the audio's
   * voice has stopped or paused. In this case, once a detected intent is
   * received, the client should close the stream and start a new request with a
   * new stream as needed. This setting is ignored when `query_input` is a piece
   * of text or an event.
   *
   * @generated from field: bool single_utterance = 4 [deprecated = true];
   * @deprecated
   */
  singleUtterance: boolean;

  /**
   * Instructs the speech synthesizer how to generate the output
   * audio. If this field is not set and agent-level speech synthesizer is not
   * configured, no output audio is generated.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.OutputAudioConfig output_audio_config = 5;
   */
  outputAudioConfig?: OutputAudioConfig;

  /**
   * Mask for
   * [output_audio_config][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.output_audio_config]
   * indicating which settings in this request-level config should override
   * speech synthesizer settings defined at agent-level.
   *
   * If unspecified or empty,
   * [output_audio_config][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.output_audio_config]
   * replaces the agent-level config in its entirety.
   *
   * @generated from field: google.protobuf.FieldMask output_audio_config_mask = 7;
   */
  outputAudioConfigMask?: FieldMask;

  /**
   * The input audio content to be recognized. Must be sent if
   * `query_input` was set to a streaming input audio config. The complete audio
   * over all streaming messages must not exceed 1 minute.
   *
   * @generated from field: bytes input_audio = 6;
   */
  inputAudio: Uint8Array;

  /**
   * If true, `StreamingDetectIntentResponse.debugging_info` will get populated.
   *
   * @generated from field: bool enable_debugging_info = 8;
   */
  enableDebuggingInfo: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.
 * Use `create(StreamingDetectIntentRequestSchema)` to create a new message.
 */
export const StreamingDetectIntentRequestSchema: GenMessage<StreamingDetectIntentRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_v2beta1_session, 6);

/**
 * Cloud conversation info for easier debugging.
 * It will get populated in `StreamingDetectIntentResponse` or
 * `StreamingAnalyzeContentResponse` when the flag `enable_debugging_info` is
 * set to true in corresponding requests.
 *
 * @generated from message google.cloud.dialogflow.v2beta1.CloudConversationDebuggingInfo
 */
export type CloudConversationDebuggingInfo = Message<"google.cloud.dialogflow.v2beta1.CloudConversationDebuggingInfo"> & {
  /**
   * Number of input audio data chunks in streaming requests.
   *
   * @generated from field: int32 audio_data_chunks = 1;
   */
  audioDataChunks: number;

  /**
   * Time offset of the end of speech utterance relative to the
   * beginning of the first audio chunk.
   *
   * @generated from field: google.protobuf.Duration result_end_time_offset = 2;
   */
  resultEndTimeOffset?: Duration;

  /**
   * Duration of first audio chunk.
   *
   * @generated from field: google.protobuf.Duration first_audio_duration = 3;
   */
  firstAudioDuration?: Duration;

  /**
   * Whether client used single utterance mode.
   *
   * @generated from field: bool single_utterance = 5;
   */
  singleUtterance: boolean;

  /**
   * Time offsets of the speech partial results relative to the beginning of
   * the stream.
   *
   * @generated from field: repeated google.protobuf.Duration speech_partial_results_end_times = 6;
   */
  speechPartialResultsEndTimes: Duration[];

  /**
   * Time offsets of the speech final results (is_final=true) relative to the
   * beginning of the stream.
   *
   * @generated from field: repeated google.protobuf.Duration speech_final_results_end_times = 7;
   */
  speechFinalResultsEndTimes: Duration[];

  /**
   * Total number of partial responses.
   *
   * @generated from field: int32 partial_responses = 8;
   */
  partialResponses: number;

  /**
   * Time offset of Speaker ID stream close time relative to the Speech stream
   * close time in milliseconds. Only meaningful for conversations involving
   * passive verification.
   *
   * @generated from field: int32 speaker_id_passive_latency_ms_offset = 9;
   */
  speakerIdPassiveLatencyMsOffset: number;

  /**
   * Whether a barge-in event is triggered in this request.
   *
   * @generated from field: bool bargein_event_triggered = 10;
   */
  bargeinEventTriggered: boolean;

  /**
   * Whether speech uses single utterance mode.
   *
   * @generated from field: bool speech_single_utterance = 11;
   */
  speechSingleUtterance: boolean;

  /**
   * Time offsets of the DTMF partial results relative to the beginning of
   * the stream.
   *
   * @generated from field: repeated google.protobuf.Duration dtmf_partial_results_times = 12;
   */
  dtmfPartialResultsTimes: Duration[];

  /**
   * Time offsets of the DTMF final results relative to the beginning of
   * the stream.
   *
   * @generated from field: repeated google.protobuf.Duration dtmf_final_results_times = 13;
   */
  dtmfFinalResultsTimes: Duration[];

  /**
   * Time offset of the end-of-single-utterance signal relative to the
   * beginning of the stream.
   *
   * @generated from field: google.protobuf.Duration single_utterance_end_time_offset = 14;
   */
  singleUtteranceEndTimeOffset?: Duration;

  /**
   * No speech timeout settings for the stream.
   *
   * @generated from field: google.protobuf.Duration no_speech_timeout = 15;
   */
  noSpeechTimeout?: Duration;

  /**
   * Speech endpointing timeout settings for the stream.
   *
   * @generated from field: google.protobuf.Duration endpointing_timeout = 19;
   */
  endpointingTimeout?: Duration;

  /**
   * Whether the streaming terminates with an injected text query.
   *
   * @generated from field: bool is_input_text = 16;
   */
  isInputText: boolean;

  /**
   * Client half close time in terms of input audio duration.
   *
   * @generated from field: google.protobuf.Duration client_half_close_time_offset = 17;
   */
  clientHalfCloseTimeOffset?: Duration;

  /**
   * Client half close time in terms of API streaming duration.
   *
   * @generated from field: google.protobuf.Duration client_half_close_streaming_time_offset = 18;
   */
  clientHalfCloseStreamingTimeOffset?: Duration;
};

/**
 * Describes the message google.cloud.dialogflow.v2beta1.CloudConversationDebuggingInfo.
 * Use `create(CloudConversationDebuggingInfoSchema)` to create a new message.
 */
export const CloudConversationDebuggingInfoSchema: GenMessage<CloudConversationDebuggingInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_v2beta1_session, 7);

/**
 * The top-level message returned from the
 * `StreamingDetectIntent` method.
 *
 * Multiple response messages can be returned in order:
 *
 * 1.  If the `StreamingDetectIntentRequest.input_audio` field was
 *     set, the `recognition_result` field is populated for one
 *     or more messages.
 *     See the
 *     [StreamingRecognitionResult][google.cloud.dialogflow.v2beta1.StreamingRecognitionResult]
 *     message for details about the result message sequence.
 *
 * 2.  The next message contains `response_id`, `query_result`,
 *     `alternative_query_results` and optionally `webhook_status` if a WebHook
 *     was called.
 *
 * 3.  If `output_audio_config` was specified in the request or agent-level
 *     speech synthesizer is configured, all subsequent messages contain
 *     `output_audio` and `output_audio_config`.
 *
 * @generated from message google.cloud.dialogflow.v2beta1.StreamingDetectIntentResponse
 */
export type StreamingDetectIntentResponse = Message<"google.cloud.dialogflow.v2beta1.StreamingDetectIntentResponse"> & {
  /**
   * The unique identifier of the response. It can be used to
   * locate a response in the training example set or for reporting issues.
   *
   * @generated from field: string response_id = 1;
   */
  responseId: string;

  /**
   * The result of speech recognition.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.StreamingRecognitionResult recognition_result = 2;
   */
  recognitionResult?: StreamingRecognitionResult;

  /**
   * The selected results of the conversational query or event processing.
   * See `alternative_query_results` for additional potential results.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.QueryResult query_result = 3;
   */
  queryResult?: QueryResult;

  /**
   * If Knowledge Connectors are enabled, there could be more than one result
   * returned for a given query or event, and this field will contain all
   * results except for the top one, which is captured in query_result. The
   * alternative results are ordered by decreasing
   * `QueryResult.intent_detection_confidence`. If Knowledge Connectors are
   * disabled, this field will be empty until multiple responses for regular
   * intents are supported, at which point those additional results will be
   * surfaced here.
   *
   * @generated from field: repeated google.cloud.dialogflow.v2beta1.QueryResult alternative_query_results = 7;
   */
  alternativeQueryResults: QueryResult[];

  /**
   * Specifies the status of the webhook request.
   *
   * @generated from field: google.rpc.Status webhook_status = 4;
   */
  webhookStatus?: Status;

  /**
   * The audio data bytes encoded as specified in the request.
   * Note: The output audio is generated based on the values of default platform
   * text responses found in the `query_result.fulfillment_messages` field. If
   * multiple default text responses exist, they will be concatenated when
   * generating audio. If no default platform text responses exist, the
   * generated audio content will be empty.
   *
   * In some scenarios, multiple output audio fields may be present in the
   * response structure. In these cases, only the top-most-level audio output
   * has content.
   *
   * @generated from field: bytes output_audio = 5;
   */
  outputAudio: Uint8Array;

  /**
   * The config used by the speech synthesizer to generate the output audio.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.OutputAudioConfig output_audio_config = 6;
   */
  outputAudioConfig?: OutputAudioConfig;

  /**
   * Debugging info that would get populated when
   * `StreamingDetectIntentRequest.enable_debugging_info` is set to true.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.CloudConversationDebuggingInfo debugging_info = 8;
   */
  debuggingInfo?: CloudConversationDebuggingInfo;
};

/**
 * Describes the message google.cloud.dialogflow.v2beta1.StreamingDetectIntentResponse.
 * Use `create(StreamingDetectIntentResponseSchema)` to create a new message.
 */
export const StreamingDetectIntentResponseSchema: GenMessage<StreamingDetectIntentResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_v2beta1_session, 8);

/**
 * Contains a speech recognition result corresponding to a portion of the audio
 * that is currently being processed or an indication that this is the end
 * of the single requested utterance.
 *
 * While end-user audio is being processed, Dialogflow sends a series of
 * results. Each result may contain a `transcript` value. A transcript
 * represents a portion of the utterance. While the recognizer is processing
 * audio, transcript values may be interim values or finalized values.
 * Once a transcript is finalized, the `is_final` value is set to true and
 * processing continues for the next transcript.
 *
 * If `StreamingDetectIntentRequest.query_input.audio_config.single_utterance`
 * was true, and the recognizer has completed processing audio,
 * the `message_type` value is set to `END_OF_SINGLE_UTTERANCE and the
 * following (last) result contains the last finalized transcript.
 *
 * The complete end-user utterance is determined by concatenating the
 * finalized transcript values received for the series of results.
 *
 * In the following example, single utterance is enabled. In the case where
 * single utterance is not enabled, result 7 would not occur.
 *
 * ```
 * Num | transcript              | message_type            | is_final
 * --- | ----------------------- | ----------------------- | --------
 * 1   | "tube"                  | TRANSCRIPT              | false
 * 2   | "to be a"               | TRANSCRIPT              | false
 * 3   | "to be"                 | TRANSCRIPT              | false
 * 4   | "to be or not to be"    | TRANSCRIPT              | true
 * 5   | "that's"                | TRANSCRIPT              | false
 * 6   | "that is                | TRANSCRIPT              | false
 * 7   | unset                   | END_OF_SINGLE_UTTERANCE | unset
 * 8   | " that is the question" | TRANSCRIPT              | true
 * ```
 * Concatenating the finalized transcripts with `is_final` set to true,
 * the complete utterance becomes "to be or not to be that is the question".
 *
 * @generated from message google.cloud.dialogflow.v2beta1.StreamingRecognitionResult
 */
export type StreamingRecognitionResult = Message<"google.cloud.dialogflow.v2beta1.StreamingRecognitionResult"> & {
  /**
   * Type of the result message.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.StreamingRecognitionResult.MessageType message_type = 1;
   */
  messageType: StreamingRecognitionResult_MessageType;

  /**
   * Transcript text representing the words that the user spoke.
   * Populated if and only if `message_type` = `TRANSCRIPT`.
   *
   * @generated from field: string transcript = 2;
   */
  transcript: string;

  /**
   * If `false`, the `StreamingRecognitionResult` represents an
   * interim result that may change. If `true`, the recognizer will not return
   * any further hypotheses about this piece of the audio. May only be populated
   * for `message_type` = `TRANSCRIPT`.
   *
   * @generated from field: bool is_final = 3;
   */
  isFinal: boolean;

  /**
   * The Speech confidence between 0.0 and 1.0 for the current portion of audio.
   * A higher number indicates an estimated greater likelihood that the
   * recognized words are correct. The default of 0.0 is a sentinel value
   * indicating that confidence was not set.
   *
   * This field is typically only provided if `is_final` is true and you should
   * not rely on it being accurate or even set.
   *
   * @generated from field: float confidence = 4;
   */
  confidence: number;

  /**
   * An estimate of the likelihood that the speech recognizer will
   * not change its guess about this interim recognition result:
   *
   * * If the value is unspecified or 0.0, Dialogflow didn't compute the
   *   stability. In particular, Dialogflow will only provide stability for
   *   `TRANSCRIPT` results with `is_final = false`.
   * * Otherwise, the value is in (0.0, 1.0] where 0.0 means completely
   *   unstable and 1.0 means completely stable.
   *
   * @generated from field: float stability = 6;
   */
  stability: number;

  /**
   * Word-specific information for the words recognized by Speech in
   * [transcript][google.cloud.dialogflow.v2beta1.StreamingRecognitionResult.transcript].
   * Populated if and only if `message_type` = `TRANSCRIPT` and
   * [InputAudioConfig.enable_word_info] is set.
   *
   * @generated from field: repeated google.cloud.dialogflow.v2beta1.SpeechWordInfo speech_word_info = 7;
   */
  speechWordInfo: SpeechWordInfo[];

  /**
   * Time offset of the end of this Speech recognition result relative to the
   * beginning of the audio. Only populated for `message_type` = `TRANSCRIPT`.
   *
   * @generated from field: google.protobuf.Duration speech_end_offset = 8;
   */
  speechEndOffset?: Duration;

  /**
   * Detected language code for the transcript.
   *
   * @generated from field: string language_code = 10;
   */
  languageCode: string;

  /**
   * DTMF digits. Populated if and only if `message_type` = `DTMF_DIGITS`.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.TelephonyDtmfEvents dtmf_digits = 5;
   */
  dtmfDigits?: TelephonyDtmfEvents;
};

/**
 * Describes the message google.cloud.dialogflow.v2beta1.StreamingRecognitionResult.
 * Use `create(StreamingRecognitionResultSchema)` to create a new message.
 */
export const StreamingRecognitionResultSchema: GenMessage<StreamingRecognitionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_v2beta1_session, 9);

/**
 * Type of the response message.
 *
 * @generated from enum google.cloud.dialogflow.v2beta1.StreamingRecognitionResult.MessageType
 */
export enum StreamingRecognitionResult_MessageType {
  /**
   * Not specified. Should never be used.
   *
   * @generated from enum value: MESSAGE_TYPE_UNSPECIFIED = 0;
   */
  MESSAGE_TYPE_UNSPECIFIED = 0,

  /**
   * Message contains a (possibly partial) transcript.
   *
   * @generated from enum value: TRANSCRIPT = 1;
   */
  TRANSCRIPT = 1,

  /**
   * Message contains DTMF digits.
   *
   * @generated from enum value: DTMF_DIGITS = 3;
   */
  DTMF_DIGITS = 3,

  /**
   * This event indicates that the server has detected the end of the user's
   * speech utterance and expects no additional speech. Therefore, the server
   * will not process additional audio (although it may subsequently return
   * additional results). The client should stop sending additional audio
   * data, half-close the gRPC connection, and wait for any additional results
   * until the server closes the gRPC connection. This message is only sent if
   * `single_utterance` was set to `true`, and is not used otherwise.
   *
   * @generated from enum value: END_OF_SINGLE_UTTERANCE = 2;
   */
  END_OF_SINGLE_UTTERANCE = 2,

  /**
   * Message contains DTMF digits. Before a message with DTMF_DIGITS is sent,
   * a message with PARTIAL_DTMF_DIGITS may be sent with DTMF digits collected
   * up to the time of sending, which represents an intermediate result.
   *
   * @generated from enum value: PARTIAL_DTMF_DIGITS = 4;
   */
  PARTIAL_DTMF_DIGITS = 4,
}

/**
 * Describes the enum google.cloud.dialogflow.v2beta1.StreamingRecognitionResult.MessageType.
 */
export const StreamingRecognitionResult_MessageTypeSchema: GenEnum<StreamingRecognitionResult_MessageType> = /*@__PURE__*/
  enumDesc(file_google_cloud_dialogflow_v2beta1_session, 9, 0);

/**
 * Represents the natural language text to be processed.
 *
 * @generated from message google.cloud.dialogflow.v2beta1.TextInput
 */
export type TextInput = Message<"google.cloud.dialogflow.v2beta1.TextInput"> & {
  /**
   * Required. The UTF-8 encoded natural language text to be processed.
   * Text length must not exceed 256 characters for virtual agent interactions.
   *
   * @generated from field: string text = 1;
   */
  text: string;

  /**
   * Required. The language of this conversational query. See [Language
   * Support](https://cloud.google.com/dialogflow/docs/reference/language)
   * for a list of the currently supported language codes. Note that queries in
   * the same session do not necessarily need to specify the same language.
   *
   * @generated from field: string language_code = 2;
   */
  languageCode: string;
};

/**
 * Describes the message google.cloud.dialogflow.v2beta1.TextInput.
 * Use `create(TextInputSchema)` to create a new message.
 */
export const TextInputSchema: GenMessage<TextInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_v2beta1_session, 10);

/**
 * Events allow for matching intents by event name instead of the natural
 * language input. For instance, input `<event: { name: "welcome_event",
 * parameters: { name: "Sam" } }>` can trigger a personalized welcome response.
 * The parameter `name` may be used by the agent in the response:
 * `"Hello #welcome_event.name! What can I do for you today?"`.
 *
 * @generated from message google.cloud.dialogflow.v2beta1.EventInput
 */
export type EventInput = Message<"google.cloud.dialogflow.v2beta1.EventInput"> & {
  /**
   * Required. The unique identifier of the event.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * The collection of parameters associated with the event.
   *
   * Depending on your protocol or client library language, this is a
   * map, associative array, symbol table, dictionary, or JSON object
   * composed of a collection of (MapKey, MapValue) pairs:
   *
   * * MapKey type: string
   * * MapKey value: parameter name
   * * MapValue type: If parameter's entity type is a composite entity then use
   * map, otherwise, depending on the parameter value type, it could be one of
   * string, number, boolean, null, list or map.
   * * MapValue value: If parameter's entity type is a composite entity then use
   * map from composite entity property names to property values, otherwise,
   * use parameter value.
   *
   * @generated from field: google.protobuf.Struct parameters = 2;
   */
  parameters?: JsonObject;

  /**
   * Required. The language of this query. See [Language
   * Support](https://cloud.google.com/dialogflow/docs/reference/language)
   * for a list of the currently supported language codes. Note that queries in
   * the same session do not necessarily need to specify the same language.
   *
   * This field is ignored when used in the context of a
   * [WebhookResponse.followup_event_input][google.cloud.dialogflow.v2beta1.WebhookResponse.followup_event_input]
   * field, because the language was already defined in the originating detect
   * intent request.
   *
   * @generated from field: string language_code = 3;
   */
  languageCode: string;
};

/**
 * Describes the message google.cloud.dialogflow.v2beta1.EventInput.
 * Use `create(EventInputSchema)` to create a new message.
 */
export const EventInputSchema: GenMessage<EventInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_v2beta1_session, 11);

/**
 * Configures the types of sentiment analysis to perform.
 *
 * @generated from message google.cloud.dialogflow.v2beta1.SentimentAnalysisRequestConfig
 */
export type SentimentAnalysisRequestConfig = Message<"google.cloud.dialogflow.v2beta1.SentimentAnalysisRequestConfig"> & {
  /**
   * Instructs the service to perform sentiment analysis on
   * `query_text`. If not provided, sentiment analysis is not performed on
   * `query_text`.
   *
   * @generated from field: bool analyze_query_text_sentiment = 1;
   */
  analyzeQueryTextSentiment: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.v2beta1.SentimentAnalysisRequestConfig.
 * Use `create(SentimentAnalysisRequestConfigSchema)` to create a new message.
 */
export const SentimentAnalysisRequestConfigSchema: GenMessage<SentimentAnalysisRequestConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_v2beta1_session, 12);

/**
 * The result of sentiment analysis. Sentiment analysis inspects user input
 * and identifies the prevailing subjective opinion, especially to determine a
 * user's attitude as positive, negative, or neutral.
 * For [Participants.DetectIntent][], it needs to be configured in
 * [DetectIntentRequest.query_params][google.cloud.dialogflow.v2beta1.DetectIntentRequest.query_params].
 * For [Participants.StreamingDetectIntent][], it needs to be configured in
 * [StreamingDetectIntentRequest.query_params][google.cloud.dialogflow.v2beta1.StreamingDetectIntentRequest.query_params].
 * And for
 * [Participants.AnalyzeContent][google.cloud.dialogflow.v2beta1.Participants.AnalyzeContent]
 * and
 * [Participants.StreamingAnalyzeContent][google.cloud.dialogflow.v2beta1.Participants.StreamingAnalyzeContent],
 * it needs to be configured in
 * [ConversationProfile.human_agent_assistant_config][google.cloud.dialogflow.v2beta1.ConversationProfile.human_agent_assistant_config]
 *
 * @generated from message google.cloud.dialogflow.v2beta1.SentimentAnalysisResult
 */
export type SentimentAnalysisResult = Message<"google.cloud.dialogflow.v2beta1.SentimentAnalysisResult"> & {
  /**
   * The sentiment analysis result for `query_text`.
   *
   * @generated from field: google.cloud.dialogflow.v2beta1.Sentiment query_text_sentiment = 1;
   */
  queryTextSentiment?: Sentiment;
};

/**
 * Describes the message google.cloud.dialogflow.v2beta1.SentimentAnalysisResult.
 * Use `create(SentimentAnalysisResultSchema)` to create a new message.
 */
export const SentimentAnalysisResultSchema: GenMessage<SentimentAnalysisResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_v2beta1_session, 13);

/**
 * The sentiment, such as positive/negative feeling or association, for a unit
 * of analysis, such as the query text. See:
 * https://cloud.google.com/natural-language/docs/basics#interpreting_sentiment_analysis_values
 * for how to interpret the result.
 *
 * @generated from message google.cloud.dialogflow.v2beta1.Sentiment
 */
export type Sentiment = Message<"google.cloud.dialogflow.v2beta1.Sentiment"> & {
  /**
   * Sentiment score between -1.0 (negative sentiment) and 1.0 (positive
   * sentiment).
   *
   * @generated from field: float score = 1;
   */
  score: number;

  /**
   * A non-negative number in the [0, +inf) range, which represents the absolute
   * magnitude of sentiment, regardless of score (positive or negative).
   *
   * @generated from field: float magnitude = 2;
   */
  magnitude: number;
};

/**
 * Describes the message google.cloud.dialogflow.v2beta1.Sentiment.
 * Use `create(SentimentSchema)` to create a new message.
 */
export const SentimentSchema: GenMessage<Sentiment> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_v2beta1_session, 14);

/**
 * A service used for session interactions.
 *
 * For more information, see the [API interactions
 * guide](https://cloud.google.com/dialogflow/docs/api-overview).
 *
 * @generated from service google.cloud.dialogflow.v2beta1.Sessions
 */
export const Sessions: GenService<{
  /**
   * Processes a natural language query and returns structured, actionable data
   * as a result. This method is not idempotent, because it may cause contexts
   * and session entity types to be updated, which in turn might affect
   * results of future queries.
   *
   * If you might use
   * [Agent Assist](https://cloud.google.com/dialogflow/docs/#aa)
   * or other CCAI products now or in the future, consider using
   * [AnalyzeContent][google.cloud.dialogflow.v2beta1.Participants.AnalyzeContent]
   * instead of `DetectIntent`. `AnalyzeContent` has additional
   * functionality for Agent Assist and other CCAI products.
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/es/docs/agents-versions).
   *
   * @generated from rpc google.cloud.dialogflow.v2beta1.Sessions.DetectIntent
   */
  detectIntent: {
    methodKind: "unary";
    input: typeof DetectIntentRequestSchema;
    output: typeof DetectIntentResponseSchema;
  },
  /**
   * Processes a natural language query in audio format in a streaming fashion
   * and returns structured, actionable data as a result. This method is only
   * available via the gRPC API (not REST).
   *
   * If you might use
   * [Agent Assist](https://cloud.google.com/dialogflow/docs/#aa)
   * or other CCAI products now or in the future, consider using
   * [StreamingAnalyzeContent][google.cloud.dialogflow.v2beta1.Participants.StreamingAnalyzeContent]
   * instead of `StreamingDetectIntent`. `StreamingAnalyzeContent` has
   * additional functionality for Agent Assist and other CCAI products.
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/es/docs/agents-versions).
   *
   * @generated from rpc google.cloud.dialogflow.v2beta1.Sessions.StreamingDetectIntent
   */
  streamingDetectIntent: {
    methodKind: "bidi_streaming";
    input: typeof StreamingDetectIntentRequestSchema;
    output: typeof StreamingDetectIntentResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_dialogflow_v2beta1_session, 0);

