// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/dialogflow/cx/v3/session.proto (package google.cloud.dialogflow.cx.v3, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../../api/annotations_pb";
import { file_google_api_client } from "../../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../../api/field_behavior_pb";
import { file_google_api_resource } from "../../../../api/resource_pb";
import type { AdvancedSettings } from "./advanced_settings_pb";
import { file_google_cloud_dialogflow_cx_v3_advanced_settings } from "./advanced_settings_pb";
import type { InputAudioConfig, OutputAudioConfig, SpeechWordInfo } from "./audio_config_pb";
import { file_google_cloud_dialogflow_cx_v3_audio_config } from "./audio_config_pb";
import type { DataStoreConnectionSignals } from "./data_store_connection_pb";
import { file_google_cloud_dialogflow_cx_v3_data_store_connection } from "./data_store_connection_pb";
import type { Flow } from "./flow_pb";
import { file_google_cloud_dialogflow_cx_v3_flow } from "./flow_pb";
import type { Intent } from "./intent_pb";
import { file_google_cloud_dialogflow_cx_v3_intent } from "./intent_pb";
import type { Page } from "./page_pb";
import { file_google_cloud_dialogflow_cx_v3_page } from "./page_pb";
import type { ResponseMessage } from "./response_message_pb";
import { file_google_cloud_dialogflow_cx_v3_response_message } from "./response_message_pb";
import type { SessionEntityType } from "./session_entity_type_pb";
import { file_google_cloud_dialogflow_cx_v3_session_entity_type } from "./session_entity_type_pb";
import type { Duration, FieldMask } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_duration, file_google_protobuf_field_mask, file_google_protobuf_struct } from "@bufbuild/protobuf/wkt";
import type { Status } from "../../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../../rpc/status_pb";
import type { LatLng } from "../../../../type/latlng_pb";
import { file_google_type_latlng } from "../../../../type/latlng_pb";
import type { JsonObject, Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/dialogflow/cx/v3/session.proto.
 */
export const file_google_cloud_dialogflow_cx_v3_session: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_api_resource, file_google_cloud_dialogflow_cx_v3_advanced_settings, file_google_cloud_dialogflow_cx_v3_audio_config, file_google_cloud_dialogflow_cx_v3_data_store_connection, file_google_cloud_dialogflow_cx_v3_flow, file_google_cloud_dialogflow_cx_v3_intent, file_google_cloud_dialogflow_cx_v3_page, file_google_cloud_dialogflow_cx_v3_response_message, file_google_cloud_dialogflow_cx_v3_session_entity_type, file_google_protobuf_duration, file_google_protobuf_field_mask, file_google_protobuf_struct, file_google_rpc_status, file_google_type_latlng]);

/**
 * Stores information about feedback provided by users about a response.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.AnswerFeedback
 */
export type AnswerFeedback = Message<"google.cloud.dialogflow.cx.v3.AnswerFeedback"> & {
  /**
   * Optional. Rating from user for the specific Dialogflow response.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.AnswerFeedback.Rating rating = 1;
   */
  rating: AnswerFeedback_Rating;

  /**
   * Optional. In case of thumbs down rating provided, users can optionally
   * provide context about the rating.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.AnswerFeedback.RatingReason rating_reason = 2;
   */
  ratingReason?: AnswerFeedback_RatingReason;

  /**
   * Optional. Custom rating from the user about the provided answer, with
   * maximum length of 1024 characters. For example, client could use a
   * customized JSON object to indicate the rating.
   *
   * @generated from field: string custom_rating = 3;
   */
  customRating: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.AnswerFeedback.
 * Use `create(AnswerFeedbackSchema)` to create a new message.
 */
export const AnswerFeedbackSchema: GenMessage<AnswerFeedback> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 0);

/**
 * Stores extra information about why users provided thumbs down rating.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.AnswerFeedback.RatingReason
 */
export type AnswerFeedback_RatingReason = Message<"google.cloud.dialogflow.cx.v3.AnswerFeedback.RatingReason"> & {
  /**
   * Optional. Custom reason labels for thumbs down rating provided by the
   * user. The maximum number of labels allowed is 10 and the maximum length
   * of a single label is 128 characters.
   *
   * @generated from field: repeated string reason_labels = 3;
   */
  reasonLabels: string[];

  /**
   * Optional. Additional feedback about the rating.
   * This field can be populated without choosing a predefined `reason`.
   *
   * @generated from field: string feedback = 2;
   */
  feedback: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.AnswerFeedback.RatingReason.
 * Use `create(AnswerFeedback_RatingReasonSchema)` to create a new message.
 */
export const AnswerFeedback_RatingReasonSchema: GenMessage<AnswerFeedback_RatingReason> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 0, 0);

/**
 * Represents thumbs up/down rating provided by user about a response.
 *
 * @generated from enum google.cloud.dialogflow.cx.v3.AnswerFeedback.Rating
 */
export enum AnswerFeedback_Rating {
  /**
   * Rating not specified.
   *
   * @generated from enum value: RATING_UNSPECIFIED = 0;
   */
  RATING_UNSPECIFIED = 0,

  /**
   * Thumbs up feedback from user.
   *
   * @generated from enum value: THUMBS_UP = 1;
   */
  THUMBS_UP = 1,

  /**
   * Thumbs down feedback from user.
   *
   * @generated from enum value: THUMBS_DOWN = 2;
   */
  THUMBS_DOWN = 2,
}

/**
 * Describes the enum google.cloud.dialogflow.cx.v3.AnswerFeedback.Rating.
 */
export const AnswerFeedback_RatingSchema: GenEnum<AnswerFeedback_Rating> = /*@__PURE__*/
  enumDesc(file_google_cloud_dialogflow_cx_v3_session, 0, 0);

/**
 * The request to set the feedback for a bot answer.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.SubmitAnswerFeedbackRequest
 */
export type SubmitAnswerFeedbackRequest = Message<"google.cloud.dialogflow.cx.v3.SubmitAnswerFeedbackRequest"> & {
  /**
   * Required. The name of the session the feedback was sent to.
   *
   * @generated from field: string session = 1;
   */
  session: string;

  /**
   * Required. ID of the response to update its feedback. This is the same as
   * DetectIntentResponse.response_id.
   *
   * @generated from field: string response_id = 2;
   */
  responseId: string;

  /**
   * Required. Feedback provided for a bot answer.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.AnswerFeedback answer_feedback = 3;
   */
  answerFeedback?: AnswerFeedback;

  /**
   * Optional. The mask to control which fields to update. If the mask is not
   * present, all fields will be updated.
   *
   * @generated from field: google.protobuf.FieldMask update_mask = 4;
   */
  updateMask?: FieldMask;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.SubmitAnswerFeedbackRequest.
 * Use `create(SubmitAnswerFeedbackRequestSchema)` to create a new message.
 */
export const SubmitAnswerFeedbackRequestSchema: GenMessage<SubmitAnswerFeedbackRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 1);

/**
 * The request to detect user's intent.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.DetectIntentRequest
 */
export type DetectIntentRequest = Message<"google.cloud.dialogflow.cx.v3.DetectIntentRequest"> & {
  /**
   * Required. The name of the session this query is sent to.
   * Format: `projects/<Project ID>/locations/<Location ID>/agents/<Agent
   * ID>/sessions/<Session ID>` or `projects/<Project ID>/locations/<Location
   * ID>/agents/<Agent ID>/environments/<Environment ID>/sessions/<Session ID>`.
   * If `Environment ID` is not specified, we assume default 'draft'
   * environment.
   * It's up to the API caller to choose an appropriate `Session ID`. It can be
   * a random number or some type of session identifiers (preferably hashed).
   * The length of the `Session ID` must not exceed 36 characters.
   *
   * For more information, see the [sessions
   * guide](https://cloud.google.com/dialogflow/cx/docs/concept/session).
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/cx/docs/concept/version).
   *
   * @generated from field: string session = 1;
   */
  session: string;

  /**
   * The parameters of this query.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.QueryParameters query_params = 2;
   */
  queryParams?: QueryParameters;

  /**
   * Required. The input specification.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.QueryInput query_input = 3;
   */
  queryInput?: QueryInput;

  /**
   * Instructs the speech synthesizer how to generate the output audio.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.OutputAudioConfig output_audio_config = 4;
   */
  outputAudioConfig?: OutputAudioConfig;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.DetectIntentRequest.
 * Use `create(DetectIntentRequestSchema)` to create a new message.
 */
export const DetectIntentRequestSchema: GenMessage<DetectIntentRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 2);

/**
 * The message returned from the DetectIntent method.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.DetectIntentResponse
 */
export type DetectIntentResponse = Message<"google.cloud.dialogflow.cx.v3.DetectIntentResponse"> & {
  /**
   * Output only. The unique identifier of the response. It can be used to
   * locate a response in the training example set or for reporting issues.
   *
   * @generated from field: string response_id = 1;
   */
  responseId: string;

  /**
   * The result of the conversational query.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.QueryResult query_result = 2;
   */
  queryResult?: QueryResult;

  /**
   * The audio data bytes encoded as specified in the request.
   * Note: The output audio is generated based on the values of default platform
   * text responses found in the
   * [`query_result.response_messages`][google.cloud.dialogflow.cx.v3.QueryResult.response_messages]
   * field. If multiple default text responses exist, they will be concatenated
   * when generating audio. If no default platform text responses exist, the
   * generated audio content will be empty.
   *
   * In some scenarios, multiple output audio fields may be present in the
   * response structure. In these cases, only the top-most-level audio output
   * has content.
   *
   * @generated from field: bytes output_audio = 4;
   */
  outputAudio: Uint8Array;

  /**
   * The config used by the speech synthesizer to generate the output audio.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.OutputAudioConfig output_audio_config = 5;
   */
  outputAudioConfig?: OutputAudioConfig;

  /**
   * Response type.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.DetectIntentResponse.ResponseType response_type = 6;
   */
  responseType: DetectIntentResponse_ResponseType;

  /**
   * Indicates whether the partial response can be cancelled when a later
   * response arrives. e.g. if the agent specified some music as partial
   * response, it can be cancelled.
   *
   * @generated from field: bool allow_cancellation = 7;
   */
  allowCancellation: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.DetectIntentResponse.
 * Use `create(DetectIntentResponseSchema)` to create a new message.
 */
export const DetectIntentResponseSchema: GenMessage<DetectIntentResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 3);

/**
 * Represents different DetectIntentResponse types.
 *
 * @generated from enum google.cloud.dialogflow.cx.v3.DetectIntentResponse.ResponseType
 */
export enum DetectIntentResponse_ResponseType {
  /**
   * Not specified. This should never happen.
   *
   * @generated from enum value: RESPONSE_TYPE_UNSPECIFIED = 0;
   */
  RESPONSE_TYPE_UNSPECIFIED = 0,

  /**
   * Partial response. e.g. Aggregated responses in a Fulfillment that enables
   * `return_partial_response` can be returned as partial response.
   * WARNING: partial response is not eligible for barge-in.
   *
   * @generated from enum value: PARTIAL = 1;
   */
  PARTIAL = 1,

  /**
   * Final response.
   *
   * @generated from enum value: FINAL = 2;
   */
  FINAL = 2,
}

/**
 * Describes the enum google.cloud.dialogflow.cx.v3.DetectIntentResponse.ResponseType.
 */
export const DetectIntentResponse_ResponseTypeSchema: GenEnum<DetectIntentResponse_ResponseType> = /*@__PURE__*/
  enumDesc(file_google_cloud_dialogflow_cx_v3_session, 3, 0);

/**
 * The top-level message sent by the client to the
 * [Sessions.StreamingDetectIntent][google.cloud.dialogflow.cx.v3.Sessions.StreamingDetectIntent]
 * method.
 *
 * Multiple request messages should be sent in order:
 *
 * 1.  The first message must contain
 *     [session][google.cloud.dialogflow.cx.v3.StreamingDetectIntentRequest.session],
 *     [query_input][google.cloud.dialogflow.cx.v3.StreamingDetectIntentRequest.query_input]
 *     plus optionally
 *     [query_params][google.cloud.dialogflow.cx.v3.StreamingDetectIntentRequest.query_params].
 *     If the client wants to receive an audio response, it should also contain
 *     [output_audio_config][google.cloud.dialogflow.cx.v3.StreamingDetectIntentRequest.output_audio_config].
 *
 * 2.  If
 * [query_input][google.cloud.dialogflow.cx.v3.StreamingDetectIntentRequest.query_input]
 * was set to
 *     [query_input.audio.config][google.cloud.dialogflow.cx.v3.AudioInput.config],
 *     all subsequent messages must contain
 *     [query_input.audio.audio][google.cloud.dialogflow.cx.v3.AudioInput.audio]
 *     to continue with Speech recognition. If you decide to rather detect an
 *     intent from text input after you already started Speech recognition,
 *     please send a message with
 *     [query_input.text][google.cloud.dialogflow.cx.v3.QueryInput.text].
 *
 *     However, note that:
 *
 *     * Dialogflow will bill you for the audio duration so far.
 *     * Dialogflow discards all Speech recognition results in favor of the
 *       input text.
 *     * Dialogflow will use the language code from the first message.
 *
 * After you sent all input, you must half-close or abort the request stream.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.StreamingDetectIntentRequest
 */
export type StreamingDetectIntentRequest = Message<"google.cloud.dialogflow.cx.v3.StreamingDetectIntentRequest"> & {
  /**
   * The name of the session this query is sent to.
   * Format: `projects/<Project ID>/locations/<Location ID>/agents/<Agent
   * ID>/sessions/<Session ID>` or `projects/<Project ID>/locations/<Location
   * ID>/agents/<Agent ID>/environments/<Environment ID>/sessions/<Session ID>`.
   * If `Environment ID` is not specified, we assume default 'draft'
   * environment.
   * It's up to the API caller to choose an appropriate `Session ID`. It can be
   * a random number or some type of session identifiers (preferably hashed).
   * The length of the `Session ID` must not exceed 36 characters.
   * Note: session must be set in the first request.
   *
   * For more information, see the [sessions
   * guide](https://cloud.google.com/dialogflow/cx/docs/concept/session).
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/cx/docs/concept/version).
   *
   * @generated from field: string session = 1;
   */
  session: string;

  /**
   * The parameters of this query.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.QueryParameters query_params = 2;
   */
  queryParams?: QueryParameters;

  /**
   * Required. The input specification.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.QueryInput query_input = 3;
   */
  queryInput?: QueryInput;

  /**
   * Instructs the speech synthesizer how to generate the output audio.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.OutputAudioConfig output_audio_config = 4;
   */
  outputAudioConfig?: OutputAudioConfig;

  /**
   * Enable partial detect intent response. If this flag is not enabled,
   * response stream still contains only one final `DetectIntentResponse` even
   * if some `Fulfillment`s in the agent have been configured to return partial
   * responses.
   *
   * @generated from field: bool enable_partial_response = 5;
   */
  enablePartialResponse: boolean;

  /**
   * If true, `StreamingDetectIntentResponse.debugging_info` will get populated.
   *
   * @generated from field: bool enable_debugging_info = 8;
   */
  enableDebuggingInfo: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.StreamingDetectIntentRequest.
 * Use `create(StreamingDetectIntentRequestSchema)` to create a new message.
 */
export const StreamingDetectIntentRequestSchema: GenMessage<StreamingDetectIntentRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 4);

/**
 * Cloud conversation info for easier debugging.
 * It will get populated in `StreamingDetectIntentResponse` or
 * `StreamingAnalyzeContentResponse` when the flag `enable_debugging_info` is
 * set to true in corresponding requests.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.CloudConversationDebuggingInfo
 */
export type CloudConversationDebuggingInfo = Message<"google.cloud.dialogflow.cx.v3.CloudConversationDebuggingInfo"> & {
  /**
   * Number of input audio data chunks in streaming requests.
   *
   * @generated from field: int32 audio_data_chunks = 1;
   */
  audioDataChunks: number;

  /**
   * Time offset of the end of speech utterance relative to the
   * beginning of the first audio chunk.
   *
   * @generated from field: google.protobuf.Duration result_end_time_offset = 2;
   */
  resultEndTimeOffset?: Duration;

  /**
   * Duration of first audio chunk.
   *
   * @generated from field: google.protobuf.Duration first_audio_duration = 3;
   */
  firstAudioDuration?: Duration;

  /**
   * Whether client used single utterance mode.
   *
   * @generated from field: bool single_utterance = 5;
   */
  singleUtterance: boolean;

  /**
   * Time offsets of the speech partial results relative to the beginning of
   * the stream.
   *
   * @generated from field: repeated google.protobuf.Duration speech_partial_results_end_times = 6;
   */
  speechPartialResultsEndTimes: Duration[];

  /**
   * Time offsets of the speech final results (is_final=true) relative to the
   * beginning of the stream.
   *
   * @generated from field: repeated google.protobuf.Duration speech_final_results_end_times = 7;
   */
  speechFinalResultsEndTimes: Duration[];

  /**
   * Total number of partial responses.
   *
   * @generated from field: int32 partial_responses = 8;
   */
  partialResponses: number;

  /**
   * Time offset of Speaker ID stream close time relative to the Speech stream
   * close time in milliseconds. Only meaningful for conversations involving
   * passive verification.
   *
   * @generated from field: int32 speaker_id_passive_latency_ms_offset = 9;
   */
  speakerIdPassiveLatencyMsOffset: number;

  /**
   * Whether a barge-in event is triggered in this request.
   *
   * @generated from field: bool bargein_event_triggered = 10;
   */
  bargeinEventTriggered: boolean;

  /**
   * Whether speech uses single utterance mode.
   *
   * @generated from field: bool speech_single_utterance = 11;
   */
  speechSingleUtterance: boolean;

  /**
   * Time offsets of the DTMF partial results relative to the beginning of
   * the stream.
   *
   * @generated from field: repeated google.protobuf.Duration dtmf_partial_results_times = 12;
   */
  dtmfPartialResultsTimes: Duration[];

  /**
   * Time offsets of the DTMF final results relative to the beginning of
   * the stream.
   *
   * @generated from field: repeated google.protobuf.Duration dtmf_final_results_times = 13;
   */
  dtmfFinalResultsTimes: Duration[];

  /**
   * Time offset of the end-of-single-utterance signal relative to the
   * beginning of the stream.
   *
   * @generated from field: google.protobuf.Duration single_utterance_end_time_offset = 14;
   */
  singleUtteranceEndTimeOffset?: Duration;

  /**
   * No speech timeout settings for the stream.
   *
   * @generated from field: google.protobuf.Duration no_speech_timeout = 15;
   */
  noSpeechTimeout?: Duration;

  /**
   * Speech endpointing timeout settings for the stream.
   *
   * @generated from field: google.protobuf.Duration endpointing_timeout = 19;
   */
  endpointingTimeout?: Duration;

  /**
   * Whether the streaming terminates with an injected text query.
   *
   * @generated from field: bool is_input_text = 16;
   */
  isInputText: boolean;

  /**
   * Client half close time in terms of input audio duration.
   *
   * @generated from field: google.protobuf.Duration client_half_close_time_offset = 17;
   */
  clientHalfCloseTimeOffset?: Duration;

  /**
   * Client half close time in terms of API streaming duration.
   *
   * @generated from field: google.protobuf.Duration client_half_close_streaming_time_offset = 18;
   */
  clientHalfCloseStreamingTimeOffset?: Duration;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.CloudConversationDebuggingInfo.
 * Use `create(CloudConversationDebuggingInfoSchema)` to create a new message.
 */
export const CloudConversationDebuggingInfoSchema: GenMessage<CloudConversationDebuggingInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 5);

/**
 * The top-level message returned from the
 * [StreamingDetectIntent][google.cloud.dialogflow.cx.v3.Sessions.StreamingDetectIntent]
 * method.
 *
 * Multiple response messages (N) can be returned in order.
 *
 * The first (N-1) responses set either the `recognition_result` or
 * `detect_intent_response` field, depending on the request:
 *
 * *   If the `StreamingDetectIntentRequest.query_input.audio` field was
 *     set, and the `StreamingDetectIntentRequest.enable_partial_response`
 *     field was false, the `recognition_result` field is populated for each
 *     of the (N-1) responses.
 *     See the
 *     [StreamingRecognitionResult][google.cloud.dialogflow.cx.v3.StreamingRecognitionResult]
 *     message for details about the result message sequence.
 *
 * *   If the `StreamingDetectIntentRequest.enable_partial_response` field was
 *     true, the `detect_intent_response` field is populated for each
 *     of the (N-1) responses, where 1 <= N <= 4.
 *     These responses set the
 *     [DetectIntentResponse.response_type][google.cloud.dialogflow.cx.v3.DetectIntentResponse.response_type]
 *     field to `PARTIAL`.
 *
 * For the final Nth response message, the `detect_intent_response` is fully
 * populated, and
 * [DetectIntentResponse.response_type][google.cloud.dialogflow.cx.v3.DetectIntentResponse.response_type]
 * is set to `FINAL`.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.StreamingDetectIntentResponse
 */
export type StreamingDetectIntentResponse = Message<"google.cloud.dialogflow.cx.v3.StreamingDetectIntentResponse"> & {
  /**
   * The output response.
   *
   * @generated from oneof google.cloud.dialogflow.cx.v3.StreamingDetectIntentResponse.response
   */
  response: {
    /**
     * The result of speech recognition.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.StreamingRecognitionResult recognition_result = 1;
     */
    value: StreamingRecognitionResult;
    case: "recognitionResult";
  } | {
    /**
     * The response from detect intent.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.DetectIntentResponse detect_intent_response = 2;
     */
    value: DetectIntentResponse;
    case: "detectIntentResponse";
  } | { case: undefined; value?: undefined };

  /**
   * Debugging info that would get populated when
   * `StreamingDetectIntentRequest.enable_debugging_info` is set to true.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.CloudConversationDebuggingInfo debugging_info = 4;
   */
  debuggingInfo?: CloudConversationDebuggingInfo;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.StreamingDetectIntentResponse.
 * Use `create(StreamingDetectIntentResponseSchema)` to create a new message.
 */
export const StreamingDetectIntentResponseSchema: GenMessage<StreamingDetectIntentResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 6);

/**
 * Contains a speech recognition result corresponding to a portion of the audio
 * that is currently being processed or an indication that this is the end
 * of the single requested utterance.
 *
 * While end-user audio is being processed, Dialogflow sends a series of
 * results. Each result may contain a `transcript` value. A transcript
 * represents a portion of the utterance. While the recognizer is processing
 * audio, transcript values may be interim values or finalized values.
 * Once a transcript is finalized, the `is_final` value is set to true and
 * processing continues for the next transcript.
 *
 * If `StreamingDetectIntentRequest.query_input.audio.config.single_utterance`
 * was true, and the recognizer has completed processing audio,
 * the `message_type` value is set to `END_OF_SINGLE_UTTERANCE and the
 * following (last) result contains the last finalized transcript.
 *
 * The complete end-user utterance is determined by concatenating the
 * finalized transcript values received for the series of results.
 *
 * In the following example, single utterance is enabled. In the case where
 * single utterance is not enabled, result 7 would not occur.
 *
 * ```
 * Num | transcript              | message_type            | is_final
 * --- | ----------------------- | ----------------------- | --------
 * 1   | "tube"                  | TRANSCRIPT              | false
 * 2   | "to be a"               | TRANSCRIPT              | false
 * 3   | "to be"                 | TRANSCRIPT              | false
 * 4   | "to be or not to be"    | TRANSCRIPT              | true
 * 5   | "that's"                | TRANSCRIPT              | false
 * 6   | "that is                | TRANSCRIPT              | false
 * 7   | unset                   | END_OF_SINGLE_UTTERANCE | unset
 * 8   | " that is the question" | TRANSCRIPT              | true
 * ```
 *
 * Concatenating the finalized transcripts with `is_final` set to true,
 * the complete utterance becomes "to be or not to be that is the question".
 *
 * @generated from message google.cloud.dialogflow.cx.v3.StreamingRecognitionResult
 */
export type StreamingRecognitionResult = Message<"google.cloud.dialogflow.cx.v3.StreamingRecognitionResult"> & {
  /**
   * Type of the result message.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.StreamingRecognitionResult.MessageType message_type = 1;
   */
  messageType: StreamingRecognitionResult_MessageType;

  /**
   * Transcript text representing the words that the user spoke.
   * Populated if and only if `message_type` = `TRANSCRIPT`.
   *
   * @generated from field: string transcript = 2;
   */
  transcript: string;

  /**
   * If `false`, the `StreamingRecognitionResult` represents an
   * interim result that may change. If `true`, the recognizer will not return
   * any further hypotheses about this piece of the audio. May only be populated
   * for `message_type` = `TRANSCRIPT`.
   *
   * @generated from field: bool is_final = 3;
   */
  isFinal: boolean;

  /**
   * The Speech confidence between 0.0 and 1.0 for the current portion of audio.
   * A higher number indicates an estimated greater likelihood that the
   * recognized words are correct. The default of 0.0 is a sentinel value
   * indicating that confidence was not set.
   *
   * This field is typically only provided if `is_final` is true and you should
   * not rely on it being accurate or even set.
   *
   * @generated from field: float confidence = 4;
   */
  confidence: number;

  /**
   * An estimate of the likelihood that the speech recognizer will
   * not change its guess about this interim recognition result:
   * * If the value is unspecified or 0.0, Dialogflow didn't compute the
   *   stability. In particular, Dialogflow will only provide stability for
   *   `TRANSCRIPT` results with `is_final = false`.
   * * Otherwise, the value is in (0.0, 1.0] where 0.0 means completely
   *   unstable and 1.0 means completely stable.
   *
   * @generated from field: float stability = 6;
   */
  stability: number;

  /**
   * Word-specific information for the words recognized by Speech in
   * [transcript][google.cloud.dialogflow.cx.v3.StreamingRecognitionResult.transcript].
   * Populated if and only if `message_type` = `TRANSCRIPT` and
   * [InputAudioConfig.enable_word_info] is set.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3.SpeechWordInfo speech_word_info = 7;
   */
  speechWordInfo: SpeechWordInfo[];

  /**
   * Time offset of the end of this Speech recognition result relative to the
   * beginning of the audio. Only populated for `message_type` =
   * `TRANSCRIPT`.
   *
   * @generated from field: google.protobuf.Duration speech_end_offset = 8;
   */
  speechEndOffset?: Duration;

  /**
   * Detected language code for the transcript.
   *
   * @generated from field: string language_code = 10;
   */
  languageCode: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.StreamingRecognitionResult.
 * Use `create(StreamingRecognitionResultSchema)` to create a new message.
 */
export const StreamingRecognitionResultSchema: GenMessage<StreamingRecognitionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 7);

/**
 * Type of the response message.
 *
 * @generated from enum google.cloud.dialogflow.cx.v3.StreamingRecognitionResult.MessageType
 */
export enum StreamingRecognitionResult_MessageType {
  /**
   * Not specified. Should never be used.
   *
   * @generated from enum value: MESSAGE_TYPE_UNSPECIFIED = 0;
   */
  MESSAGE_TYPE_UNSPECIFIED = 0,

  /**
   * Message contains a (possibly partial) transcript.
   *
   * @generated from enum value: TRANSCRIPT = 1;
   */
  TRANSCRIPT = 1,

  /**
   * This event indicates that the server has detected the end of the user's
   * speech utterance and expects no additional speech. Therefore, the server
   * will not process additional audio (although it may subsequently return
   * additional results). The client should stop sending additional audio
   * data, half-close the gRPC connection, and wait for any additional results
   * until the server closes the gRPC connection. This message is only sent if
   * [`single_utterance`][google.cloud.dialogflow.cx.v3.InputAudioConfig.single_utterance]
   * was set to `true`, and is not used otherwise.
   *
   * @generated from enum value: END_OF_SINGLE_UTTERANCE = 2;
   */
  END_OF_SINGLE_UTTERANCE = 2,
}

/**
 * Describes the enum google.cloud.dialogflow.cx.v3.StreamingRecognitionResult.MessageType.
 */
export const StreamingRecognitionResult_MessageTypeSchema: GenEnum<StreamingRecognitionResult_MessageType> = /*@__PURE__*/
  enumDesc(file_google_cloud_dialogflow_cx_v3_session, 7, 0);

/**
 * Represents the parameters of a conversational query.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.QueryParameters
 */
export type QueryParameters = Message<"google.cloud.dialogflow.cx.v3.QueryParameters"> & {
  /**
   * The time zone of this conversational query from the [time zone
   * database](https://www.iana.org/time-zones), e.g., America/New_York,
   * Europe/Paris. If not provided, the time zone specified in the agent is
   * used.
   *
   * @generated from field: string time_zone = 1;
   */
  timeZone: string;

  /**
   * The geo location of this conversational query.
   *
   * @generated from field: google.type.LatLng geo_location = 2;
   */
  geoLocation?: LatLng;

  /**
   * Additional session entity types to replace or extend developer entity types
   * with. The entity synonyms apply to all languages and persist for the
   * session of this query.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3.SessionEntityType session_entity_types = 3;
   */
  sessionEntityTypes: SessionEntityType[];

  /**
   * This field can be used to pass custom data into the webhook associated with
   * the agent. Arbitrary JSON objects are supported.
   * Some integrations that query a Dialogflow agent may provide additional
   * information in the payload.
   * In particular, for the Dialogflow Phone Gateway integration, this field has
   * the form:
   * ```
   * {
   *  "telephony": {
   *    "caller_id": "+18558363987"
   *  }
   * }
   * ```
   *
   * @generated from field: google.protobuf.Struct payload = 4;
   */
  payload?: JsonObject;

  /**
   * Additional parameters to be put into [session
   * parameters][SessionInfo.parameters]. To remove a
   * parameter from the session, clients should explicitly set the parameter
   * value to null.
   *
   * You can reference the session parameters in the agent with the following
   * format: $session.params.parameter-id.
   *
   * Depending on your protocol or client library language, this is a
   * map, associative array, symbol table, dictionary, or JSON object
   * composed of a collection of (MapKey, MapValue) pairs:
   *
   * * MapKey type: string
   * * MapKey value: parameter name
   * * MapValue type: If parameter's entity type is a composite entity then use
   * map, otherwise, depending on the parameter value type, it could be one of
   * string, number, boolean, null, list or map.
   * * MapValue value: If parameter's entity type is a composite entity then use
   * map from composite entity property names to property values, otherwise,
   * use parameter value.
   *
   * @generated from field: google.protobuf.Struct parameters = 5;
   */
  parameters?: JsonObject;

  /**
   * The unique identifier of the [page][google.cloud.dialogflow.cx.v3.Page] to
   * override the [current page][QueryResult.current_page] in the session.
   * Format: `projects/<Project ID>/locations/<Location ID>/agents/<Agent
   * ID>/flows/<Flow ID>/pages/<Page ID>`.
   *
   * If `current_page` is specified, the previous state of the session will be
   * ignored by Dialogflow, including the [previous
   * page][QueryResult.current_page] and the [previous session
   * parameters][QueryResult.parameters].
   * In most cases,
   * [current_page][google.cloud.dialogflow.cx.v3.QueryParameters.current_page]
   * and [parameters][google.cloud.dialogflow.cx.v3.QueryParameters.parameters]
   * should be configured together to direct a session to a specific state.
   *
   * @generated from field: string current_page = 6;
   */
  currentPage: string;

  /**
   * Whether to disable webhook calls for this request.
   *
   * @generated from field: bool disable_webhook = 7;
   */
  disableWebhook: boolean;

  /**
   * Configures whether sentiment analysis should be performed. If not
   * provided, sentiment analysis is not performed.
   *
   * @generated from field: bool analyze_query_text_sentiment = 8;
   */
  analyzeQueryTextSentiment: boolean;

  /**
   * This field can be used to pass HTTP headers for a webhook
   * call. These headers will be sent to webhook along with the headers that
   * have been configured through Dialogflow web console. The headers defined
   * within this field will overwrite the headers configured through Dialogflow
   * console if there is a conflict. Header names are case-insensitive.
   * Google's specified headers are not allowed. Including: "Host",
   * "Content-Length", "Connection", "From", "User-Agent", "Accept-Encoding",
   * "If-Modified-Since", "If-None-Match", "X-Forwarded-For", etc.
   *
   * @generated from field: map<string, string> webhook_headers = 10;
   */
  webhookHeaders: { [key: string]: string };

  /**
   * A list of flow versions to override for the request.
   * Format: `projects/<Project ID>/locations/<Location ID>/agents/<Agent
   * ID>/flows/<Flow ID>/versions/<Version ID>`.
   *
   * If version 1 of flow X is included in this list, the traffic of
   * flow X will go through version 1 regardless of the version configuration in
   * the environment. Each flow can have at most one version specified in this
   * list.
   *
   * @generated from field: repeated string flow_versions = 14;
   */
  flowVersions: string[];

  /**
   * The channel which this query is for.
   *
   * If specified, only the
   * [ResponseMessage][google.cloud.dialogflow.cx.v3.ResponseMessage] associated
   * with the channel will be returned. If no
   * [ResponseMessage][google.cloud.dialogflow.cx.v3.ResponseMessage] is
   * associated with the channel, it falls back to the
   * [ResponseMessage][google.cloud.dialogflow.cx.v3.ResponseMessage] with
   * unspecified channel.
   *
   * If unspecified, the
   * [ResponseMessage][google.cloud.dialogflow.cx.v3.ResponseMessage] with
   * unspecified channel will be returned.
   *
   * @generated from field: string channel = 15;
   */
  channel: string;

  /**
   * Optional. Configure lifetime of the Dialogflow session.
   * By default, a Dialogflow session remains active and its data is stored for
   * 30 minutes after the last request is sent for the session.
   * This value should be no longer than 1 day.
   *
   * @generated from field: google.protobuf.Duration session_ttl = 16;
   */
  sessionTtl?: Duration;

  /**
   * Optional. Information about the end-user to improve the relevance and
   * accuracy of generative answers.
   *
   * This will be interpreted and used by a language model, so, for good
   * results, the data should be self-descriptive, and in a simple structure.
   *
   * Example:
   *
   * ```json
   * {
   *   "subscription plan": "Business Premium Plus",
   *   "devices owned": [
   *     {"model": "Google Pixel 7"},
   *     {"model": "Google Pixel Tablet"}
   *   ]
   * }
   * ```
   *
   * @generated from field: google.protobuf.Struct end_user_metadata = 18;
   */
  endUserMetadata?: JsonObject;

  /**
   * Optional. Search configuration for UCS search queries.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.SearchConfig search_config = 20;
   */
  searchConfig?: SearchConfig;

  /**
   * Optional. If set to true and data stores are involved in serving the
   * request then
   * DetectIntentResponse.query_result.data_store_connection_signals
   * will be filled with data that can help evaluations.
   *
   * @generated from field: bool populate_data_store_connection_signals = 25;
   */
  populateDataStoreConnectionSignals: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.QueryParameters.
 * Use `create(QueryParametersSchema)` to create a new message.
 */
export const QueryParametersSchema: GenMessage<QueryParameters> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 8);

/**
 * Search configuration for UCS search queries.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.SearchConfig
 */
export type SearchConfig = Message<"google.cloud.dialogflow.cx.v3.SearchConfig"> & {
  /**
   * Optional. Boosting configuration for the datastores.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3.BoostSpecs boost_specs = 1;
   */
  boostSpecs: BoostSpecs[];

  /**
   * Optional. Filter configuration for the datastores.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3.FilterSpecs filter_specs = 2;
   */
  filterSpecs: FilterSpecs[];
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.SearchConfig.
 * Use `create(SearchConfigSchema)` to create a new message.
 */
export const SearchConfigSchema: GenMessage<SearchConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 9);

/**
 * Boost specification to boost certain documents.
 * A copy of google.cloud.discoveryengine.v1main.BoostSpec, field documentation
 * is available at
 * https://cloud.google.com/generative-ai-app-builder/docs/reference/rest/v1alpha/BoostSpec
 *
 * @generated from message google.cloud.dialogflow.cx.v3.BoostSpec
 */
export type BoostSpec = Message<"google.cloud.dialogflow.cx.v3.BoostSpec"> & {
  /**
   * Optional. Condition boost specifications. If a document matches multiple
   * conditions in the specifictions, boost scores from these specifications are
   * all applied and combined in a non-linear way. Maximum number of
   * specifications is 20.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3.BoostSpec.ConditionBoostSpec condition_boost_specs = 1;
   */
  conditionBoostSpecs: BoostSpec_ConditionBoostSpec[];
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.BoostSpec.
 * Use `create(BoostSpecSchema)` to create a new message.
 */
export const BoostSpecSchema: GenMessage<BoostSpec> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 10);

/**
 * Boost applies to documents which match a condition.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.BoostSpec.ConditionBoostSpec
 */
export type BoostSpec_ConditionBoostSpec = Message<"google.cloud.dialogflow.cx.v3.BoostSpec.ConditionBoostSpec"> & {
  /**
   * Optional. An expression which specifies a boost condition. The syntax and
   * supported fields are the same as a filter expression.
   * Examples:
   *
   * * To boost documents with document ID "doc_1" or "doc_2", and
   * color
   *   "Red" or "Blue":
   *     * (id: ANY("doc_1", "doc_2")) AND (color: ANY("Red","Blue"))
   *
   * @generated from field: string condition = 1;
   */
  condition: string;

  /**
   * Optional. Strength of the condition boost, which should be in [-1, 1].
   * Negative boost means demotion. Default is 0.0.
   *
   * Setting to 1.0 gives the document a big promotion. However, it does not
   * necessarily mean that the boosted document will be the top result at
   * all times, nor that other documents will be excluded. Results could
   * still be shown even when none of them matches the condition. And
   * results that are significantly more relevant to the search query can
   * still trump your heavily favored but irrelevant documents.
   *
   * Setting to -1.0 gives the document a big demotion. However, results
   * that are deeply relevant might still be shown. The document will have
   * an upstream battle to get a fairly high ranking, but it is not blocked
   * out completely.
   *
   * Setting to 0.0 means no boost applied. The boosting condition is
   * ignored.
   *
   * @generated from field: float boost = 2;
   */
  boost: number;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.BoostSpec.ConditionBoostSpec.
 * Use `create(BoostSpec_ConditionBoostSpecSchema)` to create a new message.
 */
export const BoostSpec_ConditionBoostSpecSchema: GenMessage<BoostSpec_ConditionBoostSpec> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 10, 0);

/**
 * Boost specifications for data stores.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.BoostSpecs
 */
export type BoostSpecs = Message<"google.cloud.dialogflow.cx.v3.BoostSpecs"> & {
  /**
   * Optional. Data Stores where the boosting configuration is applied. The full
   * names of the referenced data stores. Formats:
   * `projects/{project}/locations/{location}/collections/{collection}/dataStores/{data_store}`
   * `projects/{project}/locations/{location}/dataStores/{data_store}`
   *
   * @generated from field: repeated string data_stores = 1;
   */
  dataStores: string[];

  /**
   * Optional. A list of boosting specifications.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3.BoostSpec spec = 2;
   */
  spec: BoostSpec[];
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.BoostSpecs.
 * Use `create(BoostSpecsSchema)` to create a new message.
 */
export const BoostSpecsSchema: GenMessage<BoostSpecs> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 11);

/**
 * Filter specifications for data stores.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.FilterSpecs
 */
export type FilterSpecs = Message<"google.cloud.dialogflow.cx.v3.FilterSpecs"> & {
  /**
   * Optional. Data Stores where the boosting configuration is applied. The full
   * names of the referenced data stores. Formats:
   * `projects/{project}/locations/{location}/collections/{collection}/dataStores/{data_store}`
   * `projects/{project}/locations/{location}/dataStores/{data_store}`
   *
   * @generated from field: repeated string data_stores = 1;
   */
  dataStores: string[];

  /**
   * Optional. The filter expression to be applied.
   * Expression syntax is documented at
   * https://cloud.google.com/generative-ai-app-builder/docs/filter-search-metadata#filter-expression-syntax
   *
   * @generated from field: string filter = 2;
   */
  filter: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.FilterSpecs.
 * Use `create(FilterSpecsSchema)` to create a new message.
 */
export const FilterSpecsSchema: GenMessage<FilterSpecs> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 12);

/**
 * Represents the query input. It can contain one of:
 *
 * 1. A conversational query in the form of text.
 *
 * 2. An intent query that specifies which intent to trigger.
 *
 * 3. Natural language speech audio to be processed.
 *
 * 4. An event to be triggered.
 *
 * 5. DTMF digits to invoke an intent and fill in parameter value.
 *
 * 6. The results of a tool executed by the client.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.QueryInput
 */
export type QueryInput = Message<"google.cloud.dialogflow.cx.v3.QueryInput"> & {
  /**
   * Required. The input specification.
   *
   * @generated from oneof google.cloud.dialogflow.cx.v3.QueryInput.input
   */
  input: {
    /**
     * The natural language text to be processed.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.TextInput text = 2;
     */
    value: TextInput;
    case: "text";
  } | {
    /**
     * The intent to be triggered.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.IntentInput intent = 3;
     */
    value: IntentInput;
    case: "intent";
  } | {
    /**
     * The natural language speech audio to be processed.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.AudioInput audio = 5;
     */
    value: AudioInput;
    case: "audio";
  } | {
    /**
     * The event to be triggered.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.EventInput event = 6;
     */
    value: EventInput;
    case: "event";
  } | {
    /**
     * The DTMF event to be handled.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.DtmfInput dtmf = 7;
     */
    value: DtmfInput;
    case: "dtmf";
  } | { case: undefined; value?: undefined };

  /**
   * Required. The language of the input. See [Language
   * Support](https://cloud.google.com/dialogflow/cx/docs/reference/language)
   * for a list of the currently supported language codes. Note that queries in
   * the same session do not necessarily need to specify the same language.
   *
   * @generated from field: string language_code = 4;
   */
  languageCode: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.QueryInput.
 * Use `create(QueryInputSchema)` to create a new message.
 */
export const QueryInputSchema: GenMessage<QueryInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 13);

/**
 * Represents the result of a conversational query.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.QueryResult
 */
export type QueryResult = Message<"google.cloud.dialogflow.cx.v3.QueryResult"> & {
  /**
   * The original conversational query.
   *
   * @generated from oneof google.cloud.dialogflow.cx.v3.QueryResult.query
   */
  query: {
    /**
     * If [natural language text][google.cloud.dialogflow.cx.v3.TextInput] was
     * provided as input, this field will contain a copy of the text.
     *
     * @generated from field: string text = 1;
     */
    value: string;
    case: "text";
  } | {
    /**
     * If an [intent][google.cloud.dialogflow.cx.v3.IntentInput] was provided as
     * input, this field will contain a copy of the intent identifier. Format:
     * `projects/<Project ID>/locations/<Location ID>/agents/<Agent
     * ID>/intents/<Intent ID>`.
     *
     * @generated from field: string trigger_intent = 11;
     */
    value: string;
    case: "triggerIntent";
  } | {
    /**
     * If [natural language speech
     * audio][google.cloud.dialogflow.cx.v3.AudioInput] was provided as input,
     * this field will contain the transcript for the audio.
     *
     * @generated from field: string transcript = 12;
     */
    value: string;
    case: "transcript";
  } | {
    /**
     * If an [event][google.cloud.dialogflow.cx.v3.EventInput] was provided as
     * input, this field will contain the name of the event.
     *
     * @generated from field: string trigger_event = 14;
     */
    value: string;
    case: "triggerEvent";
  } | {
    /**
     * If a [DTMF][google.cloud.dialogflow.cx.v3.DtmfInput] was provided as
     * input, this field will contain a copy of the
     * [DtmfInput][google.cloud.dialogflow.cx.v3.DtmfInput].
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.DtmfInput dtmf = 23;
     */
    value: DtmfInput;
    case: "dtmf";
  } | { case: undefined; value?: undefined };

  /**
   * The language that was triggered during intent detection.
   * See [Language
   * Support](https://cloud.google.com/dialogflow/cx/docs/reference/language)
   * for a list of the currently supported language codes.
   *
   * @generated from field: string language_code = 2;
   */
  languageCode: string;

  /**
   * The collected [session
   * parameters][google.cloud.dialogflow.cx.v3.SessionInfo.parameters].
   *
   * Depending on your protocol or client library language, this is a
   * map, associative array, symbol table, dictionary, or JSON object
   * composed of a collection of (MapKey, MapValue) pairs:
   *
   * * MapKey type: string
   * * MapKey value: parameter name
   * * MapValue type: If parameter's entity type is a composite entity then use
   * map, otherwise, depending on the parameter value type, it could be one of
   * string, number, boolean, null, list or map.
   * * MapValue value: If parameter's entity type is a composite entity then use
   * map from composite entity property names to property values, otherwise,
   * use parameter value.
   *
   * @generated from field: google.protobuf.Struct parameters = 3;
   */
  parameters?: JsonObject;

  /**
   * The list of rich messages returned to the client. Responses vary from
   * simple text messages to more sophisticated, structured payloads used
   * to drive complex logic.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3.ResponseMessage response_messages = 4;
   */
  responseMessages: ResponseMessage[];

  /**
   * The list of webhook ids in the order of call sequence.
   *
   * @generated from field: repeated string webhook_ids = 25;
   */
  webhookIds: string[];

  /**
   * The list of webhook display names in the order of call sequence.
   *
   * @generated from field: repeated string webhook_display_names = 26;
   */
  webhookDisplayNames: string[];

  /**
   * The list of webhook latencies in the order of call sequence.
   *
   * @generated from field: repeated google.protobuf.Duration webhook_latencies = 27;
   */
  webhookLatencies: Duration[];

  /**
   * The list of webhook tags in the order of call sequence.
   *
   * @generated from field: repeated string webhook_tags = 29;
   */
  webhookTags: string[];

  /**
   * The list of webhook call status in the order of call sequence.
   *
   * @generated from field: repeated google.rpc.Status webhook_statuses = 13;
   */
  webhookStatuses: Status[];

  /**
   * The list of webhook payload in
   * [WebhookResponse.payload][google.cloud.dialogflow.cx.v3.WebhookResponse.payload],
   * in the order of call sequence. If some webhook call fails or doesn't return
   * any payload, an empty `Struct` would be used instead.
   *
   * @generated from field: repeated google.protobuf.Struct webhook_payloads = 6;
   */
  webhookPayloads: JsonObject[];

  /**
   * The current [Page][google.cloud.dialogflow.cx.v3.Page]. Some, not all
   * fields are filled in this message, including but not limited to `name` and
   * `display_name`.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.Page current_page = 7;
   */
  currentPage?: Page;

  /**
   * The current [Flow][google.cloud.dialogflow.cx.v3.Flow]. Some, not all
   * fields are filled in this message, including but not limited to `name` and
   * `display_name`.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.Flow current_flow = 31;
   */
  currentFlow?: Flow;

  /**
   * The [Intent][google.cloud.dialogflow.cx.v3.Intent] that matched the
   * conversational query. Some, not all fields are filled in this message,
   * including but not limited to: `name` and `display_name`. This field is
   * deprecated, please use
   * [QueryResult.match][google.cloud.dialogflow.cx.v3.QueryResult.match]
   * instead.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.Intent intent = 8 [deprecated = true];
   * @deprecated
   */
  intent?: Intent;

  /**
   * The intent detection confidence. Values range from 0.0 (completely
   * uncertain) to 1.0 (completely certain).
   * This value is for informational purpose only and is only used to
   * help match the best intent within the classification threshold.
   * This value may change for the same end-user expression at any time due to a
   * model retraining or change in implementation.
   * This field is deprecated, please use
   * [QueryResult.match][google.cloud.dialogflow.cx.v3.QueryResult.match]
   * instead.
   *
   * @generated from field: float intent_detection_confidence = 9 [deprecated = true];
   * @deprecated
   */
  intentDetectionConfidence: number;

  /**
   * Intent match result, could be an intent or an event.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.Match match = 15;
   */
  match?: Match;

  /**
   * The free-form diagnostic info. For example, this field could contain
   * webhook call latency. The fields of this data can change without notice,
   * so you should not write code that depends on its structure.
   *
   * One of the fields is called "Alternative Matched Intents", which may
   * aid with debugging. The following describes these intent results:
   *
   * - The list is empty if no intent was matched to end-user input.
   * - Only intents that are referenced in the currently active flow are
   *   included.
   * - The matched intent is included.
   * - Other intents that could have matched end-user input, but did not match
   *   because they are referenced by intent routes that are out of
   *   [scope](https://cloud.google.com/dialogflow/cx/docs/concept/handler#scope),
   *   are included.
   * - Other intents referenced by intent routes in scope that matched end-user
   *   input, but had a lower confidence score.
   *
   * @generated from field: google.protobuf.Struct diagnostic_info = 10;
   */
  diagnosticInfo?: JsonObject;

  /**
   * The sentiment analyss result, which depends on
   * [`analyze_query_text_sentiment`]
   * [google.cloud.dialogflow.cx.v3.QueryParameters.analyze_query_text_sentiment],
   * specified in the request.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.SentimentAnalysisResult sentiment_analysis_result = 17;
   */
  sentimentAnalysisResult?: SentimentAnalysisResult;

  /**
   * Returns the current advanced settings including IVR settings. Even though
   * the operations configured by these settings are performed by Dialogflow,
   * the client may need to perform special logic at the moment. For example, if
   * Dialogflow exports audio to Google Cloud Storage, then the client may need
   * to wait for the resulting object to appear in the bucket before proceeding.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.AdvancedSettings advanced_settings = 21;
   */
  advancedSettings?: AdvancedSettings;

  /**
   * Indicates whether the Thumbs up/Thumbs down rating controls are need to be
   * shown for the response in the Dialogflow Messenger widget.
   *
   * @generated from field: bool allow_answer_feedback = 32;
   */
  allowAnswerFeedback: boolean;

  /**
   * Optional. Data store connection feature output signals.
   * Filled only when data stores are involved in serving the query and
   * DetectIntentRequest.populate data_store_connection_quality_signals is set
   * to true in the request.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.DataStoreConnectionSignals data_store_connection_signals = 35;
   */
  dataStoreConnectionSignals?: DataStoreConnectionSignals;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.QueryResult.
 * Use `create(QueryResultSchema)` to create a new message.
 */
export const QueryResultSchema: GenMessage<QueryResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 14);

/**
 * Represents the natural language text to be processed.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.TextInput
 */
export type TextInput = Message<"google.cloud.dialogflow.cx.v3.TextInput"> & {
  /**
   * Required. The UTF-8 encoded natural language text to be processed.
   *
   * @generated from field: string text = 1;
   */
  text: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.TextInput.
 * Use `create(TextInputSchema)` to create a new message.
 */
export const TextInputSchema: GenMessage<TextInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 15);

/**
 * Represents the intent to trigger programmatically rather than as a result of
 * natural language processing.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.IntentInput
 */
export type IntentInput = Message<"google.cloud.dialogflow.cx.v3.IntentInput"> & {
  /**
   * Required. The unique identifier of the intent.
   * Format: `projects/<Project ID>/locations/<Location ID>/agents/<Agent
   * ID>/intents/<Intent ID>`.
   *
   * @generated from field: string intent = 1;
   */
  intent: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.IntentInput.
 * Use `create(IntentInputSchema)` to create a new message.
 */
export const IntentInputSchema: GenMessage<IntentInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 16);

/**
 * Represents the natural speech audio to be processed.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.AudioInput
 */
export type AudioInput = Message<"google.cloud.dialogflow.cx.v3.AudioInput"> & {
  /**
   * Required. Instructs the speech recognizer how to process the speech audio.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.InputAudioConfig config = 1;
   */
  config?: InputAudioConfig;

  /**
   * The natural language speech audio to be processed.
   * A single request can contain up to 2 minutes of speech audio data.
   * The [transcribed
   * text][google.cloud.dialogflow.cx.v3.QueryResult.transcript] cannot contain
   * more than 256 bytes.
   *
   * For non-streaming audio detect intent, both `config` and `audio` must be
   * provided.
   * For streaming audio detect intent, `config` must be provided in
   * the first request and `audio` must be provided in all following requests.
   *
   * @generated from field: bytes audio = 2;
   */
  audio: Uint8Array;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.AudioInput.
 * Use `create(AudioInputSchema)` to create a new message.
 */
export const AudioInputSchema: GenMessage<AudioInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 17);

/**
 * Represents the event to trigger.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.EventInput
 */
export type EventInput = Message<"google.cloud.dialogflow.cx.v3.EventInput"> & {
  /**
   * Name of the event.
   *
   * @generated from field: string event = 1;
   */
  event: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.EventInput.
 * Use `create(EventInputSchema)` to create a new message.
 */
export const EventInputSchema: GenMessage<EventInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 18);

/**
 * Represents the input for dtmf event.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.DtmfInput
 */
export type DtmfInput = Message<"google.cloud.dialogflow.cx.v3.DtmfInput"> & {
  /**
   * The dtmf digits.
   *
   * @generated from field: string digits = 1;
   */
  digits: string;

  /**
   * The finish digit (if any).
   *
   * @generated from field: string finish_digit = 2;
   */
  finishDigit: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.DtmfInput.
 * Use `create(DtmfInputSchema)` to create a new message.
 */
export const DtmfInputSchema: GenMessage<DtmfInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 19);

/**
 * Represents one match result of [MatchIntent][].
 *
 * @generated from message google.cloud.dialogflow.cx.v3.Match
 */
export type Match = Message<"google.cloud.dialogflow.cx.v3.Match"> & {
  /**
   * The [Intent][google.cloud.dialogflow.cx.v3.Intent] that matched the query.
   * Some, not all fields are filled in this message, including but not limited
   * to: `name` and `display_name`. Only filled for
   * [`INTENT`][google.cloud.dialogflow.cx.v3.Match.MatchType] match type.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.Intent intent = 1;
   */
  intent?: Intent;

  /**
   * The event that matched the query. Filled for
   * [`EVENT`][google.cloud.dialogflow.cx.v3.Match.MatchType],
   * [`NO_MATCH`][google.cloud.dialogflow.cx.v3.Match.MatchType] and
   * [`NO_INPUT`][google.cloud.dialogflow.cx.v3.Match.MatchType] match types.
   *
   * @generated from field: string event = 6;
   */
  event: string;

  /**
   * The collection of parameters extracted from the query.
   *
   * Depending on your protocol or client library language, this is a
   * map, associative array, symbol table, dictionary, or JSON object
   * composed of a collection of (MapKey, MapValue) pairs:
   *
   * * MapKey type: string
   * * MapKey value: parameter name
   * * MapValue type: If parameter's entity type is a composite entity then use
   * map, otherwise, depending on the parameter value type, it could be one of
   * string, number, boolean, null, list or map.
   * * MapValue value: If parameter's entity type is a composite entity then use
   * map from composite entity property names to property values, otherwise,
   * use parameter value.
   *
   * @generated from field: google.protobuf.Struct parameters = 2;
   */
  parameters?: JsonObject;

  /**
   * Final text input which was matched during MatchIntent. This value can be
   * different from original input sent in request because of spelling
   * correction or other processing.
   *
   * @generated from field: string resolved_input = 3;
   */
  resolvedInput: string;

  /**
   * Type of this [Match][google.cloud.dialogflow.cx.v3.Match].
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.Match.MatchType match_type = 4;
   */
  matchType: Match_MatchType;

  /**
   * The confidence of this match. Values range from 0.0 (completely uncertain)
   * to 1.0 (completely certain).
   * This value is for informational purpose only and is only used to help match
   * the best intent within the classification threshold. This value may change
   * for the same end-user expression at any time due to a model retraining or
   * change in implementation.
   *
   * @generated from field: float confidence = 5;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.Match.
 * Use `create(MatchSchema)` to create a new message.
 */
export const MatchSchema: GenMessage<Match> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 20);

/**
 * Type of a Match.
 *
 * @generated from enum google.cloud.dialogflow.cx.v3.Match.MatchType
 */
export enum Match_MatchType {
  /**
   * Not specified. Should never be used.
   *
   * @generated from enum value: MATCH_TYPE_UNSPECIFIED = 0;
   */
  MATCH_TYPE_UNSPECIFIED = 0,

  /**
   * The query was matched to an intent.
   *
   * @generated from enum value: INTENT = 1;
   */
  INTENT = 1,

  /**
   * The query directly triggered an intent.
   *
   * @generated from enum value: DIRECT_INTENT = 2;
   */
  DIRECT_INTENT = 2,

  /**
   * The query was used for parameter filling.
   *
   * @generated from enum value: PARAMETER_FILLING = 3;
   */
  PARAMETER_FILLING = 3,

  /**
   * No match was found for the query.
   *
   * @generated from enum value: NO_MATCH = 4;
   */
  NO_MATCH = 4,

  /**
   * Indicates an empty query.
   *
   * @generated from enum value: NO_INPUT = 5;
   */
  NO_INPUT = 5,

  /**
   * The query directly triggered an event.
   *
   * @generated from enum value: EVENT = 6;
   */
  EVENT = 6,

  /**
   * The query was matched to a Knowledge Connector answer.
   *
   * @generated from enum value: KNOWLEDGE_CONNECTOR = 8;
   */
  KNOWLEDGE_CONNECTOR = 8,

  /**
   * The query was handled by a [`Playbook`][Playbook].
   *
   * @generated from enum value: PLAYBOOK = 9;
   */
  PLAYBOOK = 9,
}

/**
 * Describes the enum google.cloud.dialogflow.cx.v3.Match.MatchType.
 */
export const Match_MatchTypeSchema: GenEnum<Match_MatchType> = /*@__PURE__*/
  enumDesc(file_google_cloud_dialogflow_cx_v3_session, 20, 0);

/**
 * Request of [MatchIntent][].
 *
 * @generated from message google.cloud.dialogflow.cx.v3.MatchIntentRequest
 */
export type MatchIntentRequest = Message<"google.cloud.dialogflow.cx.v3.MatchIntentRequest"> & {
  /**
   * Required. The name of the session this query is sent to.
   * Format: `projects/<Project ID>/locations/<Location ID>/agents/<Agent
   * ID>/sessions/<Session ID>` or `projects/<Project ID>/locations/<Location
   * ID>/agents/<Agent ID>/environments/<Environment ID>/sessions/<Session ID>`.
   * If `Environment ID` is not specified, we assume default 'draft'
   * environment.
   * It's up to the API caller to choose an appropriate `Session ID`. It can be
   * a random number or some type of session identifiers (preferably hashed).
   * The length of the `Session ID` must not exceed 36 characters.
   *
   * For more information, see the [sessions
   * guide](https://cloud.google.com/dialogflow/cx/docs/concept/session).
   *
   * @generated from field: string session = 1;
   */
  session: string;

  /**
   * The parameters of this query.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.QueryParameters query_params = 2;
   */
  queryParams?: QueryParameters;

  /**
   * Required. The input specification.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.QueryInput query_input = 3;
   */
  queryInput?: QueryInput;

  /**
   * Persist session parameter changes from `query_params`.
   *
   * @generated from field: bool persist_parameter_changes = 5;
   */
  persistParameterChanges: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.MatchIntentRequest.
 * Use `create(MatchIntentRequestSchema)` to create a new message.
 */
export const MatchIntentRequestSchema: GenMessage<MatchIntentRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 21);

/**
 * Response of [MatchIntent][].
 *
 * @generated from message google.cloud.dialogflow.cx.v3.MatchIntentResponse
 */
export type MatchIntentResponse = Message<"google.cloud.dialogflow.cx.v3.MatchIntentResponse"> & {
  /**
   * The original conversational query.
   *
   * @generated from oneof google.cloud.dialogflow.cx.v3.MatchIntentResponse.query
   */
  query: {
    /**
     * If [natural language text][google.cloud.dialogflow.cx.v3.TextInput] was
     * provided as input, this field will contain a copy of the text.
     *
     * @generated from field: string text = 1;
     */
    value: string;
    case: "text";
  } | {
    /**
     * If an [intent][google.cloud.dialogflow.cx.v3.IntentInput] was provided as
     * input, this field will contain a copy of the intent identifier. Format:
     * `projects/<Project ID>/locations/<Location ID>/agents/<Agent
     * ID>/intents/<Intent ID>`.
     *
     * @generated from field: string trigger_intent = 2;
     */
    value: string;
    case: "triggerIntent";
  } | {
    /**
     * If [natural language speech
     * audio][google.cloud.dialogflow.cx.v3.AudioInput] was provided as input,
     * this field will contain the transcript for the audio.
     *
     * @generated from field: string transcript = 3;
     */
    value: string;
    case: "transcript";
  } | {
    /**
     * If an [event][google.cloud.dialogflow.cx.v3.EventInput] was provided as
     * input, this field will contain a copy of the event name.
     *
     * @generated from field: string trigger_event = 6;
     */
    value: string;
    case: "triggerEvent";
  } | { case: undefined; value?: undefined };

  /**
   * Match results, if more than one, ordered descendingly by the confidence
   * we have that the particular intent matches the query.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3.Match matches = 4;
   */
  matches: Match[];

  /**
   * The current [Page][google.cloud.dialogflow.cx.v3.Page]. Some, not all
   * fields are filled in this message, including but not limited to `name` and
   * `display_name`.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.Page current_page = 5;
   */
  currentPage?: Page;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.MatchIntentResponse.
 * Use `create(MatchIntentResponseSchema)` to create a new message.
 */
export const MatchIntentResponseSchema: GenMessage<MatchIntentResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 22);

/**
 * Request of [FulfillIntent][]
 *
 * @generated from message google.cloud.dialogflow.cx.v3.FulfillIntentRequest
 */
export type FulfillIntentRequest = Message<"google.cloud.dialogflow.cx.v3.FulfillIntentRequest"> & {
  /**
   * Must be same as the corresponding MatchIntent request, otherwise the
   * behavior is undefined.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.MatchIntentRequest match_intent_request = 1;
   */
  matchIntentRequest?: MatchIntentRequest;

  /**
   * The matched intent/event to fulfill.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.Match match = 2;
   */
  match?: Match;

  /**
   * Instructs the speech synthesizer how to generate output audio.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.OutputAudioConfig output_audio_config = 3;
   */
  outputAudioConfig?: OutputAudioConfig;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.FulfillIntentRequest.
 * Use `create(FulfillIntentRequestSchema)` to create a new message.
 */
export const FulfillIntentRequestSchema: GenMessage<FulfillIntentRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 23);

/**
 * Response of [FulfillIntent][]
 *
 * @generated from message google.cloud.dialogflow.cx.v3.FulfillIntentResponse
 */
export type FulfillIntentResponse = Message<"google.cloud.dialogflow.cx.v3.FulfillIntentResponse"> & {
  /**
   * Output only. The unique identifier of the response. It can be used to
   * locate a response in the training example set or for reporting issues.
   *
   * @generated from field: string response_id = 1;
   */
  responseId: string;

  /**
   * The result of the conversational query.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.QueryResult query_result = 2;
   */
  queryResult?: QueryResult;

  /**
   * The audio data bytes encoded as specified in the request.
   * Note: The output audio is generated based on the values of default platform
   * text responses found in the
   * [`query_result.response_messages`][google.cloud.dialogflow.cx.v3.QueryResult.response_messages]
   * field. If multiple default text responses exist, they will be concatenated
   * when generating audio. If no default platform text responses exist, the
   * generated audio content will be empty.
   *
   * In some scenarios, multiple output audio fields may be present in the
   * response structure. In these cases, only the top-most-level audio output
   * has content.
   *
   * @generated from field: bytes output_audio = 3;
   */
  outputAudio: Uint8Array;

  /**
   * The config used by the speech synthesizer to generate the output audio.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.OutputAudioConfig output_audio_config = 4;
   */
  outputAudioConfig?: OutputAudioConfig;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.FulfillIntentResponse.
 * Use `create(FulfillIntentResponseSchema)` to create a new message.
 */
export const FulfillIntentResponseSchema: GenMessage<FulfillIntentResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 24);

/**
 * The result of sentiment analysis. Sentiment analysis inspects user input
 * and identifies the prevailing subjective opinion, especially to determine a
 * user's attitude as positive, negative, or neutral.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.SentimentAnalysisResult
 */
export type SentimentAnalysisResult = Message<"google.cloud.dialogflow.cx.v3.SentimentAnalysisResult"> & {
  /**
   * Sentiment score between -1.0 (negative sentiment) and 1.0 (positive
   * sentiment).
   *
   * @generated from field: float score = 1;
   */
  score: number;

  /**
   * A non-negative number in the [0, +inf) range, which represents the absolute
   * magnitude of sentiment, regardless of score (positive or negative).
   *
   * @generated from field: float magnitude = 2;
   */
  magnitude: number;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.SentimentAnalysisResult.
 * Use `create(SentimentAnalysisResultSchema)` to create a new message.
 */
export const SentimentAnalysisResultSchema: GenMessage<SentimentAnalysisResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_session, 25);

/**
 * A session represents an interaction with a user. You retrieve user input
 * and pass it to the
 * [DetectIntent][google.cloud.dialogflow.cx.v3.Sessions.DetectIntent] method to
 * determine user intent and respond.
 *
 * @generated from service google.cloud.dialogflow.cx.v3.Sessions
 */
export const Sessions: GenService<{
  /**
   * Processes a natural language query and returns structured, actionable data
   * as a result. This method is not idempotent, because it may cause session
   * entity types to be updated, which in turn might affect results of future
   * queries.
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/cx/docs/concept/version).
   *
   * @generated from rpc google.cloud.dialogflow.cx.v3.Sessions.DetectIntent
   */
  detectIntent: {
    methodKind: "unary";
    input: typeof DetectIntentRequestSchema;
    output: typeof DetectIntentResponseSchema;
  },
  /**
   * Processes a natural language query and returns structured, actionable data
   * as a result through server-side streaming. Server-side streaming allows
   * Dialogflow to send [partial
   * responses](https://cloud.google.com/dialogflow/cx/docs/concept/fulfillment#partial-response)
   * earlier in a single request.
   *
   * @generated from rpc google.cloud.dialogflow.cx.v3.Sessions.ServerStreamingDetectIntent
   */
  serverStreamingDetectIntent: {
    methodKind: "server_streaming";
    input: typeof DetectIntentRequestSchema;
    output: typeof DetectIntentResponseSchema;
  },
  /**
   * Processes a natural language query in audio format in a streaming fashion
   * and returns structured, actionable data as a result. This method is only
   * available via the gRPC API (not REST).
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/cx/docs/concept/version).
   *
   * @generated from rpc google.cloud.dialogflow.cx.v3.Sessions.StreamingDetectIntent
   */
  streamingDetectIntent: {
    methodKind: "bidi_streaming";
    input: typeof StreamingDetectIntentRequestSchema;
    output: typeof StreamingDetectIntentResponseSchema;
  },
  /**
   * Returns preliminary intent match results, doesn't change the session
   * status.
   *
   * @generated from rpc google.cloud.dialogflow.cx.v3.Sessions.MatchIntent
   */
  matchIntent: {
    methodKind: "unary";
    input: typeof MatchIntentRequestSchema;
    output: typeof MatchIntentResponseSchema;
  },
  /**
   * Fulfills a matched intent returned by
   * [MatchIntent][google.cloud.dialogflow.cx.v3.Sessions.MatchIntent]. Must be
   * called after
   * [MatchIntent][google.cloud.dialogflow.cx.v3.Sessions.MatchIntent], with
   * input from
   * [MatchIntentResponse][google.cloud.dialogflow.cx.v3.MatchIntentResponse].
   * Otherwise, the behavior is undefined.
   *
   * @generated from rpc google.cloud.dialogflow.cx.v3.Sessions.FulfillIntent
   */
  fulfillIntent: {
    methodKind: "unary";
    input: typeof FulfillIntentRequestSchema;
    output: typeof FulfillIntentResponseSchema;
  },
  /**
   * Updates the feedback received from the user for a single turn of the bot
   * response.
   *
   * @generated from rpc google.cloud.dialogflow.cx.v3.Sessions.SubmitAnswerFeedback
   */
  submitAnswerFeedback: {
    methodKind: "unary";
    input: typeof SubmitAnswerFeedbackRequestSchema;
    output: typeof AnswerFeedbackSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_dialogflow_cx_v3_session, 0);

