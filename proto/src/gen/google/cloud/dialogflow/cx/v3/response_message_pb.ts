// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/dialogflow/cx/v3/response_message.proto (package google.cloud.dialogflow.cx.v3, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_field_behavior } from "../../../../api/field_behavior_pb";
import { file_google_protobuf_struct } from "@bufbuild/protobuf/wkt";
import type { JsonObject, Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/dialogflow/cx/v3/response_message.proto.
 */
export const file_google_cloud_dialogflow_cx_v3_response_message: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_field_behavior, file_google_protobuf_struct]);

/**
 * Represents a response message that can be returned by a conversational agent.
 *
 * Response messages are also used for output audio synthesis. The approach is
 * as follows:
 *
 * * If at least one OutputAudioText response is present, then all
 *   OutputAudioText responses are linearly concatenated, and the result is used
 *   for output audio synthesis.
 * * If the OutputAudioText responses are a mixture of text and SSML, then the
 *   concatenated result is treated as SSML; otherwise, the result is treated as
 *   either text or SSML as appropriate. The agent designer should ideally use
 *   either text or SSML consistently throughout the bot design.
 * * Otherwise, all Text responses are linearly concatenated, and the result is
 *   used for output audio synthesis.
 *
 * This approach allows for more sophisticated user experience scenarios, where
 * the text displayed to the user may differ from what is heard.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.ResponseMessage
 */
export type ResponseMessage = Message<"google.cloud.dialogflow.cx.v3.ResponseMessage"> & {
  /**
   * Required. The rich response message.
   *
   * @generated from oneof google.cloud.dialogflow.cx.v3.ResponseMessage.message
   */
  message: {
    /**
     * Returns a text response.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.ResponseMessage.Text text = 1;
     */
    value: ResponseMessage_Text;
    case: "text";
  } | {
    /**
     * Returns a response containing a custom, platform-specific payload.
     *
     * @generated from field: google.protobuf.Struct payload = 2;
     */
    value: JsonObject;
    case: "payload";
  } | {
    /**
     * Indicates that the conversation succeeded.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.ResponseMessage.ConversationSuccess conversation_success = 9;
     */
    value: ResponseMessage_ConversationSuccess;
    case: "conversationSuccess";
  } | {
    /**
     * A text or ssml response that is preferentially used for TTS output audio
     * synthesis, as described in the comment on the ResponseMessage message.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.ResponseMessage.OutputAudioText output_audio_text = 8;
     */
    value: ResponseMessage_OutputAudioText;
    case: "outputAudioText";
  } | {
    /**
     * Hands off conversation to a human agent.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.ResponseMessage.LiveAgentHandoff live_agent_handoff = 10;
     */
    value: ResponseMessage_LiveAgentHandoff;
    case: "liveAgentHandoff";
  } | {
    /**
     * Output only. A signal that indicates the interaction with the Dialogflow
     * agent has ended. This message is generated by Dialogflow only when the
     * conversation reaches `END_SESSION` page. It is not supposed to be defined
     * by the user.
     *
     * It's guaranteed that there is at most one such message in each response.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.ResponseMessage.EndInteraction end_interaction = 11;
     */
    value: ResponseMessage_EndInteraction;
    case: "endInteraction";
  } | {
    /**
     * Signal that the client should play an audio clip hosted at a
     * client-specific URI. Dialogflow uses this to construct
     * [mixed_audio][google.cloud.dialogflow.cx.v3.ResponseMessage.mixed_audio].
     * However, Dialogflow itself does not try to read or process the URI in any
     * way.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.ResponseMessage.PlayAudio play_audio = 12;
     */
    value: ResponseMessage_PlayAudio;
    case: "playAudio";
  } | {
    /**
     * Output only. An audio response message composed of both the synthesized
     * Dialogflow agent responses and responses defined via
     * [play_audio][google.cloud.dialogflow.cx.v3.ResponseMessage.play_audio].
     * This message is generated by Dialogflow only and not supposed to be
     * defined by the user.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.ResponseMessage.MixedAudio mixed_audio = 13;
     */
    value: ResponseMessage_MixedAudio;
    case: "mixedAudio";
  } | {
    /**
     * A signal that the client should transfer the phone call connected to
     * this agent to a third-party endpoint.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.ResponseMessage.TelephonyTransferCall telephony_transfer_call = 18;
     */
    value: ResponseMessage_TelephonyTransferCall;
    case: "telephonyTransferCall";
  } | {
    /**
     * Represents info card for knowledge answers, to be better rendered in
     * Dialogflow Messenger.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3.ResponseMessage.KnowledgeInfoCard knowledge_info_card = 20;
     */
    value: ResponseMessage_KnowledgeInfoCard;
    case: "knowledgeInfoCard";
  } | { case: undefined; value?: undefined };

  /**
   * Response type.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3.ResponseMessage.ResponseType response_type = 4;
   */
  responseType: ResponseMessage_ResponseType;

  /**
   * The channel which the response is associated with. Clients can specify the
   * channel via
   * [QueryParameters.channel][google.cloud.dialogflow.cx.v3.QueryParameters.channel],
   * and only associated channel response will be returned.
   *
   * @generated from field: string channel = 19;
   */
  channel: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.ResponseMessage.
 * Use `create(ResponseMessageSchema)` to create a new message.
 */
export const ResponseMessageSchema: GenMessage<ResponseMessage> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_response_message, 0);

/**
 * The text response message.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.ResponseMessage.Text
 */
export type ResponseMessage_Text = Message<"google.cloud.dialogflow.cx.v3.ResponseMessage.Text"> & {
  /**
   * Required. A collection of text responses.
   *
   * @generated from field: repeated string text = 1;
   */
  text: string[];

  /**
   * Output only. Whether the playback of this message can be interrupted by
   * the end user's speech and the client can then starts the next Dialogflow
   * request.
   *
   * @generated from field: bool allow_playback_interruption = 2;
   */
  allowPlaybackInterruption: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.ResponseMessage.Text.
 * Use `create(ResponseMessage_TextSchema)` to create a new message.
 */
export const ResponseMessage_TextSchema: GenMessage<ResponseMessage_Text> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_response_message, 0, 0);

/**
 * Indicates that the conversation should be handed off to a live agent.
 *
 * Dialogflow only uses this to determine which conversations were handed off
 * to a human agent for measurement purposes. What else to do with this signal
 * is up to you and your handoff procedures.
 *
 * You may set this, for example:
 * * In the
 * [entry_fulfillment][google.cloud.dialogflow.cx.v3.Page.entry_fulfillment]
 * of a [Page][google.cloud.dialogflow.cx.v3.Page] if
 *   entering the page indicates something went extremely wrong in the
 *   conversation.
 * * In a webhook response when you determine that the customer issue can only
 *   be handled by a human.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.ResponseMessage.LiveAgentHandoff
 */
export type ResponseMessage_LiveAgentHandoff = Message<"google.cloud.dialogflow.cx.v3.ResponseMessage.LiveAgentHandoff"> & {
  /**
   * Custom metadata for your handoff procedure. Dialogflow doesn't impose
   * any structure on this.
   *
   * @generated from field: google.protobuf.Struct metadata = 1;
   */
  metadata?: JsonObject;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.ResponseMessage.LiveAgentHandoff.
 * Use `create(ResponseMessage_LiveAgentHandoffSchema)` to create a new message.
 */
export const ResponseMessage_LiveAgentHandoffSchema: GenMessage<ResponseMessage_LiveAgentHandoff> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_response_message, 0, 1);

/**
 * Indicates that the conversation succeeded, i.e., the bot handled the issue
 * that the customer talked to it about.
 *
 * Dialogflow only uses this to determine which conversations should be
 * counted as successful and doesn't process the metadata in this message in
 * any way. Note that Dialogflow also considers conversations that get to the
 * conversation end page as successful even if they don't return
 * [ConversationSuccess][google.cloud.dialogflow.cx.v3.ResponseMessage.ConversationSuccess].
 *
 * You may set this, for example:
 * * In the
 * [entry_fulfillment][google.cloud.dialogflow.cx.v3.Page.entry_fulfillment]
 * of a [Page][google.cloud.dialogflow.cx.v3.Page] if
 *   entering the page indicates that the conversation succeeded.
 * * In a webhook response when you determine that you handled the customer
 *   issue.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.ResponseMessage.ConversationSuccess
 */
export type ResponseMessage_ConversationSuccess = Message<"google.cloud.dialogflow.cx.v3.ResponseMessage.ConversationSuccess"> & {
  /**
   * Custom metadata. Dialogflow doesn't impose any structure on this.
   *
   * @generated from field: google.protobuf.Struct metadata = 1;
   */
  metadata?: JsonObject;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.ResponseMessage.ConversationSuccess.
 * Use `create(ResponseMessage_ConversationSuccessSchema)` to create a new message.
 */
export const ResponseMessage_ConversationSuccessSchema: GenMessage<ResponseMessage_ConversationSuccess> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_response_message, 0, 2);

/**
 * A text or ssml response that is preferentially used for TTS output audio
 * synthesis, as described in the comment on the ResponseMessage message.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.ResponseMessage.OutputAudioText
 */
export type ResponseMessage_OutputAudioText = Message<"google.cloud.dialogflow.cx.v3.ResponseMessage.OutputAudioText"> & {
  /**
   * The source, which is either plain text or SSML.
   *
   * @generated from oneof google.cloud.dialogflow.cx.v3.ResponseMessage.OutputAudioText.source
   */
  source: {
    /**
     * The raw text to be synthesized.
     *
     * @generated from field: string text = 1;
     */
    value: string;
    case: "text";
  } | {
    /**
     * The SSML text to be synthesized. For more information, see
     * [SSML](/speech/text-to-speech/docs/ssml).
     *
     * @generated from field: string ssml = 2;
     */
    value: string;
    case: "ssml";
  } | { case: undefined; value?: undefined };

  /**
   * Output only. Whether the playback of this message can be interrupted by
   * the end user's speech and the client can then starts the next Dialogflow
   * request.
   *
   * @generated from field: bool allow_playback_interruption = 3;
   */
  allowPlaybackInterruption: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.ResponseMessage.OutputAudioText.
 * Use `create(ResponseMessage_OutputAudioTextSchema)` to create a new message.
 */
export const ResponseMessage_OutputAudioTextSchema: GenMessage<ResponseMessage_OutputAudioText> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_response_message, 0, 3);

/**
 * Indicates that interaction with the Dialogflow agent has ended.
 * This message is generated by Dialogflow only and not supposed to be
 * defined by the user.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.ResponseMessage.EndInteraction
 */
export type ResponseMessage_EndInteraction = Message<"google.cloud.dialogflow.cx.v3.ResponseMessage.EndInteraction"> & {
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.ResponseMessage.EndInteraction.
 * Use `create(ResponseMessage_EndInteractionSchema)` to create a new message.
 */
export const ResponseMessage_EndInteractionSchema: GenMessage<ResponseMessage_EndInteraction> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_response_message, 0, 4);

/**
 * Specifies an audio clip to be played by the client as part of the response.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.ResponseMessage.PlayAudio
 */
export type ResponseMessage_PlayAudio = Message<"google.cloud.dialogflow.cx.v3.ResponseMessage.PlayAudio"> & {
  /**
   * Required. URI of the audio clip. Dialogflow does not impose any
   * validation on this value. It is specific to the client that reads it.
   *
   * @generated from field: string audio_uri = 1;
   */
  audioUri: string;

  /**
   * Output only. Whether the playback of this message can be interrupted by
   * the end user's speech and the client can then starts the next Dialogflow
   * request.
   *
   * @generated from field: bool allow_playback_interruption = 2;
   */
  allowPlaybackInterruption: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.ResponseMessage.PlayAudio.
 * Use `create(ResponseMessage_PlayAudioSchema)` to create a new message.
 */
export const ResponseMessage_PlayAudioSchema: GenMessage<ResponseMessage_PlayAudio> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_response_message, 0, 5);

/**
 * Represents an audio message that is composed of both segments
 * synthesized from the Dialogflow agent prompts and ones hosted externally
 * at the specified URIs.
 * The external URIs are specified via
 * [play_audio][google.cloud.dialogflow.cx.v3.ResponseMessage.play_audio].
 * This message is generated by Dialogflow only and not supposed to be
 * defined by the user.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.ResponseMessage.MixedAudio
 */
export type ResponseMessage_MixedAudio = Message<"google.cloud.dialogflow.cx.v3.ResponseMessage.MixedAudio"> & {
  /**
   * Segments this audio response is composed of.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3.ResponseMessage.MixedAudio.Segment segments = 1;
   */
  segments: ResponseMessage_MixedAudio_Segment[];
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.ResponseMessage.MixedAudio.
 * Use `create(ResponseMessage_MixedAudioSchema)` to create a new message.
 */
export const ResponseMessage_MixedAudioSchema: GenMessage<ResponseMessage_MixedAudio> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_response_message, 0, 6);

/**
 * Represents one segment of audio.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.ResponseMessage.MixedAudio.Segment
 */
export type ResponseMessage_MixedAudio_Segment = Message<"google.cloud.dialogflow.cx.v3.ResponseMessage.MixedAudio.Segment"> & {
  /**
   * Content of the segment.
   *
   * @generated from oneof google.cloud.dialogflow.cx.v3.ResponseMessage.MixedAudio.Segment.content
   */
  content: {
    /**
     * Raw audio synthesized from the Dialogflow agent's response using
     * the output config specified in the request.
     *
     * @generated from field: bytes audio = 1;
     */
    value: Uint8Array;
    case: "audio";
  } | {
    /**
     * Client-specific URI that points to an audio clip accessible to the
     * client. Dialogflow does not impose any validation on it.
     *
     * @generated from field: string uri = 2;
     */
    value: string;
    case: "uri";
  } | { case: undefined; value?: undefined };

  /**
   * Output only. Whether the playback of this segment can be interrupted by
   * the end user's speech and the client should then start the next
   * Dialogflow request.
   *
   * @generated from field: bool allow_playback_interruption = 3;
   */
  allowPlaybackInterruption: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.ResponseMessage.MixedAudio.Segment.
 * Use `create(ResponseMessage_MixedAudio_SegmentSchema)` to create a new message.
 */
export const ResponseMessage_MixedAudio_SegmentSchema: GenMessage<ResponseMessage_MixedAudio_Segment> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_response_message, 0, 6, 0);

/**
 * Represents the signal that telles the client to transfer the phone call
 * connected to the agent to a third-party endpoint.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.ResponseMessage.TelephonyTransferCall
 */
export type ResponseMessage_TelephonyTransferCall = Message<"google.cloud.dialogflow.cx.v3.ResponseMessage.TelephonyTransferCall"> & {
  /**
   * Endpoint to transfer the call to.
   *
   * @generated from oneof google.cloud.dialogflow.cx.v3.ResponseMessage.TelephonyTransferCall.endpoint
   */
  endpoint: {
    /**
     * Transfer the call to a phone number
     * in [E.164 format](https://en.wikipedia.org/wiki/E.164).
     *
     * @generated from field: string phone_number = 1;
     */
    value: string;
    case: "phoneNumber";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.ResponseMessage.TelephonyTransferCall.
 * Use `create(ResponseMessage_TelephonyTransferCallSchema)` to create a new message.
 */
export const ResponseMessage_TelephonyTransferCallSchema: GenMessage<ResponseMessage_TelephonyTransferCall> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_response_message, 0, 7);

/**
 * Represents info card response. If the response contains generative
 * knowledge prediction, Dialogflow will return a payload with Infobot
 * Messenger compatible info card.
 *
 * Otherwise, the info card response is skipped.
 *
 * @generated from message google.cloud.dialogflow.cx.v3.ResponseMessage.KnowledgeInfoCard
 */
export type ResponseMessage_KnowledgeInfoCard = Message<"google.cloud.dialogflow.cx.v3.ResponseMessage.KnowledgeInfoCard"> & {
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3.ResponseMessage.KnowledgeInfoCard.
 * Use `create(ResponseMessage_KnowledgeInfoCardSchema)` to create a new message.
 */
export const ResponseMessage_KnowledgeInfoCardSchema: GenMessage<ResponseMessage_KnowledgeInfoCard> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3_response_message, 0, 8);

/**
 * Represents different response types.
 *
 * @generated from enum google.cloud.dialogflow.cx.v3.ResponseMessage.ResponseType
 */
export enum ResponseMessage_ResponseType {
  /**
   * Not specified.
   *
   * @generated from enum value: RESPONSE_TYPE_UNSPECIFIED = 0;
   */
  RESPONSE_TYPE_UNSPECIFIED = 0,

  /**
   * The response is from an [entry
   * prompt][google.cloud.dialogflow.cx.v3.Page.entry_fulfillment] in the
   * page.
   *
   * @generated from enum value: ENTRY_PROMPT = 1;
   */
  ENTRY_PROMPT = 1,

  /**
   * The response is from [form-filling
   * prompt][google.cloud.dialogflow.cx.v3.Form.Parameter.fill_behavior] in
   * the page.
   *
   * @generated from enum value: PARAMETER_PROMPT = 2;
   */
  PARAMETER_PROMPT = 2,

  /**
   * The response is from a [transition
   * route][google.cloud.dialogflow.cx.v3.TransitionRoute] or an [event
   * handler][EventHandler] in the page or flow or transition route group.
   *
   * @generated from enum value: HANDLER_PROMPT = 3;
   */
  HANDLER_PROMPT = 3,
}

/**
 * Describes the enum google.cloud.dialogflow.cx.v3.ResponseMessage.ResponseType.
 */
export const ResponseMessage_ResponseTypeSchema: GenEnum<ResponseMessage_ResponseType> = /*@__PURE__*/
  enumDesc(file_google_cloud_dialogflow_cx_v3_response_message, 0, 0);

