// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/dialogflow/cx/v3beta1/response_message.proto (package google.cloud.dialogflow.cx.v3beta1, syntax proto3)
/* eslint-disable */

import type { GenFile, GenMessage } from "@bufbuild/protobuf/codegenv1";
import { fileDesc, messageDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_field_behavior } from "../../../../api/field_behavior_pb";
import type { ToolCall } from "./tool_call_pb";
import { file_google_cloud_dialogflow_cx_v3beta1_tool_call } from "./tool_call_pb";
import { file_google_protobuf_struct } from "@bufbuild/protobuf/wkt";
import type { JsonObject, Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/dialogflow/cx/v3beta1/response_message.proto.
 */
export const file_google_cloud_dialogflow_cx_v3beta1_response_message: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_field_behavior, file_google_cloud_dialogflow_cx_v3beta1_tool_call, file_google_protobuf_struct]);

/**
 * Represents a response message that can be returned by a conversational agent.
 *
 * Response messages are also used for output audio synthesis. The approach is
 * as follows:
 *
 * * If at least one OutputAudioText response is present, then all
 *   OutputAudioText responses are linearly concatenated, and the result is used
 *   for output audio synthesis.
 * * If the OutputAudioText responses are a mixture of text and SSML, then the
 *   concatenated result is treated as SSML; otherwise, the result is treated as
 *   either text or SSML as appropriate. The agent designer should ideally use
 *   either text or SSML consistently throughout the bot design.
 * * Otherwise, all Text responses are linearly concatenated, and the result is
 *   used for output audio synthesis.
 *
 * This approach allows for more sophisticated user experience scenarios, where
 * the text displayed to the user may differ from what is heard.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.ResponseMessage
 */
export type ResponseMessage = Message<"google.cloud.dialogflow.cx.v3beta1.ResponseMessage"> & {
  /**
   * Required. The rich response message.
   *
   * @generated from oneof google.cloud.dialogflow.cx.v3beta1.ResponseMessage.message
   */
  message: {
    /**
     * Returns a text response.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.ResponseMessage.Text text = 1;
     */
    value: ResponseMessage_Text;
    case: "text";
  } | {
    /**
     * Returns a response containing a custom, platform-specific payload.
     *
     * @generated from field: google.protobuf.Struct payload = 2;
     */
    value: JsonObject;
    case: "payload";
  } | {
    /**
     * Indicates that the conversation succeeded.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.ResponseMessage.ConversationSuccess conversation_success = 9;
     */
    value: ResponseMessage_ConversationSuccess;
    case: "conversationSuccess";
  } | {
    /**
     * A text or ssml response that is preferentially used for TTS output audio
     * synthesis, as described in the comment on the ResponseMessage message.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.ResponseMessage.OutputAudioText output_audio_text = 8;
     */
    value: ResponseMessage_OutputAudioText;
    case: "outputAudioText";
  } | {
    /**
     * Hands off conversation to a human agent.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.ResponseMessage.LiveAgentHandoff live_agent_handoff = 10;
     */
    value: ResponseMessage_LiveAgentHandoff;
    case: "liveAgentHandoff";
  } | {
    /**
     * Output only. A signal that indicates the interaction with the Dialogflow
     * agent has ended. This message is generated by Dialogflow only when the
     * conversation reaches `END_SESSION` page. It is not supposed to be defined
     * by the user.
     *
     * It's guaranteed that there is at most one such message in each response.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.ResponseMessage.EndInteraction end_interaction = 11;
     */
    value: ResponseMessage_EndInteraction;
    case: "endInteraction";
  } | {
    /**
     * Signal that the client should play an audio clip hosted at a
     * client-specific URI. Dialogflow uses this to construct
     * [mixed_audio][google.cloud.dialogflow.cx.v3beta1.ResponseMessage.mixed_audio].
     * However, Dialogflow itself does not try to read or process the URI in any
     * way.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.ResponseMessage.PlayAudio play_audio = 12;
     */
    value: ResponseMessage_PlayAudio;
    case: "playAudio";
  } | {
    /**
     * Output only. An audio response message composed of both the synthesized
     * Dialogflow agent responses and responses defined via
     * [play_audio][google.cloud.dialogflow.cx.v3beta1.ResponseMessage.play_audio].
     * This message is generated by Dialogflow only and not supposed to be
     * defined by the user.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.ResponseMessage.MixedAudio mixed_audio = 13;
     */
    value: ResponseMessage_MixedAudio;
    case: "mixedAudio";
  } | {
    /**
     * A signal that the client should transfer the phone call connected to
     * this agent to a third-party endpoint.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.ResponseMessage.TelephonyTransferCall telephony_transfer_call = 18;
     */
    value: ResponseMessage_TelephonyTransferCall;
    case: "telephonyTransferCall";
  } | {
    /**
     * Represents info card for knowledge answers, to be better rendered in
     * Dialogflow Messenger.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.ResponseMessage.KnowledgeInfoCard knowledge_info_card = 20;
     */
    value: ResponseMessage_KnowledgeInfoCard;
    case: "knowledgeInfoCard";
  } | {
    /**
     * Returns the definition of a tool call that should be executed by the
     * client.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.ToolCall tool_call = 22;
     */
    value: ToolCall;
    case: "toolCall";
  } | { case: undefined; value?: undefined };

  /**
   * The channel which the response is associated with. Clients can specify the
   * channel via
   * [QueryParameters.channel][google.cloud.dialogflow.cx.v3beta1.QueryParameters.channel],
   * and only associated channel response will be returned.
   *
   * @generated from field: string channel = 19;
   */
  channel: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.
 * Use `create(ResponseMessageSchema)` to create a new message.
 */
export const ResponseMessageSchema: GenMessage<ResponseMessage> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_response_message, 0);

/**
 * The text response message.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.Text
 */
export type ResponseMessage_Text = Message<"google.cloud.dialogflow.cx.v3beta1.ResponseMessage.Text"> & {
  /**
   * Required. A collection of text responses.
   *
   * @generated from field: repeated string text = 1;
   */
  text: string[];

  /**
   * Output only. Whether the playback of this message can be interrupted by
   * the end user's speech and the client can then starts the next Dialogflow
   * request.
   *
   * @generated from field: bool allow_playback_interruption = 2;
   */
  allowPlaybackInterruption: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.Text.
 * Use `create(ResponseMessage_TextSchema)` to create a new message.
 */
export const ResponseMessage_TextSchema: GenMessage<ResponseMessage_Text> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_response_message, 0, 0);

/**
 * Indicates that the conversation should be handed off to a live agent.
 *
 * Dialogflow only uses this to determine which conversations were handed off
 * to a human agent for measurement purposes. What else to do with this signal
 * is up to you and your handoff procedures.
 *
 * You may set this, for example:
 * * In the
 * [entry_fulfillment][google.cloud.dialogflow.cx.v3beta1.Page.entry_fulfillment]
 * of a [Page][google.cloud.dialogflow.cx.v3beta1.Page] if
 *   entering the page indicates something went extremely wrong in the
 *   conversation.
 * * In a webhook response when you determine that the customer issue can only
 *   be handled by a human.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.LiveAgentHandoff
 */
export type ResponseMessage_LiveAgentHandoff = Message<"google.cloud.dialogflow.cx.v3beta1.ResponseMessage.LiveAgentHandoff"> & {
  /**
   * Custom metadata for your handoff procedure. Dialogflow doesn't impose
   * any structure on this.
   *
   * @generated from field: google.protobuf.Struct metadata = 1;
   */
  metadata?: JsonObject;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.LiveAgentHandoff.
 * Use `create(ResponseMessage_LiveAgentHandoffSchema)` to create a new message.
 */
export const ResponseMessage_LiveAgentHandoffSchema: GenMessage<ResponseMessage_LiveAgentHandoff> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_response_message, 0, 1);

/**
 * Indicates that the conversation succeeded, i.e., the bot handled the issue
 * that the customer talked to it about.
 *
 * Dialogflow only uses this to determine which conversations should be
 * counted as successful and doesn't process the metadata in this message in
 * any way. Note that Dialogflow also considers conversations that get to the
 * conversation end page as successful even if they don't return
 * [ConversationSuccess][google.cloud.dialogflow.cx.v3beta1.ResponseMessage.ConversationSuccess].
 *
 * You may set this, for example:
 * * In the
 * [entry_fulfillment][google.cloud.dialogflow.cx.v3beta1.Page.entry_fulfillment]
 * of a [Page][google.cloud.dialogflow.cx.v3beta1.Page] if
 *   entering the page indicates that the conversation succeeded.
 * * In a webhook response when you determine that you handled the customer
 *   issue.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.ConversationSuccess
 */
export type ResponseMessage_ConversationSuccess = Message<"google.cloud.dialogflow.cx.v3beta1.ResponseMessage.ConversationSuccess"> & {
  /**
   * Custom metadata. Dialogflow doesn't impose any structure on this.
   *
   * @generated from field: google.protobuf.Struct metadata = 1;
   */
  metadata?: JsonObject;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.ConversationSuccess.
 * Use `create(ResponseMessage_ConversationSuccessSchema)` to create a new message.
 */
export const ResponseMessage_ConversationSuccessSchema: GenMessage<ResponseMessage_ConversationSuccess> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_response_message, 0, 2);

/**
 * A text or ssml response that is preferentially used for TTS output audio
 * synthesis, as described in the comment on the ResponseMessage message.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.OutputAudioText
 */
export type ResponseMessage_OutputAudioText = Message<"google.cloud.dialogflow.cx.v3beta1.ResponseMessage.OutputAudioText"> & {
  /**
   * The source, which is either plain text or SSML.
   *
   * @generated from oneof google.cloud.dialogflow.cx.v3beta1.ResponseMessage.OutputAudioText.source
   */
  source: {
    /**
     * The raw text to be synthesized.
     *
     * @generated from field: string text = 1;
     */
    value: string;
    case: "text";
  } | {
    /**
     * The SSML text to be synthesized. For more information, see
     * [SSML](/speech/text-to-speech/docs/ssml).
     *
     * @generated from field: string ssml = 2;
     */
    value: string;
    case: "ssml";
  } | { case: undefined; value?: undefined };

  /**
   * Output only. Whether the playback of this message can be interrupted by
   * the end user's speech and the client can then starts the next Dialogflow
   * request.
   *
   * @generated from field: bool allow_playback_interruption = 3;
   */
  allowPlaybackInterruption: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.OutputAudioText.
 * Use `create(ResponseMessage_OutputAudioTextSchema)` to create a new message.
 */
export const ResponseMessage_OutputAudioTextSchema: GenMessage<ResponseMessage_OutputAudioText> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_response_message, 0, 3);

/**
 * Indicates that interaction with the Dialogflow agent has ended.
 * This message is generated by Dialogflow only and not supposed to be
 * defined by the user.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.EndInteraction
 */
export type ResponseMessage_EndInteraction = Message<"google.cloud.dialogflow.cx.v3beta1.ResponseMessage.EndInteraction"> & {
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.EndInteraction.
 * Use `create(ResponseMessage_EndInteractionSchema)` to create a new message.
 */
export const ResponseMessage_EndInteractionSchema: GenMessage<ResponseMessage_EndInteraction> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_response_message, 0, 4);

/**
 * Specifies an audio clip to be played by the client as part of the response.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.PlayAudio
 */
export type ResponseMessage_PlayAudio = Message<"google.cloud.dialogflow.cx.v3beta1.ResponseMessage.PlayAudio"> & {
  /**
   * Required. URI of the audio clip. Dialogflow does not impose any
   * validation on this value. It is specific to the client that reads it.
   *
   * @generated from field: string audio_uri = 1;
   */
  audioUri: string;

  /**
   * Output only. Whether the playback of this message can be interrupted by
   * the end user's speech and the client can then starts the next Dialogflow
   * request.
   *
   * @generated from field: bool allow_playback_interruption = 2;
   */
  allowPlaybackInterruption: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.PlayAudio.
 * Use `create(ResponseMessage_PlayAudioSchema)` to create a new message.
 */
export const ResponseMessage_PlayAudioSchema: GenMessage<ResponseMessage_PlayAudio> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_response_message, 0, 5);

/**
 * Represents an audio message that is composed of both segments
 * synthesized from the Dialogflow agent prompts and ones hosted externally
 * at the specified URIs.
 * The external URIs are specified via
 * [play_audio][google.cloud.dialogflow.cx.v3beta1.ResponseMessage.play_audio].
 * This message is generated by Dialogflow only and not supposed to be
 * defined by the user.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.MixedAudio
 */
export type ResponseMessage_MixedAudio = Message<"google.cloud.dialogflow.cx.v3beta1.ResponseMessage.MixedAudio"> & {
  /**
   * Segments this audio response is composed of.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3beta1.ResponseMessage.MixedAudio.Segment segments = 1;
   */
  segments: ResponseMessage_MixedAudio_Segment[];
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.MixedAudio.
 * Use `create(ResponseMessage_MixedAudioSchema)` to create a new message.
 */
export const ResponseMessage_MixedAudioSchema: GenMessage<ResponseMessage_MixedAudio> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_response_message, 0, 6);

/**
 * Represents one segment of audio.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.MixedAudio.Segment
 */
export type ResponseMessage_MixedAudio_Segment = Message<"google.cloud.dialogflow.cx.v3beta1.ResponseMessage.MixedAudio.Segment"> & {
  /**
   * Content of the segment.
   *
   * @generated from oneof google.cloud.dialogflow.cx.v3beta1.ResponseMessage.MixedAudio.Segment.content
   */
  content: {
    /**
     * Raw audio synthesized from the Dialogflow agent's response using
     * the output config specified in the request.
     *
     * @generated from field: bytes audio = 1;
     */
    value: Uint8Array;
    case: "audio";
  } | {
    /**
     * Client-specific URI that points to an audio clip accessible to the
     * client. Dialogflow does not impose any validation on it.
     *
     * @generated from field: string uri = 2;
     */
    value: string;
    case: "uri";
  } | { case: undefined; value?: undefined };

  /**
   * Output only. Whether the playback of this segment can be interrupted by
   * the end user's speech and the client should then start the next
   * Dialogflow request.
   *
   * @generated from field: bool allow_playback_interruption = 3;
   */
  allowPlaybackInterruption: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.MixedAudio.Segment.
 * Use `create(ResponseMessage_MixedAudio_SegmentSchema)` to create a new message.
 */
export const ResponseMessage_MixedAudio_SegmentSchema: GenMessage<ResponseMessage_MixedAudio_Segment> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_response_message, 0, 6, 0);

/**
 * Represents the signal that telles the client to transfer the phone call
 * connected to the agent to a third-party endpoint.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.TelephonyTransferCall
 */
export type ResponseMessage_TelephonyTransferCall = Message<"google.cloud.dialogflow.cx.v3beta1.ResponseMessage.TelephonyTransferCall"> & {
  /**
   * Endpoint to transfer the call to.
   *
   * @generated from oneof google.cloud.dialogflow.cx.v3beta1.ResponseMessage.TelephonyTransferCall.endpoint
   */
  endpoint: {
    /**
     * Transfer the call to a phone number
     * in [E.164 format](https://en.wikipedia.org/wiki/E.164).
     *
     * @generated from field: string phone_number = 1;
     */
    value: string;
    case: "phoneNumber";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.TelephonyTransferCall.
 * Use `create(ResponseMessage_TelephonyTransferCallSchema)` to create a new message.
 */
export const ResponseMessage_TelephonyTransferCallSchema: GenMessage<ResponseMessage_TelephonyTransferCall> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_response_message, 0, 7);

/**
 * Represents info card response. If the response contains generative
 * knowledge prediction, Dialogflow will return a payload with Infobot
 * Messenger compatible info card.
 *
 * Otherwise, the info card response is skipped.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.KnowledgeInfoCard
 */
export type ResponseMessage_KnowledgeInfoCard = Message<"google.cloud.dialogflow.cx.v3beta1.ResponseMessage.KnowledgeInfoCard"> & {
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.ResponseMessage.KnowledgeInfoCard.
 * Use `create(ResponseMessage_KnowledgeInfoCardSchema)` to create a new message.
 */
export const ResponseMessage_KnowledgeInfoCardSchema: GenMessage<ResponseMessage_KnowledgeInfoCard> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_response_message, 0, 8);

