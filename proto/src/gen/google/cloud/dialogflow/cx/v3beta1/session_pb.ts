// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/dialogflow/cx/v3beta1/session.proto (package google.cloud.dialogflow.cx.v3beta1, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../../api/annotations_pb";
import { file_google_api_client } from "../../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../../api/field_behavior_pb";
import { file_google_api_resource } from "../../../../api/resource_pb";
import type { AdvancedSettings } from "./advanced_settings_pb";
import { file_google_cloud_dialogflow_cx_v3beta1_advanced_settings } from "./advanced_settings_pb";
import type { InputAudioConfig, OutputAudioConfig, SpeechWordInfo } from "./audio_config_pb";
import { file_google_cloud_dialogflow_cx_v3beta1_audio_config } from "./audio_config_pb";
import type { DataStoreConnectionSignals } from "./data_store_connection_pb";
import { file_google_cloud_dialogflow_cx_v3beta1_data_store_connection } from "./data_store_connection_pb";
import type { Example } from "./example_pb";
import { file_google_cloud_dialogflow_cx_v3beta1_example } from "./example_pb";
import type { Flow } from "./flow_pb";
import { file_google_cloud_dialogflow_cx_v3beta1_flow } from "./flow_pb";
import type { LlmModelSettings } from "./generative_settings_pb";
import { file_google_cloud_dialogflow_cx_v3beta1_generative_settings } from "./generative_settings_pb";
import type { Intent } from "./intent_pb";
import { file_google_cloud_dialogflow_cx_v3beta1_intent } from "./intent_pb";
import type { Page } from "./page_pb";
import { file_google_cloud_dialogflow_cx_v3beta1_page } from "./page_pb";
import type { ResponseMessage } from "./response_message_pb";
import { file_google_cloud_dialogflow_cx_v3beta1_response_message } from "./response_message_pb";
import type { SessionEntityType } from "./session_entity_type_pb";
import { file_google_cloud_dialogflow_cx_v3beta1_session_entity_type } from "./session_entity_type_pb";
import type { ToolCallResult } from "./tool_call_pb";
import { file_google_cloud_dialogflow_cx_v3beta1_tool_call } from "./tool_call_pb";
import type { Duration, FieldMask } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_duration, file_google_protobuf_field_mask, file_google_protobuf_struct } from "@bufbuild/protobuf/wkt";
import type { Status } from "../../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../../rpc/status_pb";
import type { LatLng } from "../../../../type/latlng_pb";
import { file_google_type_latlng } from "../../../../type/latlng_pb";
import type { JsonObject, Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/dialogflow/cx/v3beta1/session.proto.
 */
export const file_google_cloud_dialogflow_cx_v3beta1_session: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_api_resource, file_google_cloud_dialogflow_cx_v3beta1_advanced_settings, file_google_cloud_dialogflow_cx_v3beta1_audio_config, file_google_cloud_dialogflow_cx_v3beta1_data_store_connection, file_google_cloud_dialogflow_cx_v3beta1_example, file_google_cloud_dialogflow_cx_v3beta1_flow, file_google_cloud_dialogflow_cx_v3beta1_generative_settings, file_google_cloud_dialogflow_cx_v3beta1_intent, file_google_cloud_dialogflow_cx_v3beta1_page, file_google_cloud_dialogflow_cx_v3beta1_response_message, file_google_cloud_dialogflow_cx_v3beta1_session_entity_type, file_google_cloud_dialogflow_cx_v3beta1_tool_call, file_google_protobuf_duration, file_google_protobuf_field_mask, file_google_protobuf_struct, file_google_rpc_status, file_google_type_latlng]);

/**
 * Stores information about feedback provided by users about a response.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.AnswerFeedback
 */
export type AnswerFeedback = Message<"google.cloud.dialogflow.cx.v3beta1.AnswerFeedback"> & {
  /**
   * Optional. Rating from user for the specific Dialogflow response.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.AnswerFeedback.Rating rating = 1;
   */
  rating: AnswerFeedback_Rating;

  /**
   * Optional. In case of thumbs down rating provided, users can optionally
   * provide context about the rating.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.AnswerFeedback.RatingReason rating_reason = 2;
   */
  ratingReason?: AnswerFeedback_RatingReason;

  /**
   * Optional. Custom rating from the user about the provided answer, with
   * maximum length of 1024 characters. For example, client could use a
   * customized JSON object to indicate the rating.
   *
   * @generated from field: string custom_rating = 3;
   */
  customRating: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.AnswerFeedback.
 * Use `create(AnswerFeedbackSchema)` to create a new message.
 */
export const AnswerFeedbackSchema: GenMessage<AnswerFeedback> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 0);

/**
 * Stores extra information about why users provided thumbs down rating.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.AnswerFeedback.RatingReason
 */
export type AnswerFeedback_RatingReason = Message<"google.cloud.dialogflow.cx.v3beta1.AnswerFeedback.RatingReason"> & {
  /**
   * Optional. Custom reason labels for thumbs down rating provided by the
   * user. The maximum number of labels allowed is 10 and the maximum length
   * of a single label is 128 characters.
   *
   * @generated from field: repeated string reason_labels = 3;
   */
  reasonLabels: string[];

  /**
   * Optional. Additional feedback about the rating.
   * This field can be populated without choosing a predefined `reason`.
   *
   * @generated from field: string feedback = 2;
   */
  feedback: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.AnswerFeedback.RatingReason.
 * Use `create(AnswerFeedback_RatingReasonSchema)` to create a new message.
 */
export const AnswerFeedback_RatingReasonSchema: GenMessage<AnswerFeedback_RatingReason> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 0, 0);

/**
 * Represents thumbs up/down rating provided by user about a response.
 *
 * @generated from enum google.cloud.dialogflow.cx.v3beta1.AnswerFeedback.Rating
 */
export enum AnswerFeedback_Rating {
  /**
   * Rating not specified.
   *
   * @generated from enum value: RATING_UNSPECIFIED = 0;
   */
  RATING_UNSPECIFIED = 0,

  /**
   * Thumbs up feedback from user.
   *
   * @generated from enum value: THUMBS_UP = 1;
   */
  THUMBS_UP = 1,

  /**
   * Thumbs down feedback from user.
   *
   * @generated from enum value: THUMBS_DOWN = 2;
   */
  THUMBS_DOWN = 2,
}

/**
 * Describes the enum google.cloud.dialogflow.cx.v3beta1.AnswerFeedback.Rating.
 */
export const AnswerFeedback_RatingSchema: GenEnum<AnswerFeedback_Rating> = /*@__PURE__*/
  enumDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 0, 0);

/**
 * The request to set the feedback for a bot answer.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.SubmitAnswerFeedbackRequest
 */
export type SubmitAnswerFeedbackRequest = Message<"google.cloud.dialogflow.cx.v3beta1.SubmitAnswerFeedbackRequest"> & {
  /**
   * Required. The name of the session the feedback was sent to.
   *
   * @generated from field: string session = 1;
   */
  session: string;

  /**
   * Required. ID of the response to update its feedback. This is the same as
   * DetectIntentResponse.response_id.
   *
   * @generated from field: string response_id = 2;
   */
  responseId: string;

  /**
   * Required. Feedback provided for a bot answer.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.AnswerFeedback answer_feedback = 3;
   */
  answerFeedback?: AnswerFeedback;

  /**
   * Optional. The mask to control which fields to update. If the mask is not
   * present, all fields will be updated.
   *
   * @generated from field: google.protobuf.FieldMask update_mask = 4;
   */
  updateMask?: FieldMask;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.SubmitAnswerFeedbackRequest.
 * Use `create(SubmitAnswerFeedbackRequestSchema)` to create a new message.
 */
export const SubmitAnswerFeedbackRequestSchema: GenMessage<SubmitAnswerFeedbackRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 1);

/**
 * The request to detect user's intent.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.DetectIntentRequest
 */
export type DetectIntentRequest = Message<"google.cloud.dialogflow.cx.v3beta1.DetectIntentRequest"> & {
  /**
   * Required. The name of the session this query is sent to.
   * Format: `projects/<Project ID>/locations/<Location ID>/agents/<Agent
   * ID>/sessions/<Session ID>` or `projects/<Project ID>/locations/<Location
   * ID>/agents/<Agent ID>/environments/<Environment ID>/sessions/<Session ID>`.
   * If `Environment ID` is not specified, we assume default 'draft'
   * environment.
   * It's up to the API caller to choose an appropriate `Session ID`. It can be
   * a random number or some type of session identifiers (preferably hashed).
   * The length of the `Session ID` must not exceed 36 characters.
   *
   * For more information, see the [sessions
   * guide](https://cloud.google.com/dialogflow/cx/docs/concept/session).
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/cx/docs/concept/version).
   *
   * @generated from field: string session = 1;
   */
  session: string;

  /**
   * The parameters of this query.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.QueryParameters query_params = 2;
   */
  queryParams?: QueryParameters;

  /**
   * Required. The input specification.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.QueryInput query_input = 3;
   */
  queryInput?: QueryInput;

  /**
   * Instructs the speech synthesizer how to generate the output audio.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.OutputAudioConfig output_audio_config = 4;
   */
  outputAudioConfig?: OutputAudioConfig;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.DetectIntentRequest.
 * Use `create(DetectIntentRequestSchema)` to create a new message.
 */
export const DetectIntentRequestSchema: GenMessage<DetectIntentRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 2);

/**
 * The message returned from the DetectIntent method.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.DetectIntentResponse
 */
export type DetectIntentResponse = Message<"google.cloud.dialogflow.cx.v3beta1.DetectIntentResponse"> & {
  /**
   * Output only. The unique identifier of the response. It can be used to
   * locate a response in the training example set or for reporting issues.
   *
   * @generated from field: string response_id = 1;
   */
  responseId: string;

  /**
   * The result of the conversational query.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.QueryResult query_result = 2;
   */
  queryResult?: QueryResult;

  /**
   * The audio data bytes encoded as specified in the request.
   * Note: The output audio is generated based on the values of default platform
   * text responses found in the
   * [`query_result.response_messages`][google.cloud.dialogflow.cx.v3beta1.QueryResult.response_messages]
   * field. If multiple default text responses exist, they will be concatenated
   * when generating audio. If no default platform text responses exist, the
   * generated audio content will be empty.
   *
   * In some scenarios, multiple output audio fields may be present in the
   * response structure. In these cases, only the top-most-level audio output
   * has content.
   *
   * @generated from field: bytes output_audio = 4;
   */
  outputAudio: Uint8Array;

  /**
   * The config used by the speech synthesizer to generate the output audio.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.OutputAudioConfig output_audio_config = 5;
   */
  outputAudioConfig?: OutputAudioConfig;

  /**
   * Response type.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.DetectIntentResponse.ResponseType response_type = 6;
   */
  responseType: DetectIntentResponse_ResponseType;

  /**
   * Indicates whether the partial response can be cancelled when a later
   * response arrives. e.g. if the agent specified some music as partial
   * response, it can be cancelled.
   *
   * @generated from field: bool allow_cancellation = 7;
   */
  allowCancellation: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.DetectIntentResponse.
 * Use `create(DetectIntentResponseSchema)` to create a new message.
 */
export const DetectIntentResponseSchema: GenMessage<DetectIntentResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 3);

/**
 * Represents different DetectIntentResponse types.
 *
 * @generated from enum google.cloud.dialogflow.cx.v3beta1.DetectIntentResponse.ResponseType
 */
export enum DetectIntentResponse_ResponseType {
  /**
   * Not specified. This should never happen.
   *
   * @generated from enum value: RESPONSE_TYPE_UNSPECIFIED = 0;
   */
  RESPONSE_TYPE_UNSPECIFIED = 0,

  /**
   * Partial response. e.g. Aggregated responses in a Fulfillment that enables
   * `return_partial_response` can be returned as partial response.
   * WARNING: partial response is not eligible for barge-in.
   *
   * @generated from enum value: PARTIAL = 1;
   */
  PARTIAL = 1,

  /**
   * Final response.
   *
   * @generated from enum value: FINAL = 2;
   */
  FINAL = 2,
}

/**
 * Describes the enum google.cloud.dialogflow.cx.v3beta1.DetectIntentResponse.ResponseType.
 */
export const DetectIntentResponse_ResponseTypeSchema: GenEnum<DetectIntentResponse_ResponseType> = /*@__PURE__*/
  enumDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 3, 0);

/**
 * The top-level message sent by the client to the
 * [Sessions.StreamingDetectIntent][google.cloud.dialogflow.cx.v3beta1.Sessions.StreamingDetectIntent]
 * method.
 *
 * Multiple request messages should be sent in order:
 *
 * 1.  The first message must contain
 *     [session][google.cloud.dialogflow.cx.v3beta1.StreamingDetectIntentRequest.session],
 *     [query_input][google.cloud.dialogflow.cx.v3beta1.StreamingDetectIntentRequest.query_input]
 *     plus optionally
 *     [query_params][google.cloud.dialogflow.cx.v3beta1.StreamingDetectIntentRequest.query_params].
 *     If the client wants to receive an audio response, it should also contain
 *     [output_audio_config][google.cloud.dialogflow.cx.v3beta1.StreamingDetectIntentRequest.output_audio_config].
 *
 * 2.  If
 * [query_input][google.cloud.dialogflow.cx.v3beta1.StreamingDetectIntentRequest.query_input]
 * was set to
 *     [query_input.audio.config][google.cloud.dialogflow.cx.v3beta1.AudioInput.config],
 *     all subsequent messages must contain
 *     [query_input.audio.audio][google.cloud.dialogflow.cx.v3beta1.AudioInput.audio]
 *     to continue with Speech recognition. If you decide to rather detect an
 *     intent from text input after you already started Speech recognition,
 *     please send a message with
 *     [query_input.text][google.cloud.dialogflow.cx.v3beta1.QueryInput.text].
 *
 *     However, note that:
 *
 *     * Dialogflow will bill you for the audio duration so far.
 *     * Dialogflow discards all Speech recognition results in favor of the
 *       input text.
 *     * Dialogflow will use the language code from the first message.
 *
 * After you sent all input, you must half-close or abort the request stream.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.StreamingDetectIntentRequest
 */
export type StreamingDetectIntentRequest = Message<"google.cloud.dialogflow.cx.v3beta1.StreamingDetectIntentRequest"> & {
  /**
   * The name of the session this query is sent to.
   * Format: `projects/<Project ID>/locations/<Location ID>/agents/<Agent
   * ID>/sessions/<Session ID>` or `projects/<Project ID>/locations/<Location
   * ID>/agents/<Agent ID>/environments/<Environment ID>/sessions/<Session ID>`.
   * If `Environment ID` is not specified, we assume default 'draft'
   * environment.
   * It's up to the API caller to choose an appropriate `Session ID`. It can be
   * a random number or some type of session identifiers (preferably hashed).
   * The length of the `Session ID` must not exceed 36 characters.
   * Note: session must be set in the first request.
   *
   * For more information, see the [sessions
   * guide](https://cloud.google.com/dialogflow/cx/docs/concept/session).
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/cx/docs/concept/version).
   *
   * @generated from field: string session = 1;
   */
  session: string;

  /**
   * The parameters of this query.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.QueryParameters query_params = 2;
   */
  queryParams?: QueryParameters;

  /**
   * Required. The input specification.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.QueryInput query_input = 3;
   */
  queryInput?: QueryInput;

  /**
   * Instructs the speech synthesizer how to generate the output audio.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.OutputAudioConfig output_audio_config = 4;
   */
  outputAudioConfig?: OutputAudioConfig;

  /**
   * Enable partial detect intent response. If this flag is not enabled,
   * response stream still contains only one final `DetectIntentResponse` even
   * if some `Fulfillment`s in the agent have been configured to return partial
   * responses.
   *
   * @generated from field: bool enable_partial_response = 5;
   */
  enablePartialResponse: boolean;

  /**
   * If true, `StreamingDetectIntentResponse.debugging_info` will get populated.
   *
   * @generated from field: bool enable_debugging_info = 8;
   */
  enableDebuggingInfo: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.StreamingDetectIntentRequest.
 * Use `create(StreamingDetectIntentRequestSchema)` to create a new message.
 */
export const StreamingDetectIntentRequestSchema: GenMessage<StreamingDetectIntentRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 4);

/**
 * Cloud conversation info for easier debugging.
 * It will get populated in `StreamingDetectIntentResponse` or
 * `StreamingAnalyzeContentResponse` when the flag `enable_debugging_info` is
 * set to true in corresponding requests.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.CloudConversationDebuggingInfo
 */
export type CloudConversationDebuggingInfo = Message<"google.cloud.dialogflow.cx.v3beta1.CloudConversationDebuggingInfo"> & {
  /**
   * Number of input audio data chunks in streaming requests.
   *
   * @generated from field: int32 audio_data_chunks = 1;
   */
  audioDataChunks: number;

  /**
   * Time offset of the end of speech utterance relative to the
   * beginning of the first audio chunk.
   *
   * @generated from field: google.protobuf.Duration result_end_time_offset = 2;
   */
  resultEndTimeOffset?: Duration;

  /**
   * Duration of first audio chunk.
   *
   * @generated from field: google.protobuf.Duration first_audio_duration = 3;
   */
  firstAudioDuration?: Duration;

  /**
   * Whether client used single utterance mode.
   *
   * @generated from field: bool single_utterance = 5;
   */
  singleUtterance: boolean;

  /**
   * Time offsets of the speech partial results relative to the beginning of
   * the stream.
   *
   * @generated from field: repeated google.protobuf.Duration speech_partial_results_end_times = 6;
   */
  speechPartialResultsEndTimes: Duration[];

  /**
   * Time offsets of the speech final results (is_final=true) relative to the
   * beginning of the stream.
   *
   * @generated from field: repeated google.protobuf.Duration speech_final_results_end_times = 7;
   */
  speechFinalResultsEndTimes: Duration[];

  /**
   * Total number of partial responses.
   *
   * @generated from field: int32 partial_responses = 8;
   */
  partialResponses: number;

  /**
   * Time offset of Speaker ID stream close time relative to the Speech stream
   * close time in milliseconds. Only meaningful for conversations involving
   * passive verification.
   *
   * @generated from field: int32 speaker_id_passive_latency_ms_offset = 9;
   */
  speakerIdPassiveLatencyMsOffset: number;

  /**
   * Whether a barge-in event is triggered in this request.
   *
   * @generated from field: bool bargein_event_triggered = 10;
   */
  bargeinEventTriggered: boolean;

  /**
   * Whether speech uses single utterance mode.
   *
   * @generated from field: bool speech_single_utterance = 11;
   */
  speechSingleUtterance: boolean;

  /**
   * Time offsets of the DTMF partial results relative to the beginning of
   * the stream.
   *
   * @generated from field: repeated google.protobuf.Duration dtmf_partial_results_times = 12;
   */
  dtmfPartialResultsTimes: Duration[];

  /**
   * Time offsets of the DTMF final results relative to the beginning of
   * the stream.
   *
   * @generated from field: repeated google.protobuf.Duration dtmf_final_results_times = 13;
   */
  dtmfFinalResultsTimes: Duration[];

  /**
   * Time offset of the end-of-single-utterance signal relative to the
   * beginning of the stream.
   *
   * @generated from field: google.protobuf.Duration single_utterance_end_time_offset = 14;
   */
  singleUtteranceEndTimeOffset?: Duration;

  /**
   * No speech timeout settings for the stream.
   *
   * @generated from field: google.protobuf.Duration no_speech_timeout = 15;
   */
  noSpeechTimeout?: Duration;

  /**
   * Speech endpointing timeout settings for the stream.
   *
   * @generated from field: google.protobuf.Duration endpointing_timeout = 19;
   */
  endpointingTimeout?: Duration;

  /**
   * Whether the streaming terminates with an injected text query.
   *
   * @generated from field: bool is_input_text = 16;
   */
  isInputText: boolean;

  /**
   * Client half close time in terms of input audio duration.
   *
   * @generated from field: google.protobuf.Duration client_half_close_time_offset = 17;
   */
  clientHalfCloseTimeOffset?: Duration;

  /**
   * Client half close time in terms of API streaming duration.
   *
   * @generated from field: google.protobuf.Duration client_half_close_streaming_time_offset = 18;
   */
  clientHalfCloseStreamingTimeOffset?: Duration;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.CloudConversationDebuggingInfo.
 * Use `create(CloudConversationDebuggingInfoSchema)` to create a new message.
 */
export const CloudConversationDebuggingInfoSchema: GenMessage<CloudConversationDebuggingInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 5);

/**
 * The top-level message returned from the
 * [StreamingDetectIntent][google.cloud.dialogflow.cx.v3beta1.Sessions.StreamingDetectIntent]
 * method.
 *
 * Multiple response messages (N) can be returned in order.
 *
 * The first (N-1) responses set either the `recognition_result` or
 * `detect_intent_response` field, depending on the request:
 *
 * *   If the `StreamingDetectIntentRequest.query_input.audio` field was
 *     set, and the `StreamingDetectIntentRequest.enable_partial_response`
 *     field was false, the `recognition_result` field is populated for each
 *     of the (N-1) responses.
 *     See the
 *     [StreamingRecognitionResult][google.cloud.dialogflow.cx.v3beta1.StreamingRecognitionResult]
 *     message for details about the result message sequence.
 *
 * *   If the `StreamingDetectIntentRequest.enable_partial_response` field was
 *     true, the `detect_intent_response` field is populated for each
 *     of the (N-1) responses, where 1 <= N <= 4.
 *     These responses set the
 *     [DetectIntentResponse.response_type][google.cloud.dialogflow.cx.v3beta1.DetectIntentResponse.response_type]
 *     field to `PARTIAL`.
 *
 * For the final Nth response message, the `detect_intent_response` is fully
 * populated, and
 * [DetectIntentResponse.response_type][google.cloud.dialogflow.cx.v3beta1.DetectIntentResponse.response_type]
 * is set to `FINAL`.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.StreamingDetectIntentResponse
 */
export type StreamingDetectIntentResponse = Message<"google.cloud.dialogflow.cx.v3beta1.StreamingDetectIntentResponse"> & {
  /**
   * The output response.
   *
   * @generated from oneof google.cloud.dialogflow.cx.v3beta1.StreamingDetectIntentResponse.response
   */
  response: {
    /**
     * The result of speech recognition.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.StreamingRecognitionResult recognition_result = 1;
     */
    value: StreamingRecognitionResult;
    case: "recognitionResult";
  } | {
    /**
     * The response from detect intent.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.DetectIntentResponse detect_intent_response = 2;
     */
    value: DetectIntentResponse;
    case: "detectIntentResponse";
  } | { case: undefined; value?: undefined };

  /**
   * Debugging info that would get populated when
   * `StreamingDetectIntentRequest.enable_debugging_info` is set to true.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.CloudConversationDebuggingInfo debugging_info = 4;
   */
  debuggingInfo?: CloudConversationDebuggingInfo;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.StreamingDetectIntentResponse.
 * Use `create(StreamingDetectIntentResponseSchema)` to create a new message.
 */
export const StreamingDetectIntentResponseSchema: GenMessage<StreamingDetectIntentResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 6);

/**
 * Contains a speech recognition result corresponding to a portion of the audio
 * that is currently being processed or an indication that this is the end
 * of the single requested utterance.
 *
 * While end-user audio is being processed, Dialogflow sends a series of
 * results. Each result may contain a `transcript` value. A transcript
 * represents a portion of the utterance. While the recognizer is processing
 * audio, transcript values may be interim values or finalized values.
 * Once a transcript is finalized, the `is_final` value is set to true and
 * processing continues for the next transcript.
 *
 * If `StreamingDetectIntentRequest.query_input.audio.config.single_utterance`
 * was true, and the recognizer has completed processing audio,
 * the `message_type` value is set to `END_OF_SINGLE_UTTERANCE and the
 * following (last) result contains the last finalized transcript.
 *
 * The complete end-user utterance is determined by concatenating the
 * finalized transcript values received for the series of results.
 *
 * In the following example, single utterance is enabled. In the case where
 * single utterance is not enabled, result 7 would not occur.
 *
 * ```
 * Num | transcript              | message_type            | is_final
 * --- | ----------------------- | ----------------------- | --------
 * 1   | "tube"                  | TRANSCRIPT              | false
 * 2   | "to be a"               | TRANSCRIPT              | false
 * 3   | "to be"                 | TRANSCRIPT              | false
 * 4   | "to be or not to be"    | TRANSCRIPT              | true
 * 5   | "that's"                | TRANSCRIPT              | false
 * 6   | "that is                | TRANSCRIPT              | false
 * 7   | unset                   | END_OF_SINGLE_UTTERANCE | unset
 * 8   | " that is the question" | TRANSCRIPT              | true
 * ```
 *
 * Concatenating the finalized transcripts with `is_final` set to true,
 * the complete utterance becomes "to be or not to be that is the question".
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.StreamingRecognitionResult
 */
export type StreamingRecognitionResult = Message<"google.cloud.dialogflow.cx.v3beta1.StreamingRecognitionResult"> & {
  /**
   * Type of the result message.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.StreamingRecognitionResult.MessageType message_type = 1;
   */
  messageType: StreamingRecognitionResult_MessageType;

  /**
   * Transcript text representing the words that the user spoke.
   * Populated if and only if `message_type` = `TRANSCRIPT`.
   *
   * @generated from field: string transcript = 2;
   */
  transcript: string;

  /**
   * If `false`, the `StreamingRecognitionResult` represents an
   * interim result that may change. If `true`, the recognizer will not return
   * any further hypotheses about this piece of the audio. May only be populated
   * for `message_type` = `TRANSCRIPT`.
   *
   * @generated from field: bool is_final = 3;
   */
  isFinal: boolean;

  /**
   * The Speech confidence between 0.0 and 1.0 for the current portion of audio.
   * A higher number indicates an estimated greater likelihood that the
   * recognized words are correct. The default of 0.0 is a sentinel value
   * indicating that confidence was not set.
   *
   * This field is typically only provided if `is_final` is true and you should
   * not rely on it being accurate or even set.
   *
   * @generated from field: float confidence = 4;
   */
  confidence: number;

  /**
   * An estimate of the likelihood that the speech recognizer will
   * not change its guess about this interim recognition result:
   * * If the value is unspecified or 0.0, Dialogflow didn't compute the
   *   stability. In particular, Dialogflow will only provide stability for
   *   `TRANSCRIPT` results with `is_final = false`.
   * * Otherwise, the value is in (0.0, 1.0] where 0.0 means completely
   *   unstable and 1.0 means completely stable.
   *
   * @generated from field: float stability = 6;
   */
  stability: number;

  /**
   * Word-specific information for the words recognized by Speech in
   * [transcript][google.cloud.dialogflow.cx.v3beta1.StreamingRecognitionResult.transcript].
   * Populated if and only if `message_type` = `TRANSCRIPT` and
   * [InputAudioConfig.enable_word_info] is set.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3beta1.SpeechWordInfo speech_word_info = 7;
   */
  speechWordInfo: SpeechWordInfo[];

  /**
   * Time offset of the end of this Speech recognition result relative to the
   * beginning of the audio. Only populated for `message_type` =
   * `TRANSCRIPT`.
   *
   * @generated from field: google.protobuf.Duration speech_end_offset = 8;
   */
  speechEndOffset?: Duration;

  /**
   * Detected language code for the transcript.
   *
   * @generated from field: string language_code = 10;
   */
  languageCode: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.StreamingRecognitionResult.
 * Use `create(StreamingRecognitionResultSchema)` to create a new message.
 */
export const StreamingRecognitionResultSchema: GenMessage<StreamingRecognitionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 7);

/**
 * Type of the response message.
 *
 * @generated from enum google.cloud.dialogflow.cx.v3beta1.StreamingRecognitionResult.MessageType
 */
export enum StreamingRecognitionResult_MessageType {
  /**
   * Not specified. Should never be used.
   *
   * @generated from enum value: MESSAGE_TYPE_UNSPECIFIED = 0;
   */
  MESSAGE_TYPE_UNSPECIFIED = 0,

  /**
   * Message contains a (possibly partial) transcript.
   *
   * @generated from enum value: TRANSCRIPT = 1;
   */
  TRANSCRIPT = 1,

  /**
   * This event indicates that the server has detected the end of the user's
   * speech utterance and expects no additional speech. Therefore, the server
   * will not process additional audio (although it may subsequently return
   * additional results). The client should stop sending additional audio
   * data, half-close the gRPC connection, and wait for any additional results
   * until the server closes the gRPC connection. This message is only sent if
   * [`single_utterance`][google.cloud.dialogflow.cx.v3beta1.InputAudioConfig.single_utterance]
   * was set to `true`, and is not used otherwise.
   *
   * @generated from enum value: END_OF_SINGLE_UTTERANCE = 2;
   */
  END_OF_SINGLE_UTTERANCE = 2,
}

/**
 * Describes the enum google.cloud.dialogflow.cx.v3beta1.StreamingRecognitionResult.MessageType.
 */
export const StreamingRecognitionResult_MessageTypeSchema: GenEnum<StreamingRecognitionResult_MessageType> = /*@__PURE__*/
  enumDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 7, 0);

/**
 * Represents the parameters of a conversational query.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.QueryParameters
 */
export type QueryParameters = Message<"google.cloud.dialogflow.cx.v3beta1.QueryParameters"> & {
  /**
   * The time zone of this conversational query from the [time zone
   * database](https://www.iana.org/time-zones), e.g., America/New_York,
   * Europe/Paris. If not provided, the time zone specified in the agent is
   * used.
   *
   * @generated from field: string time_zone = 1;
   */
  timeZone: string;

  /**
   * The geo location of this conversational query.
   *
   * @generated from field: google.type.LatLng geo_location = 2;
   */
  geoLocation?: LatLng;

  /**
   * Additional session entity types to replace or extend developer entity types
   * with. The entity synonyms apply to all languages and persist for the
   * session of this query.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3beta1.SessionEntityType session_entity_types = 3;
   */
  sessionEntityTypes: SessionEntityType[];

  /**
   * This field can be used to pass custom data into the webhook associated with
   * the agent. Arbitrary JSON objects are supported.
   * Some integrations that query a Dialogflow agent may provide additional
   * information in the payload.
   * In particular, for the Dialogflow Phone Gateway integration, this field has
   * the form:
   * ```
   * {
   *  "telephony": {
   *    "caller_id": "+18558363987"
   *  }
   * }
   * ```
   *
   * @generated from field: google.protobuf.Struct payload = 4;
   */
  payload?: JsonObject;

  /**
   * Additional parameters to be put into [session
   * parameters][SessionInfo.parameters]. To remove a
   * parameter from the session, clients should explicitly set the parameter
   * value to null.
   *
   * You can reference the session parameters in the agent with the following
   * format: $session.params.parameter-id.
   *
   * Depending on your protocol or client library language, this is a
   * map, associative array, symbol table, dictionary, or JSON object
   * composed of a collection of (MapKey, MapValue) pairs:
   *
   * * MapKey type: string
   * * MapKey value: parameter name
   * * MapValue type: If parameter's entity type is a composite entity then use
   * map, otherwise, depending on the parameter value type, it could be one of
   * string, number, boolean, null, list or map.
   * * MapValue value: If parameter's entity type is a composite entity then use
   * map from composite entity property names to property values, otherwise,
   * use parameter value.
   *
   * @generated from field: google.protobuf.Struct parameters = 5;
   */
  parameters?: JsonObject;

  /**
   * The unique identifier of the
   * [page][google.cloud.dialogflow.cx.v3beta1.Page] to override the [current
   * page][QueryResult.current_page] in the session.
   * Format: `projects/<Project ID>/locations/<Location ID>/agents/<Agent
   * ID>/flows/<Flow ID>/pages/<Page ID>`.
   *
   * If `current_page` is specified, the previous state of the session will be
   * ignored by Dialogflow, including the [previous
   * page][QueryResult.current_page] and the [previous session
   * parameters][QueryResult.parameters].
   * In most cases,
   * [current_page][google.cloud.dialogflow.cx.v3beta1.QueryParameters.current_page]
   * and
   * [parameters][google.cloud.dialogflow.cx.v3beta1.QueryParameters.parameters]
   * should be configured together to direct a session to a specific state.
   *
   * @generated from field: string current_page = 6;
   */
  currentPage: string;

  /**
   * Whether to disable webhook calls for this request.
   *
   * @generated from field: bool disable_webhook = 7;
   */
  disableWebhook: boolean;

  /**
   * Configures whether sentiment analysis should be performed. If not
   * provided, sentiment analysis is not performed.
   *
   * @generated from field: bool analyze_query_text_sentiment = 8;
   */
  analyzeQueryTextSentiment: boolean;

  /**
   * This field can be used to pass HTTP headers for a webhook
   * call. These headers will be sent to webhook along with the headers that
   * have been configured through Dialogflow web console. The headers defined
   * within this field will overwrite the headers configured through Dialogflow
   * console if there is a conflict. Header names are case-insensitive.
   * Google's specified headers are not allowed. Including: "Host",
   * "Content-Length", "Connection", "From", "User-Agent", "Accept-Encoding",
   * "If-Modified-Since", "If-None-Match", "X-Forwarded-For", etc.
   *
   * @generated from field: map<string, string> webhook_headers = 10;
   */
  webhookHeaders: { [key: string]: string };

  /**
   * A list of flow versions to override for the request.
   * Format: `projects/<Project ID>/locations/<Location ID>/agents/<Agent
   * ID>/flows/<Flow ID>/versions/<Version ID>`.
   *
   * If version 1 of flow X is included in this list, the traffic of
   * flow X will go through version 1 regardless of the version configuration in
   * the environment. Each flow can have at most one version specified in this
   * list.
   *
   * @generated from field: repeated string flow_versions = 14;
   */
  flowVersions: string[];

  /**
   * Optional. Start the session with the specified
   * [playbook][google.cloud.dialogflow.cx.v3beta1.Playbook]. You can only
   * specify the playbook at the beginning of the session. Otherwise, an error
   * will be thrown.
   *
   * Format: `projects/<Project ID>/locations/<Location ID>/agents/<Agent
   * ID>/playbooks/<Playbook ID>`.
   *
   * @generated from field: string current_playbook = 19;
   */
  currentPlaybook: string;

  /**
   * Optional. Use the specified LLM model settings for processing the request.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.LlmModelSettings llm_model_settings = 21;
   */
  llmModelSettings?: LlmModelSettings;

  /**
   * The channel which this query is for.
   *
   * If specified, only the
   * [ResponseMessage][google.cloud.dialogflow.cx.v3beta1.ResponseMessage]
   * associated with the channel will be returned. If no
   * [ResponseMessage][google.cloud.dialogflow.cx.v3beta1.ResponseMessage] is
   * associated with the channel, it falls back to the
   * [ResponseMessage][google.cloud.dialogflow.cx.v3beta1.ResponseMessage] with
   * unspecified channel.
   *
   * If unspecified, the
   * [ResponseMessage][google.cloud.dialogflow.cx.v3beta1.ResponseMessage] with
   * unspecified channel will be returned.
   *
   * @generated from field: string channel = 15;
   */
  channel: string;

  /**
   * Optional. Configure lifetime of the Dialogflow session.
   * By default, a Dialogflow session remains active and its data is stored for
   * 30 minutes after the last request is sent for the session.
   * This value should be no longer than 1 day.
   *
   * @generated from field: google.protobuf.Duration session_ttl = 16;
   */
  sessionTtl?: Duration;

  /**
   * Optional. Information about the end-user to improve the relevance and
   * accuracy of generative answers.
   *
   * This will be interpreted and used by a language model, so, for good
   * results, the data should be self-descriptive, and in a simple structure.
   *
   * Example:
   *
   * ```json
   * {
   *   "subscription plan": "Business Premium Plus",
   *   "devices owned": [
   *     {"model": "Google Pixel 7"},
   *     {"model": "Google Pixel Tablet"}
   *   ]
   * }
   * ```
   *
   * @generated from field: google.protobuf.Struct end_user_metadata = 18;
   */
  endUserMetadata?: JsonObject;

  /**
   * Optional. Search configuration for UCS search queries.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.SearchConfig search_config = 20;
   */
  searchConfig?: SearchConfig;

  /**
   * Optional. If set to true and data stores are involved in serving the
   * request then
   * DetectIntentResponse.query_result.data_store_connection_signals
   * will be filled with data that can help evaluations.
   *
   * @generated from field: bool populate_data_store_connection_signals = 25;
   */
  populateDataStoreConnectionSignals: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.QueryParameters.
 * Use `create(QueryParametersSchema)` to create a new message.
 */
export const QueryParametersSchema: GenMessage<QueryParameters> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 8);

/**
 * Search configuration for UCS search queries.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.SearchConfig
 */
export type SearchConfig = Message<"google.cloud.dialogflow.cx.v3beta1.SearchConfig"> & {
  /**
   * Optional. Boosting configuration for the datastores.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3beta1.BoostSpecs boost_specs = 1;
   */
  boostSpecs: BoostSpecs[];

  /**
   * Optional. Filter configuration for the datastores.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3beta1.FilterSpecs filter_specs = 2;
   */
  filterSpecs: FilterSpecs[];
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.SearchConfig.
 * Use `create(SearchConfigSchema)` to create a new message.
 */
export const SearchConfigSchema: GenMessage<SearchConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 9);

/**
 * Boost specification to boost certain documents.
 * A copy of google.cloud.discoveryengine.v1main.BoostSpec, field documentation
 * is available at
 * https://cloud.google.com/generative-ai-app-builder/docs/reference/rest/v1alpha/BoostSpec
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.BoostSpec
 */
export type BoostSpec = Message<"google.cloud.dialogflow.cx.v3beta1.BoostSpec"> & {
  /**
   * Optional. Condition boost specifications. If a document matches multiple
   * conditions in the specifictions, boost scores from these specifications are
   * all applied and combined in a non-linear way. Maximum number of
   * specifications is 20.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3beta1.BoostSpec.ConditionBoostSpec condition_boost_specs = 1;
   */
  conditionBoostSpecs: BoostSpec_ConditionBoostSpec[];
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.BoostSpec.
 * Use `create(BoostSpecSchema)` to create a new message.
 */
export const BoostSpecSchema: GenMessage<BoostSpec> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 10);

/**
 * Boost applies to documents which match a condition.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.BoostSpec.ConditionBoostSpec
 */
export type BoostSpec_ConditionBoostSpec = Message<"google.cloud.dialogflow.cx.v3beta1.BoostSpec.ConditionBoostSpec"> & {
  /**
   * Optional. An expression which specifies a boost condition. The syntax and
   * supported fields are the same as a filter expression.
   * Examples:
   *
   * * To boost documents with document ID "doc_1" or "doc_2", and
   * color
   *   "Red" or "Blue":
   *     * (id: ANY("doc_1", "doc_2")) AND (color: ANY("Red","Blue"))
   *
   * @generated from field: string condition = 1;
   */
  condition: string;

  /**
   * Optional. Strength of the condition boost, which should be in [-1, 1].
   * Negative boost means demotion. Default is 0.0.
   *
   * Setting to 1.0 gives the document a big promotion. However, it does not
   * necessarily mean that the boosted document will be the top result at
   * all times, nor that other documents will be excluded. Results could
   * still be shown even when none of them matches the condition. And
   * results that are significantly more relevant to the search query can
   * still trump your heavily favored but irrelevant documents.
   *
   * Setting to -1.0 gives the document a big demotion. However, results
   * that are deeply relevant might still be shown. The document will have
   * an upstream battle to get a fairly high ranking, but it is not blocked
   * out completely.
   *
   * Setting to 0.0 means no boost applied. The boosting condition is
   * ignored.
   *
   * @generated from field: float boost = 2;
   */
  boost: number;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.BoostSpec.ConditionBoostSpec.
 * Use `create(BoostSpec_ConditionBoostSpecSchema)` to create a new message.
 */
export const BoostSpec_ConditionBoostSpecSchema: GenMessage<BoostSpec_ConditionBoostSpec> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 10, 0);

/**
 * Boost specifications for data stores.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.BoostSpecs
 */
export type BoostSpecs = Message<"google.cloud.dialogflow.cx.v3beta1.BoostSpecs"> & {
  /**
   * Optional. Data Stores where the boosting configuration is applied. The full
   * names of the referenced data stores. Formats:
   * `projects/{project}/locations/{location}/collections/{collection}/dataStores/{data_store}`
   * `projects/{project}/locations/{location}/dataStores/{data_store}
   *
   * @generated from field: repeated string data_stores = 1;
   */
  dataStores: string[];

  /**
   * Optional. A list of boosting specifications.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3beta1.BoostSpec spec = 2;
   */
  spec: BoostSpec[];
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.BoostSpecs.
 * Use `create(BoostSpecsSchema)` to create a new message.
 */
export const BoostSpecsSchema: GenMessage<BoostSpecs> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 11);

/**
 * Filter specifications for data stores.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.FilterSpecs
 */
export type FilterSpecs = Message<"google.cloud.dialogflow.cx.v3beta1.FilterSpecs"> & {
  /**
   * Optional. Data Stores where the boosting configuration is applied. The full
   * names of the referenced data stores. Formats:
   * `projects/{project}/locations/{location}/collections/{collection}/dataStores/{data_store}`
   * `projects/{project}/locations/{location}/dataStores/{data_store}
   *
   * @generated from field: repeated string data_stores = 1;
   */
  dataStores: string[];

  /**
   * Optional. The filter expression to be applied.
   * Expression syntax is documented at
   * https://cloud.google.com/generative-ai-app-builder/docs/filter-search-metadata#filter-expression-syntax
   *
   * @generated from field: string filter = 2;
   */
  filter: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.FilterSpecs.
 * Use `create(FilterSpecsSchema)` to create a new message.
 */
export const FilterSpecsSchema: GenMessage<FilterSpecs> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 12);

/**
 * Represents the query input. It can contain one of:
 *
 * 1. A conversational query in the form of text.
 *
 * 2. An intent query that specifies which intent to trigger.
 *
 * 3. Natural language speech audio to be processed.
 *
 * 4. An event to be triggered.
 *
 * 5. DTMF digits to invoke an intent and fill in parameter value.
 *
 * 6. The results of a tool executed by the client.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.QueryInput
 */
export type QueryInput = Message<"google.cloud.dialogflow.cx.v3beta1.QueryInput"> & {
  /**
   * Required. The input specification.
   *
   * @generated from oneof google.cloud.dialogflow.cx.v3beta1.QueryInput.input
   */
  input: {
    /**
     * The natural language text to be processed.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.TextInput text = 2;
     */
    value: TextInput;
    case: "text";
  } | {
    /**
     * The intent to be triggered.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.IntentInput intent = 3;
     */
    value: IntentInput;
    case: "intent";
  } | {
    /**
     * The natural language speech audio to be processed.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.AudioInput audio = 5;
     */
    value: AudioInput;
    case: "audio";
  } | {
    /**
     * The event to be triggered.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.EventInput event = 6;
     */
    value: EventInput;
    case: "event";
  } | {
    /**
     * The DTMF event to be handled.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.DtmfInput dtmf = 7;
     */
    value: DtmfInput;
    case: "dtmf";
  } | {
    /**
     * The results of a tool executed by the client.
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.ToolCallResult tool_call_result = 11;
     */
    value: ToolCallResult;
    case: "toolCallResult";
  } | { case: undefined; value?: undefined };

  /**
   * Required. The language of the input. See [Language
   * Support](https://cloud.google.com/dialogflow/cx/docs/reference/language)
   * for a list of the currently supported language codes. Note that queries in
   * the same session do not necessarily need to specify the same language.
   *
   * @generated from field: string language_code = 4;
   */
  languageCode: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.QueryInput.
 * Use `create(QueryInputSchema)` to create a new message.
 */
export const QueryInputSchema: GenMessage<QueryInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 13);

/**
 * Represents the information of a query if handled by generative agent
 * resources.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.GenerativeInfo
 */
export type GenerativeInfo = Message<"google.cloud.dialogflow.cx.v3beta1.GenerativeInfo"> & {
  /**
   * The stack of [playbooks][google.cloud.dialogflow.cx.v3beta1.Playbook] that
   * the conversation has currently entered, with the most recent one on the
   * top.
   *
   * @generated from field: repeated string current_playbooks = 1;
   */
  currentPlaybooks: string[];

  /**
   * The actions performed by the generative playbook for the current agent
   * response.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.Example action_tracing_info = 2;
   */
  actionTracingInfo?: Example;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.GenerativeInfo.
 * Use `create(GenerativeInfoSchema)` to create a new message.
 */
export const GenerativeInfoSchema: GenMessage<GenerativeInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 14);

/**
 * Represents the result of a conversational query.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.QueryResult
 */
export type QueryResult = Message<"google.cloud.dialogflow.cx.v3beta1.QueryResult"> & {
  /**
   * The original conversational query.
   *
   * @generated from oneof google.cloud.dialogflow.cx.v3beta1.QueryResult.query
   */
  query: {
    /**
     * If [natural language text][google.cloud.dialogflow.cx.v3beta1.TextInput]
     * was provided as input, this field will contain a copy of the text.
     *
     * @generated from field: string text = 1;
     */
    value: string;
    case: "text";
  } | {
    /**
     * If an [intent][google.cloud.dialogflow.cx.v3beta1.IntentInput] was
     * provided as input, this field will contain a copy of the intent
     * identifier. Format: `projects/<Project ID>/locations/<Location
     * ID>/agents/<Agent ID>/intents/<Intent ID>`.
     *
     * @generated from field: string trigger_intent = 11;
     */
    value: string;
    case: "triggerIntent";
  } | {
    /**
     * If [natural language speech
     * audio][google.cloud.dialogflow.cx.v3beta1.AudioInput] was provided as
     * input, this field will contain the transcript for the audio.
     *
     * @generated from field: string transcript = 12;
     */
    value: string;
    case: "transcript";
  } | {
    /**
     * If an [event][google.cloud.dialogflow.cx.v3beta1.EventInput] was provided
     * as input, this field will contain the name of the event.
     *
     * @generated from field: string trigger_event = 14;
     */
    value: string;
    case: "triggerEvent";
  } | {
    /**
     * If a [DTMF][google.cloud.dialogflow.cx.v3beta1.DtmfInput] was provided as
     * input, this field will contain a copy of the
     * [DtmfInput][google.cloud.dialogflow.cx.v3beta1.DtmfInput].
     *
     * @generated from field: google.cloud.dialogflow.cx.v3beta1.DtmfInput dtmf = 23;
     */
    value: DtmfInput;
    case: "dtmf";
  } | { case: undefined; value?: undefined };

  /**
   * The language that was triggered during intent detection.
   * See [Language
   * Support](https://cloud.google.com/dialogflow/cx/docs/reference/language)
   * for a list of the currently supported language codes.
   *
   * @generated from field: string language_code = 2;
   */
  languageCode: string;

  /**
   * The collected [session
   * parameters][google.cloud.dialogflow.cx.v3beta1.SessionInfo.parameters].
   *
   * Depending on your protocol or client library language, this is a
   * map, associative array, symbol table, dictionary, or JSON object
   * composed of a collection of (MapKey, MapValue) pairs:
   *
   * * MapKey type: string
   * * MapKey value: parameter name
   * * MapValue type: If parameter's entity type is a composite entity then use
   * map, otherwise, depending on the parameter value type, it could be one of
   * string, number, boolean, null, list or map.
   * * MapValue value: If parameter's entity type is a composite entity then use
   * map from composite entity property names to property values, otherwise,
   * use parameter value.
   *
   * @generated from field: google.protobuf.Struct parameters = 3;
   */
  parameters?: JsonObject;

  /**
   * The list of rich messages returned to the client. Responses vary from
   * simple text messages to more sophisticated, structured payloads used
   * to drive complex logic.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3beta1.ResponseMessage response_messages = 4;
   */
  responseMessages: ResponseMessage[];

  /**
   * The list of webhook ids in the order of call sequence.
   *
   * @generated from field: repeated string webhook_ids = 25;
   */
  webhookIds: string[];

  /**
   * The list of webhook display names in the order of call sequence.
   *
   * @generated from field: repeated string webhook_display_names = 26;
   */
  webhookDisplayNames: string[];

  /**
   * The list of webhook latencies in the order of call sequence.
   *
   * @generated from field: repeated google.protobuf.Duration webhook_latencies = 27;
   */
  webhookLatencies: Duration[];

  /**
   * The list of webhook tags in the order of call sequence.
   *
   * @generated from field: repeated string webhook_tags = 29;
   */
  webhookTags: string[];

  /**
   * The list of webhook call status in the order of call sequence.
   *
   * @generated from field: repeated google.rpc.Status webhook_statuses = 13;
   */
  webhookStatuses: Status[];

  /**
   * The list of webhook payload in
   * [WebhookResponse.payload][google.cloud.dialogflow.cx.v3beta1.WebhookResponse.payload],
   * in the order of call sequence. If some webhook call fails or doesn't return
   * any payload, an empty `Struct` would be used instead.
   *
   * @generated from field: repeated google.protobuf.Struct webhook_payloads = 6;
   */
  webhookPayloads: JsonObject[];

  /**
   * The current [Page][google.cloud.dialogflow.cx.v3beta1.Page]. Some, not all
   * fields are filled in this message, including but not limited to `name` and
   * `display_name`.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.Page current_page = 7;
   */
  currentPage?: Page;

  /**
   * The current [Flow][google.cloud.dialogflow.cx.v3beta1.Flow]. Some, not all
   * fields are filled in this message, including but not limited to `name` and
   * `display_name`.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.Flow current_flow = 31;
   */
  currentFlow?: Flow;

  /**
   * The [Intent][google.cloud.dialogflow.cx.v3beta1.Intent] that matched the
   * conversational query. Some, not all fields are filled in this message,
   * including but not limited to: `name` and `display_name`. This field is
   * deprecated, please use
   * [QueryResult.match][google.cloud.dialogflow.cx.v3beta1.QueryResult.match]
   * instead.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.Intent intent = 8 [deprecated = true];
   * @deprecated
   */
  intent?: Intent;

  /**
   * The intent detection confidence. Values range from 0.0 (completely
   * uncertain) to 1.0 (completely certain).
   * This value is for informational purpose only and is only used to
   * help match the best intent within the classification threshold.
   * This value may change for the same end-user expression at any time due to a
   * model retraining or change in implementation.
   * This field is deprecated, please use
   * [QueryResult.match][google.cloud.dialogflow.cx.v3beta1.QueryResult.match]
   * instead.
   *
   * @generated from field: float intent_detection_confidence = 9 [deprecated = true];
   * @deprecated
   */
  intentDetectionConfidence: number;

  /**
   * Intent match result, could be an intent or an event.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.Match match = 15;
   */
  match?: Match;

  /**
   * The free-form diagnostic info. For example, this field could contain
   * webhook call latency. The fields of this data can change without notice,
   * so you should not write code that depends on its structure.
   *
   * One of the fields is called "Alternative Matched Intents", which may
   * aid with debugging. The following describes these intent results:
   *
   * - The list is empty if no intent was matched to end-user input.
   * - Only intents that are referenced in the currently active flow are
   *   included.
   * - The matched intent is included.
   * - Other intents that could have matched end-user input, but did not match
   *   because they are referenced by intent routes that are out of
   *   [scope](https://cloud.google.com/dialogflow/cx/docs/concept/handler#scope),
   *   are included.
   * - Other intents referenced by intent routes in scope that matched end-user
   *   input, but had a lower confidence score.
   *
   * @generated from field: google.protobuf.Struct diagnostic_info = 10;
   */
  diagnosticInfo?: JsonObject;

  /**
   * The information of a query if handled by generative agent resources.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.GenerativeInfo generative_info = 33;
   */
  generativeInfo?: GenerativeInfo;

  /**
   * The sentiment analyss result, which depends on
   * [`analyze_query_text_sentiment`]
   * [google.cloud.dialogflow.cx.v3beta1.QueryParameters.analyze_query_text_sentiment],
   * specified in the request.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.SentimentAnalysisResult sentiment_analysis_result = 17;
   */
  sentimentAnalysisResult?: SentimentAnalysisResult;

  /**
   * Returns the current advanced settings including IVR settings. Even though
   * the operations configured by these settings are performed by Dialogflow,
   * the client may need to perform special logic at the moment. For example, if
   * Dialogflow exports audio to Google Cloud Storage, then the client may need
   * to wait for the resulting object to appear in the bucket before proceeding.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.AdvancedSettings advanced_settings = 21;
   */
  advancedSettings?: AdvancedSettings;

  /**
   * Indicates whether the Thumbs up/Thumbs down rating controls are need to be
   * shown for the response in the Dialogflow Messenger widget.
   *
   * @generated from field: bool allow_answer_feedback = 32;
   */
  allowAnswerFeedback: boolean;

  /**
   * Optional. Data store connection feature output signals.
   * Filled only when data stores are involved in serving the query and
   * DetectIntentRequest.populate data_store_connection_quality_signals is set
   * to true in the request.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.DataStoreConnectionSignals data_store_connection_signals = 35;
   */
  dataStoreConnectionSignals?: DataStoreConnectionSignals;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.QueryResult.
 * Use `create(QueryResultSchema)` to create a new message.
 */
export const QueryResultSchema: GenMessage<QueryResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 15);

/**
 * Represents the natural language text to be processed.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.TextInput
 */
export type TextInput = Message<"google.cloud.dialogflow.cx.v3beta1.TextInput"> & {
  /**
   * Required. The UTF-8 encoded natural language text to be processed.
   *
   * @generated from field: string text = 1;
   */
  text: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.TextInput.
 * Use `create(TextInputSchema)` to create a new message.
 */
export const TextInputSchema: GenMessage<TextInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 16);

/**
 * Represents the intent to trigger programmatically rather than as a result of
 * natural language processing.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.IntentInput
 */
export type IntentInput = Message<"google.cloud.dialogflow.cx.v3beta1.IntentInput"> & {
  /**
   * Required. The unique identifier of the intent.
   * Format: `projects/<Project ID>/locations/<Location ID>/agents/<Agent
   * ID>/intents/<Intent ID>`.
   *
   * @generated from field: string intent = 1;
   */
  intent: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.IntentInput.
 * Use `create(IntentInputSchema)` to create a new message.
 */
export const IntentInputSchema: GenMessage<IntentInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 17);

/**
 * Represents the natural speech audio to be processed.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.AudioInput
 */
export type AudioInput = Message<"google.cloud.dialogflow.cx.v3beta1.AudioInput"> & {
  /**
   * Required. Instructs the speech recognizer how to process the speech audio.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.InputAudioConfig config = 1;
   */
  config?: InputAudioConfig;

  /**
   * The natural language speech audio to be processed.
   * A single request can contain up to 2 minutes of speech audio data.
   * The [transcribed
   * text][google.cloud.dialogflow.cx.v3beta1.QueryResult.transcript] cannot
   * contain more than 256 bytes.
   *
   * For non-streaming audio detect intent, both `config` and `audio` must be
   * provided.
   * For streaming audio detect intent, `config` must be provided in
   * the first request and `audio` must be provided in all following requests.
   *
   * @generated from field: bytes audio = 2;
   */
  audio: Uint8Array;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.AudioInput.
 * Use `create(AudioInputSchema)` to create a new message.
 */
export const AudioInputSchema: GenMessage<AudioInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 18);

/**
 * Represents the event to trigger.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.EventInput
 */
export type EventInput = Message<"google.cloud.dialogflow.cx.v3beta1.EventInput"> & {
  /**
   * Name of the event.
   *
   * @generated from field: string event = 1;
   */
  event: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.EventInput.
 * Use `create(EventInputSchema)` to create a new message.
 */
export const EventInputSchema: GenMessage<EventInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 19);

/**
 * Represents the input for dtmf event.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.DtmfInput
 */
export type DtmfInput = Message<"google.cloud.dialogflow.cx.v3beta1.DtmfInput"> & {
  /**
   * The dtmf digits.
   *
   * @generated from field: string digits = 1;
   */
  digits: string;

  /**
   * The finish digit (if any).
   *
   * @generated from field: string finish_digit = 2;
   */
  finishDigit: string;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.DtmfInput.
 * Use `create(DtmfInputSchema)` to create a new message.
 */
export const DtmfInputSchema: GenMessage<DtmfInput> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 20);

/**
 * Represents one match result of [MatchIntent][].
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.Match
 */
export type Match = Message<"google.cloud.dialogflow.cx.v3beta1.Match"> & {
  /**
   * The [Intent][google.cloud.dialogflow.cx.v3beta1.Intent] that matched the
   * query. Some, not all fields are filled in this message, including but not
   * limited to: `name` and `display_name`. Only filled for
   * [`INTENT`][google.cloud.dialogflow.cx.v3beta1.Match.MatchType] match type.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.Intent intent = 1;
   */
  intent?: Intent;

  /**
   * The event that matched the query. Filled for
   * [`EVENT`][google.cloud.dialogflow.cx.v3beta1.Match.MatchType],
   * [`NO_MATCH`][google.cloud.dialogflow.cx.v3beta1.Match.MatchType] and
   * [`NO_INPUT`][google.cloud.dialogflow.cx.v3beta1.Match.MatchType] match
   * types.
   *
   * @generated from field: string event = 6;
   */
  event: string;

  /**
   * The collection of parameters extracted from the query.
   *
   * Depending on your protocol or client library language, this is a
   * map, associative array, symbol table, dictionary, or JSON object
   * composed of a collection of (MapKey, MapValue) pairs:
   *
   * * MapKey type: string
   * * MapKey value: parameter name
   * * MapValue type: If parameter's entity type is a composite entity then use
   * map, otherwise, depending on the parameter value type, it could be one of
   * string, number, boolean, null, list or map.
   * * MapValue value: If parameter's entity type is a composite entity then use
   * map from composite entity property names to property values, otherwise,
   * use parameter value.
   *
   * @generated from field: google.protobuf.Struct parameters = 2;
   */
  parameters?: JsonObject;

  /**
   * Final text input which was matched during MatchIntent. This value can be
   * different from original input sent in request because of spelling
   * correction or other processing.
   *
   * @generated from field: string resolved_input = 3;
   */
  resolvedInput: string;

  /**
   * Type of this [Match][google.cloud.dialogflow.cx.v3beta1.Match].
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.Match.MatchType match_type = 4;
   */
  matchType: Match_MatchType;

  /**
   * The confidence of this match. Values range from 0.0 (completely uncertain)
   * to 1.0 (completely certain).
   * This value is for informational purpose only and is only used to help match
   * the best intent within the classification threshold. This value may change
   * for the same end-user expression at any time due to a model retraining or
   * change in implementation.
   *
   * @generated from field: float confidence = 5;
   */
  confidence: number;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.Match.
 * Use `create(MatchSchema)` to create a new message.
 */
export const MatchSchema: GenMessage<Match> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 21);

/**
 * Type of a Match.
 *
 * @generated from enum google.cloud.dialogflow.cx.v3beta1.Match.MatchType
 */
export enum Match_MatchType {
  /**
   * Not specified. Should never be used.
   *
   * @generated from enum value: MATCH_TYPE_UNSPECIFIED = 0;
   */
  MATCH_TYPE_UNSPECIFIED = 0,

  /**
   * The query was matched to an intent.
   *
   * @generated from enum value: INTENT = 1;
   */
  INTENT = 1,

  /**
   * The query directly triggered an intent.
   *
   * @generated from enum value: DIRECT_INTENT = 2;
   */
  DIRECT_INTENT = 2,

  /**
   * The query was used for parameter filling.
   *
   * @generated from enum value: PARAMETER_FILLING = 3;
   */
  PARAMETER_FILLING = 3,

  /**
   * No match was found for the query.
   *
   * @generated from enum value: NO_MATCH = 4;
   */
  NO_MATCH = 4,

  /**
   * Indicates an empty query.
   *
   * @generated from enum value: NO_INPUT = 5;
   */
  NO_INPUT = 5,

  /**
   * The query directly triggered an event.
   *
   * @generated from enum value: EVENT = 6;
   */
  EVENT = 6,

  /**
   * The query was matched to a Knowledge Connector answer.
   *
   * @generated from enum value: KNOWLEDGE_CONNECTOR = 8;
   */
  KNOWLEDGE_CONNECTOR = 8,

  /**
   * The query was handled by a
   * [`Playbook`][google.cloud.dialogflow.cx.v3beta1.Playbook].
   *
   * @generated from enum value: PLAYBOOK = 9;
   */
  PLAYBOOK = 9,
}

/**
 * Describes the enum google.cloud.dialogflow.cx.v3beta1.Match.MatchType.
 */
export const Match_MatchTypeSchema: GenEnum<Match_MatchType> = /*@__PURE__*/
  enumDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 21, 0);

/**
 * Request of [MatchIntent][].
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.MatchIntentRequest
 */
export type MatchIntentRequest = Message<"google.cloud.dialogflow.cx.v3beta1.MatchIntentRequest"> & {
  /**
   * Required. The name of the session this query is sent to.
   * Format: `projects/<Project ID>/locations/<Location ID>/agents/<Agent
   * ID>/sessions/<Session ID>` or `projects/<Project ID>/locations/<Location
   * ID>/agents/<Agent ID>/environments/<Environment ID>/sessions/<Session ID>`.
   * If `Environment ID` is not specified, we assume default 'draft'
   * environment.
   * It's up to the API caller to choose an appropriate `Session ID`. It can be
   * a random number or some type of session identifiers (preferably hashed).
   * The length of the `Session ID` must not exceed 36 characters.
   *
   * For more information, see the [sessions
   * guide](https://cloud.google.com/dialogflow/cx/docs/concept/session).
   *
   * @generated from field: string session = 1;
   */
  session: string;

  /**
   * The parameters of this query.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.QueryParameters query_params = 2;
   */
  queryParams?: QueryParameters;

  /**
   * Required. The input specification.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.QueryInput query_input = 3;
   */
  queryInput?: QueryInput;

  /**
   * Persist session parameter changes from `query_params`.
   *
   * @generated from field: bool persist_parameter_changes = 5;
   */
  persistParameterChanges: boolean;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.MatchIntentRequest.
 * Use `create(MatchIntentRequestSchema)` to create a new message.
 */
export const MatchIntentRequestSchema: GenMessage<MatchIntentRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 22);

/**
 * Response of [MatchIntent][].
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.MatchIntentResponse
 */
export type MatchIntentResponse = Message<"google.cloud.dialogflow.cx.v3beta1.MatchIntentResponse"> & {
  /**
   * The original conversational query.
   *
   * @generated from oneof google.cloud.dialogflow.cx.v3beta1.MatchIntentResponse.query
   */
  query: {
    /**
     * If [natural language text][google.cloud.dialogflow.cx.v3beta1.TextInput]
     * was provided as input, this field will contain a copy of the text.
     *
     * @generated from field: string text = 1;
     */
    value: string;
    case: "text";
  } | {
    /**
     * If an [intent][google.cloud.dialogflow.cx.v3beta1.IntentInput] was
     * provided as input, this field will contain a copy of the intent
     * identifier. Format: `projects/<Project ID>/locations/<Location
     * ID>/agents/<Agent ID>/intents/<Intent ID>`.
     *
     * @generated from field: string trigger_intent = 2;
     */
    value: string;
    case: "triggerIntent";
  } | {
    /**
     * If [natural language speech
     * audio][google.cloud.dialogflow.cx.v3beta1.AudioInput] was provided as
     * input, this field will contain the transcript for the audio.
     *
     * @generated from field: string transcript = 3;
     */
    value: string;
    case: "transcript";
  } | {
    /**
     * If an [event][google.cloud.dialogflow.cx.v3beta1.EventInput] was provided
     * as input, this field will contain a copy of the event name.
     *
     * @generated from field: string trigger_event = 6;
     */
    value: string;
    case: "triggerEvent";
  } | { case: undefined; value?: undefined };

  /**
   * Match results, if more than one, ordered descendingly by the confidence
   * we have that the particular intent matches the query.
   *
   * @generated from field: repeated google.cloud.dialogflow.cx.v3beta1.Match matches = 4;
   */
  matches: Match[];

  /**
   * The current [Page][google.cloud.dialogflow.cx.v3beta1.Page]. Some, not all
   * fields are filled in this message, including but not limited to `name` and
   * `display_name`.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.Page current_page = 5;
   */
  currentPage?: Page;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.MatchIntentResponse.
 * Use `create(MatchIntentResponseSchema)` to create a new message.
 */
export const MatchIntentResponseSchema: GenMessage<MatchIntentResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 23);

/**
 * Request of [FulfillIntent][]
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.FulfillIntentRequest
 */
export type FulfillIntentRequest = Message<"google.cloud.dialogflow.cx.v3beta1.FulfillIntentRequest"> & {
  /**
   * Must be same as the corresponding MatchIntent request, otherwise the
   * behavior is undefined.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.MatchIntentRequest match_intent_request = 1;
   */
  matchIntentRequest?: MatchIntentRequest;

  /**
   * The matched intent/event to fulfill.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.Match match = 2;
   */
  match?: Match;

  /**
   * Instructs the speech synthesizer how to generate output audio.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.OutputAudioConfig output_audio_config = 3;
   */
  outputAudioConfig?: OutputAudioConfig;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.FulfillIntentRequest.
 * Use `create(FulfillIntentRequestSchema)` to create a new message.
 */
export const FulfillIntentRequestSchema: GenMessage<FulfillIntentRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 24);

/**
 * Response of [FulfillIntent][]
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.FulfillIntentResponse
 */
export type FulfillIntentResponse = Message<"google.cloud.dialogflow.cx.v3beta1.FulfillIntentResponse"> & {
  /**
   * Output only. The unique identifier of the response. It can be used to
   * locate a response in the training example set or for reporting issues.
   *
   * @generated from field: string response_id = 1;
   */
  responseId: string;

  /**
   * The result of the conversational query.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.QueryResult query_result = 2;
   */
  queryResult?: QueryResult;

  /**
   * The audio data bytes encoded as specified in the request.
   * Note: The output audio is generated based on the values of default platform
   * text responses found in the
   * [`query_result.response_messages`][google.cloud.dialogflow.cx.v3beta1.QueryResult.response_messages]
   * field. If multiple default text responses exist, they will be concatenated
   * when generating audio. If no default platform text responses exist, the
   * generated audio content will be empty.
   *
   * In some scenarios, multiple output audio fields may be present in the
   * response structure. In these cases, only the top-most-level audio output
   * has content.
   *
   * @generated from field: bytes output_audio = 3;
   */
  outputAudio: Uint8Array;

  /**
   * The config used by the speech synthesizer to generate the output audio.
   *
   * @generated from field: google.cloud.dialogflow.cx.v3beta1.OutputAudioConfig output_audio_config = 4;
   */
  outputAudioConfig?: OutputAudioConfig;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.FulfillIntentResponse.
 * Use `create(FulfillIntentResponseSchema)` to create a new message.
 */
export const FulfillIntentResponseSchema: GenMessage<FulfillIntentResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 25);

/**
 * The result of sentiment analysis. Sentiment analysis inspects user input
 * and identifies the prevailing subjective opinion, especially to determine a
 * user's attitude as positive, negative, or neutral.
 *
 * @generated from message google.cloud.dialogflow.cx.v3beta1.SentimentAnalysisResult
 */
export type SentimentAnalysisResult = Message<"google.cloud.dialogflow.cx.v3beta1.SentimentAnalysisResult"> & {
  /**
   * Sentiment score between -1.0 (negative sentiment) and 1.0 (positive
   * sentiment).
   *
   * @generated from field: float score = 1;
   */
  score: number;

  /**
   * A non-negative number in the [0, +inf) range, which represents the absolute
   * magnitude of sentiment, regardless of score (positive or negative).
   *
   * @generated from field: float magnitude = 2;
   */
  magnitude: number;
};

/**
 * Describes the message google.cloud.dialogflow.cx.v3beta1.SentimentAnalysisResult.
 * Use `create(SentimentAnalysisResultSchema)` to create a new message.
 */
export const SentimentAnalysisResultSchema: GenMessage<SentimentAnalysisResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 26);

/**
 * A session represents an interaction with a user. You retrieve user input
 * and pass it to the
 * [DetectIntent][google.cloud.dialogflow.cx.v3beta1.Sessions.DetectIntent]
 * method to determine user intent and respond.
 *
 * @generated from service google.cloud.dialogflow.cx.v3beta1.Sessions
 */
export const Sessions: GenService<{
  /**
   * Processes a natural language query and returns structured, actionable data
   * as a result. This method is not idempotent, because it may cause session
   * entity types to be updated, which in turn might affect results of future
   * queries.
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/cx/docs/concept/version).
   *
   * @generated from rpc google.cloud.dialogflow.cx.v3beta1.Sessions.DetectIntent
   */
  detectIntent: {
    methodKind: "unary";
    input: typeof DetectIntentRequestSchema;
    output: typeof DetectIntentResponseSchema;
  },
  /**
   * Processes a natural language query and returns structured, actionable data
   * as a result through server-side streaming. Server-side streaming allows
   * Dialogflow to send [partial
   * responses](https://cloud.google.com/dialogflow/cx/docs/concept/fulfillment#partial-response)
   * earlier in a single request.
   *
   * @generated from rpc google.cloud.dialogflow.cx.v3beta1.Sessions.ServerStreamingDetectIntent
   */
  serverStreamingDetectIntent: {
    methodKind: "server_streaming";
    input: typeof DetectIntentRequestSchema;
    output: typeof DetectIntentResponseSchema;
  },
  /**
   * Processes a natural language query in audio format in a streaming fashion
   * and returns structured, actionable data as a result. This method is only
   * available via the gRPC API (not REST).
   *
   * Note: Always use agent versions for production traffic.
   * See [Versions and
   * environments](https://cloud.google.com/dialogflow/cx/docs/concept/version).
   *
   * @generated from rpc google.cloud.dialogflow.cx.v3beta1.Sessions.StreamingDetectIntent
   */
  streamingDetectIntent: {
    methodKind: "bidi_streaming";
    input: typeof StreamingDetectIntentRequestSchema;
    output: typeof StreamingDetectIntentResponseSchema;
  },
  /**
   * Returns preliminary intent match results, doesn't change the session
   * status.
   *
   * @generated from rpc google.cloud.dialogflow.cx.v3beta1.Sessions.MatchIntent
   */
  matchIntent: {
    methodKind: "unary";
    input: typeof MatchIntentRequestSchema;
    output: typeof MatchIntentResponseSchema;
  },
  /**
   * Fulfills a matched intent returned by
   * [MatchIntent][google.cloud.dialogflow.cx.v3beta1.Sessions.MatchIntent].
   * Must be called after
   * [MatchIntent][google.cloud.dialogflow.cx.v3beta1.Sessions.MatchIntent],
   * with input from
   * [MatchIntentResponse][google.cloud.dialogflow.cx.v3beta1.MatchIntentResponse].
   * Otherwise, the behavior is undefined.
   *
   * @generated from rpc google.cloud.dialogflow.cx.v3beta1.Sessions.FulfillIntent
   */
  fulfillIntent: {
    methodKind: "unary";
    input: typeof FulfillIntentRequestSchema;
    output: typeof FulfillIntentResponseSchema;
  },
  /**
   * Updates the feedback received from the user for a single turn of the bot
   * response.
   *
   * @generated from rpc google.cloud.dialogflow.cx.v3beta1.Sessions.SubmitAnswerFeedback
   */
  submitAnswerFeedback: {
    methodKind: "unary";
    input: typeof SubmitAnswerFeedbackRequestSchema;
    output: typeof AnswerFeedbackSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_dialogflow_cx_v3beta1_session, 0);

