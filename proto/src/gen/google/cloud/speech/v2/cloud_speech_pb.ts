// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.1.0 with parameter "target=ts"
// @generated from file google/cloud/speech/v2/cloud_speech.proto (package google.cloud.speech.v2, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import { file_google_api_annotations } from "../../../api/annotations_pb";
import { file_google_api_client } from "../../../api/client_pb";
import { file_google_api_field_behavior } from "../../../api/field_behavior_pb";
import { file_google_api_field_info } from "../../../api/field_info_pb";
import { file_google_api_resource } from "../../../api/resource_pb";
import type { OperationSchema } from "../../../longrunning/operations_pb";
import { file_google_longrunning_operations } from "../../../longrunning/operations_pb";
import type { Duration, FieldMask, Timestamp } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_duration, file_google_protobuf_field_mask, file_google_protobuf_timestamp } from "@bufbuild/protobuf/wkt";
import type { Status } from "../../../rpc/status_pb";
import { file_google_rpc_status } from "../../../rpc/status_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file google/cloud/speech/v2/cloud_speech.proto.
 */
export const file_google_cloud_speech_v2_cloud_speech: GenFile = /*@__PURE__*/
  fileDesc("Cilnb29nbGUvY2xvdWQvc3BlZWNoL3YyL2Nsb3VkX3NwZWVjaC5wcm90bxIWZ29vZ2xlLmNsb3VkLnNwZWVjaC52MiK+AQoXQ3JlYXRlUmVjb2duaXplclJlcXVlc3QSOwoKcmVjb2duaXplchgBIAEoCzIiLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuUmVjb2duaXplckID4EECEhUKDXZhbGlkYXRlX29ubHkYAiABKAgSFQoNcmVjb2duaXplcl9pZBgDIAEoCRI4CgZwYXJlbnQYBCABKAlCKOBBAvpBIhIgc3BlZWNoLmdvb2dsZWFwaXMuY29tL1JlY29nbml6ZXIi5wwKEU9wZXJhdGlvbk1ldGFkYXRhEi8KC2NyZWF0ZV90aW1lGAEgASgLMhouZ29vZ2xlLnByb3RvYnVmLlRpbWVzdGFtcBIvCgt1cGRhdGVfdGltZRgCIAEoCzIaLmdvb2dsZS5wcm90b2J1Zi5UaW1lc3RhbXASEAoIcmVzb3VyY2UYAyABKAkSDgoGbWV0aG9kGAQgASgJEjwKDGttc19rZXlfbmFtZRgGIAEoCUIm+kEjCiFjbG91ZGttcy5nb29nbGVhcGlzLmNvbS9DcnlwdG9LZXkSSwoUa21zX2tleV92ZXJzaW9uX25hbWUYByABKAlCLfpBKgooY2xvdWRrbXMuZ29vZ2xlYXBpcy5jb20vQ3J5cHRvS2V5VmVyc2lvbhJQChdiYXRjaF9yZWNvZ25pemVfcmVxdWVzdBgIIAEoCzItLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuQmF0Y2hSZWNvZ25pemVSZXF1ZXN0SAASVAoZY3JlYXRlX3JlY29nbml6ZXJfcmVxdWVzdBgJIAEoCzIvLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuQ3JlYXRlUmVjb2duaXplclJlcXVlc3RIABJUChl1cGRhdGVfcmVjb2duaXplcl9yZXF1ZXN0GAogASgLMi8uZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5VcGRhdGVSZWNvZ25pemVyUmVxdWVzdEgAElQKGWRlbGV0ZV9yZWNvZ25pemVyX3JlcXVlc3QYCyABKAsyLy5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkRlbGV0ZVJlY29nbml6ZXJSZXF1ZXN0SAASWAobdW5kZWxldGVfcmVjb2duaXplcl9yZXF1ZXN0GAwgASgLMjEuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5VbmRlbGV0ZVJlY29nbml6ZXJSZXF1ZXN0SAASVwobY3JlYXRlX2N1c3RvbV9jbGFzc19yZXF1ZXN0GA0gASgLMjAuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5DcmVhdGVDdXN0b21DbGFzc1JlcXVlc3RIABJXCht1cGRhdGVfY3VzdG9tX2NsYXNzX3JlcXVlc3QYDiABKAsyMC5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlVwZGF0ZUN1c3RvbUNsYXNzUmVxdWVzdEgAElcKG2RlbGV0ZV9jdXN0b21fY2xhc3NfcmVxdWVzdBgPIAEoCzIwLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuRGVsZXRlQ3VzdG9tQ2xhc3NSZXF1ZXN0SAASWwoddW5kZWxldGVfY3VzdG9tX2NsYXNzX3JlcXVlc3QYECABKAsyMi5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlVuZGVsZXRlQ3VzdG9tQ2xhc3NSZXF1ZXN0SAASUwoZY3JlYXRlX3BocmFzZV9zZXRfcmVxdWVzdBgRIAEoCzIuLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuQ3JlYXRlUGhyYXNlU2V0UmVxdWVzdEgAElMKGXVwZGF0ZV9waHJhc2Vfc2V0X3JlcXVlc3QYEiABKAsyLi5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlVwZGF0ZVBocmFzZVNldFJlcXVlc3RIABJTChlkZWxldGVfcGhyYXNlX3NldF9yZXF1ZXN0GBMgASgLMi4uZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5EZWxldGVQaHJhc2VTZXRSZXF1ZXN0SAASVwobdW5kZWxldGVfcGhyYXNlX3NldF9yZXF1ZXN0GBQgASgLMjAuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5VbmRlbGV0ZVBocmFzZVNldFJlcXVlc3RIABJQChV1cGRhdGVfY29uZmlnX3JlcXVlc3QYFSABKAsyKy5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlVwZGF0ZUNvbmZpZ1JlcXVlc3RCAhgBSAASGAoQcHJvZ3Jlc3NfcGVyY2VudBgWIAEoBRJSChhiYXRjaF9yZWNvZ25pemVfbWV0YWRhdGEYFyABKAsyLi5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkJhdGNoUmVjb2duaXplTWV0YWRhdGFIAUIJCgdyZXF1ZXN0QgoKCG1ldGFkYXRhIpABChZMaXN0UmVjb2duaXplcnNSZXF1ZXN0EjkKBnBhcmVudBgBIAEoCUIp4EEC+kEjCiFsb2NhdGlvbnMuZ29vZ2xlYXBpcy5jb20vTG9jYXRpb24SEQoJcGFnZV9zaXplGAIgASgFEhIKCnBhZ2VfdG9rZW4YAyABKAkSFAoMc2hvd19kZWxldGVkGAQgASgIImsKF0xpc3RSZWNvZ25pemVyc1Jlc3BvbnNlEjcKC3JlY29nbml6ZXJzGAEgAygLMiIuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5SZWNvZ25pemVyEhcKD25leHRfcGFnZV90b2tlbhgCIAEoCSJOChRHZXRSZWNvZ25pemVyUmVxdWVzdBI2CgRuYW1lGAEgASgJQijgQQL6QSIKIHNwZWVjaC5nb29nbGVhcGlzLmNvbS9SZWNvZ25pemVyIp4BChdVcGRhdGVSZWNvZ25pemVyUmVxdWVzdBI7CgpyZWNvZ25pemVyGAEgASgLMiIuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5SZWNvZ25pemVyQgPgQQISLwoLdXBkYXRlX21hc2sYAiABKAsyGi5nb29nbGUucHJvdG9idWYuRmllbGRNYXNrEhUKDXZhbGlkYXRlX29ubHkYBCABKAgijQEKF0RlbGV0ZVJlY29nbml6ZXJSZXF1ZXN0EjYKBG5hbWUYASABKAlCKOBBAvpBIgogc3BlZWNoLmdvb2dsZWFwaXMuY29tL1JlY29nbml6ZXISFQoNdmFsaWRhdGVfb25seRgCIAEoCBIVCg1hbGxvd19taXNzaW5nGAQgASgIEgwKBGV0YWcYAyABKAkieAoZVW5kZWxldGVSZWNvZ25pemVyUmVxdWVzdBI2CgRuYW1lGAEgASgJQijgQQL6QSIKIHNwZWVjaC5nb29nbGVhcGlzLmNvbS9SZWNvZ25pemVyEhUKDXZhbGlkYXRlX29ubHkYAyABKAgSDAoEZXRhZxgEIAEoCSLFBwoKUmVjb2duaXplchIUCgRuYW1lGAEgASgJQgbgQQPgQQgSEAoDdWlkGAIgASgJQgPgQQMSFAoMZGlzcGxheV9uYW1lGAMgASgJEhQKBW1vZGVsGAQgASgJQgUYAeBBARIdCg5sYW5ndWFnZV9jb2RlcxgRIAMoCUIFGAHgQQESTQoaZGVmYXVsdF9yZWNvZ25pdGlvbl9jb25maWcYBiABKAsyKS5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlJlY29nbml0aW9uQ29uZmlnEkgKC2Fubm90YXRpb25zGAcgAygLMjMuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5SZWNvZ25pemVyLkFubm90YXRpb25zRW50cnkSPAoFc3RhdGUYCCABKA4yKC5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlJlY29nbml6ZXIuU3RhdGVCA+BBAxI0CgtjcmVhdGVfdGltZRgJIAEoCzIaLmdvb2dsZS5wcm90b2J1Zi5UaW1lc3RhbXBCA+BBAxI0Cgt1cGRhdGVfdGltZRgKIAEoCzIaLmdvb2dsZS5wcm90b2J1Zi5UaW1lc3RhbXBCA+BBAxI0CgtkZWxldGVfdGltZRgLIAEoCzIaLmdvb2dsZS5wcm90b2J1Zi5UaW1lc3RhbXBCA+BBAxI0CgtleHBpcmVfdGltZRgOIAEoCzIaLmdvb2dsZS5wcm90b2J1Zi5UaW1lc3RhbXBCA+BBAxIRCgRldGFnGAwgASgJQgPgQQMSGAoLcmVjb25jaWxpbmcYDSABKAhCA+BBAxI/CgxrbXNfa2V5X25hbWUYDyABKAlCKeBBA/pBIwohY2xvdWRrbXMuZ29vZ2xlYXBpcy5jb20vQ3J5cHRvS2V5Ek4KFGttc19rZXlfdmVyc2lvbl9uYW1lGBAgASgJQjDgQQP6QSoKKGNsb3Vka21zLmdvb2dsZWFwaXMuY29tL0NyeXB0b0tleVZlcnNpb24aMgoQQW5ub3RhdGlvbnNFbnRyeRILCgNrZXkYASABKAkSDQoFdmFsdWUYAiABKAk6AjgBIjcKBVN0YXRlEhUKEVNUQVRFX1VOU1BFQ0lGSUVEEAASCgoGQUNUSVZFEAISCwoHREVMRVRFRBAEOmrqQWcKIHNwZWVjaC5nb29nbGVhcGlzLmNvbS9SZWNvZ25pemVyEkBwcm9qZWN0cy97cHJvamVjdH0vbG9jYXRpb25zL3tsb2NhdGlvbn0vcmVjb2duaXplcnMve3JlY29nbml6ZXJ9UgEBIhoKGEF1dG9EZXRlY3REZWNvZGluZ0NvbmZpZyL5AQoWRXhwbGljaXREZWNvZGluZ0NvbmZpZxJTCghlbmNvZGluZxgBIAEoDjI8Lmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuRXhwbGljaXREZWNvZGluZ0NvbmZpZy5BdWRpb0VuY29kaW5nQgPgQQISGQoRc2FtcGxlX3JhdGVfaGVydHoYAiABKAUSGwoTYXVkaW9fY2hhbm5lbF9jb3VudBgDIAEoBSJSCg1BdWRpb0VuY29kaW5nEh4KGkFVRElPX0VOQ09ESU5HX1VOU1BFQ0lGSUVEEAASDAoITElORUFSMTYQARIJCgVNVUxBVxACEggKBEFMQVcQAyJaChhTcGVha2VyRGlhcml6YXRpb25Db25maWcSHgoRbWluX3NwZWFrZXJfY291bnQYAiABKAVCA+BBAhIeChFtYXhfc3BlYWtlcl9jb3VudBgDIAEoBUID4EECIvgDChNSZWNvZ25pdGlvbkZlYXR1cmVzEhgKEHByb2Zhbml0eV9maWx0ZXIYASABKAgSIAoYZW5hYmxlX3dvcmRfdGltZV9vZmZzZXRzGAIgASgIEh4KFmVuYWJsZV93b3JkX2NvbmZpZGVuY2UYAyABKAgSJAocZW5hYmxlX2F1dG9tYXRpY19wdW5jdHVhdGlvbhgEIAEoCBIhChllbmFibGVfc3Bva2VuX3B1bmN0dWF0aW9uGA4gASgIEhwKFGVuYWJsZV9zcG9rZW5fZW1vamlzGA8gASgIElgKEm11bHRpX2NoYW5uZWxfbW9kZRgRIAEoDjI8Lmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuUmVjb2duaXRpb25GZWF0dXJlcy5NdWx0aUNoYW5uZWxNb2RlEkwKEmRpYXJpemF0aW9uX2NvbmZpZxgJIAEoCzIwLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuU3BlYWtlckRpYXJpemF0aW9uQ29uZmlnEhgKEG1heF9hbHRlcm5hdGl2ZXMYECABKAUiXAoQTXVsdGlDaGFubmVsTW9kZRIiCh5NVUxUSV9DSEFOTkVMX01PREVfVU5TUEVDSUZJRUQQABIkCiBTRVBBUkFURV9SRUNPR05JVElPTl9QRVJfQ0hBTk5FTBABIqMBChdUcmFuc2NyaXB0Tm9ybWFsaXphdGlvbhJGCgdlbnRyaWVzGAEgAygLMjUuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5UcmFuc2NyaXB0Tm9ybWFsaXphdGlvbi5FbnRyeRpACgVFbnRyeRIOCgZzZWFyY2gYASABKAkSDwoHcmVwbGFjZRgCIAEoCRIWCg5jYXNlX3NlbnNpdGl2ZRgDIAEoCCIxChFUcmFuc2xhdGlvbkNvbmZpZxIcCg90YXJnZXRfbGFuZ3VhZ2UYASABKAlCA+BBAiK/AgoQU3BlZWNoQWRhcHRhdGlvbhJRCgtwaHJhc2Vfc2V0cxgBIAMoCzI8Lmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuU3BlZWNoQWRhcHRhdGlvbi5BZGFwdGF0aW9uUGhyYXNlU2V0EjsKDmN1c3RvbV9jbGFzc2VzGAIgAygLMiMuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5DdXN0b21DbGFzcxqaAQoTQWRhcHRhdGlvblBocmFzZVNldBI6CgpwaHJhc2Vfc2V0GAEgASgJQiT6QSEKH3NwZWVjaC5nb29nbGVhcGlzLmNvbS9QaHJhc2VTZXRIABI+ChFpbmxpbmVfcGhyYXNlX3NldBgCIAEoCzIhLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuUGhyYXNlU2V0SABCBwoFdmFsdWUingQKEVJlY29nbml0aW9uQ29uZmlnElAKFGF1dG9fZGVjb2RpbmdfY29uZmlnGAcgASgLMjAuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5BdXRvRGV0ZWN0RGVjb2RpbmdDb25maWdIABJSChhleHBsaWNpdF9kZWNvZGluZ19jb25maWcYCCABKAsyLi5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkV4cGxpY2l0RGVjb2RpbmdDb25maWdIABISCgVtb2RlbBgJIAEoCUID4EEBEhsKDmxhbmd1YWdlX2NvZGVzGAogAygJQgPgQQESPQoIZmVhdHVyZXMYAiABKAsyKy5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlJlY29nbml0aW9uRmVhdHVyZXMSPAoKYWRhcHRhdGlvbhgGIAEoCzIoLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuU3BlZWNoQWRhcHRhdGlvbhJWChh0cmFuc2NyaXB0X25vcm1hbGl6YXRpb24YCyABKAsyLy5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlRyYW5zY3JpcHROb3JtYWxpemF0aW9uQgPgQQESSgoSdHJhbnNsYXRpb25fY29uZmlnGA8gASgLMikuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5UcmFuc2xhdGlvbkNvbmZpZ0ID4EEBQhEKD2RlY29kaW5nX2NvbmZpZyLuAQoQUmVjb2duaXplUmVxdWVzdBI8CgpyZWNvZ25pemVyGAMgASgJQijgQQL6QSIKIHNwZWVjaC5nb29nbGVhcGlzLmNvbS9SZWNvZ25pemVyEjkKBmNvbmZpZxgBIAEoCzIpLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuUmVjb2duaXRpb25Db25maWcSLwoLY29uZmlnX21hc2sYCCABKAsyGi5nb29nbGUucHJvdG9idWYuRmllbGRNYXNrEhEKB2NvbnRlbnQYBSABKAxIABINCgN1cmkYBiABKAlIAEIOCgxhdWRpb19zb3VyY2UidQobUmVjb2duaXRpb25SZXNwb25zZU1ldGFkYXRhEhwKCnJlcXVlc3RfaWQYCSABKAlCCOKMz9cIAggBEjgKFXRvdGFsX2JpbGxlZF9kdXJhdGlvbhgGIAEoCzIZLmdvb2dsZS5wcm90b2J1Zi5EdXJhdGlvbiJ3ChxTcGVlY2hSZWNvZ25pdGlvbkFsdGVybmF0aXZlEhIKCnRyYW5zY3JpcHQYASABKAkSEgoKY29uZmlkZW5jZRgCIAEoAhIvCgV3b3JkcxgDIAMoCzIgLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuV29yZEluZm8iowEKCFdvcmRJbmZvEi8KDHN0YXJ0X29mZnNldBgBIAEoCzIZLmdvb2dsZS5wcm90b2J1Zi5EdXJhdGlvbhItCgplbmRfb2Zmc2V0GAIgASgLMhkuZ29vZ2xlLnByb3RvYnVmLkR1cmF0aW9uEgwKBHdvcmQYAyABKAkSEgoKY29uZmlkZW5jZRgEIAEoAhIVCg1zcGVha2VyX2xhYmVsGAYgASgJIswBChdTcGVlY2hSZWNvZ25pdGlvblJlc3VsdBJKCgxhbHRlcm5hdGl2ZXMYASADKAsyNC5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlNwZWVjaFJlY29nbml0aW9uQWx0ZXJuYXRpdmUSEwoLY2hhbm5lbF90YWcYAiABKAUSNAoRcmVzdWx0X2VuZF9vZmZzZXQYBCABKAsyGS5nb29nbGUucHJvdG9idWYuRHVyYXRpb24SGgoNbGFuZ3VhZ2VfY29kZRgFIAEoCUID4EEDIpwBChFSZWNvZ25pemVSZXNwb25zZRJACgdyZXN1bHRzGAMgAygLMi8uZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5TcGVlY2hSZWNvZ25pdGlvblJlc3VsdBJFCghtZXRhZGF0YRgCIAEoCzIzLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuUmVjb2duaXRpb25SZXNwb25zZU1ldGFkYXRhItECChxTdHJlYW1pbmdSZWNvZ25pdGlvbkZlYXR1cmVzEiQKHGVuYWJsZV92b2ljZV9hY3Rpdml0eV9ldmVudHMYASABKAgSFwoPaW50ZXJpbV9yZXN1bHRzGAIgASgIEmkKFnZvaWNlX2FjdGl2aXR5X3RpbWVvdXQYAyABKAsySS5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlN0cmVhbWluZ1JlY29nbml0aW9uRmVhdHVyZXMuVm9pY2VBY3Rpdml0eVRpbWVvdXQahgEKFFZvaWNlQWN0aXZpdHlUaW1lb3V0EjcKFHNwZWVjaF9zdGFydF90aW1lb3V0GAEgASgLMhkuZ29vZ2xlLnByb3RvYnVmLkR1cmF0aW9uEjUKEnNwZWVjaF9lbmRfdGltZW91dBgCIAEoCzIZLmdvb2dsZS5wcm90b2J1Zi5EdXJhdGlvbiLfAQoaU3RyZWFtaW5nUmVjb2duaXRpb25Db25maWcSPgoGY29uZmlnGAEgASgLMikuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5SZWNvZ25pdGlvbkNvbmZpZ0ID4EECEi8KC2NvbmZpZ19tYXNrGAMgASgLMhouZ29vZ2xlLnByb3RvYnVmLkZpZWxkTWFzaxJQChJzdHJlYW1pbmdfZmVhdHVyZXMYAiABKAsyNC5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlN0cmVhbWluZ1JlY29nbml0aW9uRmVhdHVyZXMizwEKGVN0cmVhbWluZ1JlY29nbml6ZVJlcXVlc3QSPAoKcmVjb2duaXplchgDIAEoCUIo4EEC+kEiCiBzcGVlY2guZ29vZ2xlYXBpcy5jb20vUmVjb2duaXplchJOChBzdHJlYW1pbmdfY29uZmlnGAYgASgLMjIuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5TdHJlYW1pbmdSZWNvZ25pdGlvbkNvbmZpZ0gAEg8KBWF1ZGlvGAUgASgMSABCEwoRc3RyZWFtaW5nX3JlcXVlc3QiiAQKFUJhdGNoUmVjb2duaXplUmVxdWVzdBI8CgpyZWNvZ25pemVyGAEgASgJQijgQQL6QSIKIHNwZWVjaC5nb29nbGVhcGlzLmNvbS9SZWNvZ25pemVyEjkKBmNvbmZpZxgEIAEoCzIpLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuUmVjb2duaXRpb25Db25maWcSLwoLY29uZmlnX21hc2sYBSABKAsyGi5nb29nbGUucHJvdG9idWYuRmllbGRNYXNrEkEKBWZpbGVzGAMgAygLMjIuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5CYXRjaFJlY29nbml6ZUZpbGVNZXRhZGF0YRJSChlyZWNvZ25pdGlvbl9vdXRwdXRfY29uZmlnGAYgASgLMi8uZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5SZWNvZ25pdGlvbk91dHB1dENvbmZpZxJdChNwcm9jZXNzaW5nX3N0cmF0ZWd5GAcgASgOMkAuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5CYXRjaFJlY29nbml6ZVJlcXVlc3QuUHJvY2Vzc2luZ1N0cmF0ZWd5Ik8KElByb2Nlc3NpbmdTdHJhdGVneRIjCh9QUk9DRVNTSU5HX1NUUkFURUdZX1VOU1BFQ0lGSUVEEAASFAoQRFlOQU1JQ19CQVRDSElORxABIh4KD0djc091dHB1dENvbmZpZxILCgN1cmkYASABKAkiFAoSSW5saW5lT3V0cHV0Q29uZmlnIh4KHE5hdGl2ZU91dHB1dEZpbGVGb3JtYXRDb25maWciGwoZVnR0T3V0cHV0RmlsZUZvcm1hdENvbmZpZyIbChlTcnRPdXRwdXRGaWxlRm9ybWF0Q29uZmlnItoBChJPdXRwdXRGb3JtYXRDb25maWcSRAoGbmF0aXZlGAEgASgLMjQuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5OYXRpdmVPdXRwdXRGaWxlRm9ybWF0Q29uZmlnEj4KA3Z0dBgCIAEoCzIxLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuVnR0T3V0cHV0RmlsZUZvcm1hdENvbmZpZxI+CgNzcnQYAyABKAsyMS5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlNydE91dHB1dEZpbGVGb3JtYXRDb25maWcihgIKF1JlY29nbml0aW9uT3V0cHV0Q29uZmlnEkQKEWdjc19vdXRwdXRfY29uZmlnGAEgASgLMicuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5HY3NPdXRwdXRDb25maWdIABJMChZpbmxpbmVfcmVzcG9uc2VfY29uZmlnGAIgASgLMiouZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5JbmxpbmVPdXRwdXRDb25maWdIABJNChRvdXRwdXRfZm9ybWF0X2NvbmZpZxgDIAEoCzIqLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuT3V0cHV0Rm9ybWF0Q29uZmlnQgPgQQFCCAoGb3V0cHV0IoICChZCYXRjaFJlY29nbml6ZVJlc3BvbnNlEkwKB3Jlc3VsdHMYASADKAsyOy5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkJhdGNoUmVjb2duaXplUmVzcG9uc2UuUmVzdWx0c0VudHJ5EjgKFXRvdGFsX2JpbGxlZF9kdXJhdGlvbhgCIAEoCzIZLmdvb2dsZS5wcm90b2J1Zi5EdXJhdGlvbhpgCgxSZXN1bHRzRW50cnkSCwoDa2V5GAEgASgJEj8KBXZhbHVlGAIgASgLMjAuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5CYXRjaFJlY29nbml6ZUZpbGVSZXN1bHQ6AjgBIqABChVCYXRjaFJlY29nbml6ZVJlc3VsdHMSQAoHcmVzdWx0cxgBIAMoCzIvLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuU3BlZWNoUmVjb2duaXRpb25SZXN1bHQSRQoIbWV0YWRhdGEYAiABKAsyMy5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlJlY29nbml0aW9uUmVzcG9uc2VNZXRhZGF0YSJRChJDbG91ZFN0b3JhZ2VSZXN1bHQSCwoDdXJpGAEgASgJEhYKDnZ0dF9mb3JtYXRfdXJpGAIgASgJEhYKDnNydF9mb3JtYXRfdXJpGAMgASgJIn0KDElubGluZVJlc3VsdBJBCgp0cmFuc2NyaXB0GAEgASgLMi0uZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5CYXRjaFJlY29nbml6ZVJlc3VsdHMSFAoMdnR0X2NhcHRpb25zGAIgASgJEhQKDHNydF9jYXB0aW9ucxgDIAEoCSLxAgoYQmF0Y2hSZWNvZ25pemVGaWxlUmVzdWx0EiEKBWVycm9yGAIgASgLMhIuZ29vZ2xlLnJwYy5TdGF0dXMSRQoIbWV0YWRhdGEYAyABKAsyMy5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlJlY29nbml0aW9uUmVzcG9uc2VNZXRhZGF0YRJKChRjbG91ZF9zdG9yYWdlX3Jlc3VsdBgFIAEoCzIqLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuQ2xvdWRTdG9yYWdlUmVzdWx0SAASPQoNaW5saW5lX3Jlc3VsdBgGIAEoCzIkLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuSW5saW5lUmVzdWx0SAASDwoDdXJpGAEgASgJQgIYARJFCgp0cmFuc2NyaXB0GAQgASgLMi0uZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5CYXRjaFJlY29nbml6ZVJlc3VsdHNCAhgBQggKBnJlc3VsdCJvCiNCYXRjaFJlY29nbml6ZVRyYW5zY3JpcHRpb25NZXRhZGF0YRIYChBwcm9ncmVzc19wZXJjZW50GAEgASgFEiEKBWVycm9yGAIgASgLMhIuZ29vZ2xlLnJwYy5TdGF0dXMSCwoDdXJpGAMgASgJIv4BChZCYXRjaFJlY29nbml6ZU1ldGFkYXRhEmkKFnRyYW5zY3JpcHRpb25fbWV0YWRhdGEYASADKAsySS5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkJhdGNoUmVjb2duaXplTWV0YWRhdGEuVHJhbnNjcmlwdGlvbk1ldGFkYXRhRW50cnkaeQoaVHJhbnNjcmlwdGlvbk1ldGFkYXRhRW50cnkSCwoDa2V5GAEgASgJEkoKBXZhbHVlGAIgASgLMjsuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5CYXRjaFJlY29nbml6ZVRyYW5zY3JpcHRpb25NZXRhZGF0YToCOAEipwEKGkJhdGNoUmVjb2duaXplRmlsZU1ldGFkYXRhEg0KA3VyaRgBIAEoCUgAEjkKBmNvbmZpZxgEIAEoCzIpLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuUmVjb2duaXRpb25Db25maWcSLwoLY29uZmlnX21hc2sYBSABKAsyGi5nb29nbGUucHJvdG9idWYuRmllbGRNYXNrQg4KDGF1ZGlvX3NvdXJjZSL0AQoaU3RyZWFtaW5nUmVjb2duaXRpb25SZXN1bHQSSgoMYWx0ZXJuYXRpdmVzGAEgAygLMjQuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5TcGVlY2hSZWNvZ25pdGlvbkFsdGVybmF0aXZlEhAKCGlzX2ZpbmFsGAIgASgIEhEKCXN0YWJpbGl0eRgDIAEoAhI0ChFyZXN1bHRfZW5kX29mZnNldBgEIAEoCzIZLmdvb2dsZS5wcm90b2J1Zi5EdXJhdGlvbhITCgtjaGFubmVsX3RhZxgFIAEoBRIaCg1sYW5ndWFnZV9jb2RlGAYgASgJQgPgQQMixwMKGlN0cmVhbWluZ1JlY29nbml6ZVJlc3BvbnNlEkMKB3Jlc3VsdHMYBiADKAsyMi5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlN0cmVhbWluZ1JlY29nbml0aW9uUmVzdWx0El0KEXNwZWVjaF9ldmVudF90eXBlGAMgASgOMkIuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5TdHJlYW1pbmdSZWNvZ25pemVSZXNwb25zZS5TcGVlY2hFdmVudFR5cGUSNgoTc3BlZWNoX2V2ZW50X29mZnNldBgHIAEoCzIZLmdvb2dsZS5wcm90b2J1Zi5EdXJhdGlvbhJFCghtZXRhZGF0YRgFIAEoCzIzLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuUmVjb2duaXRpb25SZXNwb25zZU1ldGFkYXRhIoUBCg9TcGVlY2hFdmVudFR5cGUSIQodU1BFRUNIX0VWRU5UX1RZUEVfVU5TUEVDSUZJRUQQABIbChdFTkRfT0ZfU0lOR0xFX1VUVEVSQU5DRRABEhkKFVNQRUVDSF9BQ1RJVklUWV9CRUdJThACEhcKE1NQRUVDSF9BQ1RJVklUWV9FTkQQAyLoAQoGQ29uZmlnEhQKBG5hbWUYASABKAlCBuBBA+BBCBI/CgxrbXNfa2V5X25hbWUYAiABKAlCKeBBAfpBIwohY2xvdWRrbXMuZ29vZ2xlYXBpcy5jb20vQ3J5cHRvS2V5EjQKC3VwZGF0ZV90aW1lGAMgASgLMhouZ29vZ2xlLnByb3RvYnVmLlRpbWVzdGFtcEID4EEDOlHqQU4KHHNwZWVjaC5nb29nbGVhcGlzLmNvbS9Db25maWcSLnByb2plY3RzL3twcm9qZWN0fS9sb2NhdGlvbnMve2xvY2F0aW9ufS9jb25maWciRgoQR2V0Q29uZmlnUmVxdWVzdBIyCgRuYW1lGAEgASgJQiTgQQL6QR4KHHNwZWVjaC5nb29nbGVhcGlzLmNvbS9Db25maWciewoTVXBkYXRlQ29uZmlnUmVxdWVzdBIzCgZjb25maWcYASABKAsyHi5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkNvbmZpZ0ID4EECEi8KC3VwZGF0ZV9tYXNrGAIgASgLMhouZ29vZ2xlLnByb3RvYnVmLkZpZWxkTWFzayKtBwoLQ3VzdG9tQ2xhc3MSFAoEbmFtZRgBIAEoCUIG4EED4EEIEhAKA3VpZBgCIAEoCUID4EEDEhkKDGRpc3BsYXlfbmFtZRgEIAEoCUID4EEBEjwKBWl0ZW1zGAUgAygLMi0uZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5DdXN0b21DbGFzcy5DbGFzc0l0ZW0SPQoFc3RhdGUYDyABKA4yKS5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkN1c3RvbUNsYXNzLlN0YXRlQgPgQQMSNAoLY3JlYXRlX3RpbWUYBiABKAsyGi5nb29nbGUucHJvdG9idWYuVGltZXN0YW1wQgPgQQMSNAoLdXBkYXRlX3RpbWUYByABKAsyGi5nb29nbGUucHJvdG9idWYuVGltZXN0YW1wQgPgQQMSNAoLZGVsZXRlX3RpbWUYCCABKAsyGi5nb29nbGUucHJvdG9idWYuVGltZXN0YW1wQgPgQQMSNAoLZXhwaXJlX3RpbWUYCSABKAsyGi5nb29nbGUucHJvdG9idWYuVGltZXN0YW1wQgPgQQMSTgoLYW5ub3RhdGlvbnMYCiADKAsyNC5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkN1c3RvbUNsYXNzLkFubm90YXRpb25zRW50cnlCA+BBARIRCgRldGFnGAsgASgJQgPgQQMSGAoLcmVjb25jaWxpbmcYDCABKAhCA+BBAxI/CgxrbXNfa2V5X25hbWUYDSABKAlCKeBBA/pBIwohY2xvdWRrbXMuZ29vZ2xlYXBpcy5jb20vQ3J5cHRvS2V5Ek4KFGttc19rZXlfdmVyc2lvbl9uYW1lGA4gASgJQjDgQQP6QSoKKGNsb3Vka21zLmdvb2dsZWFwaXMuY29tL0NyeXB0b0tleVZlcnNpb24aGgoJQ2xhc3NJdGVtEg0KBXZhbHVlGAEgASgJGjIKEEFubm90YXRpb25zRW50cnkSCwoDa2V5GAEgASgJEg0KBXZhbHVlGAIgASgJOgI4ASI3CgVTdGF0ZRIVChFTVEFURV9VTlNQRUNJRklFRBAAEgoKBkFDVElWRRACEgsKB0RFTEVURUQQBDpv6kFsCiFzcGVlY2guZ29vZ2xlYXBpcy5jb20vQ3VzdG9tQ2xhc3MSRHByb2plY3RzL3twcm9qZWN0fS9sb2NhdGlvbnMve2xvY2F0aW9ufS9jdXN0b21DbGFzc2VzL3tjdXN0b21fY2xhc3N9UgEBIq4HCglQaHJhc2VTZXQSFAoEbmFtZRgBIAEoCUIG4EED4EEIEhAKA3VpZBgCIAEoCUID4EEDEjkKB3BocmFzZXMYAyADKAsyKC5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlBocmFzZVNldC5QaHJhc2USDQoFYm9vc3QYBCABKAISFAoMZGlzcGxheV9uYW1lGAUgASgJEjsKBXN0YXRlGA8gASgOMicuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5QaHJhc2VTZXQuU3RhdGVCA+BBAxI0CgtjcmVhdGVfdGltZRgGIAEoCzIaLmdvb2dsZS5wcm90b2J1Zi5UaW1lc3RhbXBCA+BBAxI0Cgt1cGRhdGVfdGltZRgHIAEoCzIaLmdvb2dsZS5wcm90b2J1Zi5UaW1lc3RhbXBCA+BBAxI0CgtkZWxldGVfdGltZRgIIAEoCzIaLmdvb2dsZS5wcm90b2J1Zi5UaW1lc3RhbXBCA+BBAxI0CgtleHBpcmVfdGltZRgJIAEoCzIaLmdvb2dsZS5wcm90b2J1Zi5UaW1lc3RhbXBCA+BBAxJHCgthbm5vdGF0aW9ucxgKIAMoCzIyLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuUGhyYXNlU2V0LkFubm90YXRpb25zRW50cnkSEQoEZXRhZxgLIAEoCUID4EEDEhgKC3JlY29uY2lsaW5nGAwgASgIQgPgQQMSPwoMa21zX2tleV9uYW1lGA0gASgJQingQQP6QSMKIWNsb3Vka21zLmdvb2dsZWFwaXMuY29tL0NyeXB0b0tleRJOChRrbXNfa2V5X3ZlcnNpb25fbmFtZRgOIAEoCUIw4EED+kEqCihjbG91ZGttcy5nb29nbGVhcGlzLmNvbS9DcnlwdG9LZXlWZXJzaW9uGiYKBlBocmFzZRINCgV2YWx1ZRgBIAEoCRINCgVib29zdBgCIAEoAhoyChBBbm5vdGF0aW9uc0VudHJ5EgsKA2tleRgBIAEoCRINCgV2YWx1ZRgCIAEoCToCOAEiNwoFU3RhdGUSFQoRU1RBVEVfVU5TUEVDSUZJRUQQABIKCgZBQ1RJVkUQAhILCgdERUxFVEVEEAQ6aOpBZQofc3BlZWNoLmdvb2dsZWFwaXMuY29tL1BocmFzZVNldBI/cHJvamVjdHMve3Byb2plY3R9L2xvY2F0aW9ucy97bG9jYXRpb259L3BocmFzZVNldHMve3BocmFzZV9zZXR9UgEBIsUBChhDcmVhdGVDdXN0b21DbGFzc1JlcXVlc3QSPgoMY3VzdG9tX2NsYXNzGAEgASgLMiMuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5DdXN0b21DbGFzc0ID4EECEhUKDXZhbGlkYXRlX29ubHkYAiABKAgSFwoPY3VzdG9tX2NsYXNzX2lkGAMgASgJEjkKBnBhcmVudBgEIAEoCUIp4EEC+kEjEiFzcGVlY2guZ29vZ2xlYXBpcy5jb20vQ3VzdG9tQ2xhc3MikgEKGExpc3RDdXN0b21DbGFzc2VzUmVxdWVzdBI5CgZwYXJlbnQYASABKAlCKeBBAvpBIwohbG9jYXRpb25zLmdvb2dsZWFwaXMuY29tL0xvY2F0aW9uEhEKCXBhZ2Vfc2l6ZRgCIAEoBRISCgpwYWdlX3Rva2VuGAMgASgJEhQKDHNob3dfZGVsZXRlZBgEIAEoCCJxChlMaXN0Q3VzdG9tQ2xhc3Nlc1Jlc3BvbnNlEjsKDmN1c3RvbV9jbGFzc2VzGAEgAygLMiMuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5DdXN0b21DbGFzcxIXCg9uZXh0X3BhZ2VfdG9rZW4YAiABKAkiUAoVR2V0Q3VzdG9tQ2xhc3NSZXF1ZXN0EjcKBG5hbWUYASABKAlCKeBBAvpBIwohc3BlZWNoLmdvb2dsZWFwaXMuY29tL0N1c3RvbUNsYXNzIqIBChhVcGRhdGVDdXN0b21DbGFzc1JlcXVlc3QSPgoMY3VzdG9tX2NsYXNzGAEgASgLMiMuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5DdXN0b21DbGFzc0ID4EECEi8KC3VwZGF0ZV9tYXNrGAIgASgLMhouZ29vZ2xlLnByb3RvYnVmLkZpZWxkTWFzaxIVCg12YWxpZGF0ZV9vbmx5GAQgASgIIo8BChhEZWxldGVDdXN0b21DbGFzc1JlcXVlc3QSNwoEbmFtZRgBIAEoCUIp4EEC+kEjCiFzcGVlY2guZ29vZ2xlYXBpcy5jb20vQ3VzdG9tQ2xhc3MSFQoNdmFsaWRhdGVfb25seRgCIAEoCBIVCg1hbGxvd19taXNzaW5nGAQgASgIEgwKBGV0YWcYAyABKAkiegoaVW5kZWxldGVDdXN0b21DbGFzc1JlcXVlc3QSNwoEbmFtZRgBIAEoCUIp4EEC+kEjCiFzcGVlY2guZ29vZ2xlYXBpcy5jb20vQ3VzdG9tQ2xhc3MSFQoNdmFsaWRhdGVfb25seRgDIAEoCBIMCgRldGFnGAQgASgJIrsBChZDcmVhdGVQaHJhc2VTZXRSZXF1ZXN0EjoKCnBocmFzZV9zZXQYASABKAsyIS5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlBocmFzZVNldEID4EECEhUKDXZhbGlkYXRlX29ubHkYAiABKAgSFQoNcGhyYXNlX3NldF9pZBgDIAEoCRI3CgZwYXJlbnQYBCABKAlCJ+BBAvpBIRIfc3BlZWNoLmdvb2dsZWFwaXMuY29tL1BocmFzZVNldCKPAQoVTGlzdFBocmFzZVNldHNSZXF1ZXN0EjkKBnBhcmVudBgBIAEoCUIp4EEC+kEjCiFsb2NhdGlvbnMuZ29vZ2xlYXBpcy5jb20vTG9jYXRpb24SEQoJcGFnZV9zaXplGAIgASgFEhIKCnBhZ2VfdG9rZW4YAyABKAkSFAoMc2hvd19kZWxldGVkGAQgASgIImkKFkxpc3RQaHJhc2VTZXRzUmVzcG9uc2USNgoLcGhyYXNlX3NldHMYASADKAsyIS5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlBocmFzZVNldBIXCg9uZXh0X3BhZ2VfdG9rZW4YAiABKAkiTAoTR2V0UGhyYXNlU2V0UmVxdWVzdBI1CgRuYW1lGAEgASgJQifgQQL6QSEKH3NwZWVjaC5nb29nbGVhcGlzLmNvbS9QaHJhc2VTZXQinAEKFlVwZGF0ZVBocmFzZVNldFJlcXVlc3QSOgoKcGhyYXNlX3NldBgBIAEoCzIhLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuUGhyYXNlU2V0QgPgQQISLwoLdXBkYXRlX21hc2sYAiABKAsyGi5nb29nbGUucHJvdG9idWYuRmllbGRNYXNrEhUKDXZhbGlkYXRlX29ubHkYBCABKAgiiwEKFkRlbGV0ZVBocmFzZVNldFJlcXVlc3QSNQoEbmFtZRgBIAEoCUIn4EEC+kEhCh9zcGVlY2guZ29vZ2xlYXBpcy5jb20vUGhyYXNlU2V0EhUKDXZhbGlkYXRlX29ubHkYAiABKAgSFQoNYWxsb3dfbWlzc2luZxgEIAEoCBIMCgRldGFnGAMgASgJInYKGFVuZGVsZXRlUGhyYXNlU2V0UmVxdWVzdBI1CgRuYW1lGAEgASgJQifgQQL6QSEKH3NwZWVjaC5nb29nbGVhcGlzLmNvbS9QaHJhc2VTZXQSFQoNdmFsaWRhdGVfb25seRgDIAEoCBIMCgRldGFnGAQgASgJMr8lCgZTcGVlY2gS7AEKEENyZWF0ZVJlY29nbml6ZXISLy5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkNyZWF0ZVJlY29nbml6ZXJSZXF1ZXN0Gh0uZ29vZ2xlLmxvbmdydW5uaW5nLk9wZXJhdGlvbiKHAcpBHwoKUmVjb2duaXplchIRT3BlcmF0aW9uTWV0YWRhdGHaQR9wYXJlbnQscmVjb2duaXplcixyZWNvZ25pemVyX2lkgtPkkwI9OgpyZWNvZ25pemVyIi8vdjIve3BhcmVudD1wcm9qZWN0cy8qL2xvY2F0aW9ucy8qfS9yZWNvZ25pemVycxK0AQoPTGlzdFJlY29nbml6ZXJzEi4uZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5MaXN0UmVjb2duaXplcnNSZXF1ZXN0Gi8uZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5MaXN0UmVjb2duaXplcnNSZXNwb25zZSJA2kEGcGFyZW50gtPkkwIxEi8vdjIve3BhcmVudD1wcm9qZWN0cy8qL2xvY2F0aW9ucy8qfS9yZWNvZ25pemVycxKhAQoNR2V0UmVjb2duaXplchIsLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuR2V0UmVjb2duaXplclJlcXVlc3QaIi5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlJlY29nbml6ZXIiPtpBBG5hbWWC0+STAjESLy92Mi97bmFtZT1wcm9qZWN0cy8qL2xvY2F0aW9ucy8qL3JlY29nbml6ZXJzLyp9Eu4BChBVcGRhdGVSZWNvZ25pemVyEi8uZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5VcGRhdGVSZWNvZ25pemVyUmVxdWVzdBodLmdvb2dsZS5sb25ncnVubmluZy5PcGVyYXRpb24iiQHKQR8KClJlY29nbml6ZXISEU9wZXJhdGlvbk1ldGFkYXRh2kEWcmVjb2duaXplcix1cGRhdGVfbWFza4LT5JMCSDoKcmVjb2duaXplcjI6L3YyL3tyZWNvZ25pemVyLm5hbWU9cHJvamVjdHMvKi9sb2NhdGlvbnMvKi9yZWNvZ25pemVycy8qfRLEAQoQRGVsZXRlUmVjb2duaXplchIvLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuRGVsZXRlUmVjb2duaXplclJlcXVlc3QaHS5nb29nbGUubG9uZ3J1bm5pbmcuT3BlcmF0aW9uImDKQR8KClJlY29nbml6ZXISEU9wZXJhdGlvbk1ldGFkYXRh2kEEbmFtZYLT5JMCMSovL3YyL3tuYW1lPXByb2plY3RzLyovbG9jYXRpb25zLyovcmVjb2duaXplcnMvKn0S1AEKElVuZGVsZXRlUmVjb2duaXplchIxLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuVW5kZWxldGVSZWNvZ25pemVyUmVxdWVzdBodLmdvb2dsZS5sb25ncnVubmluZy5PcGVyYXRpb24ibMpBHwoKUmVjb2duaXplchIRT3BlcmF0aW9uTWV0YWRhdGHaQQRuYW1lgtPkkwI9OgEqIjgvdjIve25hbWU9cHJvamVjdHMvKi9sb2NhdGlvbnMvKi9yZWNvZ25pemVycy8qfTp1bmRlbGV0ZRL5AQoJUmVjb2duaXplEiguZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5SZWNvZ25pemVSZXF1ZXN0GikuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5SZWNvZ25pemVSZXNwb25zZSKWAdpBJXJlY29nbml6ZXIsY29uZmlnLGNvbmZpZ19tYXNrLGNvbnRlbnTaQSFyZWNvZ25pemVyLGNvbmZpZyxjb25maWdfbWFzayx1cmmC0+STAkQ6ASoiPy92Mi97cmVjb2duaXplcj1wcm9qZWN0cy8qL2xvY2F0aW9ucy8qL3JlY29nbml6ZXJzLyp9OnJlY29nbml6ZRKBAQoSU3RyZWFtaW5nUmVjb2duaXplEjEuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5TdHJlYW1pbmdSZWNvZ25pemVSZXF1ZXN0GjIuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5TdHJlYW1pbmdSZWNvZ25pemVSZXNwb25zZSIAKAEwARKEAgoOQmF0Y2hSZWNvZ25pemUSLS5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkJhdGNoUmVjb2duaXplUmVxdWVzdBodLmdvb2dsZS5sb25ncnVubmluZy5PcGVyYXRpb24iowHKQSsKFkJhdGNoUmVjb2duaXplUmVzcG9uc2USEU9wZXJhdGlvbk1ldGFkYXRh2kEjcmVjb2duaXplcixjb25maWcsY29uZmlnX21hc2ssZmlsZXOC0+STAkk6ASoiRC92Mi97cmVjb2duaXplcj1wcm9qZWN0cy8qL2xvY2F0aW9ucy8qL3JlY29nbml6ZXJzLyp9OmJhdGNoUmVjb2duaXplEo4BCglHZXRDb25maWcSKC5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkdldENvbmZpZ1JlcXVlc3QaHi5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkNvbmZpZyI32kEEbmFtZYLT5JMCKhIoL3YyL3tuYW1lPXByb2plY3RzLyovbG9jYXRpb25zLyovY29uZmlnfRKxAQoMVXBkYXRlQ29uZmlnEisuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5VcGRhdGVDb25maWdSZXF1ZXN0Gh4uZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5Db25maWciVNpBEmNvbmZpZyx1cGRhdGVfbWFza4LT5JMCOToGY29uZmlnMi8vdjIve2NvbmZpZy5uYW1lPXByb2plY3RzLyovbG9jYXRpb25zLyovY29uZmlnfRL3AQoRQ3JlYXRlQ3VzdG9tQ2xhc3MSMC5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkNyZWF0ZUN1c3RvbUNsYXNzUmVxdWVzdBodLmdvb2dsZS5sb25ncnVubmluZy5PcGVyYXRpb24ikAHKQSAKC0N1c3RvbUNsYXNzEhFPcGVyYXRpb25NZXRhZGF0YdpBI3BhcmVudCxjdXN0b21fY2xhc3MsY3VzdG9tX2NsYXNzX2lkgtPkkwJBOgxjdXN0b21fY2xhc3MiMS92Mi97cGFyZW50PXByb2plY3RzLyovbG9jYXRpb25zLyp9L2N1c3RvbUNsYXNzZXMSvAEKEUxpc3RDdXN0b21DbGFzc2VzEjAuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5MaXN0Q3VzdG9tQ2xhc3Nlc1JlcXVlc3QaMS5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkxpc3RDdXN0b21DbGFzc2VzUmVzcG9uc2UiQtpBBnBhcmVudILT5JMCMxIxL3YyL3twYXJlbnQ9cHJvamVjdHMvKi9sb2NhdGlvbnMvKn0vY3VzdG9tQ2xhc3NlcxKmAQoOR2V0Q3VzdG9tQ2xhc3MSLS5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkdldEN1c3RvbUNsYXNzUmVxdWVzdBojLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuQ3VzdG9tQ2xhc3MiQNpBBG5hbWWC0+STAjMSMS92Mi97bmFtZT1wcm9qZWN0cy8qL2xvY2F0aW9ucy8qL2N1c3RvbUNsYXNzZXMvKn0S+QEKEVVwZGF0ZUN1c3RvbUNsYXNzEjAuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5VcGRhdGVDdXN0b21DbGFzc1JlcXVlc3QaHS5nb29nbGUubG9uZ3J1bm5pbmcuT3BlcmF0aW9uIpIBykEgCgtDdXN0b21DbGFzcxIRT3BlcmF0aW9uTWV0YWRhdGHaQRhjdXN0b21fY2xhc3MsdXBkYXRlX21hc2uC0+STAk46DGN1c3RvbV9jbGFzczI+L3YyL3tjdXN0b21fY2xhc3MubmFtZT1wcm9qZWN0cy8qL2xvY2F0aW9ucy8qL2N1c3RvbUNsYXNzZXMvKn0SyQEKEURlbGV0ZUN1c3RvbUNsYXNzEjAuZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5EZWxldGVDdXN0b21DbGFzc1JlcXVlc3QaHS5nb29nbGUubG9uZ3J1bm5pbmcuT3BlcmF0aW9uImPKQSAKC0N1c3RvbUNsYXNzEhFPcGVyYXRpb25NZXRhZGF0YdpBBG5hbWWC0+STAjMqMS92Mi97bmFtZT1wcm9qZWN0cy8qL2xvY2F0aW9ucy8qL2N1c3RvbUNsYXNzZXMvKn0S2QEKE1VuZGVsZXRlQ3VzdG9tQ2xhc3MSMi5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlVuZGVsZXRlQ3VzdG9tQ2xhc3NSZXF1ZXN0Gh0uZ29vZ2xlLmxvbmdydW5uaW5nLk9wZXJhdGlvbiJvykEgCgtDdXN0b21DbGFzcxIRT3BlcmF0aW9uTWV0YWRhdGHaQQRuYW1lgtPkkwI/OgEqIjovdjIve25hbWU9cHJvamVjdHMvKi9sb2NhdGlvbnMvKi9jdXN0b21DbGFzc2VzLyp9OnVuZGVsZXRlEugBCg9DcmVhdGVQaHJhc2VTZXQSLi5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkNyZWF0ZVBocmFzZVNldFJlcXVlc3QaHS5nb29nbGUubG9uZ3J1bm5pbmcuT3BlcmF0aW9uIoUBykEeCglQaHJhc2VTZXQSEU9wZXJhdGlvbk1ldGFkYXRh2kEfcGFyZW50LHBocmFzZV9zZXQscGhyYXNlX3NldF9pZILT5JMCPDoKcGhyYXNlX3NldCIuL3YyL3twYXJlbnQ9cHJvamVjdHMvKi9sb2NhdGlvbnMvKn0vcGhyYXNlU2V0cxKwAQoOTGlzdFBocmFzZVNldHMSLS5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkxpc3RQaHJhc2VTZXRzUmVxdWVzdBouLmdvb2dsZS5jbG91ZC5zcGVlY2gudjIuTGlzdFBocmFzZVNldHNSZXNwb25zZSI/2kEGcGFyZW50gtPkkwIwEi4vdjIve3BhcmVudD1wcm9qZWN0cy8qL2xvY2F0aW9ucy8qfS9waHJhc2VTZXRzEp0BCgxHZXRQaHJhc2VTZXQSKy5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLkdldFBocmFzZVNldFJlcXVlc3QaIS5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlBocmFzZVNldCI92kEEbmFtZYLT5JMCMBIuL3YyL3tuYW1lPXByb2plY3RzLyovbG9jYXRpb25zLyovcGhyYXNlU2V0cy8qfRLqAQoPVXBkYXRlUGhyYXNlU2V0Ei4uZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5VcGRhdGVQaHJhc2VTZXRSZXF1ZXN0Gh0uZ29vZ2xlLmxvbmdydW5uaW5nLk9wZXJhdGlvbiKHAcpBHgoJUGhyYXNlU2V0EhFPcGVyYXRpb25NZXRhZGF0YdpBFnBocmFzZV9zZXQsdXBkYXRlX21hc2uC0+STAkc6CnBocmFzZV9zZXQyOS92Mi97cGhyYXNlX3NldC5uYW1lPXByb2plY3RzLyovbG9jYXRpb25zLyovcGhyYXNlU2V0cy8qfRLAAQoPRGVsZXRlUGhyYXNlU2V0Ei4uZ29vZ2xlLmNsb3VkLnNwZWVjaC52Mi5EZWxldGVQaHJhc2VTZXRSZXF1ZXN0Gh0uZ29vZ2xlLmxvbmdydW5uaW5nLk9wZXJhdGlvbiJeykEeCglQaHJhc2VTZXQSEU9wZXJhdGlvbk1ldGFkYXRh2kEEbmFtZYLT5JMCMCouL3YyL3tuYW1lPXByb2plY3RzLyovbG9jYXRpb25zLyovcGhyYXNlU2V0cy8qfRLQAQoRVW5kZWxldGVQaHJhc2VTZXQSMC5nb29nbGUuY2xvdWQuc3BlZWNoLnYyLlVuZGVsZXRlUGhyYXNlU2V0UmVxdWVzdBodLmdvb2dsZS5sb25ncnVubmluZy5PcGVyYXRpb24iaspBHgoJUGhyYXNlU2V0EhFPcGVyYXRpb25NZXRhZGF0YdpBBG5hbWWC0+STAjw6ASoiNy92Mi97bmFtZT1wcm9qZWN0cy8qL2xvY2F0aW9ucy8qL3BocmFzZVNldHMvKn06dW5kZWxldGUaScpBFXNwZWVjaC5nb29nbGVhcGlzLmNvbdJBLmh0dHBzOi8vd3d3Lmdvb2dsZWFwaXMuY29tL2F1dGgvY2xvdWQtcGxhdGZvcm1CiQMKGmNvbS5nb29nbGUuY2xvdWQuc3BlZWNoLnYyQhBDbG91ZFNwZWVjaFByb3RvUAFaMmNsb3VkLmdvb2dsZS5jb20vZ28vc3BlZWNoL2FwaXYyL3NwZWVjaHBiO3NwZWVjaHBi6kF4CiFjbG91ZGttcy5nb29nbGVhcGlzLmNvbS9DcnlwdG9LZXkSU3Byb2plY3RzL3twcm9qZWN0fS9sb2NhdGlvbnMve2xvY2F0aW9ufS9rZXlSaW5ncy97a2V5X3Jpbmd9L2NyeXB0b0tleXMve2NyeXB0b19rZXl96kGmAQooY2xvdWRrbXMuZ29vZ2xlYXBpcy5jb20vQ3J5cHRvS2V5VmVyc2lvbhJ6cHJvamVjdHMve3Byb2plY3R9L2xvY2F0aW9ucy97bG9jYXRpb259L2tleVJpbmdzL3trZXlfcmluZ30vY3J5cHRvS2V5cy97Y3J5cHRvX2tleX0vY3J5cHRvS2V5VmVyc2lvbnMve2NyeXB0b19rZXlfdmVyc2lvbn1iBnByb3RvMw", [file_google_api_annotations, file_google_api_client, file_google_api_field_behavior, file_google_api_field_info, file_google_api_resource, file_google_longrunning_operations, file_google_protobuf_duration, file_google_protobuf_field_mask, file_google_protobuf_timestamp, file_google_rpc_status]);

/**
 * Request message for the
 * [CreateRecognizer][google.cloud.speech.v2.Speech.CreateRecognizer] method.
 *
 * @generated from message google.cloud.speech.v2.CreateRecognizerRequest
 */
export type CreateRecognizerRequest = Message<"google.cloud.speech.v2.CreateRecognizerRequest"> & {
  /**
   * Required. The Recognizer to create.
   *
   * @generated from field: google.cloud.speech.v2.Recognizer recognizer = 1;
   */
  recognizer?: Recognizer;

  /**
   * If set, validate the request and preview the Recognizer, but do not
   * actually create it.
   *
   * @generated from field: bool validate_only = 2;
   */
  validateOnly: boolean;

  /**
   * The ID to use for the Recognizer, which will become the final component of
   * the Recognizer's resource name.
   *
   * This value should be 4-63 characters, and valid characters
   * are /[a-z][0-9]-/.
   *
   * @generated from field: string recognizer_id = 3;
   */
  recognizerId: string;

  /**
   * Required. The project and location where this Recognizer will be created.
   * The expected format is `projects/{project}/locations/{location}`.
   *
   * @generated from field: string parent = 4;
   */
  parent: string;
};

/**
 * Describes the message google.cloud.speech.v2.CreateRecognizerRequest.
 * Use `create(CreateRecognizerRequestSchema)` to create a new message.
 */
export const CreateRecognizerRequestSchema: GenMessage<CreateRecognizerRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 0);

/**
 * Represents the metadata of a long-running operation.
 *
 * @generated from message google.cloud.speech.v2.OperationMetadata
 */
export type OperationMetadata = Message<"google.cloud.speech.v2.OperationMetadata"> & {
  /**
   * The time the operation was created.
   *
   * @generated from field: google.protobuf.Timestamp create_time = 1;
   */
  createTime?: Timestamp;

  /**
   * The time the operation was last updated.
   *
   * @generated from field: google.protobuf.Timestamp update_time = 2;
   */
  updateTime?: Timestamp;

  /**
   * The resource path for the target of the operation.
   *
   * @generated from field: string resource = 3;
   */
  resource: string;

  /**
   * The method that triggered the operation.
   *
   * @generated from field: string method = 4;
   */
  method: string;

  /**
   * The [KMS key
   * name](https://cloud.google.com/kms/docs/resource-hierarchy#keys) with which
   * the content of the Operation is encrypted. The expected format is
   * `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}`.
   *
   * @generated from field: string kms_key_name = 6;
   */
  kmsKeyName: string;

  /**
   * The [KMS key version
   * name](https://cloud.google.com/kms/docs/resource-hierarchy#key_versions)
   * with which content of the Operation is encrypted. The expected format is
   * `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}/cryptoKeyVersions/{crypto_key_version}`.
   *
   * @generated from field: string kms_key_version_name = 7;
   */
  kmsKeyVersionName: string;

  /**
   * The request that spawned the Operation.
   *
   * @generated from oneof google.cloud.speech.v2.OperationMetadata.request
   */
  request: {
    /**
     * The BatchRecognizeRequest that spawned the Operation.
     *
     * @generated from field: google.cloud.speech.v2.BatchRecognizeRequest batch_recognize_request = 8;
     */
    value: BatchRecognizeRequest;
    case: "batchRecognizeRequest";
  } | {
    /**
     * The CreateRecognizerRequest that spawned the Operation.
     *
     * @generated from field: google.cloud.speech.v2.CreateRecognizerRequest create_recognizer_request = 9;
     */
    value: CreateRecognizerRequest;
    case: "createRecognizerRequest";
  } | {
    /**
     * The UpdateRecognizerRequest that spawned the Operation.
     *
     * @generated from field: google.cloud.speech.v2.UpdateRecognizerRequest update_recognizer_request = 10;
     */
    value: UpdateRecognizerRequest;
    case: "updateRecognizerRequest";
  } | {
    /**
     * The DeleteRecognizerRequest that spawned the Operation.
     *
     * @generated from field: google.cloud.speech.v2.DeleteRecognizerRequest delete_recognizer_request = 11;
     */
    value: DeleteRecognizerRequest;
    case: "deleteRecognizerRequest";
  } | {
    /**
     * The UndeleteRecognizerRequest that spawned the Operation.
     *
     * @generated from field: google.cloud.speech.v2.UndeleteRecognizerRequest undelete_recognizer_request = 12;
     */
    value: UndeleteRecognizerRequest;
    case: "undeleteRecognizerRequest";
  } | {
    /**
     * The CreateCustomClassRequest that spawned the Operation.
     *
     * @generated from field: google.cloud.speech.v2.CreateCustomClassRequest create_custom_class_request = 13;
     */
    value: CreateCustomClassRequest;
    case: "createCustomClassRequest";
  } | {
    /**
     * The UpdateCustomClassRequest that spawned the Operation.
     *
     * @generated from field: google.cloud.speech.v2.UpdateCustomClassRequest update_custom_class_request = 14;
     */
    value: UpdateCustomClassRequest;
    case: "updateCustomClassRequest";
  } | {
    /**
     * The DeleteCustomClassRequest that spawned the Operation.
     *
     * @generated from field: google.cloud.speech.v2.DeleteCustomClassRequest delete_custom_class_request = 15;
     */
    value: DeleteCustomClassRequest;
    case: "deleteCustomClassRequest";
  } | {
    /**
     * The UndeleteCustomClassRequest that spawned the Operation.
     *
     * @generated from field: google.cloud.speech.v2.UndeleteCustomClassRequest undelete_custom_class_request = 16;
     */
    value: UndeleteCustomClassRequest;
    case: "undeleteCustomClassRequest";
  } | {
    /**
     * The CreatePhraseSetRequest that spawned the Operation.
     *
     * @generated from field: google.cloud.speech.v2.CreatePhraseSetRequest create_phrase_set_request = 17;
     */
    value: CreatePhraseSetRequest;
    case: "createPhraseSetRequest";
  } | {
    /**
     * The UpdatePhraseSetRequest that spawned the Operation.
     *
     * @generated from field: google.cloud.speech.v2.UpdatePhraseSetRequest update_phrase_set_request = 18;
     */
    value: UpdatePhraseSetRequest;
    case: "updatePhraseSetRequest";
  } | {
    /**
     * The DeletePhraseSetRequest that spawned the Operation.
     *
     * @generated from field: google.cloud.speech.v2.DeletePhraseSetRequest delete_phrase_set_request = 19;
     */
    value: DeletePhraseSetRequest;
    case: "deletePhraseSetRequest";
  } | {
    /**
     * The UndeletePhraseSetRequest that spawned the Operation.
     *
     * @generated from field: google.cloud.speech.v2.UndeletePhraseSetRequest undelete_phrase_set_request = 20;
     */
    value: UndeletePhraseSetRequest;
    case: "undeletePhraseSetRequest";
  } | {
    /**
     * The UpdateConfigRequest that spawned the Operation.
     *
     * @generated from field: google.cloud.speech.v2.UpdateConfigRequest update_config_request = 21 [deprecated = true];
     * @deprecated
     */
    value: UpdateConfigRequest;
    case: "updateConfigRequest";
  } | { case: undefined; value?: undefined };

  /**
   * The percent progress of the Operation. Values can range from 0-100. If the
   * value is 100, then the operation is finished.
   *
   * @generated from field: int32 progress_percent = 22;
   */
  progressPercent: number;

  /**
   * Specific metadata per RPC.
   *
   * @generated from oneof google.cloud.speech.v2.OperationMetadata.metadata
   */
  metadata: {
    /**
     * Metadata specific to the BatchRecognize method.
     *
     * @generated from field: google.cloud.speech.v2.BatchRecognizeMetadata batch_recognize_metadata = 23;
     */
    value: BatchRecognizeMetadata;
    case: "batchRecognizeMetadata";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.speech.v2.OperationMetadata.
 * Use `create(OperationMetadataSchema)` to create a new message.
 */
export const OperationMetadataSchema: GenMessage<OperationMetadata> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 1);

/**
 * Request message for the
 * [ListRecognizers][google.cloud.speech.v2.Speech.ListRecognizers] method.
 *
 * @generated from message google.cloud.speech.v2.ListRecognizersRequest
 */
export type ListRecognizersRequest = Message<"google.cloud.speech.v2.ListRecognizersRequest"> & {
  /**
   * Required. The project and location of Recognizers to list. The expected
   * format is `projects/{project}/locations/{location}`.
   *
   * @generated from field: string parent = 1;
   */
  parent: string;

  /**
   * The maximum number of Recognizers to return. The service may return fewer
   * than this value. If unspecified, at most 5 Recognizers will be returned.
   * The maximum value is 100; values above 100 will be coerced to 100.
   *
   * @generated from field: int32 page_size = 2;
   */
  pageSize: number;

  /**
   * A page token, received from a previous
   * [ListRecognizers][google.cloud.speech.v2.Speech.ListRecognizers] call.
   * Provide this to retrieve the subsequent page.
   *
   * When paginating, all other parameters provided to
   * [ListRecognizers][google.cloud.speech.v2.Speech.ListRecognizers] must match
   * the call that provided the page token.
   *
   * @generated from field: string page_token = 3;
   */
  pageToken: string;

  /**
   * Whether, or not, to show resources that have been deleted.
   *
   * @generated from field: bool show_deleted = 4;
   */
  showDeleted: boolean;
};

/**
 * Describes the message google.cloud.speech.v2.ListRecognizersRequest.
 * Use `create(ListRecognizersRequestSchema)` to create a new message.
 */
export const ListRecognizersRequestSchema: GenMessage<ListRecognizersRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 2);

/**
 * Response message for the
 * [ListRecognizers][google.cloud.speech.v2.Speech.ListRecognizers] method.
 *
 * @generated from message google.cloud.speech.v2.ListRecognizersResponse
 */
export type ListRecognizersResponse = Message<"google.cloud.speech.v2.ListRecognizersResponse"> & {
  /**
   * The list of requested Recognizers.
   *
   * @generated from field: repeated google.cloud.speech.v2.Recognizer recognizers = 1;
   */
  recognizers: Recognizer[];

  /**
   * A token, which can be sent as
   * [page_token][google.cloud.speech.v2.ListRecognizersRequest.page_token] to
   * retrieve the next page. If this field is omitted, there are no subsequent
   * pages. This token expires after 72 hours.
   *
   * @generated from field: string next_page_token = 2;
   */
  nextPageToken: string;
};

/**
 * Describes the message google.cloud.speech.v2.ListRecognizersResponse.
 * Use `create(ListRecognizersResponseSchema)` to create a new message.
 */
export const ListRecognizersResponseSchema: GenMessage<ListRecognizersResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 3);

/**
 * Request message for the
 * [GetRecognizer][google.cloud.speech.v2.Speech.GetRecognizer] method.
 *
 * @generated from message google.cloud.speech.v2.GetRecognizerRequest
 */
export type GetRecognizerRequest = Message<"google.cloud.speech.v2.GetRecognizerRequest"> & {
  /**
   * Required. The name of the Recognizer to retrieve. The expected format is
   * `projects/{project}/locations/{location}/recognizers/{recognizer}`.
   *
   * @generated from field: string name = 1;
   */
  name: string;
};

/**
 * Describes the message google.cloud.speech.v2.GetRecognizerRequest.
 * Use `create(GetRecognizerRequestSchema)` to create a new message.
 */
export const GetRecognizerRequestSchema: GenMessage<GetRecognizerRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 4);

/**
 * Request message for the
 * [UpdateRecognizer][google.cloud.speech.v2.Speech.UpdateRecognizer] method.
 *
 * @generated from message google.cloud.speech.v2.UpdateRecognizerRequest
 */
export type UpdateRecognizerRequest = Message<"google.cloud.speech.v2.UpdateRecognizerRequest"> & {
  /**
   * Required. The Recognizer to update.
   *
   * The Recognizer's `name` field is used to identify the Recognizer to update.
   * Format: `projects/{project}/locations/{location}/recognizers/{recognizer}`.
   *
   * @generated from field: google.cloud.speech.v2.Recognizer recognizer = 1;
   */
  recognizer?: Recognizer;

  /**
   * The list of fields to update. If empty, all non-default valued fields are
   * considered for update. Use `*` to update the entire Recognizer resource.
   *
   * @generated from field: google.protobuf.FieldMask update_mask = 2;
   */
  updateMask?: FieldMask;

  /**
   * If set, validate the request and preview the updated Recognizer, but do not
   * actually update it.
   *
   * @generated from field: bool validate_only = 4;
   */
  validateOnly: boolean;
};

/**
 * Describes the message google.cloud.speech.v2.UpdateRecognizerRequest.
 * Use `create(UpdateRecognizerRequestSchema)` to create a new message.
 */
export const UpdateRecognizerRequestSchema: GenMessage<UpdateRecognizerRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 5);

/**
 * Request message for the
 * [DeleteRecognizer][google.cloud.speech.v2.Speech.DeleteRecognizer] method.
 *
 * @generated from message google.cloud.speech.v2.DeleteRecognizerRequest
 */
export type DeleteRecognizerRequest = Message<"google.cloud.speech.v2.DeleteRecognizerRequest"> & {
  /**
   * Required. The name of the Recognizer to delete.
   * Format: `projects/{project}/locations/{location}/recognizers/{recognizer}`
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * If set, validate the request and preview the deleted Recognizer, but do not
   * actually delete it.
   *
   * @generated from field: bool validate_only = 2;
   */
  validateOnly: boolean;

  /**
   * If set to true, and the Recognizer is not found, the request will succeed
   * and  be a no-op (no Operation is recorded in this case).
   *
   * @generated from field: bool allow_missing = 4;
   */
  allowMissing: boolean;

  /**
   * This checksum is computed by the server based on the value of other
   * fields. This may be sent on update, undelete, and delete requests to ensure
   * the client has an up-to-date value before proceeding.
   *
   * @generated from field: string etag = 3;
   */
  etag: string;
};

/**
 * Describes the message google.cloud.speech.v2.DeleteRecognizerRequest.
 * Use `create(DeleteRecognizerRequestSchema)` to create a new message.
 */
export const DeleteRecognizerRequestSchema: GenMessage<DeleteRecognizerRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 6);

/**
 * Request message for the
 * [UndeleteRecognizer][google.cloud.speech.v2.Speech.UndeleteRecognizer]
 * method.
 *
 * @generated from message google.cloud.speech.v2.UndeleteRecognizerRequest
 */
export type UndeleteRecognizerRequest = Message<"google.cloud.speech.v2.UndeleteRecognizerRequest"> & {
  /**
   * Required. The name of the Recognizer to undelete.
   * Format: `projects/{project}/locations/{location}/recognizers/{recognizer}`
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * If set, validate the request and preview the undeleted Recognizer, but do
   * not actually undelete it.
   *
   * @generated from field: bool validate_only = 3;
   */
  validateOnly: boolean;

  /**
   * This checksum is computed by the server based on the value of other
   * fields. This may be sent on update, undelete, and delete requests to ensure
   * the client has an up-to-date value before proceeding.
   *
   * @generated from field: string etag = 4;
   */
  etag: string;
};

/**
 * Describes the message google.cloud.speech.v2.UndeleteRecognizerRequest.
 * Use `create(UndeleteRecognizerRequestSchema)` to create a new message.
 */
export const UndeleteRecognizerRequestSchema: GenMessage<UndeleteRecognizerRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 7);

/**
 * A Recognizer message. Stores recognition configuration and metadata.
 *
 * @generated from message google.cloud.speech.v2.Recognizer
 */
export type Recognizer = Message<"google.cloud.speech.v2.Recognizer"> & {
  /**
   * Output only. Identifier. The resource name of the Recognizer.
   * Format: `projects/{project}/locations/{location}/recognizers/{recognizer}`.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Output only. System-assigned unique identifier for the Recognizer.
   *
   * @generated from field: string uid = 2;
   */
  uid: string;

  /**
   * User-settable, human-readable name for the Recognizer. Must be 63
   * characters or less.
   *
   * @generated from field: string display_name = 3;
   */
  displayName: string;

  /**
   * Optional. This field is now deprecated. Prefer the
   * [`model`][google.cloud.speech.v2.RecognitionConfig.model] field in the
   * [`RecognitionConfig`][google.cloud.speech.v2.RecognitionConfig] message.
   *
   * Which model to use for recognition requests. Select the model best suited
   * to your domain to get best results.
   *
   * Guidance for choosing which model to use can be found in the [Transcription
   * Models
   * Documentation](https://cloud.google.com/speech-to-text/v2/docs/transcription-model)
   * and the models supported in each region can be found in the [Table Of
   * Supported
   * Models](https://cloud.google.com/speech-to-text/v2/docs/speech-to-text-supported-languages).
   *
   * @generated from field: string model = 4 [deprecated = true];
   * @deprecated
   */
  model: string;

  /**
   * Optional. This field is now deprecated. Prefer the
   * [`language_codes`][google.cloud.speech.v2.RecognitionConfig.language_codes]
   * field in the
   * [`RecognitionConfig`][google.cloud.speech.v2.RecognitionConfig] message.
   *
   * The language of the supplied audio as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   *
   * Supported languages for each model are listed in the [Table of Supported
   * Models](https://cloud.google.com/speech-to-text/v2/docs/speech-to-text-supported-languages).
   *
   * If additional languages are provided, recognition result will contain
   * recognition in the most likely language detected. The recognition result
   * will include the language tag of the language detected in the audio.
   * When you create or update a Recognizer, these values are
   * stored in normalized BCP-47 form. For example, "en-us" is stored as
   * "en-US".
   *
   * @generated from field: repeated string language_codes = 17 [deprecated = true];
   * @deprecated
   */
  languageCodes: string[];

  /**
   * Default configuration to use for requests with this Recognizer.
   * This can be overwritten by inline configuration in the
   * [RecognizeRequest.config][google.cloud.speech.v2.RecognizeRequest.config]
   * field.
   *
   * @generated from field: google.cloud.speech.v2.RecognitionConfig default_recognition_config = 6;
   */
  defaultRecognitionConfig?: RecognitionConfig;

  /**
   * Allows users to store small amounts of arbitrary data.
   * Both the key and the value must be 63 characters or less each.
   * At most 100 annotations.
   *
   * @generated from field: map<string, string> annotations = 7;
   */
  annotations: { [key: string]: string };

  /**
   * Output only. The Recognizer lifecycle state.
   *
   * @generated from field: google.cloud.speech.v2.Recognizer.State state = 8;
   */
  state: Recognizer_State;

  /**
   * Output only. Creation time.
   *
   * @generated from field: google.protobuf.Timestamp create_time = 9;
   */
  createTime?: Timestamp;

  /**
   * Output only. The most recent time this Recognizer was modified.
   *
   * @generated from field: google.protobuf.Timestamp update_time = 10;
   */
  updateTime?: Timestamp;

  /**
   * Output only. The time at which this Recognizer was requested for deletion.
   *
   * @generated from field: google.protobuf.Timestamp delete_time = 11;
   */
  deleteTime?: Timestamp;

  /**
   * Output only. The time at which this Recognizer will be purged.
   *
   * @generated from field: google.protobuf.Timestamp expire_time = 14;
   */
  expireTime?: Timestamp;

  /**
   * Output only. This checksum is computed by the server based on the value of
   * other fields. This may be sent on update, undelete, and delete requests to
   * ensure the client has an up-to-date value before proceeding.
   *
   * @generated from field: string etag = 12;
   */
  etag: string;

  /**
   * Output only. Whether or not this Recognizer is in the process of being
   * updated.
   *
   * @generated from field: bool reconciling = 13;
   */
  reconciling: boolean;

  /**
   * Output only. The [KMS key
   * name](https://cloud.google.com/kms/docs/resource-hierarchy#keys) with which
   * the Recognizer is encrypted. The expected format is
   * `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}`.
   *
   * @generated from field: string kms_key_name = 15;
   */
  kmsKeyName: string;

  /**
   * Output only. The [KMS key version
   * name](https://cloud.google.com/kms/docs/resource-hierarchy#key_versions)
   * with which the Recognizer is encrypted. The expected format is
   * `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}/cryptoKeyVersions/{crypto_key_version}`.
   *
   * @generated from field: string kms_key_version_name = 16;
   */
  kmsKeyVersionName: string;
};

/**
 * Describes the message google.cloud.speech.v2.Recognizer.
 * Use `create(RecognizerSchema)` to create a new message.
 */
export const RecognizerSchema: GenMessage<Recognizer> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 8);

/**
 * Set of states that define the lifecycle of a Recognizer.
 *
 * @generated from enum google.cloud.speech.v2.Recognizer.State
 */
export enum Recognizer_State {
  /**
   * The default value. This value is used if the state is omitted.
   *
   * @generated from enum value: STATE_UNSPECIFIED = 0;
   */
  STATE_UNSPECIFIED = 0,

  /**
   * The Recognizer is active and ready for use.
   *
   * @generated from enum value: ACTIVE = 2;
   */
  ACTIVE = 2,

  /**
   * This Recognizer has been deleted.
   *
   * @generated from enum value: DELETED = 4;
   */
  DELETED = 4,
}

/**
 * Describes the enum google.cloud.speech.v2.Recognizer.State.
 */
export const Recognizer_StateSchema: GenEnum<Recognizer_State> = /*@__PURE__*/
  enumDesc(file_google_cloud_speech_v2_cloud_speech, 8, 0);

/**
 * Automatically detected decoding parameters.
 * Supported for the following encodings:
 *
 * * WAV_LINEAR16: 16-bit signed little-endian PCM samples in a WAV container.
 *
 * * WAV_MULAW: 8-bit companded mulaw samples in a WAV container.
 *
 * * WAV_ALAW: 8-bit companded alaw samples in a WAV container.
 *
 * * RFC4867_5_AMR: AMR frames with an rfc4867.5 header.
 *
 * * RFC4867_5_AMRWB: AMR-WB frames with an rfc4867.5 header.
 *
 * * FLAC: FLAC frames in the "native FLAC" container format.
 *
 * * MP3: MPEG audio frames with optional (ignored) ID3 metadata.
 *
 * * OGG_OPUS: Opus audio frames in an Ogg container.
 *
 * * WEBM_OPUS: Opus audio frames in a WebM container.
 *
 * * MP4_AAC: AAC audio frames in an MP4 container.
 *
 * * M4A_AAC: AAC audio frames in an M4A container.
 *
 * * MOV_AAC: AAC audio frames in an MOV container.
 *
 * @generated from message google.cloud.speech.v2.AutoDetectDecodingConfig
 */
export type AutoDetectDecodingConfig = Message<"google.cloud.speech.v2.AutoDetectDecodingConfig"> & {
};

/**
 * Describes the message google.cloud.speech.v2.AutoDetectDecodingConfig.
 * Use `create(AutoDetectDecodingConfigSchema)` to create a new message.
 */
export const AutoDetectDecodingConfigSchema: GenMessage<AutoDetectDecodingConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 9);

/**
 * Explicitly specified decoding parameters.
 *
 * @generated from message google.cloud.speech.v2.ExplicitDecodingConfig
 */
export type ExplicitDecodingConfig = Message<"google.cloud.speech.v2.ExplicitDecodingConfig"> & {
  /**
   * Required. Encoding of the audio data sent for recognition.
   *
   * @generated from field: google.cloud.speech.v2.ExplicitDecodingConfig.AudioEncoding encoding = 1;
   */
  encoding: ExplicitDecodingConfig_AudioEncoding;

  /**
   * Sample rate in Hertz of the audio data sent for recognition. Valid
   * values are: 8000-48000. 16000 is optimal. For best results, set the
   * sampling rate of the audio source to 16000 Hz. If that's not possible, use
   * the native sample rate of the audio source (instead of re-sampling).
   * Supported for the following encodings:
   *
   * * LINEAR16: Headerless 16-bit signed little-endian PCM samples.
   *
   * * MULAW: Headerless 8-bit companded mulaw samples.
   *
   * * ALAW: Headerless 8-bit companded alaw samples.
   *
   * @generated from field: int32 sample_rate_hertz = 2;
   */
  sampleRateHertz: number;

  /**
   * Number of channels present in the audio data sent for recognition.
   * Supported for the following encodings:
   *
   * * LINEAR16: Headerless 16-bit signed little-endian PCM samples.
   *
   * * MULAW: Headerless 8-bit companded mulaw samples.
   *
   * * ALAW: Headerless 8-bit companded alaw samples.
   *
   * The maximum allowed value is 8.
   *
   * @generated from field: int32 audio_channel_count = 3;
   */
  audioChannelCount: number;
};

/**
 * Describes the message google.cloud.speech.v2.ExplicitDecodingConfig.
 * Use `create(ExplicitDecodingConfigSchema)` to create a new message.
 */
export const ExplicitDecodingConfigSchema: GenMessage<ExplicitDecodingConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 10);

/**
 * Supported audio data encodings.
 *
 * @generated from enum google.cloud.speech.v2.ExplicitDecodingConfig.AudioEncoding
 */
export enum ExplicitDecodingConfig_AudioEncoding {
  /**
   * Default value. This value is unused.
   *
   * @generated from enum value: AUDIO_ENCODING_UNSPECIFIED = 0;
   */
  AUDIO_ENCODING_UNSPECIFIED = 0,

  /**
   * Headerless 16-bit signed little-endian PCM samples.
   *
   * @generated from enum value: LINEAR16 = 1;
   */
  LINEAR16 = 1,

  /**
   * Headerless 8-bit companded mulaw samples.
   *
   * @generated from enum value: MULAW = 2;
   */
  MULAW = 2,

  /**
   * Headerless 8-bit companded alaw samples.
   *
   * @generated from enum value: ALAW = 3;
   */
  ALAW = 3,
}

/**
 * Describes the enum google.cloud.speech.v2.ExplicitDecodingConfig.AudioEncoding.
 */
export const ExplicitDecodingConfig_AudioEncodingSchema: GenEnum<ExplicitDecodingConfig_AudioEncoding> = /*@__PURE__*/
  enumDesc(file_google_cloud_speech_v2_cloud_speech, 10, 0);

/**
 * Configuration to enable speaker diarization.
 *
 * @generated from message google.cloud.speech.v2.SpeakerDiarizationConfig
 */
export type SpeakerDiarizationConfig = Message<"google.cloud.speech.v2.SpeakerDiarizationConfig"> & {
  /**
   * Required. Minimum number of speakers in the conversation. This range gives
   * you more flexibility by allowing the system to automatically determine the
   * correct number of speakers.
   *
   * To fix the number of speakers detected in the audio, set
   * `min_speaker_count` = `max_speaker_count`.
   *
   * @generated from field: int32 min_speaker_count = 2;
   */
  minSpeakerCount: number;

  /**
   * Required. Maximum number of speakers in the conversation. Valid values are:
   * 1-6. Must be >= `min_speaker_count`. This range gives you more flexibility
   * by allowing the system to automatically determine the correct number of
   * speakers.
   *
   * @generated from field: int32 max_speaker_count = 3;
   */
  maxSpeakerCount: number;
};

/**
 * Describes the message google.cloud.speech.v2.SpeakerDiarizationConfig.
 * Use `create(SpeakerDiarizationConfigSchema)` to create a new message.
 */
export const SpeakerDiarizationConfigSchema: GenMessage<SpeakerDiarizationConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 11);

/**
 * Available recognition features.
 *
 * @generated from message google.cloud.speech.v2.RecognitionFeatures
 */
export type RecognitionFeatures = Message<"google.cloud.speech.v2.RecognitionFeatures"> & {
  /**
   * If set to `true`, the server will attempt to filter out profanities,
   * replacing all but the initial character in each filtered word with
   * asterisks, for instance, "f***". If set to `false` or omitted, profanities
   * won't be filtered out.
   *
   * @generated from field: bool profanity_filter = 1;
   */
  profanityFilter: boolean;

  /**
   * If `true`, the top result includes a list of words and the start and end
   * time offsets (timestamps) for those words. If `false`, no word-level time
   * offset information is returned. The default is `false`.
   *
   * @generated from field: bool enable_word_time_offsets = 2;
   */
  enableWordTimeOffsets: boolean;

  /**
   * If `true`, the top result includes a list of words and the confidence for
   * those words. If `false`, no word-level confidence information is returned.
   * The default is `false`.
   *
   * @generated from field: bool enable_word_confidence = 3;
   */
  enableWordConfidence: boolean;

  /**
   * If `true`, adds punctuation to recognition result hypotheses. This feature
   * is only available in select languages. The default `false` value does not
   * add punctuation to result hypotheses.
   *
   * @generated from field: bool enable_automatic_punctuation = 4;
   */
  enableAutomaticPunctuation: boolean;

  /**
   * The spoken punctuation behavior for the call. If `true`, replaces spoken
   * punctuation with the corresponding symbols in the request. For example,
   * "how are you question mark" becomes "how are you?". See
   * https://cloud.google.com/speech-to-text/docs/spoken-punctuation for
   * support. If `false`, spoken punctuation is not replaced.
   *
   * @generated from field: bool enable_spoken_punctuation = 14;
   */
  enableSpokenPunctuation: boolean;

  /**
   * The spoken emoji behavior for the call. If `true`, adds spoken emoji
   * formatting for the request. This will replace spoken emojis with the
   * corresponding Unicode symbols in the final transcript. If `false`, spoken
   * emojis are not replaced.
   *
   * @generated from field: bool enable_spoken_emojis = 15;
   */
  enableSpokenEmojis: boolean;

  /**
   * Mode for recognizing multi-channel audio.
   *
   * @generated from field: google.cloud.speech.v2.RecognitionFeatures.MultiChannelMode multi_channel_mode = 17;
   */
  multiChannelMode: RecognitionFeatures_MultiChannelMode;

  /**
   * Configuration to enable speaker diarization and set additional
   * parameters to make diarization better suited for your application.
   * When this is enabled, we send all the words from the beginning of the
   * audio for the top alternative in every consecutive STREAMING responses.
   * This is done in order to improve our speaker tags as our models learn to
   * identify the speakers in the conversation over time.
   * For non-streaming requests, the diarization results will be provided only
   * in the top alternative of the FINAL SpeechRecognitionResult.
   *
   * @generated from field: google.cloud.speech.v2.SpeakerDiarizationConfig diarization_config = 9;
   */
  diarizationConfig?: SpeakerDiarizationConfig;

  /**
   * Maximum number of recognition hypotheses to be returned.
   * The server may return fewer than `max_alternatives`.
   * Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
   * one. If omitted, will return a maximum of one.
   *
   * @generated from field: int32 max_alternatives = 16;
   */
  maxAlternatives: number;
};

/**
 * Describes the message google.cloud.speech.v2.RecognitionFeatures.
 * Use `create(RecognitionFeaturesSchema)` to create a new message.
 */
export const RecognitionFeaturesSchema: GenMessage<RecognitionFeatures> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 12);

/**
 * Options for how to recognize multi-channel audio.
 *
 * @generated from enum google.cloud.speech.v2.RecognitionFeatures.MultiChannelMode
 */
export enum RecognitionFeatures_MultiChannelMode {
  /**
   * Default value for the multi-channel mode. If the audio contains
   * multiple channels, only the first channel will be transcribed; other
   * channels will be ignored.
   *
   * @generated from enum value: MULTI_CHANNEL_MODE_UNSPECIFIED = 0;
   */
  MULTI_CHANNEL_MODE_UNSPECIFIED = 0,

  /**
   * If selected, each channel in the provided audio is transcribed
   * independently. This cannot be selected if the selected
   * [model][google.cloud.speech.v2.Recognizer.model] is `latest_short`.
   *
   * @generated from enum value: SEPARATE_RECOGNITION_PER_CHANNEL = 1;
   */
  SEPARATE_RECOGNITION_PER_CHANNEL = 1,
}

/**
 * Describes the enum google.cloud.speech.v2.RecognitionFeatures.MultiChannelMode.
 */
export const RecognitionFeatures_MultiChannelModeSchema: GenEnum<RecognitionFeatures_MultiChannelMode> = /*@__PURE__*/
  enumDesc(file_google_cloud_speech_v2_cloud_speech, 12, 0);

/**
 * Transcription normalization configuration. Use transcription normalization
 * to automatically replace parts of the transcript with phrases of your
 * choosing. For StreamingRecognize, this normalization only applies to stable
 * partial transcripts (stability > 0.8) and final transcripts.
 *
 * @generated from message google.cloud.speech.v2.TranscriptNormalization
 */
export type TranscriptNormalization = Message<"google.cloud.speech.v2.TranscriptNormalization"> & {
  /**
   * A list of replacement entries. We will perform replacement with one entry
   * at a time. For example, the second entry in ["cat" => "dog", "mountain cat"
   * => "mountain dog"] will never be applied because we will always process the
   * first entry before it. At most 100 entries.
   *
   * @generated from field: repeated google.cloud.speech.v2.TranscriptNormalization.Entry entries = 1;
   */
  entries: TranscriptNormalization_Entry[];
};

/**
 * Describes the message google.cloud.speech.v2.TranscriptNormalization.
 * Use `create(TranscriptNormalizationSchema)` to create a new message.
 */
export const TranscriptNormalizationSchema: GenMessage<TranscriptNormalization> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 13);

/**
 * A single replacement configuration.
 *
 * @generated from message google.cloud.speech.v2.TranscriptNormalization.Entry
 */
export type TranscriptNormalization_Entry = Message<"google.cloud.speech.v2.TranscriptNormalization.Entry"> & {
  /**
   * What to replace. Max length is 100 characters.
   *
   * @generated from field: string search = 1;
   */
  search: string;

  /**
   * What to replace with. Max length is 100 characters.
   *
   * @generated from field: string replace = 2;
   */
  replace: string;

  /**
   * Whether the search is case sensitive.
   *
   * @generated from field: bool case_sensitive = 3;
   */
  caseSensitive: boolean;
};

/**
 * Describes the message google.cloud.speech.v2.TranscriptNormalization.Entry.
 * Use `create(TranscriptNormalization_EntrySchema)` to create a new message.
 */
export const TranscriptNormalization_EntrySchema: GenMessage<TranscriptNormalization_Entry> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 13, 0);

/**
 * Translation configuration. Use to translate the given audio into text for the
 * desired language.
 *
 * @generated from message google.cloud.speech.v2.TranslationConfig
 */
export type TranslationConfig = Message<"google.cloud.speech.v2.TranslationConfig"> & {
  /**
   * Required. The language code to translate to.
   *
   * @generated from field: string target_language = 1;
   */
  targetLanguage: string;
};

/**
 * Describes the message google.cloud.speech.v2.TranslationConfig.
 * Use `create(TranslationConfigSchema)` to create a new message.
 */
export const TranslationConfigSchema: GenMessage<TranslationConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 14);

/**
 * Provides "hints" to the speech recognizer to favor specific words and phrases
 * in the results. PhraseSets can be specified as an inline resource, or a
 * reference to an existing PhraseSet resource.
 *
 * @generated from message google.cloud.speech.v2.SpeechAdaptation
 */
export type SpeechAdaptation = Message<"google.cloud.speech.v2.SpeechAdaptation"> & {
  /**
   * A list of inline or referenced PhraseSets.
   *
   * @generated from field: repeated google.cloud.speech.v2.SpeechAdaptation.AdaptationPhraseSet phrase_sets = 1;
   */
  phraseSets: SpeechAdaptation_AdaptationPhraseSet[];

  /**
   * A list of inline CustomClasses. Existing CustomClass resources can be
   * referenced directly in a PhraseSet.
   *
   * @generated from field: repeated google.cloud.speech.v2.CustomClass custom_classes = 2;
   */
  customClasses: CustomClass[];
};

/**
 * Describes the message google.cloud.speech.v2.SpeechAdaptation.
 * Use `create(SpeechAdaptationSchema)` to create a new message.
 */
export const SpeechAdaptationSchema: GenMessage<SpeechAdaptation> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 15);

/**
 * A biasing PhraseSet, which can be either a string referencing the name of
 * an existing PhraseSets resource, or an inline definition of a PhraseSet.
 *
 * @generated from message google.cloud.speech.v2.SpeechAdaptation.AdaptationPhraseSet
 */
export type SpeechAdaptation_AdaptationPhraseSet = Message<"google.cloud.speech.v2.SpeechAdaptation.AdaptationPhraseSet"> & {
  /**
   * @generated from oneof google.cloud.speech.v2.SpeechAdaptation.AdaptationPhraseSet.value
   */
  value: {
    /**
     * The name of an existing PhraseSet resource. The user must have read
     * access to the resource and it must not be deleted.
     *
     * @generated from field: string phrase_set = 1;
     */
    value: string;
    case: "phraseSet";
  } | {
    /**
     * An inline defined PhraseSet.
     *
     * @generated from field: google.cloud.speech.v2.PhraseSet inline_phrase_set = 2;
     */
    value: PhraseSet;
    case: "inlinePhraseSet";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.speech.v2.SpeechAdaptation.AdaptationPhraseSet.
 * Use `create(SpeechAdaptation_AdaptationPhraseSetSchema)` to create a new message.
 */
export const SpeechAdaptation_AdaptationPhraseSetSchema: GenMessage<SpeechAdaptation_AdaptationPhraseSet> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 15, 0);

/**
 * Provides information to the Recognizer that specifies how to process the
 * recognition request.
 *
 * @generated from message google.cloud.speech.v2.RecognitionConfig
 */
export type RecognitionConfig = Message<"google.cloud.speech.v2.RecognitionConfig"> & {
  /**
   * Decoding parameters for audio being sent for recognition.
   *
   * @generated from oneof google.cloud.speech.v2.RecognitionConfig.decoding_config
   */
  decodingConfig: {
    /**
     * Automatically detect decoding parameters.
     * Preferred for supported formats.
     *
     * @generated from field: google.cloud.speech.v2.AutoDetectDecodingConfig auto_decoding_config = 7;
     */
    value: AutoDetectDecodingConfig;
    case: "autoDecodingConfig";
  } | {
    /**
     * Explicitly specified decoding parameters.
     * Required if using headerless PCM audio (linear16, mulaw, alaw).
     *
     * @generated from field: google.cloud.speech.v2.ExplicitDecodingConfig explicit_decoding_config = 8;
     */
    value: ExplicitDecodingConfig;
    case: "explicitDecodingConfig";
  } | { case: undefined; value?: undefined };

  /**
   * Optional. Which model to use for recognition requests. Select the model
   * best suited to your domain to get best results.
   *
   * Guidance for choosing which model to use can be found in the [Transcription
   * Models
   * Documentation](https://cloud.google.com/speech-to-text/v2/docs/transcription-model)
   * and the models supported in each region can be found in the [Table Of
   * Supported
   * Models](https://cloud.google.com/speech-to-text/v2/docs/speech-to-text-supported-languages).
   *
   * @generated from field: string model = 9;
   */
  model: string;

  /**
   * Optional. The language of the supplied audio as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Language tags are normalized to BCP-47 before they are used eg "en-us"
   * becomes "en-US".
   *
   * Supported languages for each model are listed in the [Table of Supported
   * Models](https://cloud.google.com/speech-to-text/v2/docs/speech-to-text-supported-languages).
   *
   * If additional languages are provided, recognition result will contain
   * recognition in the most likely language detected. The recognition result
   * will include the language tag of the language detected in the audio.
   *
   * @generated from field: repeated string language_codes = 10;
   */
  languageCodes: string[];

  /**
   * Speech recognition features to enable.
   *
   * @generated from field: google.cloud.speech.v2.RecognitionFeatures features = 2;
   */
  features?: RecognitionFeatures;

  /**
   * Speech adaptation context that weights recognizer predictions for specific
   * words and phrases.
   *
   * @generated from field: google.cloud.speech.v2.SpeechAdaptation adaptation = 6;
   */
  adaptation?: SpeechAdaptation;

  /**
   * Optional. Use transcription normalization to automatically replace parts of
   * the transcript with phrases of your choosing. For StreamingRecognize, this
   * normalization only applies to stable partial transcripts (stability > 0.8)
   * and final transcripts.
   *
   * @generated from field: google.cloud.speech.v2.TranscriptNormalization transcript_normalization = 11;
   */
  transcriptNormalization?: TranscriptNormalization;

  /**
   * Optional. Optional configuration used to automatically run translation on
   * the given audio to the desired language for supported models.
   *
   * @generated from field: google.cloud.speech.v2.TranslationConfig translation_config = 15;
   */
  translationConfig?: TranslationConfig;
};

/**
 * Describes the message google.cloud.speech.v2.RecognitionConfig.
 * Use `create(RecognitionConfigSchema)` to create a new message.
 */
export const RecognitionConfigSchema: GenMessage<RecognitionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 16);

/**
 * Request message for the
 * [Recognize][google.cloud.speech.v2.Speech.Recognize] method. Either
 * `content` or `uri` must be supplied. Supplying both or neither returns
 * [INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]. See [content
 * limits](https://cloud.google.com/speech-to-text/quotas#content).
 *
 * @generated from message google.cloud.speech.v2.RecognizeRequest
 */
export type RecognizeRequest = Message<"google.cloud.speech.v2.RecognizeRequest"> & {
  /**
   * Required. The name of the Recognizer to use during recognition. The
   * expected format is
   * `projects/{project}/locations/{location}/recognizers/{recognizer}`. The
   * {recognizer} segment may be set to `_` to use an empty implicit Recognizer.
   *
   * @generated from field: string recognizer = 3;
   */
  recognizer: string;

  /**
   * Features and audio metadata to use for the Automatic Speech Recognition.
   * This field in combination with the
   * [config_mask][google.cloud.speech.v2.RecognizeRequest.config_mask] field
   * can be used to override parts of the
   * [default_recognition_config][google.cloud.speech.v2.Recognizer.default_recognition_config]
   * of the Recognizer resource.
   *
   * @generated from field: google.cloud.speech.v2.RecognitionConfig config = 1;
   */
  config?: RecognitionConfig;

  /**
   * The list of fields in
   * [config][google.cloud.speech.v2.RecognizeRequest.config] that override the
   * values in the
   * [default_recognition_config][google.cloud.speech.v2.Recognizer.default_recognition_config]
   * of the recognizer during this recognition request. If no mask is provided,
   * all non-default valued fields in
   * [config][google.cloud.speech.v2.RecognizeRequest.config] override the
   * values in the recognizer for this recognition request. If a mask is
   * provided, only the fields listed in the mask override the config in the
   * recognizer for this recognition request. If a wildcard (`*`) is provided,
   * [config][google.cloud.speech.v2.RecognizeRequest.config] completely
   * overrides and replaces the config in the recognizer for this recognition
   * request.
   *
   * @generated from field: google.protobuf.FieldMask config_mask = 8;
   */
  configMask?: FieldMask;

  /**
   * The audio source, which is either inline content or a Google Cloud
   * Storage URI.
   *
   * @generated from oneof google.cloud.speech.v2.RecognizeRequest.audio_source
   */
  audioSource: {
    /**
     * The audio data bytes encoded as specified in
     * [RecognitionConfig][google.cloud.speech.v2.RecognitionConfig]. As
     * with all bytes fields, proto buffers use a pure binary representation,
     * whereas JSON representations use base64.
     *
     * @generated from field: bytes content = 5;
     */
    value: Uint8Array;
    case: "content";
  } | {
    /**
     * URI that points to a file that contains audio data bytes as specified in
     * [RecognitionConfig][google.cloud.speech.v2.RecognitionConfig]. The file
     * must not be compressed (for example, gzip). Currently, only Google Cloud
     * Storage URIs are supported, which must be specified in the following
     * format: `gs://bucket_name/object_name` (other URI formats return
     * [INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more
     * information, see [Request
     * URIs](https://cloud.google.com/storage/docs/reference-uris).
     *
     * @generated from field: string uri = 6;
     */
    value: string;
    case: "uri";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.speech.v2.RecognizeRequest.
 * Use `create(RecognizeRequestSchema)` to create a new message.
 */
export const RecognizeRequestSchema: GenMessage<RecognizeRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 17);

/**
 * Metadata about the recognition request and response.
 *
 * @generated from message google.cloud.speech.v2.RecognitionResponseMetadata
 */
export type RecognitionResponseMetadata = Message<"google.cloud.speech.v2.RecognitionResponseMetadata"> & {
  /**
   * Global request identifier auto-generated by the API.
   *
   * @generated from field: string request_id = 9;
   */
  requestId: string;

  /**
   * When available, billed audio seconds for the corresponding request.
   *
   * @generated from field: google.protobuf.Duration total_billed_duration = 6;
   */
  totalBilledDuration?: Duration;
};

/**
 * Describes the message google.cloud.speech.v2.RecognitionResponseMetadata.
 * Use `create(RecognitionResponseMetadataSchema)` to create a new message.
 */
export const RecognitionResponseMetadataSchema: GenMessage<RecognitionResponseMetadata> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 18);

/**
 * Alternative hypotheses (a.k.a. n-best list).
 *
 * @generated from message google.cloud.speech.v2.SpeechRecognitionAlternative
 */
export type SpeechRecognitionAlternative = Message<"google.cloud.speech.v2.SpeechRecognitionAlternative"> & {
  /**
   * Transcript text representing the words that the user spoke.
   *
   * @generated from field: string transcript = 1;
   */
  transcript: string;

  /**
   * The confidence estimate between 0.0 and 1.0. A higher number
   * indicates an estimated greater likelihood that the recognized words are
   * correct. This field is set only for the top alternative of a non-streaming
   * result or, of a streaming result where
   * [is_final][google.cloud.speech.v2.StreamingRecognitionResult.is_final] is
   * set to `true`. This field is not guaranteed to be accurate and users should
   * not rely on it to be always provided. The default of 0.0 is a sentinel
   * value indicating `confidence` was not set.
   *
   * @generated from field: float confidence = 2;
   */
  confidence: number;

  /**
   * A list of word-specific information for each recognized word.
   * When the
   * [SpeakerDiarizationConfig][google.cloud.speech.v2.SpeakerDiarizationConfig]
   * is set, you will see all the words from the beginning of the audio.
   *
   * @generated from field: repeated google.cloud.speech.v2.WordInfo words = 3;
   */
  words: WordInfo[];
};

/**
 * Describes the message google.cloud.speech.v2.SpeechRecognitionAlternative.
 * Use `create(SpeechRecognitionAlternativeSchema)` to create a new message.
 */
export const SpeechRecognitionAlternativeSchema: GenMessage<SpeechRecognitionAlternative> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 19);

/**
 * Word-specific information for recognized words.
 *
 * @generated from message google.cloud.speech.v2.WordInfo
 */
export type WordInfo = Message<"google.cloud.speech.v2.WordInfo"> & {
  /**
   * Time offset relative to the beginning of the audio,
   * and corresponding to the start of the spoken word.
   * This field is only set if
   * [enable_word_time_offsets][google.cloud.speech.v2.RecognitionFeatures.enable_word_time_offsets]
   * is `true` and only in the top hypothesis. This is an experimental feature
   * and the accuracy of the time offset can vary.
   *
   * @generated from field: google.protobuf.Duration start_offset = 1;
   */
  startOffset?: Duration;

  /**
   * Time offset relative to the beginning of the audio,
   * and corresponding to the end of the spoken word.
   * This field is only set if
   * [enable_word_time_offsets][google.cloud.speech.v2.RecognitionFeatures.enable_word_time_offsets]
   * is `true` and only in the top hypothesis. This is an experimental feature
   * and the accuracy of the time offset can vary.
   *
   * @generated from field: google.protobuf.Duration end_offset = 2;
   */
  endOffset?: Duration;

  /**
   * The word corresponding to this set of information.
   *
   * @generated from field: string word = 3;
   */
  word: string;

  /**
   * The confidence estimate between 0.0 and 1.0. A higher number
   * indicates an estimated greater likelihood that the recognized words are
   * correct. This field is set only for the top alternative of a non-streaming
   * result or, of a streaming result where
   * [is_final][google.cloud.speech.v2.StreamingRecognitionResult.is_final] is
   * set to `true`. This field is not guaranteed to be accurate and users should
   * not rely on it to be always provided. The default of 0.0 is a sentinel
   * value indicating `confidence` was not set.
   *
   * @generated from field: float confidence = 4;
   */
  confidence: number;

  /**
   * A distinct label is assigned for every speaker within the audio. This field
   * specifies which one of those speakers was detected to have spoken this
   * word. `speaker_label` is set if
   * [SpeakerDiarizationConfig][google.cloud.speech.v2.SpeakerDiarizationConfig]
   * is given and only in the top alternative.
   *
   * @generated from field: string speaker_label = 6;
   */
  speakerLabel: string;
};

/**
 * Describes the message google.cloud.speech.v2.WordInfo.
 * Use `create(WordInfoSchema)` to create a new message.
 */
export const WordInfoSchema: GenMessage<WordInfo> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 20);

/**
 * A speech recognition result corresponding to a portion of the audio.
 *
 * @generated from message google.cloud.speech.v2.SpeechRecognitionResult
 */
export type SpeechRecognitionResult = Message<"google.cloud.speech.v2.SpeechRecognitionResult"> & {
  /**
   * May contain one or more recognition hypotheses. These alternatives are
   * ordered in terms of accuracy, with the top (first) alternative being the
   * most probable, as ranked by the recognizer.
   *
   * @generated from field: repeated google.cloud.speech.v2.SpeechRecognitionAlternative alternatives = 1;
   */
  alternatives: SpeechRecognitionAlternative[];

  /**
   * For multi-channel audio, this is the channel number corresponding to the
   * recognized result for the audio from that channel.
   * For `audio_channel_count` = `N`, its output values can range from `1` to
   * `N`.
   *
   * @generated from field: int32 channel_tag = 2;
   */
  channelTag: number;

  /**
   * Time offset of the end of this result relative to the beginning of the
   * audio.
   *
   * @generated from field: google.protobuf.Duration result_end_offset = 4;
   */
  resultEndOffset?: Duration;

  /**
   * Output only. The [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt)
   * language tag of the language in this result. This language code was
   * detected to have the most likelihood of being spoken in the audio.
   *
   * @generated from field: string language_code = 5;
   */
  languageCode: string;
};

/**
 * Describes the message google.cloud.speech.v2.SpeechRecognitionResult.
 * Use `create(SpeechRecognitionResultSchema)` to create a new message.
 */
export const SpeechRecognitionResultSchema: GenMessage<SpeechRecognitionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 21);

/**
 * Response message for the
 * [Recognize][google.cloud.speech.v2.Speech.Recognize] method.
 *
 * @generated from message google.cloud.speech.v2.RecognizeResponse
 */
export type RecognizeResponse = Message<"google.cloud.speech.v2.RecognizeResponse"> & {
  /**
   * Sequential list of transcription results corresponding to sequential
   * portions of audio.
   *
   * @generated from field: repeated google.cloud.speech.v2.SpeechRecognitionResult results = 3;
   */
  results: SpeechRecognitionResult[];

  /**
   * Metadata about the recognition.
   *
   * @generated from field: google.cloud.speech.v2.RecognitionResponseMetadata metadata = 2;
   */
  metadata?: RecognitionResponseMetadata;
};

/**
 * Describes the message google.cloud.speech.v2.RecognizeResponse.
 * Use `create(RecognizeResponseSchema)` to create a new message.
 */
export const RecognizeResponseSchema: GenMessage<RecognizeResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 22);

/**
 * Available recognition features specific to streaming recognition requests.
 *
 * @generated from message google.cloud.speech.v2.StreamingRecognitionFeatures
 */
export type StreamingRecognitionFeatures = Message<"google.cloud.speech.v2.StreamingRecognitionFeatures"> & {
  /**
   * If `true`, responses with voice activity speech events will be returned as
   * they are detected.
   *
   * @generated from field: bool enable_voice_activity_events = 1;
   */
  enableVoiceActivityEvents: boolean;

  /**
   * Whether or not to stream interim results to the client. If set to true,
   * interim results will be streamed to the client. Otherwise, only the final
   * response will be streamed back.
   *
   * @generated from field: bool interim_results = 2;
   */
  interimResults: boolean;

  /**
   * If set, the server will automatically close the stream after the specified
   * duration has elapsed after the last VOICE_ACTIVITY speech event has been
   * sent. The field `voice_activity_events` must also be set to true.
   *
   * @generated from field: google.cloud.speech.v2.StreamingRecognitionFeatures.VoiceActivityTimeout voice_activity_timeout = 3;
   */
  voiceActivityTimeout?: StreamingRecognitionFeatures_VoiceActivityTimeout;
};

/**
 * Describes the message google.cloud.speech.v2.StreamingRecognitionFeatures.
 * Use `create(StreamingRecognitionFeaturesSchema)` to create a new message.
 */
export const StreamingRecognitionFeaturesSchema: GenMessage<StreamingRecognitionFeatures> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 23);

/**
 * Events that a timeout can be set on for voice activity.
 *
 * @generated from message google.cloud.speech.v2.StreamingRecognitionFeatures.VoiceActivityTimeout
 */
export type StreamingRecognitionFeatures_VoiceActivityTimeout = Message<"google.cloud.speech.v2.StreamingRecognitionFeatures.VoiceActivityTimeout"> & {
  /**
   * Duration to timeout the stream if no speech begins. If this is set and
   * no speech is detected in this duration at the start of the stream, the
   * server will close the stream.
   *
   * @generated from field: google.protobuf.Duration speech_start_timeout = 1;
   */
  speechStartTimeout?: Duration;

  /**
   * Duration to timeout the stream after speech ends. If this is set and no
   * speech is detected in this duration after speech was detected, the server
   * will close the stream.
   *
   * @generated from field: google.protobuf.Duration speech_end_timeout = 2;
   */
  speechEndTimeout?: Duration;
};

/**
 * Describes the message google.cloud.speech.v2.StreamingRecognitionFeatures.VoiceActivityTimeout.
 * Use `create(StreamingRecognitionFeatures_VoiceActivityTimeoutSchema)` to create a new message.
 */
export const StreamingRecognitionFeatures_VoiceActivityTimeoutSchema: GenMessage<StreamingRecognitionFeatures_VoiceActivityTimeout> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 23, 0);

/**
 * Provides configuration information for the StreamingRecognize request.
 *
 * @generated from message google.cloud.speech.v2.StreamingRecognitionConfig
 */
export type StreamingRecognitionConfig = Message<"google.cloud.speech.v2.StreamingRecognitionConfig"> & {
  /**
   * Required. Features and audio metadata to use for the Automatic Speech
   * Recognition. This field in combination with the
   * [config_mask][google.cloud.speech.v2.StreamingRecognitionConfig.config_mask]
   * field can be used to override parts of the
   * [default_recognition_config][google.cloud.speech.v2.Recognizer.default_recognition_config]
   * of the Recognizer resource.
   *
   * @generated from field: google.cloud.speech.v2.RecognitionConfig config = 1;
   */
  config?: RecognitionConfig;

  /**
   * The list of fields in
   * [config][google.cloud.speech.v2.StreamingRecognitionConfig.config] that
   * override the values in the
   * [default_recognition_config][google.cloud.speech.v2.Recognizer.default_recognition_config]
   * of the recognizer during this recognition request. If no mask is provided,
   * all non-default valued fields in
   * [config][google.cloud.speech.v2.StreamingRecognitionConfig.config] override
   * the values in the Recognizer for this recognition request. If a mask is
   * provided, only the fields listed in the mask override the config in the
   * Recognizer for this recognition request. If a wildcard (`*`) is provided,
   * [config][google.cloud.speech.v2.StreamingRecognitionConfig.config]
   * completely overrides and replaces the config in the recognizer for this
   * recognition request.
   *
   * @generated from field: google.protobuf.FieldMask config_mask = 3;
   */
  configMask?: FieldMask;

  /**
   * Speech recognition features to enable specific to streaming audio
   * recognition requests.
   *
   * @generated from field: google.cloud.speech.v2.StreamingRecognitionFeatures streaming_features = 2;
   */
  streamingFeatures?: StreamingRecognitionFeatures;
};

/**
 * Describes the message google.cloud.speech.v2.StreamingRecognitionConfig.
 * Use `create(StreamingRecognitionConfigSchema)` to create a new message.
 */
export const StreamingRecognitionConfigSchema: GenMessage<StreamingRecognitionConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 24);

/**
 * Request message for the
 * [StreamingRecognize][google.cloud.speech.v2.Speech.StreamingRecognize]
 * method. Multiple
 * [StreamingRecognizeRequest][google.cloud.speech.v2.StreamingRecognizeRequest]
 * messages are sent in one call.
 *
 * If the [Recognizer][google.cloud.speech.v2.Recognizer] referenced by
 * [recognizer][google.cloud.speech.v2.StreamingRecognizeRequest.recognizer]
 * contains a fully specified request configuration then the stream may only
 * contain messages with only
 * [audio][google.cloud.speech.v2.StreamingRecognizeRequest.audio] set.
 *
 * Otherwise the first message must contain a
 * [recognizer][google.cloud.speech.v2.StreamingRecognizeRequest.recognizer] and
 * a
 * [streaming_config][google.cloud.speech.v2.StreamingRecognizeRequest.streaming_config]
 * message that together fully specify the request configuration and must not
 * contain [audio][google.cloud.speech.v2.StreamingRecognizeRequest.audio]. All
 * subsequent messages must only have
 * [audio][google.cloud.speech.v2.StreamingRecognizeRequest.audio] set.
 *
 * @generated from message google.cloud.speech.v2.StreamingRecognizeRequest
 */
export type StreamingRecognizeRequest = Message<"google.cloud.speech.v2.StreamingRecognizeRequest"> & {
  /**
   * Required. The name of the Recognizer to use during recognition. The
   * expected format is
   * `projects/{project}/locations/{location}/recognizers/{recognizer}`. The
   * {recognizer} segment may be set to `_` to use an empty implicit Recognizer.
   *
   * @generated from field: string recognizer = 3;
   */
  recognizer: string;

  /**
   * @generated from oneof google.cloud.speech.v2.StreamingRecognizeRequest.streaming_request
   */
  streamingRequest: {
    /**
     * StreamingRecognitionConfig to be used in this recognition attempt.
     * If provided, it will override the default RecognitionConfig stored in the
     * Recognizer.
     *
     * @generated from field: google.cloud.speech.v2.StreamingRecognitionConfig streaming_config = 6;
     */
    value: StreamingRecognitionConfig;
    case: "streamingConfig";
  } | {
    /**
     * Inline audio bytes to be Recognized.
     * Maximum size for this field is 15 KB per request.
     *
     * @generated from field: bytes audio = 5;
     */
    value: Uint8Array;
    case: "audio";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message google.cloud.speech.v2.StreamingRecognizeRequest.
 * Use `create(StreamingRecognizeRequestSchema)` to create a new message.
 */
export const StreamingRecognizeRequestSchema: GenMessage<StreamingRecognizeRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 25);

/**
 * Request message for the
 * [BatchRecognize][google.cloud.speech.v2.Speech.BatchRecognize]
 * method.
 *
 * @generated from message google.cloud.speech.v2.BatchRecognizeRequest
 */
export type BatchRecognizeRequest = Message<"google.cloud.speech.v2.BatchRecognizeRequest"> & {
  /**
   * Required. The name of the Recognizer to use during recognition. The
   * expected format is
   * `projects/{project}/locations/{location}/recognizers/{recognizer}`. The
   * {recognizer} segment may be set to `_` to use an empty implicit Recognizer.
   *
   * @generated from field: string recognizer = 1;
   */
  recognizer: string;

  /**
   * Features and audio metadata to use for the Automatic Speech Recognition.
   * This field in combination with the
   * [config_mask][google.cloud.speech.v2.BatchRecognizeRequest.config_mask]
   * field can be used to override parts of the
   * [default_recognition_config][google.cloud.speech.v2.Recognizer.default_recognition_config]
   * of the Recognizer resource.
   *
   * @generated from field: google.cloud.speech.v2.RecognitionConfig config = 4;
   */
  config?: RecognitionConfig;

  /**
   * The list of fields in
   * [config][google.cloud.speech.v2.BatchRecognizeRequest.config] that override
   * the values in the
   * [default_recognition_config][google.cloud.speech.v2.Recognizer.default_recognition_config]
   * of the recognizer during this recognition request. If no mask is provided,
   * all given fields in
   * [config][google.cloud.speech.v2.BatchRecognizeRequest.config] override the
   * values in the recognizer for this recognition request. If a mask is
   * provided, only the fields listed in the mask override the config in the
   * recognizer for this recognition request. If a wildcard (`*`) is provided,
   * [config][google.cloud.speech.v2.BatchRecognizeRequest.config] completely
   * overrides and replaces the config in the recognizer for this recognition
   * request.
   *
   * @generated from field: google.protobuf.FieldMask config_mask = 5;
   */
  configMask?: FieldMask;

  /**
   * Audio files with file metadata for ASR.
   * The maximum number of files allowed to be specified is 15.
   *
   * @generated from field: repeated google.cloud.speech.v2.BatchRecognizeFileMetadata files = 3;
   */
  files: BatchRecognizeFileMetadata[];

  /**
   * Configuration options for where to output the transcripts of each file.
   *
   * @generated from field: google.cloud.speech.v2.RecognitionOutputConfig recognition_output_config = 6;
   */
  recognitionOutputConfig?: RecognitionOutputConfig;

  /**
   * Processing strategy to use for this request.
   *
   * @generated from field: google.cloud.speech.v2.BatchRecognizeRequest.ProcessingStrategy processing_strategy = 7;
   */
  processingStrategy: BatchRecognizeRequest_ProcessingStrategy;
};

/**
 * Describes the message google.cloud.speech.v2.BatchRecognizeRequest.
 * Use `create(BatchRecognizeRequestSchema)` to create a new message.
 */
export const BatchRecognizeRequestSchema: GenMessage<BatchRecognizeRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 26);

/**
 * Possible processing strategies for batch requests.
 *
 * @generated from enum google.cloud.speech.v2.BatchRecognizeRequest.ProcessingStrategy
 */
export enum BatchRecognizeRequest_ProcessingStrategy {
  /**
   * Default value for the processing strategy. The request is processed as
   * soon as its received.
   *
   * @generated from enum value: PROCESSING_STRATEGY_UNSPECIFIED = 0;
   */
  PROCESSING_STRATEGY_UNSPECIFIED = 0,

  /**
   * If selected, processes the request during lower utilization periods for a
   * price discount. The request is fulfilled within 24 hours.
   *
   * @generated from enum value: DYNAMIC_BATCHING = 1;
   */
  DYNAMIC_BATCHING = 1,
}

/**
 * Describes the enum google.cloud.speech.v2.BatchRecognizeRequest.ProcessingStrategy.
 */
export const BatchRecognizeRequest_ProcessingStrategySchema: GenEnum<BatchRecognizeRequest_ProcessingStrategy> = /*@__PURE__*/
  enumDesc(file_google_cloud_speech_v2_cloud_speech, 26, 0);

/**
 * Output configurations for Cloud Storage.
 *
 * @generated from message google.cloud.speech.v2.GcsOutputConfig
 */
export type GcsOutputConfig = Message<"google.cloud.speech.v2.GcsOutputConfig"> & {
  /**
   * The Cloud Storage URI prefix with which recognition results will be
   * written.
   *
   * @generated from field: string uri = 1;
   */
  uri: string;
};

/**
 * Describes the message google.cloud.speech.v2.GcsOutputConfig.
 * Use `create(GcsOutputConfigSchema)` to create a new message.
 */
export const GcsOutputConfigSchema: GenMessage<GcsOutputConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 27);

/**
 * Output configurations for inline response.
 *
 * @generated from message google.cloud.speech.v2.InlineOutputConfig
 */
export type InlineOutputConfig = Message<"google.cloud.speech.v2.InlineOutputConfig"> & {
};

/**
 * Describes the message google.cloud.speech.v2.InlineOutputConfig.
 * Use `create(InlineOutputConfigSchema)` to create a new message.
 */
export const InlineOutputConfigSchema: GenMessage<InlineOutputConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 28);

/**
 * Output configurations for serialized `BatchRecognizeResults` protos.
 *
 * @generated from message google.cloud.speech.v2.NativeOutputFileFormatConfig
 */
export type NativeOutputFileFormatConfig = Message<"google.cloud.speech.v2.NativeOutputFileFormatConfig"> & {
};

/**
 * Describes the message google.cloud.speech.v2.NativeOutputFileFormatConfig.
 * Use `create(NativeOutputFileFormatConfigSchema)` to create a new message.
 */
export const NativeOutputFileFormatConfigSchema: GenMessage<NativeOutputFileFormatConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 29);

/**
 * Output configurations for [WebVTT](https://www.w3.org/TR/webvtt1/) formatted
 * subtitle file.
 *
 * @generated from message google.cloud.speech.v2.VttOutputFileFormatConfig
 */
export type VttOutputFileFormatConfig = Message<"google.cloud.speech.v2.VttOutputFileFormatConfig"> & {
};

/**
 * Describes the message google.cloud.speech.v2.VttOutputFileFormatConfig.
 * Use `create(VttOutputFileFormatConfigSchema)` to create a new message.
 */
export const VttOutputFileFormatConfigSchema: GenMessage<VttOutputFileFormatConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 30);

/**
 * Output configurations [SubRip
 * Text](https://www.matroska.org/technical/subtitles.html#srt-subtitles)
 * formatted subtitle file.
 *
 * @generated from message google.cloud.speech.v2.SrtOutputFileFormatConfig
 */
export type SrtOutputFileFormatConfig = Message<"google.cloud.speech.v2.SrtOutputFileFormatConfig"> & {
};

/**
 * Describes the message google.cloud.speech.v2.SrtOutputFileFormatConfig.
 * Use `create(SrtOutputFileFormatConfigSchema)` to create a new message.
 */
export const SrtOutputFileFormatConfigSchema: GenMessage<SrtOutputFileFormatConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 31);

/**
 * Configuration for the format of the results stored to `output`.
 *
 * @generated from message google.cloud.speech.v2.OutputFormatConfig
 */
export type OutputFormatConfig = Message<"google.cloud.speech.v2.OutputFormatConfig"> & {
  /**
   * Configuration for the native output format. If this field is set or if no
   * other output format field is set then transcripts will be written to the
   * sink in the native format.
   *
   * @generated from field: google.cloud.speech.v2.NativeOutputFileFormatConfig native = 1;
   */
  native?: NativeOutputFileFormatConfig;

  /**
   * Configuration for the vtt output format. If this field is set then
   * transcripts will be written to the sink in the vtt format.
   *
   * @generated from field: google.cloud.speech.v2.VttOutputFileFormatConfig vtt = 2;
   */
  vtt?: VttOutputFileFormatConfig;

  /**
   * Configuration for the srt output format. If this field is set then
   * transcripts will be written to the sink in the srt format.
   *
   * @generated from field: google.cloud.speech.v2.SrtOutputFileFormatConfig srt = 3;
   */
  srt?: SrtOutputFileFormatConfig;
};

/**
 * Describes the message google.cloud.speech.v2.OutputFormatConfig.
 * Use `create(OutputFormatConfigSchema)` to create a new message.
 */
export const OutputFormatConfigSchema: GenMessage<OutputFormatConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 32);

/**
 * Configuration options for the output(s) of recognition.
 *
 * @generated from message google.cloud.speech.v2.RecognitionOutputConfig
 */
export type RecognitionOutputConfig = Message<"google.cloud.speech.v2.RecognitionOutputConfig"> & {
  /**
   * @generated from oneof google.cloud.speech.v2.RecognitionOutputConfig.output
   */
  output: {
    /**
     * If this message is populated, recognition results are written to the
     * provided Google Cloud Storage URI.
     *
     * @generated from field: google.cloud.speech.v2.GcsOutputConfig gcs_output_config = 1;
     */
    value: GcsOutputConfig;
    case: "gcsOutputConfig";
  } | {
    /**
     * If this message is populated, recognition results are provided in the
     * [BatchRecognizeResponse][google.cloud.speech.v2.BatchRecognizeResponse]
     * message of the Operation when completed. This is only supported when
     * calling [BatchRecognize][google.cloud.speech.v2.Speech.BatchRecognize]
     * with just one audio file.
     *
     * @generated from field: google.cloud.speech.v2.InlineOutputConfig inline_response_config = 2;
     */
    value: InlineOutputConfig;
    case: "inlineResponseConfig";
  } | { case: undefined; value?: undefined };

  /**
   * Optional. Configuration for the format of the results stored to `output`.
   * If unspecified transcripts will be written in the `NATIVE` format only.
   *
   * @generated from field: google.cloud.speech.v2.OutputFormatConfig output_format_config = 3;
   */
  outputFormatConfig?: OutputFormatConfig;
};

/**
 * Describes the message google.cloud.speech.v2.RecognitionOutputConfig.
 * Use `create(RecognitionOutputConfigSchema)` to create a new message.
 */
export const RecognitionOutputConfigSchema: GenMessage<RecognitionOutputConfig> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 33);

/**
 * Response message for
 * [BatchRecognize][google.cloud.speech.v2.Speech.BatchRecognize] that is
 * packaged into a longrunning [Operation][google.longrunning.Operation].
 *
 * @generated from message google.cloud.speech.v2.BatchRecognizeResponse
 */
export type BatchRecognizeResponse = Message<"google.cloud.speech.v2.BatchRecognizeResponse"> & {
  /**
   * Map from filename to the final result for that file.
   *
   * @generated from field: map<string, google.cloud.speech.v2.BatchRecognizeFileResult> results = 1;
   */
  results: { [key: string]: BatchRecognizeFileResult };

  /**
   * When available, billed audio seconds for the corresponding request.
   *
   * @generated from field: google.protobuf.Duration total_billed_duration = 2;
   */
  totalBilledDuration?: Duration;
};

/**
 * Describes the message google.cloud.speech.v2.BatchRecognizeResponse.
 * Use `create(BatchRecognizeResponseSchema)` to create a new message.
 */
export const BatchRecognizeResponseSchema: GenMessage<BatchRecognizeResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 34);

/**
 * Output type for Cloud Storage of BatchRecognize transcripts. Though this
 * proto isn't returned in this API anywhere, the Cloud Storage transcripts will
 * be this proto serialized and should be parsed as such.
 *
 * @generated from message google.cloud.speech.v2.BatchRecognizeResults
 */
export type BatchRecognizeResults = Message<"google.cloud.speech.v2.BatchRecognizeResults"> & {
  /**
   * Sequential list of transcription results corresponding to sequential
   * portions of audio.
   *
   * @generated from field: repeated google.cloud.speech.v2.SpeechRecognitionResult results = 1;
   */
  results: SpeechRecognitionResult[];

  /**
   * Metadata about the recognition.
   *
   * @generated from field: google.cloud.speech.v2.RecognitionResponseMetadata metadata = 2;
   */
  metadata?: RecognitionResponseMetadata;
};

/**
 * Describes the message google.cloud.speech.v2.BatchRecognizeResults.
 * Use `create(BatchRecognizeResultsSchema)` to create a new message.
 */
export const BatchRecognizeResultsSchema: GenMessage<BatchRecognizeResults> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 35);

/**
 * Final results written to Cloud Storage.
 *
 * @generated from message google.cloud.speech.v2.CloudStorageResult
 */
export type CloudStorageResult = Message<"google.cloud.speech.v2.CloudStorageResult"> & {
  /**
   * The Cloud Storage URI to which recognition results were written.
   *
   * @generated from field: string uri = 1;
   */
  uri: string;

  /**
   * The Cloud Storage URI to which recognition results were written as VTT
   * formatted captions. This is populated only when `VTT` output is requested.
   *
   * @generated from field: string vtt_format_uri = 2;
   */
  vttFormatUri: string;

  /**
   * The Cloud Storage URI to which recognition results were written as SRT
   * formatted captions. This is populated only when `SRT` output is requested.
   *
   * @generated from field: string srt_format_uri = 3;
   */
  srtFormatUri: string;
};

/**
 * Describes the message google.cloud.speech.v2.CloudStorageResult.
 * Use `create(CloudStorageResultSchema)` to create a new message.
 */
export const CloudStorageResultSchema: GenMessage<CloudStorageResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 36);

/**
 * Final results returned inline in the recognition response.
 *
 * @generated from message google.cloud.speech.v2.InlineResult
 */
export type InlineResult = Message<"google.cloud.speech.v2.InlineResult"> & {
  /**
   * The transcript for the audio file.
   *
   * @generated from field: google.cloud.speech.v2.BatchRecognizeResults transcript = 1;
   */
  transcript?: BatchRecognizeResults;

  /**
   * The transcript for the audio file as VTT formatted captions. This is
   * populated only when `VTT` output is requested.
   *
   * @generated from field: string vtt_captions = 2;
   */
  vttCaptions: string;

  /**
   * The transcript for the audio file as SRT formatted captions. This is
   * populated only when `SRT` output is requested.
   *
   * @generated from field: string srt_captions = 3;
   */
  srtCaptions: string;
};

/**
 * Describes the message google.cloud.speech.v2.InlineResult.
 * Use `create(InlineResultSchema)` to create a new message.
 */
export const InlineResultSchema: GenMessage<InlineResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 37);

/**
 * Final results for a single file.
 *
 * @generated from message google.cloud.speech.v2.BatchRecognizeFileResult
 */
export type BatchRecognizeFileResult = Message<"google.cloud.speech.v2.BatchRecognizeFileResult"> & {
  /**
   * Error if one was encountered.
   *
   * @generated from field: google.rpc.Status error = 2;
   */
  error?: Status;

  /**
   * @generated from field: google.cloud.speech.v2.RecognitionResponseMetadata metadata = 3;
   */
  metadata?: RecognitionResponseMetadata;

  /**
   * @generated from oneof google.cloud.speech.v2.BatchRecognizeFileResult.result
   */
  result: {
    /**
     * Recognition results written to Cloud Storage. This is
     * populated only when
     * [GcsOutputConfig][google.cloud.speech.v2.GcsOutputConfig] is set in
     * the
     * [RecognitionOutputConfig][[google.cloud.speech.v2.RecognitionOutputConfig].
     *
     * @generated from field: google.cloud.speech.v2.CloudStorageResult cloud_storage_result = 5;
     */
    value: CloudStorageResult;
    case: "cloudStorageResult";
  } | {
    /**
     * Recognition results. This is populated only when
     * [InlineOutputConfig][google.cloud.speech.v2.InlineOutputConfig] is set in
     * the
     * [RecognitionOutputConfig][[google.cloud.speech.v2.RecognitionOutputConfig].
     *
     * @generated from field: google.cloud.speech.v2.InlineResult inline_result = 6;
     */
    value: InlineResult;
    case: "inlineResult";
  } | { case: undefined; value?: undefined };

  /**
   * Deprecated. Use `cloud_storage_result.native_format_uri` instead.
   *
   * @generated from field: string uri = 1 [deprecated = true];
   * @deprecated
   */
  uri: string;

  /**
   * Deprecated. Use `inline_result.transcript` instead.
   *
   * @generated from field: google.cloud.speech.v2.BatchRecognizeResults transcript = 4 [deprecated = true];
   * @deprecated
   */
  transcript?: BatchRecognizeResults;
};

/**
 * Describes the message google.cloud.speech.v2.BatchRecognizeFileResult.
 * Use `create(BatchRecognizeFileResultSchema)` to create a new message.
 */
export const BatchRecognizeFileResultSchema: GenMessage<BatchRecognizeFileResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 38);

/**
 * Metadata about transcription for a single file (for example, progress
 * percent).
 *
 * @generated from message google.cloud.speech.v2.BatchRecognizeTranscriptionMetadata
 */
export type BatchRecognizeTranscriptionMetadata = Message<"google.cloud.speech.v2.BatchRecognizeTranscriptionMetadata"> & {
  /**
   * How much of the file has been transcribed so far.
   *
   * @generated from field: int32 progress_percent = 1;
   */
  progressPercent: number;

  /**
   * Error if one was encountered.
   *
   * @generated from field: google.rpc.Status error = 2;
   */
  error?: Status;

  /**
   * The Cloud Storage URI to which recognition results will be written.
   *
   * @generated from field: string uri = 3;
   */
  uri: string;
};

/**
 * Describes the message google.cloud.speech.v2.BatchRecognizeTranscriptionMetadata.
 * Use `create(BatchRecognizeTranscriptionMetadataSchema)` to create a new message.
 */
export const BatchRecognizeTranscriptionMetadataSchema: GenMessage<BatchRecognizeTranscriptionMetadata> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 39);

/**
 * Operation metadata for
 * [BatchRecognize][google.cloud.speech.v2.Speech.BatchRecognize].
 *
 * @generated from message google.cloud.speech.v2.BatchRecognizeMetadata
 */
export type BatchRecognizeMetadata = Message<"google.cloud.speech.v2.BatchRecognizeMetadata"> & {
  /**
   * Map from provided filename to the transcription metadata for that file.
   *
   * @generated from field: map<string, google.cloud.speech.v2.BatchRecognizeTranscriptionMetadata> transcription_metadata = 1;
   */
  transcriptionMetadata: { [key: string]: BatchRecognizeTranscriptionMetadata };
};

/**
 * Describes the message google.cloud.speech.v2.BatchRecognizeMetadata.
 * Use `create(BatchRecognizeMetadataSchema)` to create a new message.
 */
export const BatchRecognizeMetadataSchema: GenMessage<BatchRecognizeMetadata> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 40);

/**
 * Metadata about a single file in a batch for BatchRecognize.
 *
 * @generated from message google.cloud.speech.v2.BatchRecognizeFileMetadata
 */
export type BatchRecognizeFileMetadata = Message<"google.cloud.speech.v2.BatchRecognizeFileMetadata"> & {
  /**
   * The audio source, which is a Google Cloud Storage URI.
   *
   * @generated from oneof google.cloud.speech.v2.BatchRecognizeFileMetadata.audio_source
   */
  audioSource: {
    /**
     * Cloud Storage URI for the audio file.
     *
     * @generated from field: string uri = 1;
     */
    value: string;
    case: "uri";
  } | { case: undefined; value?: undefined };

  /**
   * Features and audio metadata to use for the Automatic Speech Recognition.
   * This field in combination with the
   * [config_mask][google.cloud.speech.v2.BatchRecognizeFileMetadata.config_mask]
   * field can be used to override parts of the
   * [default_recognition_config][google.cloud.speech.v2.Recognizer.default_recognition_config]
   * of the Recognizer resource as well as the
   * [config][google.cloud.speech.v2.BatchRecognizeRequest.config] at the
   * request level.
   *
   * @generated from field: google.cloud.speech.v2.RecognitionConfig config = 4;
   */
  config?: RecognitionConfig;

  /**
   * The list of fields in
   * [config][google.cloud.speech.v2.BatchRecognizeFileMetadata.config] that
   * override the values in the
   * [default_recognition_config][google.cloud.speech.v2.Recognizer.default_recognition_config]
   * of the recognizer during this recognition request. If no mask is provided,
   * all non-default valued fields in
   * [config][google.cloud.speech.v2.BatchRecognizeFileMetadata.config] override
   * the values in the recognizer for this recognition request. If a mask is
   * provided, only the fields listed in the mask override the config in the
   * recognizer for this recognition request. If a wildcard (`*`) is provided,
   * [config][google.cloud.speech.v2.BatchRecognizeFileMetadata.config]
   * completely overrides and replaces the config in the recognizer for this
   * recognition request.
   *
   * @generated from field: google.protobuf.FieldMask config_mask = 5;
   */
  configMask?: FieldMask;
};

/**
 * Describes the message google.cloud.speech.v2.BatchRecognizeFileMetadata.
 * Use `create(BatchRecognizeFileMetadataSchema)` to create a new message.
 */
export const BatchRecognizeFileMetadataSchema: GenMessage<BatchRecognizeFileMetadata> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 41);

/**
 * A streaming speech recognition result corresponding to a portion of the audio
 * that is currently being processed.
 *
 * @generated from message google.cloud.speech.v2.StreamingRecognitionResult
 */
export type StreamingRecognitionResult = Message<"google.cloud.speech.v2.StreamingRecognitionResult"> & {
  /**
   * May contain one or more recognition hypotheses. These alternatives are
   * ordered in terms of accuracy, with the top (first) alternative being the
   * most probable, as ranked by the recognizer.
   *
   * @generated from field: repeated google.cloud.speech.v2.SpeechRecognitionAlternative alternatives = 1;
   */
  alternatives: SpeechRecognitionAlternative[];

  /**
   * If `false`, this
   * [StreamingRecognitionResult][google.cloud.speech.v2.StreamingRecognitionResult]
   * represents an interim result that may change. If `true`, this is the final
   * time the speech service will return this particular
   * [StreamingRecognitionResult][google.cloud.speech.v2.StreamingRecognitionResult],
   * the recognizer will not return any further hypotheses for this portion of
   * the transcript and corresponding audio.
   *
   * @generated from field: bool is_final = 2;
   */
  isFinal: boolean;

  /**
   * An estimate of the likelihood that the recognizer will not change its guess
   * about this interim result. Values range from 0.0 (completely unstable)
   * to 1.0 (completely stable). This field is only provided for interim results
   * ([is_final][google.cloud.speech.v2.StreamingRecognitionResult.is_final]=`false`).
   * The default of 0.0 is a sentinel value indicating `stability` was not set.
   *
   * @generated from field: float stability = 3;
   */
  stability: number;

  /**
   * Time offset of the end of this result relative to the beginning of the
   * audio.
   *
   * @generated from field: google.protobuf.Duration result_end_offset = 4;
   */
  resultEndOffset?: Duration;

  /**
   * For multi-channel audio, this is the channel number corresponding to the
   * recognized result for the audio from that channel.
   * For
   * `audio_channel_count` = `N`, its output values can range from `1` to `N`.
   *
   * @generated from field: int32 channel_tag = 5;
   */
  channelTag: number;

  /**
   * Output only. The [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt)
   * language tag of the language in this result. This language code was
   * detected to have the most likelihood of being spoken in the audio.
   *
   * @generated from field: string language_code = 6;
   */
  languageCode: string;
};

/**
 * Describes the message google.cloud.speech.v2.StreamingRecognitionResult.
 * Use `create(StreamingRecognitionResultSchema)` to create a new message.
 */
export const StreamingRecognitionResultSchema: GenMessage<StreamingRecognitionResult> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 42);

/**
 * `StreamingRecognizeResponse` is the only message returned to the client by
 * `StreamingRecognize`. A series of zero or more `StreamingRecognizeResponse`
 * messages are streamed back to the client. If there is no recognizable
 * audio then no messages are streamed back to the client.
 *
 * Here are some examples of `StreamingRecognizeResponse`s that might
 * be returned while processing audio:
 *
 * 1. results { alternatives { transcript: "tube" } stability: 0.01 }
 *
 * 2. results { alternatives { transcript: "to be a" } stability: 0.01 }
 *
 * 3. results { alternatives { transcript: "to be" } stability: 0.9 }
 *    results { alternatives { transcript: " or not to be" } stability: 0.01 }
 *
 * 4. results { alternatives { transcript: "to be or not to be"
 *                             confidence: 0.92 }
 *              alternatives { transcript: "to bee or not to bee" }
 *              is_final: true }
 *
 * 5. results { alternatives { transcript: " that's" } stability: 0.01 }
 *
 * 6. results { alternatives { transcript: " that is" } stability: 0.9 }
 *    results { alternatives { transcript: " the question" } stability: 0.01 }
 *
 * 7. results { alternatives { transcript: " that is the question"
 *                             confidence: 0.98 }
 *              alternatives { transcript: " that was the question" }
 *              is_final: true }
 *
 * Notes:
 *
 * - Only two of the above responses #4 and #7 contain final results; they are
 *   indicated by `is_final: true`. Concatenating these together generates the
 *   full transcript: "to be or not to be that is the question".
 *
 * - The others contain interim `results`. #3 and #6 contain two interim
 *   `results`: the first portion has a high stability and is less likely to
 *   change; the second portion has a low stability and is very likely to
 *   change. A UI designer might choose to show only high stability `results`.
 *
 * - The specific `stability` and `confidence` values shown above are only for
 *   illustrative purposes. Actual values may vary.
 *
 * - In each response, only one of these fields will be set:
 *     `error`,
 *     `speech_event_type`, or
 *     one or more (repeated) `results`.
 *
 * @generated from message google.cloud.speech.v2.StreamingRecognizeResponse
 */
export type StreamingRecognizeResponse = Message<"google.cloud.speech.v2.StreamingRecognizeResponse"> & {
  /**
   * This repeated list contains zero or more results that
   * correspond to consecutive portions of the audio currently being processed.
   * It contains zero or one
   * [is_final][google.cloud.speech.v2.StreamingRecognitionResult.is_final]=`true`
   * result (the newly settled portion), followed by zero or more
   * [is_final][google.cloud.speech.v2.StreamingRecognitionResult.is_final]=`false`
   * results (the interim results).
   *
   * @generated from field: repeated google.cloud.speech.v2.StreamingRecognitionResult results = 6;
   */
  results: StreamingRecognitionResult[];

  /**
   * Indicates the type of speech event.
   *
   * @generated from field: google.cloud.speech.v2.StreamingRecognizeResponse.SpeechEventType speech_event_type = 3;
   */
  speechEventType: StreamingRecognizeResponse_SpeechEventType;

  /**
   * Time offset between the beginning of the audio and event emission.
   *
   * @generated from field: google.protobuf.Duration speech_event_offset = 7;
   */
  speechEventOffset?: Duration;

  /**
   * Metadata about the recognition.
   *
   * @generated from field: google.cloud.speech.v2.RecognitionResponseMetadata metadata = 5;
   */
  metadata?: RecognitionResponseMetadata;
};

/**
 * Describes the message google.cloud.speech.v2.StreamingRecognizeResponse.
 * Use `create(StreamingRecognizeResponseSchema)` to create a new message.
 */
export const StreamingRecognizeResponseSchema: GenMessage<StreamingRecognizeResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 43);

/**
 * Indicates the type of speech event.
 *
 * @generated from enum google.cloud.speech.v2.StreamingRecognizeResponse.SpeechEventType
 */
export enum StreamingRecognizeResponse_SpeechEventType {
  /**
   * No speech event specified.
   *
   * @generated from enum value: SPEECH_EVENT_TYPE_UNSPECIFIED = 0;
   */
  SPEECH_EVENT_TYPE_UNSPECIFIED = 0,

  /**
   * This event indicates that the server has detected the end of the user's
   * speech utterance and expects no additional speech. Therefore, the server
   * will not process additional audio and will close the gRPC bidirectional
   * stream. This event is only sent if there was a force cutoff due to
   * silence being detected early. This event is only available through the
   * `latest_short` [model][google.cloud.speech.v2.Recognizer.model].
   *
   * @generated from enum value: END_OF_SINGLE_UTTERANCE = 1;
   */
  END_OF_SINGLE_UTTERANCE = 1,

  /**
   * This event indicates that the server has detected the beginning of human
   * voice activity in the stream. This event can be returned multiple times
   * if speech starts and stops repeatedly throughout the stream. This event
   * is only sent if `voice_activity_events` is set to true.
   *
   * @generated from enum value: SPEECH_ACTIVITY_BEGIN = 2;
   */
  SPEECH_ACTIVITY_BEGIN = 2,

  /**
   * This event indicates that the server has detected the end of human voice
   * activity in the stream. This event can be returned multiple times if
   * speech starts and stops repeatedly throughout the stream. This event is
   * only sent if `voice_activity_events` is set to true.
   *
   * @generated from enum value: SPEECH_ACTIVITY_END = 3;
   */
  SPEECH_ACTIVITY_END = 3,
}

/**
 * Describes the enum google.cloud.speech.v2.StreamingRecognizeResponse.SpeechEventType.
 */
export const StreamingRecognizeResponse_SpeechEventTypeSchema: GenEnum<StreamingRecognizeResponse_SpeechEventType> = /*@__PURE__*/
  enumDesc(file_google_cloud_speech_v2_cloud_speech, 43, 0);

/**
 * Message representing the config for the Speech-to-Text API. This includes an
 * optional [KMS key](https://cloud.google.com/kms/docs/resource-hierarchy#keys)
 * with which incoming data will be encrypted.
 *
 * @generated from message google.cloud.speech.v2.Config
 */
export type Config = Message<"google.cloud.speech.v2.Config"> & {
  /**
   * Output only. Identifier. The name of the config resource. There is exactly
   * one config resource per project per location. The expected format is
   * `projects/{project}/locations/{location}/config`.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Optional. An optional [KMS key
   * name](https://cloud.google.com/kms/docs/resource-hierarchy#keys) that if
   * present, will be used to encrypt Speech-to-Text resources at-rest. Updating
   * this key will not encrypt existing resources using this key; only new
   * resources will be encrypted using this key. The expected format is
   * `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}`.
   *
   * @generated from field: string kms_key_name = 2;
   */
  kmsKeyName: string;

  /**
   * Output only. The most recent time this resource was modified.
   *
   * @generated from field: google.protobuf.Timestamp update_time = 3;
   */
  updateTime?: Timestamp;
};

/**
 * Describes the message google.cloud.speech.v2.Config.
 * Use `create(ConfigSchema)` to create a new message.
 */
export const ConfigSchema: GenMessage<Config> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 44);

/**
 * Request message for the
 * [GetConfig][google.cloud.speech.v2.Speech.GetConfig] method.
 *
 * @generated from message google.cloud.speech.v2.GetConfigRequest
 */
export type GetConfigRequest = Message<"google.cloud.speech.v2.GetConfigRequest"> & {
  /**
   * Required. The name of the config to retrieve. There is exactly one config
   * resource per project per location. The expected format is
   * `projects/{project}/locations/{location}/config`.
   *
   * @generated from field: string name = 1;
   */
  name: string;
};

/**
 * Describes the message google.cloud.speech.v2.GetConfigRequest.
 * Use `create(GetConfigRequestSchema)` to create a new message.
 */
export const GetConfigRequestSchema: GenMessage<GetConfigRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 45);

/**
 * Request message for the
 * [UpdateConfig][google.cloud.speech.v2.Speech.UpdateConfig] method.
 *
 * @generated from message google.cloud.speech.v2.UpdateConfigRequest
 */
export type UpdateConfigRequest = Message<"google.cloud.speech.v2.UpdateConfigRequest"> & {
  /**
   * Required. The config to update.
   *
   * The config's `name` field is used to identify the config to be updated.
   * The expected format is `projects/{project}/locations/{location}/config`.
   *
   * @generated from field: google.cloud.speech.v2.Config config = 1;
   */
  config?: Config;

  /**
   * The list of fields to be updated.
   *
   * @generated from field: google.protobuf.FieldMask update_mask = 2;
   */
  updateMask?: FieldMask;
};

/**
 * Describes the message google.cloud.speech.v2.UpdateConfigRequest.
 * Use `create(UpdateConfigRequestSchema)` to create a new message.
 */
export const UpdateConfigRequestSchema: GenMessage<UpdateConfigRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 46);

/**
 * CustomClass for biasing in speech recognition. Used to define a set of words
 * or phrases that represents a common concept or theme likely to appear in your
 * audio, for example a list of passenger ship names.
 *
 * @generated from message google.cloud.speech.v2.CustomClass
 */
export type CustomClass = Message<"google.cloud.speech.v2.CustomClass"> & {
  /**
   * Output only. Identifier. The resource name of the CustomClass.
   * Format:
   * `projects/{project}/locations/{location}/customClasses/{custom_class}`.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Output only. System-assigned unique identifier for the CustomClass.
   *
   * @generated from field: string uid = 2;
   */
  uid: string;

  /**
   * Optional. User-settable, human-readable name for the CustomClass. Must be
   * 63 characters or less.
   *
   * @generated from field: string display_name = 4;
   */
  displayName: string;

  /**
   * A collection of class items.
   *
   * @generated from field: repeated google.cloud.speech.v2.CustomClass.ClassItem items = 5;
   */
  items: CustomClass_ClassItem[];

  /**
   * Output only. The CustomClass lifecycle state.
   *
   * @generated from field: google.cloud.speech.v2.CustomClass.State state = 15;
   */
  state: CustomClass_State;

  /**
   * Output only. Creation time.
   *
   * @generated from field: google.protobuf.Timestamp create_time = 6;
   */
  createTime?: Timestamp;

  /**
   * Output only. The most recent time this resource was modified.
   *
   * @generated from field: google.protobuf.Timestamp update_time = 7;
   */
  updateTime?: Timestamp;

  /**
   * Output only. The time at which this resource was requested for deletion.
   *
   * @generated from field: google.protobuf.Timestamp delete_time = 8;
   */
  deleteTime?: Timestamp;

  /**
   * Output only. The time at which this resource will be purged.
   *
   * @generated from field: google.protobuf.Timestamp expire_time = 9;
   */
  expireTime?: Timestamp;

  /**
   * Optional. Allows users to store small amounts of arbitrary data.
   * Both the key and the value must be 63 characters or less each.
   * At most 100 annotations.
   *
   * @generated from field: map<string, string> annotations = 10;
   */
  annotations: { [key: string]: string };

  /**
   * Output only. This checksum is computed by the server based on the value of
   * other fields. This may be sent on update, undelete, and delete requests to
   * ensure the client has an up-to-date value before proceeding.
   *
   * @generated from field: string etag = 11;
   */
  etag: string;

  /**
   * Output only. Whether or not this CustomClass is in the process of being
   * updated.
   *
   * @generated from field: bool reconciling = 12;
   */
  reconciling: boolean;

  /**
   * Output only. The [KMS key
   * name](https://cloud.google.com/kms/docs/resource-hierarchy#keys) with which
   * the CustomClass is encrypted. The expected format is
   * `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}`.
   *
   * @generated from field: string kms_key_name = 13;
   */
  kmsKeyName: string;

  /**
   * Output only. The [KMS key version
   * name](https://cloud.google.com/kms/docs/resource-hierarchy#key_versions)
   * with which the CustomClass is encrypted. The expected format is
   * `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}/cryptoKeyVersions/{crypto_key_version}`.
   *
   * @generated from field: string kms_key_version_name = 14;
   */
  kmsKeyVersionName: string;
};

/**
 * Describes the message google.cloud.speech.v2.CustomClass.
 * Use `create(CustomClassSchema)` to create a new message.
 */
export const CustomClassSchema: GenMessage<CustomClass> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 47);

/**
 * An item of the class.
 *
 * @generated from message google.cloud.speech.v2.CustomClass.ClassItem
 */
export type CustomClass_ClassItem = Message<"google.cloud.speech.v2.CustomClass.ClassItem"> & {
  /**
   * The class item's value.
   *
   * @generated from field: string value = 1;
   */
  value: string;
};

/**
 * Describes the message google.cloud.speech.v2.CustomClass.ClassItem.
 * Use `create(CustomClass_ClassItemSchema)` to create a new message.
 */
export const CustomClass_ClassItemSchema: GenMessage<CustomClass_ClassItem> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 47, 0);

/**
 * Set of states that define the lifecycle of a CustomClass.
 *
 * @generated from enum google.cloud.speech.v2.CustomClass.State
 */
export enum CustomClass_State {
  /**
   * Unspecified state.  This is only used/useful for distinguishing
   * unset values.
   *
   * @generated from enum value: STATE_UNSPECIFIED = 0;
   */
  STATE_UNSPECIFIED = 0,

  /**
   * The normal and active state.
   *
   * @generated from enum value: ACTIVE = 2;
   */
  ACTIVE = 2,

  /**
   * This CustomClass has been deleted.
   *
   * @generated from enum value: DELETED = 4;
   */
  DELETED = 4,
}

/**
 * Describes the enum google.cloud.speech.v2.CustomClass.State.
 */
export const CustomClass_StateSchema: GenEnum<CustomClass_State> = /*@__PURE__*/
  enumDesc(file_google_cloud_speech_v2_cloud_speech, 47, 0);

/**
 * PhraseSet for biasing in speech recognition. A PhraseSet is used to provide
 * "hints" to the speech recognizer to favor specific words and phrases in the
 * results.
 *
 * @generated from message google.cloud.speech.v2.PhraseSet
 */
export type PhraseSet = Message<"google.cloud.speech.v2.PhraseSet"> & {
  /**
   * Output only. Identifier. The resource name of the PhraseSet.
   * Format: `projects/{project}/locations/{location}/phraseSets/{phrase_set}`.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Output only. System-assigned unique identifier for the PhraseSet.
   *
   * @generated from field: string uid = 2;
   */
  uid: string;

  /**
   * A list of word and phrases.
   *
   * @generated from field: repeated google.cloud.speech.v2.PhraseSet.Phrase phrases = 3;
   */
  phrases: PhraseSet_Phrase[];

  /**
   * Hint Boost. Positive value will increase the probability that a specific
   * phrase will be recognized over other similar sounding phrases. The higher
   * the boost, the higher the chance of false positive recognition as well.
   * Valid `boost` values are between 0 (exclusive) and 20. We recommend using a
   * binary search approach to finding the optimal value for your use case as
   * well as adding phrases both with and without boost to your requests.
   *
   * @generated from field: float boost = 4;
   */
  boost: number;

  /**
   * User-settable, human-readable name for the PhraseSet. Must be 63
   * characters or less.
   *
   * @generated from field: string display_name = 5;
   */
  displayName: string;

  /**
   * Output only. The PhraseSet lifecycle state.
   *
   * @generated from field: google.cloud.speech.v2.PhraseSet.State state = 15;
   */
  state: PhraseSet_State;

  /**
   * Output only. Creation time.
   *
   * @generated from field: google.protobuf.Timestamp create_time = 6;
   */
  createTime?: Timestamp;

  /**
   * Output only. The most recent time this resource was modified.
   *
   * @generated from field: google.protobuf.Timestamp update_time = 7;
   */
  updateTime?: Timestamp;

  /**
   * Output only. The time at which this resource was requested for deletion.
   *
   * @generated from field: google.protobuf.Timestamp delete_time = 8;
   */
  deleteTime?: Timestamp;

  /**
   * Output only. The time at which this resource will be purged.
   *
   * @generated from field: google.protobuf.Timestamp expire_time = 9;
   */
  expireTime?: Timestamp;

  /**
   * Allows users to store small amounts of arbitrary data.
   * Both the key and the value must be 63 characters or less each.
   * At most 100 annotations.
   *
   * @generated from field: map<string, string> annotations = 10;
   */
  annotations: { [key: string]: string };

  /**
   * Output only. This checksum is computed by the server based on the value of
   * other fields. This may be sent on update, undelete, and delete requests to
   * ensure the client has an up-to-date value before proceeding.
   *
   * @generated from field: string etag = 11;
   */
  etag: string;

  /**
   * Output only. Whether or not this PhraseSet is in the process of being
   * updated.
   *
   * @generated from field: bool reconciling = 12;
   */
  reconciling: boolean;

  /**
   * Output only. The [KMS key
   * name](https://cloud.google.com/kms/docs/resource-hierarchy#keys) with which
   * the PhraseSet is encrypted. The expected format is
   * `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}`.
   *
   * @generated from field: string kms_key_name = 13;
   */
  kmsKeyName: string;

  /**
   * Output only. The [KMS key version
   * name](https://cloud.google.com/kms/docs/resource-hierarchy#key_versions)
   * with which the PhraseSet is encrypted. The expected format is
   * `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}/cryptoKeyVersions/{crypto_key_version}`.
   *
   * @generated from field: string kms_key_version_name = 14;
   */
  kmsKeyVersionName: string;
};

/**
 * Describes the message google.cloud.speech.v2.PhraseSet.
 * Use `create(PhraseSetSchema)` to create a new message.
 */
export const PhraseSetSchema: GenMessage<PhraseSet> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 48);

/**
 * A Phrase contains words and phrase "hints" so that the speech recognition
 * is more likely to recognize them. This can be used to improve the accuracy
 * for specific words and phrases, for example, if specific commands are
 * typically spoken by the user. This can also be used to add additional words
 * to the vocabulary of the recognizer.
 *
 * List items can also include CustomClass references containing groups of
 * words that represent common concepts that occur in natural language.
 *
 * @generated from message google.cloud.speech.v2.PhraseSet.Phrase
 */
export type PhraseSet_Phrase = Message<"google.cloud.speech.v2.PhraseSet.Phrase"> & {
  /**
   * The phrase itself.
   *
   * @generated from field: string value = 1;
   */
  value: string;

  /**
   * Hint Boost. Overrides the boost set at the phrase set level.
   * Positive value will increase the probability that a specific phrase will
   * be recognized over other similar sounding phrases. The higher the boost,
   * the higher the chance of false positive recognition as well. Negative
   * boost values would correspond to anti-biasing. Anti-biasing is not
   * enabled, so negative boost values will return an error. Boost values must
   * be between 0 and 20. Any values outside that range will return an error.
   * We recommend using a binary search approach to finding the optimal value
   * for your use case as well as adding phrases both with and without boost
   * to your requests.
   *
   * @generated from field: float boost = 2;
   */
  boost: number;
};

/**
 * Describes the message google.cloud.speech.v2.PhraseSet.Phrase.
 * Use `create(PhraseSet_PhraseSchema)` to create a new message.
 */
export const PhraseSet_PhraseSchema: GenMessage<PhraseSet_Phrase> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 48, 0);

/**
 * Set of states that define the lifecycle of a PhraseSet.
 *
 * @generated from enum google.cloud.speech.v2.PhraseSet.State
 */
export enum PhraseSet_State {
  /**
   * Unspecified state.  This is only used/useful for distinguishing
   * unset values.
   *
   * @generated from enum value: STATE_UNSPECIFIED = 0;
   */
  STATE_UNSPECIFIED = 0,

  /**
   * The normal and active state.
   *
   * @generated from enum value: ACTIVE = 2;
   */
  ACTIVE = 2,

  /**
   * This PhraseSet has been deleted.
   *
   * @generated from enum value: DELETED = 4;
   */
  DELETED = 4,
}

/**
 * Describes the enum google.cloud.speech.v2.PhraseSet.State.
 */
export const PhraseSet_StateSchema: GenEnum<PhraseSet_State> = /*@__PURE__*/
  enumDesc(file_google_cloud_speech_v2_cloud_speech, 48, 0);

/**
 * Request message for the
 * [CreateCustomClass][google.cloud.speech.v2.Speech.CreateCustomClass] method.
 *
 * @generated from message google.cloud.speech.v2.CreateCustomClassRequest
 */
export type CreateCustomClassRequest = Message<"google.cloud.speech.v2.CreateCustomClassRequest"> & {
  /**
   * Required. The CustomClass to create.
   *
   * @generated from field: google.cloud.speech.v2.CustomClass custom_class = 1;
   */
  customClass?: CustomClass;

  /**
   * If set, validate the request and preview the CustomClass, but do not
   * actually create it.
   *
   * @generated from field: bool validate_only = 2;
   */
  validateOnly: boolean;

  /**
   * The ID to use for the CustomClass, which will become the final component of
   * the CustomClass's resource name.
   *
   * This value should be 4-63 characters, and valid characters
   * are /[a-z][0-9]-/.
   *
   * @generated from field: string custom_class_id = 3;
   */
  customClassId: string;

  /**
   * Required. The project and location where this CustomClass will be created.
   * The expected format is `projects/{project}/locations/{location}`.
   *
   * @generated from field: string parent = 4;
   */
  parent: string;
};

/**
 * Describes the message google.cloud.speech.v2.CreateCustomClassRequest.
 * Use `create(CreateCustomClassRequestSchema)` to create a new message.
 */
export const CreateCustomClassRequestSchema: GenMessage<CreateCustomClassRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 49);

/**
 * Request message for the
 * [ListCustomClasses][google.cloud.speech.v2.Speech.ListCustomClasses] method.
 *
 * @generated from message google.cloud.speech.v2.ListCustomClassesRequest
 */
export type ListCustomClassesRequest = Message<"google.cloud.speech.v2.ListCustomClassesRequest"> & {
  /**
   * Required. The project and location of CustomClass resources to list. The
   * expected format is `projects/{project}/locations/{location}`.
   *
   * @generated from field: string parent = 1;
   */
  parent: string;

  /**
   * Number of results per requests. A valid page_size ranges from 0 to 100
   * inclusive. If the page_size is zero or unspecified, a page size of 5 will
   * be chosen. If the page size exceeds 100, it will be coerced down to 100.
   * Note that a call might return fewer results than the requested page size.
   *
   * @generated from field: int32 page_size = 2;
   */
  pageSize: number;

  /**
   * A page token, received from a previous
   * [ListCustomClasses][google.cloud.speech.v2.Speech.ListCustomClasses] call.
   * Provide this to retrieve the subsequent page.
   *
   * When paginating, all other parameters provided to
   * [ListCustomClasses][google.cloud.speech.v2.Speech.ListCustomClasses] must
   * match the call that provided the page token.
   *
   * @generated from field: string page_token = 3;
   */
  pageToken: string;

  /**
   * Whether, or not, to show resources that have been deleted.
   *
   * @generated from field: bool show_deleted = 4;
   */
  showDeleted: boolean;
};

/**
 * Describes the message google.cloud.speech.v2.ListCustomClassesRequest.
 * Use `create(ListCustomClassesRequestSchema)` to create a new message.
 */
export const ListCustomClassesRequestSchema: GenMessage<ListCustomClassesRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 50);

/**
 * Response message for the
 * [ListCustomClasses][google.cloud.speech.v2.Speech.ListCustomClasses] method.
 *
 * @generated from message google.cloud.speech.v2.ListCustomClassesResponse
 */
export type ListCustomClassesResponse = Message<"google.cloud.speech.v2.ListCustomClassesResponse"> & {
  /**
   * The list of requested CustomClasses.
   *
   * @generated from field: repeated google.cloud.speech.v2.CustomClass custom_classes = 1;
   */
  customClasses: CustomClass[];

  /**
   * A token, which can be sent as
   * [page_token][google.cloud.speech.v2.ListCustomClassesRequest.page_token] to
   * retrieve the next page. If this field is omitted, there are no subsequent
   * pages. This token expires after 72 hours.
   *
   * @generated from field: string next_page_token = 2;
   */
  nextPageToken: string;
};

/**
 * Describes the message google.cloud.speech.v2.ListCustomClassesResponse.
 * Use `create(ListCustomClassesResponseSchema)` to create a new message.
 */
export const ListCustomClassesResponseSchema: GenMessage<ListCustomClassesResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 51);

/**
 * Request message for the
 * [GetCustomClass][google.cloud.speech.v2.Speech.GetCustomClass] method.
 *
 * @generated from message google.cloud.speech.v2.GetCustomClassRequest
 */
export type GetCustomClassRequest = Message<"google.cloud.speech.v2.GetCustomClassRequest"> & {
  /**
   * Required. The name of the CustomClass to retrieve. The expected format is
   * `projects/{project}/locations/{location}/customClasses/{custom_class}`.
   *
   * @generated from field: string name = 1;
   */
  name: string;
};

/**
 * Describes the message google.cloud.speech.v2.GetCustomClassRequest.
 * Use `create(GetCustomClassRequestSchema)` to create a new message.
 */
export const GetCustomClassRequestSchema: GenMessage<GetCustomClassRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 52);

/**
 * Request message for the
 * [UpdateCustomClass][google.cloud.speech.v2.Speech.UpdateCustomClass] method.
 *
 * @generated from message google.cloud.speech.v2.UpdateCustomClassRequest
 */
export type UpdateCustomClassRequest = Message<"google.cloud.speech.v2.UpdateCustomClassRequest"> & {
  /**
   * Required. The CustomClass to update.
   *
   * The CustomClass's `name` field is used to identify the CustomClass to
   * update. Format:
   * `projects/{project}/locations/{location}/customClasses/{custom_class}`.
   *
   * @generated from field: google.cloud.speech.v2.CustomClass custom_class = 1;
   */
  customClass?: CustomClass;

  /**
   * The list of fields to be updated. If empty, all fields are considered for
   * update.
   *
   * @generated from field: google.protobuf.FieldMask update_mask = 2;
   */
  updateMask?: FieldMask;

  /**
   * If set, validate the request and preview the updated CustomClass, but do
   * not actually update it.
   *
   * @generated from field: bool validate_only = 4;
   */
  validateOnly: boolean;
};

/**
 * Describes the message google.cloud.speech.v2.UpdateCustomClassRequest.
 * Use `create(UpdateCustomClassRequestSchema)` to create a new message.
 */
export const UpdateCustomClassRequestSchema: GenMessage<UpdateCustomClassRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 53);

/**
 * Request message for the
 * [DeleteCustomClass][google.cloud.speech.v2.Speech.DeleteCustomClass] method.
 *
 * @generated from message google.cloud.speech.v2.DeleteCustomClassRequest
 */
export type DeleteCustomClassRequest = Message<"google.cloud.speech.v2.DeleteCustomClassRequest"> & {
  /**
   * Required. The name of the CustomClass to delete.
   * Format:
   * `projects/{project}/locations/{location}/customClasses/{custom_class}`
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * If set, validate the request and preview the deleted CustomClass, but do
   * not actually delete it.
   *
   * @generated from field: bool validate_only = 2;
   */
  validateOnly: boolean;

  /**
   * If set to true, and the CustomClass is not found, the request will succeed
   * and  be a no-op (no Operation is recorded in this case).
   *
   * @generated from field: bool allow_missing = 4;
   */
  allowMissing: boolean;

  /**
   * This checksum is computed by the server based on the value of other
   * fields. This may be sent on update, undelete, and delete requests to ensure
   * the client has an up-to-date value before proceeding.
   *
   * @generated from field: string etag = 3;
   */
  etag: string;
};

/**
 * Describes the message google.cloud.speech.v2.DeleteCustomClassRequest.
 * Use `create(DeleteCustomClassRequestSchema)` to create a new message.
 */
export const DeleteCustomClassRequestSchema: GenMessage<DeleteCustomClassRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 54);

/**
 * Request message for the
 * [UndeleteCustomClass][google.cloud.speech.v2.Speech.UndeleteCustomClass]
 * method.
 *
 * @generated from message google.cloud.speech.v2.UndeleteCustomClassRequest
 */
export type UndeleteCustomClassRequest = Message<"google.cloud.speech.v2.UndeleteCustomClassRequest"> & {
  /**
   * Required. The name of the CustomClass to undelete.
   * Format:
   * `projects/{project}/locations/{location}/customClasses/{custom_class}`
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * If set, validate the request and preview the undeleted CustomClass, but do
   * not actually undelete it.
   *
   * @generated from field: bool validate_only = 3;
   */
  validateOnly: boolean;

  /**
   * This checksum is computed by the server based on the value of other
   * fields. This may be sent on update, undelete, and delete requests to ensure
   * the client has an up-to-date value before proceeding.
   *
   * @generated from field: string etag = 4;
   */
  etag: string;
};

/**
 * Describes the message google.cloud.speech.v2.UndeleteCustomClassRequest.
 * Use `create(UndeleteCustomClassRequestSchema)` to create a new message.
 */
export const UndeleteCustomClassRequestSchema: GenMessage<UndeleteCustomClassRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 55);

/**
 * Request message for the
 * [CreatePhraseSet][google.cloud.speech.v2.Speech.CreatePhraseSet] method.
 *
 * @generated from message google.cloud.speech.v2.CreatePhraseSetRequest
 */
export type CreatePhraseSetRequest = Message<"google.cloud.speech.v2.CreatePhraseSetRequest"> & {
  /**
   * Required. The PhraseSet to create.
   *
   * @generated from field: google.cloud.speech.v2.PhraseSet phrase_set = 1;
   */
  phraseSet?: PhraseSet;

  /**
   * If set, validate the request and preview the PhraseSet, but do not
   * actually create it.
   *
   * @generated from field: bool validate_only = 2;
   */
  validateOnly: boolean;

  /**
   * The ID to use for the PhraseSet, which will become the final component of
   * the PhraseSet's resource name.
   *
   * This value should be 4-63 characters, and valid characters
   * are /[a-z][0-9]-/.
   *
   * @generated from field: string phrase_set_id = 3;
   */
  phraseSetId: string;

  /**
   * Required. The project and location where this PhraseSet will be created.
   * The expected format is `projects/{project}/locations/{location}`.
   *
   * @generated from field: string parent = 4;
   */
  parent: string;
};

/**
 * Describes the message google.cloud.speech.v2.CreatePhraseSetRequest.
 * Use `create(CreatePhraseSetRequestSchema)` to create a new message.
 */
export const CreatePhraseSetRequestSchema: GenMessage<CreatePhraseSetRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 56);

/**
 * Request message for the
 * [ListPhraseSets][google.cloud.speech.v2.Speech.ListPhraseSets] method.
 *
 * @generated from message google.cloud.speech.v2.ListPhraseSetsRequest
 */
export type ListPhraseSetsRequest = Message<"google.cloud.speech.v2.ListPhraseSetsRequest"> & {
  /**
   * Required. The project and location of PhraseSet resources to list. The
   * expected format is `projects/{project}/locations/{location}`.
   *
   * @generated from field: string parent = 1;
   */
  parent: string;

  /**
   * The maximum number of PhraseSets to return. The service may return fewer
   * than this value. If unspecified, at most 5 PhraseSets will be returned.
   * The maximum value is 100; values above 100 will be coerced to 100.
   *
   * @generated from field: int32 page_size = 2;
   */
  pageSize: number;

  /**
   * A page token, received from a previous
   * [ListPhraseSets][google.cloud.speech.v2.Speech.ListPhraseSets] call.
   * Provide this to retrieve the subsequent page.
   *
   * When paginating, all other parameters provided to
   * [ListPhraseSets][google.cloud.speech.v2.Speech.ListPhraseSets] must match
   * the call that provided the page token.
   *
   * @generated from field: string page_token = 3;
   */
  pageToken: string;

  /**
   * Whether, or not, to show resources that have been deleted.
   *
   * @generated from field: bool show_deleted = 4;
   */
  showDeleted: boolean;
};

/**
 * Describes the message google.cloud.speech.v2.ListPhraseSetsRequest.
 * Use `create(ListPhraseSetsRequestSchema)` to create a new message.
 */
export const ListPhraseSetsRequestSchema: GenMessage<ListPhraseSetsRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 57);

/**
 * Response message for the
 * [ListPhraseSets][google.cloud.speech.v2.Speech.ListPhraseSets] method.
 *
 * @generated from message google.cloud.speech.v2.ListPhraseSetsResponse
 */
export type ListPhraseSetsResponse = Message<"google.cloud.speech.v2.ListPhraseSetsResponse"> & {
  /**
   * The list of requested PhraseSets.
   *
   * @generated from field: repeated google.cloud.speech.v2.PhraseSet phrase_sets = 1;
   */
  phraseSets: PhraseSet[];

  /**
   * A token, which can be sent as
   * [page_token][google.cloud.speech.v2.ListPhraseSetsRequest.page_token] to
   * retrieve the next page. If this field is omitted, there are no subsequent
   * pages. This token expires after 72 hours.
   *
   * @generated from field: string next_page_token = 2;
   */
  nextPageToken: string;
};

/**
 * Describes the message google.cloud.speech.v2.ListPhraseSetsResponse.
 * Use `create(ListPhraseSetsResponseSchema)` to create a new message.
 */
export const ListPhraseSetsResponseSchema: GenMessage<ListPhraseSetsResponse> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 58);

/**
 * Request message for the
 * [GetPhraseSet][google.cloud.speech.v2.Speech.GetPhraseSet] method.
 *
 * @generated from message google.cloud.speech.v2.GetPhraseSetRequest
 */
export type GetPhraseSetRequest = Message<"google.cloud.speech.v2.GetPhraseSetRequest"> & {
  /**
   * Required. The name of the PhraseSet to retrieve. The expected format is
   * `projects/{project}/locations/{location}/phraseSets/{phrase_set}`.
   *
   * @generated from field: string name = 1;
   */
  name: string;
};

/**
 * Describes the message google.cloud.speech.v2.GetPhraseSetRequest.
 * Use `create(GetPhraseSetRequestSchema)` to create a new message.
 */
export const GetPhraseSetRequestSchema: GenMessage<GetPhraseSetRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 59);

/**
 * Request message for the
 * [UpdatePhraseSet][google.cloud.speech.v2.Speech.UpdatePhraseSet] method.
 *
 * @generated from message google.cloud.speech.v2.UpdatePhraseSetRequest
 */
export type UpdatePhraseSetRequest = Message<"google.cloud.speech.v2.UpdatePhraseSetRequest"> & {
  /**
   * Required. The PhraseSet to update.
   *
   * The PhraseSet's `name` field is used to identify the PhraseSet to update.
   * Format: `projects/{project}/locations/{location}/phraseSets/{phrase_set}`.
   *
   * @generated from field: google.cloud.speech.v2.PhraseSet phrase_set = 1;
   */
  phraseSet?: PhraseSet;

  /**
   * The list of fields to update. If empty, all non-default valued fields are
   * considered for update. Use `*` to update the entire PhraseSet resource.
   *
   * @generated from field: google.protobuf.FieldMask update_mask = 2;
   */
  updateMask?: FieldMask;

  /**
   * If set, validate the request and preview the updated PhraseSet, but do not
   * actually update it.
   *
   * @generated from field: bool validate_only = 4;
   */
  validateOnly: boolean;
};

/**
 * Describes the message google.cloud.speech.v2.UpdatePhraseSetRequest.
 * Use `create(UpdatePhraseSetRequestSchema)` to create a new message.
 */
export const UpdatePhraseSetRequestSchema: GenMessage<UpdatePhraseSetRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 60);

/**
 * Request message for the
 * [DeletePhraseSet][google.cloud.speech.v2.Speech.DeletePhraseSet] method.
 *
 * @generated from message google.cloud.speech.v2.DeletePhraseSetRequest
 */
export type DeletePhraseSetRequest = Message<"google.cloud.speech.v2.DeletePhraseSetRequest"> & {
  /**
   * Required. The name of the PhraseSet to delete.
   * Format: `projects/{project}/locations/{location}/phraseSets/{phrase_set}`
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * If set, validate the request and preview the deleted PhraseSet, but do not
   * actually delete it.
   *
   * @generated from field: bool validate_only = 2;
   */
  validateOnly: boolean;

  /**
   * If set to true, and the PhraseSet is not found, the request will succeed
   * and  be a no-op (no Operation is recorded in this case).
   *
   * @generated from field: bool allow_missing = 4;
   */
  allowMissing: boolean;

  /**
   * This checksum is computed by the server based on the value of other
   * fields. This may be sent on update, undelete, and delete requests to ensure
   * the client has an up-to-date value before proceeding.
   *
   * @generated from field: string etag = 3;
   */
  etag: string;
};

/**
 * Describes the message google.cloud.speech.v2.DeletePhraseSetRequest.
 * Use `create(DeletePhraseSetRequestSchema)` to create a new message.
 */
export const DeletePhraseSetRequestSchema: GenMessage<DeletePhraseSetRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 61);

/**
 * Request message for the
 * [UndeletePhraseSet][google.cloud.speech.v2.Speech.UndeletePhraseSet]
 * method.
 *
 * @generated from message google.cloud.speech.v2.UndeletePhraseSetRequest
 */
export type UndeletePhraseSetRequest = Message<"google.cloud.speech.v2.UndeletePhraseSetRequest"> & {
  /**
   * Required. The name of the PhraseSet to undelete.
   * Format: `projects/{project}/locations/{location}/phraseSets/{phrase_set}`
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * If set, validate the request and preview the undeleted PhraseSet, but do
   * not actually undelete it.
   *
   * @generated from field: bool validate_only = 3;
   */
  validateOnly: boolean;

  /**
   * This checksum is computed by the server based on the value of other
   * fields. This may be sent on update, undelete, and delete requests to ensure
   * the client has an up-to-date value before proceeding.
   *
   * @generated from field: string etag = 4;
   */
  etag: string;
};

/**
 * Describes the message google.cloud.speech.v2.UndeletePhraseSetRequest.
 * Use `create(UndeletePhraseSetRequestSchema)` to create a new message.
 */
export const UndeletePhraseSetRequestSchema: GenMessage<UndeletePhraseSetRequest> = /*@__PURE__*/
  messageDesc(file_google_cloud_speech_v2_cloud_speech, 62);

/**
 * Enables speech transcription and resource management.
 *
 * @generated from service google.cloud.speech.v2.Speech
 */
export const Speech: GenService<{
  /**
   * Creates a [Recognizer][google.cloud.speech.v2.Recognizer].
   *
   * @generated from rpc google.cloud.speech.v2.Speech.CreateRecognizer
   */
  createRecognizer: {
    methodKind: "unary";
    input: typeof CreateRecognizerRequestSchema;
    output: typeof OperationSchema;
  },
  /**
   * Lists Recognizers.
   *
   * @generated from rpc google.cloud.speech.v2.Speech.ListRecognizers
   */
  listRecognizers: {
    methodKind: "unary";
    input: typeof ListRecognizersRequestSchema;
    output: typeof ListRecognizersResponseSchema;
  },
  /**
   * Returns the requested
   * [Recognizer][google.cloud.speech.v2.Recognizer]. Fails with
   * [NOT_FOUND][google.rpc.Code.NOT_FOUND] if the requested Recognizer doesn't
   * exist.
   *
   * @generated from rpc google.cloud.speech.v2.Speech.GetRecognizer
   */
  getRecognizer: {
    methodKind: "unary";
    input: typeof GetRecognizerRequestSchema;
    output: typeof RecognizerSchema;
  },
  /**
   * Updates the [Recognizer][google.cloud.speech.v2.Recognizer].
   *
   * @generated from rpc google.cloud.speech.v2.Speech.UpdateRecognizer
   */
  updateRecognizer: {
    methodKind: "unary";
    input: typeof UpdateRecognizerRequestSchema;
    output: typeof OperationSchema;
  },
  /**
   * Deletes the [Recognizer][google.cloud.speech.v2.Recognizer].
   *
   * @generated from rpc google.cloud.speech.v2.Speech.DeleteRecognizer
   */
  deleteRecognizer: {
    methodKind: "unary";
    input: typeof DeleteRecognizerRequestSchema;
    output: typeof OperationSchema;
  },
  /**
   * Undeletes the [Recognizer][google.cloud.speech.v2.Recognizer].
   *
   * @generated from rpc google.cloud.speech.v2.Speech.UndeleteRecognizer
   */
  undeleteRecognizer: {
    methodKind: "unary";
    input: typeof UndeleteRecognizerRequestSchema;
    output: typeof OperationSchema;
  },
  /**
   * Performs synchronous Speech recognition: receive results after all audio
   * has been sent and processed.
   *
   * @generated from rpc google.cloud.speech.v2.Speech.Recognize
   */
  recognize: {
    methodKind: "unary";
    input: typeof RecognizeRequestSchema;
    output: typeof RecognizeResponseSchema;
  },
  /**
   * Performs bidirectional streaming speech recognition: receive results while
   * sending audio. This method is only available via the gRPC API (not REST).
   *
   * @generated from rpc google.cloud.speech.v2.Speech.StreamingRecognize
   */
  streamingRecognize: {
    methodKind: "bidi_streaming";
    input: typeof StreamingRecognizeRequestSchema;
    output: typeof StreamingRecognizeResponseSchema;
  },
  /**
   * Performs batch asynchronous speech recognition: send a request with N
   * audio files and receive a long running operation that can be polled to see
   * when the transcriptions are finished.
   *
   * @generated from rpc google.cloud.speech.v2.Speech.BatchRecognize
   */
  batchRecognize: {
    methodKind: "unary";
    input: typeof BatchRecognizeRequestSchema;
    output: typeof OperationSchema;
  },
  /**
   * Returns the requested [Config][google.cloud.speech.v2.Config].
   *
   * @generated from rpc google.cloud.speech.v2.Speech.GetConfig
   */
  getConfig: {
    methodKind: "unary";
    input: typeof GetConfigRequestSchema;
    output: typeof ConfigSchema;
  },
  /**
   * Updates the [Config][google.cloud.speech.v2.Config].
   *
   * @generated from rpc google.cloud.speech.v2.Speech.UpdateConfig
   */
  updateConfig: {
    methodKind: "unary";
    input: typeof UpdateConfigRequestSchema;
    output: typeof ConfigSchema;
  },
  /**
   * Creates a [CustomClass][google.cloud.speech.v2.CustomClass].
   *
   * @generated from rpc google.cloud.speech.v2.Speech.CreateCustomClass
   */
  createCustomClass: {
    methodKind: "unary";
    input: typeof CreateCustomClassRequestSchema;
    output: typeof OperationSchema;
  },
  /**
   * Lists CustomClasses.
   *
   * @generated from rpc google.cloud.speech.v2.Speech.ListCustomClasses
   */
  listCustomClasses: {
    methodKind: "unary";
    input: typeof ListCustomClassesRequestSchema;
    output: typeof ListCustomClassesResponseSchema;
  },
  /**
   * Returns the requested
   * [CustomClass][google.cloud.speech.v2.CustomClass].
   *
   * @generated from rpc google.cloud.speech.v2.Speech.GetCustomClass
   */
  getCustomClass: {
    methodKind: "unary";
    input: typeof GetCustomClassRequestSchema;
    output: typeof CustomClassSchema;
  },
  /**
   * Updates the [CustomClass][google.cloud.speech.v2.CustomClass].
   *
   * @generated from rpc google.cloud.speech.v2.Speech.UpdateCustomClass
   */
  updateCustomClass: {
    methodKind: "unary";
    input: typeof UpdateCustomClassRequestSchema;
    output: typeof OperationSchema;
  },
  /**
   * Deletes the [CustomClass][google.cloud.speech.v2.CustomClass].
   *
   * @generated from rpc google.cloud.speech.v2.Speech.DeleteCustomClass
   */
  deleteCustomClass: {
    methodKind: "unary";
    input: typeof DeleteCustomClassRequestSchema;
    output: typeof OperationSchema;
  },
  /**
   * Undeletes the [CustomClass][google.cloud.speech.v2.CustomClass].
   *
   * @generated from rpc google.cloud.speech.v2.Speech.UndeleteCustomClass
   */
  undeleteCustomClass: {
    methodKind: "unary";
    input: typeof UndeleteCustomClassRequestSchema;
    output: typeof OperationSchema;
  },
  /**
   * Creates a [PhraseSet][google.cloud.speech.v2.PhraseSet].
   *
   * @generated from rpc google.cloud.speech.v2.Speech.CreatePhraseSet
   */
  createPhraseSet: {
    methodKind: "unary";
    input: typeof CreatePhraseSetRequestSchema;
    output: typeof OperationSchema;
  },
  /**
   * Lists PhraseSets.
   *
   * @generated from rpc google.cloud.speech.v2.Speech.ListPhraseSets
   */
  listPhraseSets: {
    methodKind: "unary";
    input: typeof ListPhraseSetsRequestSchema;
    output: typeof ListPhraseSetsResponseSchema;
  },
  /**
   * Returns the requested
   * [PhraseSet][google.cloud.speech.v2.PhraseSet].
   *
   * @generated from rpc google.cloud.speech.v2.Speech.GetPhraseSet
   */
  getPhraseSet: {
    methodKind: "unary";
    input: typeof GetPhraseSetRequestSchema;
    output: typeof PhraseSetSchema;
  },
  /**
   * Updates the [PhraseSet][google.cloud.speech.v2.PhraseSet].
   *
   * @generated from rpc google.cloud.speech.v2.Speech.UpdatePhraseSet
   */
  updatePhraseSet: {
    methodKind: "unary";
    input: typeof UpdatePhraseSetRequestSchema;
    output: typeof OperationSchema;
  },
  /**
   * Deletes the [PhraseSet][google.cloud.speech.v2.PhraseSet].
   *
   * @generated from rpc google.cloud.speech.v2.Speech.DeletePhraseSet
   */
  deletePhraseSet: {
    methodKind: "unary";
    input: typeof DeletePhraseSetRequestSchema;
    output: typeof OperationSchema;
  },
  /**
   * Undeletes the [PhraseSet][google.cloud.speech.v2.PhraseSet].
   *
   * @generated from rpc google.cloud.speech.v2.Speech.UndeletePhraseSet
   */
  undeletePhraseSet: {
    methodKind: "unary";
    input: typeof UndeletePhraseSetRequestSchema;
    output: typeof OperationSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_google_cloud_speech_v2_cloud_speech, 0);

